{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Objective**\n\n\nSeveral machine learning models are fit to a synthetic dataset of nine features to predict water potability in Kaggle task:\n\nhttps://www.kaggle.com/adityakadiwal/water-potability\n\n\n\n# **Notebook Contents**\n\n- Feature correlation and distributions are considered. \n\n- Missing data is analyzed and imputed. \n\n- The data is resampled to decrease class imbalance. \n\n- A model spot search is done using scikit-learn. \n\n- A three layer neural network is implemented using PyTorch Lightning. \n\n- Hyperparameters are found using Ray Tune from a user defined search space. \n\n- Hyperparameters and their resulting modelâ€™s accuracy are visualized in an interactive parallel coordinates plot.","metadata":{"id":"KUzVYKUVSVdu","_kg_hide-input":false}},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"pkd7nNjcOrjr"}},{"cell_type":"markdown","source":"## Library Imports","metadata":{"id":"Wtbf9aRZP_Kx"}},{"cell_type":"code","source":"%%capture \n\n# Debugging\nfrom pdb import set_trace as bp\n\n# Utility\nimport pandas as pd\nimport numpy as np \nimport random\nfrom itertools import combinations\nfrom tqdm import tqdm\nimport os\nimport json \n!pip install py7zr\nimport py7zr\nfrom py7zr import SevenZipFile\n!pip install py7zr multivolumefile\nimport multivolumefile\n\n# Stats metrics\nfrom scipy.stats import zscore, ks_2samp\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n!pip install beautifultable\nfrom beautifultable import BeautifulTable\n!pip install -U hiplot\nimport hiplot as hip\nfrom IPython.display import Image\n\n# Data over and under sampling\n!pip install imbalanced-learn\nimport imblearn\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.combine import SMOTEENN \n\n# Split and preprocess data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n# Scikit-learn models\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\n# Scikit-learn metrics\nfrom sklearn.metrics import precision_score,accuracy_score, roc_auc_score\n\n# PyTorch (for neural network models and pipeline)\n!pip install torch\n!pip install pytorch_lightning\n!pip install pytorch-metric-learning\n!pip install ray[tune]\nimport torch as th \nimport torchmetrics\nimport pytorch_lightning as pl\nfrom pytorch_metric_learning import losses\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nfrom ray import tune\nfrom ray.tune.integration.pytorch_lightning import TuneReportCallback\n\n","metadata":{"id":"YQdNmGy16mWW","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:18:39.248831Z","iopub.execute_input":"2021-07-22T21:18:39.249287Z","iopub.status.idle":"2021-07-22T21:20:00.581517Z","shell.execute_reply.started":"2021-07-22T21:18:39.249188Z","shell.execute_reply":"2021-07-22T21:20:00.580431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset import","metadata":{"id":"OX1K9tAHOuPr"}},{"cell_type":"code","source":"# read dataset into data frame\ndf = pd.read_csv('../input/water-potability/water_potability.csv')\n\n# Shorten decimals in data frames when printing\npd.options.display.float_format = \"{:,.4f}\".format","metadata":{"id":"XBQEZtFrvYLf","executionInfo":{"elapsed":8132,"status":"ok","timestamp":1626228884445,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"089c0299-5c69-4970-8175-380c11e6b995","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:20:00.583399Z","iopub.execute_input":"2021-07-22T21:20:00.583821Z","iopub.status.idle":"2021-07-22T21:20:00.639776Z","shell.execute_reply.started":"2021-07-22T21:20:00.583772Z","shell.execute_reply":"2021-07-22T21:20:00.6389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values Analysis","metadata":{"id":"Y0NjpTnTOzsR"}},{"cell_type":"code","source":"# Missing values check\n\n\n# Print dataset size\nsample_count = df.shape[0]            # Get row count from data frame\nprint('Total Samples:', sample_count, '\\n')\n\n# Table of missing value counts\nnull_sample_count = df.isnull().sum()      # Make Boolean table for null values, count 'True' for each column\nprint('Missing Values:')\nprint(null_sample_count, '\\n')\n\n# Table of missing values as a percentage\nnull_sample_count_potability_1 = df.loc[df['Potability']==1].isnull().sum()             # Get rows with 'Potability'=1, count null values by column\nnull_sample_count_potability_0 = df.loc[df['Potability']==0].isnull().sum()             # Above but for 'Potability'=0\n\ncount_potability_1 = df.loc[df['Potability']==1].shape[0]                             # Count potable samples\ncount_potability_0 = df.loc[df['Potability']==0].shape[0]                             # Count non-potable samples\nnull_sample_ratio_potability_1 = null_sample_count_potability_1.apply( lambda x: (x/count_potability_1)*100 )   # Get ratio of null potable samples to total potable samples \nnull_sample_ratio_potability_0 = null_sample_count_potability_0.apply( lambda x: (x/count_potability_0)*100 )   # Do above for non-potable                           \nnull_sample_ratio = null_sample_count.apply( lambda x: (x/sample_count)*100 )                        # Do above for whole dataset                               \n\ndrop_zero_columns = lambda df: df[df!=0]                                 # Define function to drop columns with only zero entries\nnull_sample_ratio_potability_1 = drop_zero_columns(null_sample_ratio_potability_1)    # Apply above function to data frame of: potable samples\nnull_sample_ratio_potability_0 = drop_zero_columns(null_sample_ratio_potability_0)    #                                       non-potable samples\nnull_sample_ratio          = drop_zero_columns(null_sample_ratio)             #                                               all data\n\nfor column in null_sample_ratio.index:  # Iterate through column headers of features in data with null samples\n    print('Missing',column, 'data as a % of')\n    print('All data         ', \"{:.2f}\".format(null_sample_ratio[column]))\n    print('Potable Data     ', \"{:.2f}\".format(null_sample_ratio_potability_1[column]))\n    print('Non-Potable Data ', \"{:.2f}\".format(null_sample_ratio_potability_0[column]),'\\n')\n\n\n# Class representation in dataset\nprint('Class representation:')\nprint(  round( (count_potability_1/sample_count)*100 ), '% potable data' )\nprint(  round( (count_potability_0/sample_count)*100 ), '% non-potable data', '\\n' )\n\n# Table counting multiple missing values in same sample\nnull_binary_df = df.isnull().astype(int)                    # Convert Boolean missing values table to 0/1 integers for samplewise sum operation\nco_occurring_null = null_binary_df.sum(axis=1).value_counts()   # Samplewise sum operation\nprint('Number of samples with exactly 0, 1, 2, or 3 missing values:')\nprint(co_occurring_null, '\\n')\n\ncolumns_w_null = df.isnull().any()                # Get column names with null values\ndf_null = df.isnull().loc[:, columns_w_null]      # Boolean table for null values excluding columns/features with no null values\nprint('Effect of one null value on the presence of another (Pearson correlation):')\nprint( df_null.corr().rename(columns={\"Trihalomethanes\": \"THM \"}) , '\\n')     # Rename Trihalomethanes to THM to fit in output window and print","metadata":{"id":"GbURNer809C4","executionInfo":{"elapsed":14,"status":"ok","timestamp":1626228884446,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"cd5f6bad-432b-4175-b317-5765b9727900","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:20:00.641668Z","iopub.execute_input":"2021-07-22T21:20:00.642043Z","iopub.status.idle":"2021-07-22T21:20:00.715489Z","shell.execute_reply.started":"2021-07-22T21:20:00.642011Z","shell.execute_reply":"2021-07-22T21:20:00.71433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are missing values. Most null values don't co-occur in the same sample. Too many to delete samples containing at least one missing value if it were decided they don't provide special value. The presence of one null value is not correlated to the presence of another.\n\nNon-potable data has a bit higher percentage of missing values than potable.\n\nThe dataset does look complete enough to potentially use with ML algorithms.","metadata":{"id":"3iT4nKo3OGXt"}},{"cell_type":"markdown","source":"# Feature Correlation Map","metadata":{"id":"79Fyz19HO8f3"}},{"cell_type":"code","source":"# Feature correlation heat map\n\n\n# Get Pearson correlation values\ndata = df.corr()    # Pairwise correlation with a null value is ignored\n\n# Generate heat map using seaborn\nfig, ax = plt.subplots(figsize=(12,8))                          # Create grid of empty subplots using matplotlib library                      \nmask = np.triu(np.ones_like(data, dtype=bool))                   # Mask correlation matrix along its line of symmetry to remove redencency and correlation of a feature with itself\nsns.heatmap(data, cmap='seismic', annot=True, mask=mask, ax=ax, vmin=-0.2, vmax=0.2)    # Create heat map useing seaborn library\nfig.text(0.5, 1.05, 'Correlation Heat Map', horizontalalignment='center', verticalalignment='center', fontsize=14, fontweight='bold', transform=ax.transAxes)   # Add title\nsns.set_style('white')        # Remove tick marks\n","metadata":{"id":"njbOs9BoF6Uq","executionInfo":{"elapsed":1176,"status":"ok","timestamp":1626228885957,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"2a9ab36e-dd11-40f8-9300-ce9b1877a366","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:20:49.248964Z","iopub.execute_input":"2021-07-22T21:20:49.24935Z","iopub.status.idle":"2021-07-22T21:20:49.936899Z","shell.execute_reply.started":"2021-07-22T21:20:49.249317Z","shell.execute_reply":"2021-07-22T21:20:49.935703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest coefficient is -0.17, between sulfate and solids. This indicates low correlation between all features. No features are redundant but this will make imputing missing values harder as they can't be estimated by regressing on another feature. Deletion or replacement with the median will be used over more sophisticated methods such as Multiple Imputation by Chained Equations.\n\nAdditionally, none of these features individually are very correlated with the dependent variable, Potability.","metadata":{"id":"-U7LuSQ_2acP"}},{"cell_type":"markdown","source":"# Feature Distribution Analysis","metadata":{"id":"VfTFcZ47PBP7"}},{"cell_type":"code","source":"# Box Plots\n\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))  # Create empty grid of subplots\nfig.subplots_adjust(hspace=.5)                       # Adjust vertical/height spacing \n\n# Fill each subplot with the distribution of a feature separated by potability\na=0                               # Increment subplot coordinates\nfor feature in df.drop('Potability', axis=1):    # Iterate through features ('Potability' is a label) \n  df.boxplot(by='Potability', column=[feature], ax=axes[ a%2, a%5 ], grid=False)    # Create boxplots for each feature grouped by potable or not (df.boxplot() auto handles nan correctly). Subplot coordinates [a%2, a%5] start top left and vertically zig zag moving right.\n  a+=1\n\naxes[1,4].remove()        # Remove unnecessary subplot from 2x5 grid\nplt.show()","metadata":{"id":"2mzMeFdgikAW","executionInfo":{"elapsed":1897,"status":"ok","timestamp":1626228887847,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"d58cd98b-ac1d-47c4-ca7c-f19c4b2cc9ac","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:20:54.658841Z","iopub.execute_input":"2021-07-22T21:20:54.659208Z","iopub.status.idle":"2021-07-22T21:20:55.76485Z","shell.execute_reply.started":"2021-07-22T21:20:54.659176Z","shell.execute_reply":"2021-07-22T21:20:55.763881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data looks normally distributed with a few outliers.","metadata":{"id":"-aVksoRdNpcL"}},{"cell_type":"code","source":"# Compute Z-score for each distribution to quantify outliers\n\ndef score_summary(df):\n  scores = df.apply(zscore, nan_policy='omit')                     # Compute zscore columnwise, omit null values\n  outlier_count = scores[scores > 3.0].count()                     # Count all data over 3 deviations from the mean as outliers\n  outlier_percent = ( outlier_count / df.count() )*100             # Get percent of non-null data considered an outlier featurewise\n  return outlier_count, outlier_percent\n\ndf_potability_1 = df.loc[ df['Potability']==1 ].drop('Potability', axis=1)                   # Separate data out by class\ndf_potability_0 = df.loc[ df['Potability']==0 ].drop('Potability', axis=1)\n\ndf_potability_1_count   = score_summary(df_potability_1)[0]       # Get outlier count   featurewise for potable data\ndf_potability_1_percent = score_summary(df_potability_1)[1]       # Get outlier percent featurewise for potable data\n\ndf_potability_0_count   = score_summary(df_potability_0)[0]       # Get outlier count   featurewise for non-potable data\ndf_potability_0_percent = score_summary(df_potability_0)[1]       # Get outlier percent featurewise for non-potable data\n\nfor column in df.drop('Potability', axis=1):      # Iterate through features (drop class)\n    print('\\033[1m'+column+'\\033[0m')\n    print('\\033[4m'+'Outlier Count'+'\\033[0m')\n    print('Potable                ', df_potability_1_count[column])\n    print('Non-Potable            ', df_potability_0_count[column])\n    print('\\033[4m'+'As a percent of Samples'+'\\033[0m')\n    print('Potable                ', \"{:.2f}\".format(df_potability_1_percent[column]),'%')\n    print('Non-Potable            ', \"{:.2f}\".format(df_potability_0_percent[column]),'%', '\\n')\n","metadata":{"id":"UVEar4TkxTJg","executionInfo":{"elapsed":20,"status":"ok","timestamp":1626228887849,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"5cf559cc-cd3f-4482-f4ce-6613313d712d","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:20:59.236513Z","iopub.execute_input":"2021-07-22T21:20:59.237083Z","iopub.status.idle":"2021-07-22T21:20:59.300464Z","shell.execute_reply.started":"2021-07-22T21:20:59.237047Z","shell.execute_reply":"2021-07-22T21:20:59.299746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers may be an important rare case in this dataset of only 9 features to predict water portability. However, creating a separate model to handle these cases may be hard with the number of outliers.","metadata":{"id":"Vs9L0ynGXmAS"}},{"cell_type":"markdown","source":"# Train/Test Split","metadata":{"id":"6oNzT1ajPGMx"}},{"cell_type":"code","source":"# Split train/test\n\nprint('Whole data frame shape:', df.shape, '\\n')\n\ndf_y = df['Potability']                                                     # Create data frame for only data labels \ndf_x = df.drop('Potability', axis=1)                                                # Create data frame for only data features\n\ntrain_x, test_x, train_y, test_y = train_test_split(df_x, df_y, stratify=df_y, test_size=0.2, random_state=1)        # Split dataset 20% test, 80% train. Stratify 'Potability'.\n\ntrain = pd.concat([train_x, train_y], axis=1)       # Recombine features and label into one data frame\ntest  = pd.concat([test_x,  test_y ], axis=1)\n\nprint('Train data frame shape:', train.shape)\nprint('Test data frame shape:', test.shape)\nprint('All rows accounted for:', train.shape[0] + test.shape[0] == df.shape[0] , '\\n')\n","metadata":{"id":"l8FLJufFdhab","executionInfo":{"elapsed":16,"status":"ok","timestamp":1626228887850,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"6301988e-f07d-45f4-d32e-98b40b293834","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:26.373852Z","iopub.execute_input":"2021-07-22T21:21:26.37422Z","iopub.status.idle":"2021-07-22T21:21:26.393251Z","shell.execute_reply.started":"2021-07-22T21:21:26.374185Z","shell.execute_reply":"2021-07-22T21:21:26.392262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handle Missing Values","metadata":{"id":"4G3R57npPJ9Y"}},{"cell_type":"code","source":"# Consider imputing missing values with median\n\nprint('Difference in featurewise medians between potable and non-potable samples in train data:')\nprint(  ( train[train['Potability']==1].median() - train[train['Potability']==0].median() ) [df.isnull().any()]  )    # Of potable samples, find featurewise medians. Do same for Non-potable samples. Subtract the difference. Drop features with no null values. \n\n\ndef hist_features_w_null(df, subset):\n  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,3))      # Create empty figure with three subplots\n  features = df.columns[df.isnull().any()]                        # Get list of features with null values\n  for i in range(len(features)):                                  # Iterate through range that will index axis location of subplot and feature within features iterable\n    subset.hist(features[i], bins=100, ax=axes[i])                 # Plot histogram using pandas extension of matplotlib\n  plt.show()\n\nprint('\\n\\n' + 'Feature distributions in train dataset:'+ '\\n')\nhist_features_w_null(df, train)\n\nprint('\\n' + 'Median values in train data:')\nprint( train.median()[df.isnull().any()] )","metadata":{"id":"N_VcDkz3ZPne","executionInfo":{"elapsed":1042,"status":"ok","timestamp":1626228888880,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"384e8e34-09af-4a2e-fde8-003dba8f7e97","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:29.299874Z","iopub.execute_input":"2021-07-22T21:21:29.30027Z","iopub.status.idle":"2021-07-22T21:21:30.413005Z","shell.execute_reply.started":"2021-07-22T21:21:29.300235Z","shell.execute_reply":"2021-07-22T21:21:30.411837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The median of a feature with null samples is similar between potable and non-potable groups. The differences in median value don't represent large movement on their respective feature histograms. Median imputation will fill null values since this does not affect feature distributions much.\n","metadata":{"id":"C4LV8-bVdOoM"}},{"cell_type":"code","source":"# Impute missing values\ntrain_median = train.median()                                    # Find each columns median value in train set\nfor feature in df.drop('Potability', axis=1):                            # For feature in original data frame\n  train[feature].fillna(train_median[feature], inplace = True)   # Replace missing values in a feature's data with its respective median value (found on train data to prevent data leakage)\n  test[feature].fillna( train_median[feature], inplace = True)\n","metadata":{"id":"G2ouzfeWJYX9","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:33.463612Z","iopub.execute_input":"2021-07-22T21:21:33.463956Z","iopub.status.idle":"2021-07-22T21:21:33.477083Z","shell.execute_reply.started":"2021-07-22T21:21:33.463924Z","shell.execute_reply":"2021-07-22T21:21:33.476347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histograms of features after imputing\n\nprint('\\n\\n' + 'Feature distributions in train dataset:'+ '\\n')\nhist_features_w_null(df, train)\n\nprint('\\n\\n' + 'Feature distributions in test dataset:'+ '\\n')\nhist_features_w_null(df, test)","metadata":{"id":"JqDE2rVtBwGt","executionInfo":{"elapsed":3026,"status":"ok","timestamp":1626228891897,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"020b643a-69d2-4e89-cfb7-635455fbc0b6","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:36.301365Z","iopub.execute_input":"2021-07-22T21:21:36.301851Z","iopub.status.idle":"2021-07-22T21:21:38.194013Z","shell.execute_reply.started":"2021-07-22T21:21:36.30182Z","shell.execute_reply":"2021-07-22T21:21:38.192892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box plot original data, train data, and test data after imputing missing values\n\nfor feature in df.drop('Potability', axis=1):                                 # For feature in data\n  fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True)        # Create three empty subplots\n  df.boxplot(by='Potability', column=[feature], ax=axes[0], grid=False).set_title('All data')   # Boxplot original data\n  train.boxplot(by='Potability', column=[feature], ax=axes[1], grid=False).set_title('Train')   # Boxplot train set\n  test.boxplot(by='Potability', column=[feature], ax=axes[2], grid=False).set_title('Test')     # Boxplot test set\n  fig.suptitle(feature)               # Title overall figure the feature name\n  fig.tight_layout()\n  plt.subplots_adjust(top=0.85)\n  plt.show()","metadata":{"id":"N84NfE0wZ6AM","executionInfo":{"elapsed":4961,"status":"ok","timestamp":1626228896851,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"505697bc-55d6-4f2f-da22-a23bf08de0f4","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:40.863693Z","iopub.execute_input":"2021-07-22T21:21:40.86407Z","iopub.status.idle":"2021-07-22T21:21:45.53931Z","shell.execute_reply.started":"2021-07-22T21:21:40.864034Z","shell.execute_reply":"2021-07-22T21:21:45.538253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All distributions remain approximately the same. The mean of sulfate's non-potable test set is a bit low, skewing its distribution to the right. Sulfate had the most data missing at 23.84%","metadata":{"id":"C4RHSTUbTPkZ"}},{"cell_type":"markdown","source":"# Resample To Balance Classes","metadata":{"id":"OQP4HHO5PWY7"}},{"cell_type":"code","source":"# SMOTE over sampling and Tomek links under sampling to let the model better focus the potable class\n\n# Print counts before resampling\nprint('Potable sample count by feature')\nprint(train[train['Potability']==1].count())\nprint('\\n'+'Non-Potable sample count by feature')\nprint(train[train['Potability']==0].count())\n\n\n# Separate out features from label\ntrain_x = train.drop('Potability', axis=1)\ntrain_y = train['Potability']\n\n# Preform over and under sampling using imbalanced-learn library\nsm = SMOTETomek(random_state=42, sampling_strategy=0.8)       # sampling_strategy = ratio of minority class to majority class after resampling\ntrain_x, train_y = sm.fit_resample(train_x, train_y)\n\n# Get train data back into pandas data frame and recombine labels with features\ntrain_x = pd.DataFrame(train_x, columns=df.drop('Potability', axis=1).columns)    # Get corresponding column names from original data frame\ntrain_y = pd.DataFrame(train_y, columns=['Potability'])\ntrain = pd.concat([train_x, train_y], axis=1)\n\n\n# Print counts after resampling\nprint('\\n\\n'+'After SMOTE over sampling and Tomek links under sampling')\nprint('\\n'+'Potable sample count by feature')\nprint(train[train['Potability']==1].count())\nprint('\\n'+'Non-Potable sample count by feature')\nprint(train[train['Potability']==0].count())\n","metadata":{"id":"IeqnMtga7zj2","executionInfo":{"elapsed":28,"status":"ok","timestamp":1626228897476,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"ee5414e8-60b4-45ef-df7a-ae59308a7928","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:51.412104Z","iopub.execute_input":"2021-07-22T21:21:51.412441Z","iopub.status.idle":"2021-07-22T21:21:51.463592Z","shell.execute_reply.started":"2021-07-22T21:21:51.412408Z","shell.execute_reply":"2021-07-22T21:21:51.462906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardize Data","metadata":{"id":"jd7WvMN8PhGQ"}},{"cell_type":"code","source":"# Normalize and zero mean train and test set\n\n# Separate out features from labels for train and test sets\ntrain_x = train.drop('Potability', axis = 1)\ntest_x  = test.drop('Potability', axis = 1)\ntrain_y = train['Potability']\ntest_y  = test['Potability']\n\nscaler = StandardScaler()\nscaler.fit(train_x)                     # Fit scaler to train data\ntrain_x = scaler.transform(train_x)     # Transform train data with scaler fit on train data\ntest_x  = scaler.transform(test_x)      # Transform test  data with scaler fit on train data","metadata":{"id":"aTVSldRROdxR","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:21:56.652401Z","iopub.execute_input":"2021-07-22T21:21:56.652927Z","iopub.status.idle":"2021-07-22T21:21:56.665189Z","shell.execute_reply.started":"2021-07-22T21:21:56.652893Z","shell.execute_reply":"2021-07-22T21:21:56.664257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class imbalance in train and test sets\n\n# Train set class imbalance\nprint('Train set (Resampled)')\nprint( 'Percent Potable    ', round((train_y[train_y==1].count() / train_y.shape[0])*100) ) # Use separate label data frame to count potable samples and divide by total. Multiply by 100 to get percentage then round.\nprint( 'Percent Non-Potable', round((train_y[train_y==0].count() / train_y.shape[0])*100) ) # Same as above but count non-potable this time\n\n# Test set class imbalance\nprint('\\n'+'Test set ')\nprint( 'Percent Potable    ', round((test_y[test_y==1].count() / test_y.shape[0])*100) )\nprint( 'Percent Non-Potable', round((test_y[test_y==0].count() / test_y.shape[0])*100) )","metadata":{"id":"22eac93Lcb3k","executionInfo":{"elapsed":26,"status":"ok","timestamp":1626228897481,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"5352db7b-2d7b-4536-8464-eb6e5ba1e35b","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:22:00.545442Z","iopub.execute_input":"2021-07-22T21:22:00.545787Z","iopub.status.idle":"2021-07-22T21:22:00.557676Z","shell.execute_reply.started":"2021-07-22T21:22:00.545757Z","shell.execute_reply":"2021-07-22T21:22:00.556601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{"id":"txZ521GbPmiJ"}},{"cell_type":"markdown","source":"## sklearn Models ","metadata":{"id":"Ty3l_ZNdPvoT"}},{"cell_type":"code","source":"models = [  \n  LogisticRegression(),\n  SVC(),\n  KNeighborsClassifier(),\n  DecisionTreeClassifier(),\n  GaussianNB(),\n  RandomForestClassifier(),\n  AdaBoostClassifier(),\n  GradientBoostingClassifier(),\n  MLPClassifier()   \n  ]\n\n\nfor model in models:\n  model = model\n  model.fit(train_x, train_y)\n  model_results = model.predict(test_x)\n  precision = precision_score(test_y, model_results,average='macro')\n  roc_auc = roc_auc_score(test_y, model_results,average='macro')\n  acc = accuracy_score(test_y, model_results)\n\n  print('\\n\\n\\n'+'________________________________________________________________________________________'+'\\n\\n\\n')\n  print(model)\n  print('\\n')\n  print('Precision                                ', precision)\n  print('Receiver Operating Characteristic Curve  ', roc_auc)\n  print('Accuracy                                 ', acc)","metadata":{"id":"iXP870pkT16c","executionInfo":{"elapsed":5687,"status":"ok","timestamp":1626229017618,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"5e911019-3d4f-4ef1-bcf4-bfa0f46e419e","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:22:05.138437Z","iopub.execute_input":"2021-07-22T21:22:05.138917Z","iopub.status.idle":"2021-07-22T21:22:11.564503Z","shell.execute_reply.started":"2021-07-22T21:22:05.138887Z","shell.execute_reply":"2021-07-22T21:22:11.563372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note the train set was resampled so its class imbalance is 43% potable, 57% non-potable. The test set was not resampled, it retains a class imbalance of 39% potable,\n61% non-potable. So if a model only guessed non-potable it would have 61% accuracy. Precision and area under the ROC curve are used to evaluate performance alongside accuracy. \n\nLogistic Regression's accuracy is the closest to the test set's percent of non-potable data. This is consistent with an area under the ROC curve of 0.5, the lowest predictive power. Logistic Regression had a precision score of 80% showing precision alone is not always a good substitute for accuracy on an imbalanced dataset.\n\nSupport vector classifier did the best at 70.6% accuracy, 71.0% precision, and 0.65 area under ROC curve. The data must be somewhat linearly separable by class.","metadata":{"id":"Yc38R7Y5R8dY"}},{"cell_type":"markdown","source":"## PyTorch Lightning Model","metadata":{"id":"1PNA5spsK8RT"}},{"cell_type":"code","source":"# Data Prep\n\n\ndef prepare_data(features, labels):\n  dataset = []\n  for index in zip(features, labels):               # Loop through the feature and label data frames while matching their indices  \n    input = th.Tensor(index[0])                  # Convert the feature vector to the Tensor type default; floating point\n    label = th.tensor(index[1])                  # Convert the label vector to a tensor type inferred from integer data\n    dataset.append( {'input': input, 'label': label} )    # Append to running list in record format\n  return dataset\n\ntrain = prepare_data(train_x, train_y)              # Call above function on train dataset (which has been split into feature and label data frames for the sklearn models)\ntest = prepare_data(test_x, test_y)               # Call above function to preprocess test set","metadata":{"id":"0ur9Il-VRA2x","_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model\n\n%reload_ext tensorboard\n%tensorboard --logdir drive/MyDrive/Water_Potability/train_model --port 6006      # Specifying the port sometimes prevents an error keeping tensorboard from appearing\n\n\nclass PotableWaterClassifier(pl.LightningModule):\n  def __init__(self, config, train_ds, val_ds):                                 # PyTorch lightning model is wrapped with Ray Tune, so Ray Tune's config file and train/val data are passed to __init__\n    super().__init__()\n \n    self.lr = config[\"lr\"]             # Learning rate, batch size, and layer sizes are all picked from Ray Tune's defined search space then passed to the class instance \n    self.batch_size = config[\"batch_size\"]\n    layer_1, layer_2, layer_3 = config[\"layer_1\"], config[\"layer_2\"], config[\"layer_3\"]\n \n    self.layer_1 = th.nn.Linear(9, layer_1)                     # Input has 9 features\n    self.layer_2 = th.nn.Linear(  layer_1, layer_2)\n    self.layer_3 = th.nn.Linear(        layer_2, layer_3)\n    self.layer_4 = th.nn.Linear(              layer_3, 2)       # Two output neurons for the binary potable/non-potable classification\n\n    self.accuracy = torchmetrics.Accuracy()\n    self.loss = th.nn.CrossEntropyLoss()        # CrossEntropyLoss class defined here to be used with softmax function defined in final layer of forward method\n    self.train_ds = train_ds\n    self.val_ds = val_ds\n    \n  def forward(self, x):           # x = one batch of shape: (batch size, input feature number)\n    x = self.layer_1(x)          \n    x = th.nn.functional.elu_(x)    # ELU activation is used after each layer until final layer which uses a softmax\n    x = self.layer_2(x)\n    x = th.nn.functional.elu_(x)\n    x = self.layer_3(x)\n    x = th.nn.functional.softmax(x, dim=1)  # x.shape = (batch size, layer size) so softmax is applied along layer dimension (dim=1)\n    return x\n\n  def configure_optimizers(self):\n    optimizer = th.optim.AdamW( self.parameters(), lr = self.lr )\n    scheduler = th.optim.lr_scheduler.ReduceLROnPlateau( optimizer, patience = 1 )    # Patience sets number of epochs with no improvement to reduce learning rate, default threshold for improvement: threshold=0.0001\n    return {\n          'optimizer': optimizer,\n          'lr_scheduler': scheduler,\n          'monitor': 'ptl/train_loss'     # 'monitor' is the metric 'lr_scheduler' evaluates before changing the learning rate\n      }\n\n  def training_step(self, batch, batch_idx):  # Batch is a list of records with ids: 'input' and 'label'\n    logits = self.forward(batch['input'])     # Get forward pass using batch of inputs\n    loss  = self.loss(logits, batch['label'])   # Calculate loss from batch of model outputs and their labels\n    self.log('ptl/train_loss', loss)          # Log loss for the learning rate scheduler to monitor \n    return loss\n\n  def validation_step(self, batch, batch_idx):    # The entire test/val set is passed as one batch\n    logits = self.forward(batch['input'])        \n    acc  = self.accuracy(logits, batch['label'])\n    self.log('ptl/val_accuracy', acc)           # Log test/val accuracy for Ray Tune to decide what epoch to abandon trial\n\n  def train_dataloader(self):\n    return th.utils.data.DataLoader(\n        self.train_ds,\n        batch_size = self.batch_size,\n        drop_last = True,\n        shuffle = True,\n    )\n\n  def val_dataloader(self):\n    return th.utils.data.DataLoader(\n        self.val_ds,\n        batch_size = len(self.val_ds),    # Send entire test/val set to validation_step in one batch\n        drop_last = False,\n        shuffle = False,\n    )\n\ndef train_model(config, num_epochs, train_ds, val_ds):\n  model = PotableWaterClassifier(config, train_ds, val_ds)     # Create model instance with Ray Tune config file, train data, and test/val data\n  metrics = {'acc': 'ptl/val_accuracy'}                    # Define metrics to pass to Ray Tune callback so it can stop trails early\n  trainer = pl.Trainer(\n      progress_bar_refresh_rate=0,                          # Make the output cleaner\n      callbacks=[TuneReportCallback(metrics, on='validation_end')],    # Report PyTorch lightning metrics to Ray Tune\n      max_epochs = num_epochs,                          # Max number of epochs to run a trial\n      gpus = (1 if th.cuda.is_available() else 0),\n      )\n  trainer.fit(model)\n\nnum_trials = 4000\nepochs = 30\n\n# Define hyperparameter search space\nconfig = {                                    \n    'layer_1': tune.choice( list(range(2,200,2)) ),\n    'layer_2': tune.choice( list(range(2,200,2)) ),\n    'layer_3': tune.choice( list(range(2,200,2)) ),\n    'lr': tune.loguniform(0.00001, 0.001),\n    'batch_size': tune.choice( list(range(20,500,20)) ),\n}\n\n# Ray Tune wrapper for PyTorch lightning\ntrainable = tune.with_parameters(     \n    train_model,                 # Pass train_model() function\n    num_epochs=epochs,           # Everything else is passed to train_model()\n    train_ds=train,               \n    val_ds=test\n    )\n\nanalysis = tune.run(\n    trainable,\n    resources_per_trial={ 'cpu': 1, 'gpu': (1 if th.cuda.is_available() else 0), },\n    metric='acc',        # Abandon trial based on accuracy \n    mode='max',        # Maximize metric defined above\n    config=config,\n    num_samples=num_trials,\n    name='train_model',       # Name folder created in local_dir to store Ray Tune and PyTorch lightning output\n    local_dir = 'drive/MyDrive/Water_Potability/',\n    )","metadata":{"id":"YARc4YXHWwBX","_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ray Tune results from PyTorch Lightning model collected in different environment. Results imported here.\nwith multivolumefile.open('../input/import/train_model/train_model.7z', mode='rb') as target_archive:\n    with SevenZipFile(target_archive, 'r') as archive:\n        archive.extractall()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:22:16.213715Z","iopub.execute_input":"2021-07-22T21:22:16.214098Z","iopub.status.idle":"2021-07-22T21:23:58.500987Z","shell.execute_reply.started":"2021-07-22T21:22:16.214061Z","shell.execute_reply":"2021-07-22T21:23:58.499805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter visualization\n\nmodel_logs = [entry for entry in os.listdir('/kaggle/working/train_model/') if entry[:11]=='train_model']    # Get Ray Tune output file names\n\nplt_data = []                   # Data to be plotted in parallel coordinates graph\nfor model_log in tqdm(model_logs):   # For Ray Tune trial\n  path = '/kaggle/working/train_model/' + model_log + '/result.json'   # Get file path of trial result  \n  f = open(path)\n  if os.path.getsize(path) != 0:          # If trial result is not empty\n    result = json.loads(f.readlines()[-1])    # Load data from the last epoch reached\n    info = result['config']             # Get trial hyperparameters (returns dictionary)\n    info['acc'] = result['acc']           # Add 'accuracy' key to info and assign trial accuracy\n    plt_data.append(info)             # Append trial to list of trials to be plotted\n\nhip.Experiment.from_iterable(plt_data).display()    # Create parallel coordinates graph using HiPlot library from Facebook","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:23:58.502549Z","iopub.execute_input":"2021-07-22T21:23:58.50284Z","iopub.status.idle":"2021-07-22T21:23:58.925341Z","shell.execute_reply.started":"2021-07-22T21:23:58.502814Z","shell.execute_reply":"2021-07-22T21:23:58.92444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest accuracy achieved is 68%. The test set has a class imbalance of 61% non-potable to 39% potable, so the best model beats a model only choosing non-potable by seven points. Additional metrics were not calculated but would help interpretation of results. For reference, accuracy in the Scikit-learn MLPClassifier is two points above its 65% precision. \n\nThe lowest accuracy models mostly have an initial learning rate below 1e-4. Batch sizes tend to be over 250. The training set has 1620 examples making batch sizes over 250 split the data six ways at the most.","metadata":{"id":"Lh0eja0AOBsz"}},{"cell_type":"code","source":"Image('../input/import/images/low_acc.PNG')","metadata":{"id":"p09Qh6BlCays","executionInfo":{"elapsed":26403,"status":"ok","timestamp":1626278979237,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"f6e83375-6be8-4fdd-a793-c734b802ea12","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:26:17.007355Z","iopub.execute_input":"2021-07-22T21:26:17.008248Z","iopub.status.idle":"2021-07-22T21:26:17.352818Z","shell.execute_reply.started":"2021-07-22T21:26:17.008204Z","shell.execute_reply":"2021-07-22T21:26:17.351739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest accuracy models use initial learning rates mostly between 1e-4 and 1e-3. Batch sizes are mostly under 250.","metadata":{"id":"W5Z9_Pk-FVQJ"}},{"cell_type":"code","source":"Image('../input/import/images/high_acc.PNG')","metadata":{"id":"nECC80qaFOXU","executionInfo":{"elapsed":44,"status":"ok","timestamp":1626278940683,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"62d1777d-cfbd-4eb3-dcab-a4a252d2f1ca","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:32:16.855882Z","iopub.execute_input":"2021-07-22T21:32:16.856534Z","iopub.status.idle":"2021-07-22T21:32:17.147449Z","shell.execute_reply.started":"2021-07-22T21:32:16.856483Z","shell.execute_reply":"2021-07-22T21:32:17.146625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a network gets deeper, decreasing layer size encourages gradual consolidation of information down to the decision point. Making a network wider with depth encourages extrapolation to erroneous details which could amplify noise and cause overfitting. Here, models that expanded width with depth actually achieved high and low accuracy in similar numbers. However, less models that decreased layer size with depth had low accuracy rather than high.","metadata":{"id":"HMCKgP_lFVno"}},{"cell_type":"code","source":"Image('../input/import/images/increasing_layers.PNG')","metadata":{"id":"ziEiHp0NFOsc","executionInfo":{"elapsed":17257,"status":"ok","timestamp":1626278997462,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"cd9f5e3d-88b6-4944-9919-278f5f384abd","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:32:24.100221Z","iopub.execute_input":"2021-07-22T21:32:24.100851Z","iopub.status.idle":"2021-07-22T21:32:24.287149Z","shell.execute_reply.started":"2021-07-22T21:32:24.100816Z","shell.execute_reply":"2021-07-22T21:32:24.286106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image('../input/import/images/decreasing_layers.PNG')","metadata":{"id":"IGdfe6hSFO9e","executionInfo":{"elapsed":13741,"status":"ok","timestamp":1626279011155,"user":{"displayName":"Drive drive","photoUrl":"","userId":"11674995439267798122"},"user_tz":300},"outputId":"ba79be90-a3b9-436b-86ac-ab3cd70784e5","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T21:32:31.031991Z","iopub.execute_input":"2021-07-22T21:32:31.032373Z","iopub.status.idle":"2021-07-22T21:32:31.190311Z","shell.execute_reply.started":"2021-07-22T21:32:31.032342Z","shell.execute_reply":"2021-07-22T21:32:31.187551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top three models:\n\n| layer_1 | layer_2 | layer_3 | lr | batch_size | acc |\n| --- | --- | --- | --- | --- | --- |\n| 80 | 106 | 2 | 0.0007332761721359008 | 60 | 0.6844512224197388 |\n| 184 | 128 | 78 | 0.0005416626161799641 | 340 | 0.6615853905677795 |\n| 170 | 188 | 198 | 0.0005559121098672098 | 100 | 0.6631097793579102 |","metadata":{"id":"7kkw76vaFV9g"}},{"cell_type":"markdown","source":"It would be interesting to train neural networks on data already shown to allow prediction with high accuracy and take more samples from the hyperparameter search space. Additionally, the success or failure of models under different configurations could be quantified to aid the visual representation.","metadata":{"id":"qr-GiVWkHqpH"}},{"cell_type":"markdown","source":"[More complete version of notebook that runs in Google Colab](https://github.com/MeganKern/WaterPotability/tree/main \"More complete version of notebook that runs in Google Colab\")","metadata":{}}]}