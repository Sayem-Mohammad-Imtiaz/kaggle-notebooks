{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Automated Hyperparameter Tuning and EDA\n\n This notebook would be focusing on automated hyperparameter techniques. We would be skipping Grid Search and Randomised Search as they are already commonly used in many of the notebooks\n \n### Automated Hyperparameter Tuning helps since we dont have to use time and resource intensive grid search techniques to get good results.\n\nThe three hyperparameter optimization techniques that we would use are as below:-\n\n1. Scikit-optimize\n\n2. Hyperopt\n\n3. Optuna\n\nEdit-\nDocumentation for the libraries are below:-\n\nhttps://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html\n\nhttp://hyperopt.github.io/hyperopt/\n\nhttps://optuna.readthedocs.io/en/stable/\n\n\n### We would do some basic EDA before we start with the optimization. We would not be doing any feature engineering since our focus is hyperparameter tuning which gives us good results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's import all the necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U scikit-learn==0.23\n!pip install scikit-optimize==0.8.1\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm\nfrom skopt import gp_minimize,space\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom skopt.utils import use_named_args\nfrom hyperopt import hp,Trials,tpe,fmin\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.plotting import main_plot_history\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the input data into a dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the dimensions of the dataset imported","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at a few rows to get a sense of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will check for any missing values. Upon checking it seem there are no missing values in this dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the percentage of males and females","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f , ax = plt.subplots()\nplt.pie(dataset[\"sex\"].value_counts(),explode=[0,.1],labels=[\"Male\",\"Female\"],startangle=90,shadow=True,autopct = '%1.1f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to our dataset females are at a higher risk of heart disease than males.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,7))\nsns.countplot(\"sex\",hue=\"target\",data=dataset)\nbars = ax.patches\nhalf = int(len(bars)/2)\nax.set_xticklabels([\"female\",\"male\"])\nax.legend([\"absence\",\"presence\"])\nfor first,second in (zip(bars[:half],bars[half:])):\n    height1= first.get_height()\n    height2= second.get_height()\n    total = height1 + height2\n    ax.text(first.get_x()+first.get_width()/2,height1+2,'{0:.0%}'.format(height1/total),ha=\"center\")\n    ax.text(second.get_x()+second.get_width()/2,height2+2,'{0:.0%}'.format(height2/total),ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 35 to 45 age band has the highest percentage of affected cases.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.loc[:,\"age_band\"] = pd.cut(dataset.age,bins=[25,35,45,60,80])\nf,ax = plt.subplots(figsize=(10,8))\nsns.countplot(\"age_band\",hue=\"target\",data=dataset)\nbars = ax.patches\nhalf = int(len(ax.patches)/2)\nax.legend([\"absence\",\"presence\"])\n\nfor first,second in zip(bars[:half],bars[half:]):\n    height1 =  first.get_height()\n    height2 = second.get_height()\n    total_height= height1+height2\n    ax.text(first.get_x()+first.get_width()/2, height1+1,'{0:.0%}'.format(height1/total_height), ha ='center')\n    ax.text(second.get_x()+second.get_width()/2, height2+1,'{0:.0%}'.format(height2/total_height), ha ='center')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except for the 60 to 80 age band, rest of the bands are highly skewed towards males.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax= plt.subplots()\nsns.countplot(\"age_band\",hue=\"sex\",data=dataset)\nax.legend([\"female\",\"male\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is higher cholestrol count in cases where there in no disease, contrary to common knowledge.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,7))\nsns.boxplot(\"target\",\"chol\",data=dataset)\nax.set_xticklabels([\"absence\",\"presence\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets move on to modelling. First we would split our dataset in to train and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y= dataset[\"target\"]\ndataset.drop([\"target\",\"age_band\"],axis=1,inplace=True)\nX_train,X_test,y_train,y_test = train_test_split(dataset,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scikit-optimize Hyperparamter Tuning\nWe would use RandomForest for tuning. \n\nBelow we create the parameter space and the objective function to be minimized.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_space_skopt =[\n    space.Integer(3,10,name=\"max_depth\"),\n    space.Integer(50,1000,name=\"n_estimators\"),\n    space.Categorical([\"gini\",\"entropy\"],name=\"criterion\"),\n    space.Real(0.1,1,name=\"max_features\"),\n    space.Integer(2,10,name=\"min_samples_leaf\")\n]\n\nmodel = RandomForestClassifier()\n\n@use_named_args(param_space_skopt)\ndef objective_skopt(**params_skopt):\n    model.set_params(**params_skopt)\n    skf = StratifiedKFold(n_splits=5,random_state=42)\n    scores = -np.mean(cross_val_score(model,X_train,y_train,cv=skf,scoring=\"accuracy\"))\n    return scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We call the gp_minimize function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = gp_minimize(objective_skopt,dimensions= param_space_skopt, n_calls=25, n_random_starts=10,verbose=10,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the best score received.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"-result.fun","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the results vs the calls to the objective function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from skopt.plots import plot_convergence\nplot_convergence(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are testing the best parameters on our test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_skopt =RandomForestClassifier(n_estimators= result.x[1],criterion=result.x[2],max_depth=result.x[0],min_samples_leaf=result.x[4],max_features=result.x[3],random_state=42)\nmodel_skopt.fit(X_train,y_train)\ny_pred_skopt = model_skopt.predict(X_test)\nskopt_score = accuracy_score(y_test,y_pred_skopt)\nskopt_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperopt Hyperparameter Tuning\n\nBelow we defind the parameter space and the objective function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_space_hopt = {\n    \"max_depth\":scope.int(hp.quniform(\"max_depth\",3,10,1)),\n              \"n_estimators\":scope.int(hp.quniform(\"n_estimators\",50,1000,1)),\n               \"criterion\":hp.choice(\"criterion\",[\"gini\",\"entropy\"]),\n               \"max_features\":hp.uniform(\"max_features\",0.1,1),\n               \"min_samples_leaf\":scope.int(hp.quniform(\"min_samples_leaf\",2,10,1))\n              }\n\ndef objective_hopt(params_hopt):\n    model_hopt = RandomForestClassifier(**params_hopt)\n    skf = StratifiedKFold(n_splits=5,random_state=42)\n    scores = -np.mean(cross_val_score(model_hopt,X_train,y_train,cv=skf,scoring=\"accuracy\"))\n    return scores\n\ntrial_hopt = Trials()\nhyopt = fmin(fn=objective_hopt,space = param_space_hopt, algo=tpe.suggest,max_evals=25,trials=trial_hopt) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hyopt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the scores against the calls to the objective function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"main_plot_history(trial_hopt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_hopt =RandomForestClassifier(n_estimators= int(hyopt[\"n_estimators\"]),criterion=\"gini\",max_depth=int(hyopt[\"max_depth\"]),min_samples_leaf=int(hyopt[\"min_samples_leaf\"]),max_features=hyopt[\"max_features\"],random_state=42)\nmodel_hopt.fit(X_train,y_train)\ny_pred_hyopt = model_hopt.predict(X_test)\nhyopt_score = accuracy_score(y_test,y_pred_hyopt)\nhyopt_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optuna Hyperparamter Tuning\n\nWe define the objective function below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimization_optuna(trial_optuna):\n    \n    n_estimators = trial_optuna.suggest_int(\"n_estimators\",50,1000)\n    max_depth = trial_optuna.suggest_int(\"max_depth\",3,10)\n    criterion = trial_optuna.suggest_categorical(\"criterion\",[\"entropy\",\"gini\"])\n    min_samples_split = trial_optuna.suggest_int(\"min_samples_leaf\",2,10)\n    max_features = trial_optuna.suggest_uniform(\"max_features\",0.1,1)\n    \n\n    model_optuna = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,criterion=criterion,\n                                         min_samples_split=min_samples_split,max_features=max_features)\n    skf = StratifiedKFold(n_splits=5)\n    score = cross_val_score(model_optuna,X_train,y_train,cv=skf,scoring=\"accuracy\")\n    return np.mean(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In optuna we can give the direction in which we evaluate the objective function. Earlier we used -ve since those objective functions evaluated for minimizing.\n\nHere we can define the direction and we choose maximize since it we use accuracy score. We haven't negated the score in the objective function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nresult = study.optimize(optimization_optuna,n_trials=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the best parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We evaluate the best parameters on the test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_optuna =RandomForestClassifier(n_estimators= study.best_params[\"n_estimators\"],criterion=study.best_params[\"criterion\"],max_depth=study.best_params[\"max_depth\"],min_samples_leaf=study.best_params[\"min_samples_leaf\"],max_features=study.best_params[\"max_features\"],random_state=42)\nmodel_optuna.fit(X_train,y_train)\ny_pred_optuna = model_optuna.predict(X_test)\noptuna_score = accuracy_score(y_test,y_pred_optuna)\noptuna_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We visualize the movement of scores according to the calls to the objective functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}