{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake Job Postings\n\nSorry for my English please /\\\n\n## Data\n\nFeatures list (`Variable`: Definition):\n\n- `job_id`: Unique Job ID<br>\n- `title`: The title of the job ad entry<br>\n- `location`: Geographical location of the job ad<br>\n- `department`: Corporate department (e.g. sales)<br>\n- `salary_range`: Indicative salary range (e.g. $50,000-60,000)<br>\n- `company_profile`: A brief company description<br>\n- `description`: The details description of the job ad<br>\n- `requirements`: Enlisted requirements for the job opening<br>\n- `benefits`: Enlisted offered benefits by the employer<br>\n- `telecommuting`: True for telecommuting positions<br>\n- `has_company_logo`: True if company logo is present<br>\n- `has_questions`: True if screening questions are present<br>\n- `employment_type`: Full-type, Part-time, Contract, etc<br>\n- `required_experience`: Executive, Entry level, Intern, etc<br>\n- `required_education`: Doctorate, Master’s Degree, Bachelor, etc<br>\n- `industry`: Automotive, IT, Health care, Real estate, etc<br>\n- `function`: Consulting, Engineering, Research, Sales etc<br>\n- `fraudulent`: target - Classification attribute","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport string\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom itertools import tee\n\nfrom IPython.display import display\n\nimport numpy as np\nfrom scipy import stats\nfrom scipy.sparse import hstack as sparse_hstack\nimport pandas as pd\n\nimport matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns; sns.set()\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# statistic methods\ndef tconfint(sample, alpha=0.05):\n    '''Confidence interval based on Student t distribution.'''\n    mean = np.mean(sample)\n    S = np.std(sample, ddof=1)\n    n = len(sample)\n\n    t = stats.t.ppf(1 - alpha / 2, n - 1)\n    left_boundary = mean - t * S / np.sqrt(n)\n    right_boundary = mean + t * S / np.sqrt(n)\n\n    return left_boundary, right_boundary\n\n\ndef tconfint_diff(sample1, sample2, alpha=0.05):\n    '''Confidence interval based on Student t distribution for\n    the difference in means of two samples.'''\n    mean1 = np.mean(sample1)\n    mean2 = np.mean(sample2)\n    s1 = np.std(sample1, ddof=1)\n    s2 = np.std(sample2, ddof=1)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    sem1 = np.var(sample1) / (n1 - 1)\n    sem2 = np.var(sample2) / (n2 - 1)\n    semsum = sem1 + sem2\n    z1 = (sem1 / semsum) ** 2 / (n1 - 1)\n    z2 = (sem2 / semsum) ** 2 / (n2 - 1)\n    dof = 1 / (z1 + z2)\n\n    t = stats.t.ppf(1 - alpha / 2, dof)\n    left_boundary = (mean1 - mean2) - t * np.sqrt((s1 ** 2) / n1 + (s2 ** 2) / n2)\n    right_boundary = (mean1 - mean2) + t * np.sqrt((s1 ** 2) / n1 + (s2 ** 2) / n2)\n\n    return left_boundary, right_boundary\n\n\ndef bootstrap_statint(sample, stat=np.mean, n_samples=5000, alpha=0.05):\n    '''Statistical interval for a `stat` of a `sample` calculation\n    using bootstrap sampling mechanism. `stat` is a numpy function\n    like np.mean, np.std, np.median, np.max, np.min, etc.'''\n    indices = np.random.randint(0, len(sample), (n_samples, len(sample)))\n    samples = sample[indices]\n\n    stat_scores = stat(samples, axis=1)\n    boundaries = np.percentile(stat_scores, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n    return boundaries\n\n\ndef bootstrap_statint_diff(sample1, sample2, stat=np.mean, n_samples=5000, alpha=0.05):\n    '''Statistical interval for a difference in `stat` of two samples\n    calculation using bootstrap sampling mechanism. `stat` is a numpy\n    function like np.mean, np.std, np.median, np.max, np.min, etc.'''\n    indices1 = np.random.randint(0, len(sample1), (n_samples, len(sample1)))\n    indices2 = np.random.randint(0, len(sample2), (n_samples, len(sample2)))\n    samples1 = sample1[indices1]\n    samples2 = sample2[indices2]\n\n    stat_scores1 = stat(samples1, axis=1)\n    stat_scores2 = stat(samples2, axis=1)\n    stat_scores_diff = stat_scores1 - stat_scores2\n    boundaries = np.percentile(stat_scores_diff, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n    return boundaries\n\n\ndef proportion_confint(sample, alpha=0.05):\n    '''Wilson\\'s сonfidence interval for a proportion.'''\n    p = np.mean(sample)\n    n = len(sample)\n\n    z = stats.norm.ppf(1 - alpha / 2)\n    left_boundary = 1 / (1 + z ** 2 / n) * (p + z ** 2 / (2 * n) \\\n                                            - z * np.sqrt(p * (1 - p) / n + z ** 2 / (4 * n ** 2)))\n    right_boundary = 1 / (1 + z ** 2 / n) * (p + z ** 2 / (2 * n) \\\n                                             + z * np.sqrt(p * (1 - p) / n + z ** 2 / (4 * n ** 2)))\n\n    return left_boundary, right_boundary\n\n\ndef proportions_diff_confint_ind(sample1, sample2, alpha=0.05):\n    '''Confidence interval for the difference of two independent proportions.'''\n    z = stats.norm.ppf(1 - alpha / 2)\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2)\n    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2)\n\n    return left_boundary, right_boundary\n\n\ndef permutation_test_ind(sample1, sample2, max_permutations=None, alternative='two-sided'):\n    '''Permutation test for two independent samples.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    t_stat = np.mean(sample1) - np.mean(sample2)\n\n    joined_sample = np.hstack((sample1, sample2))\n    n1 = len(sample1)\n    n = len(joined_sample)\n\n    if max_permutations:\n        index = list(range(n))\n        indices = set([tuple(index)])\n        for _ in range(max_permutations - 1):\n            np.random.shuffle(index)\n            indices.add(tuple(index))\n\n        indices = [(index[:n1], index[n1:]) for index in indices]\n    else:\n        indices = [(list(index), list(filter(lambda i: i not in index, range(n)))) \\\n                    for index in itertools.combinations(range(n), n1)]\n\n    zero_distr = [joined_sample[list(i[0])].mean() - joined_sample[list(i[1])].mean() \\\n                  for i in indices]\n\n    if alternative == 'two-sided':\n        p_value = sum([abs(x) >= abs(t_stat) for x in zero_distr]) / len(zero_distr)\n\n    if alternative == 'less':\n        p_value = sum([x <= t_stat for x in zero_distr]) / len(zero_distr)\n\n    if alternative == 'greater':\n        p_value = sum([x >= t_stat for x in zero_distr]) / len(zero_distr)\n\n    return t_stat, p_value\n\n\ndef proportions_ztest_ind(sample1, sample2, alternative='two-sided'):\n    '''Z-test for two independent proportions.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    P = (p1 * n1 + p2 * n2) / (n1 + n2)\n    z_stat = (p1 - p2) / np.sqrt(P * (1 - P) * (1 / n1 + 1 / n2))\n\n    if alternative == 'two-sided':\n        p_value = 2 * (1 - stats.norm.cdf(np.abs(z_stat)))\n\n    if alternative == 'less':\n        p_value = stats.norm.cdf(z_stat)\n\n    if alternative == 'greater':\n        p_value = 1 - stats.norm.cdf(z_stat)\n\n    return z_stat, p_value\n\n\ndef cramers_v(contingency_table):\n    '''Cramer\\'s V coefficient.'''\n    n = np.sum(contingency_table)\n    ct_nrows, ct_ncols = contingency_table.shape\n    if n < 40 or np.sum(contingency_table < 5) / (ct_nrows * ct_ncols) > 0.2:\n        raise ValueError('Contingency table isn\\'t suitable for Cramers\\'s V coefficient calculation.')\n\n    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n    corr = np.sqrt(chi2 / (n * (min(ct_nrows, ct_ncols) - 1)))\n    return corr, p_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First look\n\nThe dataset:","execution_count":null},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_csv('../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many NA-values in the table and there are features that have to be preprocessed for further usage. Let's save names of each feature-type and work on complex ones.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bin_features = ['telecommuting', 'has_company_logo', 'has_questions']\ncat_features = ['department', 'employment_type', 'required_experience', \n                'required_education', 'industry', 'function']\n\ntext_features = ['title', 'company_profile', 'description', 'requirements', 'benefits']\ncomplex_features = ['location', 'salary_range']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And drop `job_id`, it's useless.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.drop('job_id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature preparation\n\n### Text features\n\nFeatures that describe textual components of a job post:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data[text_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding indicators of specified values (there is no NA-values in `title`):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for feature_name in text_features[1:]:\n    unspec_feature_name = f'{feature_name}_specified'\n    data[unspec_feature_name] = (~data[feature_name].isna()).astype('int')\n    bin_features += [unspec_feature_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()[text_features + bin_features[-4:]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling NA-values with an empty string:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for feature_name in text_features[1:]:\n    data[feature_name].fillna('', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to clean our texts from punctuation marks and stop-words, and apply stemming:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# nltk.download('stopwords')\n# nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nltk_supported_languages = ['hungarian', 'swedish', 'kazakh', 'norwegian',\n                            'finnish', 'arabic', 'indonesian', 'portuguese',\n                            'turkish', 'azerbaijani', 'slovene', 'spanish',\n                            'danish', 'nepali', 'romanian', 'greek', 'dutch',\n                            'tajik', 'german', 'english', 'russian',\n                            'french', 'italian']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# stop words list\nstop_words = set(stopwords.words(nltk_supported_languages))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# stemmer\nporter = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocess_texts(texts):\n    '''Returns a list of clean and word-stemmed strings.'''\n    preprocessed_texts = []\n    for text in tqdm(texts):\n        # punctuation marks cleaning\n        text = ''.join([sym.lower() for sym in text if sym.isalpha() or sym == ' '])\n        \n        # tokenization\n        tokenized_text = word_tokenize(text)\n        \n        # stop words cleaning\n        tokenized_text_wout_sw = [word for word in tokenized_text if word not in stop_words]\n        \n        # stemming\n        tokenized_text_wout_sw_stem = [porter.stem(word) for word in tokenized_text_wout_sw]\n        \n        # saving result\n        preprocessed_texts += [' '.join(tokenized_text_wout_sw_stem)]\n    \n    return preprocessed_texts","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nfor feature_name in text_features:\n    data[feature_name] = preprocess_texts(data[feature_name])\n\ndata[text_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! Now move on to complex features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Complex features\n\n#### `location`\n\nThe main structure of `location`'s values is `Country, State, City`:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"location = data['location'].copy()\nlocation.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's divide and extract these elements. We will use them as a categorical features in the future.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"location_splitted = list(location.str.split(', ').values)\nlocation_splitted[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling in missing values (we will use `'Unspecified'` word to replace NA-values for all categorical features):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for loc_ind, loc in enumerate(location_splitted):\n    if loc is np.nan:\n        location_splitted[loc_ind] = ['Unpecified'] * 3\n    else:\n        for el_ind, el in enumerate(loc):\n            if el == '':\n                loc[el_ind] = 'Unpecified'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"location_splitted[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But there are some troubles:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"any([len(loc) > 3 for loc in location_splitted])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"any([len(loc) < 3 for loc in location_splitted])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not all values of `location` were described in 3 elements. Let's look at unusual values:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) > 3:\n        print(loc_ind, loc)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) < 3:\n        print(loc_ind, loc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To resolve these problems a strange move have to be undertaken due to this oddity:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"location_splitted[0] is list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"type(location_splitted[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"location_splitted = list(map(lambda loc: list(loc), location_splitted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the problems arose due to the refinement of the position at the third element using a comma. Let's resolve it simply (and supplement values in which only the country is specified):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for loc_ind, loc in enumerate(location_splitted):\n    if len(loc) > 3:\n        location_splitted[loc_ind] = loc[:2] + [', '.join(loc[2:])]\n    if len(loc) < 3:\n        location_splitted[loc_ind] += ['Unpecified'] * 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"any([len(loc) != 3 for loc in location_splitted])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's add new features to the dataset table and remove the old one from it:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_location = pd.DataFrame(location_splitted, columns=['country', 'state', 'city'])\ndata_location.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# complementing the list of categorical features\ncat_features += ['country', 'state', 'city']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.concat([data, data_location], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.drop('location', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### `salary_range`\n\nNow we need to do something with the `salary_range` column because we can't work with it as with a categorical feature:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"salary_range = data.salary_range.copy()\nsalary_range.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling in the missing values with a `0-0` value (in the future we will create an indicator for the unspecified data):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"salary_range.fillna('0-0', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And splitting them:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"salary_range_sep = list(salary_range.str.split('-').values)\nsalary_range_sep[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for unusual values:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"for range_ind, s_range in enumerate(salary_range_sep):\n    if len(s_range) < 2 or len(s_range) > 2:\n        print(range_ind, s_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And fixing it:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"salary_range_sep[5538] = ['40000', '40000']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not all gained values are numerical:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"error_range_inds = []\nfor range_ind, s_range in enumerate(salary_range_sep):\n    min_value, max_value = s_range\n    if not min_value.isdigit() or not max_value.isdigit():\n        print(range_ind, (min_value, max_value))\n        error_range_inds += [range_ind]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Somebody specified some kind of dates instead of salary range, let's replace these values with a `['0', '0']`:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for range_ind in error_range_inds:\n    salary_range_sep[range_ind] = ['0', '0']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving results into a `pandas.DataFrame` object:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_salary_range = pd.DataFrame(np.array(salary_range_sep, dtype='int64'), \n                                 columns=['min_salary', 'max_salary'])\ndata_salary_range.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a column for marking specified salary ranges:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_salary_range['salary_specified'] = ((data_salary_range.min_salary != 0) | \n                                         (data_salary_range.max_salary != 0)).astype('int64')\ndata_salary_range.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# creating the list of numerical features names and complementing the list of binary ones\nnum_features = ['min_salary', 'max_salary']\nbin_features += ['salary_specified']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And saving results into the original table:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.concat([data, data_salary_range], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.drop('salary_range', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other features\n\nWe still have NA-values in other columns:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But the rest features are categorical so we will fill the missing values using `'Unspecified'` value:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.fillna('Unspecified', inplace=True)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of the target feature:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nax = sns.countplot(data.fraudulent)\nplt.title('The distribution of the target feature (fraudulent)')\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x()+0.33, p.get_height()))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes are not balanced, so we have to use oversampling/undersampling and calculate \\[not-only-accuracy\\] metrics (ROC AUC, etc.) to estimate our models in the future.\n\nDistributions of `fraudulent` for the binary features:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25, 30))\nouter = gridspec.GridSpec(4, 2, wspace=0.2, hspace=0.1)\n\nfor feature_ind, feature_name in enumerate(bin_features):\n    inner = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=outer[feature_ind], \n                                             wspace=0.5, hspace=0.7)\n    \n    ax = plt.Subplot(fig, outer[feature_ind])\n    ax.set_title(f'The distribution of fraudulent for each {feature_name}\\'s class')\n    ax.axis('off')\n    fig.add_subplot(ax)\n    \n    for feature_class in [0, 1]:\n        ax = plt.Subplot(fig, inner[feature_class])\n        feature_cl_vc = data[data[feature_name] == feature_class].fraudulent.value_counts().sort_index()\n        if len(feature_cl_vc) == 2:\n            feature_cl_vc.index = ['non-fraudulent', 'fraudulent']\n        else:\n            feature_cl_vc.index = ['fraudulent']\n        \n        ax.pie(feature_cl_vc.values, labels=feature_cl_vc.index, autopct='%1.1f%%')\n        ax.set_title(f'{feature_name} = {feature_class}')\n        fig.add_subplot(ax)\n\nfig.suptitle('Distributions of fraudulent for the binary features')\nfig.subplots_adjust(top=0.95)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look's like those who post fraudulent posts more often don't have company logo/company profile and more often indicate in their posts that there will be no screening questions at the survey. Also fraudulent post writers less often specify salary and more often offer remote employment.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The distributions of fraudulent for `description_specified` feature looks strange because there is only one record in the table that have an unspecified description and it's fraudulent:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cont_table = pd.crosstab(data.fraudulent, data.description_specified)\nprint('Contingency table (fraudulent x description_specified):')\ndisplay(cont_table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check how some of the binary features may be related:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def show_feature1_x_feature2_info(feature_name1, feature_name2, figsize=(12, 4), is_binxcat=False):\n    '''Shows info about a combination of two binary/categorical features.'''\n    cont_table = pd.crosstab(data[feature_name1], data[feature_name2]).fillna(0)\n    prop_table = pd.pivot_table(data, index=feature_name1, columns=feature_name2, \n                                values='fraudulent', aggfunc=np.mean).fillna(0)\n    \n    corr, p = cramers_v(cont_table.values)\n    \n    if is_binxcat:\n        fig, axes = plt.subplots(2, 1, figsize=figsize, sharex=True)\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n    \n    sns.heatmap(cont_table, annot=True, fmt='d', ax=axes[0])\n    axes[0].set_title(f'Contingency table:')\n    if is_binxcat:\n        axes[0].set_xlabel('')\n    \n    sns.heatmap(prop_table, annot=True, ax=axes[1])\n    axes[1].set_title(f'Proportion of fraudulent posts:')\n    \n    fig_title = f'{feature_name1} x {feature_name2} (Correlation: {round(corr, 4)}, p-value: {round(p, 4)}))'\n    if is_binxcat:\n        fig.suptitle(fig_title, y=1.05, x=0.45)\n    else:\n        fig.suptitle(fig_title, y=1.05)\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('has_company_logo', 'company_profile_specified')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest one of these probabilities of being fraudulent have posts without company's profile and logo, the smallest - posts that have them.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('benefits_specified', 'has_questions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest one of these probabilities of being fraudulent have posts with specified benefits and without announcement of any questions during interview.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('telecommuting', 'has_questions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest one of these probabilities of being fraudulent have posts that offer remote work and promise an interview without questions.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('telecommuting', 'benefits_specified')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest one of these probabilities of being fraudulent have posts that offer remote work and don't specify any benefits.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('benefits_specified', 'salary_specified')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest one of these probabilities of being fraudulent have posts that include specified benefits and specified salaries.\n\nLet's compare proportions of fraudulent posts for `has_questions` and `salary_specified` classes (0 and 1):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"round_confint = lambda confint: list(map(lambda lim: round(lim, 4), confint))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_stats_for_proportions(feature_name):\n    fraudulent_0 = data[data[feature_name] == 0].fraudulent\n    fraudulent_1 = data[data[feature_name] == 1].fraudulent\n    \n    prop_0 = round(np.mean(fraudulent_0), 4)\n    prop_1 = round(np.mean(fraudulent_1), 4)\n    prop_0_confint = round_confint(proportion_confint(fraudulent_0))\n    prop_1_confint = round_confint(proportion_confint(fraudulent_1))\n    \n    bigger_prop, smaller_prop = (fraudulent_0, fraudulent_1) if prop_0 > prop_1 else (fraudulent_1, fraudulent_0)\n    props_diff = round(np.mean(bigger_prop) - np.mean(smaller_prop), 4)\n    props_diff_confint = round_confint(proportions_diff_confint_ind(bigger_prop, smaller_prop))\n    z_test_p = proportions_ztest_ind(fraudulent_0, fraudulent_1)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Proportion of fraudulent posts for 0: {prop_0}')\n    print(f'Proportion of fraudulent posts for 1: {prop_1}')\n    print(f'Confidence interval for the proportion of fraudulent posts for 0: {prop_0_confint}')\n    print(f'Confidence interval for the proportion of fraudulent posts for 1: {prop_1_confint}')\n    print(f'Difference in these proportions: {props_diff}')\n    print(f'Confidence interval for the difference in these proportions: {props_diff_confint}')\n    print(f'Z-test result: {z_test_p} (p-value)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print_stats_for_proportions('has_questions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round((0.0331 / 0.0284) * 100, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chance to meet a fraudulent post among posts that don't announce any questions is at least 116.5% higher than the chance to meet it among posts that do it.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print_stats_for_proportions('salary_specified')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round((0.0273 / 0.0427) * 100, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When you browse posts with specified salaries it's at least 63.9% higher chance to meet a fraudulent one than when you check posts without specified salaries.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at the categorical features:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for feature_name in cat_features:\n    print(f'Count of {feature_name}\\'s unique values: {data[feature_name].unique().shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will be difficult to make any kind of plot for every categorical feature due to the number of classes in most of them. Let's plot ones that have fewer amount of classes (`plotly` charts fit better for this):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_cat_feature_distribution(feature_name):\n    '''Makes a plotly chart with categorical feature\\'s distribution.'''\n    feature_0f = data[data.fraudulent == 0][feature_name].value_counts()\n    feature_1f = data[data.fraudulent == 1][feature_name].value_counts()\n    \n    fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]], \n                        subplot_titles=['non-fraudulent', 'fraudulent'])\n    fig.add_trace(go.Pie(labels=feature_0f.index, \n                         values=feature_0f.values), \n                  row=1, col=1)\n    fig.add_trace(go.Pie(labels=feature_1f.index, \n                         values=feature_1f.values), \n                  row=1, col=2)\n    \n    fig.update_layout(title_text=f'The distribution of {feature_name}')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_cat_feature_distribution('employment_type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_cat_feature_distribution('required_experience')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_cat_feature_distribution('required_education')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"func_meanfr_pt = pd.pivot_table(data, index='function', values='fraudulent', \n                                aggfunc=np.mean).sort_values(by='fraudulent', ascending=False)\nfunc_meanfr_pt.columns = ['Proportion of fraudulent posts']\nprint('Top-15 function\\'s values with the biggest proportions of fraudulent posts:')\ndisplay(func_meanfr_pt.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"country_meanfr_pt = pd.pivot_table(data, index='country', values='fraudulent', \n                                   aggfunc=np.mean).sort_values(by='fraudulent', ascending=False)\ncountry_meanfr_pt.columns = ['Proportion of fraudulent posts']\nprint('Top-15 country\\'s values with the biggest proportions of fraudulent posts:')\ndisplay(country_meanfr_pt.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check how some of categorical and binary features may be related:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('employment_type', 'required_experience', (18, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('benefits_specified', 'required_education', (14, 4.5), True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_feature1_x_feature2_info('has_questions', 'required_education', (14, 4.5), True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the numerical features (salary info):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n\nfor ind, feature_name in enumerate(num_features):\n    sns.boxplot(y=feature_name, x='fraudulent', data=data[data.salary_specified == 1], ax=axes[ind])\n    axes[ind].set_ylim([-1e4, 2e5])\n    axes[ind].set_xticklabels(['non-fraudulent', 'fraudulent'])\n    axes[ind].set_title(f'Distributions of specified {feature_name}')\n\nfig.suptitle('Distributions of min_salary and max_salary')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those who write fake job posts often offer slightly lower salaries... Let's look at differencies between min and max salaries:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"diff_salary = data[data.salary_specified == 1]['max_salary'] - data[data.salary_specified == 1]['min_salary']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.boxplot(y=diff_salary, x='fraudulent', data=data[data.salary_specified == 1])\nplt.ylim([-1e4, 1e5])\nplt.xticks([0, 1], ['non-fraudulent', 'fraudulent'])\nplt.ylabel('Difference')\nplt.title('Distribution of difference between\\n min and max salary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a difference in medians, let's calculate them and some descriptive statistics (there is no sense in compairing means because there are too many outliers here):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"specified_salaries = data[data.salary_specified == 1][num_features]\nspecified_salaries['difference'] = diff_salary\nspecified_salaries['fraudulent'] = data.fraudulent\nspecified_salaries.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't use Mann–Whitney U test for distributions comparison because -","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"np.sum(np.unique(specified_salaries.min_salary, return_counts=True)[1] > 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"np.sum(np.unique(specified_salaries.max_salary, return_counts=True)[1] > 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\\- so we will use Permutation test:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_stats_for_salary(feature_name):\n    '''Calculates statistics for fraudulent and non-fraudulent salary-feature.'''\n    np.random.seed(42)\n    feature_0f = specified_salaries[specified_salaries.fraudulent == 0][feature_name]\n    feature_1f = specified_salaries[specified_salaries.fraudulent == 1][feature_name]\n    \n    med_0f = np.median(feature_0f)\n    med_1f = np.median(feature_1f)\n    med_0f_confint = bootstrap_statint(feature_0f.values, stat=np.median)\n    med_1f_confint = bootstrap_statint(feature_1f.values, stat=np.median)\n    \n    bigger_med, smaller_med = (feature_0f, feature_1f) if med_0f > med_1f else (feature_1f, feature_0f)\n    med_diff = np.median(bigger_med) - np.median(smaller_med)\n    med_diff_confint = bootstrap_statint_diff(bigger_med.values, smaller_med.values, stat=np.median)\n    perm_test_p = permutation_test_ind(feature_0f, feature_1f, max_permutations=5000)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Median of {feature_name} in non-fraudulent posts: {med_0f}')\n    print(f'Median of {feature_name} in fraudulent posts:     {med_1f}')\n    print(f'Statistical interval for the median of {feature_name} in non-fraudulent posts: {med_0f_confint}')\n    print(f'Statistical interval for the median of {feature_name} in fraudulent posts:     {med_1f_confint}')\n    print(f'Difference in these medians: {med_diff}')\n    print(f'Statistical interval for the difference in these medians: {med_diff_confint}')\n    print(f'Permutation test result: {perm_test_p} (p-value)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print_stats_for_salary('min_salary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print_stats_for_salary('max_salary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print_stats_for_salary('difference')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Differencies between `min_salary`'s medians of 0 and 1 `fraudulent` groups are much more significant than between `max_salary`'s and `different`'s ones. But there isn't difference in distributions for any of them. \n\nLet's compare mean count of words and distributions of lenghts of each textual feature for groups of 0 and 1 `fraudulent`:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n\ntext_features_gen = iter(text_features)\n\nfor row in range(3):\n    for col in range(2):\n        try:\n            feature_name = next(text_features_gen)\n        except StopIteration:\n            break\n        \n        if feature_name == 'title':\n            feature_values_0f = data[(data.fraudulent == 0)][feature_name].astype(str)\n            feature_values_1f = data[(data.fraudulent == 1)][feature_name].astype(str)\n        else:\n            feature_values_0f = data[(data.fraudulent == 0) & data[f'{feature_name}_specified']][feature_name].astype(str)\n            feature_values_1f = data[(data.fraudulent == 1) & data[f'{feature_name}_specified']][feature_name].astype(str)\n\n        fv_0f_len = feature_values_0f.str.split(' ').apply(len)\n        fv_1f_len = feature_values_1f.str.split(' ').apply(len)\n        \n        sns.distplot(fv_0f_len, label='non-fraudulent', ax=axes[row, col])\n        sns.distplot(fv_1f_len, label='fraudulent', ax=axes[row, col])\n        axes[row, col].set_title(f'The distribution of {feature_name}\\'s count of words')\n        axes[row, col].legend()\n        \nfig.suptitle('Distributions of count of words for each text feature', y=0.92)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_stats_for_texts(feature_name):\n    '''Calculates statistics for fraudulent and non-fraudulent count of words in feature\\'s texts.'''\n    if feature_name == 'title':\n        feature_values_0f = data[(data.fraudulent == 0)][feature_name].astype(str)\n        feature_values_1f = data[(data.fraudulent == 1)][feature_name].astype(str)\n    else:\n        feature_values_0f = data[(data.fraudulent == 0) & data[f'{feature_name}_specified']][feature_name].astype(str)\n        feature_values_1f = data[(data.fraudulent == 1) & data[f'{feature_name}_specified']][feature_name].astype(str)\n    \n    lens_0f = feature_values_0f.str.split(' ').apply(len)\n    lens_1f = feature_values_1f.str.split(' ').apply(len)\n    \n    mean_lens_0f = round(np.mean(lens_0f), 4)\n    mean_lens_1f = round(np.mean(lens_1f), 4)\n    mean_lens_0f_confint = round_confint(tconfint(lens_0f.values))\n    mean_lens_1f_confint = round_confint(tconfint(lens_1f.values))\n    \n    bigger_mean, smaller_mean = (lens_0f, lens_1f) if mean_lens_0f > mean_lens_1f else (lens_1f, lens_0f)\n    mean_diff = round(np.mean(bigger_mean) - np.mean(smaller_mean), 4)\n    \n    mean_diff_confint = round_confint(tconfint_diff(bigger_mean.values, smaller_mean.values))\n    perm_test_p = permutation_test_ind(lens_0f, lens_1f, max_permutations=5000)[1]\n    \n    print(f'Feature: {feature_name}\\n======')\n    print(f'Mean of {feature_name}\\'s count of words in non-fraudulent posts: {mean_lens_0f}')\n    print(f'Mean of {feature_name}\\'s count of words in fraudulent posts:     {mean_lens_1f}')\n    print(f'Confidence interval for the mean of {feature_name}\\'s count of words in non-fraudulent posts: {mean_lens_0f_confint}')\n    print(f'Confidence interval for the mean of {feature_name}\\'s count of words in fraudulent posts:     {mean_lens_1f_confint}')\n    print(f'Difference in these means: {mean_diff}')\n    print(f'Confidence interval for the difference in these means: {mean_diff_confint}')\n    print(f'Permutation test result: {perm_test_p} (p-value)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for feature_name in text_features:\n    print_stats_for_texts(feature_name)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create two new numerical features for `company_profile`'s and `requirements`'s count of words (because their distributions are probably different, as well as `title`'s, but the maximum difference in means of 0.4 of a word not very noticeable in terms of logic):","execution_count":null},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['company_profile_count_of_words'] = data['company_profile'].astype(str).str.split(' ').apply(len)\ndata['requirements_count_of_words'] = data['requirements'].astype(str).str.split(' ').apply(len)\ndata.head()[['company_profile_count_of_words', 'requirements_count_of_words']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"num_features += ['company_profile_count_of_words', 'requirements_count_of_words']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to fit models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Transforming features and fitting models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Firstly let's increase the count of 1 `fraudulent` records in the dataset using oversampling:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_1f = data[data.fraudulent == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"original_data = data.copy()\ndata = pd.concat([data] + [data_1f] * 7, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nax = sns.countplot(data.fraudulent)\nplt.title('The distribution of the target feature (fraudulent)')\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x()+0.33, p.get_height()))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross-validation splitter:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividing features and targets:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X, y = data.drop('fraudulent', axis=1), data.fraudulent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numerical features have to be scaled, categorical features have to be transformed into sets of binary ones and text features have to be vectorized (I'm using TF-IDF method):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"num_transformer = Pipeline(steps=[('scaler', StandardScaler())])\ncat_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\ntext_transformer = Pipeline(steps=[('tfidf', TfidfVectorizer(ngram_range=(1, 2)))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features),\n        *[(feature_name, text_transformer, feature_name) \n          for feature_name in text_features]\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll start with a logistic regression model (default parameters):","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"log_reg_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('classifier', LogisticRegression())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncv_scores = cross_validate(log_reg_pipe, X, y, return_train_score=True, cv=skf, \n                           scoring=['accuracy', 'roc_auc'], n_jobs=-1)\n\nprint(f'Accuracy on train part: {cv_scores[\"train_accuracy\"]}, mean: {cv_scores[\"train_accuracy\"].mean()}')\nprint(f'Accuracy on test part:  {cv_scores[\"test_accuracy\"]}, mean: {cv_scores[\"test_accuracy\"].mean()}')\nprint(f'ROC AUC on train part: {cv_scores[\"train_roc_auc\"]}, mean: {cv_scores[\"train_roc_auc\"].mean()}')\nprint(f'ROC AUC on test part:  {cv_scores[\"test_roc_auc\"]}, mean: {cv_scores[\"test_roc_auc\"].mean()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think we can stop at this point. Maybe the dataset wasn't assembled correctly or there are too few records in it... Or maybe I've done something wrong, but the data is too easy to classify. Or maybe everything is alright? Let's check model's weights:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nfeature_names = num_features.copy()\n\nnum_features_scaled = StandardScaler().fit_transform(data[num_features])\nX = num_features_scaled\n\nfeature_names += bin_features\nX = np.hstack([X, data[bin_features]])\n\n\nfor feature_name in cat_features:\n    encoder = OneHotEncoder()\n    encoded_feature = encoder.fit_transform(data[feature_name].values.reshape(-1, 1))\n    \n    X = sparse_hstack([X, encoded_feature])\n    f_names = list(map(lambda cat: f'{feature_name}:{cat}', encoder.categories_[0]))\n    feature_names += f_names\n\nfor feature_name in text_features:\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n    vectorized_feature = vectorizer.fit_transform(data[feature_name])\n    \n    X = sparse_hstack([X, vectorized_feature])\n    sorted_phrases = [pair[0] for pair in list(sorted(vectorizer.vocabulary_.items(), \n                                                      key=lambda pair: pair[1]))]\n    f_names = list(map(lambda phrase: f'{feature_name}:{phrase}', sorted_phrases))\n    feature_names += f_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X.shape[1], len(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"log_reg = LogisticRegression(random_state=42, n_jobs=-1).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weights x feature names:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"eli5.explain_weights(log_reg, feature_names=feature_names, top=(30, 30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think everything is pretty good though... \n\nThe `country` value with the biggest positive weight is `MY`, which is top-1 country in \"Proportion of fraudulent posts\" table (for the `country` feature, value of proportion - 0.571429). The country with the smallest count of fraudulent posts is `GR`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.country == 'MY'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.country == 'GR'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using info about weights we can figure out which values of a feature are associated with the biggest and the smallest proportions of fraudulent posts:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.industry == 'Accounting'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.industry == 'Internet'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we can also conclude that fraudulent post writers often start writing `London` (and many other city names) using the lowercase letter:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.city == 'london'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.city == 'London'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.city == 'chicago'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data[original_data.city == 'Chicago'].fraudulent.value_counts().sort_index()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}