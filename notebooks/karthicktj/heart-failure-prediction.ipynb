{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Prediction \nIn this Notebook we discuss various Machine learning model to Classify Heart Failure Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Discuss Algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Logistic Regression\n1. Random Forest Classfier \n1. Gradient Boosting \n1. KNeighbours \n1. SVC \n1. Decision Tree Classifier\n1. Random Forest Classifier Hyper parameter tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data \ndf = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning and visualization","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape:{} and size:{} of the data\".format(df.shape , df.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check the Missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"If any Missing Value is present in the data\")\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Describe and visualize independent Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.DEATH_EVENT.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# visuvalize the Target Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(10,5))\nsns.countplot(data = df , x= \"DEATH_EVENT\" ,palette = \"Set3\" ,ax=ax[0])\nplt.title(\"Death_event\")\ndf.DEATH_EVENT.value_counts().plot.pie(explode =[0.1,0] ,autopct = \"%0.2f%%\" ,shadow = True ,ax = ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# univarient Analysis and Detect outlier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def univarient(data , feature):\n    plt.figure(figsize = (10,10))\n    sns.distplot(data[feature])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    univarient(df,var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.boxplot(data[feature])\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    outlier(df,var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def univarient_cat(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.countplot(x = feature ,hue =\"DEATH_EVENT\" , data = data , palette = \"rainbow\" )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_b = [\"anaemia\"  ,\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\nfor var in feature_b:\n    univarient_cat(df ,var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivarient Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bivarient(data ,feature):\n    plt.figure(figsize = (10,10))\n    sns.boxplot(y = feature ,x =\"DEATH_EVENT\" , data = data , palette = \"rainbow\" )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = [\"age\" , \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in feature:\n    bivarient(df,var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bivarient_conti(data , feature):\n    plt.figure(figsize =(10,10))\n    sns.lineplot(x = \"age\" , y = feature , hue = \"DEATH_EVENT\" , data = data)\n    plt.title(\"Relationship between Age and Hue =DEATH_EVENT\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_c = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\"]\nfor var in feature_c:\n    bivarient_conti(df,var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.title(\"Relation between death based on Sex and Diabetes \")\nsns.catplot(kind = \"count\",x = \"sex\" ,hue = \"diabetes\" ,col = \"DEATH_EVENT\" ,data = df,palette = \"rainbow\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.title(\"Relation between death based on Sex and High_blood_pressure \")\nx = sns.catplot(kind = \"count\",x = \"sex\" ,hue = \"high_blood_pressure\" ,col = \"DEATH_EVENT\" ,data = df,palette = \"rainbow\")\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation about the feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nsns.heatmap(df.corr() , annot = True ,cmap = \"Blues\" )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier Handling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier calculation for Extreme and Nominal\ndef IQR_CAL(data , feature):\n    IQR = data[feature].quantile(0.75) - data[feature].quantile(0.25)\n    E_upper = data[feature].quantile(0.75) + (3 * IQR)\n    E_lower = data[feature].quantile (0.25)- (3 * IQR)\n    N_upper = data[feature].quantile(0.75) + (1.5 * IQR) # apply Nominal outlier\n    N_lower = data[feature].quantile(0.25) + (1.5 * IQR)\n    print(\"Inter Quantile Range  {}:{}\".format(feature,IQR))\n    print(\"Extreme outlier for Upper Boundary  {}:{}\".format(feature,E_upper))\n    print(\"Extreme outlier for Lower Boundary  {}:{}\".format(feature,E_lower))\n    print(\"Nominal outlier for Upper Boundary  {}:{}\".format(feature,N_upper))\n    print(\"Nominal outlier for Lower Boundary  {}:{}\".format(feature,N_lower))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_f = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in outlier_f:\n    IQR_CAL(df,var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above code we will calculate the outlier using IQR with Nominal and Extreme Outlier\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Extreme Outlier\nExtreme outliers are any data values which lie more than 3.0 times the interquartile range below the first quartile or above the third quartile","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Nominal Outlier\nNominal outliers are any data values which lie more than 1.5 times the interquartile range below the first quartile or above the third quartile","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier removal\ndf.loc[df[\"serum_sodium\"] < 125 ,  \"serum_sodium\" ] = 125.0\ndf.loc[df[\"serum_creatinine\"] > 2.14 ,\"serum_creatinine\" ] = 2.14\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df[\"platelets\"] > 440000 , \"platelets\"] =440000  #nominal outlier\ndf.loc[df[\"ejection_fraction\"] > 67.5 , \"ejection_fraction\"] =  67.5   #nominal outlier\ndf.loc[df[\"creatinine_phosphokinase\"] > 1280.25 , \"creatinine_phosphokinase\"] = 1280.25 #nominal outlier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# After removing outlier ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_removal_f(data,var):\n    plt.figure(figsize = (10,10))\n    sns.distplot(data[var],color=\"y\")\n    plt.title(var)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_f = [ \"creatinine_phosphokinase\" ,\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\nfor var in outlier_f:\n    outlier_removal_f(df,var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X-y Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report, precision_score,accuracy_score\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train_Test_spliting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\nprint(\"shape of x_train:{} and x_test:{}\".format(x_train.shape,x_test.shape))\nprint(\"shape of y_train:{} and y_test:{}\".format(y_train.shape,y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Apply Standardisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_std = StandardScaler().fit_transform(x_train)\nx_test_std = StandardScaler().fit_transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(penalty = \"l2\" , fit_intercept=True,verbose = 2 ,n_jobs = -1)\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for Logistic Regression\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for Logistic Regression Before appling Standardisation:{}\".format(accuracy_score(y_test,pred) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply Standardised Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(penalty = \"l2\" , fit_intercept=True)\nlr.fit(x_train_std,y_train)\npred = lr.predict(x_test_std)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for Logistic Regression\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for Logistic Regression after appling Standardisation:{}\".format(accuracy_score(y_test,pred) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\npred = rf.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for RandomForestClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"Blues\")\nprint(\"accuracy Score for RandomClassifier:{}\".format(accuracy_score(y_test,pred) * 100 ,\"%\"))\nprint(\"Classification_report for Random_forest\")\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier(n_estimators = 50 ,max_depth = 10 ,criterion = \"mse\",random_state = 3)\ngb.fit(x_train,y_train)\npred = rf.predict(x_test)\ncon = confusion_matrix(y_test,pred)\nprint(con)\nplt.title(\"confusion_matrix for GradientBoostinClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"bone\")\nprint(\"accuracy Score for GradientBoosting:{}\".format(accuracy_score(y_test,pred) * 100 ,\"%\"))\nprint(\"Classification_report for GradientBoosting\")\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNearest Neighbors Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Knn = KNeighborsClassifier(n_neighbors=100, weights='uniform')\nKnn.fit(x_train,y_train)\nknnpred = Knn.predict(x_test)\ncon = confusion_matrix(y_test,knnpred)\nprint(con)\nplt.title(\"confusion_matrix for KneighborsClassifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for KneighborsClassifier:{}\".format(accuracy_score(y_test,knnpred) * 100 ))\nprint(\"Classification_report for KneighborsClassifier\")\nprint(classification_report(y_test,knnpred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvcpred = svc.predict(x_test)\ncon = confusion_matrix(y_test,svcpred)\nprint(con)\nplt.title(\"confusion_matrix for Support Vector Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"Pastel2\")\nprint(\"accuracy Score for Support Vector Classifier:{}\".format(accuracy_score(y_test,svcpred) * 100 ))\nprint(\"Classification_report for Support Vector Classifier\")\nprint(classification_report(y_test,svcpred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classifier Standardized","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train_std,y_train)\nsvcpred = svc.predict(x_test_std)\ncon = confusion_matrix(y_test,svcpred)\nprint(con)\nplt.title(\"confusion_matrix for Support Vector Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for Support Vector Classifier Standardized:{}\".format(accuracy_score(y_test,svcpred) * 100 ))\nprint(\"Classification_report for Support Vector Classifier\")\nprint(classification_report(y_test,svcpred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(max_depth = 10 ,criterion = \"entropy\",splitter = \"best\")\ndt.fit(x_train,y_train)\ndtpred = dt.predict(x_test)\ncon = confusion_matrix(y_test,dtpred)\nprint(con)\nplt.title(\"confusion_matrix for Decision Tree Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"YlGnBu\")\nprint(\"accuracy Score for Decision Tree Classifier:{}\".format(accuracy_score(y_test,dtpred) * 100 ))\nprint(\"Classification_report for Decision Tree Classifier\")\nprint(classification_report(y_test,dtpred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross_val_score Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\ncr = cross_val_score(rf,x,y,cv = 10)\nprint(\"Cross Value Score Random Forest:{}\".format(cr.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# play with Hyper Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Manual Hypertuning  Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100)\nrf.fit(x_train,y_train)\nrfpred = rf.predict(x_test)\ncon = confusion_matrix(y_test,rfpred)\nprint(con)\nplt.title(\"confusion_matrix for RandomForest Classifier\")\nsns.heatmap(con , annot = True ,cmap = \"Pastel1\")\nprint(\"accuracy Score for RandomForest Classifier:{}\".format(accuracy_score(y_test,rfpred) * 100 ))\nprint(\"Classification_report for RandomForest Classifier\")\nprint(classification_report(y_test,rfpred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest HuperParem Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 2, stop = 2000, num = 1000)]\nmax_features = ['auto', 'sqrt','log2']\nmax_depth = [int(x) for x in np.linspace(1, 1000,500)]\nmin_samples_split = [2, 5, 10,14]\nmin_samples_leaf = [1, 2, 4,6,8]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\nrf_randomcv.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_randomcv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rbest = rf_randomcv.best_estimator_\nrbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfpred = rbest.predict(x_test)\nconfusion = confusion_matrix(y_test , rfpred)\nprint(confusion)\nprint(\"Accuray Score HyperTuning Random forest:{}\".format(accuracy_score(y_test,rfpred) * 100))\nsns.heatmap(confusion ,annot = True , cmap = \"rainbow\")\nplt.title(\"Confusion_matrix for HyperTuning Random forest\")\nprint(\"Classification Report HyperTuning Random forest:{}\".format(classification_report(y_test , rfpred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparision\n1.   Logistic Regression - 76.666\n1.   Logistic Regression(std) - 81.66\n1.   RandomForestClassifier - 85.00\n1.   GradientBoosting -85.00\n1.   Support vector Classifeir - 61.66               \n1.   SVC (std)                 - 80.5\n1.   KNN Classifier            - 61.66\n1.   Decision Tree Classifier  - 83.5\n1.   Random Forest Classifier( Manual hyper parem) - 85.0\n1.   Random Forest Classifier(hyper Parem) - 85.00 \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nApplying All Algorithm RandomforestClassifier and Gradient Boosting Perform Good Accuracy nearly 85.00","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}