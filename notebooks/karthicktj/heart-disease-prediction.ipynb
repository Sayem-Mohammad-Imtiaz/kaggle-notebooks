{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Heart Disease Prediction \n\nThis Dataset has Whether the Person affected by Heart Diseses or not "},{"metadata":{},"cell_type":"markdown","source":"# Importing Library\n\nBefore To starts with,I imported necessary libraries "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt# data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import cm\nfrom IPython.display import Image \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Dataset "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# When Easy To UnderStand ALL Feature\n\n1. age: The person's age in years\n1. sex: The person's sex (1 = male, 0 = female)\n1. cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n1. trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n1. chol: The person's cholesterol measurement in mg/dl\n1. fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n1. restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n1. thalach: The person's maximum heart rate achieved\n1. exang: Exercise induced angina (1 = yes; 0 = no)\n1. oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n1. slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n1. ca: The number of major vessels (0-3)\n1. thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n1. target: Heart disease (0 = no, 1 = yes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()  #last Five rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() #about the Dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the Data:{}\".format(data.shape)) #shape of the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Value \nHandling Missing Value is very Important in Machine Learning ,Because Algorithms Does Not Support  Missing Value to process it"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check any missing value in the data set\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the Data set There is no missing value is present"},{"metadata":{},"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in data.columns:\n    print(\"{} in Unique Values:{}\".format(feature ,data[feature].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.distplot(data.age,kde = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Check whether The  data is Balanced Dataset or Imbalance Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"sex\" ,data = data ,palette = \"Set3\" ,ax = ax[0])\nax[0].set_title(\"sex (Male =1 ,,Female =0)\")\nlabels = [\"Male\" ,\"Female\"]\ncolors = [\"lightskyblue\" ,\"gold\"]\nax[1].set_title(\"sex (Male =1 ,,Female =0)\")\ndata.sex.value_counts().plot.pie(explode = [0.1,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"cp\" ,data = data ,palette = \"Accent\" ,ax = ax[0])\nax[0].set_title(\"Cheast Pain(cp)\")\nax[1].set_title(\"CheastPain(cp)\")\ncolors = [\"lightskyblue\" ,\"gold\",\"red\",\"blue\"]\ndata.cp.value_counts().plot.pie(explode = [0.1,0,0,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1],colors = colors)\nlabels = [\"0\",\"1\",\"2\",\"3\"]\nax[1].legend(labels ,loc = \"lower right\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax =plt.subplots(1,2,figsize=(20,10))\nsns.countplot(x=\"cp\" ,hue = \"sex\" ,orient = 'h' ,data = data ,saturation = 0.9,palette = \"ocean\",ax=ax[0])\nax[0].set_title(\"Relationship Between Cp(chest Pain) and Sex(Male:1 , Female :0) \")\nsns.countplot(x = \"sex\" , data = data ,hue = \"target\" ,palette = \"Set2\" ,ax = ax[1])\nax[1].set_title(\"Heart Disease affected by sex(Male:1 ,Female:0)\")\nlabels = [\"0-Not affected \",\"1-affted\"]\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(data.trestbps ,ax =ax[1])\nax[0].set_title(\"trestbps(resting blood pressure\")\ndata.hist(column = \"trestbps\",bins = 10 ,ax = ax[0])\nax[1].set_title(\"trestbps(resting blood pressure)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"target\" ,data = data ,palette = \"Set2\" ,ax = ax[0])\nax[0].set_title(\"Target (1= Affected by Heart diseases ,0=Not Affected by Heart diseases)\")\nlabels = [\"Affected by Heart diseases\" ,\"Not Affected by Heart diseases\"]\ncolors = [\"lightskyblue\" ,\"gold\"]\nax[1].set_title(\"Target (1= Affected by Heart diseases ,0=Not Affected by Heart diseases)\")\ndata.target.value_counts().plot.pie(explode = [0.1,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.kdeplot(data.chol,shade = True)\nplt.title(\"Serum cholestoral in mg/dl\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"fbs\" ,data = data ,palette = \"Pastel1\" ,ax = ax[0])\nax[0].set_title(\"Fasting blood sugar &gt; 120 mg/dl) (1 = True; 0 = False)\")\nlabels = [\"1-True\" , \"0-False\"]\ncolors = [\"lightskyblue\" ,\"red\"]\nax[1].set_title(\"Fasting blood sugar &gt; 120 mg/dl) (1 = True; 0 = False)\")\ndata.fbs.value_counts().plot.pie(explode = [0.1,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"restecg\" ,data = data ,palette = \"Pastel2\" ,ax = ax[0])\nax[0].set_title(\"Resting electrocardiographic results\")\ncolors = [\"gold\" ,\"red\" ,\"lightskyblue\"]\nax[1].set_title(\"Resting electrocardiographic results\")\nlabels =[0,1,2]\ndata.restecg.value_counts().plot.pie(explode = [0,0,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.violinplot(x=\"target\", y =\"thalach\" ,hue =\"sex\" ,data = data,palette = \"Set3\")\nplt.title(\"Maximum heart rate affeted person heart Diseases (1-Affected ,0-Not Affected)\")\nlabels = [\"1-Male\" ,\"0-Female\"]\nplt.legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(10,5))\nsns.boxplot(x=\"exang\" ,y = \"age\" , data = data ,palette = \"Reds\")\nplt.title(\"Relationship Between Exang and Age\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(\"target\")[\"oldpeak\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,3,figsize = (10,5))\nsns.boxplot(x = \"oldpeak\", data = data,ax=ax[0])\nax[0].set_title(\"Oldpeak\")\nsns.kdeplot(data.oldpeak ,ax = ax[1] )\nax[1].set_title(\"oldpeak\")\nsns.scatterplot(x= \"age\" , y = \"oldpeak\" ,data = data ,ax=ax[2])\nax[2].set_title(\"Relationship between Age and old peak\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target(data ,feature):\n    x = data.groupby(feature)[\"target\"].sum()\n    print(\"Feature {}:{}\".format(feature ,x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns =[\"slope\" ,\"ca\" ,\"thal\"]\nfor var in columns:\n    target(data , var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"slope\" ,data = data ,palette = \"Pastel1\",hue = \"target\" ,ax = ax[0] )\nax[0].set_title(\"The slope of the peak exercise ST segment (0: upsloping, 1: flat, 2: downsloping)\")\nlabels = [ \"0: upsloping\", \"1: flat\", \"2: downsloping\"]\ncolors = [\"lightskyblue\" ,\"red\" ,\"gold\"]\nax[1].set_title(\"The slope of the peak exercise ST segment (0: upsloping, 1: flat, 2: downsloping)\")\ndata.slope.value_counts().plot.pie(explode = [0.1,0,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x=\"ca\" ,data = data ,palette = \"Pastel2\",hue = \"target\" ,ax = ax[0] )\nax[0].set_title(\"Number of Major Vessels Affectd by Heart Diseases\")\nax[0].set_xlabel(\"Ca Major Vessels\")\nlabels = [\"0\",\"1\",\"2\",\"3\" ,\"4\"]\ncolors = [\"lightskyblue\" ,\"red\" ,\"gold\" ,\"blue\" ,\"green\"]\nax[1].set_title(\"Number of Major Vessels \")\ndata.ca.value_counts().plot.pie(explode = [0.1,0,0,0,0] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig ,ax = plt.subplots(1,2,figsize =(20,10))\nsns.countplot(x = \"thal\" ,data = data ,palette = \"Set1\",hue = \"target\" ,ax = ax[0] )\nax[0].set_title(\"Thalassemia A Blood Disorder Affectd by Heart Diseases\")\nax[0].set_xlabel(\"thal - Thalassemia\")\nlabels = [\"0\" ,\"1\" ,\"2\" ,\"3\"]\ncolors = [\"red\" ,\"gold\" ,\"blue\",\"lightskyblue\"]\nax[1].set_title(\"Thalassemia A Blood Disorder \")\ndata.thal.value_counts().plot.pie(explode = [0,0,0 ,0.1] ,autopct=\"%1.1f%%\" ,shadow = True,ax=ax[1] , labels = labels , colors = colors )\nax[1].legend(labels ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier\nAn Outliers, being the most extreme observations,Oulier can affect the model perfomance,A good practice remove outlier gives a good result."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = [\"trestbps\",\"chol\"]\nfig = plt.figure(figsize=(15,5))\nfor i ,var in zip(range(1,3),df):\n    ax = fig.add_subplot(1,3,i)\n    sns.boxplot(data[var] ,ax= ax ,palette = \"Set3\")\n    plt.xlabel(var)\n    plt.title(var)\nplt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We she some oulier is present The threstbps featue 99% of value present below 170 and same as feature chol 99% data present below 380 the other values are outlier so we take a IQR "},{"metadata":{},"cell_type":"markdown","source":"# IQR (Inter Quantile Range)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nIQR = data[\"trestbps\"].quantile(0.75) - data[\"trestbps\"].quantile(0.25)\nupper_boundary = data[\"trestbps\"].quantile(0.75) + (1.5 * IQR)\nLower_boundary = data[\"trestbps\"].quantile(0.25) - (1.5 * IQR)\nprint(\"IQR: {} upperBoundary:{} and Lowerboundary:{}\".format(IQR , upper_boundary ,Lower_boundary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#chol feature\nIQR = data[\"chol\"].quantile(0.75) - data[\"trestbps\"].quantile(0.25)\nupper_boundary = data[\"chol\"].quantile(0.75) + (1.5 * IQR)\nLower_boundary = data[\"chol\"].quantile(0.25) - (1.5 * IQR)\nprint(\"IQR: {} upperBoundary:{} and Lowerboundary:{}\".format(IQR , upper_boundary ,Lower_boundary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"trestbps\"] > 170 ,\"trestbps\" ] = 170\ndata.loc[data[\"chol\"]>400 , \"chol\"] = 400","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncorr = data.corr()\nsns.heatmap(corr ,annot = True ,cmap = \"Pastel1\")\nplt.title(\"Correlation Between All Feature\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good practice to remove correlated variables during feature selection.In the data there is no one Feature in highly Correlated so we cannot remove any one feature"},{"metadata":{},"cell_type":"markdown","source":"# Convert Some feature into categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()\ndata['sex'] = data['sex'].astype('object')\ndata['cp'] = data['cp'].astype('object')\ndata['fbs'] = data['fbs'].astype('object')\ndata['restecg'] = data['restecg'].astype('object')\ndata['exang'] = data['exang'].astype('object')\ndata['slope'] = data['slope'].astype('object')\ndata['thal'] = data['thal'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The above feature are coverted into catagorical "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Encoding "},{"metadata":{},"cell_type":"markdown","source":"For the categorical varibles, we need to create dummy variables. I'm also going to drop the first category of each. For example, rather than having 'male' and 'female', we'll have 'male' with values of 0 or 1 (1 being male, and 0 therefore being female)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data ,drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Encoding Check Shape of Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X-y split"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.drop(columns =\"target\" ,axis = 1)\ny = data[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score ,recall_score ,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting Model"},{"metadata":{},"cell_type":"markdown","source":"When I spliting my data  80% Training and 20% Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y ,test_size =0.2 , random_state = 0)\nprint(\"Shape of Training  and  Testing\")\nprint(x_train.shape ,x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizing  features"},{"metadata":{},"cell_type":"markdown","source":"Before I train the Model first Normalizing the features \nNormalization Simply said the data feature range from 0 to 1\n\n# Xnorm = X - Xmin / Xmax - Xmin"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nx_train_std = MinMaxScaler().fit_transform(x_train)\nx_test_std = MinMaxScaler().fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithms used\n1. LogisticRegression\n1. RandomForestClassifier\n1. DecisionTreeClassifier\n1. SupportVectorClassifier\n1. KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = {\"LG\":LogisticRegression() ,\"RF\":RandomForestClassifier() ,\"DT\": DecisionTreeClassifier() ,\"svc\":SVC()}\n\ndef create_modle(model ,x_train ,y_train ,x_test,y_test): \n    model_score_train = {}\n    model_score_test = {}\n    cnn = {}\n    for name,model in model.items():\n        np.random.seed(42)\n        model.fit(x_train ,y_train) #fit model\n        pred = model.predict(x_test)\n        model_score_train[name] = model.score(x_train ,y_train)\n        model_score_test[name] = model.score(x_test ,y_test)\n        cnn[name] = confusion_matrix(y_test ,pred)\n        \n    return model_score_train,model_score_test,cnn\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_score = create_modle(model , x_train ,y_train ,x_test ,y_test)\ntrain ,test,cnn = training_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pd.DataFrame({\"Train_score\":train ,\"Testing_score\":test })\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,20))\ndata1.T.plot.bar(width = 0.9)\nplt.title(\"Model Comparison of Training  and Testing Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig  = plt.figure(figsize=(10,5))\na =1\nfor key,name in cnn.items():\n    ax = fig.add_subplot(2,2,a)\n    plt.title(key)\n    sns.heatmap(name ,annot =True , ax =ax ,cmap = \"Set3\")\n    a = a+1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_score_std = create_modle(model , x_train_std ,y_train ,x_test_std ,y_test)\ntrain_std ,test_std,cnn_std = training_score_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Score of After Normalization :{}\".format(train_std))\nprint(\"Testing Score of After Normalization :{}\".format(test_std))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig  = plt.figure(figsize=(10,5))\na =1\nfor key,name in cnn_std.items():\n    ax = fig.add_subplot(2,2,a)\n    plt.title(key)\n    sns.heatmap(name ,annot =True , ax =ax ,cmap = \"Set3\")\n    a = a+1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model HyperTuning"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression with HyperTuning using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {\"penalty\": [\"l1\",\"l2\" ,\"elasticnet\" ,\"none\"],    #Regularization paramater\n         \"C\" : [0.1,0.001,0.1,1.0,1.5 ,3.0],             #strength of regularization\n         \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\"],\n         \"multi_class\":['auto', 'ovr', \"multinomial\"],\n         \"max_iter\" :[10,20,30,50,100]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LG_H = LogisticRegression()\nGrid_lg = GridSearchCV(LG_H , param_grid = param ,cv = 5 , scoring = 'accuracy' ,n_jobs = -1).fit(x_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_lg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_best_est = Grid_lg.best_estimator_\nLG_Gr_pred = Grid_best_est.predict(x_test_std)\nLg_accuracy = accuracy_score(y_test , LG_Gr_pred)\nLg_precision = precision_score(y_test , LG_Gr_pred)\nLg_recall = recall_score(y_test ,LG_Gr_pred)\nLg_conf = confusion_matrix(y_test , LG_Gr_pred)\nprint(\"Accuracy_score of Logistic Regression:{}\".format(round(Lg_accuracy * 100)))\nprint(\"RecallScore(Positive Prediction,Low False Negative Rate):{}\".format(round(Lg_recall*100)))\nprint(\"PrecisionScore(Low false Postitive rate):{}\".format(round(Lg_precision *100)))\nsns.heatmap(Lg_conf ,annot = True , cmap = \"Pastel1\")\nplt.title(\"Confusion_matrix on LogisticRegression\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classifier HyperTuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = ['linear', 'poly', 'rbf', 'sigmoid']  # what type of algorithm is used\nC = [0.001,0.005,0.01,0.05, 0.1, 0.5, 1, 5, 10, 50,100,500,1000] \ngamma = [0.001, 0.01, 0.1, 0.5, 1]     #this parem used for Rbf\ndegree =[1,2,3,4]                     #used for polmonial algorithm\nparam = {'kernel':kernel , #\n         \"gamma\":gamma,\n         \"degree\": degree,\n            \"C\" :C}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc_grid = GridSearchCV(svc , param_grid = param ,scoring = \"accuracy\" ,n_jobs = -1 ,verbose = 2 ,cv = 10).fit(x_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_est = svc_grid.best_estimator_\nsvc_Gr_pred = Grid_best_est.predict(x_test_std)\nsvc_accuracy = accuracy_score(y_test , svc_Gr_pred)\nsvc_precision = precision_score(y_test , svc_Gr_pred)\nsvc_recall = recall_score(y_test ,svc_Gr_pred)\nsvc_conf = confusion_matrix(y_test , svc_Gr_pred)\nprint(\"Accuracy_score of Support Vector Classifier:{}\".format(round(svc_accuracy * 100)))\nprint(\"RecallScore(Positive Prediction,Low False Negative Rate):{}\".format(round(svc_recall*100)))\nprint(\"PrecisionScore(Low false Postitive rate):{}\".format(round(svc_precision *100)))\nsns.heatmap(Lg_conf ,annot = True , cmap = \"Blues\")\nplt.title(\"Confusion_matrix on Support Vector Classifier\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kneighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbour = [i for i in range(1,30)]\nprint(neighbour ,end = \" \")\ntrain_score,test_score =[] ,[]\nfor n in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors = n).fit(x_train ,y_train)\n    train_score.append(knn.score(x_train ,y_train))\n    test_score.append(knn.score(x_test ,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.plot(train_score)\nplt.plot(test_score)\nplt.title(\"Training and Testing Accuracy\")\nplt.xlabel(\"Kneighbors\")\nplt.ylabel(\"accuracy\")\nplt.xticks(range(1,21))\nplt.legend(labels=[\"Train_score\" ,\"Test_score\"] ,loc = \"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier HyperTuning using RandomizedCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nparem ={\"criterion\" : ['gini', 'entropy'] ,\n       \"splitter\":['best', 'random'] ,\n        \"max_depth\" : [int(x) for x in np.linspace(1, 1000,500)],\n        'min_samples_split': [2, 5, 10,14],\n        'min_samples_leaf' :[1, 2, 4,6,8]\n       }\nDT = DecisionTreeClassifier()\nRandomized = RandomizedSearchCV(estimator=DT,param_distributions=parem,n_iter=100,cv=5,verbose=2,\n                               random_state=100,n_jobs=-1)\nRandomized.fit(x_train ,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Randomized.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best_estimater = Randomized.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = Best_estimater.predict(x_test)\ny_pred = Best_estimater.predict_proba(x_test)[:, 1]\nConfusion_Dt = confusion_matrix(y_test ,pred)\nprint(\"Accuracy_score for Decision Tree:{}\".format(accuracy_score(y_test ,pred)))\nsns.heatmap(Confusion_Dt , annot = True ,cmap = \"Set3\")\nplt.title(\"Confusion_matrix for Decision Tree\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nFalse_positive_rate, True_pos_rate, thresholds = roc_curve(y_test, y_pred)\n\nfig, ax = plt.subplots()\nplt.plot(False_positive_rate ,True_pos_rate)\nplt.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC  classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier with HyperTuning using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT = DecisionTreeClassifier()\nparem ={\"criterion\" : ['gini', 'entropy'] ,\n       \"splitter\":['best', 'random'] ,\n        \"max_depth\" : [int(x) for x in np.linspace(1, 100,50)],\n        'min_samples_split': [2, 5, 10,14],\n        'min_samples_leaf' :[1, 2, 4,6,8]\n       }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GS_T = GridSearchCV(estimator = DT , param_grid = parem , scoring = \"accuracy\" ,n_jobs = -1 ,cv = 5  ).fit(x_train ,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GS_T.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GSVT = GS_T.best_estimator_\ndpred = GSVT.predict(x_test)\nd_con = confusion_matrix(y_test ,dpred)\nd_precision = precision_score(y_test ,dpred)\nd_Recall = recall_score(y_test ,dpred)\nd_accuracy = accuracy_score(y_test , dpred) \nprint(\"Accuracy_score of Decision Tree Classifier:{}\".format(round(d_accuracy * 100)))\nprint(\"RecallScore(Positive Prediction,Low False Negative Rate):{}\".format(round(d_Recall*100)))\nprint(\"PrecisionScore(Low false Postitive rate):{}\".format(round(d_precision *100)))\nsns.heatmap(d_con ,annot = True , cmap = \"Blues\")\nplt.title(\"Confusion_matrix on Decision_tree\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier Hyper Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 2, stop = 2000, num = 1000)]\nmax_features = ['auto', 'sqrt','log2']\nmax_depth = [int(x) for x in np.linspace(1, 1000,500)]\nmin_samples_split = [2, 5, 10,14,15]\nmin_samples_leaf = [1, 2, 4,6,8,10,12]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nRF=RandomForestClassifier()\nRf_tuning=RandomizedSearchCV(estimator=RF,param_distributions=random_grid,n_iter=100,cv=5,verbose=2,\n                               random_state=100,n_jobs=-1)\nRf_tuning.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rf_tuning.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = Rf_tuning.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_pred = rf.predict(x_test)\nConfusion_Dt = confusion_matrix(y_test ,RF_pred)\npre = precision_score(y_test ,RF_pred)\nRecall = recall_score(y_test ,RF_pred)\nprint(\"Accuracy_score for Random Forest Classifier:{}\".format(accuracy_score(y_test ,RF_pred)))\nprint(\"RecallScore(Positive Prediction,Low False Negative Rate):{}\".format(round(Recall*100)))\nprint(\"PrecisionScore(Low false Postitive rate):{}\".format(round(pre *100)))\nsns.heatmap(Confusion_Dt , annot = True ,cmap = \"Set3\")\nplt.title(\"Confusion_matrix for RandomForestClassifier\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}