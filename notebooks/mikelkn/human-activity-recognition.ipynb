{"cells":[{"metadata":{"_uuid":"b0c6009a596ad4d3ca4b9e024d274f9b5d451e8a"},"cell_type":"raw","source":""},{"metadata":{"_uuid":"1ee2c8b3e1bf52ded30736d64f526e5d0ea1a644"},"cell_type":"markdown","source":"\nWORK PLAN\n\nIn this Project:\n* Import the necessary Libraries\n* Load and analyse the data\n* Find Correlations among the faetures\n* Split the data into train and test data(validation data)\n* Predict the activity using Logistic Regression and Logisctic Regression CV\n* Calculate the Classification error metrics \n* Feature selection to pick the best features for the a better prediction\n* Calculate the new classification error metric\n* Compare 6 and 8 above to get the best model\n* Conclusion and submission"},{"metadata":{"_uuid":"f19afc9fd7458c45670e586cba4e08b4d828bf89"},"cell_type":"markdown","source":"I am using the Kaggle data which can be found here:\n\nhttps://www.kaggle.com/uciml/human-activity-recognition-with-smartphones/downloads/human-activity-recognition-with-smartphones.zip  "},{"metadata":{"_uuid":"8c0cfff95bda2b600b1f65a44051bff19a91841b"},"cell_type":"markdown","source":"## 1 - Importing thr Necessary Libraries"},{"metadata":{"trusted":true,"_uuid":"665af8f0bdd3a44a5833b640ef5491833ce26463"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as error_metric\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6675b96bbda6daa79de310f98dc34261199b8131"},"cell_type":"markdown","source":"## 2 - Load and analyse the data"},{"metadata":{"trusted":true,"_uuid":"1da6c8b2944249a5feeea241344734229b4380cd"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc785d9325f03d9ae75c87b9bf74a4529a94f045"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2279093b87e66bf895b02087fde7e73e96f7248"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f8612b7bc3255fa2fe5a6302cad38236d1bbbee"},"cell_type":"code","source":"#Check for null values\ntrain.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"609a4127f3bf357443b93150187fce1053024fb0"},"cell_type":"code","source":"test.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6d5f0761acf6cd620bdbae425e1590e7b86de92"},"cell_type":"markdown","source":"There are no null values in either the test and the train datasets"},{"metadata":{"_uuid":"194c89232b734115000c47af25e7a4ac29a934b4"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"bdebd2a8a479a9312f208b749f0bd238e68c2819"},"cell_type":"markdown","source":"The subject column is not going to be usefull here so i will drop it from both data sets"},{"metadata":{"trusted":true,"_uuid":"aa4c38088ec780b77bd705de0c8c99093e1f7ac4"},"cell_type":"code","source":"train.drop('subject', axis =1, inplace=True)\ntest.drop('subject', axis =1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"757e3c987572e251252fc45b78f2f79e4e63fe57","scrolled":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92f7774ba6b1ec9313eccb35cb26ca9e5b479278"},"cell_type":"code","source":"rem_cols2 = test.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faeda7dfb001582a78f9bc190ec733a967f6f7a3"},"cell_type":"code","source":"# We check the datatypes \ntrain.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c837ce1b8844c6480eafb577b2f33fa36a3ce7d3"},"cell_type":"code","source":"test.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e22d319cc7726f0e8f286a89d835828153840ce"},"cell_type":"markdown","source":"****Should we rescale the data? Scaling a dataset usually produces better dataset and more accurate predictions. First we check the range( the min and the max) for each of the datasets. Lets try using the .describe() method and lets exclude the activity column which is the last column. ****"},{"metadata":{"trusted":true,"_uuid":"3651dafb2f6485e6be828085dc8a23bb2239a5e5"},"cell_type":"code","source":"train.describe()  #we see that the min = -1 and the max = +1. so no need for scaling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66361aeca9d1f0fb23eedfb42921fe3f80f50cce"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"031b3410ef41205db8726ebb88cda6f6f931c3bd"},"cell_type":"code","source":"train.dtypes.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f759b881eeb1f9bc7755d0dc48722a428812d78f"},"cell_type":"markdown","source":"They have the same data types. That is, mostly floats and one object feature. Lets see what the object feature is abd extract it from the rest"},{"metadata":{"trusted":true,"_uuid":"d469c1b657620b4a9314e5d3ccc5b74b1eb04a69"},"cell_type":"code","source":"object_feature = train.dtypes == np.object\nobject_feature = train.columns[object_feature]\nobject_feature","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41a0f95258b951d4c6701c2a54961113b83fd1d8"},"cell_type":"markdown","source":"As we can see, the only object data type in both train and the test dataset is the Activity feature. Lets take a closer look at it..."},{"metadata":{"trusted":true,"_uuid":"50c0f87665b06a2bb2b229011d9b279f83e70f65"},"cell_type":"code","source":"train.Activity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4af9c6d713ccf89522fcf49d0769fe6c6f6adcac"},"cell_type":"markdown","source":"We need to encode the Activity column becasue sklearn won't accept sparse matrix as prediction columns . WEe will use LabelEncoder to encode the Activities "},{"metadata":{"trusted":true,"_uuid":"f0449a47956e256507bfd1cdaa3fc6d7339812c9"},"cell_type":"code","source":"le = LabelEncoder()\nfor x in [train, test]:\n    x['Activity'] = le.fit_transform(x.Activity)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b62942c0bb687f05a0c8a91d1b2c36a5e829908b"},"cell_type":"code","source":"train.Activity.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b536955db4552b394045c96f719fc2783e8f0aac"},"cell_type":"code","source":"test.Activity.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8388250ebd435fd3a129b195f6ab82dcff7db6f"},"cell_type":"markdown","source":"## 3- Finding the Correlation/ Relationships between the features\n\nCorrelation refers to the mutual relationship and association between quantities and it is generaly used to express one quantity in terns of its relationship with other quantities. The can either be Positive(variables change in the same direction), negative(variables change in opposite direction or neutral(No correlation).\n\nVariable within a dataset can be related in lots of ways and for lost of reasons:\n    - They could depend on values of other variable\n    - They could be associated to each other\n    - They could both depend on a thirf variable.\n    \nIn this project, we will be using the pandas method .corr() for calculating correlation between dataframe columns"},{"metadata":{"trusted":true,"_uuid":"be672b74e3fc102922b1b502746b57acf4a63f2d"},"cell_type":"code","source":"feature_cols = train.columns[: -1]   #exclude the Activity column\n#Calculate the correlation values\ncorrelated_values = train[feature_cols].corr()\n#stack the data and convert to a dataframe\n\ncorrelated_values = (correlated_values.stack().to_frame().reset_index()\n                    .rename(columns={'level_0': 'Feature_1', 'level_1': 'Feature_2', 0:'Correlations'}))\ncorrelated_values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea18c361b3cd72ab4c1c010ec8888e32cd587ff2"},"cell_type":"code","source":"#create an abs_correlation column\ncorrelated_values['abs_correlation'] = correlated_values.Correlations.abs()\ncorrelated_values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d45f4e316219d8e764ce11428c6a2dd1f9df1d"},"cell_type":"code","source":"#Picking most correlated features\ntrain_fields = correlated_values.sort_values('Correlations', ascending = False).query('abs_correlation>0.8')\ntrain_fields.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73b43d31220091e9ae65f298f1c99acaec475ae7"},"cell_type":"markdown","source":"## 4 - Splitting the data into train and validation "},{"metadata":{"trusted":true,"_uuid":"d0ad37996bbd59446db40248abbb5876d6d6fe9b"},"cell_type":"code","source":"#Getting the split indexes\n\nsplit_data = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 42)\ntrain_idx, val_idx = next(split_data.split(train[feature_cols], train.Activity))\n\n#creating the dataframes\n\nx_train = train.loc[train_idx, feature_cols]\ny_train = train.loc[train_idx, 'Activity']\n\nx_val = train.loc[val_idx, feature_cols]\ny_val = train.loc[val_idx, 'Activity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"718c89e83589f7cc52b9dbf200b001af77615ff2"},"cell_type":"code","source":"y_train.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84290580974aaca028fa6c6fbaef9ccc91403e81","scrolled":true},"cell_type":"code","source":"y_val.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b73a840624d4e6e9c30af1a5940aaa51df79361"},"cell_type":"code","source":"#Same ratio of classes in both the train and validation data thanks to StratifiedShuffleSPlit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caceb4ec079b587086a31daa4bdf3c3fbe32d17a"},"cell_type":"markdown","source":"## 5 - Predictive Modelling"},{"metadata":{"trusted":true,"_uuid":"18519b63921793021c3d3b1f51dc2adae8caf363"},"cell_type":"code","source":"lr = LogisticRegression()\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2')\nrf = RandomForestClassifier(n_estimators = 10)\n\nlr = lr.fit(x_train, y_train)\n\nrf = rf.fit(x_train, y_train)\n\nlr_l2 = lr_l2.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"818dd86490e167e73a7d836769a03f82765233cc"},"cell_type":"code","source":"#predict the classes and probability  for each\n\ny_predict = list()\ny_proba = list()\n\nlabels = ['lr', 'lr_l2', 'rf']\nmodels = [lr, lr_l2, rf]\n\nfor lab, mod in zip(labels, models):\n    y_predict.append(pd.Series(mod.predict(x_val), name = lab))\n    y_proba.append(pd.Series(mod.predict_proba(x_val).max(axis=1), name = lab))\n    #.max(axis = 1) for a 1 dimensional dataframe\n\ny_predict = pd.concat(y_predict, axis = 1)\ny_proba = pd.concat(y_proba, axis = 1)\n\ny_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbbb49fa44819d6b2bbaffa6a4d5f7cce3495664","scrolled":true},"cell_type":"code","source":"y_proba.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cb56116116def7bac0eee2c0882579d5a4b6044"},"cell_type":"markdown","source":"## 6 - Calculating the Error Metrics"},{"metadata":{"trusted":true,"_uuid":"26a2a7d026c65feb67836c3cfcfa3989ab5059bc"},"cell_type":"code","source":"metrics = list()\nconfusion_m = dict()\n\nfor lab in labels:\n    precision, recall, f_score, _ = error_metric(y_val, y_predict[lab], average = 'weighted')\n    \n    accuracy = accuracy_score(y_val, y_predict[lab])\n    \n    confusion_m[lab] = confusion_matrix(y_val, y_predict[lab])\n    \n    metrics.append(pd.Series({'Precision': precision, 'Recall': recall,\n                            'F_score': f_score, 'Accuracy': accuracy}, name = lab))\n    \nmetrics= pd.concat(metrics, axis =1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc9b037b49d8985df84ffb1723055361a1072f0"},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6f58b0337bfe1858cde5476a77b5b7589c0151f9"},"cell_type":"code","source":"fig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], labels):\n    sns.heatmap(confusion_m[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7815b0df1ab8b40de887c9a34358a1bf38f2b7a"},"cell_type":"markdown","source":"Observation: \n\nWe can see that the Logistic regression with L2 regularization gives slightly better error metric than the other models. In part 2 of this porject, we will look at the effect of correlation on the error metrics. The question we ask here is:\n\nWhat happens when we discard the most correlated feature? do we have a better model or not?\n\nwe will discard the features whose threshold is less that 0.8 that is, features with low variance. We will be using the sklearn feature_selection method VarianceThreshold.\n        "},{"metadata":{"_uuid":"bbcd400cd9934e9dc0554238e70e3d26edc25f20"},"cell_type":"markdown","source":"## PART 2 - Feature_selection: Discarding the Most Correlated Features"},{"metadata":{"trusted":true,"_uuid":"0383da0b9a36aa2cc1cedec79370eb8837ff64a1"},"cell_type":"code","source":"#Remeber>..\ntrain_fields.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cea3cc08813bcc6c3398b1f481fa23207f3e754a"},"cell_type":"code","source":"#Getting the features with high Variance and split the data into train and test\n\nlow_var = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))\n\ntrain2 = pd.concat([x_train,x_val])\ntrain_new = pd.DataFrame(low_var.fit_transform(train2))\n                         \ntest_new = pd.concat([y_train,y_val])\n\n                         \nx_new,x_val_new = train_test_split(train_new)\ny_new,y_val_new = train_test_split(test_new)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"765b6f82faf3b909e36f24616e2a75314bfd9d5e"},"cell_type":"markdown","source":"## Predictive Models"},{"metadata":{"trusted":true,"_uuid":"e9838a9cf51a1a68af9b108c8f5567cf62b34792"},"cell_type":"code","source":"lr_new = lr.fit(x_new, y_new)\n\nlr_l2_new = lr_l2.fit(x_new, y_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa9ee6a0c47a64ef6a7f0a414b3fb7ee57cd0ed"},"cell_type":"code","source":"#predict the classes and probability  for each\n\ny_predict_new = list()\ny_proba_new = list()\n\nlabels_new = ['lr_new', 'lr_l2_new']\nmodels_new = [lr_new, lr_l2_new]\n\nfor lab, mod in zip(labels_new, models_new):\n    y_predict_new.append(pd.Series(mod.predict(x_val_new), name = lab))\n    y_proba_new.append(pd.Series(mod.predict_proba(x_val_new).max(axis=1), name = lab))\n    #.max(axis = 1) for a 1 dimensional dataframe\n\ny_predict_new = pd.concat(y_predict_new, axis = 1)\ny_proba_new = pd.concat(y_proba_new, axis = 1)\n\ny_predict_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01959296dfc40a7a706656a9007ec915991bb11c"},"cell_type":"code","source":"y_proba_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"921998c762f3de1d357a44cce75808e434214be7"},"cell_type":"markdown","source":"## Calculating the error metrics"},{"metadata":{"trusted":true,"_uuid":"f02513a8984d94d2dc366b87b3b0dd711002e620"},"cell_type":"code","source":"metrics_new = list()\ncon_mat = dict()\n\nfor lab in labels_new:\n    precision, recall, f_score, _ = error_metric(y_val_new, y_predict_new[lab], average = 'weighted')\n    \n    accuracy = accuracy_score(y_val_new, y_predict_new[lab])\n    \n    con_mat[lab] = confusion_matrix(y_val, y_predict[lab])\n    \n    metrics_new.append(pd.Series({'precision': precision, 'recall': recall,\n                            'f_score': f_score, 'accuracy': accuracy}, name = lab))\n    \nmetrics_new= pd.concat(metrics_new, axis =1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e00d7d1bc2e500eb5d2e6774ee749283f5585ec"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}