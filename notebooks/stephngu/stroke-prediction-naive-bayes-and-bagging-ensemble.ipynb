{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The objective of this project is to create models (i.e. Naive Bayes and Bagging Ensemble) that could help predict a future stroke based on certain lifestyle features such as Gender, Age, Hypertension, Heart Disease, Ever Married, Work Type, Residence Type, Avg. Glucose Level, BMI, Smoking Status.","metadata":{}},{"cell_type":"markdown","source":"These are the libraries used to help achieve this project","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics, model_selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing healthcare dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\n                 \ndf.columns = [\"ID\", \"Gender\", \"Age\", \"Hypertension\", \"Heart Disease\", \"Ever Married\", \"Work Type\", \"Residence Type\", \"Avg. Glucose Level\", \"BMI\", \"Smoking Status\", \"Stroke\"]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping ID column. This feature does not correspond with the data analysis being conducted.","metadata":{}},{"cell_type":"code","source":"#drop ID column\ndf = df.drop('ID', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we want to visualize our dataset and understand the distribution of each feature and label.","metadata":{}},{"cell_type":"code","source":"#plots other feature sets\ndf.hist(figsize = (15, 15))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have identified that stroke label set is very unbalanced which could affect how we process our data. We need to fix this issue.","metadata":{}},{"cell_type":"code","source":"#plots Stroke feature\ndf['Stroke'].value_counts(dropna = False).plot.bar(color = 'blue')\nplt.title('Imbalanced Stroke Feature')\nplt.xlabel('zero & one')\nplt.ylabel('count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identifying null values in our dataset. BMI column had missing values therefore deleting the entire row was the decision. We have enough data that deleting will not affect our classification significantly.","metadata":{}},{"cell_type":"code","source":"print(df.isnull().sum())\nprint(df.count())\n\n#removing null values in BMI column\ndf.dropna(axis=0, inplace=True)\ndf.reset_index(drop=True, inplace=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verifying null values were deleted.","metadata":{}},{"cell_type":"code","source":"print(df.isnull().sum())\nprint(df.count())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transforming categorical data into binary integers. Method used is get_dummies. Label_encoders was not used since it is more fit for ranking hierarchy. Get_dummies allows us to transform features with multiple categories into separate feature. This increases our feature set size. ","metadata":{}},{"cell_type":"code","source":"\n#transforming dataset with dummies variables to replace characters with binary integers\ndf[\"Hypertension\"].replace([0,1], [\"No\",\"Yes\"], inplace=True)\ndf[\"Heart Disease\"].replace([0,1], [\"No\",\"Yes\"], inplace=True)\n\ndf2 = df[[\"Gender\",\"Age\",\"Hypertension\",\"Heart Disease\",\"Ever Married\",\"Work Type\",\"Residence Type\",\"Avg. Glucose Level\",\"BMI\", \"Smoking Status\",\"Stroke\"]]\n\ngender = pd.get_dummies(df2[\"Gender\"], drop_first=True)\nhypertension = pd.get_dummies(df2[\"Hypertension\"], drop_first=True, prefix=\"HT\")\nheartdisease = pd.get_dummies(df2[\"Heart Disease\"], drop_first=True, prefix=\"HD\")\nevermarried = pd.get_dummies(df2[\"Ever Married\"], drop_first=True, prefix=\"EM\")\nworktype = pd.get_dummies(df2[\"Work Type\"], drop_first=True)\nresidence = pd.get_dummies(df2[\"Residence Type\"],drop_first=True)\nsmoking = pd.get_dummies(df2[\"Smoking Status\"], drop_first=True)\n\ndf3 = pd.concat([df2,gender,hypertension,heartdisease,evermarried,worktype,residence,smoking], axis=1, join='outer', ignore_index=False)\nprint(df3.head(15))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping original feature set after the split using get_dummies. Relabeling all new feature sets.","metadata":{}},{"cell_type":"code","source":"df3.drop([\"Gender\",\"Hypertension\",\"Heart Disease\",\"Ever Married\",\"Work Type\", \"Residence Type\",\"Smoking Status\"], axis=1, inplace=True)\n\n#relabeling dataset with proper headers\ndf4 = df3.reindex(labels=[\"Age\",\"Male\",\"HT_Yes\",\"HD_Yes\",\"EM_Yes\",\"Never_worked\",\"Private\",\"Self-employed\",\"children\",\"BMI\",\"Urban\",\"Avg. Glucose Level\",\"formerly smoked\", \"never smoked\", \"smokes\",\"Stroke\"], axis=1)\nprint(df4.head(15))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature set and Label set. Verifying the number of rows present.","metadata":{}},{"cell_type":"code","source":"\n#feature set\nX = df4[[\"Age\",\"Male\",\"HT_Yes\",\"HD_Yes\",\"EM_Yes\",\"Never_worked\",\"Private\",\"Self-employed\",\"children\",\"BMI\",\"Avg. Glucose Level\",\"formerly smoked\", \"never smoked\", \"smokes\"]]\n\n#label set\ny = df4[\"Stroke\"]\n\nprint(X.count())\nprint(y.count())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train test split our dataset","metadata":{}},{"cell_type":"code","source":"#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ploting the label set after the train test split to visually see the train/test size. train size 70% and test size 30%.","metadata":{}},{"cell_type":"code","source":"y_train.value_counts(dropna = False).plot.bar(color = 'blue')\nplt.title('Stroke Feature Training Set')\nplt.xlabel('zero & one')\nplt.ylabel('count')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature scaling to standardize the independent features\nRescale feature with distribution value of 0 mean and variance equal to 1","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are fixing the unbalanced dataset. Here we can understand the total count.","metadata":{}},{"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oversampling dataset via SMOTE. This oversamples the dataset. We did not want to downsample since we would lose important parts of the dataset.","metadata":{}},{"cell_type":"code","source":"sm = SMOTE()\nX_train, y_train = sm.fit_resample(X_train, y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train==0)))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fitting our dataset to a Gaussian Naive Bayes model.","metadata":{}},{"cell_type":"code","source":"#Gaussian naive bayes model\nclf = GaussianNB()\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprint(\"Accuracy Score Gaussian = \", accuracy_score(y_test, y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the bagging classifier for ensemble method.","metadata":{}},{"cell_type":"code","source":"seed = 42\nkfold = model_selection.KFold(n_splits = 3,random_state = seed,shuffle=True)\n  \n# initialize the base classifier\nbase_cls = DecisionTreeClassifier()\n  \n# no. of base classifier\n# #Total Number of decision trees that will be used to train an ensemble\nnum_trees = 100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bagging classifier\nmodel = BaggingClassifier(base_estimator = base_cls,            # base estimator to fit on random subsets of the datraset\n                            n_estimators = num_trees,           # number of base estimators in the ensemble\n                            max_samples=50,                     # the number of features to draw from X to train each base estimator\n                            bootstrap = True,                   # Bootstrap = True means use bagging method\n                            random_state = seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtaining the accuracy score for bagging method via cross_val_score","metadata":{}},{"cell_type":"code","source":"results = model_selection.cross_val_score(model, X_train, y_train, cv = kfold)\n\nprint(\"Bagging Accuracy Score:\\t\", results.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}