{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fighting COVID-19 Infodemics \n## Project Design\n\nKnowledge gap and uncertainties between general public and specialists are important factors that drive pandemic anxiety. In this Kaggle competition task, we will examine the papers that discuss some of the controversial topics that trigger social rumours and pandenic anxiety in general public during the spread of the pandemic covid-19. \n\nEver since the beginning of 2020, there have been are lots of unverified statements about the virus and how to prevent it from spreading circulating in many social media. One way to validate these statements is to examine the relevant academic papers. However, most of the public are struggling to find out the right keywords to search the academic papers, and they are usually reluctant to go through the entire papers to search for the relevant information. In this task, we aim to develop a searching system to extract the sentences from abstracts in the CORD-91 dataset that are relevant to evaluate a specific topic.\n\nFor each statment, we first use the system to extract the topic relevant sentences from the paper abstracts, and then manually annotate those sentences based on whether they are for or against the statement.\n\nHere are some of the example statements about covid-19:\n\n1. Wearing mask is an effective way to stop the spreading of the virus\n\n2. The incubation period of covid-19 ranges from 2-14 days with a median of 5 days\n\n3. Asymptomatic patients can drive the spread of the virus\n\nFor each statement, we define at least two stance levels according to whether the exatracted sentences in the abstracts support or oppose the statement. Here are the steps to achieve this goal:\n\n### Step 1: Developing a topic searching system \n\nThe searching system first extracts abstracts contain a particular keyword (e.g. ‘mask’), then we use LDA to group the abstract topics. We identify a topic that is the most relevant to the statement and extract the abstracts that contain the target topic. The system then extract the sentences that contain the keyword from the relevant abstracts. The standard apporach of a searching system is to use TFIDF to rank documents, here we use LDA topic modeling on nouns, verbs, and adjectives of the abstracts. Users can decide the relevant information when knowing what are the most frequent keywords in each topic. For some queries, users want to identify articles for covid-19 only. Therefore, users are opt to add a title filer for different queries in the system.\n\nThe benefit of this approach is that when we want to know the relevant contents for a question, we don't know beforehand what are the keywords in an article that are more relevant to the question we ask, because users from general public are usually not farmiliar with academic papers. In our system, the topic keywords serve as the primes for the query in the next step of sentence extraction in the abstracts.\n\n### Step 2: Mannually annotating stance and relevance\n\nWe manually annotate the key sentences to identify the stance of the result sentences and whether these sentences are relevant to the question asked. Relevance annotation is an important part for evaluating a searching system.\n\n\n#### Annotation results\nTo understand the answer to the relevant question, we need to annotate the stance of the results, e.g., whether the abstract is for / against the statement. \n\nTo evaluate the searching system, we need to annotate the relevance of the retrieved sentences. Please refer to each section for the annotation guildlines\n\nRetrieved results and annotations can be found in this document \nhttps://docs.google.com/spreadsheets/d/1-eWEqji7mLXNF0Z9KH8RE5djcxK-97dUHzPWY7GEhI8/edit?usp=sharing\n\nThe document contains:\n\n1. Annotation of stance: See the column 'stance' in the sheets 'mask', 'incubation', 'asymtomatic', \n\n2. Annotation for relevance: See the column 'relevance' in the sheets 'mask', 'incubation', 'asymtomatic', \n\n3. Annotation for system evaluation: See the column 'relevance' in the sheets 'system_eval_varname'. \n\n\n### Results:\n\n#### Statement 1: Wearing mask is an effective way to stop the spreading of the virus\n\nAccording to the key sentences in the 40 paper abstracts that discuss the topic of public using masks, 12 papers support that using a mask during a pandemic is useful, 18 papers assume masks to be useful and examine the public’s willingness to comply the mask wearing rules, 1 paper shows no obvious evidence that using mask is protective or the protection is very limited.\n\nWe also see that the governments in some regions advocate using masks as a standard approach to reduce the risk of infection, papers in these regions focus on whether people comply the rules. Some government advocate that there is little evidence showing that mask is effective in controlling the pandemic, whereas nearly half of the academic papers from our search results consider wearing masks as a standard practice and these papers examine whether the public comply the recommended practice. Another half of the papers found evidence to support that wearing masks is effective in controlling the pandemic.\n\n\n#### Statement 2: The incubation period range from 2-14 days with a media of 5 days\n\nThere are 16 papers showing that the incubation period of covid-19 is 2-14 days with median of 5 days, 51 papers show different numbers. We can see that the majority of the papers show slightly different results than what the authorities reported. \n\n#### Statement 3: Asymptomatic patients drive the spread of the virus\n\nAccording to 53 papers relevant to this topic, 28 papers show that there is clear evidence that asymtomatic cases contribute to the spread of the virus, 25 papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus. Therefore, more research is needed for this topic. As general public, one need to take some precautions when contacting others who might be asymptomatic patients.\n\n\n### Evaluation of the system:\n\nThe evaluation of the system is currently being implemented. A test collection will be annotated in order to compare our search system with a baseline system.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nfrom collections import defaultdict\nimport string\nfrom gensim.models import CoherenceModel\nimport gensim\nfrom pprint import pprint\nimport spacy,en_core_web_sm\nfrom nltk.stem import PorterStemmer\nimport os\nimport json\nfrom gensim.models import Word2Vec\nimport nltk\nimport re\nimport collections\nfrom sklearn.metrics import cohen_kappa_score\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom collections import defaultdict\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Read the metadata in a dictionary format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MetaData:\n    def __init__(self):\n        \"\"\"Define varibles.\"\"\"\n        # path and data\n        self.path = '../input/CORD-19-research-challenge/'\n        self.meta_data = pd.read_csv(self.path + 'metadata.csv')\n\n    def data_dict(self):\n        \"\"\"Convert df to dictionary. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        meta_data_dict = mydict()\n\n        for cord_uid, abstract, title, sha in zip(self.meta_data['cord_uid'], self.meta_data['abstract'], self.meta_data['title'], self.meta_data['sha']):\n            meta_data_dict[cord_uid]['title'] = title\n            meta_data_dict[cord_uid]['abstract'] = abstract\n            meta_data_dict[cord_uid]['sha'] = sha\n\n        return meta_data_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract the documents containing keywords, preprocessing "},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExtractText:\n    \"\"\"Extract text according to keywords or phrases\"\"\"\n\n    def __init__(self, metaDict, keyword, variable):\n        \"\"\"Define varibles.\"\"\"\n        self.path = '../input/CORD-19-research-challenge/'\n        self.metadata = metaDict\n        self.keyword = keyword\n        self.variable = variable\n\n\n    def simple_preprocess(self):\n        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        cleaned = mydict()\n        for k, v in self.metadata.items():\n            sent = v[self.variable]\n            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n            cleaned[k]['processed_text'] = sent\n            cleaned[k]['sha'] = v['sha']\n            cleaned[k]['title'] = v['title']\n\n        return cleaned\n\n    def very_simple_preprocess(self):\n        \"\"\"Simple text process: lower case only. \"\"\"\n        mydict = lambda: defaultdict(mydict)\n        cleaned = mydict()\n        for k, v in self.metadata.items():\n            sent = v[self.variable]\n            sent = str(sent)\n            #sent = str(sent).lower()\n            cleaned[k]['processed_text'] = sent\n            cleaned[k]['sha'] = v['sha']\n            cleaned[k]['title'] = v['title']\n\n        return cleaned\n     \n\n    def extract_w_keywords(self):\n        \"\"\"Select content with keywords.\"\"\"\n        ps = PorterStemmer()\n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        textdict = self.simple_preprocess()\n        \n        for k, v in textdict.items():\n            if self.keyword in v['processed_text'].split():\n                #print(ps.stem(str(self.keyword)))\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n                selected[k]['title'] = v['title']\n        return selected\n\n    def extract_w_keywords_punc(self):\n        \"\"\"Select content with keywords, with punctuations in text\"\"\"\n        ps = PorterStemmer()\n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        textdict = self.very_simple_preprocess()\n        \n        for k, v in textdict.items():\n            #keywords are stemmed before matching\n            if ps.stem(str(self.keyword)) in ps.stem(str(v['processed_text'].split())):\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n                selected[k]['title'] = v['title']\n        return selected\n\n    def get_noun_verb(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for LDA\"\"\"\n        ps = PorterStemmer()\n      \n        #find nound trunks\n        nlp = en_core_web_sm.load()\n        all_extracted = {}\n        for k, v in text.items():\n            #v = v.replace('incubation period', 'incubation_period')\n            doc = nlp(v)\n            nouns = ' '.join(str(v) for v in doc if v.pos_ is 'NOUN').split()\n            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n            all_w = nouns + verbs + adj\n            all_extracted[k] = all_w\n      \n        return all_extracted\n\n    def get_noun_verb2(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for LDA\"\"\"\n        ps = PorterStemmer()\n      \n        #find nound trunks\n        nlp = en_core_web_sm.load()\n        all_extracted = {}\n        for k, v in text.items():\n            #v = v.replace('incubation period', 'incubation_period')\n            doc = nlp(v['processed_text'])\n            nouns = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'NOUN').split()\n            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n            all_w = nouns + verbs + adj\n            all_extracted[k] = all_w\n      \n        return all_extracted\n\n    def tokenization(self, text):\n        \"\"\"get noun trunks for the lda model,\n        change noun and verb part to decide what\n        you want to use as input for the next step\"\"\"\n        nlp = spacy.load(\"en_core_web_sm\")\n\n        all_extracted = {}\n        for k, v in text.items():\n            doc = nlp(v)\n            all_extracted[k] = [w.text for w in doc]\n      \n        return all_extracted\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using LDA to rank documents\nLDA is optimized by coherence score u_mass"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LDATopic:\n    def __init__(self, processed_text, topic_num, alpha, eta):\n        \"\"\"Define varibles.\"\"\"\n        self.path = '../input/CORD-19-research-challenge/'\n        self.text = processed_text\n        self.topic_num = topic_num\n        self.alpha = alpha\n        self.eta = eta\n\n    def get_lda_score_eval(self, dictionary, bow_corpus):\n        \"\"\"LDA model and coherence score.\"\"\"\n\n        lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=self.topic_num, id2word=dictionary, passes=10,  update_every=1, random_state = 300, alpha=self.alpha, eta=self.eta)\n        #pprint(lda_model.print_topics())\n\n        # get coherence score\n        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n        coherence = cm.get_coherence()\n        print('coherence score is {}'.format(coherence))\n\n        return lda_model, coherence\n\n    def get_score_dict(self, bow_corpus, lda_model_object):\n        \"\"\"\n        get lda score for each document\n        \"\"\"\n        all_lda_score = {}\n        for i in range(len(bow_corpus)):\n            lda_score ={}\n            for index, score in sorted(lda_model_object[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n                lda_score[index] = score\n                od = collections.OrderedDict(sorted(lda_score.items()))\n            all_lda_score[i] = od\n        return all_lda_score\n\n\n    def topic_modeling(self):\n        \"\"\"Get LDA topic modeling.\"\"\"\n        # generate dictionary\n        dictionary = gensim.corpora.Dictionary(self.text.values())\n        bow_corpus = [dictionary.doc2bow(doc) for doc in self.text.values()]\n        # modeling\n        model, coherence = self.get_lda_score_eval(dictionary, bow_corpus)\n\n        lda_score_all = self.get_score_dict(bow_corpus, model)\n\n        all_lda_score_df = pd.DataFrame.from_dict(lda_score_all)\n        all_lda_score_dfT = all_lda_score_df.T\n        all_lda_score_dfT = all_lda_score_dfT.fillna(0)\n\n        return model, coherence, all_lda_score_dfT\n\n    def get_ids_from_selected(self, text):\n        \"\"\"Get unique id from text \"\"\"\n        id_l = []\n        for k, v in text.items():\n            id_l.append(k)\n            \n        return id_l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we extract articles contain the most relevant topic\n\ndef selected_best_LDA(keyword, varname):\n        \"\"\"Select the best lda model with extracted text \"\"\"\n        # convert data to dictionary format\n        m = MetaData()\n        metaDict = m.data_dict()\n\n        #process text and extract text with keywords\n        et = ExtractText(metaDict, keyword, varname)\n        text1 = et.extract_w_keywords()\n\n\n        # extract nouns, verbs and adjetives\n        text = et.get_noun_verb2(text1)\n\n        # optimized alpha and beta\n        alpha = [0.1, 0.3, 0.5, 0.7, 0.9]\n        beta = [0.1, 0.3, 0.5, 0.7, 0.9]\n\n        mydict = lambda: defaultdict(mydict)\n        cohere_dict = mydict()\n        for a in alpha:\n            for b in beta:\n                lda = LDATopic(text, 20, a, b)\n                model, coherence, scores = lda.topic_modeling()\n                cohere_dict[coherence]['a'] = a\n                cohere_dict[coherence]['b'] = b\n    \n        # sort result dictionary to identify the best a, b\n        # select a,b with the largest coherence score \n        sort = sorted(cohere_dict.keys())[0] \n        a = cohere_dict[sort]['a']\n        b = cohere_dict[sort]['b']\n        \n        # run LDA with the optimized values\n        lda = LDATopic(text, 20, a, b)\n        model, coherence, scores_best = lda.topic_modeling()\n        pprint(model.print_topics())\n\n        # select merge ids with the LDA topic scores\n        id_l = lda.get_ids_from_selected(text)\n        scores_best['cord_uid'] = id_l\n\n        return scores_best\n\n\n\n\ndef select_text_from_LDA_results(keyword, varname, scores_best, topic_num):\n        # choose papers with the most relevant topic\n        # convert data to dictionary format\n        m = MetaData()\n        metaDict = m.data_dict()\n\n        # process text and extract text with keywords\n        et = ExtractText(metaDict, keyword, varname)\n        # extract text together with punctuation\n        text1 = et.extract_w_keywords_punc()\n        # need to decide which topic to choose after training\n        sel = scores_best[scores_best[topic_num] > 0] \n        \n        mydict = lambda: defaultdict(mydict)\n        selected = mydict()\n        for k, v in text1.items():\n            if k in sel.cord_uid.tolist():\n                selected[k]['title'] = v['title']\n                selected[k]['processed_text'] = v['processed_text']\n                selected[k]['sha'] = v['sha']\n    \n        return selected\n\ndef extract_relevant_sentences(cor_dict, search_keywords, filter_title=None):\n    \"\"\"Extract sentences contain keyword in relevant articles. \"\"\"\n    #here user can also choose whether they would like to only select title contain covid keywords\n\n    mydict = lambda: defaultdict(mydict)\n    sel_sentence = mydict()\n    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n    \n    for k, v in cor_dict.items():\n        keyword_sentence = []\n        sentences = v['processed_text'].split('.')\n        for sentence in sentences:\n            # for each sentence, check if keyword exist\n            # append sentences contain keyword to list\n            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n            if keyword_sum > 0:\n                keyword_sentence.append(sentence)         \n\n        # store results\n        if not keyword_sentence:\n            pass\n        elif filter_title is not None:\n            for f in filter_w:\n                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n                if (f in title) or (f in abstract):\n                    sel_sentence[k]['sentences'] = keyword_sentence\n                    sel_sentence[k]['sha'] = v['sha']\n                    sel_sentence[k]['title'] = v['title'] \n        else:\n            sel_sentence[k]['sentences'] = keyword_sentence\n            sel_sentence[k]['sha'] = v['sha']\n            sel_sentence[k]['title'] = v['title'] \n            \n    print('{} articles are relevant to the topic you choose'.format(len(sel_sentence)))\n\n    path = '../input/CORD-19-research-challenge/'\n    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n    #df.to_csv(path + 'search_results_{}.csv'.format(search_keywords))\n    #sel_sentence_df = pd.read_csv(path + 'search_results_{}.csv'.format(search_keywords))\n    return sel_sentence, df\n\ndef extract_relevant_sentences2(cor_dict, search_keywords, filter_title=None):\n    \"\"\"Extract sentences contain keyword in relevant articles for system evaluation. \"\"\"\n    #here user can also choose whether they would like to only select title contain covid keywords\n    #difference from the previous one is where we store the result\n\n    mydict = lambda: defaultdict(mydict)\n    sel_sentence = mydict()\n    filter_w = ['covid19','ncov','2019-ncov','covid-19','sars-cov','wuhan']\n    \n    for k, v in cor_dict.items():\n        keyword_sentence = []\n        sentences = v['processed_text'].split('.')\n        for sentence in sentences:\n            # for each sentence, check if keyword exist\n            # append sentences contain keyword to list\n            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n            if keyword_sum > 0:\n                keyword_sentence.append(sentence)         \n\n        # store results\n        if not keyword_sentence:\n            pass\n        \n        elif filter_title is not None:\n            for f in filter_w:\n                title = v['title'].lower().translate(str.maketrans('', '', string.punctuation))\n                abstract = v['processed_text'].lower().translate(str.maketrans('', '', string.punctuation))\n                if (f in title) or (f in abstract):\n                    sel_sentence[k]['sentences'] = keyword_sentence\n                    sel_sentence[k]['sha'] = v['sha']\n                    sel_sentence[k]['title'] = v['title'] \n        else:\n            sel_sentence[k]['sentences'] = keyword_sentence\n            sel_sentence[k]['sha'] = v['sha']\n            sel_sentence[k]['title'] = v['title'] \n    print('{} articles contain keyword {}'.format(len(sel_sentence),  search_keywords))\n\n    path = '../input/CORD-19-research-challenge/'\n    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n    df.to_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n    sel_sentence_df = pd.read_csv(path + 'eval_results_{}.csv'.format(search_keywords))\n    return sel_sentence, sel_sentence_df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 1: Is wearing mask an effective way to control the pandemic?"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#here we select the LDA model with the lowe\nscores_best_mask = selected_best_LDA('mask', 'abstract')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"scores_best_mask.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# topic number 10 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_mask = select_text_from_LDA_results('mask', 'abstract', scores_best_mask, 10)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_mask)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_mask, sel_sentence_df_mask = extract_relevant_sentences(cor_dict_mask, ['mask'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read extracted article\nsel_sentence_df_mask.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Annotation guidline for question 1\nWe extracted 33 papers that are supposed to discuss whether using masks is useful. We annotate  whether the key sentences suggest using mask can reduce the risk of infection.\n\n#### Stance Annotation \n* '1' support using a mask during a pandemic is useful \n* '2' assume masks as useful and examine the public’s willingness to comply the rules,\n* '0' no obvious evidence that shows using mask is protective or the protection is very little\n* '3' not relevant to the above stance\n\nresult from annotator 1\n\n#### Relevance annotation\n* '1' the result is relevent to the question  \n* '0' the result is not relevant to the question"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#here we annotated a sample of 40 abstracts\npath = '../input/annotation/'\nannotation_mask = pd.read_csv(path + 'wear_mask.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# view file\nannotation_mask.head(5)\nprint('there are {} articles relevant to the topic'.format(annotation_mask.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotation_mask['stance'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('there are {} papers support using a mask during a pandemic is useful, {} assume masks as useful and examine the public’s willingness to comply the rules,  {} papers show no obvious evidence that shows using mask is protective or the protection is very little'. format(str(annotation_mask['stance'].value_counts()[1]), str(annotation_mask['stance'].value_counts()[2]), annotation_mask['stance'].value_counts()[0]) )\n          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 2: How long in incubation period? In some region (e.g. China), there’s rumour circulating that the incubation period is longer than 14 days\n\n### Annotation guideline for question 2:\n\n#### stance annotation\nHere we want to identify papers that report a result aligns with the incubation period reported by the governments\nUK government advocated: 2-14 days, median 5\n* '1' same as government advocate \n* '0' different from what the government\n* Not relevant to the question \n\n#### relevance annotation\n* '1' the result is relevent to the question  \n* '0' the result is not relevant to the question\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_best_incu = selected_best_LDA('incubation', 'abstract')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# topic number 0 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_incu = select_text_from_LDA_results('incubation', 'abstract', scores_best_incu, 0)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_incu)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_incu, sel_sentence_df_incu = extract_relevant_sentences(cor_dict_incu, ['incubation','day'], 'title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read extracted article\nsel_sentence_df_incu.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical analysis on the incubation period"},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we need to add the stats analysis \npath = '../input/annotation/'\nannotation_incubation = pd.read_csv(path + 'incubation.csv')\nprint('there are {} articles relevant to the topic'.format(annotation_incubation.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incubation = annotation_incubation['stance'].value_counts()\nprint('there are {} paper shows the incubation period is 2-14 days with mean 5 days, {} papers shows a different number'. format(incubation[1], incubation[0])\n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incubation = annotation_incubation['relevance'].value_counts()\nprint('there are {} papers relevant to the topic, {} papers not relevant to the topic'. format(incubation[1], incubation[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3: Are asymptomatic patients infectious?\n\n### Annotation guideline for question 3:\nHere we want to identify whether asymtomatic cases contribute to the spread of the virus\n\n#### stance annotation\n* ‘1’  there is clear evidence show that asymtomatic cases contribute to the spread of the virus\n* ‘0’  it is unlikely that asymtomatic cases contribute to the spread of the virus\n* '3' Not relevant to the question\n\n#### relevance annotation\n* '1' the result is relevent to the question  \n* '0' the result is not relevant to the question"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_best_asym = selected_best_LDA('asymptomatic', 'abstract')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# topic number 8 is most relevant to public wearing mask\n# which topic do you think is most relevant to your search\ncor_dict_asym = select_text_from_LDA_results('asymptomatic', 'abstract', scores_best_asym, 8)\nprint (\"There are {} abstracts selected\". format(len(cor_dict_asym)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract relevant sentences  #search keywords can be a list\nsel_sentence_asym, sel_sentence_df_asym = extract_relevant_sentences(cor_dict_asym, ['asymptomatic','transmission'], 'title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_sentence_df_asym.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Asymptomatic Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we need to add the stats analysis \nannotation_asymptomatic = pd.read_csv(path + 'asymtomatic.csv')\nprint('there are {} articles relevant to the topic'.format(annotation_asymptomatic.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asymptomatic = annotation_asymptomatic['stance'].value_counts()\nprint('{} papers show that there is clear evidence show that asymtomatic cases contribute to the spread of the virus, {} papers show that it is unlikely that asymtomatic cases contribute to the spread of the virus'.format(asymptomatic[1], asymptomatic[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}