{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing liberaries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"importing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/zomato-bangalore-restaurants/zomato.csv\")\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thisa data needs cleaning and the testing dataset will be isolated from this!"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Data Cleaning\ndataset = dataset.drop(['url', 'address', 'phone', 'name', 'menu_item', 'dish_liked'], axis = 1)\ndataset = dataset.dropna(axis = 0, how ='any')\ndataset = dataset.rename(columns = {'approx_cost(for two people)' : 'cost', 'listed_in(type)' : 'type', 'listed_in(city)' : 'city'})\n\ndataset['cost'] = dataset['cost'].astype(str)\ndataset['cost'] = dataset['cost'].apply(lambda x: x.replace(',','.'))\ndataset['cost'] = dataset['cost'].astype(float)\n\ndataset_test = dataset.loc[dataset.rate == 'NEW']\ndataset = dataset.loc[dataset.rate != 'NEW'].reset_index(drop = True)\ndataset = dataset.loc[dataset.rate != '-'].reset_index(drop = True)\nremove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\ndataset.rate = dataset.rate.apply(remove_slash).str.strip().astype('float')\ndataset['rate'] = round(dataset['rate'])\n\ndataset['online_order'].replace('Yes', 1,inplace = True)\ndataset['online_order'].replace('No', 0,inplace = True)\ndataset_test['online_order'].replace('Yes', 1,inplace = True)\ndataset_test['online_order'].replace('No', 0,inplace = True)\ndataset['book_table'].replace('Yes', 1,inplace = True)\ndataset['book_table'].replace('No', 0,inplace = True)\ndataset_test['book_table'].replace('Yes', 1,inplace = True)\ndataset_test['book_table'].replace('No', 0,inplace = True)\n\ndataset.info()\ndataset_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is understood thoroughly and cleaned in such a way to retain all the\nmeaningful things while eliminating as much as possible.\n\nThe column dropped completely are as follows ['url', 'address', 'phone',\n'name', 'menu_item', 'dish_liked'] and columns whose name has been changed are\n{'approx_cost(for two people)' : 'cost', 'listed_in(type)' : 'type',\n'listed_in(city)' : 'city'}\n\nThe date in dataset[‘cost’] column is converted into integer. The\nORDER_ONLINE and BOOK_TABLE are also converted to numerical values rather\nthan a simple Y or N. the coatagory data is also converted to numerical values\nin cloumns [‘type’, ‘cuisine’, ‘location’, ‘city’, ‘rest_type’] data cleaning is also\ndone where the dataset is null is removed.\n\nThe dataset is divided into test and train set based on the dataset[‘rate’], if\nrate is NEW it is taken in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### NLP for cleaning and rating predictions\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    s = s.replace(\"rated rated n\",\"\")\n    return s\n\ndataset['reviews_list'] = [cleaning(s) for s in dataset['reviews_list']]\ndataset_test['reviews_list'] = [cleaning(s) for s in dataset_test['reviews_list']]\ndataset = dataset.loc[dataset.reviews_list != ' '].reset_index(drop = True)\ndataset_test = dataset_test.loc[dataset_test.reviews_list != ' '].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above function cleans the text is the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Data Encoding\ndef catagory_encoding(data):\n    for i in range (0, len(dataset[data].unique())):\n        dataset[data].replace(dataset[data].unique()[i], i, inplace = True)\n    for i in range (0, len(dataset_test[data].unique())):\n        dataset_test[data].replace(dataset_test[data].unique()[i], i, inplace = True)\n    return\n\ncatagory_encoding('type')\ncatagory_encoding('cuisines')\ncatagory_encoding('location')\ncatagory_encoding('city')\ncatagory_encoding('rest_type')\n\ndataset_test = dataset_test.drop(['rate'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above function converts catagorial data into numeric and stores in same column"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Data Corellation\ncorr = dataset.corr(method = 'kendall')\nsns.heatmap(corr, annot = True, annot_kws = {\"size\" : 7})\nprint(\"Most corellation is found between votes and rates\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Data Visualisation\ndef plotting_sns(data, title):\n    sns.countplot(data)\n    sns.countplot(data).set_xticklabels(sns.countplot(data).get_xticklabels(), rotation = 90)\n    plt.title(title)\n    fig = plt.gcf()\n    fig.set_size_inches(15, 15)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above function is used for plotting the heatmaps"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_sns(dataset['city'], 'Location')\nprint(\"Most famous city is Bannerghatta Road\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_sns(dataset['type'], 'Type of Service')\nprint(\"Most famous type of service is Delivery and Dine-Out\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_sns(dataset['cost'], 'Cost')\nprint(\"Most 2 person cost is between 200 and 400\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_sns(dataset['location'], 'No of resturant in a location')\nprint(\"Most famous location is bhanashankari\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_sns(dataset['rest_type'], 'Type of resturants')\nprint(\"Most famous type of resturant is quick bytes and causal dining\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Function for emotional analysis per gender\ndataset_temp = dataset.copy()\n\ndef emotional_analysis(text):\n    tokenize_words = word_tokenize(text)\n    clean_words=[]\n    for i in tokenize_words:\n        if i not in stopwords.words(\"english\"):\n            clean_words.append(i)\n    emotions = []\n    with open(\"../input/emotion/emotion.txt\",\"r\") as file:\n        for i in file:\n            temp = i.replace(\"\\n\",\"\")\n            temp = temp.strip()\n            temp = temp.replace(\" \",\"\")\n            temp = temp.replace(\",\",\"\")\n            temp = temp.replace(\"'\",\"\")\n            word, emotion = temp.split(\":\")\n            if word in clean_words:\n                emotions.append(emotion)\n    return emotions\n        \ndef emotional_plotting(i):\n    temp = ''\n    for j in text[i - 1]:\n        temp = temp + j\n    emotion = emotional_analysis(temp)\n    title = 'Emotion in review rated - ' + str(i)\n    plotting_sns(emotion, title)\n    return\n\ntext = [[],[],[],[],[]]\nfor rating in range (1, 6):\n    for i in dataset['reviews_list'].loc[dataset['rate'] == rating]:\n        text[rating - 1].append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above functions provide the type and quantity of each emotions used in revioews of resturants sorted by their rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"emotional_plotting(2)\nprint(\"Most used emotion in 2 rated places is :-> happy and attracted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emotional_plotting(3)\nprint(\"Most used emotion in 3 rated places is :-> happy and angry\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emotional_plotting(4)\nprint(\"Most used emotion in 4 rated places is :-> happy and attracted\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"emotional_plotting(5)\nprint(\"Most used emotion in 5 rated places is :-> happy and sad\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The amount of emotion used is increasing as we move\nfrom lower rated reviews to higher rated reviews.. but\nthe type of emotion remains same.\n\nThis indicated that the restaurant which as established\nfor a long time do not get very different reviews than\nnew restaurant. Only time is a major factor which seems\nto invade the review system.\n\nExcept some bad review restaurant. The type of emotion\nin any rated restaurant is same. Only the amount of\nemotional word used is different. "},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Feature Engineering\ncount_vectorizer = CountVectorizer(max_features = 1000, stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(dataset_test['reviews_list']).toarray()\ndataset_sparce_matrix = pd.DataFrame(data = sparce_matrix)\ndataset_test = dataset_test.drop(['reviews_list'], axis = 1)\ndataset_test = pd.concat([dataset_test, dataset_sparce_matrix], axis=1)\n\nsparce_matrix = count_vectorizer.fit_transform(dataset['reviews_list']).toarray()\ndataset_sparce_matrix = pd.DataFrame(data = sparce_matrix)\ndataset = dataset.drop(['reviews_list'], axis = 1)\ndataset = pd.concat([dataset, dataset_sparce_matrix], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here the review gets converted into a matrix of numbers to be feeded into our super learner model"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### get the dataset\ndef get_dataset(dataset_y, dataset_x):\n    y = dataset_y.values\n    X = dataset_x\n    return X, y\n\n################################################# get a stacking ensemble of models\ndef get_stacking():\n    ################################################# define the base models\n    level0 = list()\n    level0.append(('Logistic Regression', LogisticRegression()))\n    level0.append(('K Nearest Neighbour', KNeighborsClassifier()))\n    level0.append(('Decision Tree Classifier', DecisionTreeClassifier()))\n    level0.append(('Support Vector Classifier', SVC()))\n    level0.append(('Gaussian Navy Bayse', GaussianNB()))\n    level0.append(('ADA boost', AdaBoostClassifier()))\n    level0.append(('Bagging Classifier', BaggingClassifier(n_estimators = 10)))\n    level0.append(('Random Forest Classifier', RandomForestClassifier(n_estimators = 10)))\n    level0.append(('Extra Trees Classifier', ExtraTreesClassifier(n_estimators = 10)))\n    ################################################# define meta learner model\n    level1 = LogisticRegression()\n    ################################################# define the stacking ensemble\n    model = StackingClassifier(estimators = level0, final_estimator = level1, cv = 5)\n    return model\n\n################################################# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['Logistic Regression'] = LogisticRegression()\n    models['K Nearest Neighbour'] = KNeighborsClassifier()\n    models['Decision Tree Classifier'] = DecisionTreeClassifier()\n    models['Support Vector Classifier'] = SVC()\n    models['Gaussian Navy Bayse'] = GaussianNB()\n    models['ADA boost'] = AdaBoostClassifier()\n    models['Bagging Classifier'] = BaggingClassifier()\n    models['Random Forest Classifier'] = RandomForestClassifier()\n    models['Extra Trees Classifier'] = ExtraTreesClassifier()\n    models['Stacking'] = get_stacking()\n    return models\n\n################################################# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n    scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################# get database and plot the result of all models & predicting on new dataset\nX, y = get_dataset(dataset_train_refined['rate'], pd.get_dummies(dataset_train_refined.drop('rate', 1)))\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\nplt.boxplot(results, labels = names, showmeans = True)\nplt.xticks(rotation = 90)\nplt.show()\n\nmodel_test = get_stacking()\nmodel_test.fit(X, y)\nyhat = model_test.predict(dataset_test)\ndataset_test['rate'] = yhat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this above is the structure of our super learner model used for prediction"},{"metadata":{},"cell_type":"markdown","source":"The K-folds used are 10 due to system limitations\nand the word sample size used is 999 from the library of\nall words used due to system limitations.\nThe score are represented as (name, mean(scores),\nstd(scores)). The percentages and standard deviations \nare written to better understand what each model is\ndoing.\n\nThe logistic regression is chosen as level 1 in stacking\nfor defining the best algorithm approach to the train set\nprovided.\n\nThe stacking super learner provided the best score\nof 94.4% accuracy with just 0.7% standard deviations.\nWhich seems to be a case of overfitting the data. But in\nthis problem is correct as we are predicting the rating of\nthe restaurant which are not rated. Generating rating as\nclose as the trained model is good and not overfitting.\nIdeally the K-Folds should be near 50 to 100 and the\nWords used in training should be 15000 to 20000 but\ndue to some hardware limitations the Script with K-Fold\n10 and word of 999 took almost 4 Hours to run on my\nsystem.\n\nThis super learner will now be used to predict the\ngender of those in the test dataset of Zomato restaurant\nrate review.\n\nThe prediction algorithm also shows that time is a\nmajor factor in rate review system. Since the deviation\nis close to null and 90% accuracy which is very good way\nto understand the two major aspect of a feature which \nare QUALITY and QUANTITY. Here standard deviation of\nemotions at any rate is minimal which contribute to\nQUALITY of the prediction, while 90% accuracy is\ndirectly related to QUANTITY of emotions used in\ndifferent rated restaurant.\n\nThe HIGH QUALITY and GOOD ACCURACY is an\nindication that review of any rated restaurant is same,\nthe more the quantity the more the rate of that\nrestaurant, directly relating rating system to time of\nestablishment of the restaurant."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}