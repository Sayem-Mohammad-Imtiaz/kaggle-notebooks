{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Data Source: https://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression**\n# Data's demographic:\n• Sex: male(2) or female(0) (Nominal)\n\n• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous) \n\nBehavioral\n\n• Current Smoker: whether or not the patient is a current smoker (Nominal)\n\n• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n\nMedical( history)\n\n• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n\n• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n\n• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n\n• Diabetes: whether or not the patient had diabetes (Nominal)\n\nMedical(current)\n\n• Tot Chol: total cholesterol level (Continuous)\n\n• Sys BP: systolic blood pressure (Continuous)\n\n• Dia BP: diastolic blood pressure (Continuous)\n\n• BMI: Body Mass Index (Continuous)\n\n• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n• Glucose: glucose level (Continuous)\nPredict variable (desired target)\n\n• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).\n#The dataset provides the patients’ information\nheartDiseaseData = pd.read_csv(\"/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\")\nheartDiseaseData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see my labels are ordered -male = 1, female = 0-. Then i need to delete that useless features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#heartDiseaseData.drop([\"education\"], axis = 1, inplace = True) # Delete useless feature.\nheartDiseaseData.dropna(how=\"any\", inplace = True)  # Delete useless raw\n\nchd = heartDiseaseData.TenYearCHD.values \nfeaturees = heartDiseaseData.drop([\"TenYearCHD\"], axis = 1) # features before normalization\nfeaturees","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **NORMALIZATION**\n\nFormula = (x -min(x))/(max(x)-min(x)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = (featurees - np.min(featurees))/(np.max(featurees) - np.min(featurees)).values # features after normalization\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TRAIN-TEST SPLIT**\n\nTrain Test Split data==> 80% of data set for Train, 20% of data set for Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, chd_train, chd_test = train_test_split(features, chd ,test_size = 0.2 , random_state = 42)\n# test_size ==> 80% of data set for Train, 20% of data set for Test\n\n\nfeatures_train = features_train.T\nfeatures_test = features_test.T\nchd_train = chd_train.T\nchd_test = chd_test.T\n\nprint(\"Changed of Features and Values place.\")\n\nprint(\"features_train: \", features_train.shape)\nprint(\"features_test \", features_test.shape)\nprint(\"chd_train: \", chd_train.shape)\nprint(\"chd_test: \", chd_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PARAMETER INITALIZE AND SIGMOID FUNCTION**\n\nTime to start defining functions.First of all I need to initialize my weights and bias, then I will need a sigmoid function.\n\nSigmoid Function : f(x) = 1 / ( 1 + (e ^ -x)\nInitialize weight = 0.01 for each data\nInitialize bias = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    \n    weights = np.full((dimension,1), 0.01) \n    bias = 0.0 \n    return weights,bias\n    \ndef sigmoid(z):\n    chd_head = 1/(1+np.exp(-z))\n    return chd_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **FORWARD AND BACKWARD PROPAGATION FUNCTION**\nz = bias + px1w1 + px2w2 + ... + pxn*wn\nloss function = -(1 - y) log(1- y_head) - y log(y_head)\ncost function = sum(loss value) / train dataset sample count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(weights, bias , features_train, chd_train):\n    #forward propagation\n    \n    z = np.dot(weights.T,features_train) + bias\n    chd_head = sigmoid(z)\n    loss = -chd_train*np.log(chd_head) - (1- chd_train)*np.log(1-chd_head)\n    cost = (np.sum(loss))/features_train.shape[1]\n    \n    #backward propagation\n    derivative_weights = (np.dot(features_train,((chd_head-chd_train).T)))/features_train.shape[1] \n    derivative_bias = np.sum(chd_head-chd_train)/features_train.shape[1] \n    gradients = {\"derivative_weights\" : derivative_weights, \"derivative_bias\" : derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **UPDATE**\nUpdate weights and bias with backward-forward propagation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(weights, bias, features_train, chd_train, learning_rate, number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterations):\n        \n        cost, gradients = forward_backward_propagation(weights, bias, features_train, chd_train)\n        cost_list.append(cost)\n        \n        weights = weights - learning_rate* gradients[\"derivative_weights\"]\n        bias = bias - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0: # her 10 adımda bir depolar\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f \" %(i,cost))\n            \n        \n    parameters =  {\"weights\" : weights, \"bias\" : bias}\n    plt.plot(index,cost_list2)\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number Of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PREDICT**\nPredict function for testing purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(weights, bias, features_test):\n    \n    z = sigmoid(np.dot(weights.T,features_test)+bias)\n    chd_prediction = np.zeros((1,features_test.shape[1]))\n    \n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5 :\n            chd_prediction[0,i] = 0\n        else:\n            chd_prediction[0,i] = 1\n            \n    return chd_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LOGISTIC REGRESSION**\nMain part.\nPut it all together."},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(features_train, chd_train, features_test, chd_test, learning_rate, number_of_iterations):\n    dimension = features_train.shape[0] # that is 14(features)\n    weights, bias = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(weights, bias, features_train, chd_train, learning_rate, number_of_iterations) \n    \n    chd_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], features_test)\n    \n    print(\"Test occuracy: {}% \".format(100-np.mean(np.abs(chd_prediction_test - chd_test))*100))\n    \nlogistic_regression(features_train, chd_train, features_test, chd_test, learning_rate = 5, number_of_iterations = 300) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Logistic Regression with Sklearn Library**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(features_train.T,chd_train.T)\nprint(\"test accuracy {}\".format(lr.score(features_test.T,chd_test.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}