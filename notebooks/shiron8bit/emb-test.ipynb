{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport os.path\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_limit = 20000\n\nid2names = {}\nname2id = {}\nwith open('../input/celeba-cropped/identity_CelebA.txt') as f:\n    line = f.readline()\n    cnt = 0\n    while(line):\n        name, id = line.split()\n        name2id[name] = id\n        if id not in id2names:\n            id2names[id] = [name]\n        else:\n            id2names[id].append(name)\n        cnt+=1\n        if cnt>=img_limit:\n            break\n        line = f.readline()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# person = list(id2names['2937'])   # 2880\n# plt.figure(figsize=(20, 5*len(person)))    \n# for i,fname in enumerate(person):\n#     im = Image.open('../input/celeba-dataset/img_align_celeba/img_align_celeba/'+fname)\n#     plt.subplot(len(person),1,i+1)\n#     plt.imshow(im)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_embs = np.zeros((len(id2names.keys()),512), dtype=float)\nidx = 0\nfor k in id2names.keys():\n    count = 0\n    mean_emb = np.zeros((1,512))\n    for fname in id2names[k]:\n        if os.path.exists('../input/celeba-cropped/emb/emb/'+fname.replace('.jpg','.npy')):\n            mean_emb += np.load('../input/celeba-cropped/emb/emb/'+fname.replace('.jpg','.npy'))\n            count+=1\n    mean_embs[idx,:] = mean_emb / count\n    idx+=1\nprint(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_embs[1,:]\n#list(id2names.keys())[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"person = list(id2names['2937'])   # 2880\n\nnum_emb = 0\nmean_emb = np.zeros((1,512))\nfor fname in person:\n    if os.path.exists('../input/celeba-cropped/emb/emb/'+fname.replace('.jpg','.npy')):\n        emb = np.load('../input/celeba-cropped/emb/emb/'+fname.replace('.jpg','.npy'))\n        num_emb+=1\n        mean_emb+=emb\nprint(num_emb)\nprint(mean_emb/num_emb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_emb0 = []\n# for fname in person:\n#     if os.path.exists('../input/celeba-cropped/mean_emb/mean_emb/'+fname.replace('.jpg','.npy')):\n#         mean_emb0 = np.load('../input/celeba-cropped/mean_emb/mean_emb/'+fname.replace('.jpg','.npy'))\n#         break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nimport torch.utils.data\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport matplotlib.image as mpimg\nimport torchvision\nimport torchvision.datasets as dset\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open('../input/celeba-cropped/bboxes_celeba.json') as file:  \n    bboxes_json = json.load(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 666\nkey = list(bboxes_json.keys())[idx]\nprint(key)\nbox = [round(x) for x in bboxes_json[key]]\nplt.imshow(Image.open('../input/celeba-dataset/img_align_celeba/img_align_celeba/'+key).crop(box))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = 128\nfolder_w_imgs = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n\nclass faces_with_mean_emb(Dataset):\n    def __init__(self, imgs, mean_embs, bboxes_json, ident_list, name2id, transform=None):\n        self.imgs = imgs\n        self.transform = transform\n        self.bboxes = bboxes_json\n        self.cropped = self.get_cropped()\n        self.mean_embs = mean_embs\n        self.ident = ident_list\n        self.name2id = name2id\n            \n    def __len__(self):\n        return len(self.imgs)\n    \n    def get_cropped(self):\n        \n        required_transforms = torchvision.transforms.Compose([\n                torchvision.transforms.Resize(SIZE),\n                torchvision.transforms.CenterCrop(SIZE),\n        ])\n        \n        cropped_list = []\n        for imname in self.imgs:\n            im = Image.open(folder_w_imgs + imname)\n            if imname in self.bboxes.keys():\n                box = [round(x) for x in self.bboxes[imname]]\n                im = im.crop(box)\n            im_t = required_transforms(im)\n            cropped_list.append(im_t)\n                \n        return cropped_list\n    \n    def __getitem__(self, idx):\n        im = self.cropped[idx]\n        if self.transform:\n            im = self.transform(im)\n        emb = self.mean_embs[ident_list.index(name2id[imgs[idx]])]\n        return np.asarray(im), torch.from_numpy(emb).float()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = list(name2id.keys())\nident_list = list(id2names.keys())\n\nbatch_size = 32\n\ntransform = transforms.Compose([#transforms.RandomHorizontalFlip(p=0.1),\n                                #transforms.RandomApply(random_transforms, p=0),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_data = faces_with_mean_emb(imgs, mean_embs, bboxes_json, ident_list, name2id, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=False,\n                                           batch_size=batch_size, num_workers=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_batch = next(iter(train_loader))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0], padding=2, normalize=True).cpu(),(1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PixelwiseNorm(nn.Module):\n    def __init__(self):\n        super(PixelwiseNorm, self).__init__()\n\n    def forward(self, x, alpha=1e-8):\n        \"\"\"\n        forward pass of the module\n        :param x: input activations volume\n        :param alpha: small number for numerical stability\n        :return: y => pixel normalized activations\n        \"\"\"\n        y = x.pow(2.).mean(dim=1, keepdim=True).add(alpha).sqrt()  # [N1HW]\n        y = x / y  # normalize the input x volume\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.utils import spectral_norm\n\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim=50, channels=3):\n        super(Encoder, self).__init__()\n        self.channels = channels\n        self.latent_dim = latent_dim\n    \n        def convlayer_enc(n_input, n_output, k_size=4, stride=2, padding=1, bn=False, add=False):\n            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]\n            if bn:\n                block.append(nn.BatchNorm2d(n_output))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            if add:\n                block.append(nn.Conv2d(n_output, n_output, kernel_size=3, stride=1, padding=1, bias=False)) # add depth\n                block.append(nn.LeakyReLU(0.2, inplace=True))\n            return block\n    \n        self.encoder = nn.Sequential(\n                *convlayer_enc(self.channels, 32, 4, 2, 1),                # (32,64,64)\n                *convlayer_enc(32, 64, 4, 2, 1, add=True),                             #(64, 32, 32)\n                *convlayer_enc(64, 128, 4, 2, 1, add=True),                         # (128, 16, 16)\n                *convlayer_enc(128, 256, 4, 2, 1, bn=True, add=True),               # (256, 8, 8)\n                *convlayer_enc(256, 512, 4, 2, 1, bn=True),               # (512, 4, 4)\n                #nn.Conv2d(512, self.latent_dim, 4, 1, 1, bias=False),   # (latent_dim, 4, 4)\n                #nn.LeakyReLU(0.2, inplace=True)\n            )\n        self.linear_layer = nn.Linear(512*4*4,self.latent_dim)\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.linear_layer(x.view(x.size(0),-1))\n        return x\n    \nclass Decoder(nn.Module):\n    def __init__(self, nz, nchannels, nfeats):\n        super(Decoder, self).__init__()\n\n        # input is Z, going into a convolution\n        #self.conv1 = spectral_norm(nn.ConvTranspose2d(nz, nfeats * 8, 4, 1, 0, bias=False))\n        self.conv1 = nn.ConvTranspose2d(nz, nfeats * 8, 4, 1, 0, bias=False)\n        #self.bn1 = nn.BatchNorm2d(nfeats * 8)\n        # state size. (nfeats*8) x 4 x 4\n        \n        #self.conv2 = spectral_norm(nn.ConvTranspose2d(nfeats * 8, nfeats * 8, 4, 2, 1, bias=False))\n        self.conv2 = nn.ConvTranspose2d(nfeats * 8, nfeats * 8, 4, 2, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(nfeats * 8)\n        # state size. (nfeats*8) x 8 x 8\n        \n        #self.conv3 = spectral_norm(nn.ConvTranspose2d(nfeats * 8, nfeats * 4, 4, 2, 1, bias=False))\n        self.conv3 = nn.ConvTranspose2d(nfeats * 8, nfeats * 4, 4, 2, 1, bias=False)\n        #self.bn3 = nn.BatchNorm2d(nfeats * 4)\n        # state size. (nfeats*4) x 16 x 16\n        \n        #self.conv4 = spectral_norm(nn.ConvTranspose2d(nfeats * 4, nfeats * 2, 4, 2, 1, bias=False))\n        self.conv4 = nn.ConvTranspose2d(nfeats * 4, nfeats * 2, 4, 2, 1, bias=False)\n        #self.bn4 = nn.BatchNorm2d(nfeats * 2)\n        # state size. (nfeats * 2) x 32 x 32\n        \n        #self.conv5 = spectral_norm(nn.ConvTranspose2d(nfeats * 2, nfeats, 4, 2, 1, bias=False))\n        self.conv5 = nn.ConvTranspose2d(nfeats * 2, nfeats, 4, 2, 1, bias=False)\n        #self.bn5 = nn.BatchNorm2d(nfeats)\n        # state size. (nfeats) x 64 x 64\n        \n        self.conv6 = nn.ConvTranspose2d(nfeats, nfeats, 4, 2, 1, bias=False)\n        # nf x 128 x 128\n        self.conv7 = spectral_norm(nn.ConvTranspose2d(nfeats, nchannels, 3, 1, 1, bias=False))\n        # state size. (nchannels) x 64 x 64\n        self.pixnorm = PixelwiseNorm()\n        self.add_depth3 = nn.Conv2d(nfeats*4, nfeats*4, kernel_size=3, stride=1, padding=1, bias=False)\n        self.add_depth4 = nn.Conv2d(nfeats*2, nfeats*2, kernel_size=3, stride=1, padding=1, bias=False)\n        self.add_depth5 = nn.Conv2d(nfeats, nfeats, kernel_size=3, stride=1, padding=1, bias=False)\n    \n    \n        \n    def forward(self, x):\n#         x = F.leaky_relu(self.bn1(self.conv1(x)))\n#         x = F.leaky_relu(self.bn2(self.conv2(x)))\n#         x = F.leaky_relu(self.bn3(self.conv3(x)))\n#         x = F.leaky_relu(self.bn4(self.conv4(x)))\n#         x = F.leaky_relu(self.bn5(self.conv5(x)))\n        x = F.leaky_relu(self.conv1(x))\n        x = F.leaky_relu(self.conv2(x))\n        #x = self.pixnorm(x)\n        x = F.leaky_relu(self.conv3(x))\n        x = F.leaky_relu(self.add_depth3(x))\n        #x = self.pixnorm(x)\n        x = F.leaky_relu(self.conv4(x))\n        x = F.leaky_relu(self.add_depth4(x))\n        #x = self.pixnorm(x)\n        x = F.leaky_relu(self.conv5(x))\n        x = F.leaky_relu(self.add_depth5(x))\n        \n        x = F.leaky_relu(self.conv6(x))\n        x = self.pixnorm(x)\n        x = torch.tanh(self.conv7(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_dim = 20\nchannels= 3\nemb_size = 512\nbeta1 = 0.5\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nenc = Encoder(latent_dim, channels).to(device)\ndec = Decoder(latent_dim+emb_size, channels, 32).to(device)\ncriterion = nn.MSELoss()\n#optim_enc = optim.Adam(enc.parameters(), lr=0.0002, betas=(beta1, 0.999))\noptim_enc = optim.Adam(enc.parameters())\n#optim_dec = optim.Adam(dec.parameters(), lr=0.0002, betas=(beta1, 0.999))\noptim_dec = optim.Adam(dec.parameters())\n\nepochs = 300\nsteps = 0\nfor epoch in range(epochs):\n    for ii, (x,emb) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        enc.zero_grad()\n        dec.zero_grad()\n\n        x = x.to(device)\n        emb = emb.to(device)\n        z = enc(x)\n        z = torch.cat((z,emb), dim=1)\n        z = z.unsqueeze(2).unsqueeze(3)\n        x_hat = dec(z)\n\n        loss = criterion(x_hat, x)\n        loss.backward()\n        optim_enc.step()\n        optim_dec.step()\n        \n        steps+=1\n        \n        if steps%1000==0: print('epoch: %d, loss: %.5f' % (epoch, loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(enc.state_dict(),'enc1.pth')\ntorch.save(dec.state_dict(),'dec1.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim_enc = optim.Adam(enc.parameters(), lr=0.0005, betas=(beta1, 0.999))\noptim_dec = optim.Adam(dec.parameters(), lr=0.0005, betas=(beta1, 0.999))\nepochs = 300\nsteps = 0\nfor epoch in range(epochs):\n    for ii, (x,emb) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        enc.zero_grad()\n        dec.zero_grad()\n\n        x = x.to(device)\n        emb = emb.to(device)\n        z = enc(x)\n        z = torch.cat((z,emb), dim=1)\n        z = z.unsqueeze(2).unsqueeze(3)\n        x_hat = dec(z)\n\n        loss = criterion(x_hat, x)\n        loss.backward()\n        optim_enc.step()\n        optim_dec.step()\n        \n        steps+=1\n        \n        if steps%1000==0: print('epoch: %d, loss: %.5f' % (epoch, loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(enc.state_dict(),'enc2.pth')\ntorch.save(dec.state_dict(),'dec2.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim_enc = optim.Adam(enc.parameters(), lr=0.0001, betas=(beta1, 0.999))\noptim_dec = optim.Adam(dec.parameters(), lr=0.0001, betas=(beta1, 0.999))\nepochs = 300\nsteps = 0\nfor epoch in range(epochs):\n    for ii, (x,emb) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        enc.zero_grad()\n        dec.zero_grad()\n\n        x = x.to(device)\n        emb = emb.to(device)\n        z = enc(x)\n        z = torch.cat((z,emb), dim=1)\n        z = z.unsqueeze(2).unsqueeze(3)\n        x_hat = dec(z)\n\n        loss = criterion(x_hat, x)\n        loss.backward()\n        optim_enc.step()\n        optim_dec.step()\n        \n        steps+=1\n        \n        if steps%1000==0: print('epoch: %d, loss: %.5f' % (epoch, loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(enc.state_dict(),'enc3.pth')\ntorch.save(dec.state_dict(),'dec3.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"it = iter(train_loader)\nreal_batch = next(it)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = real_batch[0][10].unsqueeze(0)\nx2 = real_batch[0][11].unsqueeze(0)\nemb1 = real_batch[1][11].unsqueeze(0)\nemb2 = real_batch[1][10].unsqueeze(0)\n\n\nx1 = x1.to(device)\nemb1 = emb1.to(device)\nz = enc(x1)\n\nz = torch.cat((z,emb1), dim=1)\nz = z.unsqueeze(2).unsqueeze(3)\nx_hat1 = dec(z).to(\"cpu\").clone().detach().squeeze(0)\nx1 = x1.squeeze(0).to(\"cpu\")\n\nx2 = x2.to(device)\nemb2 = emb2.to(device)\nz = enc(x2)\n\nz = torch.cat((z,emb2), dim=1)\nz = z.unsqueeze(2).unsqueeze(3)\nx_hat2 = dec(z).to(\"cpu\").clone().detach().squeeze(0)\nx2 = x2.squeeze(0).to(\"cpu\")\n\nprint(x1.shape,x2.shape,x_hat1.shape, x_hat2.shape)\n\nplt.imshow(np.transpose(x1,(1,2,0)))\nplt.show()\nplt.imshow(np.transpose(x_hat1,(1,2,0)))\nplt.show()\nplt.imshow(np.transpose(x2,(1,2,0)))\nplt.show()\nplt.imshow(np.transpose(x_hat2,(1,2,0)))\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}