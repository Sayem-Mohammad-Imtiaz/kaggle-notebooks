{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GENDER RECOGNITION BY VOICE USING PYTHON","metadata":{}},{"cell_type":"code","source":"#Import les bibliothéques pour les manipulation des données, pour le calcul scientifique,visualisation et graphiques.\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LIRE LE FICHIER DES DONNEES","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/voicegender/voice.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n    meanfreq: mean frequency (in kHz)\n    sd: standard deviation of frequency\n    median: median frequency (in kHz)\n    Q25: first quantile (in kHz)\n    Q75: third quantile (in kHz)\n    IQR: interquantile range (in kHz)\n    skew: skewness (see note in specprop description)\n    kurt: kurtosis (see note in specprop description)\n    sp.ent: spectral entropy\n    sfm: spectral flatness\n    mode: mode frequency\n    centroid: frequency centroid (see specprop)\n    peakf: peak frequency (frequency with highest energy)\n    meanfun: average of fundamental frequency measured across acoustic signal\n    minfun: minimum fundamental frequency measured across acoustic signal\n    maxfun: maximum fundamental frequency measured across acoustic signal\n    meandom: average of dominant frequency measured across acoustic signal\n    mindom: minimum of dominant frequency measured across acoustic signal\n    maxdom: maximum of dominant frequency measured across acoustic signal\n    dfrange: range of dominant frequency measured across acoustic signal\n    modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n    label: male or female\n","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"POUR AFFICHER L'ENSEMBLE DE TABLE","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import HTML\ndisplay(HTML(df.head(10).to_html()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DONNEES**","metadata":{}},{"cell_type":"code","source":"#à indiquer la forme(range, colonne)\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verifier si il ya des colonnes sont bien alignés\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vérifier les types des données\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identfier combien des voix sont masculin et féminin dans la base de données avant le test\ndf.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vérifier si ila ya certaines valeurs nulle ou pas \ndf.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VISUALISATIONS**","metadata":{}},{"cell_type":"code","source":"#Identification des critérés retenir\nsns.pairplot(df,hue=\"label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"IQR,Meanfun,Q25 ont été choisi car les données masculin et féminin sont bien distincts.","metadata":{}},{"cell_type":"code","source":"#Joint plot pour visualiser dans un plan de distributions d'un couple de paramètres\nsns.jointplot(\"meanfreq\",\"sd\",df,kind=\"kde\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.kdeplot(df.meanfreq,df.sd,shade=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"homme= df.label=='male'\nfemme= df.label=='female'\nplt.figure(figsize=(8,8))\nsns.kdeplot(df[homme].meanfreq, df[homme].sd, cmap=\"Blues\",  shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[femme].meanfreq, df[femme].sd, cmap=\"Reds\", shade=True, alpha=0.3, shade_lowest=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=\"label\", y=\"sd\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.violinplot(x=\"label\",y=\"meanfreq\",data=df,scale=\"width\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = sns.FacetGrid(df, hue=\"label\", aspect=3, palette=\"Set2\") # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"meanfreq\", shade=True)\nfig.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x=\"meanfreq\", y=\"sd\", data=df, fit_reg=False, hue='label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MACHINE LEARNING\n\n\nOn sépare le dataset en deux parties :\n\n    un ensemble d'apprentissage (entre 70% et 90% des données), qui va permettre d'entraîner le modèle\n    un ensemble de test (entre 10% et 30% des données), qui va permettre d'estimer la pertinence de la prédiction","metadata":{}},{"cell_type":"code","source":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des données avec frac=0.8\ndata_test =df.drop(data_train.index)     # le reste des données pour le test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On sépare les données d'apprentissage (X_train) et la cible (y_train, la colonnes des données classe)","metadata":{}},{"cell_type":"code","source":"x_train = data_train.drop(['label'], axis=1)\ny_train = data_train['label']\nx_test = data_test.drop(['label'], axis=1)\ny_test = data_test['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Régression logistique**\n\nOn veut prédire une variable aléatoire Y\nà partir d'un vecteur de variables explicatives X=(X1,...,Xn)\n\nOn\n\nLa fonction logistique ex1+ex\nvarie entre −∞ et +∞ pour x variant entre 0 et 1.\nElle est souvent utilisée pour \"mapper\" une probabilité et un espace réel","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La régression logistique consiste à trouver une fonction linéaire C(X) qui permette d'estimer la probabilité de Y=1 connaissant X :\np(Y=1|X)=eC(X)1+eC(X)\n\nAutrement dit, cela revient à trouver une séparation linéaire des caractéristiques qui minimise un critère d'erreur.\n\nPour plus de détails, cf par exemple :\nhttp://eric.univ-lyon2.fr/~ricco/cours/cours/pratique_regression_logistique.pdf\n\nOn peut tracer la courbe de régression logistique pour prédire l'espèce Virginica à partir de la longueur du sépale avec la fonction lmplot :\n\nOn veut maintenant prédire l'espèce à partir de toutes les caractéristiques, et évaluer la qualité de cette prédiction en utilisant la régression logistique définie dans la librairie sklearn :\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On entraîne le modèle de régression logistique avec fit ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression(max_iter=500)\nlog.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut prédire les valeurs sur l'ensemble de test avec le modèle entraîné :","metadata":{}},{"cell_type":"code","source":"y_lr = log.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Score et matrice de confusion**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La mesure de pertinence compte le nombre de fois où l'algorithme a fait une bonne prédiction (en pourcentage) :","metadata":{}},{"cell_type":"code","source":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Une mesure plus fine consiste à compter le nombre de faux positif (valeur prédite 1 et réelle 0) et de vrai négatif (valeur prédite 0 et réelle 1). On utilise une matrice de confusion :","metadata":{}},{"cell_type":"code","source":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_lr)\nprint(cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(y_test, y_lr, rownames=['Reel'], colnames=['Prediction'], margins=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le résultat est il-satisfaisant?\nQuel pourrait etre le problème?","metadata":{}},{"cell_type":"markdown","source":"92,5% est le résultat du log regression. Mais dans le fichier données , l'étude sur les données en utilisant Log régression a eu un taux de précision de 71%. Donc le résultat est satisfaisant.\nRecherche comparaison:http://www.primaryobjects.com/2016/06/22/identifying-the-gender-of-a-voice-using-machine-learning/","metadata":{}},{"cell_type":"markdown","source":"**ARBRES DE DECISION**","metadata":{}},{"cell_type":"code","source":"fig = sns.FacetGrid(df, hue=\"label\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"meanfun\", shade=True)\nfig.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = sns.FacetGrid(df[df.meanfun>0.14], hue=\"label\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"Q25\", shade=True)\nfig.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = sns.FacetGrid(df[(df.meanfun>0.14)&(df.Q25>0.17)], hue=\"label\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"IQR\", shade=True)\nfig.add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L'indice GINI mesure avec quelle fréquence un élément aléatoire de l'ensemble serait mal classé si son étiquette était sélectionnée aléatoirement depuis la distribution des étiquettes dans le sous-ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_dtc = dtc.predict(x_test)\nprint(accuracy_score(y_test, y_dtc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=x_train.columns, class_names=['homme','femme'], fontsize=14, filled=True)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut modifier certains paramètres : Le paramètre max_depth est un seuil sur la profondeur maximale de l’arbre. Le paramètre min_samples_leaf donne le nombre minimal d’échantillons dans un noeud feuille.","metadata":{}},{"cell_type":"code","source":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On obtient un arbre un peu différent :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntree.plot_tree(dtc1, feature_names=x_train.columns, class_names=['homme','femme'], fontsize=14, filled=True)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_dtc1 = dtc1.predict(x_test)\nprint(accuracy_score(y_test, y_dtc1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RANDOM FORESTS","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(x_train, y_train)\ny_rf = rf.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Et la matrice de confusion \npd.crosstab(y_test, y_rf, rownames=['Reel'], colnames=['Prediction'], margins=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nL'attribut featureimportances renvoie un tableau du poids de chaque caractéristique dans la décision :\n","metadata":{}},{"cell_type":"code","source":"importances = rf.feature_importances_\nindices = np.argsort(importances)\nplt.figure(figsize=(12,8))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), df.columns[indices])\nplt.title('Importance des caracteristiques')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}