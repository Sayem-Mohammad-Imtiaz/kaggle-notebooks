{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Inspection","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# value cannot be 0 for these columns\ncols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing 0 with null values\nfor x in cols:\n    df[x] = df[x].where(df[x]!=0,np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CHecking missing values","metadata":{}},{"cell_type":"code","source":"df.isna().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating age group wise mean for Insulin,SkinThickness,BloodPressure,BMI\ndef compute_agewise(col):\n    \n    # age 21 to 35, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>20)&(df.Age<=35))].index] = \\\n                            round(df[(df.Age>20)&(df.Age<=35)][col].mean(),1)\n    \n    # age 36 to 50, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>35)&(df.Age<=50))].index] = \\\n                            round(df[(df.Age>35)&(df.Age<=50)][col].mean(),1)\n    \n    # age 51 to 70, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&((df.Age>50)&(df.Age<=70))].index] = \\\n                            round(df[(df.Age>50)&(df.Age<=70)][col].mean(),1)\n    \n    # age geater than 71, replace null values with mean \n    df[col].loc[df[(df[col].isna()==True)&(df.Age>70)].index] = \\\n                            round(df[df.Age>70][col].mean(),1)\ncompute_agewise('Insulin')\ncompute_agewise('SkinThickness')\ncompute_agewise('BloodPressure')\ncompute_agewise('BMI')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing with mean\ndf.Glucose.fillna(round(df.Glucose.mean(),1),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No outliers, all data points looks in clusters # Checking Outliers","metadata":{}},{"cell_type":"code","source":"for x in df.columns:\n    sns.boxplot(y=df[x])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No outliers, all data points looks in clusters ","metadata":{}},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"X = df.drop('Outcome',axis=1)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = df.Outcome\nY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X) \nprint(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \nrandom_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train, Y_train)\nY_pred=lr.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score,recall_score,roc_curve,roc_auc_score\n\ncfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nlracc=round(accuracy_score(Y_test,Y_pred),2)\nlrrecall = round(recall_score(Y_test,Y_pred),2)\nlrprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',lracc,'Recall:',lrrecall,'Precision:',lrprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nlrprob = lr.predict_proba(X_test)\n# roc curve\nlr_fpr, lr_tpr, lr_thresh = roc_curve(Y_test, lrprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nlr_auc_score = round(roc_auc_score(Y_test, lrprob[:,1]),2)\nprint('AUC Score:',lr_auc_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot roc curve\nplt.plot(lr_fpr, lr_tpr, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\nY_pred=dt.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\ndtacc=round(accuracy_score(Y_test,Y_pred),2)\ndtrecall = round(recall_score(Y_test,Y_pred),2)\ndtprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',dtacc,'Recall:',dtrecall,'Precision:',dtprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\ndtprob = dt.predict_proba(X_test)\n# roc curve\ndt_fpr, dt_tpr, dt_thresh = roc_curve(Y_test, dtprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\ndt_auc_score = round(roc_auc_score(Y_test, dtprob[:,1]),2)\nprint('AUC Score:',dt_auc_score)\n\n\n#plot roc curve\nplt.plot(dt_fpr, dt_tpr, linestyle='--',color='orange', label='Decision Tree')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_pred=rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nrfacc=round(accuracy_score(Y_test,Y_pred),2)\nrfrecall = round(recall_score(Y_test,Y_pred),2)\nrfprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',rfacc,'Recall:',rfrecall,'Precision:',rfprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nrfprob = rf.predict_proba(X_test)\n# roc curve\nrf_fpr, rf_tpr, rf_thresh = roc_curve(Y_test, rfprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nrf_auc_score = round(roc_auc_score(Y_test, rfprob[:,1]),2)\nprint('AUC Score:',rf_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Random Forest')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,Y_train)\nY_pred = gb.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\ngbacc=round(accuracy_score(Y_test,Y_pred),2)\ngbrecall = round(recall_score(Y_test,Y_pred),2)\ngbprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',gbacc,'Recall:',gbrecall,'Precision:',gbprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\ngbprob = gb.predict_proba(X_test)\n# roc curve\ngb_fpr, gb_tpr, gb_thresh = roc_curve(Y_test, gbprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\ngb_auc_score = round(roc_auc_score(Y_test, gbprob[:,1]),2)\nprint('AUC Score:',gb_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Gradient Boosting')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XG Boost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxg = XGBClassifier()\nxg.fit(X_train,Y_train)\nY_pred = xg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nxgacc=round(accuracy_score(Y_test,Y_pred),2)\nxgrecall = round(recall_score(Y_test,Y_pred),2)\nxgprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',xgacc,'Recall:',xgrecall,'Precision:',xgprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nxgprob = xg.predict_proba(X_test)\n# roc curve\nxg_fpr, xg_tpr, xg_thresh = roc_curve(Y_test, xgprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nxg_auc_score = round(roc_auc_score(Y_test, xgprob[:,1]),2)\nprint('AUC Score:',xg_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='XG Boost')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# create sub models\nestimators = []\n\nmodel1 = DecisionTreeClassifier()\nestimators.append(('dt',model1))\nmodel2 = GradientBoostingClassifier()\nestimators.append(('gb',model2))\nmodel3 = XGBClassifier()\nestimators.append(('xgb',model3))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators,voting='soft')\nensemble.fit(X_train,Y_train)\nY_pred = ensemble.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(Y_test,Y_pred)\n\nprint(cfm)\n\nprint(\"Classification Report\")\n\nprint(classification_report(Y_test,Y_pred))\n\nvcacc=round(accuracy_score(Y_test,Y_pred),2)\nvcrecall = round(recall_score(Y_test,Y_pred),2)\nvcprec = round(precision_score(Y_test,Y_pred),2)\n\nprint('Accuracy:',vcacc,'Recall:',vcrecall,'Precision:',vcprec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nvcprob = ensemble.predict_proba(X_test)\n# roc curve\nvc_fpr, vc_tpr, vc_thresh = roc_curve(Y_test, vcprob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)\n\n# auc scores\nvc_auc_score = round(roc_auc_score(Y_test, vcprob[:,1]),2)\nprint('AUC Score:',vc_auc_score)\n\n\n#plot roc curve\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='orange', label='Voting Classifier')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing Models","metadata":{}},{"cell_type":"code","source":"result = pd.DataFrame({'Accuracy':[lracc,dtacc,rfacc,gbacc,xgacc,vcacc],\n                         'Recall':[lrrecall,dtrecall,rfrecall,gbrecall,xgrecall,vcrecall],\n                         'Precision':[lrprec,dtprec,rfprec,gbprec,xgprec,vcprec],\n                         'Auc':[lr_auc_score,dt_auc_score,rf_auc_score,gb_auc_score,xg_auc_score,vc_auc_score]},\n                      \n                        index=['Logistic Regression','Decision Tree','Random Forest','Gradient Boosting','XG Boost','Voting Classifier'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy","metadata":{}},{"cell_type":"code","source":"sns.barplot(y = result.Accuracy.sort_values(ascending=False).index,\n           x = result.Accuracy.sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Precision","metadata":{}},{"cell_type":"code","source":"sns.barplot(y = result.Precision.sort_values(ascending=False).index,\n           x = result.Precision.sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recall","metadata":{}},{"cell_type":"code","source":"sns.barplot(y = result.Recall.sort_values(ascending=False).index,\n           x = result.Recall.sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AUC Score","metadata":{}},{"cell_type":"code","source":"sns.barplot(y = result.Auc.sort_values(ascending=False).index,\n           x = result.Auc.sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(lr_fpr, lr_tpr, linestyle='--',color='blue', label='Logistic Regression')\nplt.plot(dt_fpr, dt_tpr, linestyle='--',color='brown', label='Decision Tree')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='green', label='Random Forest')\nplt.plot(rf_fpr, rf_tpr, linestyle='-.',color='orange', label='Gradient Boosting')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='purple', label='XG Boost')\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='red', label='Voting Classifier')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='yellow')\n\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}