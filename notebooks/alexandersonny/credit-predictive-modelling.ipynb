{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Scoring Predictive Model"},{"metadata":{"hideCode":false,"hidePrompt":false},"cell_type":"markdown","source":"This predictive model using a dataset from Kaggle : Default of Credit Card Clients Dataset\n\n---\n\n**OBJECTIVE:** <br>\nTo create predictive model for a faster processing to determine which individual will default in the next month.\n\n---"},{"metadata":{"hideCode":false,"hidePrompt":false},"cell_type":"markdown","source":"### Data Description\n\nThere are 25 features in this dataset:\n\n1. ID: ID of each client \n2. LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n3. SEX: Gender (1=male, 2=female)\n4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n6. AGE: Age in years\n7. PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n8. PAY_2: Repayment status in August, 2005 (scale same as above)\n9. PAY_3: Repayment status in July, 2005 (scale same as above)\n10. PAY_4: Repayment status in June, 2005 (scale same as above)\n11. PAY_5: Repayment status in May, 2005 (scale same as above)\n12. PAY_6: Repayment status in April, 2005 (scale same as above)\n13. BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n14. BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n15. BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n16. BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n17. BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n18. BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n19. PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n20. PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n21. PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n22. PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n23. PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n24. PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n25. default.payment.next.month: Default payment (1=yes, 0=no)"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as pyo\nimport plotly.tools as tls\nimport plotly.graph_objs as go\npyo.init_notebook_mode(connected= True)\n\npd.options.display.max_columns = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**and the dataset too!**"},{"metadata":{"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard = pd.read_csv('../input/UCI_Credit_Card.csv')","execution_count":null,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false},"cell_type":"markdown","source":"## 1. EDA (Exploratory Data Analysis)"},{"metadata":{},"cell_type":"markdown","source":"**Get the information of our dataset:**"},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.info()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.head()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.drop(['ID'], axis=1, inplace = True)\ndf_ucicreditcard.rename(columns = {'default.payment.next.month': 'default_payment_next_month', 'PAY_0':'PAY_1'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1) We can just remove the 'ID' as we will not use it any further. <br>\n2) Rename the 'default.payment.next.month' feature to avoid any error while we are exploring or pre-processing the data.<br>\n3) Rename the 'PAY_0' into 'PAY_1' to avoid any confusion."},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":true,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.shape","execution_count":null,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.describe().T","execution_count":null,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verified the null values inside our dataset and seems like our dataset is null-free."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ucicreditcard_numerical_columns = df_ucicreditcard.columns\ndf_ucicreditcard_numerical_columns = ['LIMIT_BAL', 'AGE', 'PAY_1', \n             'PAY_2', 'PAY_3', 'PAY_4', \n             'PAY_5', 'PAY_6', 'BILL_AMT1', \n             'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n             'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', \n             'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', \n             'PAY_AMT5', 'PAY_AMT6']\ndf_ucicreditcard_numerical_columns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defined all of the numerical features to be used later on."},{"metadata":{},"cell_type":"markdown","source":"### Lets take a look at the categorical features first"},{"metadata":{},"cell_type":"markdown","source":"Peek all of the unique values for all of our categorical features:"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(\"Sex : \", df_ucicreditcard.SEX.unique())\nprint(\"Education : \", df_ucicreditcard.EDUCATION.unique())\nprint(\"Marriage : \", df_ucicreditcard.MARRIAGE.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the unique values above, we could see some undefined values.<br>\nThere is '0' values in both 'Education' and 'Marriage' features and we will handle it in the Data Pre-processing stage."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ucicreditcard.default_payment_next_month.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Default Distribution**"},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.title('Default Next Month Payment')\nax = sns.countplot(x = df_ucicreditcard.default_payment_next_month ,palette=\"Set2\")\nsns.set(font_scale= 1.0)\nax.set_ylim(top = 30000)\nax.set_xticklabels(['No Default','Default'])\nax.set_xlabel('Next Month Default Payment')\nax.set_ylabel('Frequency')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we want to predict the 'default' classification, based on the graph above, although the 'Default' is only  than 'No Default', we still consider it as **NOT** imbalanced data."},{"metadata":{},"cell_type":"markdown","source":"**SEX Distribution**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#First plot\ntrace0 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['SEX'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['SEX'].value_counts().values,\n    name = 'No Default'\n)\n\n#First plot 2\ntrace1 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['SEX'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['SEX'].value_counts().values,\n    name = 'Default'\n)\n\n#Second plot\ntrace2 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['SEX'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['LIMIT_BAL'],\n    name = trace0.name,\n    boxmean = True\n)\n\n#Second plot 2\ntrace3 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['SEX'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['LIMIT_BAL'],\n    name = trace1.name,\n    boxmean = True\n)\n\ndata = [trace0, trace1, trace2,trace3]\n\n\nfig = tls.make_subplots(rows= 1, cols= 2, \n                        subplot_titles= ('Sex Count', 'Credit Amount by Sex'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\n\nfig['layout'].update(height= 400, width= 800, title= 'Sex Distribution', boxmode= 'group')\npyo.iplot(fig, filename= 'sex-subplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Notes: 1= Male 2= Female*"},{"metadata":{"trusted":false},"cell_type":"code","source":"default_ratio = pd.pivot_table(df_ucicreditcard,\n                               columns= 'default_payment_next_month',\n                               values= 'AGE',\n                               index= 'SEX',\n                               aggfunc= 'count')\n\ndefault_ratio.reset_index(inplace= True)\ndefault_ratio.columns = ['Sex', 'No Default', 'Default']\n\ntotal = default_ratio['No Default'] + default_ratio['Default']\ndefault_ratio['Total'] = total\n\ndefault_ratio['No Default ratio'] = default_ratio['No Default'] / total\ndefault_ratio['Default ratio'] = default_ratio['Default'] / total\ndefault_ratio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from the first barplot, the 'Female' population is bigger than 'Male' in this dataset. <br>\nThe interesting part is eventhough 'Female' population is larger, the 'Default ratio' for 'Female' is smaller than the 'Male' category.\n\nIn the box plot, we got a clear idea that most of the 'Default' comes from that population that has smaller limit balance."},{"metadata":{},"cell_type":"markdown","source":"**EDUCATION Distribution**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#First plot\ntrace0 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['EDUCATION'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['EDUCATION'].value_counts().values,\n    name = 'No Default'\n)\n\n#First plot 2\ntrace1 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['EDUCATION'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['EDUCATION'].value_counts().values,\n    name = 'Default'\n)\n\n#Second plot\ntrace2 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['EDUCATION'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['LIMIT_BAL'],\n    name = trace0.name,\n    boxmean = True\n)\n\n#Second plot 2\ntrace3 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['EDUCATION'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['LIMIT_BAL'],\n    name = trace1.name,\n    boxmean = True\n)\n\ndata = [trace0, trace1, trace2,trace3]\n\n\nfig = tls.make_subplots(rows= 1, cols= 2, \n                        subplot_titles= ('Education Distribution', 'Credit Amount by Education'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\n\nfig['layout'].update(height= 400, width= 1000, title= 'Education Distribution', boxmode= 'group')\npyo.iplot(fig, filename= 'education-subplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Notes: 1= Graduate school; 2= University; 3= High school; 4= Others; 5= Unknown; 6= Unknown*"},{"metadata":{},"cell_type":"markdown","source":"As from 'Education' perspective, most of the population for 'Default' is from 'University' category."},{"metadata":{},"cell_type":"markdown","source":"**MARRIAGE Distribution**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#First plot\ntrace0 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['MARRIAGE'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['MARRIAGE'].value_counts().values,\n    name = 'No Default'\n)\n\n#First plot 2\ntrace1 = go.Bar(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['MARRIAGE'].value_counts().index.values,\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['MARRIAGE'].value_counts().values,\n    name = 'Default'\n)\n\n#Second plot\ntrace2 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['MARRIAGE'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 0]['LIMIT_BAL'],\n    name = trace0.name,\n    boxmean = True\n)\n\n#Second plot 2\ntrace3 = go.Box(\n    x = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['MARRIAGE'],\n    y = df_ucicreditcard[df_ucicreditcard['default_payment_next_month'] == 1]['LIMIT_BAL'],\n    name = trace1.name,\n    boxmean = True\n)\n\ndata = [trace0, trace1, trace2,trace3]\n\n\nfig = tls.make_subplots(rows= 1, cols= 2, \n                        subplot_titles= ('Marriage Distribution', 'Credit Amount by Marital status'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\n\nfig['layout'].update(height= 400, width= 1000, title= 'Marriage Distribution', boxmode= 'group')\npyo.iplot(fig, filename= 'marriage-subplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Notes: 1= Married; 2= Single; 3= Others*"},{"metadata":{},"cell_type":"markdown","source":"**Histogram for every Measurable features**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"def plot_histogram(data, cols, bins = 10, hist = True, norm_hist=False):\n    for col in cols:\n        fig = plt.figure(figsize = (7,3))\n        sns.set_style('whitegrid')\n        sns.distplot(a=data[col].dropna(), hist = hist)\n        plt.title('Histogram of ' + col, fontweight='bold', size=11)\n        plt.xlabel(col, size=10)\n        plt.ylabel('Frequency Density', size=10)\n        plt.show\n\nplot_histogram(df_ucicreditcard, df_ucicreditcard_numerical_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AGE Distribution**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df_good = df_ucicreditcard.loc[df_ucicreditcard['default_payment_next_month'] == 0]['AGE'].values.tolist()\ndf_bad = df_ucicreditcard.loc[df_ucicreditcard['default_payment_next_month'] == 1]['AGE'].values.tolist()\ndf_age = df_ucicreditcard['AGE'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x= df_good,\n    histnorm= 'probability',\n    name= 'No Default', \n    marker= dict(\n                    color= 'rgb(61,145,64)'\n                ) \n)\n#Second plot\ntrace1 = go.Histogram(\n    x= df_bad,\n    histnorm= 'probability',\n    name= 'Default',\n    marker= dict(\n                    color= 'rgb(179,27,27)'\n                ) \n    \n)\n#Third plot\ntrace2 = go.Histogram(\n    x= df_age,\n    histnorm= 'probability',\n    name= 'Overall Age',\n    marker= dict(\n                    color= 'rgb(0,72,186)'\n                ) \n)\n\n#Creating the grid\nfig = tls.make_subplots(rows= 2, cols= 2, specs= [[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('No Default', 'Default', 'Overall Age Distribution'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend= True, title= 'Age Distribution', bargap=0.05)\npyo.iplot(fig, filename= 'Age Distribution subplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram above, we get a clear idea that our audience mostly comes from '26 - 30' years old people."},{"metadata":{},"cell_type":"markdown","source":"**FEATURE CORRELATION**"},{"metadata":{},"cell_type":"markdown","source":"Let's see all numerical feature correlation to get a further understanding correlation for all numerical values."},{"metadata":{"hideCode":false,"hidePrompt":false,"scrolled":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard_numerical_columns_with_y = ['LIMIT_BAL', 'AGE', 'PAY_1', \n             'PAY_2', 'PAY_3', 'PAY_4', \n             'PAY_5', 'PAY_6', 'BILL_AMT1', \n             'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n             'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', \n             'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', \n             'PAY_AMT5', 'PAY_AMT6', 'default_payment_next_month']\ndf_ucicreditcard_num =  df_ucicreditcard[df_ucicreditcard_numerical_columns_with_y]\n\nplt.figure(figsize=(50,30))\nsns.set(font_scale=2.0)\nsns.heatmap(df_ucicreditcard_num.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the Feature Correlation, we could see all of the Numerical Correlation. In the graph above, we could see in the 'BILL_AMT' feature is highlight correlated. We just need to choose one of the features that highly correlated."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ucicreditcard_numerical_columns_bill_amt = ['BILL_AMT1', \n             'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n             'BILL_AMT5', 'BILL_AMT6', 'default_payment_next_month']\ndf_ucicreditcard_num_bill =  df_ucicreditcard[df_ucicreditcard_numerical_columns_bill_amt]\n\nplt.figure(figsize=(50,30))\nsns.set(font_scale=2.0)\nsns.heatmap(df_ucicreditcard_num_bill.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decided to choose only 'BILL_AMT1' for our predictive models and remove all unnecessary features for 'BILL_AMT'."},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"As a starter, we gonna drop all of the unnecessary 'BILL_AMT' features first."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Drop the unnecessary features for BILL_AMT\n\ndf_ucicreditcard = df_ucicreditcard.drop(['BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', \n             'BILL_AMT5', 'BILL_AMT6'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Planning to get all of the outliers using quartile metrics"},{"metadata":{"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers\n\n# These are the numerical features present in the dataset\nOutliers_to_drop = detect_outliers(df_ucicreditcard,2,['LIMIT_BAL',\n                                                 'PAY_1',\n                                                 'PAY_2',\n                                                 'PAY_3',\n                                                 'PAY_4',\n                                                 'PAY_5',\n                                                 'PAY_6',\n                                                 'BILL_AMT1',\n                                                 'PAY_AMT1',\n                                                 'PAY_AMT2',\n                                                 'PAY_AMT3',\n                                                 'PAY_AMT4',\n                                                 'PAY_AMT5',\n                                                 'PAY_AMT6'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.loc[Outliers_to_drop]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are all of the outlier records that has more than 2 outlier features per row. We could remove this record as a tryout later on if needed to compare the model with and without outliers."},{"metadata":{"trusted":false},"cell_type":"code","source":"edu_replacement = (df_ucicreditcard.EDUCATION == 5) | (df_ucicreditcard.EDUCATION == 6) | (df_ucicreditcard.EDUCATION == 0)\ndf_ucicreditcard.loc[edu_replacement, 'EDUCATION'] = 4\ndf_ucicreditcard.EDUCATION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to clean the data up, for the 'Education' feature, we replaced the 'unknown' value and any other undocumented value as '4 = Others'."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_ucicreditcard.loc[df_ucicreditcard.MARRIAGE == 0, 'MARRIAGE'] = 3\ndf_ucicreditcard.MARRIAGE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As for Marriage feature, we replaced the '0' value as 'Others'."},{"metadata":{},"cell_type":"markdown","source":"**Standard Scaler**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#StandardScaller is being used to normalize the features\nfrom sklearn.preprocessing import StandardScaler\n\nstandardized_features = ['LIMIT_BAL', 'AGE', 'PAY_1',  'PAY_2',  'PAY_3',  'PAY_4', 'PAY_5', 'PAY_6',\n                         'BILL_AMT1', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n\nnumeric_features = df_ucicreditcard[standardized_features]\nsc = StandardScaler()\nstandardized = pd.DataFrame(sc.fit_transform(numeric_features))\nstandardized.columns = ['LIMIT_BAL',\n                         'AGE',\n                         'PAY_1',\n                         'PAY_2',\n                         'PAY_3',\n                         'PAY_4',\n                         'PAY_5',\n                         'PAY_6',\n                         'BILL_AMT1',\n                         'PAY_AMT1',\n                         'PAY_AMT2',\n                         'PAY_AMT3',\n                         'PAY_AMT4',\n                         'PAY_AMT5',\n                         'PAY_AMT6']\n\ndf_ucicreditcard_stdized = df_ucicreditcard.copy()\ndf_ucicreditcard_stdized[standardized_features] = standardized\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make our dataset more robust and have a faster performance, we gonna standardized all of the numerical features in our dataset."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_ucicreditcard_stdized.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df_ucicreditcard_stdized.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ucicreditcard_stdized = pd.get_dummies(df_ucicreditcard_stdized, columns = [\"SEX\"], prefix= \"SEX\")\ndf_ucicreditcard_stdized = pd.get_dummies(df_ucicreditcard_stdized, columns = [\"EDUCATION\"], prefix= \"EDUCATION\")\ndf_ucicreditcard_stdized = pd.get_dummies(df_ucicreditcard_stdized, columns = [\"MARRIAGE\"], prefix= \"MARRIAGE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As for Categorical features, we will convert it into a boolean column as we don't want to mislead the model with numerical values."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_ucicreditcard_stdized.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the train data and test data"},{"metadata":{},"cell_type":"markdown","source":"Before we proceed to building our model, we will separate the test data and training data."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, make_scorer, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Prepare the features to predict Y\nfeatures = ['LIMIT_BAL', \n            'AGE', \n            'PAY_1', \n            'PAY_2',\n            'PAY_3', \n            'PAY_4', \n            'PAY_5', \n            'PAY_6', \n            'BILL_AMT1', \n            'PAY_AMT1',\n            'PAY_AMT2', \n            'PAY_AMT3', \n            'PAY_AMT4', \n            'PAY_AMT5', \n            'PAY_AMT6', \n            'SEX_1', \n            'SEX_2', \n            'EDUCATION_1', 'EDUCATION_2', 'EDUCATION_3', 'EDUCATION_4', 'MARRIAGE_1', 'MARRIAGE_2', 'MARRIAGE_3']\n\nX = df_ucicreditcard_stdized[features].copy()\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Prepare the target variable as Y\ny = df_ucicreditcard_stdized['default_payment_next_month'].copy()\ny.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split train and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    shuffle = True, \n                                                    random_state = 5)\n\nprint(\"Number of X_train dataset: \", X_train.shape)\nprint(\"Number of y_train dataset: \", y_train.shape)\nprint(\"Number of X_test dataset: \", X_test.shape)\nprint(\"Number of y_test dataset: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE"},{"metadata":{},"cell_type":"markdown","source":"SMOTE (Synthetic Minority Over-sampling Technique) is a statistical technique to balance out the number of cases in our dataset. Because of the dataset has label '0' more than label '1', I'm gonna try to upsample the label '1' using SMOTE and we will compare it later on."},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Before Over-sampling, the shape of train_X: {}'.format(X_train.shape))\nprint('Before Over-sampling, the shape of train_y: {} \\n'.format(y_train.shape))\n\nprint(\"Before Over-sampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before Over-sampling, counts of label '0': {}\".format(sum(y_train == 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state= 2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n\nprint(\"Number of X_train_res dataset: \", X_train_res.shape)\nprint(\"Number of y_train_res dataset: \", y_train_res.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('After Over-sampling, the shape of train_X_res: {}'.format(X_train_res.shape))\nprint('After Over-sampling, the shape of train_y_res: {} \\n'.format(y_train_res.shape))\n\nprint(\"After Over-sampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\nprint(\"After Over-sampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Build a model"},{"metadata":{},"cell_type":"markdown","source":"In this experiment, we will use the following algorithms:\n- Decision Tree (Tree Based)\n- Logistic Regression (The Basic One)\n- Random Forest (Ensemble & Tree Based)\n- Support Vector Machine (Kernel Based)\n\nFor each algorithms, we will conduct train & test, optimize it by adding tuning parameters, visualize the most important features of the model and finally we evaluate each model with evaluation classification metrics. Since the prediction label (default) has 2 values, with one outweight the other one, we gonna use SMOTE technique to reduce potential imbalanced label.\n\nAt the end, we will conclude the best machine learning model that is suitable for the case and elaborate some important findings regarding this experiment. "},{"metadata":{},"cell_type":"markdown","source":"### Decision tree"},{"metadata":{},"cell_type":"markdown","source":"#### Before resampling train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create Decision Tree model before optimization \nclf_DT_before_opt = DecisionTreeClassifier(random_state= 0)\n\n# Apply the model\nclf_DT_before_opt.fit(X_train, y_train)\n\n# Predicted value\ny_pred_DT_before_opt = clf_DT_before_opt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP = np.sum(np.logical_and(y_pred_DT_before_opt == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_DT_before_opt == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_DT_before_opt == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_DT_before_opt == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_DT_before_opt))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test,y_pred_DT_before_opt).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_DT_before_opt).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_DT_before_opt).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_DT_before_opt).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_DT_before_opt).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- TP (True Positive) : Predicted Default = Yes; Actual Default = Yes\n- TN (True Negative) : Predicted Default = No; Actual Default = No\n- FP (False Positive) : Predicted Default = Yes; Actual Default = No\n- FN (False Negative) : Predicted Default = No; Actual Default = Yes"},{"metadata":{},"cell_type":"markdown","source":"Before we continue, i will explain a little bit about the scoring evaluation metrics that we gonna use.\n\nScoring Evaluation Metrics:<br>\n1) Accuracy: Proportion of correct classification compared to the **Overall** number of cases <br>\n2) Precision: Proportion of correct classification compared to **Predicted** positive cases <br>\n3) Recall / Sensitivity: Proportion of correct classification compared to **Actual** positive cases <br>\n4) F1 score: Combination of both **Precision** and **Recall** to produce the score <br>\n5) ROC: Graph showing the performance of a classification model at all classification thresholds <br>\n6) Specificity / True Negative Rate: Measures the proportion of negative cases with actual negative cases <br>\n7) False Negative ratio: Measures the proportion of False Predicted cases with actual positive cases"},{"metadata":{},"cell_type":"markdown","source":"In this case, I think **Recall** and **Specificity** evaluation metrics is suitable for this kind of scenario. Other metrics information that being provided is only for our information if there is a further analysis needed. "},{"metadata":{},"cell_type":"markdown","source":"Before optimization, we got rather bad result as we only score 41% for **Recall** score. How to optimize?"},{"metadata":{},"cell_type":"markdown","source":"#### Hyper Parameter Tuning"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'max_depth': np.arange(3, 10),\n              'criterion' : ['gini','entropy'],\n              'max_leaf_nodes': [5,10,20,100],\n              'min_samples_split': [2, 5, 10, 20]}\n\n# create the grid\ngrid_DT_opt = GridSearchCV(DecisionTreeClassifier(), param_grid, cv= 5, scoring= 'recall')\n\n#training\ngrid_DT_opt.fit(X_train, y_train)\n#let's see the best estimator\nprint(grid_DT_opt.best_estimator_)\n#with its score\nprint(np.abs(grid_DT_opt.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_DT = DecisionTreeClassifier(class_weight= None, criterion= 'entropy', max_depth= 3,\n            max_features= None, max_leaf_nodes=20,\n            min_impurity_decrease= 0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf= 0.0, presort= False, random_state= None,\n            splitter='best')\n\nclf_DT.fit(X_train, y_train)\n\ny_pred_DT = clf_DT.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_DT == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_DT == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_DT == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_DT == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_DT))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_DT).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_DT).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_DT).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_DT).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_DT).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it's getting worse as unsatisfied result still persists after hyper parameter tuning process and our **Recall** score drops to 38%."},{"metadata":{},"cell_type":"markdown","source":"#### Using Resampling train data"},{"metadata":{},"cell_type":"markdown","source":"Let's try with the x_train and y_train that we resampled before."},{"metadata":{"trusted":false},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'max_depth': np.arange(3, 10),\n              'criterion' : ['gini','entropy'],\n              'max_leaf_nodes': [5,10,20,100],\n              'min_samples_split': [2, 5, 10, 20]}\n\n# create the grid\ngrid_DT_res = GridSearchCV(DecisionTreeClassifier(), param_grid, cv= 5, scoring= 'recall')\n\n#training\ngrid_DT_res.fit(X_train_res, y_train_res)\n#let's see the best estimator\nprint(grid_DT_res.best_estimator_)\n#with its score\nprint(np.abs(grid_DT_res.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_DT_res = DecisionTreeClassifier(class_weight= None, criterion= 'gini', max_depth= 9,\n            max_features= None, max_leaf_nodes=100,\n            min_impurity_decrease= 0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf= 0.0, presort= False, random_state= None,\n            splitter='best')\n\nclf_DT_fit_res = clf_DT_res.fit(X_train_res, y_train_res)\n\ny_pred_DT_res = clf_DT_res.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_DT_res == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_DT_res == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_DT_res == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_DT_res == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_DT_res))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_DT_res).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_DT_res).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_DT_res).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_DT_res).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_DT_res).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **Recall** improves alot and showing better result than before that reaches 59%."},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"trusted":false},"cell_type":"code","source":"cm = pd.crosstab(y_test.values, y_pred_DT_res, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.set(font_scale=1.0)\nsns.heatmap(cm, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\", fmt='g')\nplt.title('Confusion Matrix', fontsize= 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Predicting proba\ny_pred_prob_DT = clf_DT_fit_res.predict_proba(X_test)[:,1]\n\n\nsns.set(font_scale=1.0)\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_DT)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve', fontsize= 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features Importance**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"features_DT = pd.DataFrame()\nfeatures_DT['feature'] = X_train.columns\nfeatures_DT['importance'] = clf_DT_fit_res.feature_importances_\nfeatures_DT.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures_DT.set_index('feature', inplace=True)\n\nfeatures_DT.plot(kind='barh', figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Before using Resampling train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_LR_before_opt = LogisticRegression()\n\nclf_LR_before_opt.fit(X_train, y_train)\n\ny_pred_LR_before_opt = clf_LR_before_opt.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_LR_before_opt == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_LR_before_opt == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_LR_before_opt == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_LR_before_opt == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_LR_before_opt))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_LR_before_opt).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_LR_before_opt).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_LR_before_opt).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_LR_before_opt).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_LR_before_opt).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define the parameters grid\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\nparam_grid = {'tol': tol,\n              'max_iter' : max_iter}\n\n# create the grid\ngrid_LR = GridSearchCV(LogisticRegression(), param_grid, cv= 5, scoring= 'recall')\n\n#training\ngrid_LR.fit(X_train, y_train)\n#let's see the best estimator\nprint(grid_LR.best_estimator_)\n#with its score\nprint(np.abs(grid_LR.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_LR_tuned = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.001, verbose=0,\n                   warm_start=False)\n\nclf_LR_fit = clf_LR_tuned.fit(X_train, y_train)\n\ny_pred_LR = clf_LR_tuned.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_LR == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_LR == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_LR == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_LR == 0, y_test == 1))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_LR).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_LR).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_LR).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_LR).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_LR).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Resampling train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# define the parameters grid\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\nparam_grid = {'tol': tol,\n              'max_iter' : max_iter}\n\n# create the grid\ngrid_LR_res = GridSearchCV(LogisticRegression(), param_grid, cv= 5, scoring= 'recall')\n\n#training\ngrid_LR_res.fit(X_train_res, y_train_res)\n#let's see the best estimator\nprint(grid_LR_res.best_estimator_)\n#with its score\nprint(np.abs(grid_LR_res.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_LR_res_tuned = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.01, verbose=0,\n                   warm_start=False)\n\nclf_LR_fit_res = clf_LR_res_tuned.fit(X_train_res, y_train_res)\n\ny_pred_LR_res = clf_LR_res_tuned.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_LR_res == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_LR_res == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_LR_res == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_LR_res == 0, y_test == 1))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_LR_res).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_LR_res).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_LR_res).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_LR_res).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_LR_res).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"trusted":false},"cell_type":"code","source":"cm_LR = pd.crosstab(y_test.values, y_pred_LR_res, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm_LR, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\", fmt='g')\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Predicting proba\ny_pred_proba_LR = clf_LR_fit_res.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_LR)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"As the previous 2 models that using resampled train data produced better result, we will straightly using resampled train data to save up the time"},{"metadata":{},"cell_type":"markdown","source":"#### Using Resampled train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_RF = RandomForestClassifier(random_state = 0)\nclf_RF_fit = clf_RF.fit(X_train_res, y_train_res)\n\ny_pred_RF = clf_RF_fit.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP = np.sum(np.logical_and(y_pred_RF == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_RF == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_RF == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_RF == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_RF))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_RF).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_RF).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_RF).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_RF).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_RF).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Setting the Hyper Parameters\nparam_grid = {'max_depth': [3, 5, 7, 10],\n              'n_estimators':[3, 5, 10, 25, 50, 100],\n              'max_features': ['auto', 'sqrt', 4, 7, 15, 20], \n              'bootstrap': [True, False],\n              'criterion': ['gini', 'entropy']\n             }\n\ngrid_RF_res = GridSearchCV(RandomForestClassifier(), param_grid= param_grid, cv=5, scoring= 'recall', verbose= 4)\ngrid_RF_res.fit(X_train_res, y_train_res)\n\n#let's see the best estimator\nprint(grid_RF_res.best_estimator_)\n#with its score\nprint(np.abs(grid_RF_res.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_RF_res_tuned = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=10, max_features=20, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=25,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\n\nclf_RF_fit_res_tuned = clf_RF_res_tuned.fit(X_train_res, y_train_res)\n\ny_pred_RF_res_tuned = clf_RF_res_tuned.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_RF_res_tuned == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_RF_res_tuned == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_RF_res_tuned == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_RF_res_tuned == 0, y_test == 1))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_RF_res_tuned).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_RF_res_tuned).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_RF_res_tuned).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_RF_res_tuned).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_RF_res_tuned).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cm_LR = pd.crosstab(y_test.values, y_pred_RF_res_tuned, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm_LR, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\", fmt='g')\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Predicting proba\ny_pred_prob = clf_RF_fit_res_tuned.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features Importance**"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"features_RF = pd.DataFrame()\nfeatures_RF['feature'] = X_train.columns\nfeatures_RF['importance'] = clf_RF_fit_res_tuned.feature_importances_\nfeatures_RF.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures_RF.set_index('feature', inplace=True)\n\nfeatures_RF.plot(kind='barh', figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_SVC_before_opt = SVC()\n\nclf_SVC_before_opt.fit(X_train_res, y_train_res)\n\ny_pred_SVC_before_opt = clf_SVC_before_opt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP = np.sum(np.logical_and(y_pred_SVC_before_opt == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_SVC_before_opt == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_SVC_before_opt == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_SVC_before_opt == 0, y_test == 1))\n\n#Overall Classification Report\nprint(classification_report(y_test, y_pred_SVC_before_opt))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_SVC_before_opt).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_SVC_before_opt).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_SVC_before_opt).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_SVC_before_opt).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_SVC_before_opt).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define the parameters grid\nC= [0.123,0.124, 0.125, 0.126, 0.127]\nkernel = ['linear','rbf','poly']\ngamma = [0, 0.0000000000001, 0.000000000001, 0.00000000001]\n\nrandom_grid_svm = {'C': C,\n                   'kernel': kernel,\n                   'gamma': gamma}\n# create the grid\ngrid_LR = GridSearchCV(SVC(), param_grid= random_grid_svm, cv=5, scoring= 'recall', verbose= 4)\n\n#training\ngrid_LR.fit(X_train_res, y_train_res)\n#let's see the best estimator\nprint(grid_LR.best_estimator_)\n#with its score\nprint(np.abs(grid_LR.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_SVC_res_tuned = SVC(C=0.123, cache_size=200, class_weight=None, coef0=0.0,\n                        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1,\n                        probability=True, random_state=None, shrinking=True, tol=0.001,\n                        verbose=False)\n\nclf_SVC_fit_res_tuned = clf_SVC_res_tuned.fit(X_train_res, y_train_res)\n\ny_pred_SVC_res_tuned = clf_SVC_res_tuned.predict(X_test)\n\nTP = np.sum(np.logical_and(y_pred_SVC_res_tuned == 1, y_test == 1))\nTN = np.sum(np.logical_and(y_pred_SVC_res_tuned == 0, y_test == 0))\nFP = np.sum(np.logical_and(y_pred_SVC_res_tuned == 1, y_test == 0))\nFN = np.sum(np.logical_and(y_pred_SVC_res_tuned == 0, y_test == 1))\n\n# Accuracy Score\nprint('Accuracy score is ', accuracy_score(y_test, y_pred_SVC_res_tuned).round(2))\n\n# Precision Score\nprint('Precision score is ', precision_score(y_test, y_pred_SVC_res_tuned).round(2))\n\n# Recall Score\nprint('Recall_score is ', recall_score(y_test, y_pred_SVC_res_tuned).round(2))\n\n# F1 Score\nprint('F1 score is ', f1_score(y_test, y_pred_SVC_res_tuned).round(2))\n\n# ROC_AUC\nprint('ROC AUC is ', roc_auc_score(y_test, y_pred_SVC_res_tuned).round(2))\n\n# Specificity Ratio\nprint('Specificity ratio: {}'.format(round(TN/(TN+FP),2)))\n\n# False Negative Ratio\nprint('False Negative ratio: {}'.format(round(FN/(FN+TP),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"trusted":false},"cell_type":"code","source":"cm_svm = pd.crosstab(y_test.values, y_pred_SVC_res_tuned, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm_svm, \n            xticklabels=['Not Default', 'Default'],\n            yticklabels=['Not Default', 'Default'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\", fmt='g')\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Predicting proba\ny_SVC_pred_prob = clf_SVC_fit_res_tuned.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_SVC_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"•) Decision Tree model<br>\n**Recall** score = 59% and **Specificity** score = 77%<br>\n\n•) Logistic Regression model<br>\n**Recall** score = 66% and **Specificity** score = 71%<br>\n\n•) Random Forest model<br>\n**Recall** score = 54% and **Specificity** score = 86%<br>\n\n•) Support Vector Machine<br>\n**Recall** score = 57% and **Specificity** score = 84%<br>\n"},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"- In this report, we compared 4 models to be implemented in our Credit Scoring prediction and they have reached a maximum performance of prediction. The best model for this dataset is **Logistic Regression Model** because it produces **Recall** score = 66% and Specificity score = 71%. <br>\n- To maximize the performance, we used SMOTE to upsampling the label '1' and it achieved a better result compared to original train data as can be seen in Decision Tree and Logistic Regression training.<br>\n- Based on Decision Tree and Random Forest Features Importance, the highest correlation causes default payment is **PAY_1** feature which means most of the individual that has default payment had a problem in Repayment status September 2005.<br>\n- Excluding outliers from the dataset might improve the prediction result but the actual result what I received got worsened. It needs a certain domain knowledge to judge outliers from the data.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"•) Accuracy, Recall, Precision, F-Score & Specificity, which to optimize on?, https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124<br>\n•) Dealing with Imbalanced Data, https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18<br>\n•) Selecting the Right Metric for Skewed Classification Problems, https://towardsdatascience.com/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7<br>\n•) Simplifying the ROC and AUC metrics, https://towardsdatascience.com/understanding-the-roc-and-auc-curves-a05b68550b69<br>\n•) Simplifying The Confusion Matrix, https://medium.com/datadriveninvestor/simplifying-the-confusion-matrix-aa1fa0b0fc35<br>\n•) Introduction to Decision Trees, https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb<br>\n•) Understanding Random Forest, https://towardsdatascience.com/understanding-random-forest-58381e0602d2 <br>\n•) Hyper Parameter Tuning, https://www.jeremyjordan.me/hyperparameter-tuning/<br>\n•) Beyond Accuracy: Precision and Recall, https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c<br>\n•) Selecting the Right Metric for Skewed Classification Problems, https://towardsdatascience.com/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7<br>\n•) SMOTE Over-sampling, https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html"}],"metadata":{"hide_code_all_hidden":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}