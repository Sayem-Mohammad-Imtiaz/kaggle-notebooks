{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers > /dev/null","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport torch\n\nimport transformers\nfrom transformers import BertTokenizer\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\nfrom xgboost import XGBClassifier\n\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertSequenceVectorizer:\n    def __init__(self):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = 'bert-base-uncased'\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = 128\n\n    def vectorize(self, sentence : str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n\n        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy()\n        else:\n            return seq_out[0][0].detach().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam0 = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding = \"ISO-8859-1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam=spam0.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BSV = BertSequenceVectorizer()\nspam['v2_feature']=spam['v2'].progress_apply(lambda x: BSV.vectorize(x))\nspam.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(spam.shape)\nprint((spam['v2_feature'][0]).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Name=spam['v1'].unique()\nprint(Name)\nprint(len(Name))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=[]\nfor i in range(len(Name)):\n    N+=[i]\n    \nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) \n\ndef mapper(value):\n    return reverse_mapping[value]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label=[]\nfor item in spam['v1']:\n    label+=[normal_mapping[item]]\nprint(label[0:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v3=pd.DataFrame()\nfor i,item in tqdm(enumerate(spam['v2_feature'])):\n    for j in range(768):\n        v3.loc[i,j]=item[j]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v3[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=v3\nn=len(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data[0:(n//10)*8]\ntest_data = data[(n//10)*8:]\ntrain_label = label[0:(n//10)*8]\ntest_label = label[(n//10)*8:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train0 = train_label\nX_train0 = train_data\nX_test0 = test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X_train0)\ny = np.array(y_train0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = XGBClassifier( objective='binary:logistic',max_depth=3,n_estimators=1000,learning_rate=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = ShuffleSplit(n_splits=5,train_size=0.8,test_size=0.2,random_state=0) \n\nfor train_index, test_index in ss.split(X): \n    X_train, X_test = X[train_index], X[test_index]\n    Y_train, Y_test = y[train_index], y[test_index]\n    clf.fit(X_train, Y_train) \n    print(clf.score(X_test, Y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(np.array(X_test0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred2=clf.predict(X_test0)\nprint(X_test0.shape)\nprint(pred2.shape)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANS=test_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}