{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Hello everyone!</h1></center>\nI created a notebook where I made a little of analysis and prediction. I hope you will like it.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align:center\"><img src=\"https://www.eehealth.org/-/media/images/modules/blog/posts/heartline.jpg?h=500&w=750&hash=8147AFA9A68A838E7227B6524E566A99\" /></div>","metadata":{}},{"cell_type":"code","source":"# Visualization\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Machine learning models\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Data Preparation</h1>\nFirst we need to check if there is any missing values or outliers","metadata":{}},{"cell_type":"code","source":"# Check for missing values\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's looks like our data is fully filled with values. We don't need to worry.","metadata":{}},{"cell_type":"code","source":"def box_plot(column, plot_name):\n    fig = go.Figure()\n\n    fig.add_trace(go.Box(\n        y = column,\n        name = ''\n    ))\n\n    fig.update_layout(\n        template = 'plotly_dark',\n        title_text = plot_name\n    )\n\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plot(data['age'], 'Age box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plot(data['trtbps'], 'Resting blood pressure box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see couple of outliers but they are not that significante and probably they are not outliers at all. Someone probably had such a high score.","metadata":{}},{"cell_type":"code","source":"box_plot(data['chol'], 'Cholesterol box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['chol'] > 500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this situation we should think about this high cholesterol measurement. After a couple of minutes of research I can tell that measurement of over 500 is possible if we measure triglycerides but still, it would be a problem for our prediction in the future. We will delete this row from our dataset.","metadata":{}},{"cell_type":"code","source":"data_del = data[data['chol'] < 500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plot(data_del['chol'], 'Cholesterol box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plot(data_del['thalachh'], 'Maximum heart rate achieved box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plot(data_del['oldpeak'], 'Previous peak box plot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Exploratory data analysis</h1>\nFirst we have to answer a very important question. How does each feature affect our goal?","metadata":{}},{"cell_type":"code","source":"fig = go.Figure()\n\nto_plot = data_del['output'].replace({0: 'Less chance of heart attack', 1: 'More chance of heart attack'}).value_counts()\nlabels = to_plot.index\nvalues = to_plot.values\n\nfig.add_trace(go.Pie(\n    labels = labels,\n    values = values,\n    textinfo='percent'\n))\n\nfig.update_layout(\n    title_text='Survival',\n    template='plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can be happy that our dataset is balanced. It will be easier to perform prediction. ","metadata":{}},{"cell_type":"code","source":"# Age\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(\n    x = data_del[data_del['output'] == 0]['age'],\n    name = 'Less chance of heart attack'\n))\n\nfig.add_trace(go.Histogram(\n    x = data_del[data_del['output'] == 1]['age'],\n    name = 'More chance of heart attack'\n))\n\nfig.update_layout(\n    width = 1000,\n    template = 'plotly_dark',\n    title_text = 'Age by chance of heart stroke'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: People between 40 and 55 years of age have more chance to have a stroke. Which is a little weird in my opinion. I always thought that stroke chance increase with age. Unfortunately with this dataset we don't have enough data to verify it. Therefore, this thought will remain only a guess.  ","metadata":{}},{"cell_type":"code","source":"# Male vs female chance of stroke\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]], subplot_titles=['Female', 'Male'])\n\nto_plot_female = data_del[data_del['sex'] == 0]['output'].replace({0: 'Less chance of heart attack', 1: 'More chance of heart attack'}).value_counts()\nlabels_female = to_plot_female.index\nvalues_female = to_plot_female.values\n\nto_plot_male = data_del[data_del['sex'] == 1]['output'].replace({0: 'Less chance of heart attack', 1: 'More chance of heart attack'}).value_counts()\nlabels_male = to_plot_male.index\nvalues_male = to_plot_male.values\n\nfig.add_trace(\n    go.Pie(\n    labels = labels_female,\n    values = values_female,\n    textinfo='percent',\n    name='Female'),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Pie(\n    labels = labels_male,\n    values = values_male,\n    textinfo='percent',\n    name='Man'),\n    row=1,\n    col=2\n)\n\nfig.update_layout(\n    title_text='Male vs Female chance of stroke',\n    template='plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balance of sex feature\nto_plot_balance = data_del['sex'].replace({0: 'Female', 1: 'Male'}).value_counts()\nlabels_balance = to_plot_balance.index\nvalues_balance = to_plot_balance.values\n\nfig = go.Figure()\n\nfig.add_trace(go.Pie(\n    labels = labels_balance,\n    values = values_balance\n))\n\nfig.update_layout(\n    title_text='Population by gender',\n    template='plotly_dark'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: Women are more prone to heart attacks <br>\nFrom plots above we can tell that females have more chance of stroke but at the same time data are fairly unbalanced. This makes this conclusion irrelevant in my opinion.","metadata":{}},{"cell_type":"code","source":"# Resting blood pressure \ntrtbps_1 = data_del[data_del['output'] == 1]['trtbps']\ntrtbps_0 = data_del[data_del['output'] == 0]['trtbps']\n\n# Group data together\nhist_data = [trtbps_1, trtbps_0]\ngroup_labels = ['More chance of heart attack', 'Less chance of heart attack']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels)\n\nfig.update_layout(\n    title_text = 'Resting blood pressure by stroke chance'\n)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: Resting blood pressure isn't correlated with chance of heart stroke","metadata":{}},{"cell_type":"code","source":"# Cholesterol\nchol_1 = data_del[data_del['output'] == 1]['chol']\nchol_0 = data_del[data_del['output'] == 0]['chol']\n\n# Group data together\nhist_data = [chol_1, chol_0]\ngroup_labels = ['More chance of heart attack', 'Less chance of heart attack']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels)\n\nfig.update_layout(\n    title_text = 'Cholesterol by stroke chance'\n)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: People with cholesterol level (mg/dl) between 170 and 250 have more chance of stroke","metadata":{}},{"cell_type":"code","source":"# Maximum heart rate achieved\nthalachh_1 = data_del[data_del['output'] == 1]['thalachh']\nthalachh_0 = data_del[data_del['output'] == 0]['thalachh']\n\n# Group data together\nhist_data = [thalachh_1, thalachh_0]\ngroup_labels = ['More chance of heart attack', 'Less chance of heart attack']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels)\n\nfig.update_layout(\n    title_text = 'Maximum heart rate achieved by stroke chance'\n)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: People who achieved maximum heart rate larger than 150 have very big chance to experience a heart attack","metadata":{}},{"cell_type":"markdown","source":"To check correlation of other features with heart stroke chance we'll go the easy way and simply display pearson correlation values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\n\nheatmap = sns.heatmap(data.corr()[['output']].sort_values(by='output', ascending=False),\n                     vmin=-1, vmax=1, annot=True, cmap='BrBG')\n\nheatmap.set_title('Features correlated with chance of stroke', fontdict={'fontsize': 18}, pad=16);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: This chart shows that only two features (fbs and cho) aren't correlated with stroke chance at all. Rest of them are fairly correlated which can be helpful in prediction.","metadata":{}},{"cell_type":"markdown","source":"<h1>Prediction</h1>","metadata":{}},{"cell_type":"code","source":"data_del","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data\nX = data_del.drop(['output'], axis=1)\ny = data_del['output']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale values\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_evaluation(model):\n    # Train our model and predict\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Cross validation\n    scores = cross_val_score(model, X, y, cv=3, scoring='f1')\n    f1_scores_mean = scores.mean()\n    print('F1 scores: {}'.format(scores))\n    print('F1 mean score: {}'.format(f1_scores_mean))\n    \n    # Confusion matrix\n    print('Confusion Matrix: ')\n    matrix = confusion_matrix(y_test, y_pred)\n    group_names = ['True Negative','False Positive','False Negative','True Positive']\n    group_counts =['{0:0.0f}'.format(value) for value in matrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in matrix.flatten()/np.sum(matrix)]\n    \n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2, 2)\n\n    sns.heatmap(matrix, annot=labels, fmt='', cmap='rocket_r')\n    \n    return f1_scores_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-nearest neighbors \nKNN_model = KNeighborsClassifier()\n\n# SVC\nSVC_model = SVC()\n\n# Logistic regression\nLR_model = LogisticRegression()\n\n# Decision tree\nDT_model = DecisionTreeClassifier()\n\n# Random Forest\nRF_model = RandomForestClassifier()\n\n# XGBoost\nXGB_model = xgb.XGBClassifier()\n\n# LightGBM\nLGBM_model = LGBMClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_KNN = model_evaluation(KNN_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_SVC = model_evaluation(SVC_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_LR = model_evaluation(LR_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_DT = model_evaluation(DT_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_RF = model_evaluation(RF_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_XGB = model_evaluation(XGB_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_LGBM = model_evaluation(LGBM_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above results we see that models are unstable and accuracy jumps uncontrollably. I'm pretty sure that is a result of the small size of the dataset. We will try to tune the logistic regression model to make him more stable. I chose this algorithm because he is simple and that kind of models works best for small datasets.","metadata":{}},{"cell_type":"code","source":"LR_new = LogisticRegression(\n    solver = 'liblinear',\n    penalty = 'l1',\n    # C = 1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(LR_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! After we added a regularization parameter our model become more stable. We achieved our goal, the model doesn't look overfitted and accuracy is fair enough for such small dataset. I think we can leave it like this. ","metadata":{}}]}