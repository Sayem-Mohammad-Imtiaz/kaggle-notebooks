{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cancer Diagnosis : Approaches Comparison","metadata":{}},{"cell_type":"markdown","source":"In this notebook we're going to use the **Breast Cancer Wisconsin (Diagnostic) Data Set** and we're going to apply different methods for classification\n\nThe problem in hand here is a binary classification problem, given a set of features we wanna know whether a specific tumor is Malignant or Benign ","metadata":{}},{"cell_type":"markdown","source":"## The Approaches We're Going to Test Are\n\n1. Machine Learning Approaches :\n    1. Logistic Regression\n    2. K Neighbors Classifier\n    3. C-Support Vector Classification (SVC) With Linear Kernel\n    4. C-Support Vector Classification (SVC) With rbf Kernel\n    5. Gaussian Naive Bayes\n    6. Decision Trees\n    7. Random Forest\n\n2. A Fuzzy Logic Approach\n3. Neural Network Approach\n4. A Neural Network Trained With Genetic Algorithms (Hybrid Approach)","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation\n\nLet's start by loading some important libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.714573Z","iopub.execute_input":"2021-05-20T09:45:05.714892Z","iopub.status.idle":"2021-05-20T09:45:05.720403Z","shell.execute_reply.started":"2021-05-20T09:45:05.714862Z","shell.execute_reply":"2021-05-20T09:45:05.719602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's import our dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.72555Z","iopub.execute_input":"2021-05-20T09:45:05.725802Z","iopub.status.idle":"2021-05-20T09:45:05.772542Z","shell.execute_reply.started":"2021-05-20T09:45:05.72577Z","shell.execute_reply":"2021-05-20T09:45:05.771791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at our dataset","metadata":{}},{"cell_type":"code","source":"print(dataset.count().unique())\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.773979Z","iopub.execute_input":"2021-05-20T09:45:05.77435Z","iopub.status.idle":"2021-05-20T09:45:05.813579Z","shell.execute_reply.started":"2021-05-20T09:45:05.774315Z","shell.execute_reply":"2021-05-20T09:45:05.81262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see we have 33 columns, 30 of them represents our features and 1 is the classification label and also we have 569 rows","metadata":{}},{"cell_type":"markdown","source":"Let's check for null values in the dataset","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.815535Z","iopub.execute_input":"2021-05-20T09:45:05.815879Z","iopub.status.idle":"2021-05-20T09:45:05.824735Z","shell.execute_reply.started":"2021-05-20T09:45:05.815843Z","shell.execute_reply":"2021-05-20T09:45:05.823875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a whole column of nulls in our dataset, so let's drop that column","metadata":{}},{"cell_type":"code","source":"del dataset['Unnamed: 32']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.826492Z","iopub.execute_input":"2021-05-20T09:45:05.827016Z","iopub.status.idle":"2021-05-20T09:45:05.834833Z","shell.execute_reply.started":"2021-05-20T09:45:05.826979Z","shell.execute_reply":"2021-05-20T09:45:05.833612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After Cleaning Our Data We do The Necessary Splits","metadata":{}},{"cell_type":"markdown","source":"First we need to split the dataset into **Features:X** and **Labels:y**","metadata":{}},{"cell_type":"code","source":"X = dataset.iloc[:, 2:].values\ny = dataset.iloc[:, 1].values","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.836223Z","iopub.execute_input":"2021-05-20T09:45:05.836798Z","iopub.status.idle":"2021-05-20T09:45:05.844928Z","shell.execute_reply.started":"2021-05-20T09:45:05.836761Z","shell.execute_reply":"2021-05-20T09:45:05.84386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the first values of X and y","metadata":{}},{"cell_type":"code","source":"X[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.846207Z","iopub.execute_input":"2021-05-20T09:45:05.846866Z","iopub.status.idle":"2021-05-20T09:45:05.857099Z","shell.execute_reply.started":"2021-05-20T09:45:05.846821Z","shell.execute_reply":"2021-05-20T09:45:05.856167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[:20]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.858587Z","iopub.execute_input":"2021-05-20T09:45:05.8593Z","iopub.status.idle":"2021-05-20T09:45:05.870605Z","shell.execute_reply.started":"2021-05-20T09:45:05.859263Z","shell.execute_reply":"2021-05-20T09:45:05.869475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see out **Dataframe** turned into a **Numpy Array** which is what we need for our models\n\nbut we can see that our labels:y is characters, we need to convert it to binary values, so let's do that using LabelEncoder","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ny = labelencoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:05.872321Z","iopub.execute_input":"2021-05-20T09:45:05.8732Z","iopub.status.idle":"2021-05-20T09:45:06.018593Z","shell.execute_reply.started":"2021-05-20T09:45:05.873153Z","shell.execute_reply":"2021-05-20T09:45:06.017816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[:20]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.021524Z","iopub.execute_input":"2021-05-20T09:45:06.021902Z","iopub.status.idle":"2021-05-20T09:45:06.027255Z","shell.execute_reply.started":"2021-05-20T09:45:06.021862Z","shell.execute_reply":"2021-05-20T09:45:06.026288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great\nNow we need to split the data into **Train Data** and **Test Data**\nwe're going to set the ratio to 20% Test : 80% Train ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.029723Z","iopub.execute_input":"2021-05-20T09:45:06.030158Z","iopub.status.idle":"2021-05-20T09:45:06.097979Z","shell.execute_reply.started":"2021-05-20T09:45:06.030119Z","shell.execute_reply":"2021-05-20T09:45:06.096835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's take a look at X_train for example","metadata":{}},{"cell_type":"code","source":"print(len(X_train))\nX_train","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.099203Z","iopub.execute_input":"2021-05-20T09:45:06.099744Z","iopub.status.idle":"2021-05-20T09:45:06.107147Z","shell.execute_reply.started":"2021-05-20T09:45:06.099706Z","shell.execute_reply":"2021-05-20T09:45:06.10615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, now we have on last problem to solve in our dataset, let's take a look at some statistics form our original data","metadata":{}},{"cell_type":"code","source":"dataset.describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:06.108479Z","iopub.execute_input":"2021-05-20T09:45:06.109206Z","iopub.status.idle":"2021-05-20T09:45:06.201618Z","shell.execute_reply.started":"2021-05-20T09:45:06.109158Z","shell.execute_reply":"2021-05-20T09:45:06.200767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the mean values of each feature we notice that our data have very different ranges, so we need to **Normalize** our data, or whats called **Feature Scalling** so let's do that","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.202805Z","iopub.execute_input":"2021-05-20T09:45:06.203157Z","iopub.status.idle":"2021-05-20T09:45:06.209Z","shell.execute_reply.started":"2021-05-20T09:45:06.20312Z","shell.execute_reply":"2021-05-20T09:45:06.208128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Machine Learning Approaches","metadata":{}},{"cell_type":"markdown","source":"## 1.A Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Now with our first classifier which is Logistic Regression, Let's Create a Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLogisticRegressionModel = LogisticRegression(random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.210383Z","iopub.execute_input":"2021-05-20T09:45:06.21096Z","iopub.status.idle":"2021-05-20T09:45:06.311879Z","shell.execute_reply.started":"2021-05-20T09:45:06.210915Z","shell.execute_reply":"2021-05-20T09:45:06.311179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train the Model using our train data","metadata":{}},{"cell_type":"code","source":"LogisticRegressionModel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.313024Z","iopub.execute_input":"2021-05-20T09:45:06.313543Z","iopub.status.idle":"2021-05-20T09:45:06.348488Z","shell.execute_reply.started":"2021-05-20T09:45:06.313489Z","shell.execute_reply":"2021-05-20T09:45:06.347664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we start classifying the test data","metadata":{}},{"cell_type":"code","source":"y_pred_A1 = LogisticRegressionModel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.352401Z","iopub.execute_input":"2021-05-20T09:45:06.354488Z","iopub.status.idle":"2021-05-20T09:45:06.360288Z","shell.execute_reply.started":"2021-05-20T09:45:06.354447Z","shell.execute_reply":"2021-05-20T09:45:06.359094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create a Confusion Matrix to see how our results look like","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm_A1 = confusion_matrix(y_test, y_pred_A1)\nprint('Confusion Matrix for Logistic Regression Model')\nsns.heatmap(cm_A1,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.365672Z","iopub.execute_input":"2021-05-20T09:45:06.368174Z","iopub.status.idle":"2021-05-20T09:45:06.725862Z","shell.execute_reply.started":"2021-05-20T09:45:06.368131Z","shell.execute_reply":"2021-05-20T09:45:06.724975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Logistic Regression Model accuracy is {}%\".format(((cm_A1[0][0] + cm_A1[1][1])/cm_A1.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:06.727106Z","iopub.execute_input":"2021-05-20T09:45:06.727439Z","iopub.status.idle":"2021-05-20T09:45:06.732688Z","shell.execute_reply.started":"2021-05-20T09:45:06.727401Z","shell.execute_reply":"2021-05-20T09:45:06.73188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.B K Neighbors Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nKNeighborsModel = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n\nKNeighborsModel.fit(X_train, y_train)\n\ny_pred_A2 = KNeighborsModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A2 = confusion_matrix(y_test, y_pred_A2)\nprint('Confusion Matrix for KNeighbors Model')\nsns.heatmap(cm_A2,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:06.733894Z","iopub.execute_input":"2021-05-20T09:45:06.734435Z","iopub.status.idle":"2021-05-20T09:45:07.055764Z","shell.execute_reply.started":"2021-05-20T09:45:06.734392Z","shell.execute_reply":"2021-05-20T09:45:07.054984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"KNeighbors Model accuracy is {}%\".format(((cm_A2[0][0] + cm_A2[1][1])/cm_A2.sum())*100))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.056986Z","iopub.execute_input":"2021-05-20T09:45:07.057376Z","iopub.status.idle":"2021-05-20T09:45:07.064137Z","shell.execute_reply.started":"2021-05-20T09:45:07.057337Z","shell.execute_reply":"2021-05-20T09:45:07.06301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.C C-Support Vector Classification (SVC) With Linear Kernel","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nSVCModel = SVC(kernel = 'linear', random_state=0)\n\nSVCModel.fit(X_train, y_train)\n\ny_pred_A3 = SVCModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A3 = confusion_matrix(y_test, y_pred_A3)\nprint('Confusion Matrix for SVC Model')\nsns.heatmap(cm_A3,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.06551Z","iopub.execute_input":"2021-05-20T09:45:07.065913Z","iopub.status.idle":"2021-05-20T09:45:07.27779Z","shell.execute_reply.started":"2021-05-20T09:45:07.065877Z","shell.execute_reply":"2021-05-20T09:45:07.276995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"SVC Model accuracy is {}%\".format(((cm_A3[0][0] + cm_A3[1][1])/cm_A3.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:07.278931Z","iopub.execute_input":"2021-05-20T09:45:07.279306Z","iopub.status.idle":"2021-05-20T09:45:07.285963Z","shell.execute_reply.started":"2021-05-20T09:45:07.279259Z","shell.execute_reply":"2021-05-20T09:45:07.2844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.D C-Support Vector Classification (SVC) With rbf Kernel","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nSVCrModel = SVC(kernel = 'rbf', random_state = 0)\n\nSVCrModel.fit(X_train, y_train)\n\ny_pred_A4 = SVCrModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A4 = confusion_matrix(y_test, y_pred_A4)\nprint('Confusion Matrix for SVC Kernelized Model')\nsns.heatmap(cm_A4,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.287282Z","iopub.execute_input":"2021-05-20T09:45:07.287968Z","iopub.status.idle":"2021-05-20T09:45:07.50472Z","shell.execute_reply.started":"2021-05-20T09:45:07.287926Z","shell.execute_reply":"2021-05-20T09:45:07.503963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"SVC Kernelized Model accuracy is {}%\".format(((cm_A4[0][0] + cm_A4[1][1])/cm_A4.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:07.505908Z","iopub.execute_input":"2021-05-20T09:45:07.506282Z","iopub.status.idle":"2021-05-20T09:45:07.513227Z","shell.execute_reply.started":"2021-05-20T09:45:07.506245Z","shell.execute_reply":"2021-05-20T09:45:07.512221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.E Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nGaussianNBModel = GaussianNB()\n\nGaussianNBModel.fit(X_train, y_train)\n\ny_pred_A5 = GaussianNBModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A5 = confusion_matrix(y_test, y_pred_A5)\nprint('Confusion Matrix for Gaussian NB Model')\nsns.heatmap(cm_A5,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.514795Z","iopub.execute_input":"2021-05-20T09:45:07.515158Z","iopub.status.idle":"2021-05-20T09:45:07.722741Z","shell.execute_reply.started":"2021-05-20T09:45:07.51512Z","shell.execute_reply":"2021-05-20T09:45:07.721983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Gaussian NB Model accuracy is {}%\".format(((cm_A5[0][0] + cm_A5[1][1])/cm_A5.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:07.725514Z","iopub.execute_input":"2021-05-20T09:45:07.725783Z","iopub.status.idle":"2021-05-20T09:45:07.731079Z","shell.execute_reply.started":"2021-05-20T09:45:07.725757Z","shell.execute_reply":"2021-05-20T09:45:07.730076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.F Decision Trees","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDecisionTreeModel = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\nDecisionTreeModel.fit(X_train, y_train)\n\ny_pred_A6 = DecisionTreeModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A6 = confusion_matrix(y_test, y_pred_A6)\nprint('Confusion Matrix for Decision Tree Model')\nsns.heatmap(cm_A6,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.737246Z","iopub.execute_input":"2021-05-20T09:45:07.737488Z","iopub.status.idle":"2021-05-20T09:45:07.981886Z","shell.execute_reply.started":"2021-05-20T09:45:07.737464Z","shell.execute_reply":"2021-05-20T09:45:07.980793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Decision Tree Model accuracy is {}%\".format(((cm_A6[0][0] + cm_A6[1][1])/cm_A6.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:07.984446Z","iopub.execute_input":"2021-05-20T09:45:07.984795Z","iopub.status.idle":"2021-05-20T09:45:07.990432Z","shell.execute_reply.started":"2021-05-20T09:45:07.984756Z","shell.execute_reply":"2021-05-20T09:45:07.989035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.G Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRandomForestModel = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\nRandomForestModel.fit(X_train, y_train)\n\ny_pred_A7 = RandomForestModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A7 = confusion_matrix(y_test, y_pred_A7)\nprint('Confusion Matrix for Random Forest Model')\nsns.heatmap(cm_A7,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:07.99214Z","iopub.execute_input":"2021-05-20T09:45:07.992568Z","iopub.status.idle":"2021-05-20T09:45:08.254557Z","shell.execute_reply.started":"2021-05-20T09:45:07.992528Z","shell.execute_reply":"2021-05-20T09:45:08.253648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Random Forest Model accuracy is {}%\".format(((cm_A7[0][0] + cm_A7[1][1])/cm_A7.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:45:08.255805Z","iopub.execute_input":"2021-05-20T09:45:08.256172Z","iopub.status.idle":"2021-05-20T09:45:08.261898Z","shell.execute_reply.started":"2021-05-20T09:45:08.256132Z","shell.execute_reply":"2021-05-20T09:45:08.260575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now Let's Rank These Machine Learning Approaches Before Moving Forward","metadata":{}},{"cell_type":"markdown","source":"1. SVC Model accuracy is 98.24561403508771%\n2. SVC Kernelized Model accuracy is 98.24561403508771%\n3. Random Forest Model accuracy is 97.36842105263158%\n4. Logistic Regression Model accuracy is 96.49122807017544%\n5. KNeighbors Model accuracy is 95.6140350877193%\n6. Decision Tree Model accuracy is 92.98245614035088%\n7. Gaussian NB Model accuracy is 90.35087719298247%","metadata":{}},{"cell_type":"markdown","source":"## 2. A Fuzzy Logic Approach","metadata":{}},{"cell_type":"markdown","source":"To use fuzzy logic on our dataset we first we'll need to create 2 new features\n- uniformity : difference between the radius extreme value and the radius mean value\n- homogeneity : difference between the extreme value of symmetry and the mean value of symmetry\n\nso let's create these 2 columns in a new dataset copied from our cleaned data","metadata":{}},{"cell_type":"code","source":"fuzzy_data = dataset.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:08.263781Z","iopub.execute_input":"2021-05-20T09:45:08.264261Z","iopub.status.idle":"2021-05-20T09:45:08.271989Z","shell.execute_reply.started":"2021-05-20T09:45:08.264223Z","shell.execute_reply":"2021-05-20T09:45:08.270586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fuzzy_data['uniformity'] = fuzzy_data['radius_worst'] - fuzzy_data['radius_mean']\nfuzzy_data['homogeneity'] = fuzzy_data['symmetry_worst'] - fuzzy_data['symmetry_mean']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:08.273555Z","iopub.execute_input":"2021-05-20T09:45:08.273967Z","iopub.status.idle":"2021-05-20T09:45:08.297204Z","shell.execute_reply.started":"2021-05-20T09:45:08.273929Z","shell.execute_reply":"2021-05-20T09:45:08.296452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's take a look at our modified new dataset","metadata":{}},{"cell_type":"code","source":"fuzzy_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:08.299765Z","iopub.execute_input":"2021-05-20T09:45:08.300018Z","iopub.status.idle":"2021-05-20T09:45:08.327915Z","shell.execute_reply.started":"2021-05-20T09:45:08.299992Z","shell.execute_reply":"2021-05-20T09:45:08.327187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll base our model on 4 features \n- AREA\n- PERIMETER\n- UNIFORMITY\n- HOMOGENEITY","metadata":{}},{"cell_type":"markdown","source":"Let's limit our dataset only to these values","metadata":{}},{"cell_type":"code","source":"fuzzy_data = fuzzy_data[['area_mean', 'perimeter_mean', 'uniformity', 'homogeneity', 'diagnosis']]\nfuzzy_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:08.329016Z","iopub.execute_input":"2021-05-20T09:45:08.329365Z","iopub.status.idle":"2021-05-20T09:45:08.345603Z","shell.execute_reply.started":"2021-05-20T09:45:08.329331Z","shell.execute_reply":"2021-05-20T09:45:08.344522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we import the fuzzy libraries","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy\nimport numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:08.346937Z","iopub.execute_input":"2021-05-20T09:45:08.347331Z","iopub.status.idle":"2021-05-20T09:45:18.848714Z","shell.execute_reply.started":"2021-05-20T09:45:08.347295Z","shell.execute_reply":"2021-05-20T09:45:18.847842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now before we create our antecedents let's first check the ranges of values in our dataset so we can set the limits of our universe","metadata":{}},{"cell_type":"code","source":"fuzzy_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:18.850147Z","iopub.execute_input":"2021-05-20T09:45:18.850486Z","iopub.status.idle":"2021-05-20T09:45:18.88018Z","shell.execute_reply.started":"2021-05-20T09:45:18.850445Z","shell.execute_reply":"2021-05-20T09:45:18.87937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's set our limits based on these maximum values","metadata":{}},{"cell_type":"code","source":"AREA = ctrl.Antecedent(np.arange(0, 2501.000000, 0.0001), 'AREA')\nPERIMETER = ctrl.Antecedent(np.arange(0, 188.500000, 0.0001), 'PERIMETER')\nUNIFORMITY = ctrl.Antecedent(np.arange(0, 11.760000, 0.0001), 'UNIFORMITY')\nHOMOGENEITY = ctrl.Antecedent(np.arange(0, 0.404100, 0.0001), 'HOMOGENEITY')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:18.881563Z","iopub.execute_input":"2021-05-20T09:45:18.881934Z","iopub.status.idle":"2021-05-20T09:45:18.949082Z","shell.execute_reply.started":"2021-05-20T09:45:18.881895Z","shell.execute_reply":"2021-05-20T09:45:18.948162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also our diagnosis will have a value of [0:1] which will then be mapped to Malignant and Bengin","metadata":{}},{"cell_type":"code","source":"DIAGNOSIS = ctrl.Consequent(np.arange(0, 1, 0.0001), 'DIAGNOSIS')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:18.950312Z","iopub.execute_input":"2021-05-20T09:45:18.950672Z","iopub.status.idle":"2021-05-20T09:45:18.955409Z","shell.execute_reply.started":"2021-05-20T09:45:18.950635Z","shell.execute_reply":"2021-05-20T09:45:18.954484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's the universe for our Antecedent, note that these values are based on a research by \"National Center for Biotechnology Information, U.S. National Library of Medicine\"","metadata":{}},{"cell_type":"markdown","source":"Area Universe","metadata":{}},{"cell_type":"code","source":"AREA['Smaller'] = fuzz.trapmf(AREA.universe, [0, 0, 748.8,1000])\nAREA['Larger'] = fuzz.trapmf(AREA.universe, [508.1, 2194, 2501,2501])\nAREA.view()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:18.956895Z","iopub.execute_input":"2021-05-20T09:45:18.957262Z","iopub.status.idle":"2021-05-20T09:45:57.715715Z","shell.execute_reply.started":"2021-05-20T09:45:18.957225Z","shell.execute_reply":"2021-05-20T09:45:57.713702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perimeter Universe","metadata":{}},{"cell_type":"code","source":"PERIMETER['Smaller'] = fuzz.trapmf(PERIMETER.universe, [0, 0, 92.58,103])\nPERIMETER['Larger'] = fuzz.trapmf(PERIMETER.universe, [85.1, 159.8, 188.5,188.5])\nPERIMETER.view()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:45:57.717246Z","iopub.execute_input":"2021-05-20T09:45:57.717629Z","iopub.status.idle":"2021-05-20T09:46:00.743681Z","shell.execute_reply.started":"2021-05-20T09:45:57.717588Z","shell.execute_reply":"2021-05-20T09:46:00.742429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uniformity Universe","metadata":{}},{"cell_type":"code","source":"UNIFORMITY['Smaller'] = fuzz.trapmf(UNIFORMITY.universe, [0, 0, 1.669,2.6])\nUNIFORMITY['Larger'] = fuzz.trapmf(UNIFORMITY.universe, [0.65, 6.205, 11.76,11.76])\nUNIFORMITY.view()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:00.745372Z","iopub.execute_input":"2021-05-20T09:46:00.745798Z","iopub.status.idle":"2021-05-20T09:46:00.983354Z","shell.execute_reply.started":"2021-05-20T09:46:00.745749Z","shell.execute_reply":"2021-05-20T09:46:00.982392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Homogenity Universe","metadata":{}},{"cell_type":"code","source":"HOMOGENEITY['Smaller'] = fuzz.trapmf(HOMOGENEITY.universe, [0, 0, 0.1232,.19])\nHOMOGENEITY['Larger'] = fuzz.trapmf(HOMOGENEITY.universe, [0.0295, 0.2168, 0.4041,0.4041])\nHOMOGENEITY.view()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:00.98468Z","iopub.execute_input":"2021-05-20T09:46:00.985021Z","iopub.status.idle":"2021-05-20T09:46:01.146066Z","shell.execute_reply.started":"2021-05-20T09:46:00.984984Z","shell.execute_reply":"2021-05-20T09:46:01.144824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finaly the Diagnosis Universe Which will not be trapezoidals but triangles","metadata":{}},{"cell_type":"code","source":"DIAGNOSIS['B'] = fuzz.trimf(DIAGNOSIS.universe, [0, 0, 1])\nDIAGNOSIS['M'] = fuzz.trimf(DIAGNOSIS.universe, [0, 1, 1])\nDIAGNOSIS.view()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:01.147987Z","iopub.execute_input":"2021-05-20T09:46:01.148602Z","iopub.status.idle":"2021-05-20T09:46:01.308891Z","shell.execute_reply.started":"2021-05-20T09:46:01.148533Z","shell.execute_reply":"2021-05-20T09:46:01.30794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we need to define our rules, based on the research mentioned above out of 16 combination of rules we have, only 2 of them have an output and the other 14 have undefined output neither Malignant nor Bengin\n\nso we're only going to define these 2 rules","metadata":{}},{"cell_type":"code","source":"rule1 = ctrl.Rule(AREA['Smaller'] & PERIMETER['Smaller'] & UNIFORMITY['Smaller'] & HOMOGENEITY['Smaller'], DIAGNOSIS['B'])\nrule2 = ctrl.Rule(AREA['Larger'] & PERIMETER['Larger'] & UNIFORMITY['Larger'] & HOMOGENEITY['Larger'], DIAGNOSIS['M'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:01.310206Z","iopub.execute_input":"2021-05-20T09:46:01.31055Z","iopub.status.idle":"2021-05-20T09:46:01.316075Z","shell.execute_reply.started":"2021-05-20T09:46:01.310513Z","shell.execute_reply":"2021-05-20T09:46:01.314995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now Let's create a Control Systems using these 2 rules","metadata":{}},{"cell_type":"code","source":"Diag_ctrl = ctrl.ControlSystem([rule1, rule2])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:01.317418Z","iopub.execute_input":"2021-05-20T09:46:01.317761Z","iopub.status.idle":"2021-05-20T09:46:01.328521Z","shell.execute_reply.started":"2021-05-20T09:46:01.317724Z","shell.execute_reply":"2021-05-20T09:46:01.327622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And in order to simulate this control system, we will create a ControlSystemSimulation","metadata":{}},{"cell_type":"code","source":"Diag = ctrl.ControlSystemSimulation(Diag_ctrl)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:01.329676Z","iopub.execute_input":"2021-05-20T09:46:01.330136Z","iopub.status.idle":"2021-05-20T09:46:01.339121Z","shell.execute_reply.started":"2021-05-20T09:46:01.3301Z","shell.execute_reply":"2021-05-20T09:46:01.338256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now as we've created our Fuzzy System, Let's use it to classify our dataset","metadata":{}},{"cell_type":"code","source":"# A list to store predections generated by the fuzzy system\nfuzzy_preds = []\n# A list to store the equivilant real label, that's because we're skipping some rows, we'll talk why\nfuzzy_real_vals = []\n\n# looping over the rows of the dataset\nfor index, row in fuzzy_data.iterrows():\n    \n    #assigning antecedents values\n    Diag.input['AREA'] = row['area_mean']\n    Diag.input['PERIMETER'] = row['perimeter_mean']\n    Diag.input['UNIFORMITY'] = row['uniformity']\n    Diag.input['HOMOGENEITY'] = row['homogeneity']\n    '''\n    here we'll try to compute the output of the fuzzy system\n    but why TRY ? why not compute it directly\n    do you remember when we said we have a total of 16 rules for our fuzzy system\n    and we're using only 2 of them becuase the rest outputs Undefined\n    these undefined values will cause some errors so we'll just ignore these rows\n    that why we're using Try Except, ok let's continue :D\n    '''\n    try:\n        Diag.compute()\n        \n        #as we said the fuzzy system outputs value in range [0:1]\n        #so here we discretize it and then store it\n        fuzzy_preds.append(Diag.output['DIAGNOSIS'] > 0.5)\n        \n        fuzzy_real_vals.append(y[index])\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:01.340301Z","iopub.execute_input":"2021-05-20T09:46:01.340786Z","iopub.status.idle":"2021-05-20T09:46:45.738039Z","shell.execute_reply.started":"2021-05-20T09:46:01.340745Z","shell.execute_reply":"2021-05-20T09:46:45.737146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's see how many rows got skipped","metadata":{}},{"cell_type":"code","source":"print(len(y) - len(fuzzy_preds))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:45.739305Z","iopub.execute_input":"2021-05-20T09:46:45.739654Z","iopub.status.idle":"2021-05-20T09:46:45.746381Z","shell.execute_reply.started":"2021-05-20T09:46:45.739618Z","shell.execute_reply":"2021-05-20T09:46:45.745362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now let's see the result's of our fuzzy system, shall we!","metadata":{}},{"cell_type":"code","source":"cm_fuzz = confusion_matrix(fuzzy_preds, fuzzy_real_vals)\nsns.heatmap(cm_fuzz,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:45.74765Z","iopub.execute_input":"2021-05-20T09:46:45.748064Z","iopub.status.idle":"2021-05-20T09:46:45.962552Z","shell.execute_reply.started":"2021-05-20T09:46:45.748011Z","shell.execute_reply":"2021-05-20T09:46:45.961748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Fuzzy System accuracy is {}%\".format(((cm_fuzz[0][0] + cm_fuzz[1][1])/cm_fuzz.sum())*100))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:45.963783Z","iopub.execute_input":"2021-05-20T09:46:45.964125Z","iopub.status.idle":"2021-05-20T09:46:45.969487Z","shell.execute_reply.started":"2021-05-20T09:46:45.964087Z","shell.execute_reply":"2021-05-20T09:46:45.968248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, Not Bad Right?","metadata":{}},{"cell_type":"markdown","source":"## 3. Neural Network Approach","metadata":{}},{"cell_type":"markdown","source":"In this approach we'll use a simple Neural Network with only 3 layers using Keras library, so let's start","metadata":{}},{"cell_type":"markdown","source":"First we import some libraries to be used","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:16.648651Z","iopub.execute_input":"2021-05-20T11:13:16.648968Z","iopub.status.idle":"2021-05-20T11:13:16.653146Z","shell.execute_reply.started":"2021-05-20T11:13:16.648937Z","shell.execute_reply":"2021-05-20T11:13:16.652143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we create a sequential model to add layers to","metadata":{}},{"cell_type":"code","source":"NNmodel = Sequential()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:17.567887Z","iopub.execute_input":"2021-05-20T11:13:17.568242Z","iopub.status.idle":"2021-05-20T11:13:17.578Z","shell.execute_reply.started":"2021-05-20T11:13:17.568209Z","shell.execute_reply":"2021-05-20T11:13:17.57716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now We'll add our 3 layers to the model, the 3 layers will be Dense layers, the output layer will have only 1 unit becuase we're doing binary classification, also we add a dropout after each layer of the first 2 to avoid over fitting","metadata":{}},{"cell_type":"code","source":"#We add the input layer\nNNmodel.add(Input(shape=(30,)))\n\n#we add our first hidden layer and a dropout to avoid overfitting\nNNmodel.add(Dense(30, activation='relu', kernel_initializer='uniform'))\nNNmodel.add(Dropout(0.1))\n\n\n#we add the second layer and a dropout to avoid overfitting\nNNmodel.add(Dense(16, activation='relu', kernel_initializer='uniform'))\nNNmodel.add(Dropout(0.1))\n\n\n#now we add the output layer with a sigmoid activation cuz it's a binary classification\nNNmodel.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:18.979878Z","iopub.execute_input":"2021-05-20T11:13:18.980245Z","iopub.status.idle":"2021-05-20T11:13:19.013308Z","shell.execute_reply.started":"2021-05-20T11:13:18.98021Z","shell.execute_reply":"2021-05-20T11:13:19.012521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's Look at a summary of our model before compiling it","metadata":{}},{"cell_type":"code","source":"NNmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:20.829841Z","iopub.execute_input":"2021-05-20T11:13:20.830184Z","iopub.status.idle":"2021-05-20T11:13:20.838219Z","shell.execute_reply.started":"2021-05-20T11:13:20.830152Z","shell.execute_reply":"2021-05-20T11:13:20.837129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's compile our model","metadata":{}},{"cell_type":"code","source":"NNmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:23.859014Z","iopub.execute_input":"2021-05-20T11:13:23.85937Z","iopub.status.idle":"2021-05-20T11:13:23.87208Z","shell.execute_reply.started":"2021-05-20T11:13:23.859339Z","shell.execute_reply":"2021-05-20T11:13:23.871215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train it using our training data","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n#first we create a checkpointer to store best epochs\ncheckpointer = ModelCheckpoint(\n    filepath=\"NNweights\",\n    save_weights_only=True,\n    monitor='accuracy',\n    mode='max',\n    save_best_only=True)\n\n#we'll set epochs to 300, it won't take long\nhistory = NNmodel.fit(X_train, y_train, batch_size=100, epochs=300, callbacks=[checkpointer])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:46:53.459295Z","iopub.execute_input":"2021-05-20T09:46:53.459769Z","iopub.status.idle":"2021-05-20T09:47:00.346713Z","shell.execute_reply.started":"2021-05-20T09:46:53.459732Z","shell.execute_reply":"2021-05-20T09:47:00.345951Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's classify our test data and discretize the results","metadata":{}},{"cell_type":"code","source":"\n#first we load the best saved weights\nNNmodel.load_weights(\"NNweights\")\n\nnn_pred = NNmodel.predict(X_test)\nnn_pred = (nn_pred > 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:47:00.348225Z","iopub.execute_input":"2021-05-20T09:47:00.348604Z","iopub.status.idle":"2021-05-20T09:47:00.471877Z","shell.execute_reply.started":"2021-05-20T09:47:00.348564Z","shell.execute_reply":"2021-05-20T09:47:00.470923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see a confusion matrix and some accuracy","metadata":{}},{"cell_type":"code","source":"cm_nn = confusion_matrix(y_test, nn_pred)\nsns.heatmap(cm_nn,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:47:00.473196Z","iopub.execute_input":"2021-05-20T09:47:00.473534Z","iopub.status.idle":"2021-05-20T09:47:00.679196Z","shell.execute_reply.started":"2021-05-20T09:47:00.473484Z","shell.execute_reply":"2021-05-20T09:47:00.678371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Neural Network accuracy is {}%\".format(((cm_nn[0][0] + cm_nn[1][1])/cm_nn.sum())*100))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T09:47:00.680424Z","iopub.execute_input":"2021-05-20T09:47:00.680759Z","iopub.status.idle":"2021-05-20T09:47:00.687699Z","shell.execute_reply.started":"2021-05-20T09:47:00.68072Z","shell.execute_reply":"2021-05-20T09:47:00.686495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Neural Network Trained With Genetic Algorithms","metadata":{}},{"cell_type":"markdown","source":"This is a Hybrid Approach, we'll be using the same previous Neural Network model, but we'll be training it using Genetic algorithms, instead of the usual Adam optimizer and fit function\n\nwe'll be using PyGAD library to do the genetic algorithms stuff","metadata":{}},{"cell_type":"markdown","source":"First lets import some libraries","metadata":{}},{"cell_type":"code","source":"!pip install pygad\nimport tensorflow.keras\nimport pygad.kerasga\nimport numpy\nimport pygad","metadata":{"execution":{"iopub.status.busy":"2021-05-20T10:36:07.406418Z","iopub.execute_input":"2021-05-20T10:36:07.406752Z","iopub.status.idle":"2021-05-20T10:36:13.143612Z","shell.execute_reply.started":"2021-05-20T10:36:07.406718Z","shell.execute_reply":"2021-05-20T10:36:13.142637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we define the fitness function which is used to evaluate each population, since our problem is binary classification we'll use tensorflow.keras.losses.BinaryCrossentropy() as mesurment for the fitness","metadata":{}},{"cell_type":"code","source":"def fitness_func(solution, sol_idx):\n    model_weights_matrix = pygad.kerasga.model_weights_as_matrix(model=NNmodel,\n                                                                 weights_vector=solution)\n    NNmodel.set_weights(weights=model_weights_matrix)\n\n    predictions = NNmodel.predict(X_train)\n    \n    bce = tensorflow.keras.losses.BinaryCrossentropy()\n    solution_fitness = 1.0 / (bce(y_train, predictions).numpy() + 0.00000001)\n\n    return solution_fitness\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:34.99819Z","iopub.execute_input":"2021-05-20T11:13:34.998507Z","iopub.status.idle":"2021-05-20T11:13:35.005266Z","shell.execute_reply.started":"2021-05-20T11:13:34.998475Z","shell.execute_reply":"2021-05-20T11:13:35.00451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also we define some callbacks to show us some statistics while training","metadata":{}},{"cell_type":"code","source":"def callback_generation(ga_instance):\n    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:13:36.452089Z","iopub.execute_input":"2021-05-20T11:13:36.452409Z","iopub.status.idle":"2021-05-20T11:13:36.456724Z","shell.execute_reply.started":"2021-05-20T11:13:36.452381Z","shell.execute_reply":"2021-05-20T11:13:36.455774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initialize the weights vector as a chromosome from the NN Model\nweights_vector = pygad.kerasga.model_weights_as_vector(model=NNmodel)\n\n#Create Genetic Population\nkeras_ga = pygad.kerasga.KerasGA(model=NNmodel,\n                                 num_solutions=100)\n\n\nnum_generations = 100\nnum_parents_mating = 50\ncrossover_type = \"single_point\"\nmutation_type = \"random\"\nmutation_percent_genes = 10\n\ninitial_population = keras_ga.population_weights\n\nga_instance = pygad.GA(num_generations=num_generations, \n                       num_parents_mating=num_parents_mating, \n                       initial_population=initial_population,\n                       fitness_func=fitness_func,\n                       on_generation=callback_generation,\n                       crossover_type=crossover_type,\n                       mutation_type=mutation_type,\n                       mutation_percent_genes=mutation_percent_genes)\nga_instance.run()\n\n# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\nga_instance.plot_result(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n\n# Returning the details of the best solution.\nsolution, solution_fitness, solution_idx = ga_instance.best_solution()\nprint(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\nprint(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T11:14:29.09327Z","iopub.execute_input":"2021-05-20T11:14:29.093595Z","iopub.status.idle":"2021-05-20T11:30:04.616405Z","shell.execute_reply.started":"2021-05-20T11:14:29.093564Z","shell.execute_reply":"2021-05-20T11:30:04.614787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load the best chromosome of weights and classify the test data","metadata":{}},{"cell_type":"code","source":"# Fetch the parameters of the best solution.\nbest_solution_weights = pygad.kerasga.model_weights_as_matrix(model=NNmodel,\n                                                              weights_vector=solution)\n# Set the Weights\nNNmodel.set_weights(best_solution_weights)\n\n#predict\npredictions = NNmodel.predict(X_test)\npredictions = (predictions > 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:04.61798Z","iopub.execute_input":"2021-05-20T11:30:04.618344Z","iopub.status.idle":"2021-05-20T11:30:04.663609Z","shell.execute_reply.started":"2021-05-20T11:30:04.618304Z","shell.execute_reply":"2021-05-20T11:30:04.662743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see a confusion matrix and some accuracy","metadata":{}},{"cell_type":"code","source":"cm_ga = confusion_matrix(y_test, predictions)\nsns.heatmap(cm_ga,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:04.665538Z","iopub.execute_input":"2021-05-20T11:30:04.665886Z","iopub.status.idle":"2021-05-20T11:30:04.890524Z","shell.execute_reply.started":"2021-05-20T11:30:04.665848Z","shell.execute_reply":"2021-05-20T11:30:04.889729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Hybrid Neural Network/Genetic accuracy is {}%\".format(((cm_ga[0][0] + cm_ga[1][1])/cm_ga.sum())*100))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:04.892397Z","iopub.execute_input":"2021-05-20T11:30:04.892741Z","iopub.status.idle":"2021-05-20T11:30:04.89854Z","shell.execute_reply.started":"2021-05-20T11:30:04.892704Z","shell.execute_reply":"2021-05-20T11:30:04.897509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}