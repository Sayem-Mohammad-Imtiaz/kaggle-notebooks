{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThe purpose of this notebook is to practice working with geography (with shapefiles) and time (with datetime types), in the context of the Iowa Liquor Sales dataset. This will also focus on looking specifically at this data on the county level, with the inclusion of additional datasets for Iowa county populations during the 2010's and shapefiles for the Iowa county boundaries. These will allow us to do operations such as normalizing sales by the population in each county and vizualizing metrics associated with each county on a physical map."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport fiona\nimport geopandas\nimport plotnine\nimport warnings\nfrom plotnine import *\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading in the datasets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Reading in all of the files.\ngeo_df = geopandas.read_file(\"/kaggle/input/iowa-county-boundaries/IowaCounties.shp\")\npop_df = pd.read_csv(\"/kaggle/input/iowa-county-populations/IowaCountyPopulations.csv\")\nliq_df = pd.read_csv(\"/kaggle/input/iowa-liquor-sales/Iowa_Liquor_Sales.csv\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The main dataset has the following columns and datatypes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"liq_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional dataset cleaning and organization"},{"metadata":{},"cell_type":"markdown","source":"The cleaned dataset with a subset of just the columns to be utilized looks like this."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Cleaning up some of the columns in the liquor sales dataframe and subsetting with variables of interest.\nliq_df[\"datetime\"] = pd.to_datetime(liq_df[\"Date\"])\nliq_df[\"sale_usd\"] = liq_df[\"Sale (Dollars)\"].map(lambda x: float(str(x).replace(\"$\",\"\").replace(\",\",\"\")), na_action=\"ignore\")\nliq_df[\"city\"] = liq_df[\"City\"].map(lambda x: str(x).lower().strip(), na_action=\"ignore\")\nliq_df[\"county\"] = liq_df[\"County\"].map(lambda x: str(x).lower().replace(\"'\",\"\").strip(), na_action=\"ignore\")\nliq_df[\"category\"] = liq_df[\"Category Name\"].map(lambda x: str(x).lower().strip(), na_action=\"ignore\")\nliq_df[\"year\"] = liq_df[\"datetime\"].dt.year\nliq_df = liq_df.rename({\"Volume Sold (Liters)\":\"volume_liters\"},axis=\"columns\")\nliq_df = liq_df[[\"county\", \"city\", \"datetime\", \"year\", \"sale_usd\", \"category\", \"volume_liters\"]]\nliq_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at just these fist couple rows, there are some columns that have missing values. We can look at the percentage of each column in this dataset that is missing."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"liq_df.isna().apply(lambda x: (x.sum()/len(liq_df))*100).to_frame(name=\"percent_missing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fewer that 1% of each variable consists of missing values. The county variable is missing the most values, with ~0.6% of the values missing. For the purposes of this notebook, we will subset the dataset to only include rows where all these variables are present."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"liq_df.dropna(axis=0, how=\"any\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fixing typos to make the datasets compatible"},{"metadata":{},"cell_type":"markdown","source":"The following values are present in the county column for the main liquor sales datase but not the geography or population sizes datasets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Making sure the county colums in the the other two dataframes will be compatable so we can merge them.\ngeo_df[\"county\"] = geo_df[\"CountyName\"].map(lambda x: x.lower().replace(\"'\",\"\"))\npop_df[\"county\"] = pop_df[\"County\"].map(lambda x: x.lower().replace(\"'\",\"\"))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"counties_in_geo_df = pd.unique(geo_df.county)\ncounties_in_pop_df = pd.unique(pop_df.county)\ncounties_in_liq_df = pd.unique(liq_df.county)\nassert len(set(counties_in_geo_df)-set(counties_in_pop_df)) == 0\nprint(set(counties_in_liq_df)-set(counties_in_pop_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These look like typos that will have to be corrected manually. We can simply map them to the correct spellings and double-check that the counties mentioned in all three datasets are now the same."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"county_name_typo_map = {\"pottawatta\":\"pottawattamie\",\n                       \"cerro gord\":\"cerro gordo\",\n                       \"buena vist\":\"buena vista\"}\nliq_df[\"county\"] = liq_df[\"county\"].map(lambda x: county_name_typo_map.get(x,x))\ncounties_in_geo_df = pd.unique(geo_df.county)\ncounties_in_pop_df = pd.unique(pop_df.county)\ncounties_in_liq_df = pd.unique(liq_df.county)\nassert len(set(counties_in_geo_df)-set(counties_in_pop_df)) == 0\nassert len(set(counties_in_geo_df)-set(counties_in_liq_df)) == 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the dataset"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's look at the number of sales recorded in this dataset during each year the dataset covers, as well as the total amount of sales in terms of USD."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Making a plot of the total number of sales logged during each year in the dataset.\nplot_data = liq_df.groupby(liq_df.datetime.dt.year).size().reset_index()\nplot_data.columns = [\"year\",\"number_of_sales\"]\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,4)\n(ggplot(data=plot_data)\n + aes(x=\"year\", y=\"number_of_sales\")\n + geom_bar(stat=\"identity\", fill=\"lightgray\", color=\"black\", alpha=0.5)\n + scale_x_continuous(breaks=list(range(plot_data[\"year\"].min(), plot_data[\"year\"].max()+1)))\n + theme_bw()\n + ylab(\"Number of Sales\")\n + xlab(\"Year\")\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Making a plot of the total number of sales logged during each year in the dataset.\nplot_data = liq_df.groupby(liq_df.datetime.dt.year)[\"sale_usd\"].sum().reset_index()\nplot_data.columns = [\"year\",\"sale_usd\"]\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,4)\n(ggplot(data=plot_data)\n + aes(x=\"year\", y=\"sale_usd_m\")\n + geom_bar(stat=\"identity\", fill=\"lightgray\", color=\"black\", alpha=0.5)\n + scale_x_continuous(breaks=list(range(plot_data[\"year\"].min(), plot_data[\"year\"].max()+1)))\n + theme_bw()\n + ylab(\"Total Sales (Million USD)\")\n + xlab(\"Year\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also look at the total amount of sales in terms of USD that each city recorded over the entire duration. This figure is showing only the top 40 cities, ordered by total sales amount."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Looking at how much each of the city sold over the entire period.\nnum_cities_to_show = 40\nplot_data = pd.DataFrame(liq_df.groupby(\"city\")[\"sale_usd\"].sum()).reset_index()\nplot_data = plot_data.sort_values(by=\"sale_usd\",ascending=True).tail(num_cities_to_show)\nplot_data[\"city\"] = pd.Categorical(plot_data[\"city\"], categories=plot_data[\"city\"].values)\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,6)\n(ggplot(data=plot_data)\n + aes(x=\"city\", y=\"sale_usd_m\")\n + geom_bar(stat=\"identity\", fill=\"lightgray\", color=\"black\", alpha=0.5)\n + theme_bw()\n + ylab(\"Total Sales (Million USD)\")\n + xlab(\"City\")\n + coord_flip()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can look at the same plot with respect to counties instead.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Looking at how much each of the county sold over the entire period.\nnum_counties_to_show = 40\nplot_data = pd.DataFrame(liq_df.groupby(\"county\")[\"sale_usd\"].sum()).reset_index()\nplot_data = plot_data.sort_values(by=\"sale_usd\",ascending=True).tail(num_cities_to_show)\nplot_data[\"county\"] = pd.Categorical(plot_data[\"county\"], categories=plot_data[\"county\"].values)\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,6)\n(ggplot(data=plot_data)\n + aes(x=\"county\", y=\"sale_usd_m\")\n + geom_bar(stat=\"identity\", fill=\"lightgray\", color=\"black\", alpha=0.5)\n + theme_bw()\n + ylab(\"Total Sales (Million USD)\")\n + xlab(\"County\")\n + coord_flip()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visualize this same information through a geographical view by merging the data for these plots with a dataframe containing a geometry column for specifying the shape of each county."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Now let's visualize the total sales in each county over the entire period.\nplot_data = pd.DataFrame(liq_df.groupby(\"county\")[\"sale_usd\"].sum()).reset_index()\nplot_data = geo_df.merge(right=plot_data, how=\"left\", on=\"county\")\nplot_data = plot_data.dropna(axis=\"index\", how=\"any\")\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\nplot_data[\"centroid_x\"] = plot_data.centroid.x\nplot_data[\"centroid_y\"] = plot_data.centroid.y\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(10,7)\n(ggplot(plot_data)\n + geom_map(aes(fill=\"sale_usd_m\"), show_legend=True)\n + geom_text(aes(x=\"centroid_x\", y=\"centroid_y\", label=\"county\"), size=5)\n + scale_fill_gradient(name=\"Total Sales (Million USD)\", low=\"#CCFF99\", high=\"#FFC300\")\n + theme(legend_position=\"right\", panel_background=element_rect(fill=\"white\"))\n + theme(axis_text=element_blank(), axis_ticks=element_blank(), axis_title=element_blank())\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at just data from one particular year instead, normalize by the population of each county in that year."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Let's take just one particular year, and normalize sales in each county during\n# that year by the population of that county estimate for that year.\nplot_data = liq_df[liq_df[\"year\"]==2016]\nplot_data = pd.DataFrame(plot_data.groupby(\"county\")[\"sale_usd\"].sum()).reset_index()\nplot_data = geo_df.merge(right=plot_data, how=\"left\", on=\"county\")\nplot_data = plot_data.dropna(axis=\"index\", how=\"any\")\nplot_data[\"centroid_x\"] = plot_data.centroid.x\nplot_data[\"centroid_y\"] = plot_data.centroid.y\nplot_data = plot_data.merge(right=pop_df[[\"county\",\"2016\"]], on=\"county\", how=\"left\")\nplot_data = plot_data.rename({\"2016\":\"population_in_2016\"}, axis=\"columns\")\nplot_data\nplot_data[\"sale_usd_per_person\"] = plot_data[\"sale_usd\"]/plot_data[\"population_in_2016\"]\nplot_data.head()\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(10,7)\n(ggplot(plot_data)\n + geom_map(aes(fill=\"sale_usd_per_person\"), show_legend=True)\n + geom_text(aes(x=\"centroid_x\", y=\"centroid_y\", label=\"county\"), size=5)\n + scale_fill_gradient(low=\"#CCFF99\", high=\"#FFC300\", name=\"2016 Sales (USD per Person)\")\n + theme(legend_position=\"right\", panel_background=element_rect(fill=\"white\"))\n + theme(axis_text=element_blank(), axis_ticks=element_blank(), axis_title=element_blank())\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the total amount of sales in each county is tightly correlated with population size. We can directly look at the relationship between these two variables. Here, Dickinson county is highlighted in a different color to indicate its position, as this is a potential data point of interest when looking at the previous plot."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# We can look at how as expected, total sales are tightly correlated with population.\n# Let's label the data point that looks interesting on the plot above.\n# This is a county with a low population where total sales were higher than would be expected.\nplot_data[\"is_dickinson\"] = plot_data[\"county\"].map(lambda x: (x==\"dickinson\"))\nplot_data[\"population_in_2016_k\"] = plot_data[\"population_in_2016\"]/1000\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,4)\n(ggplot(plot_data)\n + geom_point(aes(x=\"population_in_2016_k\", y=\"sale_usd_m\", fill=\"is_dickinson\"), show_legend=False)\n #+ geom_smooth(aes(x=\"population_in_2016_k\", y=\"sale_usd_m\"), method=\"lm\", se=False)\n + scale_fill_manual(values={True:\"black\",False:\"white\"})\n + theme_bw()\n + ylab(\"2016 Sales (Million USD)\")\n + xlab(\"2016 Population (Thousands)\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also use the datetime column of this dataset to compare the volume of total sales during each day of the week."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"liq_df[\"day_of_week_name\"] = liq_df.datetime.dt.day_name()\nliq_df[\"day_of_week_id\"] = liq_df.datetime.dt.day_of_week","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# What do the total sales look like in terms of cost and volume distributed over days of the week?\nplot_data = liq_df.groupby([\"day_of_week_name\",\"day_of_week_id\"])[\"sale_usd\"].sum().reset_index()\nplot_data = plot_data.sort_values(by=\"day_of_week_id\")\nplot_data[\"day_of_week_name\"] = pd.Categorical(plot_data[\"day_of_week_name\"], categories=plot_data[\"day_of_week_name\"].values)\nplot_data[\"sale_usd_m\"] = plot_data[\"sale_usd\"]/1000000\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,4)\n(ggplot(data=plot_data)\n + aes(x=\"day_of_week_name\", y=\"sale_usd_m\")\n + geom_bar(stat=\"identity\", fill=\"lightgray\", color=\"black\", alpha=0.5)\n + theme_bw()\n + ylab(\"Total Sales (Million USD)\")\n + xlab(\"Day of the Week\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of looking at the total sales for each day of the week in this dataset, we can also look at the mean for each county, and the spread of the total sales for all the counties on each day, to get a sense of how consisent this pattern is across all of the counties in the state. Not that in this case, the y-axis now refers to the fraction of total sales for a given county on all days of the week, rather than raw amount."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_data = liq_df.groupby([\"county\",\"day_of_week_name\",\"day_of_week_id\"])[\"sale_usd\"].sum().reset_index()\nplot_data = plot_data.sort_values(by=\"day_of_week_id\", ignore_index=True)\nplot_data[\"day_of_week_name\"] = pd.Categorical(plot_data[\"day_of_week_name\"], categories=plot_data[\"day_of_week_name\"].unique())\n\n# We want to be able to look at sales in each county as a fraction of their sales throughout the week, \n# not just their total number of sales. So let's normalize by the total sale amount in each county.\ncounty_to_total_sales = dict(liq_df.groupby(\"county\")[\"sale_usd\"].sum())\nplot_data[\"total_sale_usd\"] = plot_data[\"county\"].map(county_to_total_sales)\nplot_data[\"day_fraction\"] = plot_data.apply(lambda row: row[\"sale_usd\"]/row[\"total_sale_usd\"], axis=1)\nplot_data.head(10)\n\nplotnine.options.dpi = 100\nplotnine.options.figure_size=(6,4)\n(ggplot(plot_data)\n + geom_boxplot(aes(x=\"day_of_week_name\", y=\"day_fraction\", group=\"day_of_week_id\"), show_legend=True)\n + theme_bw()\n + ylab(\"Fraction of Total Sales\")\n + xlab(\"Day of the week\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, Monday through Wednesday sees the heaviest sales, decreasing over the Thursday to Saturday period. There are interesting outliers in counties where specific days of the week account for nearly all of the recorded. We can sort the dataframe by the fraction of sales for a given day and county to look at which counties these are."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_data.sort_values(by=\"day_fraction\", ascending=False, ignore_index=True)[[\"county\",\"day_of_week_name\",\"day_fraction\"]].head(50)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sum_to_one_check = [(x>0.99 and x<1.01) for x in plot_data.groupby(\"county\")[\"day_fraction\"].sum().values]\nassert all(sum_to_one_check)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}