{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nimport os\nfrom tqdm import tqdm,tqdm_notebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Récuperer 7500 articles depuis le dossier pdf_json\n\npath_data = '../input/CORD-19-research-challenge/document_parses/pdf_json'\ncount = 0\ndocs = []\nfor file in tqdm(os.listdir(path_data)):\n    file_path = f\"{path_data}/{file}\"\n    j = json.load(open(file_path,\"rb\"))\n    paper_id = j['paper_id']\n    # minimizing the id\n    paper_id = paper_id[-7:]\n    title = j['metadata']['title']\n\n    try: \n        abstract = j['abstract'][0]['text']\n    except:\n        abstract = \"\"\n\n    full_text = \"\"\n    bib_entries = []\n    \n    for txt in j['body_text']:\n        full_text += txt['text']\n\n    docs.append([paper_id, title, abstract, full_text])\n\n    count += 1\n\n    if (count >= 5000) :\n        break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Récuperer 7500 articles depuis le dossier pmc_json\n\n\npath_data = '../input/CORD-19-research-challenge/document_parses/pmc_json'\ncount = 0\nfor file in tqdm(os.listdir(path_data)):\n    file_path = f\"{path_data}/{file}\"\n    j = json.load(open(file_path,\"rb\"))\n    paper_id = j['paper_id']\n    # minimizing the id\n    paper_id = paper_id[-7:]\n    title = j['metadata']['title']\n\n    try: \n        abstract = j['abstract'][0]['text']\n    except:\n        abstract = \"\"\n\n    full_text = \"\"\n    bib_entries = []\n    \n    for txt in j['body_text']:\n        full_text += txt['text']\n\n    docs.append([paper_id, title, abstract, full_text])\n\n    count += 1\n\n    if (count >= 5000) :\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ici on va créer un DataFrame où on va regrouper tous ce qu'on a récuperer pour faciliter la manipulation de tout ces données\n\n# Create dataframe containing the files we gathered \nmy_data = pd.DataFrame(docs,columns=['paper_id','title','abstract','body'])\nmy_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(my_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Pendant notre travail sur ces articles, on a remarqué qu'il y a des articles qui ne parle pas du COVID, alors on a décider d'effectuer une recherche sur les termes relatives au COVID afin de garder seulment les articles relatives","metadata":{}},{"cell_type":"code","source":"topic = ['covid','covid19','corona','coronavirus','corona-virus','SARS','SARSCOV2','severe acute resperatory syndrom']\n\nlabels = []\n\nfor abst in tqdm(my_data[\"body\"]):\n    if any(x in abst for x in topic):\n        labels.append(1)\n    else :\n        labels.append(0)\n        \nmy_data['labels'] = labels\n\nmy_data.drop(my_data.index[my_data['labels']==0], inplace = True)\n\n\nlen(my_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En ce qui suit, on va essayer d'éliminer les articles non anglais, car ces derneiers peuvent affecter notre modèle","metadata":{}},{"cell_type":"code","source":"# getting rid of non english articles \n!pip install langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\nDetectorFactory.seed = 0\n\nfor body in tqdm(my_data['body']):\n    try:\n        if detect(body) != \"en\":\n            my_data.drop(my_data.index[my_data['body']==body], inplace = True)\n    except:\n        my_data.drop(my_data.index[my_data['body']==body], inplace = True)\n\n\nlen(my_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calcule de nombre de mots des articles pour choisir les articles qui sont riches en mots","metadata":{}},{"cell_type":"code","source":"# Ici on va devoir analyser les données par savoir les nombre des mots dans le résumé et le body l'article\n\n#my_data[\"nb_mot_abstract\"] = my_data[\"abstract\"].apply(lambda phrase: len(phrase.strip().split()))\nmy_data[\"nb_mot_body\"] = my_data[\"body\"].apply(lambda phrase: len(phrase.strip().split()))\n#my_data[\"nb_mot_body_unique\"] = my_data[\"body\"].apply(lambda phrase: len(set(phrase.strip().split())))\n\nmy_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dans notre étude on va se limiter seulement sur le texte de l'article qui est réferencié dans le DataFrame par \"body\", on garder seulement les articles dont le nombre de mots dépasse 200, sinon on considère l'article incomplet","metadata":{}},{"cell_type":"code","source":"# Delete rows with less than 200 words in full text\nmy_data.drop(my_data.index[my_data['nb_mot_body'] <= 200], inplace = True)\nlen(my_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA CLEANING & PREPARATION","metadata":{}},{"cell_type":"markdown","source":"on va tout d'abord transformer les textes en miniscule pour rechercher sans perte de données, et aussi pour éviter la sensibilité des modèles à la casse par exemple pour un modèle d'apprenstissage automatique **Youssef != youssef**","metadata":{}},{"cell_type":"code","source":"my_data[\"body\"] = my_data[\"body\"].str.lower()\n\nmy_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maintenant il est temps de réduire la dimensionalité de notre données, puisque avec ce nombre immense de mots, il est indispensable de se concentrer sur les mots important","metadata":{}},{"cell_type":"code","source":"# Reduce dimensionnality \n# with this big data (after toeknization) we'll have a lot of words, so in order to accelerate the learning of Neural Network\n# We'll eliminate the unimportant words called stopwords (ex : 'the', 'is' ...)\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# téléchargement les stopwords et ajouter d'autres\nstopwords_custom = nltk.corpus.stopwords.words('english')\n\n\"\"\"\nAjouter d'autres mots qui peuvent étres absents dans la liste mais qui peuvent êtres fréquement utilisés dans les \narticles scientifiques\n\"\"\" \n\nstopwords_custom.extend(\n                        ['common','review','describes','abstract','retrospective','chart','patients','study','may', 'g', 'show',\n                        'associated','results','including','high','found','one','well','among','abstract','provide', 'e', 'shown',\n                        'objective','background','range','features','participates','doi', 'preprint', 'copyright', 'many',\n                        'org', 'https', 'et','al', 'author', 'figure', 'table', 'rights', 'reserved', 'figures', 'reported',\n                        'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'thu',\n                        'elsevier', 'pmc', 'czi', 'editor', 'brazil', 'article', 'figures', 'tables', \"the\", 'a', 'all', 'thus',\n                        'pubmed', 'editors', 'authors', 'methods', 'method', 'result', 'paper', 'introduction', 'editor', \n                         'although', 'letter', 'reviews', 'papers', 'tables', 'addition', 'example', 'even', 'within', 'report']\n                        )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# élimination des ponctuations ( ? ; , \"\" ....)\nfrom nltk.tokenize import RegexpTokenizer\n\nnew_data = pd.DataFrame()\ntokenizer_pattern = RegexpTokenizer('\\w+')\nnew_data['text'] = my_data['body'].apply(lambda x: \" \".join(tokenizer_pattern.tokenize(x.lower())))\nnew_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elimination des stop words de la data,\n\nnew_data['text'] = new_data['text'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords_custom)]))\nnew_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Une petite visualisation des mots les plus fréquents dans notre texte","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Join the different processed titles together.\nlong_string = ','.join(list(new_data['text'].values))\n\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=500, contour_width=3, contour_color='steelblue')\n\n# Generate a word cloud\nwordcloud.generate(long_string)\n\n# Visualize the word cloud\nwordcloud.to_image()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"à ce stade il nous reste que tokeniser les textes comme une dernière étape, et puis on peut construire notre modèle.","metadata":{}},{"cell_type":"code","source":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finalement on va faire une tokenization des textes \n\ndata = new_data['text'].values.tolist()\n\n\ndef tokenize_and_clean(data):\n    for d in data:\n        yield(gensim.utils.simple_preprocess(str(d),deacc=True))\n        \nwords = list(tokenize_and_clean(data))\n\nwords[:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"création des modèle de Bi-Gram et Tri-Gram ","metadata":{}},{"cell_type":"code","source":"bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[words[0]]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lemmatization des mots pour que le modèle comprend les différentes variations des mots","metadata":{}},{"cell_type":"code","source":"# Définition des fonctions utiles\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Céation des BiGrams\n\nbigrams = make_bigrams(words)\n\nnlp = spacy.load('en', disable=['parser','ner'])\nnlp.max_length = 10000000\n\n# lemmatization \n\nlemmatized = lemmatization(bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nlemmatized[:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING THE MODEL","metadata":{}},{"cell_type":"code","source":"from gensim.corpora import Dictionary\n\nid2word = corpora.Dictionary(lemmatized)\n\ntexts = lemmatized\n\ncorpus = [id2word.doc2bow(text) for text in texts]\n\ncorpus[:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\n\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}