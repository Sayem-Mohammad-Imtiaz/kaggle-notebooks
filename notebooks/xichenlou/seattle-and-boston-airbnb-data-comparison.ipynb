{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Seattle and Boston AirBNB homes price and reviews comparison"},{"metadata":{},"cell_type":"markdown","source":"## Importing Data Analysis and Visulization Libraries and Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import both Seattle and Boston datasets and checking the missing values proportion\n**In this project, only listing dataset (including all host lisiting home properties and reveiw scores) is considered** "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# all Seattle related data, named with \"_sl\", all Boston related data, named with \"_bl\" \ndf_sl = pd.read_csv(\"/kaggle/input/seattle/listings.csv\")\ndf_bl = pd.read_csv(\"/kaggle/input/boston/listings.csv\")\n\nnum_rows_s = df_sl.shape[0]\nnum_cols_s = df_sl.shape[1]\n\nnum_rows_b = df_sl.shape[0]\nnum_cols_b = df_sl.shape[1]\n\nmost_missing_cols_s = set(df_sl.columns[df_sl.isnull().mean() > 0.75])\nmost_missing_cols_b = set(df_bl.columns[df_bl.isnull().mean() > 0.75])\n\nprint(num_rows_s, num_cols_s, num_rows_b, num_cols_b)\nprint(most_missing_cols_s, most_missing_cols_b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling First Step: Cleaning all numerical data\n* drop all columns which have all unique values\n* drop all columns which have most missing values\n* convert dollar values to float values\n* convert percentage values to float values\n* introduce new review metric, since the both frequencies and review scores are important in measuring reviews"},{"metadata":{},"cell_type":"markdown","source":"### Check the columns in both datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sl.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Data Cleaning function for Seattle\ndef clean_dataset(df):\n    '''\n    INPUT\n    df - pandas dataframe containing data \n    \n    OUTPUT\n    new_df - cleaned dataset, which contains:\n    1. string containing price are converted into numbers;\n    2. missing values are imputed with mean or mode or drop\n    '''\n    \n    useless_columns = ['access', 'interaction', 'house_rules','name', 'host_name', 'square_feet', 'id', 'host_id','summary', 'space', 'description', 'neighborhood_overview', 'notes', \n                       'host_since', 'host_location', 'host_about', 'host_neighbourhood', 'host_total_listings_count', 'street', 'neighbourhood', \n                       'minimum_nights', 'maximum_nights', 'city', 'zipcode', 'smart_location', 'latitude', \n                       'longitude', 'is_location_exact', 'weekly_price', 'monthly_price', 'require_guest_profile_picture', \n                       'require_guest_phone_verification', 'calculated_host_listings_count', 'availability_30', 'availability_60', 'availability_90', \n                       'availability_365', 'calendar_updated','transit']\n    \n    # if all values are unique in this column, like ID, or if the values are url links, then drop it\n    for col in df.columns:\n        if len(df[col].unique()) == 1:\n            df.drop(col, inplace=True, axis=1)\n        if ('url' in col):\n            df.drop(col, inplace=True, axis=1)\n        if col in useless_columns:\n            df.drop(col, inplace=True, axis=1)\n    \n    # generate review columns\n    review_columns = []\n    for col in df:\n        if 'review' in col:\n            review_columns.append(col)\n    \n    \n    #convert all related 'price' columns values from string to number\n    df['price'] = df['price'].str.replace(\"[$, ]\", \"\").astype(\"float\")\n    df['security_deposit'] = df['security_deposit'].str.replace(\"[$, ]\", \"\").astype(\"float\")\n    df['cleaning_fee'] = df['cleaning_fee'].str.replace(\"[$, ]\", \"\").astype(\"float\")\n    df['extra_people'] = df['extra_people'].str.replace(\"[$, ]\", \"\").astype(\"float\")\n    #convert all percentage columns values to float number\n    df['host_response_rate'] = df['host_response_rate'].str.replace(\"[%, ]\", \"\").astype(\"float\")/100\n    df['host_acceptance_rate'] = df['host_acceptance_rate'].str.replace(\"[%, ]\", \"\").astype(\"float\")/100\n    #generate new review metric\n    df['new_review_metric'] = df['reviews_per_month'] * df['review_scores_rating']/100\n    #drop original review columns\n    df = df.drop(review_columns, axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apply the clean dataset function on the original dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply data cleaning functions above to clean dataset\nclen_df_sl = clean_dataset(df_sl)\nclen_df_bl = clean_dataset(df_bl)\n# 'neighbourhood_group_cleansed' and 'state' are all null in Boston dataset, so we need to drop these two columns in Seattle dataset manually\nclen_df_sl.drop('neighbourhood_group_cleansed', axis=1, inplace = True)\nclen_df_sl.drop('state', axis=1, inplace = True)\nclen_df_bl.drop('market', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check the after data wrangling numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_sl.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_bl.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Wrangling Second Step: Preprocessing the categorical data\n* Preprocessing the complicated multi-catigorical data which has no formal type (like 'amenities' and 'host_verifications')\n* Generate dummy variables for the categorical data and avoid the Dummy Variable Regression to drop one dummy variables\n* Since the real categories of the 'amenities' and 'host_verifications' may not affect the respondent variable, but the number of the categories may matter, generate the length of each rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing the complicated multi-catigories data\n#cat_df_sl = clen_df_sl.copy()\n\ndef element_len(df, colname):\n    coliloc = df.columns.get_loc(colname)\n    \n    for i, row in enumerate(df[colname]):\n        df.iloc[i, coliloc] = row.replace('[', '').replace(\"'\", '').replace(\"]\", '').replace('\"', '').replace('{', '').replace('}', '').replace(' ','')\n        df.iloc[i, coliloc] = len(df.iloc[i, coliloc].split(','))\n    return df\n\ndef create_dummy_df(df, dummy_na):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as categorical\n            2. removes all the original columns in cat_cols\n            3. dummy columns for each of the categorical columns in cat_cols\n            4. if dummy_na is True - it also contains dummy columns for the NaN values\n            5. Use a prefix of the column name with an underscore (_) for separating \n    '''\n    # Dummy the categorical variables\n    cat_cols = ['host_response_time', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'instant_bookable', 'cancellation_policy']\n\n    for col in  cat_cols:\n        try:\n            # for each cat add dummy var, drop original column\n            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n        except:\n            continue\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_sl = element_len(clen_df_sl, 'amenities')\nclen_df_sl = element_len(clen_df_sl, 'host_verifications')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_bl = element_len(clen_df_bl, 'amenities')\nclen_df_bl = element_len(clen_df_bl, 'host_verifications')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic quantitive information from both datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_sl.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_bl.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clen_df_sl = create_dummy_df(clen_df_sl, dummy_na=False)\nclen_df_bl = create_dummy_df(clen_df_bl, dummy_na=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check the after data wrangling categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(clen_df_sl.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(clen_df_bl.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in clen_df_bl:\n    if col not in clen_df_sl:\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q1: What effects do AirBNB homes properties have on prices in both west and east coasts?"},{"metadata":{},"cell_type":"markdown","source":"### 1). The relationship between bed or property type on price"},{"metadata":{"trusted":true},"cell_type":"code","source":"cpsl=sns.catplot(x='property_type', y='price', kind='bar', data=clen_df_sl)\ncpsl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cpbl=sns.catplot(x='property_type', y='price', kind='bar', data=clen_df_bl)\ncpbl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbrpl=sns.catplot(x='bed_type', y='price', col = 'room_type', kind='bar', data=clen_df_sl)\ncbrpl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbrbl=sns.catplot(x='bed_type', y='price', col = 'room_type', kind='bar', data=clen_df_bl)\ncbrbl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2). The relationship among the number of beds, bathrooms, bedrooms, accommodates and price"},{"metadata":{"trusted":true},"cell_type":"code","source":"cbsl=sns.catplot(x='price', y='beds', orient ='h', kind='bar',data=clen_df_sl)\ncbsl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbbl=sns.catplot(x='price', y='beds', orient ='h', kind='bar',data=clen_df_bl)\ncbbl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbasl=sns.catplot(x='price', y='bathrooms', kind='bar', orient = 'h', data=clen_df_sl)\ncbasl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbabl=sns.catplot(x='price', y='bathrooms', kind='bar', orient = 'h', data=clen_df_bl)\ncbabl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"casl=sns.catplot(x='price', y='accommodates', orient = 'h', kind='bar',data=clen_df_sl)\ncasl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cabl=sns.catplot(x='price', y='accommodates', orient = 'h', kind='bar',data=clen_df_bl)\ncabl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbesl=sns.catplot(x='price', y='bedrooms', orient = 'h', kind='bar',data=clen_df_sl)\ncbesl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbebl=sns.catplot(x='price', y='bedrooms', orient = 'h', kind='bar',data=clen_df_bl)\ncbebl.set_xticklabels(rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q2: What hosts' behaviors or profiles would influence AirBNB tenants reviews in both west and east coasts?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generata new behavior_review dataframe for analysis\nbehavior_review_bl_cols =  ['host_response_rate', 'host_acceptance_rate',\n                        'host_response_time_within a day',\n                        'host_response_time_within a few hours',\n                        'host_response_time_within an hour',\n                        'host_has_profile_pic_t', \n                        'host_identity_verified_t', \n                        'host_is_superhost_t', \n                        'instant_bookable_t', \n                        'cancellation_policy_moderate',\n                        'cancellation_policy_strict',\n                        'cancellation_policy_super_strict_30',\n                        'amenities',\n                        'host_verifications',\n                        'guests_included', 'extra_people', 'price',\n                        'new_review_metric']\n\nbehavior_review_sl_cols = behavior_review_bl_cols.copy()\nbehavior_review_sl_cols.remove('cancellation_policy_super_strict_30')\n\nbehavior_review_sl = clen_df_sl[behavior_review_sl_cols].copy()\nbehavior_review_bl = clen_df_bl[behavior_review_bl_cols].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation between Seattle's review and behavior features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = behavior_review_sl.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.rcParams['figure.figsize'] = [11, 9]\nsns.heatmap(corr, mask=mask, annot = True, fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation between Boston's review and behavior features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = behavior_review_bl.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.rcParams['figure.figsize'] = [11, 9]\nsns.heatmap(corr, mask=mask, annot = True, fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q3: Which AirBNB listing property is the most important one in reviews?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy the cleaned dataset\nreview_df_sl= clen_df_sl.copy()\nreview_df_bl= clen_df_bl.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fin_clean_data(df):\n    '''\n    INPUT\n    df - pandas dataframe \n    \n    OUTPUT\n    X - A matrix holding all of the variables you want to consider when predicting the response\n    y - the corresponding response vector\n    \n    Perform to obtain the correct X and y objects\n    This function cleans df using the following steps to produce X and y:\n    1. Drop all the rows with no salaries\n    2. Create X as all the columns that are not the Salary column\n    3. Create y as the Salary column\n    4. Drop the Salary, Respondent, and the ExpectedSalary columns from X\n    5. For each numeric variable in X, fill the column with the mean value of the column.\n    6. Create dummy columns for all the categorical variables in X, drop the original columns\n    '''\n    # drop irrelavent variables\n    irrelavent_cols = ['cleaning_fee', 'security_deposit', 'host_verifications']\n    \n    for col in  irrelavent_cols:\n        # for each cat add dummy var, drop original column\n        df = df.drop(col, axis=1)\n    \n    # Drop rows with missing salary values\n    df = df.dropna(subset=['new_review_metric'], axis=0)\n    y = df['new_review_metric']\n    \n    # Fill numeric columns with the mean\n    num_vars = df.select_dtypes(include=['float', 'int']).columns\n    for col in num_vars:\n        df[col].fillna((df[col].mean()), inplace=True)\n        \n\n\n    # Dummy the categorical variables\n    cat_vars = df.select_dtypes(include=['object']).copy().columns\n    for var in  cat_vars:\n    #    # for each cat add dummy var, drop original column\n        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n    \n\n    \n    X = df.drop(['new_review_metric'], axis=1)\n    #X = df['host_acceptance_rate'].values\n\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the above function to finalize the data preprocessing for X and y\nX_sl, y_sl = fin_clean_data(review_df_sl)\nX_bl, y_bl = fin_clean_data(review_df_bl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spliting the training set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into train and test\nX_train_sl, X_test_sl, y_train_sl, y_test_sl = train_test_split(X_sl, y_sl, test_size=0.3, random_state=42)\nX_train_bl, X_test_bl, y_train_bl, y_test_bl = train_test_split(X_bl, y_bl, test_size=0.3, random_state=42)\n\nsc = StandardScaler()\nX_train_sl = sc.fit_transform(X_train_sl)\nX_test_sl = sc.transform(X_test_sl)\nX_train_bl = sc.fit_transform(X_train_bl)\nX_test_bl = sc.transform(X_test_bl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate the Random Forest Regressor for data modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_sl = RandomForestRegressor(n_estimators=100, \n                               criterion='mse', \n                               random_state=42, \n                               n_jobs=-1)\nregressor_bl = RandomForestRegressor(n_estimators=100, \n                               criterion='mse', \n                               random_state=42, \n                               n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_sl.fit(X_train_sl, y_train_sl.squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_bl.fit(X_train_bl, y_train_bl.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The model accurracies and validation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_sl_preds = regressor_sl.predict(X_train_sl)\ny_test_sl_preds = regressor_sl.predict(X_test_sl)\n\nprint('Random Forest MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train_sl, y_train_sl_preds),\n        mean_squared_error(y_test_sl, y_test_sl_preds)))\nprint('Random Forest R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train_sl, y_train_sl_preds),\n        r2_score(y_test_sl, y_test_sl_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_bl_preds = regressor_bl.predict(X_train_bl)\ny_test_bl_preds = regressor_bl.predict(X_test_bl)\n\nprint('Random Forest MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train_bl, y_train_bl_preds),\n        mean_squared_error(y_test_bl, y_test_bl_preds)))\nprint('Random Forest R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train_bl, y_train_bl_preds),\n        r2_score(y_test_bl, y_test_bl_preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature importancies ranking plot for both Seattle's and Boston's reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = regressor_sl.feature_importances_\nfeat_names = X_sl.columns\ntree_result = pd.DataFrame({'feature': feat_names, 'importance': importances})\ntree_result_sort = tree_result.sort_values(by='importance',ascending=False)[:10]\nchart = sns.catplot(x='feature', y='importance', kind='bar', data=tree_result_sort)\nchart.set_xticklabels(rotation=45, horizontalalignment='right')\n#chart.set_titles(\"Seattle's feature importances analysis for reviews\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = regressor_bl.feature_importances_\nfeat_names = X_bl.columns\ntree_result = pd.DataFrame({'feature': feat_names, 'importance': importances})\ntree_result_sort = tree_result.sort_values(by='importance',ascending=False)[:10]\nchart = sns.catplot(x='feature', y='importance', kind='bar', data=tree_result_sort)\nchart.set_xticklabels(rotation=45, horizontalalignment='right')\n#chart.set_titles(\"Boston's feature importances analysis for reviews\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}