{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi, I'm going to apply 5 supervised machine learning classification models on the given dataset to classify mushrooms as poisonous or edible.\n1. Logistic Regression\n2. K-Nearest Neighbours(K-NN)\n3. Naive Bayes classifier\n4. Decision Tree Classifier\n5. Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"I'll proceed by converting categorical variables into dummy/indicator variables, then applying 3 feature selection techniques to reduce 23 categorical variables (which will become 95 variables after conversion to dummy variables) to only 20 variables and choose the best feature elemination technique for given dataset. Then training different classification models over these 20 features. Here  the goal is to choose best feature selection technique for such datasets with optimum accuracy.\n"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset contains discrete values for each variable. So, standardization/normalization should not be applied on this."},{"metadata":{},"cell_type":"markdown","source":"### Getting information of the data"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Describing the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class is dependent variable and rest are independent variables"},{"metadata":{},"cell_type":"markdown","source":"### Checking whether the data is equally distributed between poisonous (p) and edible (e)\n"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above classes are not imbalanced. To use such discrete features we'll first encode these to natural numbers using LabelEncoder then One-Hot-Encoding will be applied."},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Data"},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nfor c in df.columns:\n    df[c]=label.fit_transform(df[c])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 = p , 0 = e"},{"metadata":{},"cell_type":"markdown","source":"### Separating dependent and independent variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"x = df.drop('class', axis=1)\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One Hot Encoding"},{"metadata":{"trusted":false},"cell_type":"code","source":"x = pd.get_dummies(x,columns=x.columns ,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Feature Selection Techniques"},{"metadata":{},"cell_type":"markdown","source":"Now, we have 95 features, let's try some feature selection techniques to extract useful features."},{"metadata":{},"cell_type":"markdown","source":"## 1. Selecting features with highest correlation with independent variable (y)"},{"metadata":{},"cell_type":"markdown","source":"### Making list of correlation values"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr = []\nfor i in range(x.shape[1]):\n    c = np.corrcoef(x.iloc[:,i],y)\n    corr.append(abs(c[0][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making DataFrame of correaltion values"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_data = pd.DataFrame({'correlation': corr}, index=x.columns)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"corr_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of corr DataFrame"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,9))\nsns.barplot(x=corr_data.index, y = corr_data['correlation'])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph we can conclude that there are only a few number of features which have higher correlation than most of the features with respect to target feature."},{"metadata":{},"cell_type":"markdown","source":"### Choosing features with correlation values greater than 0.5."},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_data = corr_data.sort_values(by = 'correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_imp = corr_data[corr_data['correlation'] >= 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_X = x[corr_imp.index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining new DataFrame of selected independent variables (x)"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"## Applying Logistic Regression Model (Independent variable = corr_x)"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Dataset into X_train, X_test, y_train and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(corr_X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting Logistic Regression Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions1 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Classification report and Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(predictions1,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(predictions1,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 2. Univariate feature selection"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Dataset into X_train, X_test, y_train and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_indices = np.arange(x.shape[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing SelectKBest method "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Appying SelectKBest with k = 20"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"selector = SelectKBest(chi2, k=20)\nselector.fit(X_train, y_train)\nscores = selector.scores_/1000\n\nplt.figure(figsize=(50,10))\nsns.barplot(data=pd.DataFrame({'Feature':x.columns, 'Scores': scores}),x='Feature',y='Scores',ci=None)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores_data = pd.DataFrame({'Feature':x.columns, 'Scores': scores})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing scores "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(scores_data['Scores'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting 20 Scores with highest value"},{"metadata":{"trusted":false},"cell_type":"code","source":"scores_data = scores_data.sort_values(by = 'Scores',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"scores_x = scores_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores_x = x[scores_x['Feature']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining new DataFrame of selected independent variables (x)"},{"metadata":{"trusted":false},"cell_type":"code","source":"scores_x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## Applying Logistic Regression Model (Independent variable = scores_x)"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Dataset into X_train, X_test, y_train and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scores_x, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting Logistic Regression Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Classification report and Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(predictions,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(predictions,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 3. Recurssive feature elimination (RFE)"},{"metadata":{},"cell_type":"markdown","source":"### Importing RFE"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"estimator = LogisticRegression(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check how many features to preserve with RFE"},{"metadata":{"trusted":false},"cell_type":"code","source":"d = {}\nfor k in range(2, 25,2):  \n    selector = RFE(estimator, n_features_to_select=k, step=2)\n    selector = selector.fit(x, y)\n    selector.support_\n    selector.ranking_\n\n    sel_fea  = [i for i,j in zip(x.columns,selector.ranking_) if j==1]\n\n    x_new = x[sel_fea]\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)\n    from sklearn.linear_model import LogisticRegression\n    classifier = LogisticRegression(n_jobs=-1)\n    classifier.fit(X_train, y_train)\n    y_pred1 = classifier.predict(X_test)\n\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(y_pred1,y_test)\n    print(\"features: %s\"%k, \" Accuracy: %f\"%acc)\n    d[str(k)]=acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying RFE with 20 features"},{"metadata":{"trusted":false},"cell_type":"code","source":"selector = RFE(estimator, n_features_to_select=20, step=2)\nselector = selector.fit(x, y)\nselector.support_\nselector.ranking_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sel_fea  = [i for i,j in zip(x.columns,selector.ranking_) if j==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_new = x[sel_fea]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining new DataFrame of selected independent variables (x)"},{"metadata":{"trusted":false},"cell_type":"code","source":"x_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## Applying Logistic Regression Model (Independent variable = scores_x)"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Dataset into X_train, X_test, y_train and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting Logistic Regression Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = LogisticRegression(n_jobs=-1)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred1 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting Accuracy score and confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred1,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_pred1,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy scores of Logistic Regression with above mentioned feature selection techniques."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('correlation :')\nprint(accuracy_score(predictions1,y_test))\nprint('selectKBest :')\nprint(accuracy_score(predictions,y_test))\nprint('RFE :' )\nprint(accuracy_score(y_pred1,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying 5 classification models"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Dataset into X_train, X_test, y_train and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### Importing Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = LogisticRegression(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting Logistic Regression Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred1 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Classification report and Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred1,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_pred1,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 2. KNN Classifier"},{"metadata":{},"cell_type":"markdown","source":"### Importing KNN Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting KNN Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting the test set results"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred2 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making classification report and confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 3. Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"### Fitting the naive bayes model"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting the test set results"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred3 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making classification report and confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred3))\nprint(confusion_matrix(y_test,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 4. Decision Tree classification"},{"metadata":{},"cell_type":"markdown","source":"### Fitting the Decision Tree classification"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting the test set results"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred4 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making classification report and confusion matrix"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred4))\nprint(confusion_matrix(y_test,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## 5. Random Forest Classification"},{"metadata":{},"cell_type":"markdown","source":"### Fitting the Random Forest Classification"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10,criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting the test set results"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred5 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making classification report and confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_pred5))\nprint(confusion_matrix(y_test,y_pred5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"### Making DataFrame of all the predictions made by 5 models with recpect to actual target value (y_test)"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame({'y_test': y_test,'logistic_reg': y_pred1, 'KNN': y_pred2, 'Naive_Bayes': y_pred3\n                  , 'Decision Tree': y_pred4, 'Random Forest': y_pred5})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating Accuracy Scores of above mentioned Classification models."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in df.columns[1:]:\n    print(i+': ',accuracy_score(df['y_test'], df[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So, we conclude that on this dataset RFE performed best among different feature selection techniques and successfully reduced number of variables without hampering accuracy.\nIf you like my work, an upvote will motivate me to persue this never ending ML/Data Science journey.\nI am new to this field, if you feel I made some mistakes or have any suggestions please comment. I trust this community will help me to hone my skills."},{"metadata":{},"cell_type":"markdown","source":"  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}