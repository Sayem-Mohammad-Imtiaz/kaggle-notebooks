{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c77cbb46fd0cd7bfd2b8fb86c716b09a13a1a841"},"cell_type":"markdown","source":"> Lets Import the heart.csv file into a pandas dataframe\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06397f0b9cc2cba7ed85c6625ac999edcbc2c48e"},"cell_type":"markdown","source":"First of all we need an insight of the data what it contains and how to interpret the data into a more meaningful statistics. So we will first check the contents using head. Head() will give 5 rows as default from the top. We can also use tail, which provides data from the bottom. For now we will stick to head(). "},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"4c732b7a1053b120284f8a0d761e11709ca06cb6"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3c9a4e6a08b94d993a3d6126fa6dc72d09e4443"},"cell_type":"markdown","source":"\nage :            age in years\n\nsex:            (1 = male; 0 = female)\n\ncp:              chest pain type\n\ntrestbps       resting blood pressure (in mm Hg on admission to the hospital)\n\ncholserum   cholestoral in mg/dl\n\nfbs(fasting blood sugar > 120 mg/dl): (1 = true; 0 = false)\n\nrestecg       resting electrocardiographic results\n\nthalachmaximum heart rate achieved\n\nexangexercise induced angina (1 = yes; 0 = no)\n\noldpeakST depression induced by exercise relative to rest\n\nslope        the slope of the peak exercise ST segment\n\nca             number of major vessels (0-3) colored by flourosopy\n\nthal           3 = normal; 6 = fixed defect; 7 = reversable defect\n\ntarget       1 or 0\n"},{"metadata":{"trusted":true,"_uuid":"3198594e9be0ece3efc05f9260704daaaf945c0a"},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98fe623e8e801bd8bc94b0732e0108831a93e934"},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the shape of the data, i.e count and columns available\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gender distribution in the file using Seaborn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='sex',data=dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gender Ratio:**\n\nLets see percentage wise ratio of dataset for gender."},{"metadata":{"trusted":true,"_uuid":"3023ab971792ca73bd455cc33cc9ea7b227a6877"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nexplode =[0.1,0]\nlabels='Male','Female'\nplt.pie(dataset['sex'].value_counts(),explode=explode,autopct='%1.1f%%',labels=labels,shadow=True,startangle=140)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Chest Pain** : We can see there are different pain type, so lets build a pie chart which will show the data distribution.\n"},{"metadata":{"trusted":true,"_uuid":"34f912609db247814e4190784024e4db704cb51c"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nexplode=[0.1,0,0,0]\nlabels='Pain-Type 0','Pain Type-1','Pain-Type2','Pain-Type3'\nplt.pie(dataset['cp'].value_counts(),explode=explode,labels=labels,autopct='%1.1f%%',shadow=True,startangle=140)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09074b6c4d417d44429f20af997718d1279c46b9"},"cell_type":"code","source":"sns.boxplot(dataset['trestbps'],orient='v',color='Magenta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"107a6715b33ae1cc63522c99762c22e2dacc7302"},"cell_type":"code","source":"sns.boxplot(dataset['chol'],orient='v',color='Magenta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3860a40e2f7cb9711539d0c0b9d08546cf7cdeca"},"cell_type":"code","source":"\n#dataset.plot.scatter(x='age',y='trestbps')\nplt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='trestbps',data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c2fbd39bde94b7068a3874af47d22b3d0743851"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='thalach',data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7679cbf82cec074f965a2800bf1e20981b3d9853"},"cell_type":"code","source":"sns.set()\ncol=['age','trestbps','chol','thalach']\nsns.pairplot(dataset[col])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets build a heat map to check the co relation between variables. From the below it is evident that hardly strong co realtion exists between variables. "},{"metadata":{"trusted":true,"_uuid":"e68b8e11856f5e756c66fb14a8d1c07da9d8acd7"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(dataset.corr(),annot=True,cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Dummy Variables:**\n\nFrom the above we can see there are categorical values, which includes: sex,cp,fbs etc.\n\nSo we will create dummy variables. We will also use prefix so that categorical columns when converted are recognized properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"sex = pd.get_dummies(dataset['sex'],prefix='sex',drop_first=True)\nfbs = pd.get_dummies(dataset['fbs'],prefix='fbs',drop_first=True)\nrestecg = pd.get_dummies(dataset['restecg'],prefix='restecg',drop_first=True)\nexang = pd.get_dummies(dataset['exang'],prefix='exang',drop_first=True)\ncp = pd.get_dummies(dataset['cp'],prefix='cp',drop_first=True)\nslope = pd.get_dummies(dataset['slope'],prefix='slope',drop_first=True)\nthal = pd.get_dummies(dataset['thal'],prefix='thal',drop_first=True)\n\ndataset = pd.concat([dataset,sex,fbs,restecg,exang,cp,slope,thal],axis=1)\n\n\n\n#Will do a quick check if it worked or not :P\ndataset.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the columns since we have already converted the categorical data and taken care the dummy trap above"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns=['sex','fbs','restecg','exang','cp','slope','thal'])\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Predictions**\n\n"},{"metadata":{},"cell_type":"markdown","source":"Extracting the dependent (Y) and X variables.\n\n"},{"metadata":{"trusted":true,"_uuid":"f438bc32ec1a78c4a21da87ac41aea17755f8e5a"},"cell_type":"code","source":"X= dataset.drop('target',axis=1)\ny = dataset['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Test Splitting**\n\nWe will split the data into train test based on 80:20 "},{"metadata":{"trusted":true,"_uuid":"19ede5abca00adfad64896e4d7d87c70cb902529"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standard Scaler**\n\nLets Standarize the data before fitting the data into the model.Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data.\n\n"},{"metadata":{"trusted":true,"_uuid":"f7d135e911f5afb986a26e44bd361421c8bc1e93"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA component **\n\n"},{"metadata":{"trusted":true,"_uuid":"135c9575a7631b1beae45b6c880ddeef6908b02c"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=None,random_state=0)\nX_train = pca.fit_transform(X_train)\nX_test =pca.transform(X_test)\n\npca.explained_variance_ratio_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"79c3d48bc8a2f7c11543914e11a6f202da7f6e14"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nlr_score = lr.score(X_test,y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsv = SVC(kernel ='rbf',random_state=0)\nsv.fit(X_train,y_train)\nsv_pred = sv.predict(X_test)\nsv_score = sv.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_regressor = RandomForestClassifier(n_estimators = 1000, random_state = 0)\nrf_regressor.fit(X_train, y_train)\nrf_pred = rf_regressor.predict(X_test)\nrf_score = rf_regressor.score(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_score = knn.score(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnv = GaussianNB()\nnv.fit(X_train,y_train)\nnv_sc = nv.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Logistic Regression Model Score is \",round(lr_score*100))\nprint(\"SVC Model Score is \",round(sv_score*100))\n#print(\"Decision tree  Regression Model Score is \",round(tr_regressor.score(X_test,y_test)*100))\nprint(\"Random Forest Regression Model Score is \",round(rf_score*100))\n\nprint(\"KNeighbors Classifiers Model score is\",round(knn_score*100))\nprint(\"Naive Bayes model score is\",round(nv_sc*100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation Score with 10 iteration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom sklearn.model_selection import cross_val_score\naccuracies_lr = cross_val_score(estimator = lr,X = X_train,y = y_train,cv = 10)\naccuracies_sv = cross_val_score(estimator = sv,X = X_train,y = y_train,cv = 10)\naccuracies_rf = cross_val_score(estimator = rf_regressor,X = X_train,y = y_train,cv = 10)\n\naccuracies_knn = cross_val_score(estimator = knn,X = X_train,y = y_train,cv = 10)\naccuracies_nv = cross_val_score(estimator = nv,X = X_train,y = y_train,cv = 10)\n\nprint(\"Mean Accuracies based on cross val score for logistic regression\",round(accuracies_lr.mean()*100))\nprint(\"Mean Accuracies based on cross val score for SVM \",round(accuracies_sv.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Random Forest\",round(accuracies_rf.mean()*100))\n\nprint(\"Mean Accuracies based on cross val score for KNN\",round(accuracies_knn.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Naive Bayes\",round(accuracies_nv.mean()*100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix:**\n\nLogistic Regression and Random Forest since this performs a better model in comparison to other\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncm_lr = confusion_matrix(y_test,y_pred)\ncm_lr\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix** for Random Forest is as below\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_rf = confusion_matrix(y_test,rf_pred)\ncm_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nThough there are weak co relation between variables and also exists other model, but Logistic Model and Random Forest much better than other model. \n\n\nPlease ****Upvote**** my work if you like it :)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}