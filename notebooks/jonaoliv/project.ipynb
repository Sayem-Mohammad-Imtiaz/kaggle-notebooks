{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texto_input_classify = '/kaggle/input/coronavirus-covid19-tweets/'\ntexto_input_created = '/kaggle/input/tweets-categories/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_to_categorize1 = pd.read_csv(texto_input_classify+'2020-03-00 Coronavirus Tweets (pre 2020-03-12).CSV')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_categorize1 = df_to_categorize1.loc[(df_to_categorize1['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize1['lang'] == 'en'),:]\nlen(df_to_categorize1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_categorize1['tweet_id'] = df_to_categorize1.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\ndf_to_categorize1 = df_to_categorize1.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n       'reply_to_screen_name', 'is_quote', 'is_retweet',\n       'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n       'friends_count', 'account_lang', 'account_created_at', 'verified',\n       'lang'], axis=1).reset_index(drop=True)\ndf_to_categorize1[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_to_categorize2 = pd.read_csv(texto_input_classify+'2020-03-19 Coronavirus Tweets.CSV')\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-20 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-21 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-22 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-23 Coronavirus Tweets.CSV'), ignore_index= True)\n# len(df_to_categorize2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_to_categorize2 = df_to_categorize2.loc[(df_to_categorize2['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize2['lang'] == 'en'),:]\n# len(df_to_categorize2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_to_categorize2['tweet_id'] = df_to_categorize2.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\n# df_to_categorize2 = df_to_categorize2.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n#        'reply_to_screen_name', 'is_quote', 'is_retweet',\n#        'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n#        'friends_count', 'account_lang', 'account_created_at', 'verified',\n#        'lang'], axis=1).reset_index(drop=True)\n# df_to_categorize2[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_categorize3 = pd.read_csv(texto_input_classify+'2020-03-24 Coronavirus Tweets.CSV')\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-25 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-26 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-27 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-28 Coronavirus Tweets.CSV'), ignore_index= True)\nlen(df_to_categorize3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_categorize3 = df_to_categorize3.loc[(df_to_categorize3['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize3['lang'] == 'en'),:]\nlen(df_to_categorize3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_to_categorize3['tweet_id'] = df_to_categorize3.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\ndf_to_categorize3 = df_to_categorize3.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n       'reply_to_screen_name', 'is_quote', 'is_retweet',\n       'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n       'friends_count', 'account_lang', 'account_created_at', 'verified',\n       'lang'], axis=1).reset_index(drop=True)\ndf_to_categorize3[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorized = pd.read_excel(texto_input_created+'Tweets.xlsx')\ndf_categorized[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_us = df_to_categorize1.loc[df_to_categorize1['country_code']=='US',:]\nprint(len(df_us))\ndf_us = df_us.append(df_to_categorize3.loc[df_to_categorize3['country_code']=='US',:], ignore_index = True)\nprint(len(df_us))\ndf_us = df_us.sample(n=400)\ndf_us[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n#nltk.download('punkt')\nfrom nltk import wordpunct_tokenize\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_english = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -U spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def first_part_process(sentence):\n    sentence = sentence.lower()\n    \n    # stopword and remove puntuationcs\n    stopword_applied = [word for word in nltk.word_tokenize(sentence) if not word in stopwords_english and word.isalnum()] \n    stopword_applied = \" \".join(stopword_applied) # I change it back to full sentence\n\n    # lemma and pos\n    spacy_applied = nlp(stopword_applied)\n    spacy_applied = [word.lemma_ for word in spacy_applied if word.pos_ in ['ADJ','VERB']] # rember fix emojis\n\n    return \" \".join(spacy_applied)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef countWords(tweets):\n    stop_words_included = frozenset(['14th','1st'])\n    cv = CountVectorizer(analyzer=\"word\",stop_words=stop_words_included)\n    word_count_vector=cv.fit_transform(tweets)\n    return word_count_vector, cv\n\ndef define_idfs(word_count_vector):\n    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n    tfidf_transformer.fit(word_count_vector)\n    return tfidf_transformer\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorized['text_processed'] = df_categorized.apply(lambda x: first_part_process(x['text']), axis=1)\nprint(len(df_categorized))\ndf_categorized[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_us['text_processed'] = df_us.apply(lambda x: first_part_process(x['text']), axis=1)\nprint(len(df_us))\ndf_us[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# tf and tf-idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count frequency of terms\nword_count_vector, tf_v = countWords(df_categorized['text_processed'])\nword_count_vector.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_v1 = pd.DataFrame(data=word_count_vector.toarray(),index=np.arange(1, 81),columns=tf_v.get_feature_names())\ndf_tf_v1['cat'] = df_categorized['cat']\nprint(len(df_tf_v1))\ndf_tf_v1[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = tf_v.get_feature_names()\ndic_terms = {} \ndic_terms_pos = {} \ndic_terms_neg = {} \n\nfor f in feature_names:\n    dic_terms[f] = 0\n    dic_terms_pos[f] = 0\n    dic_terms_neg[f] = 0\n\ndef count_tf_terms(row):\n    for key in feature_names:\n        dic_terms[key] += row[key]\n    return 0\n\ndef count_tf_terms_pos(row):\n    if row['cat'] == 'positive ':\n        for key in feature_names:\n            dic_terms_pos[key] += row[key]\n    return 0\n\ndef count_tf_terms_neg(row):\n    if row['cat'] == 'negative':\n        for key in feature_names:\n            dic_terms_neg[key] += row[key]\n    return 0\n    \nb = df_tf_v1.apply(lambda x: count_tf_terms(x), axis=1)\nb = df_tf_v1.apply(lambda x: count_tf_terms_pos(x), axis=1)\nb = df_tf_v1.apply(lambda x: count_tf_terms_neg(x), axis=1)\ndf_terms_counts= pd.DataFrame(dic_terms.items(), columns=['term', 'tf'])\ndf_terms_counts['positive'] = df_terms_counts['term'].map(dic_terms_pos) \ndf_terms_counts['negative'] = df_terms_counts['term'].map(dic_terms_neg) \ndf_terms_counts[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top ten general\ndf_terms_counts.nlargest(10, 'tf')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top ten negative\ndf_terms_counts.nlargest(10, 'negative')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top ten negative\ndf_terms_counts.nlargest(10, 'positive')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']][:10].positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_terms_counts.positive.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata_upper = df_terms_counts.positive.max() + 5\nfig, ax = plt.subplots(figsize=(15,7))\nax.set_ylabel('Tf', fontsize=20)\n\ndf_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']].positive.plot(kind = 'bar', \n                                           ylim = [0, data_upper], \n                                           rot = 0, fontsize = 16, ax=ax)\nax.set_xlabel('Terms', fontsize=20)\nax.set_title('Positive Term Frequency - 80 Tweets', fontsize=24)\nax.set_xticklabels(df_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']].term)\nax.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_transformer = define_idfs(word_count_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count matrix\ncount_vector = tf_v.transform(df_categorized['text_processed'])\n \n# tf-idf scores\ntf_idf_vector = tfidf_transformer.transform(count_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_idf_v1 = pd.DataFrame(data=tf_idf_vector.toarray(),index=np.arange(0, len(df_categorized)),columns=tf_v.get_feature_names())\nprint(tf_idf_vector.shape)\nprint(len(df_categorized))\ndf_tf_idf_v1['tweet_id'] = df_categorized['status_id']\ndf_tf_idf_v1['code'] = df_categorized['code']\ndf_tf_idf_v1['created_at'] = df_categorized['created_at']\ndf_tf_idf_v1['cat_original'] = df_categorized['cat']\ndf_tf_idf_v1[75:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tf_v.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_idf_v1.cat_original.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_idf_v1['cat_ori_en'] = df_tf_idf_v1.apply(lambda x: 1 if str(x['cat_original']) == 'positive ' else 0, axis=1)\ndf_tf_idf_v1[40:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_idf_v1.cat_ori_en.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_idf_v1.iloc[:5,0:562]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble\n\nfrom sklearn import metrics\n\n# define 10-fold cross validation test harness\nseed = 7\nnp.random.seed(seed)\nkfold = StratifiedShuffleSplit(n_splits=10, test_size=0.35, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = {}\nhistory['acc'] = []\nhistory['precision_p'] = []\nhistory['recall_p'] = []\nhistory['precision_n'] = []\nhistory['recall_n'] = []\nhistory['true_values_x'] = []\nhistory['true_values_y'] = []\nhistory['pred_values_y'] = []\n\nmodels_list = []\n\ndef classifier_tree(train_x, valid_x, train_y, valid_y):\n    classifier_tree = DecisionTreeClassifier(random_state= 7)\n#     classifier_tree = ensemble.RandomForestClassifier(random_state= 1)\n    classifier_tree = classifier_tree.fit(train_x, train_y)\n\n    # predict the labels on validation dataset\n    valid_y_pred_tree = classifier_tree.predict(valid_x)\n    \n    accuracy_tree = metrics.accuracy_score(valid_y, valid_y_pred_tree)\n    \n    recall_tree_p = metrics.recall_score(valid_y, valid_y_pred_tree,pos_label=1,zero_division = 0 )\n    precision_tree_p = metrics.precision_score(valid_y, valid_y_pred_tree,pos_label=1,zero_division = 0)\n    \n    recall_tree_n = metrics.recall_score(valid_y, valid_y_pred_tree,pos_label=0, zero_division = 0)\n    precision_tree_n = metrics.precision_score(valid_y, valid_y_pred_tree,pos_label=0, zero_division = 0)\n    \n    history['acc'].append(accuracy_tree)\n    history['precision_p'].append(precision_tree_p)\n    history['recall_p'].append(recall_tree_p)\n    history['precision_n'].append(precision_tree_n)\n    history['recall_n'].append(recall_tree_n)\n    history['true_values_x'].append(valid_x)\n    history['true_values_y'].append(valid_y)\n    history['pred_values_y'].append(valid_y_pred_tree)\n    models_list.append(classifier_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X = df_tf_idf_v1.iloc[:,0:294]\ndf_Y = df_tf_idf_v1['cat_ori_en']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train, test in kfold.split(df_X, df_Y):\n    classifier_tree(df_X.iloc[train],df_X.iloc[test],df_Y[train],df_Y[test])\n\nprint(\"Accuracy kfold=10\", sum(history['acc'])/len(history['acc']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(models_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(history['true_values_y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 12))\nfor i, mod in enumerate(models_list):\n    koko = metrics.plot_roc_curve(mod,history['true_values_x'][i],history['true_values_y'][i],name='ROC fold {}'.format(i+1), lw=3, ax=ax)\n\nplt.show()       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 12))\nfor i, mod in enumerate(models_list):\n    koko = metrics.plot_precision_recall_curve(mod,history['true_values_x'][i],history['true_values_y'][i],name='ROC fold {}'.format(i+1), lw=3, ax=ax)\n\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=history['true_values_y'][9], y_pred=history['pred_values_y'][9]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n                          cmap=sns.cubehelix_palette(as_cmap=True)):\n    \"\"\"\n    This function is modified from: \n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    classes.sort()\n    tick_marks = np.arange(len(classes))    \n    plt.figure(figsize=(4, 4),dpi=115)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm, classes=[0,1], title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# other datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vector = tf_v.transform(df_us['text_processed'])\ntf_idf_vector = tfidf_transformer.transform(count_vector)\ndf_us['cat_pre'] = models_list[7].predict(tf_idf_vector)\ndf_us.loc[:,['text','cat_pre']][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_us.to_csv('./df_us.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}