{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Heart Disease UCI** \n"},{"metadata":{},"cell_type":"markdown","source":"![Cardiovascular Diseases](https://www.who.int/images/default-source/default-album/who-diabetes-vn-ha-nam-who-diabetes-vn-hanoi-dsc07650.tmb-1366v.jpg?sfvrsn=94b9b70a_1%201366w)\n[Source](https://www.who.int/health-topics/cardiovascular-diseases/#tab=tab_1)\n                         Cardiovascular Diseases"},{"metadata":{},"cell_type":"markdown","source":"**Key facts:**\n- CVDs are the number 1 cause of death globally: more people die annually from CVDs than from any other cause.\n- An estimated 17.9 million people died from CVDs in 2016, representing 31% of all global deaths. Of these deaths, 85% are due to heart attack and stroke.\n- Over three quarters of CVD deaths take place in low- and middle-income countries.\n- Out of the 17 million premature deaths (under the age of 70) due to noncommunicable diseases in 2015, 82% are in low- and middle-income countries, and 37% are caused by CVDs.\n- Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n- People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management using counselling and medicines, as appropriate.\n[Source](https://www.who.int/en/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds))"},{"metadata":{},"cell_type":"markdown","source":"**Some Popular Heart Conditions are:**\n- Acute Heart Attack\n- Angina\n- Atherosclerosis\n- Atrial Fibrillation\n- Atrial Septal Defect (ASD)\n- Cardiac Arrest\n- Cardiac Arrhythmia\n- Cardiomyopathy\n- Chest Pain\n- Congenital Heart Disease\n- Coronary Artery Disease in Women\n- Coronary Bypass Surgery\n- Heart Failure\n- Patent Ductus Arteriosus\n- Rheumatic Fever\n- Tetralogy of Fallot\n- Ventricular Septal Defect\n[Source](https://www.apollohospitals.com/departments/heart/diseases-and-conditions)"},{"metadata":{},"cell_type":"markdown","source":"# Importing Required Libraries"},{"metadata":{},"cell_type":"markdown","source":"Here we used some standard libraries like:\n- Pandas: pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for numerical tables and time series. [Source](https://en.wikipedia.org/wiki/Pandas_(software))\n- Numpy: NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.[Source](https://en.wikipedia.org/wiki/NumPy)\n- Sklearn: Sklearn features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.[Source](https://en.wikipedia.org/wiki/Scikit-learn)\n- XGBoost: XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala. It works on Linux, Windows, and macOS. From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting Library\" [Source](https://en.wikipedia.org/wiki/XGBoost)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # for data manipulation and analysis\nimport numpy as np #for numerical computation\n\n#for vizualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Model Building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n#model Evaluation\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking into Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features in our data:**\n- age\n- sex\n- chest pain type (4 values)\n- resting blood pressure\n- serum cholestoral in mg/dl\n- fasting blood sugar > 120 mg/dl\n- resting electrocardiographic results (values 0,1,2)\n- maximum heart rate achieved\n- exercise induced angina\n- oldpeak = ST depression induced by exercise relative to rest\n- the slope of the peak exercise ST segment\n- number of major vessels (0-3) colored by flourosopy\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"},{"metadata":{},"cell_type":"markdown","source":"# Exploratary Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.[Source](https://en.wikipedia.org/wiki/Exploratory_data_analysis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of null values in the dataset are {}'.format(df.isnull().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Our dataset have {} data points and {} features'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Our target variable have {} % of target 0 and {} % of target 1'.format(round(100*df.target.value_counts(normalize = True)[0],2),round(100*df.target.value_counts(normalize = True)[1],2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate. [Source](https://en.wikipedia.org/wiki/Univariate_analysis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(df.target.value_counts().index,100*df.target.value_counts(normalize = True))\nplt.title('Target distribution')\nplt.xlabel('Target')\nplt.ylabel('percentage %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can say that it is a balanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(df.cp.value_counts().index,100*df.cp.value_counts(normalize = True))\nplt.title('Chest pain type distribution')\nplt.xlabel('Chest pain Type',fontsize = 15)\nplt.ylabel('percentage %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.[Source](https://en.wikipedia.org/wiki/Bivariate_analysis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.barplot(df[df.sex == 1].target.value_counts().index,df[df.sex == 1].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Male vs Target')\nplt.subplot(1,2,2)\nsns.barplot(df[df.sex == 0].target.value_counts().index,df[df.sex == 0].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Female vs Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observaton:** Female have high chances of getting heart attack than Male From above graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nsns.barplot(df[df.cp == 0].target.value_counts().index,df[df.cp == 1].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Type 0 vs Target')\nplt.subplot(2,2,2)\nsns.barplot(df[df.cp == 1].target.value_counts().index,df[df.cp == 1].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Type 1 vs Target')\nplt.subplot(2,2,3)\nsns.barplot(df[df.cp == 2].target.value_counts().index,df[df.cp == 2].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Type 2 vs Target')\nplt.subplot(2,2,4)\nsns.barplot(df[df.cp == 3].target.value_counts().index,df[df.cp == 3].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Type 3 vs Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** pain type 0 is less sever then pain type 1,2 and 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.barplot(df[df.fbs == 1].target.value_counts().index,df[df.fbs == 1].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Fasting blood suger = 1 vs Target')\nplt.subplot(1,2,2)\nsns.barplot(df[df.fbs == 0].target.value_counts().index,df[df.fbs == 0].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Fasting blood suger = 0 vs Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** fasting blood sugar doesnot have much impact on target"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nplt.subplot(1,3,1)\nsns.barplot(df[df.restecg == 0].target.value_counts().index,100*df[df.restecg == 0].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Resting electrocardiographic results = 0 vs Target')\nplt.subplot(1,3,2)\nsns.barplot(df[df.restecg == 1].target.value_counts().index,100*df[df.restecg == 1].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Resting electrocardiographic results = 1 vs Target')\nplt.subplot(1,3,3)\nsns.barplot(df[df.restecg == 2].target.value_counts().index,100*df[df.restecg == 2].target.value_counts(normalize = True))\nplt.xlabel('Target')\nplt.ylabel('Percentage %')\nplt.title('Resting electrocardiographic results = 2 vs Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** Resting electrocardiographic results = 1 have more chances of taget = 1"},{"metadata":{},"cell_type":"markdown","source":"# Outlier Analysis"},{"metadata":{},"cell_type":"markdown","source":"In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.[Source](https://en.wikipedia.org/wiki/Outlier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df.trestbps)\nplt.title('Resting blood pressure dist')\nplt.subplot(1,2,2)\nplt.boxplot(df.trestbps)\nplt.title('Resting blood pressure boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df.thalach)\nplt.title('maximum heart rate achieved distribution')\nplt.subplot(1,2,2)\nplt.boxplot(df.thalach)\nplt.title('maximum heart rate achieved boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df.age)\nplt.title('Age distribution')\nplt.subplot(1,2,2)\nplt.boxplot(df.age)\nplt.title('Age boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df.chol)\nplt.title('Serum cholestoral distribution')\nplt.subplot(1,2,2)\nplt.boxplot(df.chol)\nplt.title('Serum cholestoral boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"Multivariate analysis is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time.[Source](https://en.wikipedia.org/wiki/Multivariate_analysis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df.trestbps,df.chol,hue=df.target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df.chol,df.thalach,hue=df.target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df.trestbps,df.thalach,hue=df.target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Analysis"},{"metadata":{},"cell_type":"markdown","source":"![Correlation analysis](https://www.gstatic.com/education/formulas2/-1/en/correlation_coefficient_formula.svg)A correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables. The variables may be two columns of a given data set of observations, often called a sample, or two components of a multivariate random variable with a known distribution.[Source](https://en.wikipedia.org/wiki/Correlation_coefficient)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,10))\nsns.heatmap(df.corr(),annot=True,cmap = 'YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No strong correlation among our variables"},{"metadata":{},"cell_type":"markdown","source":"# Splitting data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('target',1)\ny = df.target\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling Data"},{"metadata":{},"cell_type":"markdown","source":"Min-max scale also known as min-max scaling or min-max normalization, is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data.[Source](https://en.wikipedia.org/wiki/Feature_scaling#:~:text=Also%20known%20as%20min%2Dmax,the%20nature%20of%20the%20data.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the data\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(X_train,columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal'])\nX_test = pd.DataFrame(X_test,columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Buliding and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"![LR](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Exam_pass_logistic_curve.jpeg/400px-Exam_pass_logistic_curve.jpeg)In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.[Source](https://en.wikipedia.org/wiki/Logistic_regression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred_train =lr.predict(X_train)\ny_pred_test = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Metrics:')\naccuracy_train = accuracy_score(y_train,y_pred_train)\ncon_mat_train = confusion_matrix(y_train,y_pred_train)\nf1score_train = f1_score(y_train,y_pred_train)\nprint(f'Accuracy: {accuracy_train}')\nprint(f'Confusion matrix:' )\nprint(con_mat_train)\nprint(f'F1_score: {f1score_train}')\nprint('--------------------------------')\nprint('Test Metrics:')\naccuracy_test = accuracy_score(y_test,y_pred_test)\ncon_mat_test = confusion_matrix(y_test,y_pred_test)\nf1score_test = f1_score(y_test,y_pred_test)\nprint(f'Accuracy: {accuracy_test}')\nprint(f'Confusion matrix:')\nprint(con_mat_test)\nprint(f'F1_score: {f1score_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"![SVC](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png)In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.[Source](https://en.wikipedia.org/wiki/Support_vector_machine)"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train,y_train)\ny_pred_train =svc.predict(X_train)\ny_pred_test = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Metrics:')\naccuracy_train = accuracy_score(y_train,y_pred_train)\ncon_mat_train = confusion_matrix(y_train,y_pred_train)\nf1score_train = f1_score(y_train,y_pred_train)\nprint(f'Accuracy: {accuracy_train}')\nprint(f'Confusion matrix:' )\nprint(con_mat_train)\nprint(f'F1_score: {f1score_train}')\nprint('--------------------------------')\nprint('Test Metrics:')\naccuracy_test = accuracy_score(y_test,y_pred_test)\ncon_mat_test = confusion_matrix(y_test,y_pred_test)\nf1score_test = f1_score(y_test,y_pred_test)\nprint(f'Accuracy: {accuracy_test}')\nprint(f'Confusion matrix:')\nprint(con_mat_test)\nprint(f'F1_score: {f1score_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.explorium.ai/wp-content/uploads/2019/12/Decision-Trees-2.png)A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.[Source](https://en.wikipedia.org/wiki/Decision_tree)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(max_depth=5,min_samples_split=3)\ndtc.fit(X_train,y_train)\ny_pred_train =dtc.predict(X_train)\ny_pred_test = dtc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Metrics:')\naccuracy_train = accuracy_score(y_train,y_pred_train)\ncon_mat_train = confusion_matrix(y_train,y_pred_train)\nf1score_train = f1_score(y_train,y_pred_train)\nprint(f'Accuracy: {accuracy_train}')\nprint(f'Confusion matrix:' )\nprint(con_mat_train)\nprint(f'F1_score: {f1score_train}')\nprint('--------------------------------')\nprint('Test Metrics:')\naccuracy_test = accuracy_score(y_test,y_pred_test)\ncon_mat_test = confusion_matrix(y_test,y_pred_test)\nf1score_test = f1_score(y_test,y_pred_test)\nprint(f'Accuracy: {accuracy_test}')\nprint(f'Confusion matrix:')\nprint(con_mat_test)\nprint(f'F1_score: {f1score_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{},"cell_type":"markdown","source":"![XGBClassifier](https://d1rwhvwstyk9gu.cloudfront.net/2020/02/XG-Boost-FINAL-01.png)XGBoost provides a wrapper class to allow models to be treated like classifiers or regressors in the scikit-learn framework. This means we can use the full scikit-learn library with XGBoost models. The XGBoost model for classification is called XGBClassifier. Models are fit using the scikit-learn API and the model.[Source](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/#:~:text=XGBoost%20provides%20a%20wrapper%20class,for%20classification%20is%20called%20XGBClassifier.&text=Models%20are%20fit%20using%20the%20scikit%2Dlearn%20API%20and%20the%20model.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(learning_rate=0.001)\nxgb.fit(X_train,y_train)\ny_pred_train =xgb.predict(X_train)\ny_pred_test = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Metrics:')\naccuracy_train = accuracy_score(y_train,y_pred_train)\ncon_mat_train = confusion_matrix(y_train,y_pred_train)\nf1score_train = f1_score(y_train,y_pred_train)\nprint(f'Accuracy: {accuracy_train}')\nprint(f'Confusion matrix:' )\nprint(con_mat_train)\nprint(f'F1_score: {f1score_train}')\nprint('--------------------------------')\nprint('Test Metrics:')\naccuracy_test = accuracy_score(y_test,y_pred_test)\ncon_mat_test = confusion_matrix(y_test,y_pred_test)\nf1score_test = f1_score(y_test,y_pred_test)\nprint(f'Accuracy: {accuracy_test}')\nprint(f'Confusion matrix:')\nprint(con_mat_test)\nprint(f'F1_score: {f1score_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{},"cell_type":"markdown","source":"![Random Forest](https://miro.medium.com/max/1200/0*YEwFetXQGPB8aDFV)Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean/average prediction of the individual trees.[Source](https://en.wikipedia.org/wiki/Random_forest)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(max_depth=4,min_samples_split=4)\nrfc.fit(X_train,y_train)\ny_pred_train =rfc.predict(X_train)\ny_pred_test = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Metrics:')\naccuracy_train = accuracy_score(y_train,y_pred_train)\ncon_mat_train = confusion_matrix(y_train,y_pred_train)\nf1score_train = f1_score(y_train,y_pred_train)\nprint(f'Accuracy: {accuracy_train}')\nprint(f'Confusion matrix:' )\nprint(con_mat_train)\nprint(f'F1_score: {f1score_train}')\nprint('--------------------------------')\nprint('Test Metrics:')\naccuracy_test = accuracy_score(y_test,y_pred_test)\ncon_mat_test = confusion_matrix(y_test,y_pred_test)\nf1score_test = f1_score(y_test,y_pred_test)\nprint(f'Accuracy: {accuracy_test}')\nprint(f'Confusion matrix:')\nprint(con_mat_test)\nprint(f'F1_score: {f1score_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here We finalized with Random Forest as our final model Having \n- train accuracy of 88% \n- test accuracy of 91.2% "},{"metadata":{},"cell_type":"markdown","source":"# ***Hope You liked this Kernal please upvote if you found it usefull***"},{"metadata":{},"cell_type":"markdown","source":"#                                ***Thank You*** "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}