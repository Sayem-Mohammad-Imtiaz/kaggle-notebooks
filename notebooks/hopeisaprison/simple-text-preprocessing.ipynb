{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['imdb_master.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\nimport itertools","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Рассмотрим преобразование текстов в удобоваримый для нейронной сети вид.<br>\nА именно:<br>\n    - Текст разбивается на слова (токенизация, знаки препинания считаются словами)<br>\n    - Слова подсчитываются для формирования ограниченного словаря. Каждому слову сопоставляется определеннный номер в словаре. \n    Редким словам назначается специальный номер (эквивалентно замене редких слов на спец. слово <UNK> (неизвестное слово)). \n    - Последовательности слов преобразуются в последовательности номеров слов. Добавляются спец. слова для обозначения начала и конца текста. \n    - Полученные последовательности выравниваются по заданной максимальной длине через обрезание или дополнение номером спец.символа <PAD>\n"},{"metadata":{},"cell_type":"markdown","source":"Класс для хранения текста в виде последовательности номеров и его закодированной метки"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Класс словаря. Метод word2id возвращает номер слова, id2word - наоборот, восстанавливает слово."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Интерфейс объекта, преобразующего тексты в последовательности номеров.\ntransform выполняет преобразование при помощи словаря.\nfit_transform выучивает словарь из текста и возвращает такое же преобразование при помощи свежеполученного словаря."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\n\n","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Простая реализация данного интерфейса. Разбиение на слова производится с помощью библиотеки NLTK.\nВ словаре содержатся несколько спец. слов.\nПосле токенизации, к полученной последовательности слов добавляются слева и справа спец. слова для начала и конца текста."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Строим экземпляр входных данных. Обеспечиваем длину последовательности номеров равной max_seq_len. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])\n        ","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Собираем экземпляры в тензоры"},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_df = pd.read_csv('../input/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])","execution_count":15,"outputs":[{"output_type":"stream","text":"I won't mention any of the plot, because, although it would be highly predictable anyway, the one notable plot twist is given away everywhere, in the movie comm\n[2, 18, 513, 33, 748, 117, 9, 4, 136, 5, 104, 5, 269, 16, 70, 41, 542, 723, 575, 5, 4, 42, 2754, 136, 998, 11, 354, 261, 2629, 5]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tensor.size())\nprint(len(text2id.vocab))","execution_count":18,"outputs":[{"output_type":"stream","text":"torch.Size([23750, 200])\n10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\ntrain_loader = DataLoader(TensorDataset(train_tensor,train_labels),64)\nval_loader = DataLoader(TensorDataset(val_tensor,val_labels),64)\ntest_loader = DataLoader(TensorDataset(test_tensor,test_labels),64)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for xx,yy in train_loader:\n    print(xx)\n    print(yy)\n    break","execution_count":20,"outputs":[{"output_type":"stream","text":"tensor([[   2,   18,  513,  ...,    0,    0,    0],\n        [   2,    4,  202,  ...,    0,    0,    0],\n        [   2,    7,   19,  ..., 9160,    9,  545],\n        ...,\n        [   2,   66,   18,  ...,   11,  996,  102],\n        [   2,   19,  244,  ...,  733,  848,    4],\n        [   2, 8180, 5109,  ...,    0,    0,    0]])\ntensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\nclass intel(nn.Module):\n    def __init__(self):\n        super(intel, self).__init__()\n        self.channel = 100\n        self.embedded = nn.Embedding(10000,100)\n        self.conv1 = nn.Conv1d(100, self.channel, 3)\n        self.pool1 = nn.MaxPool1d(1750)\n        self.norm = nn.BatchNorm1d(self.channel)\n        \n        self.classifier1 = nn.Linear(self.channel, 1)\n        \n    def forward(self,x):\n        x = self.embedded(x)\n        x = x.transpose(2,1)\n        \n        #x = self.norm(x)\n        a = self.conv1(x)\n        a = self.pool1(a)\n        a = a.relu()\n    \n        e = a.view(-1, self.channel) #225*64\n        e = self.classifier1(e)\n        e = e.sigmoid()\n        return e\n    def convweight(self):\n        return self.conv1.weight","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.manual_seed(1488)\nmodel = intel()\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nprint(device)","execution_count":115,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model,train_ds,val_ds,optimizer, epochs, tolerance):\n    running_tolerance = tolerance\n    val_loss_best = 555\n    criterion = nn.BCELoss()\n    for i in range(epochs):\n        model.train()\n        epoch_loss = 0\n        val_loss = 0\n        for xx,yy in train_ds:\n            xx, yy = xx.cuda(), yy.cuda()\n            batchsize = xx.size(0)\n            optimizer.zero_grad()\n            y = model.forward(xx).view(-1)\n            yy = yy.float().view(-1)\n            loss = criterion(y,yy)\n            epoch_loss += loss\n            loss.backward()\n            optimizer.step()\n        epoch_loss /= len(train_loader)\n        with torch.no_grad():\n            model.eval()\n            for xx,yy in val_ds:\n                xx, yy = xx.cuda(), yy.cuda()\n                batchsize = xx.size(0)\n                y = model.forward(xx).view(-1)\n                yy = yy.float()\n                loss = criterion(y,yy)\n                val_loss += loss\n            val_loss /= len(val_loader)\n            status = \"epoch=%d, loss=%f, val_loss=%f, best_loss=%f\" % (i,epoch_loss,val_loss,val_loss_best)\n            print(status)\n            if val_loss<val_loss_best:\n                torch.save(model.state_dict(), \"../best_model.md\")\n                val_loss_best = val_loss\n                running_tolerance = tolerance\n            else:\n                running_tolerance -=1\n                if running_tolerance==0:\n                    print(\"Stop training\")   \n                    break\n                print(\"Running tolerance is \", str(running_tolerance), \"best is \",str(val_loss_best))\n            \n    model.load_state_dict(torch.load(\"../best_model.md\"))    \n    model.eval()\n    model.cpu()\n","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = model.convweight()\nprint(a)","execution_count":117,"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[[-0.0523,  0.0548, -0.0291],\n         [-0.0404, -0.0102, -0.0328],\n         [ 0.0242,  0.0216, -0.0276],\n         ...,\n         [ 0.0167, -0.0573,  0.0436],\n         [ 0.0571,  0.0190,  0.0017],\n         [ 0.0238, -0.0290,  0.0169]],\n\n        [[-0.0458,  0.0513,  0.0389],\n         [ 0.0116, -0.0003, -0.0275],\n         [-0.0516, -0.0180,  0.0542],\n         ...,\n         [-0.0459, -0.0187,  0.0119],\n         [-0.0074, -0.0068,  0.0219],\n         [ 0.0215, -0.0362, -0.0192]],\n\n        [[ 0.0218, -0.0429, -0.0443],\n         [-0.0507,  0.0438, -0.0022],\n         [ 0.0569,  0.0416,  0.0398],\n         ...,\n         [-0.0116, -0.0199, -0.0502],\n         [ 0.0196,  0.0075, -0.0160],\n         [ 0.0120,  0.0514, -0.0421]],\n\n        ...,\n\n        [[-0.0061,  0.0454, -0.0533],\n         [ 0.0382, -0.0172,  0.0371],\n         [ 0.0430,  0.0574, -0.0027],\n         ...,\n         [-0.0004, -0.0110, -0.0112],\n         [ 0.0298, -0.0568, -0.0572],\n         [-0.0160,  0.0010, -0.0230]],\n\n        [[ 0.0400,  0.0471, -0.0214],\n         [-0.0461,  0.0266,  0.0399],\n         [-0.0123, -0.0157,  0.0497],\n         ...,\n         [ 0.0176, -0.0268,  0.0178],\n         [ 0.0432, -0.0440,  0.0563],\n         [ 0.0552,  0.0390, -0.0320]],\n\n        [[ 0.0329, -0.0210, -0.0264],\n         [-0.0549, -0.0502, -0.0406],\n         [ 0.0572,  0.0426,  0.0511],\n         ...,\n         [-0.0182, -0.0109,  0.0445],\n         [-0.0430, -0.0334, -0.0279],\n         [-0.0386,  0.0014, -0.0160]]], device='cuda:0', requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(model,train_loader,val_loader,optimizer,10,tolerance=5)","execution_count":118,"outputs":[{"output_type":"stream","text":"epoch=0, loss=0.497908, val_loss=0.411067, best_loss=555.000000\nepoch=1, loss=0.329436, val_loss=0.453545, best_loss=0.411067\nRunning tolerance is  4 best is  tensor(0.4111, device='cuda:0')\nepoch=2, loss=0.221745, val_loss=0.604020, best_loss=0.411067\nRunning tolerance is  3 best is  tensor(0.4111, device='cuda:0')\nepoch=3, loss=0.170913, val_loss=0.362664, best_loss=0.411067\nepoch=4, loss=0.101298, val_loss=0.417206, best_loss=0.362664\nRunning tolerance is  4 best is  tensor(0.3627, device='cuda:0')\nepoch=5, loss=0.059894, val_loss=0.428038, best_loss=0.362664\nRunning tolerance is  3 best is  tensor(0.3627, device='cuda:0')\nepoch=6, loss=0.031902, val_loss=0.634125, best_loss=0.362664\nRunning tolerance is  2 best is  tensor(0.3627, device='cuda:0')\nepoch=7, loss=0.013461, val_loss=0.857701, best_loss=0.362664\nRunning tolerance is  1 best is  tensor(0.3627, device='cuda:0')\nepoch=8, loss=0.005415, val_loss=0.479222, best_loss=0.362664\nStop training\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = model.convweight()\nprint(a)","execution_count":119,"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[[ 4.1411e-02,  4.9257e-02, -1.0855e-01],\n         [ 1.4669e-01, -8.9354e-02, -7.4858e-02],\n         [-3.9532e-03,  6.1721e-02,  5.2131e-02],\n         ...,\n         [-1.5316e-01,  2.1955e-02,  1.2843e-01],\n         [ 1.8534e-01,  9.7612e-02, -2.2576e-01],\n         [-2.8751e-02, -1.4800e-01,  6.4884e-02]],\n\n        [[-2.0434e-01, -1.1605e-02,  8.3507e-02],\n         [ 6.9467e-02, -8.8488e-03, -1.5448e-01],\n         [-1.4281e-03, -1.6088e-01,  2.3736e-01],\n         ...,\n         [-1.9737e-01,  7.8212e-02,  7.0181e-03],\n         [-8.8084e-02, -4.4708e-02,  1.0634e-01],\n         [-7.9531e-02, -4.2765e-02, -1.4516e-01]],\n\n        [[ 2.7406e-02,  3.7796e-02, -1.2133e-01],\n         [-7.3388e-02, -1.1821e-01, -7.0801e-02],\n         [ 2.4078e-01,  5.2167e-02,  1.0919e-01],\n         ...,\n         [-6.9856e-02, -4.6379e-02, -3.2054e-01],\n         [ 2.2162e-01,  1.5694e-01, -3.5315e-02],\n         [ 7.6887e-03,  5.4617e-02,  2.9532e-02]],\n\n        ...,\n\n        [[-5.1968e-02,  1.8110e-01, -7.0713e-02],\n         [ 2.8806e-02,  4.4696e-03,  9.6924e-02],\n         [-1.7782e-02,  1.3971e-02,  2.5017e-01],\n         ...,\n         [ 1.5913e-01, -5.9470e-02,  3.5601e-02],\n         [-3.2293e-02, -1.0635e-01, -1.6220e-01],\n         [-9.0062e-02,  1.9305e-01, -8.8825e-02]],\n\n        [[-1.7934e-03,  8.7718e-03,  1.6519e-04],\n         [-1.1751e-01,  1.2183e-01,  3.2366e-02],\n         [ 7.7160e-02,  7.3722e-02,  9.5168e-02],\n         ...,\n         [ 2.2413e-02,  9.7626e-02,  1.0800e-01],\n         [ 3.2270e-01,  4.6650e-02,  1.1638e-01],\n         [-1.3136e-02,  4.7061e-02, -8.4055e-02]],\n\n        [[ 1.2079e-02, -1.0608e-01,  5.0324e-02],\n         [-2.1793e-02,  9.3604e-03,  1.4035e-01],\n         [ 1.8813e-01, -1.1370e-01,  2.0377e-01],\n         ...,\n         [ 1.5267e-02,  5.6783e-02,  2.7338e-02],\n         [-1.3189e-01,  4.6122e-02, -3.1081e-02],\n         [-1.4839e-02,  5.1714e-02, -5.8330e-02]]], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.eval()\nall_preds = []\ncorrect_preds = []\nfor xx,yy in test_loader:\n    xx = xx.cuda()\n    model.cuda()\n    y_pred = model.forward(xx)\n    all_preds.extend([i[0]>0.5 for i in y_pred.tolist()])\n    correct_preds.extend(yy.tolist())\nprint(classification_report(correct_preds,all_preds))","execution_count":120,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85     12500\n           1       0.86      0.83      0.84     12500\n\n   micro avg       0.85      0.85      0.85     25000\n   macro avg       0.85      0.85      0.85     25000\nweighted avg       0.85      0.85      0.85     25000\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}