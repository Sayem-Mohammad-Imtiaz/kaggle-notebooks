{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Problem statement \n\nAnalysing breast cancer data and building a classification model to classify the cell of type either malignant or benign based on different cell's features"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing libraries for visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\n\n## Importing libraries for modeling and evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to plot confusion matrix\ndef plot_confusion_matrix(prediction, actual):\n    cm = confusion_matrix(prediction, actual)\n    sns.heatmap(cm, annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to printing the accuracy \ndef print_accuracy(model_name, prediction, actual):\n    print('Accuracy for {} classifier: {}% '.format(model_name, round(accuracy_score(prediction, actual), 2)*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Importing the dataset\ndf = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Shape of the dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Initial Observation\n\n* Data have 31 features including id which can be ignored. \n* All the features are of float types and the response variable is categorical (M & B).\n* There seems to have no missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting features from the dataset\nnot_predictor_list = ['Unnamed: 32','id','diagnosis']\nfeatures = df.drop(not_predictor_list, axis=1)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Details ( like mean, ranges, quantiles) of different features from above table.\nfeatures.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Creating dataframe for the box plot for different features with diagnosis "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a copy of the features dataframe\ndf_combined = features.copy()\n\n## Standarding the dataset \nsc = StandardScaler()\ndf_combined = sc.fit_transform(df_combined)\n\n## Converting dataset back to dataframe from numpy array (standard scaler returns array)\ndf_combined = pd.DataFrame(df_combined, columns=features.columns)\n\n## Creating separate dataframes for mean, se and worst features\ndf_mean_combined = df_combined.loc[:, ['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean']]\ndf_se_combined = df_combined.loc[:, ['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se','compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se','fractal_dimension_se']]\ndf_worst_combined = df_combined.loc[:,['radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']]\n\n## Adding diagnosis column to above dataframes\ndf_mean_combined['diagnosis'] = df.loc[:, 'diagnosis']\ndf_se_combined['diagnosis'] = df.loc[:, 'diagnosis']\ndf_worst_combined['diagnosis'] = df.loc[:, 'diagnosis']\n\n## Melting the dataframe\ndf_mean_melted = pd.melt(df_mean_combined, id_vars = 'diagnosis',\n                                 var_name = 'features',\n                                 value_name = 'value')\n\ndf_se_melted = pd.melt(df_se_combined, id_vars = 'diagnosis',\n                                 var_name = 'features',\n                                 value_name = 'value')\n\ndf_worst_melted = pd.melt(df_worst_combined, id_vars = 'diagnosis',\n                                 var_name = 'features',\n                                 value_name = 'value')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ploting the box plot for different features to understand it's relation with diagnosis parameters (type of cell)\nfig, ax = plt.subplots(figsize=(12,12))\nsns.boxplot(x='features', y='value', hue='diagnosis', data=df_mean_melted, showfliers=False)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Median and overall range (25%-75%) value for most of the features are smaller for benign but for fractal dimension mean its amlost same for either cells. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ploting the box plot for different features to understand it's relation with diagnosis parameters (type of cell)\nfig, ax = plt.subplots(figsize=(12,12))\nsns.boxplot(x='features', y='value', hue='diagnosis', data=df_se_melted, showfliers=False)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Median or range value for texture_se, smoothness_se is almost same.\n* Median or range value for symmetry_se is slightly higher for benign.\n* Median or range value for fractal dimension is slighly lower for benign.\n* Median or range value for rest is smaller for benign."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ploting the box plot for different features to understand it's relation with diagnosis parameters (type of cell)\nfig, ax = plt.subplots(figsize=(12,12))\nsns.boxplot(x='features', y='value', hue='diagnosis', data=df_worst_melted, showfliers=False)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Median or range value for all worst case features are smaller for benign."},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = features.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18,18))\nsns.heatmap(corr_mat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features that are correlated: \n* Mean value and worst value of different parameters are highly correlated \n* Area, radius, perimeter and concave point\n* concavity, concave point and compactness\n* compactness and fractal dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Removing the correlated feature \ncorrelated_feature_list =  ['radius_mean','perimeter_mean','compactness_mean','concave points_mean',\n                            'radius_se','perimeter_se', 'compactness_se','concave points_se',\n                            'radius_worst','texture_worst', 'perimeter_worst','area_worst', \n                            'smoothness_worst', 'compactness_worst', 'concavity_worst','concave points_worst']\n\nX = features.drop(correlated_feature_list, axis=1)\ny = df[['diagnosis']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Encoding the categorical response variable\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = encoder.fit_transform(y.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Displayng the final features dataset\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Displaying the info about the features\nX.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing correlated features we are left with 13 parameters. \n\n*Note*: the features with around 0.8 or higher correlation value are considered.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Spiliting the dataset into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Using RFE & random forest classifier to select top 5 features \nrandom_forest_classifier = RandomForestClassifier()\nrfe = RFE(estimator=random_forest_classifier, n_features_to_select=5, step=1)\nrfe = rfe.fit(X_train, y_train)\n\n## Printing list of top features\ntop_feature = X_train.columns[rfe.support_].tolist()\nprint('Top 5 feature by rfe and random forest:',top_feature)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following are the best feature among all:\n* texture_mean\n* area_mean\n* concavity_mean\n* area_se\n* symmetry_worst"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Limiting the features with top features\nX_train = X_train.loc[:, top_feature]\nX_test = X_test.loc[:, top_feature]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling "},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression: It is a type of classification model based on logit function to model the binary response variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scaling before fitting it to model\nsc_ = StandardScaler()\nX_train = sc_.fit_transform(X_train)\nX_test = sc_.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the logistic regression classifier\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_reg_classifier = LogisticRegression(random_state=10, max_iter=150)\nlogistic_reg_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nlogistic_y_train_pred = logistic_reg_classifier.predict(X_train)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(logistic_y_train_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'logistic regression'\nprint_accuracy(model_name, logistic_y_train_pred, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nlogistic_y_pred = logistic_reg_classifier.predict(X_test)\n\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(logistic_y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'logistic regression'\nprint_accuracy(model_name, logistic_y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors: It is a type of classification model which uses k nearest neighbor to classify the new observation data. Based on the class of neighbors the majority class is assigned to new data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the K nearest neighbors classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier(n_neighbors=5)\nknn_classifier.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nknn_y_train_pred = knn_classifier.predict(X_train)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(knn_y_train_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'knn'\nprint_accuracy(model_name, knn_y_train_pred, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nknn_y_pred = knn_classifier.predict(X_test)\n\n## Plotting a confusion matrix for KNN \nplot_confusion_matrix(knn_y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'KNN'\nprint_accuracy(model_name, knn_y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier: It is a type of ensemble method which uses multiple decision trees to make the prediction. Different decision trees are created with different datasets and features. Based on individual decision tree prediction, a majorly predicted class is considered a final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_classifier = RandomForestClassifier(n_estimators=500, criterion='entropy', random_state=0)\nrandom_forest_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nrandom_forest_y_pred = random_forest_classifier.predict(X_test)\n\n## Plotting a confusion matrix for KNN \nplot_confusion_matrix(random_forest_y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'random forest'\nprint_accuracy(model_name, random_forest_y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nrandom_forest_y_train_pred = random_forest_classifier.predict(X_train)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(random_forest_y_train_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'random forest'\nprint_accuracy(model_name, random_forest_y_train_pred, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost :  It is an ensemble method that creates a strong classifier based on weak classifiers. It is an implementation of gradient boosted decision trees designed to provide speed and better performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the logistic regression classifier\nfrom xgboost import XGBClassifier\n\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nxgb_y_train_pred = xgb_classifier.predict(X_train)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(xgb_y_train_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'xgb'\nprint_accuracy(model_name, xgb_y_train_pred, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nxgb_y_pred = xgb_classifier.predict(X_test)\n\n## Plotting a confusion matrix for XGB \nplot_confusion_matrix(xgb_y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'XGB'\nprint_accuracy(model_name, xgb_y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network: It is model that tries to mimic the neurons in human brain. It has an input layer, hidden layer and an output layer. Hidden layer are uses weight and activiation function to provide non-linearity to data."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Preparing the train and test set for Neural Network\n\n## Spiliting the dataset into train and test set\nX_ann_train, X_ann_test, y_ann_train, y_ann_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n## Standradizing the train and test data\nsc = StandardScaler()\nX_ann_train = sc.fit_transform(X_ann_train)\nX_ann_test = sc.transform(X_ann_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\nbatch_size = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the artificial neural network classifier\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\n\nann_classifier = Sequential()\n\n## Adding hidden layers\nann_classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=14))\nann_classifier.add(Dropout(0.2))\nann_classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu')) \nann_classifier.add(Dropout(0.2))\n\n## Adding output layer                   \nann_classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n                   \n## Compiling the classifier                \nann_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n           \n## Fitting the classifier\nann_classifier.fit(X_ann_train, y_ann_train, batch_size = batch_size, epochs = epochs)                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nann_y_train_pred = ann_classifier.predict(X_ann_train)\nann_y_train_pred = (ann_y_train_pred > 0.5)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(ann_y_train_pred, y_ann_train)\n\n## Printing the accuracy \nmodel_name = 'artificial neural network'\nprint_accuracy(model_name, ann_y_train_pred, y_ann_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nann_y_pred = ann_classifier.predict(X_ann_test)\nann_y_pred = (ann_y_pred > 0.5)\n\n## Plotting a confusion matrix for KNN \nplot_confusion_matrix(ann_y_pred, y_ann_test)\n\n## Printing the accuracy \nmodel_name = 'artificial neural network'\nprint_accuracy(model_name, ann_y_pred, y_ann_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Accuracy comparision for all the above model\nprint_accuracy('logistic regression', logistic_y_pred, y_test)\nprint_accuracy('KNN', knn_y_pred, y_test)\nprint_accuracy('random forest', random_forest_y_pred, y_test)\nprint_accuracy('XGB', xgb_y_pred, y_test)\nprint_accuracy('artificial neural network', ann_y_pred, y_ann_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Selection\nFrom all the model created above, logistic regresion (95%), xgboost (95%) and neural network (95%) provided best accuracy.Though the training and test accuracy have large difference for xgboost as well as neural network which can be sign of overfitting. "},{"metadata":{},"cell_type":"markdown","source":"### Model Tuning : Hyperparameterization "},{"metadata":{},"cell_type":"markdown","source":"Lets tune the logistic regression, as it have better train as well as test accuracy for given dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n## Creating the params for logistic regression\nclassifier_lr = LogisticRegression()\nsolver_list = ['newton-cg', 'lbfgs', 'liblinear']\npenalty_list = ['l2']\nc_list = [100, 10, 1.0, 0.1, 0.01]\n\n# Defining the grid and folds\ngrid = dict(solver=solver_list,penalty=penalty_list,C=c_list)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n## Creating the grid search \ngrid_search = GridSearchCV(estimator=classifier_lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# Grid search results \nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"Test score : %f (%f) with parameters: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best parameters: \", grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Applying the hyperparameter logistic regression classifier\nlogistic_reg_hp_classifier = LogisticRegression(C= 1, penalty='l2', solver= 'newton-cg', random_state=10, max_iter=150)\nlogistic_reg_hp_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting training value\nlogistic_y_train_pred = logistic_reg_hp_classifier.predict(X_train)\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(logistic_y_train_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'logistic regression'\nprint_accuracy(model_name, logistic_y_train_pred, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the classifier output\nlogistic_y_pred = logistic_reg_hp_classifier.predict(X_test)\n\n\n## Plotting a confusion matrix for logistic regression classifier\nplot_confusion_matrix(logistic_y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'logistic regression'\nprint_accuracy(model_name, logistic_y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With hypertparameters the model is giving similar result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import STATUS_OK, Trials, fmin, hp, tpe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating the hyperparams list \nparams ={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 0,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 1000,\n        'learning_rate':hp.quniform('learning_rate', 0.01, 0.1, 0.01)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(params):\n    classifier = XGBClassifier(\n                    n_estimators =params['n_estimators'], \n                    max_depth = int(params['max_depth']),\n                    gamma = params['gamma'],\n                    reg_alpha = params['reg_alpha'],\n                    min_child_weight=params['min_child_weight'],\n                    colsample_bytree=params['colsample_bytree'],\n                    learning_rate=params['learning_rate'])\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    classifier.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n    pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test, pred)\n    \n    print('Socre:', accuracy)\n    \n    return {'loss': -accuracy, 'status': STATUS_OK }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = params,\n                        algo = tpe.suggest,\n                        max_evals = 500,\n                        trials = trials)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best parameters :\", best_hyperparams)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try tunning XGB model and compare it with logistic to see it better result."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_hp_classifier = XGBClassifier( \n                    n_estimators = 1000, \n                    max_depth = int(round(best_hyperparams['max_depth'], 3)),\n                    gamma = round(best_hyperparams['gamma'], 3),\n                    reg_alpha = int(round(best_hyperparams['reg_alpha'], 3)),\n                    min_child_weight=round(best_hyperparams['min_child_weight'], 3),\n                    colsample_bytree=round(best_hyperparams['colsample_bytree'], 3),\n                    learning_rate=best_hyperparams['learning_rate'])\n    \nevaluation = [( X_train, y_train), ( X_test, y_test)]\n    \nxgb_hp_classifier.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the train data\ny_pred = xgb_hp_classifier.predict(X_train)\n\n## Plotting a confusion matrix for KNN \nplot_confusion_matrix(y_pred, y_train)\n\n## Printing the accuracy \nmodel_name = 'XGB'\nprint_accuracy(model_name, y_pred, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predicting the test data\ny_pred = xgb_hp_classifier.predict(X_test)\n\n## Plotting a confusion matrix for KNN \nplot_confusion_matrix(y_pred, y_test)\n\n## Printing the accuracy \nmodel_name = 'XGB'\nprint_accuracy(model_name, y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Printing the accuracy \nmodel_name = 'XGB'\nprint_accuracy(model_name, y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the hyperparameters for xgboost model provided similar or slightly lesser accuracy. Though there are more similarities between the train and test accuracy indicating there is no overfitting.\n\nAgain the accuracy will vary as the best parameter value list might change every time we run the search.  "},{"metadata":{},"cell_type":"markdown","source":"### Best Model \n\nLogistic regression seems to provided best result with hyperparameters: \n* C=1\n* penalty='l2'\n* solver= 'newton-cg'"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}