{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SENTIMENT ANALYSIS FOR FINANCIAL NEWS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"IMPORTING THE LIBRARY FILES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport string\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA LOADING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    data = pd.read_csv('../input/sentiment-analysis-for-financial-news/all-data.csv', sep=',', encoding='latin-1',names = [\"category\",\"comment\"])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df = load_data()\ndf=load_data()\ntweet_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(tweet_df.shape)\nprint(\"COLUMN NAMES\" , tweet_df.columns)\n\nprint(tweet_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VISUALIZATION OF CATEGORIES OF TEXT DATA - EXPLORATORY DATA ANALYSIS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TEXT VISUALIZATION \nsns.countplot(x=\"category\",data=tweet_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT PRE-PROCESSING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. REMOVING PUNCTUATIONS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove punctuations\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ntweet_df['comment'] = tweet_df['comment'].apply(lambda x: remove_punct(x))\ntweet_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. STOPWORDS REMOVAL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#stopwords removal\nimport nltk\nnltk.download('stopwords')\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntweet_df[\"text_wo_stop\"] = tweet_df[\"comment\"].apply(lambda text: remove_stopwords(text))\ntweet_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove \n%matplotlib inline\npd.set_option('display.max_colwidth', 100)\n\ntweet_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. STEMMING AND LEMMATIZATION OF TEXT DATA\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemming and lemmatization\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\ntweet_df[\"text_stemmed\"] = tweet_df[\"text_wo_stop\"].apply(lambda text: stem_words(text))\ntweet_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. COUNT VECTORIZATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove frequent words - countvectorization\nfrom collections import Counter\ncnt = Counter()\nfor text in tweet_df[\"text_stemmed\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. REMOVAL OF THE MOST FREQUENT WORDS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ntweet_df[\"text__stopfreq\"] = tweet_df[\"text_stemmed\"].apply(lambda text: remove_freqwords(text))\ntweet_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ntweet_df[\"text_lemmatized\"] = tweet_df[\"text__stopfreq\"].apply(lambda text: lemmatize_words(text))\ntweet_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. DROPPING THE UN-USED COLUMNS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the columns\ntweet_df=tweet_df.drop([\"text_stemmed\",\"text__stopfreq\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. LABEL ENCODING OF THE CATEGORICAL VARIABLES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding\nfrom sklearn.preprocessing import LabelEncoder\ntweet_df['encoded_category'] = LabelEncoder().fit_transform(tweet_df['category'])\ntweet_df[[\"category\", \"encoded_category\"]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return clean_text\n\ndef join_text(text):\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df=tweet_df.drop([\"category\",\"text_wo_stop\",\"comment\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PREVIEW OF THE CLEAN AND PRE-PROCESSED TEXT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CLASSIFICATION MODEL BUILDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SPLITTING OF TRAIN AND TEST DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n\nx_train,x_test,y_train,y_test = train_test_split(tweet_df.text_lemmatized,tweet_df.encoded_category,test_size = 0.3 , random_state = 0)\n\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. LINEAR SUPPORT VECTOR MACHINE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('tfidf', TfidfVectorizer()),\n                 ('model', LinearSVC())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LINEAR SVC\")\nprint(\"accuracy score: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. LOGISTIC REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LOGISTIC REGRESSION\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. MULTINOMIAL NAIVE BAYES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', MultinomialNB())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MULTINOMIAL NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. BERNOULLI NAIVE BAYES ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', BernoulliNB())])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"BERNOULLIS NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. GRADIENT BOOSTING CLASSIFICATION MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=55))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"GRADIENT BOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. XGBOOST CLASSIFICATION MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', XGBClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"XGBOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. DECISION TREE CLASSIFICATION MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = 10, \n                                           splitter='best', \n                                           random_state=2020))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"DECISION TREE\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. K- NEAREST NEIGHBOUR CLASSIFIER MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"K NEAREST NEIGHBOR\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CONCLUSION\n\nBased on the above model comparison we can infer that Linear SVC model predicts the text classification at a better rate of accuracy than other models.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is my first kernel and first attempt in text analytics. Critics are expected as in to improve me further. Thanks in Advance!! :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}