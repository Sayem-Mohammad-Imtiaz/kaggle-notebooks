{"cells":[{"metadata":{"_cell_guid":"8c1b995c-01e5-488f-8991-d83cb785eecf","_uuid":"7cfaa1ffb88aaec9fad056807de2a9e10c442dde"},"cell_type":"markdown","source":"## Objective:- To find out the factors on which Happiness is dependent upon and then performing cluster analysis using various algorithm\n"},{"metadata":{"_cell_guid":"d86e9b1b-a75b-44ad-a059-378ddda275fc","_uuid":"45478269fa1f1f25b064dddad29099a5e23da67c"},"cell_type":"markdown","source":"#### 1. ) We will load the Liberaries required to perform the analysis"},{"metadata":{"_cell_guid":"b852f91e-5591-4c62-b9d5-c64cb1055148","_uuid":"08c73b120c54a29896caa4cd739adaf89fb4d5e6","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time                          # To time processes \nimport warnings                      # To suppress warnings\nimport matplotlib.pyplot as plt      # For Graphics\nimport seaborn as sns\nfrom sklearn import cluster, mixture # For clustering \nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n%matplotlib inline\nwarnings.filterwarnings('ignore')","execution_count":182,"outputs":[]},{"metadata":{"_cell_guid":"d6b8742e-7dfe-48d9-b908-754e24e01e19","_uuid":"57df2f320e8d2ad6440322f741e4692ce6d2f676"},"cell_type":"markdown","source":"#### 2. ) We will load the data into a dataframe data structure and then we will take a peek at the data to see what all dependent variables are available in data"},{"metadata":{"_cell_guid":"bfedbcfa-a86a-4b26-a787-1f3ef441a360","_uuid":"549eb2c0639bcde7c7348c52d1800e99dcb917ee","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/2017.csv\")\ndata.head()","execution_count":183,"outputs":[]},{"metadata":{"_cell_guid":"cd165f63-c84f-4856-9a34-bffc70b41a0a","_uuid":"4014e53f5a1b87e8c0e85468bb0722da6be64b26"},"cell_type":"markdown","source":"#### 3. ) We will check the columns we have in our dataset , so that we can start studying relationship between columns before starting clustering"},{"metadata":{"_cell_guid":"86edf512-c3a1-459a-9da0-041d118b9cab","_uuid":"8c4031f83f3121437933a9ecd269fb372a761883","trusted":true},"cell_type":"code","source":"data.columns","execution_count":184,"outputs":[]},{"metadata":{"_cell_guid":"14274614-430b-457e-a6aa-50c55ab325ee","_uuid":"c9804242c181854cab0e631353bb0a86e68fe946"},"cell_type":"markdown","source":"#### 4. )   Basic Data Pre-processing\n#### This is a important step.We will perform following tasks for data pre-procession on this data\n1.  We will check if there is any Null or NAN in our data set\n2. We will rename our column names to more appropriate name"},{"metadata":{"_cell_guid":"5695bc18-85ef-404c-b977-26896b8945a9","_uuid":"bf4dcee4bddb2390345757975164d45ae16ec848","trusted":true},"cell_type":"code","source":"data.isnull().any()\n","execution_count":185,"outputs":[]},{"metadata":{"_cell_guid":"798c5e84-ebab-4748-9450-cad704e2420c","_uuid":"abb1d9d015a4e7422661b2a1824640b24c1f810b","collapsed":true,"trusted":true},"cell_type":"code","source":"data=data.rename(columns={'Economy..GDP.per.Capita.':'Economy_GDP_Per_Capita','Health..Life.Expectancy.':'Health_Life_Expectancy','Trust..Government.Corruption.':'Trust_Government_Corruption','Happiness.Rank':'Happiness_Rank','Happiness.Score':'Happiness_Score'})","execution_count":186,"outputs":[]},{"metadata":{"_cell_guid":"dcf6b6c5-2119-4554-8ccb-8100ab273154","_uuid":"9568c82878d4543563b3184cd55e1a95df2e423f","trusted":true},"cell_type":"code","source":"data.columns","execution_count":187,"outputs":[]},{"metadata":{"_cell_guid":"ef5cde34-1cfa-4307-9a65-fce1548e9374","_uuid":"ccb71b869f45a9302e636c80170ca6bfc8761956"},"cell_type":"markdown","source":"#### 5. ) Now we drew coorelation plots between different columns of data to see relation ship between data sets which contributes to overall happiness score. we can Gather from this that there is strong relation between Family,Health and Economy.Freedom and Generosity are very less related to any of these factor or to each other."},{"metadata":{"_cell_guid":"a3e3214e-2fb0-4d0a-ace0-bf3062f03989","_uuid":"1d4390c8206a107a57eb324bf352c83f0888d8fc","trusted":true},"cell_type":"code","source":"    \ndf=data.loc[:, 'Economy_GDP_Per_Capita':'Trust_Government_Corruption']\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data=df.corr(),annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","execution_count":188,"outputs":[]},{"metadata":{"_cell_guid":"cfd516b3-d115-42ba-8087-4a59563f374d","_uuid":"93d477b6c16cc2ac87eaeae2c0cd9fd264baf08f"},"cell_type":"markdown","source":"#### 6. ) We will draw vertical bar plots corresponding to each country to compare different factors contributing to happiness visually "},{"metadata":{"_cell_guid":"353d7d34-347c-46bd-9d71-ff9a8d42d3ee","_uuid":"05cf1c055d7905171d5d7a595a9ede2cb9c14627","trusted":true},"cell_type":"code","source":"data=data.sort_values('Happiness_Rank',ascending=False)\ndf_wh=data.filter(['Country','Economy_GDP_Per_Capita','Family','Health_Life_Expectancy','Freedom','Generosity','Trust_Government_Corruption'])\ndf_wh=df_wh.set_index('Country')\ndf_wh.plot.barh(stacked=True,  figsize=(10,28))","execution_count":189,"outputs":[]},{"metadata":{"_uuid":"57382f0e4153f245ffad542ec8c43deb5408cea9"},"cell_type":"markdown","source":"#### 7. ) Visualizing Global Happiness on Map"},{"metadata":{"trusted":true,"_uuid":"30a8aaaf1d130009325bcd7ac8672dc64421d32d"},"cell_type":"code","source":"df_wh_map = data\ndf_wh_map.head()","execution_count":190,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e38af9544ed7366a991a2b2b88094463b5f7cde1"},"cell_type":"code","source":"df_wh_map = data.reset_index()\ncolorRange = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],[0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = dict(type = 'choropleth', \n           locations = df_wh_map['Country'],\n           locationmode = 'country names',\n           z = df_wh_map['Happiness_Score'], \n            colorscale = colorRange,\n           text = df_wh_map['Country'],\n           \n            marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) )\n            \n            ,\n           colorbar = {'title':'Cluster Group'})\n\nlayout = dict(title = 'World Happiness Visualization on Map', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\n\nchoromap3 = go.Figure(data = [data], layout=layout)\n\niplot(choromap3)","execution_count":191,"outputs":[]},{"metadata":{"_cell_guid":"6b4c8918-6fd0-42a4-acc6-8aaa658b7bc3","_uuid":"0ee9d8625321132e610d4cb8852cad21e0dd6edb"},"cell_type":"markdown","source":"#### 8. ) We will load world happiness data for all the three years and we will see how world hapiness have changed for last 2 years"},{"metadata":{"_cell_guid":"eac8d713-6187-4869-86d4-4fbfb9a29fe7","_uuid":"faf81256dbabe80fcf8acdcf831f3dbefe7a39c0","collapsed":true,"trusted":true},"cell_type":"code","source":"data_2016 = pd.read_csv(\"../input/2015.csv\")\ndata_2016=data_2016.rename(columns={'Economy (GDP per Capita)':'Economy_GDP_Per_Capita','Health (Life Expectancy)':'Health_Life_Expectancy','Trust (Government Corruption)':'Trust_Government_Corruption','Happiness Rank':'Happiness_Rank','Happiness Score':'Happiness_Score'})\ndata_2016=data_2016.sort_values('Happiness_Rank',ascending=False)\n\ndata_2017 = pd.read_csv(\"../input/2017.csv\")\ndata_2017=data_2017.rename(columns={'Economy..GDP.per.Capita.':'Economy_GDP_Per_Capita','Health..Life.Expectancy.':'Health_Life_Expectancy','Trust..Government.Corruption.':'Trust_Government_Corruption','Happiness.Rank':'Happiness_Rank','Happiness.Score':'Happiness_Score'})\ndata_2017=data_2017.sort_values('Happiness_Rank',ascending=False)","execution_count":192,"outputs":[]},{"metadata":{"_cell_guid":"bfe5a32c-8fd2-47ec-ac61-787befae46c2","_uuid":"fdf6fcd39149aedd70992558fc91e2cbfe58fe19"},"cell_type":"markdown","source":"#### 9.) We will plot data from two years to see how factors contributing to happiness have changes over the course of time. We will see that family factor has increased in last year"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"633ef777-eafe-48e4-adfd-56f4afceaf18","_uuid":"fccac243f0841dd0469fb169a5b6733b6f753113","collapsed":true,"trusted":true},"cell_type":"code","source":"##We will use Plotly library to draw graphs\n#import plotly.plotly as py\n#import plotly.graph_objs as go\n#fig = plt.figure()\n#trace1 = Scatter(x = frame['Country'],y = frame['Happiness_Rank'],mode = 'lines+markers', name = 'lines+markers')\n#data = [trace1]\n#plt.show()","execution_count":193,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"42eba017-0d7e-406c-a909-e572c864022e","_uuid":"dfadaaf9ebd3fa07e42a4abd6415b00932f36eaa","collapsed":true,"trusted":true},"cell_type":"code","source":"#import plotly.graph_objs as go\n#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n#init_notebook_mode(connected=True)","execution_count":194,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_cell_guid":"11bcddfb-ccfc-4943-8ef6-9c1e7111e94a","_uuid":"3f9e79758c410388536ab7c997fe86ace0a90a84","collapsed":true,"trusted":true},"cell_type":"code","source":"#from plotly import __version__\n#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n#from plotly.graph_objs import *\n# For offline use\n#import cufflinks as cf\n#cf.go_offline()\n#for frame in [data_2015,data_2016,data_2017]:\n#    frame[['Family','Freedom']].iplot(kind='spread')","execution_count":195,"outputs":[]},{"metadata":{"_cell_guid":"27aa5fae-a231-488f-8e14-c2c787e4abaf","_uuid":"a541895f20bf0d59bc4a99244c8c753828884211","trusted":true},"cell_type":"code","source":"for frame in [data_2016,data_2017]:\n    plot_family=sns.kdeplot(frame['Family'], shade=True)\n\nplot_family.legend_.set_title(\"Year\")\nplot_family.set_title(\"Family\")\n# we will replace labels\nnew_labels = ['2016','2017']\n\nfor current_label_obj, label in zip(plot_family.legend_.texts, new_labels): \n    current_label_obj.set_text(label)","execution_count":196,"outputs":[]},{"metadata":{"_cell_guid":"ee3c99eb-acca-40df-89d0-0efeb3411c3f","_uuid":"9e5abe592b3abf4d5c42aab0a408e7888632d60a"},"cell_type":"markdown","source":"#### 10. ) Above we saw family factor has been increased in 2017 .Now we will draw plot for other factors contributing towards happiness"},{"metadata":{"_cell_guid":"17a03f5c-7d6e-4ece-92b9-d5e8cb63c87a","_uuid":"bfbfa0fc3dfc1a9111f560521ebc9bd44a5abb02","trusted":true},"cell_type":"code","source":"for frame in [data_2016,data_2017]:\n    plot_1=sns.kdeplot(frame['Economy_GDP_Per_Capita'], shade=True)\n\nplot_1.legend_.set_title(\"Year\")\nplot_1.set_title(\"Economy\")\n# we will replace labels\nnew_labels = ['2016','2017']\nfor current_label_obj_1,label in zip(plot_1.legend_.texts,new_labels): \n    current_label_obj_1.set_text(label)","execution_count":197,"outputs":[]},{"metadata":{"_cell_guid":"979b9e85-04b9-4c26-bd18-a4028b1fcb95","_uuid":"785e82065f2f0dd3ba43917f4d1258ac41babaec","trusted":true},"cell_type":"code","source":"for frame in [data_2016,data_2017]:\n    plot_2=sns.kdeplot(frame['Health_Life_Expectancy'], shade=True)\nplot_2.legend_.set_title(\"Year\")\nplot_2.set_title(\"Health_Life_Expectancy\")\nfor current_label_obj_2,label in zip(plot_2.legend_.texts,new_labels): \n    current_label_obj_2.set_text(label)","execution_count":198,"outputs":[]},{"metadata":{"_cell_guid":"925355fe-677e-4aae-89f8-6015ca84403a","_uuid":"1b5ab02ed0ff3e8660951a856047e9019091946b","trusted":true},"cell_type":"code","source":"for frame in [data_2016,data_2017]:\n    plot_3=sns.kdeplot(frame['Trust_Government_Corruption'], shade=True)\nplot_3.legend_.set_title(\"Year\")\nplot_3.set_title(\"Trust_Government_Corruption\")\nfor current_label_obj_3,label in zip(plot_3.legend_.texts,new_labels): \n    current_label_obj_3.set_text(label)","execution_count":199,"outputs":[]},{"metadata":{"_cell_guid":"39eeef71-cdf6-47cd-b094-0b0b6ef094d7","_uuid":"665ec3585090d46f9509ea9eb2cd43a6533fc956","trusted":true},"cell_type":"code","source":"for frame in [data_2016,data_2017]:\n    plot_4=sns.kdeplot(frame['Freedom'], shade=True)\nplot_4.legend_.set_title(\"Year\")\nplot_4.set_title(\"Freedom\")\nfor current_label_obj_4,label in zip(plot_4.legend_.texts,new_labels): \n    current_label_obj_4.set_text(label)","execution_count":200,"outputs":[]},{"metadata":{"_cell_guid":"900a72ab-6e9e-4971-9dfc-74f3a4873721","_uuid":"52789d9ab9407175fb9cc565fedb5692a56f73b1"},"cell_type":"markdown","source":"#### We have seen above, Freedom, family,Economy and Trust in goverment have increased over the time.But Health expectancy have been decreased."},{"metadata":{"_cell_guid":"4bd0b201-2aae-4469-8000-7049205d6387","_uuid":"d15f516dade2504aaa26173d6448878ff5a1f10a"},"cell_type":"markdown","source":"## What is Clustering ?\n#### Clustering is divding data into groups of homogenous or similar data. So that data in same group is more similar to each other and very different from data in other groups. Each group of similar data is called cluster\n#### 11 . ) We will now perform clustering of World Happiness Report data using different methods of clustering.There are following clustering algorithems and techniques available for clustering.\n\n1. K-Means Clustering(Partitioning Based)\n2. Spectral\n3. Affinity Propagation\n4. Mean Shift\n5. DBSCAN(Density Based)\n6. Mini Batch K-Means(Partitioning Based)\n7. Birch(Hierarchical)\n8. Gaussian Mixture Modeling","attachments":{}},{"metadata":{"_cell_guid":"97630bef-afe4-4a2c-b9d8-01a1ba18d3e2","_uuid":"ce40a627e9eea96e372c0f7196c299ee35448e3f"},"cell_type":"markdown","source":"#### A .) K Means Clustering - K means clustering works by selecting centroids randomly and number of centroids are inputs to clustering algorithm.Once random centroids are selected then distance from each centroid for each observations are calculated and each observation data is allocated to a centroid to which distance of observation is minimum."},{"metadata":{"_cell_guid":"94d2806b-b50d-48f7-b170-45cdbc623fb7","_uuid":"85c804120129a734522bda768eb3a4eef66cc81f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Method for K means clustering\ndef kmeans_Clustering(data,numberOfClusters):\n       #Initializing Kmeans.cluster object was imported from sklearn in begining.\n       kmeans = cluster.KMeans(n_clusters=numberOfClusters)\n       # Fitting the input data and getting the cluster labels\n       cluster_labels = kmeans.fit_predict(data)\n       # Getting the cluster centers\n       cluster_centers = kmeans.cluster_centers_\n       cluster_centers.shape\n       return cluster_labels,cluster_centers","execution_count":201,"outputs":[]},{"metadata":{"_cell_guid":"ea14a2bf-66fc-4fdb-86f5-e8939e04728c","_uuid":"49c2ad8b6564397965663841719468d86bc1b5e1"},"cell_type":"markdown","source":"We will plots clusters of all the 6 dimensions with 2 dimensions in each 2 dimensional plot"},{"metadata":{"_cell_guid":"823e9dab-5938-4997-9b4a-31a2d93d44a1","_uuid":"6d6f27a54588e5772b9b51e99b7a6b974cc9aff6","collapsed":true,"trusted":true},"cell_type":"code","source":"#Plot the cluster\ndef plot_cluster(labels,centers,df_wh):\n    #Getting number of columns\n    numOfDimensions = df_wh.columns.size\n    #Number of plots required for 6 dimension with 2 dimensions in each plot\n    numberOfPlots = int(numOfDimensions/2)\n    #Number of rows and columns for subplots\n    fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n    for i,j in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n         ax[j].scatter(df_wh.iloc[:, i], df_wh.iloc[:, i+1], c=labels, s=50, cmap='viridis')\n         ax[j].scatter(centers[:,i], centers[:, i+1], c='black', s=200, alpha=0.5)\n         #print(i)\n    plt.subplots_adjust(bottom=-0.5, top=1.5)\n    plt.show()","execution_count":202,"outputs":[]},{"metadata":{"_cell_guid":"70c56f0e-293c-4282-b873-c50720c9e2f6","_uuid":"73d6f8dcab624e790aaff939c03706dc1b22b876"},"cell_type":"markdown","source":"We will do clustering of data using K means method and we will create plots for clustered data"},{"metadata":{"_cell_guid":"afda5952-d86b-4174-82ee-2bc39b0ee0b5","_uuid":"9c2228daa567657a1859ddd612a1ed2654b0a908","trusted":true},"cell_type":"code","source":"labels,centers = kmeans_Clustering(df_wh,3)     \nplot_cluster(labels,centers,df_wh)","execution_count":203,"outputs":[]},{"metadata":{"_cell_guid":"910d397e-59fb-42e2-bcb5-202f09d8d667","_uuid":"36212fb1daede37df89726ca81560363d2d940fc"},"cell_type":"markdown","source":"> #### B .) Spectral Clustering - Objective of spectral clustering is to cluster data that which is connected but not necessarily clustered within convex boundaries.Spectral clustering use the affnity matrix to detemine the connectivity of data for clustering"},{"metadata":{"_cell_guid":"64cca1e3-05b7-4bff-97ed-35db7a83cb37","_uuid":"45b10ef672099f99724bd895c9c0893a8e565b7e","collapsed":true,"trusted":true},"cell_type":"code","source":"def spectral_Clustering(data,numberOfClusters):\n       #Initializing cluster.cluster object was imported from sklearn in begining.\n       spectral = cluster.SpectralClustering(n_clusters=numberOfClusters,affinity=\"nearest_neighbors\",eigen_solver='arpack')\n       # Fitting the input data and getting the cluster labels\n       cluster_labels = spectral.fit_predict(data)\n       # Getting the cluster centers\n       return cluster_labels","execution_count":204,"outputs":[]},{"metadata":{"_cell_guid":"da812362-ba21-45e1-bc10-0a5550335ba4","_uuid":"15900e1ab46af8740a79f2ed4bc13902d0b2a5a3","collapsed":true,"trusted":true},"cell_type":"code","source":"#Plot the cluster without center\ndef plot_clusterWithoutCenter(labels,df_wh):\n    #Getting number of columns\n    numOfDimensions = df_wh.columns.size\n    #Number of plots required for 6 dimension with 2 dimensions in each plot\n    numberOfPlots = int(numOfDimensions/2)\n    #Number of rows and columns for subplots\n    fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n    for i,j in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n         ax[j].scatter(df_wh.iloc[:, i], df_wh.iloc[:, i+1], c=labels, s=50, cmap='viridis')\n    plt.subplots_adjust(bottom=-0.5, top=1.5)\n    plt.show()","execution_count":205,"outputs":[]},{"metadata":{"_cell_guid":"b0c24564-749e-40f7-b7d0-5de66b09547a","_uuid":"4ca243d1220bdf19a09fca5f4a3850572ce52818","trusted":true},"cell_type":"code","source":"labels = spectral_Clustering(df_wh,3)    \nplot_clusterWithoutCenter(labels,df_wh)","execution_count":206,"outputs":[]},{"metadata":{"_cell_guid":"5b284fd2-4816-45a7-8b9d-c8d908d4d83c","_uuid":"cb47f7ed6525b0d06b9de0ca04d21516be80a933"},"cell_type":"markdown","source":"#### C.)  Affinity Propagation- Affinity Propagation works by setting up a factor graph that describes the objective function used to identify exemplars and cluster data. Each item in a dataset can be mapped into Euclidean space using feature values. Affinity propagation depends on a matrix containing Euclidean distances between data points. Since the matrix can quickly become quite large, we should be careful not to take up too much memory"},{"metadata":{"_cell_guid":"dbcc3ffd-cb6a-43fa-b04d-8822507c770d","_uuid":"1aaec7156af978d68f4a8d7d804340d1b7cc4abe","collapsed":true,"trusted":true},"cell_type":"code","source":" def affinityPropagation_Clustering(data):\n        affinity_propagation =  cluster.AffinityPropagation(preference=-10,damping=0.5,affinity='euclidean')\n        af = affinity_propagation.fit(data)\n        cluster_centers_indices = af.cluster_centers_indices_\n        labels = af.labels_\n        n_clusters_ = len(cluster_centers_indices)\n        #print(n_clusters_)\n        return affinity_propagation.predict(data),n_clusters_,cluster_centers_indices","execution_count":207,"outputs":[]},{"metadata":{"_cell_guid":"a272d3bd-3a78-400c-8c29-036490db73a2","_uuid":"8b246b3cc48d27239fa5933eeab06929d134ea90","trusted":true},"cell_type":"code","source":"from itertools import cycle\nplt.close('all')\nplt.figure(1)\nplt.clf()\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n\ndef drawAffinityCluster(labels,n_clusters_,cluster_centers_indices):\n     #Getting number of columns\n     numOfDimensions = df_wh.columns.size\n     #Number of plots required for 6 dimension with 2 dimensions in each plot\n     numberOfPlots = int(numOfDimensions/2)\n     #Number of rows and columns for subplots\n     fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n     for column,plot in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n        for cluster, col in zip(range(n_clusters_), colors):\n            # This will provide cluster center for both clusters for all dimensions\n            cluster_center = df_wh.iloc[cluster_centers_indices[cluster],:]\n            #This will plot cluster center for first 2 dimensions\n            ax[plot].plot(cluster_center[column], cluster_center[column+1], 'o', markerfacecolor=col,\n                 markeredgecolor='k', markersize=14)\n            # we will traverse through cluster labels and if cluster label is equal cluster 0 then it will \n            # plot data point for tht and if cluster label is equal to cluster number 1(k) then it will plot cluster\n            # 1 data\n            for i in range(labels.size):\n                if cluster==labels[i]:\n                    ax[plot].plot(df_wh.iloc[i, column], df_wh.iloc[i, column+1], col + '.')\n                    # This will draw affinity line between center and data point\n                    ax[plot].plot([cluster_center[column], df_wh.iloc[i, column]], [cluster_center[column+1], df_wh.iloc[i, column+1]], col)\n\n     plt.title('Estimated number of clusters: %d' % n_clusters_)\n     plt.show()\n","execution_count":208,"outputs":[]},{"metadata":{"_cell_guid":"cf134a49-ddd2-4f5e-b050-2e2384a8dc74","_uuid":"aa5149569528fcc318436301efce989ba37be778","trusted":true},"cell_type":"code","source":"labels,n_clusters_,cluster_centers_indices = affinityPropagation_Clustering(df_wh)    \ndrawAffinityCluster(labels,n_clusters_,cluster_centers_indices)","execution_count":209,"outputs":[]},{"metadata":{"_cell_guid":"fcb471cd-8a56-4304-a664-7903fa9febac","_uuid":"5dab3e01cec8f9a84be19c1ddbb9162bd10e058e"},"cell_type":"markdown","source":"#### D.) Mean Shift:-Meanshift is a clustering algorithm that assigns the datapoints to the clusters iteratively by shifting points towards  highest density of datapoints .  Meanshift algorithm has applications in the field of image processing and computer vision.Mean shift exploits this KDE idea by imagining what the points would do if they all climbed up hill to the nearest peak on the KDE surface. It does so by iteratively shifting each point uphill until it reaches a peak."},{"metadata":{"_cell_guid":"ffb58534-43d2-466c-8467-055dd7fc4b65","_uuid":"4c5b68acff23c538c9c6c16a7eab716356649b8e","collapsed":true,"trusted":true},"cell_type":"code","source":"  def meanshift_Cluster(data):\n        meanShift = cluster.MeanShift(bandwidth=0.4,bin_seeding=True )\n        labels = meanShift.fit_predict(data)\n        labels_unique = np.unique(labels)\n        n_clusters = len(labels_unique)\n        cluster_centers = meanShift.cluster_centers_\n        return  labels,cluster_centers,n_clusters\n    ","execution_count":210,"outputs":[]},{"metadata":{"_cell_guid":"7ad44cb1-b40c-4c10-b0ba-78b458f0d733","_uuid":"7264a3f05e401d8f34fcad7acca6347adfa357fe","collapsed":true,"trusted":true},"cell_type":"code","source":"def plotMeanShift(labels,cluster_centers,n_clusters):\n     #Getting number of columns\n     numOfDimensions = df_wh.columns.size\n     #Number of plots required for 6 dimension with 2 dimensions in each plot\n     numberOfPlots = int(numOfDimensions/2)\n     #Number of rows and columns for subplots\n     fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n     for column,plot in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n        for cluster, col in zip(range(n_clusters), colors):\n            #This will plot cluster center for first 2 dimensions\n            ax[plot].plot(cluster_centers[cluster,column], cluster_centers[cluster,column+1], 'o', markerfacecolor=col,\n                 markeredgecolor='k', markersize=14)\n            # we will traverse through cluster labels and if cluster label is equal cluster 0 then it will \n            # plot data point for tht and if cluster label is equal to cluster number 1(k) then it will plot cluster\n            # 1 data\n            for i in range(labels.size):\n                if cluster==labels[i]:\n                    ax[plot].plot(df_wh.iloc[i, column], df_wh.iloc[i, column+1], col + '.')\n                    # This will draw affinity line between center and data point\n                    #ax[plot].plot([cluster_centers[column], df_wh.iloc[i, column]], [cluster_centers[column+1], df_wh.iloc[i, column+1]], col)\n\n     plt.title('Estimated number of clusters: %d' % n_clusters)\n     plt.show()","execution_count":211,"outputs":[]},{"metadata":{"_cell_guid":"970b2034-7907-43e5-8770-6457f46354cb","_uuid":"015b109b3302b30f16bb5752912666d9c78061dd","trusted":true},"cell_type":"code","source":"labels,cluster_centers,n_clusters = meanshift_Cluster(df_wh)    \nplotMeanShift(labels,cluster_centers,n_clusters)","execution_count":212,"outputs":[]},{"metadata":{"_cell_guid":"148cc13e-e987-4cfb-a04b-cbe2501ae0cf","_uuid":"b1b9b7e97a5a0980821489613bbdd315d2bd152b"},"cell_type":"markdown","source":"#### E.) DBSCAN - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is the most well-known density-based clustering algorithm.Mostly it is used to demonstrate how to reduce the size of a spatial data set of GPS latitude-longitude coordinates"},{"metadata":{"_cell_guid":"c5edebba-f787-4ada-b857-0181e41a197d","_uuid":"67ef6f03f61fd22206546e30a201ffe5ff2ba673","trusted":true},"cell_type":"code","source":"def dbscan_Cluster(data):\n    dbscan=cluster.DBSCAN(eps=0.3, min_samples=10)\n    dbsclabels=dbscan.fit_predict(data)\n    core_indices=dbscan.core_sample_indices_ \n    return dbsclabels,core_indices\n    \ndbsclabels,coreIndices=dbscan_Cluster(df_wh)\nlength=dbsclabels\nprint(dbsclabels)","execution_count":213,"outputs":[]},{"metadata":{"_cell_guid":"f9701726-84db-4945-81ff-55d0ec02da43","_uuid":"e548d7ed05186ea5b7ed0e4ab2aed6a1dea4ff5a"},"cell_type":"markdown","source":" If you look very closely above, you’ll see that DBSCAN produced three groups (–1, 0, and 1).It shows only 17 instances of label – 1. That’s because it’s a two-cluster solution; the third group (–1) is noise (outliers)"},{"metadata":{"_cell_guid":"8e2969e0-85b5-450f-85d5-d1e94b41e432","_uuid":"0091a2d24417aa899d17ecda15610a4a9063dcdd","collapsed":true,"trusted":true},"cell_type":"code","source":"def drawDbscanPlot(dbsclabels):\n        #Getting number of columns\n        numOfDimensions = df_wh.columns.size\n        #Number of plots required for 6 dimension with 2 dimensions in each plot\n        numberOfPlots = int(numOfDimensions/2)\n        #Number of rows and columns for subplots\n        colorsArray=['b','g','r']\n        #print(type(dbsclabels))\n        fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n        for column,pltnum in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n            for i,label in enumerate(dbsclabels):\n                colour=colorsArray[label]\n                marker_size=12\n                if label == -1:\n                    #black color\n                    colour=[0, 0, 0, 1]\n                    marker_size=6\n                ax[pltnum].plot(df_wh.iloc[i,column],df_wh.iloc[i,column+1],'o',markerfacecolor=colour,markeredgecolor='k', markersize=marker_size)\n\n        plt.show()","execution_count":214,"outputs":[]},{"metadata":{"_cell_guid":"9e0517b2-7415-47a6-9f9e-0762bb2a5bdf","_uuid":"1e0a11734c670954cf3bdc310f1fa7cbc77c5066","trusted":true},"cell_type":"code","source":"drawDbscanPlot(dbsclabels)","execution_count":215,"outputs":[]},{"metadata":{"_cell_guid":"63901861-7c9e-444f-bdb3-6491fef4f6cc","_uuid":"446d6856287f6448c04bd93b00aae7f83d0ba80b"},"cell_type":"markdown","source":"#### F.  ) Mini Batch K-Means - Mini-batch KMeans is very useful in case of extremely large datasets and/or very high dimensional data which is often the case in text mining. One can switch to Mini-batch KMeans training while creating KMeans object as follows.Mini Batch K-means has been proposed as an alternative to the K-means algorithm for clustering massive datasets. The advantage of this algorithm is to reduce the computational cost by not using all the dataset each iteration but a subsample of a fixed size"},{"metadata":{"_cell_guid":"09bf3872-908c-45ab-ba10-efa49f27e9fd","_uuid":"6977004932ea31f9f43c343f37b9990259159b0d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Method for Mini batch K means clustering\ndef minikmeans_Clustering(data,numberOfClusters):\n       #Initializing Kmeans.cluster object was imported from sklearn in begining.\n       minikmeans = cluster.MiniBatchKMeans(n_clusters=numberOfClusters, max_iter=100, batch_size=100)\n       # Fitting the input data and getting the cluster labels\n       cluster_labels = minikmeans.fit_predict(data)\n       # Getting the cluster centers\n       cluster_centers = minikmeans.cluster_centers_\n       cluster_centers.shape\n       return cluster_labels,cluster_centers","execution_count":216,"outputs":[]},{"metadata":{"_cell_guid":"b6283668-6610-4f37-93c5-080229f34916","_uuid":"ab257cbf557d1da4601f46919c52f070836af085","collapsed":true,"trusted":true},"cell_type":"code","source":"def plot_miniKMeans(mini_labels,mini_centers):\n    numberOfClusters=np.unique(mini_labels).size\n    #Getting number of columns\n    numOfDimensions = df_wh.columns.size\n    #number of rows\n    numOfRows=len(df_wh.index)\n    #Number of plots required for 6 dimension with 2 dimensions in each plot\n    numberOfPlots = int(numOfDimensions/2)\n    #Number of rows and columns for subplots\n    colorsArray=['b','g','r']\n    #print(type(dbsclabels))\n    fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n    for column,pltnum in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n        #For every dimension first we will plot the Kmean cluster center\n        for row in range(0,numberOfClusters):\n             #we need to select unique labels for 3 centers to have same colors as rest of the cluster data but with different marker\n             colour=colorsArray[np.unique(mini_labels)[row]]\n             ax[pltnum].plot(mini_centers[row,column],mini_centers[row,column+1],'o',markerfacecolor=colour,markeredgecolor=colour, markersize=12)\n        #For Every dimension we will plot the cluster data\n        for row in range(0,numOfRows):\n             #for every row in every dimension we need to assign different color for cluster label assign to row, so we will select label number and which will select color using tht number\n             colour=colorsArray[mini_labels[row]]\n             ax[pltnum].plot(df_wh.iloc[row,column],df_wh.iloc[row,column+1],'.',markerfacecolor=colour,markeredgecolor=colour,markersize=4)\n\n    plt.show()","execution_count":217,"outputs":[]},{"metadata":{"_cell_guid":"da2e8a48-7597-4274-8f95-bfeab3e00f1f","_uuid":"d9044f681045f1eed48ba1d79e91c25b7da20945","trusted":true},"cell_type":"code","source":"mini_labels,mini_centers=minikmeans_Clustering(df_wh,3)\nplot_miniKMeans(mini_labels,mini_centers)","execution_count":218,"outputs":[]},{"metadata":{"_cell_guid":"3cef6c1e-86a5-4de8-8f0c-9b5d3ae309d8","_uuid":"5429912a8918bbc589a47a630c2ca7efcbf67346"},"cell_type":"markdown","source":"####  G.) Birch-Balanced Iterative Reducing and Clustering using Hierarchies is hierarichal clustering algorithm. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as AgglomerativeClusterin\nThere are two key phases for Birch clustering-\n1. Scans the database to build an in-memory tree\n2. Applies clustering algorithm to cluster the leaf nodes\nIt builds a dendrogram called clustering feature tree (CF tree) while scanning the data set. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children."},{"metadata":{"_cell_guid":"c32e3a28-345a-4247-88bc-d71c92d5f7f6","_uuid":"8baca40603b7f1fa96e135b94ad1066cee62f333","collapsed":true,"trusted":true},"cell_type":"code","source":"# Method for Birch clustering\ndef birch_Clustering(data,numberOfClusters):\n       #Initializing Kmeans.cluster object was imported from sklearn in begining.\n       birch_clust = cluster.Birch(n_clusters=numberOfClusters)\n       # Fitting the input data and getting the cluster labels\n       cluster_labels = birch_clust.fit_predict(data)\n       # Getting the cluster centers\n       cluster_centers = birch_clust.subcluster_centers_\n      \n       return cluster_labels,cluster_centers","execution_count":219,"outputs":[]},{"metadata":{"_cell_guid":"32aae3d3-ac46-4b6f-8663-68afe6f1dd76","_uuid":"a864920fcc024ce18877526d2394fceb45328988","collapsed":true,"trusted":true},"cell_type":"code","source":"def plot_birch(birch_labels,birch_centers): \n    numberOfClusters=np.unique(birch_labels).size\n    #Getting number of columns\n    numOfDimensions = df_wh.columns.size\n    #number of rows\n    numOfRows=len(df_wh.index)\n    #Number of plots required for 6 dimension with 2 dimensions in each plot\n    numberOfPlots = int(numOfDimensions/2)\n    #Number of rows and columns for subplots\n    colorsArray=['b','g','r']\n    #print(type(dbsclabels))\n    fig,ax = plt.subplots(numberOfPlots,1, figsize=(10,10))\n    for column,pltnum in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n        #For every dimension first we will plot the Kmean cluster center\n        for row in range(0,numberOfClusters):\n             #we need to select unique labels for 3 centers to have same colors as rest of the cluster data but with different marker\n             colour=colorsArray[np.unique(birch_labels)[row]]\n             ax[pltnum].plot(birch_centers[row,column],birch_centers[row,column+1],'o',markerfacecolor=colour,markeredgecolor=colour, markersize=12)\n        #For Every dimension we will plot the cluster data\n        for row in range(0,numOfRows):\n             #for every row in every dimension we need to assign different color for cluster label assign to row, so we will select label number and which will select color using tht number\n             colour=colorsArray[birch_labels[row]]\n             ax[pltnum].plot(df_wh.iloc[row,column],df_wh.iloc[row,column+1],'x',markerfacecolor=colour,markeredgecolor=colour,markersize=4)\n\n    plt.show()","execution_count":220,"outputs":[]},{"metadata":{"_cell_guid":"0ebc9f0a-42e6-4e29-81d7-a623ba0aab38","_uuid":"33d86b37a315315579fca4b72ed8777f1c3615ca","trusted":true},"cell_type":"code","source":"birch_labels,birch_centers=birch_Clustering(df_wh,3)\nplot_birch(birch_labels,birch_centers)","execution_count":221,"outputs":[]},{"metadata":{"_cell_guid":"5a0e8e4d-801e-4fcb-a8a6-e6bd4794fc63","_uuid":"f9ae216088840d1789f33c185efcaed3a3804383"},"cell_type":"markdown","source":"#### H. ) GMM -  A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset.But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments.It can also draw confidence ellipsoids for multivariate models\nhttps://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"},{"metadata":{"_cell_guid":"4bc0205e-8fae-4b09-bbef-97aa1411e76c","_uuid":"c9220e7752440cc43bf3019afde2dd56e52149ef","trusted":true,"collapsed":true},"cell_type":"code","source":"# Method for GMM clustering\ndef gmm_Clustering(data,numberOfComponents):\n       #Initializing Kmeans.cluster object was imported from sklearn in begining.\n       gmm_clust = mixture.GaussianMixture(n_components=numberOfComponents, covariance_type='full')\n       # Fitting the input data and getting the cluster labels\n       cluster_labels = gmm_clust.fit(data).predict(data)\n       cmeans = gmm_clust.means_ \n       cvariance =  gmm_clust.covariances_ \n       return cluster_labels,cmeans,cvariance,gmm_clust","execution_count":222,"outputs":[]},{"metadata":{"_cell_guid":"f7e88b72-4e89-4c78-b355-b900e7c97481","_uuid":"64de4e998b1353621d7f6a7c501e26317cef9080","trusted":true},"cell_type":"code","source":"def plot_gmm(gmm_Labels,cluster_means,covariances,gmm): \n    numberOfClusters=np.unique(gmm_Labels).size\n    #Getting number of columns\n    numOfDimensions = df_wh.columns.size\n    #number of rows\n    numOfRows=len(df_wh.index)\n    #Number of plots required for 6 dimension with 2 dimensions in each plot\n    numberOfPlots = int(numOfDimensions/2)\n    #Number of rows and columns for subplots\n    colorsArray=['b','g','r','c','m','y']\n    fig,ax = plt.subplots(numberOfPlots,1, figsize=(15,15))\n    for column,pltnum in zip(range(0,numOfDimensions,2),range(0,numberOfPlots)):\n        #For Every dimension we will plot the cluster data\n        for row in range(0,numOfRows):\n             #for every row in every dimension we need to assign different color for cluster label assign to row, so we will select label number and which will select color using tht number\n             colour=colorsArray[gmm_Labels[row]]\n             ax[pltnum].scatter(df_wh.iloc[row,column],df_wh.iloc[row,column+1],c=colour, s=40, cmap='viridis', zorder=2,alpha=0.8)\n             w_factor = 0.2 / gmm.weights_.max()\n    plt.show()","execution_count":223,"outputs":[]},{"metadata":{"_uuid":"810e8f30533f49a85d629ec4c4b3ca96aa495960"},"cell_type":"markdown","source":"#### GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the predict_proba method. This returns a matrix of size [n_samples, n_clusters] which measures the probability that any point belongs to the given cluster.\n    1. Gaussian mixture model is very similar to k-means: it uses an expectation–maximization approach which qualitatively does the following:\n    2. Choose starting guesses for the location and shape\n            Repeat until converged:\n                    E-step: for each point, find weights encoding the probability of membership in each cluster\n                    M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights\n  \n  The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0802593b33a541bcd78d2903a0ef8998574f0a08"},"cell_type":"code","source":"#Draw ellipses for GMM\nfrom matplotlib.patches import Ellipse ## foor creating GMM ellipse\nfrom scipy import linalg\nimport matplotlib as mpl\n\ndef draw_ellipse(mean, covariance, ax):\n             v, w = linalg.eigh(covariance)\n             v = 2. * np.sqrt(2.) * np.sqrt(v)\n             u = w[0] / linalg.norm(w[0])\n             angle = np.arctan(u[1] / u[0])\n             angle = 180. * angle / np.pi  # convert to degrees\n             ell = mpl.patches.Ellipse(mean, v[0], v[1], angle, color='b')\n             ell.set_clip_box(ax.bbox)\n             ell.set_alpha(0.5)\n             ax.add_artist(ell)   ","execution_count":224,"outputs":[]},{"metadata":{"_cell_guid":"b9bd02b8-fce0-47bf-8de4-1182ce9d3bb9","_uuid":"44d76c7d830dc712e23594a25eb0d084c33aa0fa","trusted":true},"cell_type":"code","source":"gmm_Labels,cluster_means,covariances,gmm=gmm_Clustering(df_wh,6)\nplot_gmm(gmm_Labels,cluster_means,covariances,gmm)","execution_count":225,"outputs":[]},{"metadata":{"_uuid":"0f8f986c9d7eee61e58a9423cd2c5e1007d62668"},"cell_type":"markdown","source":"#### Gaussian Mixture Clustering Visualization in global Map"},{"metadata":{"trusted":true,"_uuid":"9408e0d1f51c22e638f38f04d427e81d53588f5a"},"cell_type":"code","source":"df_wh_map = df_wh.reset_index()\ncolorRange = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],[0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\ndata = dict(type = 'choropleth', \n           locations = df_wh_map['Country'],\n           locationmode = 'country names',\n           z = gmm_Labels, \n            colorscale = colorRange,\n           text = df_wh_map['Country'],\n           \n            marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) )\n            \n            ,\n           colorbar = {'title':'Cluster Group'})\n\nlayout = dict(title = 'Gaussian Mixture Clustering group Visualization', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\n\nchoromap3 = go.Figure(data = [data], layout=layout)\n\niplot(choromap3)","execution_count":226,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}