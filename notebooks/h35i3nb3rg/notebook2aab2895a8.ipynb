{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_filepath = os.path.join(dirname, filename)\nprint(filename)\ncsv_file = pd.read_csv(csv_filepath)\ncsv_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are any columns with empty/null dataset ?\nprint('Check if there are any columns with empty/null dataset ?')\nprint(csv_file.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('correlation between all predative features w.r.t total_vaccinations feature')\ncor = csv_file.corr()['total_vaccinations']\nprint(cor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a heatmap of the correlation matrix obtained above.\nprint('heatmap of the correlation matrix ')\nimport seaborn as sb \ndataplot = sb.heatmap(csv_file.corr(), cmap=\"YlGnBu\", annot=True) #rainbow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a heatmap of the correlation matrix obtained above.\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\nprint('heatmap of the correlation matrix ')\nimport seaborn as sb \ndataplot = sb.heatmap(csv_file.corr(), cmap=\"YlGnBu\", annot=True) #rainbow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"csv_file.plot(kind = \"hist\",bins = 40,figsize = (15,15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\ndf_usa = csv_file.loc[csv_file[\"iso_code\"] == 'USA']\ndf_usa = df_usa.dropna()\n\n# df_usa['date'] = pd.to_datetime(df_usa.date, format=\"%Y-%M-%d\")\n# df_usa['date'] = df_usa['date'].map(datetime.datetime.toordinal)\n\n\ndf_usa['date'] = df_usa['date'].str.replace('-','').astype('int')\n\n#training dataset\ndf_usa_training_x = df_usa['date'][:-20].values.reshape(74, 1)\ndf_usa_training_y = df_usa['daily_vaccinations'][:-20].values.reshape(74, 1)\n\n\n#test dataset\ndf_usa_testing_x = df_usa['date'][-20:].values.reshape(20,1)\ndf_usa_testing_y = df_usa['daily_vaccinations'][-20:].values.reshape(20,1)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linearRegression = linear_model.LinearRegression()\nlinearRegression.fit(df_usa_training_x, df_usa_training_y)\ny_prediction = linearRegression.predict(df_usa_testing_x)\n\nplt.scatter(df_usa_testing_x,df_usa_testing_y,  color='black')\n# plt.plot(df_usa_testing_x, y_prediction, color='blue', linewidth=3)\n\nplt.xlabel(\"date\")\nplt.ylabel(\"daily vaccinations (predicted)\")\n\n\nplt.show()\nplt.scatter(df_usa_testing_x,df_usa_testing_y,  color='black')\n# plt.plot(df_usa_testing_x, df_usa_testing_y, color='blue', linewidth=3)\n\nplt.xlabel(\"date\")\nplt.ylabel(\"daily vaccinations (actual)\")\n\n\nprint(f\"r2 score = {r2_score(df_usa_testing_y, y_prediction)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_usa_training_x, df_usa_testing_x, df_usa_training_y, df_usa_testing_y = train_test_split(df_usa['date'], df_usa['daily_vaccinations'], test_size=0.2)\nreg = ensemble.GradientBoostingRegressor()\nreg.fit(df_usa_training_x.values.reshape(75,1), df_usa_training_y)\ny_predictions = reg.predict(df_usa_testing_x.values.reshape(19,1))\nprint(y_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\ndf_usa_training_x.values.reshape(75,1)\ndf_usa_training_y.values.reshape(75,1)\n\ndf_usa_testing_x.values.reshape(19,1)\ndf_usa_testing_y.values.reshape(19,1)\n\n # create regressor object\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n  \n# fit the regressor with x and y data\nregressor.fit(df_usa_training_x, df_usa_training_y)\nregressor.predict(df_usa_testing_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}