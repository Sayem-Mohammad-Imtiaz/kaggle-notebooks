{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"829d5cfe-a294-e132-3e65-09fa10c581d1"},"source":"I am new to ML, so it is a try to explore this dataset and predict speaker\nIf you have any suggestions, I will be pleased to read"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63edcf94-3642-dc81-a453-1e7676b7816c"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nraw_data = pd.read_csv('../input/All-seasons.csv')\nraw_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1eed5ef3-220e-612f-0a62-83e0647936e8"},"outputs":[],"source":"#little summary, not all lines are unique. Moreover alot of characters\nraw_data.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90c9a726-09da-862d-f20e-b240a268e06b"},"outputs":[],"source":"#count lines for character. Data have big \"tale\" of more or less unique characters\n#It seems reasonable to take only those caracters, who spoke at least 100 times (sligtly more than once per episode)\nraw_data.groupby(['Character']).size().sort_values(ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c5e104f-1a65-c18a-5fba-e4c2845db98d"},"outputs":[],"source":"#select lines for top speakres\ntop_speakers = raw_data.groupby(['Character']).size().loc[raw_data.groupby(['Character']).size() > 2000]\n#print(top_speakers.index.values)\nmain_char_lines = raw_data.loc[raw_data['Character'].isin(top_speakers.index.values)]\nmain_char_lines.describe()\n#main_char_lines"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c30ebf4f-7d42-580b-dc8a-b52f3f82daec"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nmain_char_lines['Line'] = [line.replace('\\n','') for line in main_char_lines['Line']]\ntrain, test = train_test_split(main_char_lines, test_size=0.3, random_state=14)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1037e3a9-4ef0-a9b5-14dd-8a4c0b7c543a"},"outputs":[],"source":"#preprocess data, vectorizing lines\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem.lancaster import LancasterStemmer\n\nst = LancasterStemmer()\ndef token(text):\n    txt = nltk.word_tokenize(text.lower())\n    return [st.stem(word) for word in txt]\n\n\nstop = set(stopwords.words(\"english\"))\ncv = CountVectorizer(#lowercase=True, \n                     tokenizer=token, #stop_words=stop,# token_pattern=u'(?u)\\b\\w\\w+\\b',\n                     analyzer=u'word', min_df=4)\n#print(train['Line'].tolist())\n\nvec_train = cv.fit_transform(train['Line'].tolist())\nvec_test = cv.transform(test['Line'].tolist())\n\n#print(vec_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"128ab8d8-e0d1-bb0f-d0dd-a29ce6bc7df4"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\n\nrf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrf.fit(X = vec_train, y = train['Character'])\n\naccuracy_score(rf.predict(vec_test), test['Character'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbd5dd95-be69-b1fb-48ed-6fd6822e4df7"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()#multi_class='multinomial')\nlr.fit(X = vec_train, y = train['Character'])\n\naccuracy_score(lr.predict(vec_test), test['Character'])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}