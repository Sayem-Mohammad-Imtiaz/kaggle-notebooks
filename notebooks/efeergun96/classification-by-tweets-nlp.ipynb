{"cells":[{"metadata":{"_uuid":"117596e590af55f4da0437748add7418a0742f39"},"cell_type":"markdown","source":"## Hello, in this Notebook I will work on Natural Language Processing.\n## I will try classify Male Tweets from Female Tweets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bf219886d71d721975f51a19bbe214097867d34"},"cell_type":"markdown","source":"### EDA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/gender-classifier-DFE-791531.csv\",encoding=\"latin1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8347966e049b370157cffc2b78cd3adda83cf5a6"},"cell_type":"code","source":"df.head()     # There might be some very useful columns but I will only work on text. So I am going to create simpler dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9a80ca715d8318faacf154f57ddaacb57c63ec4"},"cell_type":"markdown","source":"### Data PreProcessing"},{"metadata":{"trusted":true,"_uuid":"37e560eca78d8886b73c48fd350ae615edf567d8"},"cell_type":"code","source":"work_data = pd.DataFrame()\nwork_data[\"tweet\"] = df.description\nwork_data[\"gender\"] = df.gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70a0d2e88d20ee70fdf50aef6b9f241ac7ba38dc"},"cell_type":"code","source":"work_data.head()            # this is one is much easy to read and work with.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2b9f43b462f815bf439a4ab3c230e39b005b30c"},"cell_type":"code","source":"get_female = work_data[\"gender\"] == \"female\"\nget_male = work_data[\"gender\"] == \"male\"\nget_brand = work_data[\"gender\"] == \"brand\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc5ff8afbb4eb3f21e3f4874c6e88bfe35a33298"},"cell_type":"code","source":"female_rows = work_data[get_female]\nmale_rows = work_data[get_male]\nbrand_rows = work_data[get_brand]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"222c9fcd727d632b02ee5bcd8c5046f2b4a8e8c2"},"cell_type":"code","source":"print(\"total female tweets: \",female_rows.tweet.count())\nprint(\"total male tweets:   \",male_rows.tweet.count())\nprint(\"total brand tweets:  \",brand_rows.tweet.count())           # they are evenly distributed. Which is good","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"504069b77d72ef81cb79709ac4f6968b998b557f"},"cell_type":"markdown","source":"### Labelling"},{"metadata":{"trusted":true,"_uuid":"6f41f4eff6956a788e491f38637e64ac68069ab2"},"cell_type":"code","source":"female_rows.gender = 0     # female\nmale_rows.gender = 1       # male\nbrand_rows.gender = 2      # brand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cdb254e9f91ca0d19dfbbd0965499b7f7db33ed"},"cell_type":"markdown","source":"### Concating the brand and person rows"},{"metadata":{"trusted":true,"_uuid":"c09fd0205cb9690395834426cca8202062114577"},"cell_type":"code","source":"frames = [female_rows, male_rows, brand_rows]\ndata = pd.concat(frames,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f7487e85fdb97e40b6b431d2311679c48bfd63c"},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38109aec56b21b65574c0a16fdc50c68108df9bd"},"cell_type":"code","source":"data.info()   # I dont want the null values to became most used words in my bag of words. I will drop them.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"464a78db23407fd0785e56455d55e1cf3cfeaa94"},"cell_type":"code","source":"data.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"055db418be3901a512c12a0511503eca9deca425"},"cell_type":"code","source":"data.info()   # now we have much more clean and useful dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9cbe9137e2ef0507cda710ae214676f70c79e4e"},"cell_type":"markdown","source":"## Natural Language Processing"},{"metadata":{"trusted":true,"_uuid":"55a9f027a36c2273c3e89daf95692a52e0966330"},"cell_type":"code","source":"import re\nimport nltk as nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4da98909752f1fe0fdfb0a1b9023d7714891477"},"cell_type":"code","source":"from nltk.corpus import stopwords\nlemma = nlp.WordNetLemmatizer()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58561308fca5854b7f6ca5fef91cf131e79da081"},"cell_type":"code","source":"tweets_list = []            # empty list\nfor each in data.tweet:\n    each = re.sub(\"[^a-zA-Z]\",\" \", str(each))                                        # regex to clean unnecesarry chars\n    each = each.lower()                                                              # lowercase all\n    each = nlp.word_tokenize(each)                                                   # split all by tokenizing\n    each = [word for word in each if not word in set(stopwords.words(\"english\"))]    # delete stop words from your array\n    each = [lemma.lemmatize(word) for word in each]                                  # lemmatize \"memories\" -> \"memory\"\n    each = \" \".join(each)                                                            # make them one string again\n    tweets_list.append(each)                                                         # put them into big array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"761a08f33ea371eb35332d6656ff9ba051bae75b"},"cell_type":"code","source":"print(\"Original version: \", data.tweet.iloc[2174])\nprint(\"New version:      \", tweets_list[2174])    # no unnecesary words or symbols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"071b64b44c55fe47d3a7d209cd634dad6d23d6c9"},"cell_type":"markdown","source":"## Bag of Words"},{"metadata":{"trusted":true,"_uuid":"2c0cb1bfae2efebb691c06b88f858a405a91e308"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nmax_features = 600\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(tweets_list).toarray()\nwords = count_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c3220349c8dc9921e79f455abfe5d8aa1cea078"},"cell_type":"code","source":"print(\"Most used 600 words on all tweets (alphabetically first 100) :\", words[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fa7bc3e3c586a2be2ea4c0c499a4fc47e05c14c"},"cell_type":"markdown","source":"## Train & Test Split"},{"metadata":{"trusted":true,"_uuid":"ae8ff039864edde31947412e7dd2e63f0e6b31f0"},"cell_type":"code","source":"y = data.gender.values\nx = sparce_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c2c614618e748aa01dbd8082529f392e203a0cae"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\ntrain_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd1cdf11bd9667d5abd5bf0392fcccf39a02f2b"},"cell_type":"markdown","source":"## Machine Learning Model"},{"metadata":{"trusted":true,"_uuid":"24fbdf0352b395094c7742d0bee25b26e555c22c","scrolled":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 100)\n\nrfc.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67ba74715146299d887ed282f12800d7f85dc23c"},"cell_type":"code","source":"rfc.score(test_x,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"705867861640a761fa1fdbddf859492ca632d6df"},"cell_type":"code","source":"y_head_ml = rfc.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d08ed4b0d78aa705d66c9b921f50251a4143f8"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(test_y,y_head_ml)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2249d1ae299624b46ef4df20ccbde02e199b5734"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.heatmap(cm,cbar=False,annot=True,cmap=\"Blues\",fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a902fcb221e1357fa0258012decbb75718e13d88"},"cell_type":"markdown","source":"## ** Conclusion **\n\n### Brand is easy to seperate but genders are relatively more complex"},{"metadata":{"trusted":true,"_uuid":"8571d354fe1a17d6813e94ded1d1d91bfa6f9c10"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}