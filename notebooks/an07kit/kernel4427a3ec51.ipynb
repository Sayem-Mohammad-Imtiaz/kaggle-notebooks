{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat May  9 03:05:02 2020\n\n@author: Ankit\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n\n#models\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n\n\n#Downloading dataset\ndata=pd.read_csv(r'../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\nprint(data)\ntop_data=data.head(3)\ndata.info()\n\n# Categorical boolean mask\ncategorical_feature_mask = data.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = data.columns[categorical_feature_mask].tolist()\n# instantiate labelencoder object\nle = LabelEncoder()\n# apply le on categorical feature columns\ndata[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col))\ndata[categorical_cols].head(10)\nprint(data)\n\n#I split data on 30% in the test dataset, the remaining 70% - in the training dataset\ntrain, test, target, target_test = train_test_split(data[[\"pelvic_incidence\",\"pelvic_tilt numeric\",\"lumbar_lordosis_angle\",\"sacral_slope\",\"pelvic_radius\",\"degree_spondylolisthesis\"]], data[[\"class\"]], test_size=0.3, random_state=1)\ntrain.info()\ntest.info()\n\n\n\n\n\n## Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nacc_log = round(logreg.score(train, target) * 100, 2)\nprint(acc_log)\n\nacc_test_log = round(logreg.score(test, target_test) * 100, 2)\nprint(acc_test_log)\n\nroc_auc_logistic_reg=roc_auc_score(target_test,logreg.predict(test))\nfpr,tpr,thresholds=roc_curve(target_test,logreg.predict(test))\n\n\n\n\n\n## Support Vector Machines\nsvc = SVC()\nsvc.fit(train, target)\nacc_svc = round(svc.score(train, target) * 100, 2)\nprint(acc_svc)\n\nacc_test_svc = round(svc.score(test, target_test) * 100, 2)\nprint(acc_test_svc) \nroc_auc_support_vector=roc_auc_score(target_test,svc.predict(test))\nfpr2,tpr2,thresholds=roc_curve(target_test,svc.predict(test))\n\n\n## Decision Tree Classifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train, target)\nacc_decision_tree = round(decision_tree.score(train, target) * 100, 2)\nprint(acc_decision_tree)\n\nacc_test_decision_tree = round(decision_tree.score(test, target_test) * 100, 2)\nprint(acc_test_decision_tree)\n\nroc_auc_decision_tree=roc_auc_score(target_test,decision_tree.predict(test))\nfpr3,tpr3,thresholds=roc_curve(target_test,decision_tree.predict(test))\n\n## Random Forest\n\nacc_final_random_forest= []\nacc_test_final_random_forest= []\n\n\n\nfor m in range(80,126):\n    random_forest=RandomForestClassifier(n_estimators= m,random_state=1)\n    random_forest.fit(train, target)\n    acc_random_forest=round(random_forest.score(train,target) * 100, 2)\n    acc_final_random_forest.append(acc_random_forest)\n    acc_test_random_forest=round(random_forest.score(test, target_test) * 100, 2)\n    acc_test_final_random_forest.append(acc_test_random_forest)\n\n\n#here it is visible that max. accuracy of test data is at n = 82\nplt.figure()        \nl = range(80,126)\nfor j in range(len(l)):     \n    plt.plot( l, acc_test_final_random_forest)\n    plt.xlabel('Values of n_estimators')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n_estimators values in random forest')\n\n\n\n\n\n\n\n## Ridge Classifier\n\nridge_classifier = RidgeClassifier()\nridge_classifier.fit(train, target)\nacc_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\nprint(acc_ridge_classifier)\n\nacc_test_ridge_classifier = round(ridge_classifier.score(test, target_test) * 100, 2)\nprint(acc_test_ridge_classifier)\n\nroc_auc_ridge_classifier=roc_auc_score(target_test,ridge_classifier.predict(test))\nfpr5,tpr5,thresholds=roc_curve(target_test,ridge_classifier.predict(test))\n\n\n##KNN CLASSIFIER\n\nacc_knn_classifier= np.empty((10, 1))\n\n\nacc_test_knn_classifier= np.empty((10, 1))\n\n\nfor i in range(0,10):\n      knn =KNeighborsClassifier(n_neighbors=i+1)\n      knn.fit(train, target)\n      acc_knn_classifier[i,:]=round(knn.score(train, target) * 100, 2)\n      acc_test_knn_classifier[i,:]= round(knn.score(test, target_test) * 100, 2)\n      \n      \n      \n    \n     \n#here it is visible that max. accuracy of test data is with value of n=3   \nplt.figure()        \nl = range(1,11)\nfor j in range(len(l)):     \n    plt.plot( l, acc_test_knn_classifier)\n    plt.xlabel('Values of n_neighbors')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n values in knn method')\n        \n        \n##K-mean CLASSIFIER\n\nacc_k_mean_classifier= np.empty((10, 1))\nacc_test_k_mean_classifier= np.empty((10, 1))\n\nfor p in range(0,10):\n      k_mean =KMeans(n_clusters=p+1)\n      k_mean.fit(train, target)\n      acc_k_mean_classifier[p,:]=round(knn.score(train, target) * 100, 2)\n      acc_test_k_mean_classifier[p,:]= round(knn.score(test, target_test) * 100, 2)\n      \n     \n#here we can see the number of cluster are not able to affect the accuracy of test data   \nplt.figure()       \nm = range(1,11)\nfor k in range(len(m)):     \n    plt.plot( m, acc_test_k_mean_classifier)\n    plt.xlabel('Values of n_clusters')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n values in k-mean')\n    \n    \n\n\n##plotting roc curves\n\nplt.figure()\nplt.plot(fpr,tpr,Label=\"roc_auc_logistic_reg(area=%0.2f)\" % roc_auc_logistic_reg )\nplt.plot(fpr2,tpr2,Label=\"roc_auc_support_vector(area=%0.2f)\" % roc_auc_support_vector )\nplt.plot(fpr3,tpr3,Label=\"roc_auc_decision_tree(area=%0.2f)\" % roc_auc_decision_tree )\nplt.plot(fpr5,tpr5,Label=\"roc_auc_ridge_classifier(area=%0.2f)\" % roc_auc_ridge_classifier )\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\nplt.title('RECIEVER OPERATING CHARACTERISTIC')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n##DETAILS\ndetails_data=data.describe()\nprint(details_data)\n\n##Models evaluation\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines','Decision Tree Classifier', 'Random Forest','Ridge Classifier','KNN CLASSIFIER','K-mean CLASSIFIER'],\n    \n    'Score_train': [acc_log, acc_svc,acc_decision_tree, acc_final_random_forest[4],acc_ridge_classifier,acc_k_mean_classifier[2,:],acc_k_mean_classifier[2,:]],\n    'Score_test': [acc_test_log, acc_test_svc,acc_test_decision_tree, acc_test_final_random_forest[4], acc_test_ridge_classifier,acc_test_k_mean_classifier[2,:],acc_test_k_mean_classifier[2,:]]})\n\n\nmodels['Score_diff'] = abs(models['Score_train'] - models['Score_test'])\nmodels.sort_values(by=['Score_diff'], ascending=True)\nprint(models)\n\n\n## Final Plot\nplt.figure()\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score_train'], label = 'Score_train')\nplt.plot(xx, models['Score_test'], label = 'Score_test')\nplt.legend()\nplt.title('Score of 7 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}