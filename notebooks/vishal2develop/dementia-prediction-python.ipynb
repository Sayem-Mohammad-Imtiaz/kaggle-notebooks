{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# For Imputation\nfrom sklearn.preprocessing import LabelEncoder\n\n# For data preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\n\n# For model building\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# For visualizing the descision tree\nfrom sklearn import tree\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score,auc\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import (accuracy_score,\n                             f1_score,\n                             log_loss,\n                             roc_auc_score,\n                             roc_curve,\n                             confusion_matrix)\nfrom sklearn.model_selection import (cross_val_score,\n                                     GridSearchCV,\n                                     RandomizedSearchCV,\n                                     learning_curve,\n                                     validation_curve,\n                                     train_test_split)\n\nfrom sklearn.pipeline import make_pipeline # For performing a series of operations\n\nfrom sklearn.metrics import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mri-and-alzheimers/oasis_longitudinal.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting a feel of the data types of the columns\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **SES has 19 missing values**\n* **MMSE has 2 missing values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # for numerical cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imputing Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> For MMSE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.MMSE.fillna(df.MMSE.median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> For SES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SES.fillna(df.SES.median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No More Missing Values**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dropping Hand Column\n> It contains only one kind of values and hence isnt useful for our model. These type of features are called ***Zero Varaince Predictor*** and should be avoided","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns='Hand',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reversing the order of SES values\n> Like CDR, SES is also a level based feature. In CDR the value start from 0 and goes till 2 defining the seriousness of dementia. Whereas in SES , SES=1 (Highest Status) and SES = 5 (Lowest Status) which is the opposite of the trend in CDR.\n\n> Therefore reversing the values of SES so that, SES = 1 (Lowest Status) and SES = 5 (Highest Status)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reversing using mapping\nses_map = {5:1,4:2,3:3,2:4,1:5}\ndf.SES = df.SES.map(ses_map)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SES.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saving clean data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()\ndf.to_csv('oasis_longitude.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding M/F - Gender","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_map = {'M':0, 'F':1}\ndf['Gender'] = df['M/F'].map(gender_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns='M/F',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_map = {'Nondemented':0,'Demented':1,'Converted':2}\n\ndf['Group'] = df.Group.map(target_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the figure\nfig, ax = plt.subplots(figsize=(12,8))\n\n# Generate a custom colormap\ncmap = sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio (mask to not display upper triangle part)\nsns.heatmap(corr, mask=mask, cmap=cmap, ax=ax, annot=True);\nplt.savefig('corr.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Between ASF and eTIV there is a high negative correlation**\n* **Between MR Delay and Visit there is a high positive correlation**\n\n**Hence we will have to drop any one from both sets**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> **Checking the distribution of the target variable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(df['CDR'])\nplt.title('Distribution of CDR Levels')\nplt.xlabel('CDR LEVEL')\nplt.ylabel('COUNT')\nplt.savefig('CDR_distribution.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The problem we have is a Multi-Class Classification Problem**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Plotting against Target Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='CDR',y='SES',data=df,kind='box',size=5,aspect=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = df.SES.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(a.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create list of indicies of SES counts\nses_count = df['SES'].value_counts()\nses_indexes = list(ses_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor s in ses_indexes:\n    # Select the SES category\n    subset = df[df['SES'] == s]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = s, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20);\nplt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by SES', size = 28);\nplt.savefig('SES_CDR.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **High SES group (4) have CDR score 0 as a common value**\n* **Low SES group (1) have CDR score 0.5 as a common value**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='CDR',kind='count',col='SES',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking if Education has an effect on CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.EDUC.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create list of indicies of SES counts\nedu_count= df['EDUC'].value_counts()\nedu_index = list(edu_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor el in edu_index:\n    # Select the SES category\n    subset = df[df['EDUC'] == el]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = el, shade = False, alpha = 0.8,bw=0.5);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Years of Education', size = 28);\n#plt.xlim([0,2]);\nplt.savefig('EDU_CDR.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not helpful lets plot for the top 2 values of EDUC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Min and Max years of education among subjects\nmin_edu = df.loc[df['EDUC']==12]\nmax_edu = df.loc[df['EDUC']==16]\n\n# Stack them into a combine dataframe\nedu_concat = pd.concat([min_edu,max_edu])\nedu_concat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create list of indicies of SES counts\nedu_= edu_concat['EDUC'].value_counts()\nedu_index = list(edu_.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 8))\n\n# Plot each building\nfor el in edu_index:\n    # Select the SES category\n    subset = edu_concat[edu_concat['EDUC'] == el]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = el, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Years of Education', size = 28);\n#plt.xlim([0,2]);\nplt.savefig('EDU_CDR.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Not a remarkable insight but subject with 12 Years of education has slightly greater CDR score than subject with 16 years of education ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Does Gender have an effect?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create list of indicies of SES counts\ngender_count= df['Gender'].value_counts()\ngender_indicies = list(gender_count.index)\n\n# Plot of distribution of scores for building categories\nplt.figure(figsize=(12, 10))\n\n# Plot each building\nfor g in gender_indicies:\n    # Select the SES category\n    subset = df[df['Gender']==g]\n    \n    # Density plot of CDR scores\n    sns.kdeplot(subset['CDR'],\n               label = g, shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('CDR Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of CDR Scores by Gender', size = 28, );\nplt.savefig('Gender_CDR.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 = Female have lower CDR level than male (0). Females seems to be healthier according to the dataset at hand","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Age vs CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='Age',data=df,hue='Gender')\nplt.savefig('Age_CDR.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Not really insightful, but cdr scores for the age range of 65 -85 vary a lot. But that in no way indicates that age influences CDR score**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MMSE vs CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='MMSE',data=df, hue='Gender')\nplt.savefig('MMSE_CDR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MMSE Scores below 25 have a higher probability of getting CDR**\n* The Ones with moderate Dementia have MMSE < 25\n* The Ones with Mild Dementia have MMSE < 25","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## eTIV vs CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='eTIV',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ASF vs CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR',y='ASF',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## nWBV vs CDR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.catplot(x='CDR', y='nWBV', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **eTIV & ASF plots doesnt signify much**\n* **However, nWBV vs CDR, the normal Whole Brain Volume decreases as CDR level increases.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Summary of EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **MMSE: ** From the plots above we can infer that high MMSE scores relate with low CDR levels. Therefore, MMSE is an important feature in predicting CDR Levels\n\n* **SES: ** Couldn't understand much from the plots to certainly say that SES has an influence on CDR scores. But would like to keep it. Going with my Intiution!\n\n* **Gender: ** Gender did suggest that females are heralthier than males and hence it is an important feature\n\n* **ASF: ** No idea!\n\n* **eTIV: ** No Idea!\n\n* **nWBV: ** As Normal Whole Brain Volume decreases CDR Increases. nWBV has an influence on CDR\n\n* **EDUC: ** As seen in the above plots lower Education subject has slighlty greater CDR score than the subjects with higher Education. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlated Features\n\n* **ASF and eTIV - drop one**\n* **Visit and MR_Delay - drop one**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Drop features that are highly correlated when using Linear Classifiers**\n* **We can drop CDR column and keep Group Column as both represent the same thing**\n* **We can drop Subject Id and MRI ID as they are irrevelant**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_df = df.drop(['Subject ID','MRI ID','CDR'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns\nrename_cols_dict = {'EDUC':'Education',\n                   'Group':'Diagnosis'}\nselected_df.rename(rename_cols_dict,axis=1,inplace=True)\nselected_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Target & Predictor feature(s)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = selected_df.Diagnosis.values\n\npredictors_df = selected_df.drop(['Diagnosis'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into training and testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(selected_df.Diagnosis)\nplt.title('Distribution of Diagnosis')\nplt.xlabel('Diagnosis')\nplt.ylabel('COUNT')\nplt.savefig('Diagnosis_distribution.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Stratify to maintaine the same ratio of target variable values in both train and test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(predictors_df,target,test_size=0.2,stratify=target,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Data - Predictors\",x_train.shape)\nprint(\"Testing Data - Predictors\",x_test.shape)\nprint(\"Training Data - Target\",y_train.shape)\nprint(\"Testing Data - Target\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing Evaluation Metric\n\nWe will be going forward with AUC and Diagnostic Odds Ratio","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline # For performing a series of operations\n\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build random forest classifier\nmethods_data = {'Original': (x_train,y_train)}\n\nfor method in methods_data.keys():\n    pip_rf = make_pipeline(StandardScaler(),\n                           RandomForestClassifier(n_estimators=500,\n                                                  class_weight=\"balanced\",\n                                                  random_state=123))\n    hyperparam_grid = {\n        \"randomforestclassifier__n_estimators\": [10, 50, 100, 500],\n        \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5],\n        \"randomforestclassifier__min_samples_leaf\": [1, 3, 5],\n        \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n    \n    gs_rf = GridSearchCV(pip_rf,\n                         hyperparam_grid,\n                         scoring=\"f1_macro\",\n                         cv=10,\n                         n_jobs=-1)\n    \n    gs_rf.fit(methods_data[method][0], methods_data[method][1])\n    \n    print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters for {} data:\".format(method))\n    for hyperparam in gs_rf.best_params_.keys():\n        print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_rf.best_params_[hyperparam])\n    \n    print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_rf.best_score_) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refit RF classifier using best params\nclf_rf = make_pipeline(StandardScaler(),\n                       RandomForestClassifier(n_estimators=10,\n                                              criterion=\"gini\",\n                                              max_features=0.4,\n                                              min_samples_leaf=3,\n                                              class_weight=\"balanced\",\n                                              n_jobs=-1,\n                                              random_state=123))\n\n\nclf_rf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Gradient Boosting classifier\npip_gb = make_pipeline(StandardScaler(),\n                       GradientBoostingClassifier(loss=\"deviance\",\n                                                  random_state=123))\n\nhyperparam_grid = {\"gradientboostingclassifier__max_features\": [\"log2\", 0.5],\n                   \"gradientboostingclassifier__n_estimators\": [100, 300, 500],\n                   \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, 0.1],\n                   \"gradientboostingclassifier__max_depth\": [1, 2, 3]}\n\ngs_gb = GridSearchCV(pip_gb,\n                      param_grid=hyperparam_grid,\n                      scoring=\"f1_macro\",\n                      cv=10,\n                      n_jobs=-1)\n\ngs_gb.fit(x_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_gb.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_gb.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_gb.best_score_) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build logistic model classifier\npip_logmod = make_pipeline(StandardScaler(),\n                           LogisticRegression(class_weight=\"balanced\"))\n\nhyperparam_range = np.arange(0.5, 20.1, 0.5)\n\nhyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n                   \"logisticregression__C\":  hyperparam_range,\n                   \"logisticregression__fit_intercept\": [True, False]\n                  }\n\ngs_logmodel = GridSearchCV(pip_logmod,\n                           hyperparam_grid,\n                           scoring=\"accuracy\",\n                           cv=2,\n                           n_jobs=-1)\n\ngs_logmodel.fit(x_train, y_train)\n\nprint(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\nprint(\"-\" * 25)\nfor hyperparam in gs_logmodel.best_params_.keys():\n    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_logmodel.best_params_[hyperparam])\n\nprint(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_logmodel.best_score_) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\"RF\": clf_rf,\n              \"LR\": gs_logmodel,\n              \"GBT\": gs_gb\n             }\n\n# Print out accuracy score on test data\nprint(\"The accuracy rate on test data are:\")\nfor estimator in estimators.keys():\n    print(\"{}: {:.2f}%\".format(estimator,\n        accuracy_score(y_test, estimators[estimator].predict(x_test)) * 100\n          ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = gs_gb.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_df.Diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names=['RandomForestClassifier','Logistic Regression','GradientBoostingClassifier']\nmodels = [clf_rf,gs_logmodel,gs_gb]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_models(model):\n    clf=model\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    \n    # Calculating various metrics\n    \n    acc.append(accuracy_score(pred,y_test))\n    #prec.append(precision_score(pred,y_test))\n    #rec.append(recall_score(pred,y_test))\n    #auroc.append(roc_auc_score(pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=[]\nprec=[]\nrec=[]\nauroc=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    compare_models(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={'Modelling Algo':model_names,'Accuracy':acc}\nmet_df=pd.DataFrame(d)\nmet_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **GBT has better accuracy, but its not enough**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Todo - Label Encode Diagnosis column**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}