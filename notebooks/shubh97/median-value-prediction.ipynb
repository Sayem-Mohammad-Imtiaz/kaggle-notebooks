{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing the required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '../input/california-housing-prices/housing.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!head -10 $data_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_data = pd.read_csv(data_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Changing the ocean_proximity datatype to category"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_df_copy = housing_data.copy()\n\nhousing_df_copy['ocean_proximity'] = housing_df_copy.ocean_proximity.astype('category')\nhousing_df_copy['ocean_proximity'] = pd.get_dummies(housing_df_copy.ocean_proximity)\nhousing_df_copy.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_df_copy.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_df_copy.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eda(dataframe):\n    assert type(dataframe) == pd.core.frame.DataFrame, 'Invalid Input Passed'\n    \n    column_headers = dataframe.columns\n    number_rows = len(dataframe)\n    \n    print('{: >20} {: >10} {: >10} {: >10} {: >70}'.format('Column Name', 'DataType', \n                                                           '# Null', '# unique', \n                                                           'Top 5 unique'), end='\\n\\n')\n    for column in column_headers:\n        datatype = dataframe[column].dtype\n        null_values = sum(dataframe[column].isnull())\n        unique_values_count = dataframe[column].nunique()\n        unique_values = dataframe[column].unique()[:5]\n        \n        \n        print('{: >20} {: >10} {: >10} {: >10} {: >70}'.format(column, str(datatype), \n                                                               null_values, unique_values_count, \n                                                               str(unique_values)))\n\n    fig = plt.gcf()\n    fig.set_size_inches(12, 8)\n    \n    sns.heatmap(dataframe.corr(), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda(housing_df_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(housing_df_copy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping Unnecessary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since total_bedrooms has missing values and its highly correlated \n# with other features but very less correlated with target value, hence dropping it.\n\nhousing_df_copy.drop('total_bedrooms', axis=1, inplace=True)\n\n# Dropping Highly Correlated Features\n\nhousing_df_copy.drop(['latitude', 'total_rooms', 'households'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = housing_df_copy.pop('median_house_value')\nX = housing_df_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a tensor from dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = torch.from_numpy(X.values)\ny = torch.from_numpy(y.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.00002\n\nX -= X.min(1, keepdim=True)[0]\nX /= X.max(1, keepdim=True)[0]\n\ny -= y.min(-1, keepdim=True)[0]\ny /= y.max(-1, keepdim=True)[0]\n\nW = torch.randn(X.size()[1], requires_grad=True)\nb = torch.randn(1, requires_grad=True)\n\nloss_list = []\n\nweights = []\n\nfor epoch in range(1, 10001):\n    weights.append(W)\n    y_hat = W.float() @ X.t().float() + b.float()\n    loss = torch.sum((y_hat - y)**2)\n    loss_list.append(loss)\n    loss.backward()\n\n    with torch.no_grad():\n        W -= learning_rate * W.grad\n        b -= learning_rate * b.grad\n\n    W.grad.zero_()\n    b.grad.zero_()\n    \n    if epoch % 1000 == 0:\n        print(f'Running {epoch} epoch')\n        print('loss', loss.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_list)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}