{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how this data looks like"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"suicide = pd.read_csv('/kaggle/input/suicide-rates-overview-1985-to-2016/master.csv')\nsuicide.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick first look. There seems to be a lot of null values in HDI column. Let's see how many data available in total and how many of them are missing in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'gdp-for-year' is a categorical column instead of int column. We convert this column into int dtype"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide[' gdp_for_year ($) ']=suicide[' gdp_for_year ($) '].apply(lambda val: val.replace(',', ''))\nsuicide[' gdp_for_year ($) '] = pd.to_numeric(suicide[' gdp_for_year ($) '])\nsuicide[' gdp_for_year ($) ']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a good news that we all data for most of our columns except HDI for year. We may consider dropping it depending on how important it is to predict the result.\n\nThere are 6 categorical columns. We may consider about transform these columns into classifications. The same with some numerical columns such as year, suicides_n. \n\nLet's explore the data more before considering about what we should do with each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out how many countries are in this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide['country'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A pretty good data with a variety of countries. And those countries are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide['country'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a lot of repetives in generation. Shall we explore a little bit more about this feature?"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide['generation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This becomes more interesting. I am intrigued to find our more about the difference between generations and how the those difference affects the suicide rates. But before I do that, I should split my data into train and test set so that I am not prone to choose some models and find insights because of the test set."},{"metadata":{},"cell_type":"markdown","source":"Use 20% of our data for testing. We choose a random state of 1 so that we have the same test set every time we rerun the code"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(suicide, test_size=0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the function split the data the way we want it to be"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.count()[0]/suicide.count()[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the test set is 20% of the orignal data. Perfect"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at our train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observe the distribution of each category in each column to make sure the test set is the representation of the whole population. We don't want to have so many more data of one category compared to the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot('sex',data=test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('age',data=test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the distribution of sex and age are fairly equal "},{"metadata":{},"cell_type":"markdown","source":"The generation is quite important factor if suicide rate. We want to make sure that the generation in the test and the train set is the representation of the whole population"},{"metadata":{},"cell_type":"markdown","source":"Observe the distribution of each geneneration compared to other generations in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['generation'].value_counts()/len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide['generation'].value_counts()/len(suicide)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test set is quite a good representation of the test set. But could we make it better?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state = 1)\nfor train_index, test_index in split.split(suicide, suicide['generation']):\n    strat_train = suicide.loc[train_index]\n    strat_test = suicide.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check our stratified test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test['generation'].value_counts()/len(strat_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like the new strat_test represents the population better compared to the random test set. We put everything into one table to we could see the difference better"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a function to compare the proportions of different set\ndef generation_proportions(test_set):\n    return test_set['generation'].value_counts()/len(test_set)\ncompare_props = pd.DataFrame({\n                'Overall': generation_proportions(suicide),\n                'Random': generation_proportions(test),\n                'Stratified': generation_proportions(strat_test)}\n                            )\ncompare_props['%err random'] = 100*(compare_props['Random'] - compare_props['Overall'])/ compare_props['Overall']\ncompare_props['%err stratified'] = 100*(compare_props['Stratified'] - compare_props['Overall'])/ compare_props['Overall']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our stratified test set represents the population significanly better. "},{"metadata":{},"cell_type":"markdown","source":"Now it is time for us to explore the train data. Make a copy of the training set so that we do not harm the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide = strat_train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observe the correlation between different features with the rates of suicide"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = suicide.corr()['suicides/100k pop'].sort_values(ascending=False)\ncorr_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(suicide.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(suicide)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be no strong correlation between other features and suicide rates. "},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsuicide['age'].replace({'5-14 years':'05-14','15-24 years':'15-24','25-34 years':'25-34','35-54 years':'35-54','55-74 years':'55-74','75+ years':'75+'},inplace=True)\nsns.set_style('whitegrid')\nsns.catplot('age','suicides/100k pop',kind='bar',data=suicide.sort_values(by='age'), hue ='sex',palette='coolwarm')\nplt.xlim(0,5.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look like there are much more male committing suicide in each age range than female. And the gap gets bigger as the age range increase "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(suicide['suicides/100k pop'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 10 countries with highest suicide rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = suicide.groupby('country').mean().sort_values(by='suicides/100k pop',ascending=False)['suicides/100k pop']\ncountries.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use geopandas to visualize the distribution of suicide rates on the world map"},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas as gpd\nimport geoplot as gplt\n#Create a variable holding the map of the world\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge the map data and our suicide data\nmerge = world.set_index('name').join(countries,how='inner')\nmerge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like we can just get access to 78 countries out of 101 countries from our map data. But good enough for us to visualize"},{"metadata":{},"cell_type":"markdown","source":"Compare the suicide rate and population"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, sharey=True,figsize=(20,20))\nax1.set_title('Suicide Rate')\nax2.set_title('Population')\nmerge.plot(column='suicides/100k pop',cmap='Reds',ax=ax1)\nmerge.plot(column='pop_est',cmap='Reds',ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The countries with less population tend to be the countries with higher suicide rate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"merge[['suicides/100k pop','pop_est']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the correlation is not really strong, the negative sign valididates our observation. This correlation cannot indicate anything since the rate of suicide is collected from different years and the population of each country varies every year."},{"metadata":{},"cell_type":"markdown","source":"Look at the distribution of the year "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(suicide['year'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an increase in suicide rates from 1985 to 2000. This can be due to increase in stress as technology develops or due to the increase in the quality of recording data"},{"metadata":{},"cell_type":"markdown","source":"Now it is time to prepare the data for machine learning algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperate the feature we wants to predict from the training data\n#Drop suicides_no and suicides/100k pop they are dependent variables. We could easily predict the rate of suicides by using suicides_no and population.  \nsuicide = strat_train.drop(['suicides_no','country-year'],axis=1)\nsuicide_labels = strat_train['suicides/100k pop']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step is to clean our data.\nLet's take a look at our data again"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the information of the data, we identify two first things we could do to prepare the data for training:\n1. Convert categorical data into numerical data\n2. Fill in the missing data\n\nLet's start with task 1. \n\nWe will use LabelEncoder to transform text categories to integer categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ncat_attribs = suicide[[column for column in suicide.columns if suicide[column].dtype == 'object']]\n\nle = LabelEncoder()\n\nsuicide_cat = cat_attribs.apply(lambda col: le.fit_transform(col))\n\nsuicide_cat.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ML algorithm will consider the closer integer to have more simlarity, which is not the case here. The second option is to use one hot encoder to change our categorical columns into vectors of values 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide_cat_dummies = pd.get_dummies(suicide, columns=cat_attribs.columns, drop_first=True )\nsuicide_cat_dummies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this method would signicantly increase the dimension of our data, this could lead to over-fitting. We decide to preprocess the category data with LabelEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"1 - suicide['HDI for year'].count()/len(suicide)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 70% missing data in column 'HDI for year'. We could not use missing data for training model. We can either drop that feature entirely or fill in the misising data. Since this feature is important for training the model, we choose to fill in the missing data. \n\nTo decide which value to fill in, first observe the distribution of 'HDI for year'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(suicide['HDI for year'].dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide.describe()['HDI for year']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the mean and the median is quite similar, we feel safe to use median as value to fill in"},{"metadata":{"trusted":true},"cell_type":"code","source":"median = suicide['HDI for year'].median()\nfilled_HDI = suicide['HDI for year'].fillna(median)\nfilled_HDI.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our filled HDI has roughly the same mean, min, and max as our original column. We want to preserve as much as we can the information of the original data. This result is what we want."},{"metadata":{},"cell_type":"markdown","source":"Now look at the distribution of HDI again with the values filled in"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(filled_HDI)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since about 70% of our data is filled with the median, it makes sense that the majority of our data centers in the median value. "},{"metadata":{},"cell_type":"markdown","source":"Take care of our missing values using Imputer to see if we obtain the same result. Imputer becomes handy if we want to fill in missing values in different columns at once"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n \nimputer = Imputer(strategy='median')\n\nnum_attribs = suicide[suicide.columns[suicide.dtypes != 'object']]\n\n#Since imputer just applies to numerical columns, we drop categorical columns\nsuicide_num = imputer.fit_transform(num_attribs)\nsuicide_num = pd.DataFrame(suicide_num,columns=suicide.columns[suicide.dtypes != 'object'])\nsuicide_num['HDI for year'].describe()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got the same result for HDI column as we did earlier"},{"metadata":{},"cell_type":"markdown","source":"Since ML algorithms do not do well with different scales between different features. We use Standard Scaler to standardize our values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_suicide_num = scaler.fit_transform(suicide_num)\n\nscaled_suicide_num = pd.DataFrame(scaled_suicide_num,columns=suicide.columns[suicide.dtypes != 'object'])\n\nscaled_suicide_num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see from the dataframe, every of our numerical is standardized. This would help the ML algorithms run more efficiently.\n\nSo far we have:\n1. Preprocess numerical data with Imputer, StandardScaler. \n2. Preprocess categorical data with LabelEncoder\n\nWe could combine steps in step 1 and 2 in 2 pipelines. Then combine those pipeline using ColumnTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nnum_pipeline = Pipeline([\n    ('imputer', Imputer(strategy='median')),\n    ('scaler', StandardScaler(with_mean=False))\n])\n\ncat_pipeline = Pipeline([\n    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n    ('scaler', StandardScaler(with_mean=False))\n])\n\nfull_pipeline = ColumnTransformer([\n    ('num_pipeline', num_pipeline, list(num_attribs.columns)),\n    ('cat_pipeline', cat_pipeline, list(cat_attribs.columns)),\n])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide_prepared = full_pipeline.fit_transform(suicide)\ntype(suicide_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time for selecting and training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(suicide_prepared,suicide_labels)\nlr_predictions = lr.predict(suicide_prepared)\nlr_predictions\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the mean difference between the predictions and the real values \n(lr_predictions-list(suicide_labels)).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Mean Absolute Error for more accurate evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlrmse = np.sqrt(mean_squared_error(suicide_labels,lr_predictions))\nlrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Try DecisionTreeRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndr = DecisionTreeRegressor(random_state=0)\ndr.fit(suicide_prepared,suicide_labels)\ndr_predictions = dr.predict(suicide_prepared)\n\ndrmse = np.sqrt(mean_squared_error(suicide_labels,dr_predictions))\ndrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get really good results with both linear regressor and decision tree regressor using mean squared error. Decision tree regressor seems to be a better choice than linear regression. But this is not an accurate evaluation since we use one data to train and evaluate. We could instead use Cross-Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores_1 = cross_val_score(dr, suicide_prepared, suicide_labels, scoring = \"neg_mean_squared_error\", cv = 10)\ntree_scores = np.sqrt(-scores_1)\ntree_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_2 = cross_val_score(lr, suicide_prepared, suicide_labels, scoring = \"neg_mean_squared_error\", cv = 10)\nlr_scores = np.sqrt(-scores_2)\nlr_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a better evaluation of our model. Now linear regression has about the same error score as the previous evaluation method while decision tree has much worse performance. This shows that the Decision Tree model is overfitting really badly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=30, random_state=42)\nforest_reg.fit(suicide_prepared, suicide_labels)\n\nscores_3 = cross_val_score(forest_reg, suicide_prepared, suicide_labels, scoring = \"neg_mean_squared_error\", cv = 10)\nrf_scores = np.sqrt(-scores_3)\nrf_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RandomForestRegressor performs well but not as good as LinearRegression. This indicates that the random forest regressor is also overfitting. We could choose the right hyperparameter to prevent overfitting. This can be done using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8, 10]}\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True,\n                          error_score=np.nan)\ngrid_search.fit(suicide_prepared, suicide_labels) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres_df = pd.DataFrame(cvres)\ncvres_df[\"mean_score\"] = cvres_df['mean_test_score'].apply(lambda x:np.sqrt(-x) )\ncvres_df[[\"mean_score\",\"params\"]].sort_values(by='mean_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=10),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(suicide_prepared, suicide_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_search.best_params_\nrnd_results = rnd_search.cv_results_\nrnd_results = pd.DataFrame(rnd_results)\nrnd_results['mean_score'] = rnd_results['mean_test_score'].apply(lambda x: np.sqrt(-x))\nrnd_results[[\"mean_score\",\"params\"]].sort_values(by='mean_score')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}