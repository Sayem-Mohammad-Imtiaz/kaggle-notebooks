{"cells":[{"metadata":{},"cell_type":"markdown","source":"***This code has successfully ran using kernel conda_tensorflow_p36 in AWS Sagemaker***"},{"metadata":{"colab_type":"text","id":"xT7MKZuMRaCg"},"cell_type":"markdown","source":"# Sentiment Classification using IMDB Dataset\n"},{"metadata":{"colab_type":"text","id":"Wq4RCyyPSYRp"},"cell_type":"markdown","source":"### Loading the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\n\n#filter out top 10000 used words\nvocab_size = 10000 ","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"fCPC_WN-eCyw","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"qMEsHYrWxdtk"},"cell_type":"markdown","source":"## Train test split "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# save np.load\nnp_load_old = np.load\n\n# modify the default parameters of np.load\nnp.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n\n# call load_data with allow_pickle implicitly set to true\n#load dataset as a list of ints\n# vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n\n# restore np.load for future normal usage\nnp.load = np_load_old","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"h0g381XzeCyz","trusted":true},"cell_type":"code","source":"#Maximum sequence length\n#number of words used from each review\nmaxlen = 300  \n\n#make all sequences of the same length using pad_sequences\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test =  pad_sequences(X_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"colab_type":"code","id":"Jy6n-uM2eCy2","outputId":"218e86b6-2190-4aa6-9d58-4e8fdcc0470d","trusted":true},"cell_type":"code","source":"print(X_train[8],y_train[8])\n\n#Here the X_train is sequence representing the most commonly used words in the overall data say 1:1st commonly used word,171:171st commonly used word in the data.","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"colab_type":"code","id":"fvI7tsJKFXNp","outputId":"902c9038-101e-46a2-b5ac-ca6500812a4b","trusted":true},"cell_type":"code","source":"print(X_train[558],y_train[558])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"A5OLM4eBeCy9","trusted":true},"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"TxNDNhrseCzA","trusted":true},"cell_type":"code","source":"def create_seq_model():\n  model = Sequential()\n  #Here the 10000 is some random number, which is much larger than needed to reduce the probability of collisions from the hash function\n  #The number 10k should be greater than the total no of letters in each sequence\n  model.add(Embedding(10000,256,input_length=300))\n  model.add(Bidirectional(LSTM(32, return_sequences = True)))\n  model.add(GlobalMaxPool1D())\n  model.add(Dense(20, activation=\"relu\"))\n  model.add(Dropout(0.05))\n  model.add(Dense(1, activation=\"sigmoid\"))\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  return model","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Igq8Qm8GeCzG"},"cell_type":"markdown","source":"Model Creation using Keras"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"colab_type":"code","id":"-dUDSg7VeCzM","outputId":"03aac925-f5ce-418d-99ad-77c1db9ffdb1","trusted":true},"cell_type":"code","source":"seq_nlp_model=create_seq_model()\n\n# summarize the model\n\nprint(seq_nlp_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"colab_type":"code","id":"Tskt_1npeCzP","outputId":"60d6392f-6ea9-476f-83b5-5d934f6cc4b5","trusted":true},"cell_type":"code","source":"''' \nbatch_size = 100\nepochs = 3\n'''\n\n\nbatch_size = 10\nepochs = 3\n\n# fit the model\nseq_nlp_model.fit(X_test,y_test, batch_size=batch_size, epochs=epochs, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"rIUe__knjQdy","trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"JIjQ7hgLjTwS","trusted":true},"cell_type":"code","source":"y_pred = seq_nlp_model.predict(X_test)\n\n\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"SeqNLP_Project1_Questions-1.ipynb","provenance":[]},"kernelspec":{"display_name":"conda_tensorflow_p36","language":"python","name":"conda_tensorflow_p36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}