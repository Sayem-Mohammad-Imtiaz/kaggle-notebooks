{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# keras model\n# for text classification, with multiple classes (multi-class)\n# but single label\n# character-level tokenization\n# with fixed length input\n# based on convolutional layers\n\n# using customer complaints dataset\n# we classify a narrative text about an issue into a product category\n# https://www.kaggle.com/cfpb/us-consumer-finance-complaints\n\n# See also\n# https://www.kaggle.com/kadhambari/multi-class-text-classification\n# https://www.kaggle.com/anucool007/multi-class-text-classification-bag-of-words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utility functions for later\n\ndef dict_to_csv(d, path):\n    df = pd.DataFrame.from_dict(d, orient='index')\n    df.to_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\ndf = pd.read_csv('../input/consumer_complaints.csv', usecols=('product', 'consumer_complaint_narrative'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df))\ndf.head()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove nan's\ndf = df.dropna() # drop row if have nan in any column\nprint(len(df))\ndf.head()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode product","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this turns each string into a number (most popular are lowest)\nproduct_encoding = pd.factorize(df['product'])\nprint(product_encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, index = product_encoding\nprint(labels) # encoding for each product in the dataset\nprint(index) # index -> string map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build label <-> index maps to use later\nproduct_to_id = {name: i for i, name in enumerate(index)}\nid_to_product = {i: name for i, name in enumerate(index)}\nprint(product_to_id)\nprint(id_to_product)\nprint(len(index)) # number of classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_to_csv(product_to_id, 'labels_index.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that the classes are imbalanced\nfor product in index:\n    print(product, len(df.loc[df['product'] == product]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encode\ny = keras.utils.to_categorical(labels)\nprint(len(y))\nprint(y[0], labels[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode input with character level tokenization and embeddings\n\nfrom keras.preprocessing.text import Tokenizer\n\ntok = Tokenizer(num_words=None, # don't limit number of characters\n                lower=False, # don't lower\n                char_level=True, # character-level tokenization\n                oov_token='<OOV>', # token for unknown characters\n                                   # FIXME: multi-character token but should be single char?\n               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = df['consumer_complaint_narrative'].values\nprint(texts[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok.fit_on_texts(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tok.texts_to_sequences(texts)\nprint(x[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(x)\ndf.to_csv('x_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(y)\nnp.savetxt('y_data.csv', y, fmt=\"%d\", delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create word index to use later\n\n# word -> index map\nword_index = tok.word_index\nword_index['<PAD>'] = 0 # set unused index to padding token\nprint(word_index['<PAD>'], word_index['<OOV>'], word_index[' '])\n\n# index -> word map\nreversed_word_index = {v:k for k, v in word_index.items()}\nprint(reversed_word_index[0], reversed_word_index[1], reversed_word_index[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_to_csv(word_index, 'word_index.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorized_to_tokens(sample):\n    return [reversed_word_index.get(num, '<OOV>') for num in sample]\n    \ndef tokens_to_string(tokens):\n    return ''.join(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(x), len(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad to fixed length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find a good length to pad to\nlengths = [len(sample) for sample in x]\nprint(len(lengths))\nprint(lengths[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = np.percentile(lengths, 95)\nprint(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = int(p)\nprint(maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx = pad_sequences(x,\n                  padding='post',\n                  truncating='post',\n                  value=word_index['<PAD>'],\n                  maxlen=maxlen,\n                )\nprint(len(x))\nprint(len(x[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train and test data\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)\n\nprint(len(x_train), len(x_test))\nprint(len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokens_to_string(vectorized_to_tokens(x_test[0])))\nprint(id_to_product[np.argmax(y_test[0])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\n\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding,\n    Conv1D,\n    Dense,\n    MaxPooling1D,\n    AveragePooling1D,\n    Flatten,\n    GlobalAveragePooling1D,\n    Dropout,\n)\n\nmodel = Sequential([\n    Embedding(len(word_index), 8, input_length=maxlen),\n    Conv1D(128, 15, activation='relu'),\n    Dropout(0.2),\n    MaxPooling1D(2),\n    Conv1D(128, 10, activation='relu'),\n    Dropout(0.2),\n    AveragePooling1D(2),\n    Conv1D(128, 5, activation='relu'),\n    Dropout(0.2),\n    MaxPooling1D(2),\n    GlobalAveragePooling1D(),\n    Dense(32, activation='relu'),\n    Dense(11, activation='softmax'),\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\n\nepochs = 47\nbatch_size = 512\n\nhistory = model.fit(x, \n                    y, \n                    epochs=epochs, \n                    batch_size=batch_size,\n                    verbose=2, \n                    validation_split=0.3,\n                   )\n\n# baseline: 1/11 ~= 0.1 accuracy with random guessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\nprint(model.evaluate(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('keras_text_model_multiclass.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}