{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gensim\nimport string\n\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.utils.data_utils import get_file\n\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad4c6ebbf87e6859c268397f808a811e15fee69"},"cell_type":"code","source":"data = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\")\ndata[:2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32d78c769835b4bc6ce5274460c0a8f4782a5619"},"cell_type":"markdown","source":"# Preparation"},{"metadata":{"trusted":true,"_uuid":"d1bd88d44805baa1f07db0af85f53469433485b4"},"cell_type":"code","source":"print(\"Preparing text\")\nreviews = data[\"Review Text\"].dropna()\ntokens = [[word for word in nltk.word_tokenize(doc.lower()) if word] for doc in reviews]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1455ec78cf15e7e7332cabac9679be7dc341f152"},"cell_type":"markdown","source":"# Word2Vec using this corpus"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# from this gist: https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\nprint('Training word2vec')\nword_model = gensim.models.Word2Vec(tokens, size=25, min_count=1, window=5, iter=100)\npretrained_weights = word_model.wv.syn0\nvocab_size, embedding_size = pretrained_weights.shape\nprint('Result embedding shape:', pretrained_weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c09f4a7f64f2146b9c6dee3203accfffcb6d03d6"},"cell_type":"code","source":"print('Checking similar words:')\nfor word in ['dress', 'tall', 'return', 'sweater']:\n    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n    print('  %s -> %s' % (word, most_similar))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f362e0701849f9a998c07ceee11cff7598961695"},"cell_type":"code","source":"model.save(\"word2vec-clothes-25d.model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88449746d00d148bffedb320bedf02065fb8ceee"},"cell_type":"markdown","source":"## Bonus: Pretrained Word2Vec\nI'll concatenate my trained word2vec to a pretrained one. Let's see the similar words after.\n(todo)"},{"metadata":{"_uuid":"d84648bb28988f7bd301dba9abde4be519b6a2bc"},"cell_type":"markdown","source":"# Clustering\nWe'll use homogeneity score against the rating to come up with topics with uniform ratings.\n## Baseline: Vanilla LDA on 10 topics using count vectorizer "},{"metadata":{"trusted":true,"_uuid":"a2a228e6a69e248442fda70344b6febb7cff5562"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nN_TOPICS = 10\n\nvec = TfidfVectorizer(stop_words = nltk.corpus.stopwords.words('english'), ngram_range=(1,3), min_df=5, max_df = 0.9)\nlda = LatentDirichletAllocation(n_components = N_TOPICS, )\n\npipeline = Pipeline([(\"vec\", vec), (\"lda\", lda)])\npipeline.fit(reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91583c6ed495ac4b25d5b3122e74792d28f39208"},"cell_type":"code","source":"vec_model = pipeline.steps[0][1]\nlda_model = pipeline.steps[1][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d60f2cf8ac63b9cc2f0887694f79d681f1bffee3"},"cell_type":"code","source":"from sklearn.metrics import homogeneity_score\n\ncluster_labels = np.argmax(pipeline.transform(reviews), axis=1)\nhomogeneity_score(data.loc[reviews.index, \"Rating\"], cluster_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dece77b5a1c9815228edd20dfad011093194cb5d"},"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\"Mean: {:.2f}\".format(data.loc[np.where(cluster_labels == topic_idx)[0], \"Rating\"].mean()))\n        print(\"Std: {:.2f}\".format(data.loc[np.where(cluster_labels == topic_idx)[0], \"Rating\"].std()))\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 10\ndisplay_topics(lda_model, vec_model.get_feature_names(), no_top_words)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c5c16b2e8df26f7f3df675ea438687f9541fa32"},"cell_type":"markdown","source":"## K-Means on the word2vec\n- Average each word in the sentence\n- Do clustering"},{"metadata":{"trusted":true,"_uuid":"323bf9443f5dd7094c1de5e0dd83433959750ccb"},"cell_type":"code","source":"# some utility code\ndef word2idx(word):\n    return word_model.wv.vocab[word].index\ndef idx2word(idx):\n    return word_model.wv.index2word[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"846fb657a43708dd69376de5b6d366ff15b005ba"},"cell_type":"code","source":"# get mean word2vec of each sentence\nX_averaged_word2vec = [np.mean([word_model.wv.word_vec(word) for word in sentence], axis=0) for sentence in tokens]\nX_averaged_word2vec = np.array(X_averaged_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7549628d50ddc3aec4b95c3adea24b83f3b35ba"},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters= N_TOPICS)\nkmeans.fit(X_averaged_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2cabbf01ac3d37df2ad690a5b35920a074f4d1"},"cell_type":"code","source":"from sklearn.metrics import homogeneity_score\n\ncluster_labels = kmeans.predict(X_averaged_word2vec)\nhomogeneity_score(data.loc[reviews.index, \"Rating\"], cluster_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f49c17a3bf9de8a85795181bf0fb192c1ad9319"},"cell_type":"markdown","source":"## Getting the usual words per topic\n- Get frequency of all words\n- Get frequency of words inside each cluster\n- Divide the frequencies.\n- Intuition: if a word appeared only in 1 topic and it's frequent in that topic then it may be a unique word"},{"metadata":{"trusted":true,"_uuid":"790dec61d9f381b5bf1f571ad63c38629079062c"},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\n\ndef get_freq_per_topic(topic_idx):\n    topic_sentences = reviews.reindex(np.where(cluster_labels == topic_idx)[0])\n    topic_words = [nltk.word_tokenize(v) for v in topic_sentences.dropna()]\n    flattened_topic_words = [word.lower() for sublist in topic_words for word \n                             in sublist if len(word) >= 3 and word not in stopwords]\n    return pd.Series(nltk.FreqDist(flattened_topic_words))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85c4ec39e298777244e3f34f55c47056d9fefdc2"},"cell_type":"code","source":"# frequency of all words across all reviews\nall_words = [nltk.word_tokenize(v) for v in reviews.dropna()]\nflattened_all_words = [word.lower() for sublist in all_words \n                       for word in sublist if len(word) >= 3 and word not in stopwords]\nall_words_frequency = pd.Series(nltk.FreqDist(flattened_all_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8422c68a57a5bebb06ceba698a3d8386351795"},"cell_type":"code","source":"list_series_frequencies = [get_freq_per_topic(v) for v in range(N_TOPICS)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"883f7bfe1fa9aa9c5e119261b7f4ce1ebfbdd91f"},"cell_type":"code","source":"for cluster in range(N_TOPICS):\n    top_words_cluster = list_series_frequencies[cluster].sort_values(ascending=False)[:10000]\n    uniqueness_index = top_words_cluster.div(all_words_frequency, ).dropna().sort_values(ascending=False)\n    uniqueness_index = uniqueness_index[(uniqueness_index > 0.2)]\n    top_20 = list_series_frequencies[cluster].reindex(uniqueness_index.index)\\\n                                            .sort_values(ascending=False).index.tolist()[:20]\n    print(\"--- Cluster\", cluster, \"---\")\n    print(\"Mean: {:.2f}\".format(data.loc[np.where(cluster_labels == cluster)[0], \"Rating\"].mean()))\n    print(\"Std: {:.2f}\".format(data.loc[np.where(cluster_labels == cluster)[0], \"Rating\"].std()))\n    print(top_20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b76c5fc5c8c6f5f03dc712f48c1514ee1fce660"},"cell_type":"markdown","source":"# Preparing Input for Text Generation"},{"metadata":{"trusted":true,"_uuid":"9da5fe546508c9b675cd8726af302eecd2fc37a0"},"cell_type":"code","source":"%%time\nprint('\\nPreparing the data for LSTM...')\n\nmaxlen = max([len(v) for v in tokens])\nTIMESTEPS = 100\n\n# train_x = np.zeros([len(tokens), maxlen], dtype=np.int32)\n# train_y = np.zeros([len(tokens)], dtype=np.int32)\n\nlist_x = []\nlist_y = []\n\nfor _, sentence in enumerate(tokens):\n    for i in range(len(sentence)):\n        if len(sentence) <= i + TIMESTEPS:\n            # the last word is the target\n            input_words = sentence[:-1]\n            target_word = sentence[-1]\n            \n            list_input_words = []\n            for t, word in enumerate(input_words):\n                list_input_words.append(word2idx(word))\n            list_x.append(list_input_words)\n            list_y.append(word2idx(target_word))\n            break\n        else:\n            input_words = sentence[i:i+TIMESTEPS]\n            target_word = sentence[i+TIMESTEPS]\n            \n            list_input_words = []\n            for t, word in enumerate(input_words):\n                list_input_words.append(word2idx(word))\n            list_x.append(list_input_words)\n            list_y.append(word2idx(target_word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905d6ac06e97c86c16e0912889eccea4ab183c82"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_indexes = pad_sequences(list_x, maxlen=TIMESTEPS, padding='post')\ny_indexes = np.array(list_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"accd97c0e6bf7deae7b189cdd719a2a442bceb1c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Train/Test splits\nX_train, X_test, y_train, y_test = train_test_split(x_indexes, y_indexes, test_size=0.4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff74f38e331e28bb3e868654c280c174c04481c4"},"cell_type":"markdown","source":"# LSTM Architecture"},{"metadata":{"trusted":true,"_uuid":"8c4769e022ef1be05d1529ba4fe430ae99e3d371"},"cell_type":"code","source":"from keras.layers import Input, LSTM, RepeatVector, TimeDistributed\nfrom keras.models import Model\n\nLSTM_SIZE = 50\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, \n                    weights=[pretrained_weights]), )\nmodel.add(LSTM(units=LSTM_SIZE))\nmodel.add(Dense(units=vocab_size))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n\n# # autoencoder model\n# model = Sequential()\n# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], input_length=TIMESTEPS))\n# model.add(LSTM(units=LSTM_SIZE))\n# model.add(RepeatVector(TIMESTEPS))\n# model.add(LSTM(embedding_size, return_sequences=True))\n# model.add(LSTM(embedding_size, ))\n# model.add(TimeDistributed(Dense(1)))\n\n# model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae', 'mse'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf2780d95e9d1b7e0de8057e9a1a29e4d9fc9ec9"},"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    if temperature <= 0:\n        return np.argmax(preds)\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef generate_next(text, num_generated=10):\n    word_idxs = [word2idx(word) for word in text.lower().split()]\n    for i in range(num_generated):\n        prediction = model.predict(x=np.array(word_idxs))\n        idx = sample(prediction[-1], temperature=0.7)\n        word_idxs.append(idx)\n    return ' '.join(idx2word(idx) for idx in word_idxs)\n\ndef on_epoch_end(epoch, _):\n    print('\\nGenerating text after epoch: %d' % epoch)\n    texts = [\n    'it\\'s a very',\n    'i',\n    'this is',\n    'i wanted',\n    'again'\n    ]\n    for text in texts:\n        sample = generate_next(text)\n        print('%s... -> %s' % (text, sample))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a8009874e4c3bde7539c3348125bdc1ed42ec69d"},"cell_type":"code","source":"from keras.callbacks import LambdaCallback\n\nhistory = model.fit(X_train, y_train, validation_split= 0.2,\n                  batch_size=2048,\n                  epochs=100,\n                  callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c615efcd164a46853cb921ee00a5ef104fc988b9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b341866bf131ce09f29e169dcc07fa4d3df863e9"},"cell_type":"code","source":"# import keras.backend as K\n\n# K.clear_session()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}