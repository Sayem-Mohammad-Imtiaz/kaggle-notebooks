{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:10:34.086279Z","iopub.status.busy":"2021-03-31T03:10:34.085583Z","iopub.status.idle":"2021-03-31T03:10:39.899262Z","shell.execute_reply":"2021-03-31T03:10:39.900173Z"},"papermill":{"duration":5.838977,"end_time":"2021-03-31T03:10:39.900543","exception":false,"start_time":"2021-03-31T03:10:34.061566","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport shutil\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### input an image to see what the image size is\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path=r'../input/english-handwritten-characters-dataset/Img/img001-001.png'\nimg=cv2.imread(image_path)\nprint (img.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### very large image size - will use 224 X224 in the model\n### read in the csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(r'../input/english-handwritten-characters-dataset/english.csv')\nprint (df.head())\nclasses= list(df['label'].unique())\nclass_count = len(classes)\nprint (class_count)\nprint (classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### split the data frame into a train_df, a test_df and a valid_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_split=.7\ntest_split=.15\ndummy_split=test_split/(1-train_split)\ntrain_df, dummy_df=train_test_split (df, train_size=train_split, shuffle=True, random_state=123)\ntest_df, valid_df= train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=123)\nprint ('train size: ', len(train_df), '  test size: ', len(test_df), '  valid size: ', len(valid_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### need to check that split resulted in test and valid dataframes have all 62 classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_count= len(train_df['label'].unique())\ntest_count= len(test_df['label'].unique())\nvalid_count= len(valid_df['label'].unique())\nprint ('train count: ', train_count, '  test count: ', test_count, '  valid count: ', valid_count )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### each data frame has all 62 classes as required"},{"metadata":{},"cell_type":"markdown","source":"### for test you want to go through all the samples exactly once so you want to set the batch size and test steps such that\n### batch_size X test_steps= number of test samples= len(test_df) code below finds those values"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 80\nlength=len(test_df)\ntest_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=batch_size],reverse=True)[0]  \ntest_steps=int(length/test_batch_size)  \nprint ('test batch size: ', test_batch_size, '   test steps: ', test_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create the train, test and valid generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"sdir=r'../input/english-handwritten-characters-dataset'\nimage_shape=(224,224)\nclass_mode='categorical'\ndef scalar(img):\n        return img/127.5 -1\ntgen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)\ngen= tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=scalar)\ntrain_gen=tgen.flow_from_dataframe(train_df, sdir, x_col='image', y_col='label',target_size=image_shape,\n                                        class_mode=class_mode, batch_size=batch_size, shuffle=True, random_state=123)\ntest_gen=tgen.flow_from_dataframe(test_df, sdir,  x_col='image', y_col='label',target_size=image_shape, \n                                       class_mode=class_mode, batch_size=test_batch_size, shuffle=False)\nvalid_gen=tgen.flow_from_dataframe(valid_df, sdir, x_col='image', y_col='label',target_size=image_shape,\n                                       class_mode=class_mode, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.050526,"end_time":"2021-03-31T03:12:28.774976","exception":false,"start_time":"2021-03-31T03:12:28.72445","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define function to show some test images"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:28.884092Z","iopub.status.busy":"2021-03-31T03:12:28.883574Z","iopub.status.idle":"2021-03-31T03:12:28.887604Z","shell.execute_reply":"2021-03-31T03:12:28.88718Z"},"papermill":{"duration":0.061819,"end_time":"2021-03-31T03:12:28.887714","exception":false,"start_time":"2021-03-31T03:12:28.825895","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def show_image_samples(gen):\n    class_dict=gen.class_indices\n    new_dict={}\n    # make a new dictionary with keys and values reversed\n    for key, value in class_dict.items(): # dictionary is now {numeric class label: string of class_name}\n        new_dict[value]=key        \n    images,labels=next(gen) # get a sample batch from the generator   \n    plt.figure(figsize=(15, 15))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):\n        plt.subplot(5, 5, i + 1)\n        image=(images[i]+1 )/2 # scale images between 0 and 1 becaue pre-processor set them between -1 and +1\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=new_dict[index]\n        plt.title(class_name, color='blue', fontsize=16)\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:29.020271Z","iopub.status.busy":"2021-03-31T03:12:29.01622Z","iopub.status.idle":"2021-03-31T03:12:33.946636Z","shell.execute_reply":"2021-03-31T03:12:33.947069Z"},"papermill":{"duration":5.009292,"end_time":"2021-03-31T03:12:33.947233","exception":false,"start_time":"2021-03-31T03:12:28.937941","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"show_image_samples(train_gen)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.099074,"end_time":"2021-03-31T03:12:34.146872","exception":false,"start_time":"2021-03-31T03:12:34.047798","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### create model using MobilenetV2"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:34.353678Z","iopub.status.busy":"2021-03-31T03:12:34.353118Z","iopub.status.idle":"2021-03-31T03:12:38.298554Z","shell.execute_reply":"2021-03-31T03:12:38.297581Z"},"papermill":{"duration":4.053767,"end_time":"2021-03-31T03:12:38.298707","exception":false,"start_time":"2021-03-31T03:12:34.24494","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"height=224\nwidth=224\nimg_shape=(height, width, 3)\ndropout=.3\nlr=.001\nimg_shape=(height, width, 3)\nbase_model=tf.keras.applications.MobileNetV2( include_top=False, input_shape=img_shape, pooling='max', weights='imagenet') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(512, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu', kernel_initializer= tf.keras.initializers.GlorotUniform(seed=123))(x)\nx=Dropout(rate=dropout, seed=123)(x)        \noutput=Dense(class_count, activation='softmax',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123))(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(lr=lr), loss='categorical_crossentropy', metrics=['accuracy']) ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.098063,"end_time":"2021-03-31T03:12:38.494654","exception":false,"start_time":"2021-03-31T03:12:38.396591","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define nice function to print text in rgb foreground and background colors"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:38.697654Z","iopub.status.busy":"2021-03-31T03:12:38.696837Z","iopub.status.idle":"2021-03-31T03:12:38.699964Z","shell.execute_reply":"2021-03-31T03:12:38.699465Z"},"papermill":{"duration":0.10745,"end_time":"2021-03-31T03:12:38.700097","exception":false,"start_time":"2021-03-31T03:12:38.592647","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.097629,"end_time":"2021-03-31T03:12:38.893934","exception":false,"start_time":"2021-03-31T03:12:38.796305","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define subclass of keras callbacks to control learning rate"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:39.11897Z","iopub.status.busy":"2021-03-31T03:12:39.102344Z","iopub.status.idle":"2021-03-31T03:12:39.122314Z","shell.execute_reply":"2021-03-31T03:12:39.122702Z"},"papermill":{"duration":0.12992,"end_time":"2021-03-31T03:12:39.122851","exception":false,"start_time":"2021-03-31T03:12:38.992931","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class LRA(keras.callbacks.Callback):\n    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,end_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        self.count=0 # initialize counter that counts epochs with no improvement\n        self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n        self.end_epoch=end_epoch # value of the number of epochs to run\n        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed\n        msg=' '\n        if freeze==True:\n            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n        else:\n            msgs=f' Starting training using base model { model_name} training all layers '            \n        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n            \n    def on_epoch_begin(self, epoch, logs=None): # just used to print data from previous epoch\n        if epoch != 0:\n            msgs=f'for epoch {epoch} '\n            msgs=msgs + LRA.msg\n            print_in_color(msgs, (255,255,0), (55,65,80))\n            \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            if acc>self.highest_tracc: # training accuracy improved in the epoch\n                LRA.msg= f' training accuracy improved from  {self.highest_tracc:7.4f} to {acc:7.4f} learning rate held at {lr:10.8f}'\n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss             \n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1:\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss\n                    msgs=f' training accuracy {acc:7.4f} < highest accuracy of {self.highest_tracc:7.4f} '\n                    LRA.msg=msgs + f' for {self.patience } epochs, lr adjusted to {self.lr:10.8f}'                    \n                else:\n                    self.count=self.count +1 # increment patience counter\n                    LRA.msg=f' training accuracy {acc:7.4f} < highest accuracy of {self.highest_tracc:7.4f} '\n                    #print_in_color(msg, (255,255,0), (55,65,80))\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved\n                msgs=f' validation loss improved from {self.lowest_vloss:8.5f} to {v_loss:8.5}, saving best weights'\n                LRA.msg=msgs + f' learning rate held at {self.lr:10.8f}'\n                #print_in_color(msg, (0,255,0), (55,65,80))\n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0                    \n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    self.lr=self.lr * self.factor # adjust the learning rate\n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted                    \n                    msgs=f' val_loss of {v_loss:8.5f} > {self.lowest_vloss:8.5f} for {self.patience} epochs'\n                    LRA.msg=msgs + f', lr adjusted to {self.lr:10.8f}'\n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter\n                    LRA.msg=f' validation loss of {v_loss:8.5f} > {self.lowest_vloss:8.5f}'\n                    #print_in_color(msg, (255,255,0), (55,65,80)) \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        if epoch==self.end_epoch:\n            print_in_color(LRA.msg, (255,255,0), (55,65,80)) # print out data for the final epoch\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            LRA.msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(LRA.msg, (0,255,0), (55,65,80))\n            self.model.stop_training = True # stop training","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.102686,"end_time":"2021-03-31T03:12:39.330397","exception":false,"start_time":"2021-03-31T03:12:39.227711","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### instantiate the callback\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:39.539178Z","iopub.status.busy":"2021-03-31T03:12:39.538205Z","iopub.status.idle":"2021-03-31T03:12:39.618397Z","shell.execute_reply":"2021-03-31T03:12:39.619026Z"},"papermill":{"duration":0.186036,"end_time":"2021-03-31T03:12:39.619236","exception":false,"start_time":"2021-03-31T03:12:39.4332","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"patience=1\nstop_patience=3\nthreshold=.9\nfactor=.5\ndwell=False\nmodel_type='MobilenetV2'\nfreeze=False\nepochs=20\n\ncallbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,dwell=dwell, model_name=model_type, freeze=freeze, end_epoch=epochs - 1 )]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.12752,"end_time":"2021-03-31T03:12:40.042851","exception":false,"start_time":"2021-03-31T03:12:39.915331","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### train the model"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:12:40.373168Z","iopub.status.busy":"2021-03-31T03:12:40.372396Z","iopub.status.idle":"2021-03-31T03:46:32.283938Z","shell.execute_reply":"2021-03-31T03:46:32.28226Z"},"papermill":{"duration":2032.078577,"end_time":"2021-03-31T03:46:32.28413","exception":false,"start_time":"2021-03-31T03:12:40.205553","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"history=model.fit(train_gen, epochs=epochs, validation_data= valid_gen, callbacks=callbacks, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.132152,"end_time":"2021-03-31T03:46:32.542732","exception":false,"start_time":"2021-03-31T03:46:32.41058","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define function to plot training data"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:46:32.808581Z","iopub.status.busy":"2021-03-31T03:46:32.806676Z","iopub.status.idle":"2021-03-31T03:46:32.809266Z","shell.execute_reply":"2021-03-31T03:46:32.809657Z"},"papermill":{"duration":0.13746,"end_time":"2021-03-31T03:46:32.809784","exception":false,"start_time":"2021-03-31T03:46:32.672324","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(15,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:46:33.072918Z","iopub.status.busy":"2021-03-31T03:46:33.072091Z","iopub.status.idle":"2021-03-31T03:46:33.537376Z","shell.execute_reply":"2021-03-31T03:46:33.537846Z"},"papermill":{"duration":0.60768,"end_time":"2021-03-31T03:46:33.538009","exception":false,"start_time":"2021-03-31T03:46:32.930329","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"tr_plot(history, 0)  # plot the loss and accuracy metrics","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.124226,"end_time":"2021-03-31T03:46:33.787921","exception":false,"start_time":"2021-03-31T03:46:33.663695","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define function to display model.evaluate data"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:46:34.043831Z","iopub.status.busy":"2021-03-31T03:46:34.043271Z","iopub.status.idle":"2021-03-31T03:46:34.047637Z","shell.execute_reply":"2021-03-31T03:46:34.047109Z"},"papermill":{"duration":0.136189,"end_time":"2021-03-31T03:46:34.047802","exception":false,"start_time":"2021-03-31T03:46:33.911613","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def display_eval_metrics(e_data):\n    msg='Model Metrics after Training'\n    print_in_color(msg, (255,255,0), (55,65,80))\n    msg='{0:^24s}{1:^24s}'.format('Metric', 'Value')\n    print_in_color(msg, (255,255,0), (55,65,80))\n    for key,value in e_data.items():\n        print (f'{key:^24s}{value:^24.5f}')\n    acc=e_data['accuracy']* 100\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.123637,"end_time":"2021-03-31T03:46:42.912631","exception":false,"start_time":"2021-03-31T03:46:42.788994","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### define function to print model.predict data"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-31T03:46:43.446904Z","iopub.status.busy":"2021-03-31T03:46:43.44605Z","iopub.status.idle":"2021-03-31T03:47:01.78009Z","shell.execute_reply":"2021-03-31T03:47:01.779162Z"},"papermill":{"duration":18.468583,"end_time":"2021-03-31T03:47:01.780248","exception":false,"start_time":"2021-03-31T03:46:43.311665","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"save_dir=r'./'\nsubject='characters'\nmodel.set_weights(LRA.best_weights) # load the best weights saved during training\ne_dict=model.evaluate( test_gen, batch_size=test_batch_size, steps=test_steps, verbose=1,  return_dict=True)\nacc=display_eval_metrics(e_dict)\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (0,255,0),(55,65,80))\nsave_id=str (model_type +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\nsave_loc=os.path.join(save_dir, save_id)\nmodel.save(save_loc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation accuracy is low but continues to decrease - run more epcohs  if accuracy < 80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"if acc < 80.0:\n    extra_epochs=20\n    total_epochs= epochs + extra_epochs\n    history=model.fit(train_gen, epochs=total_epochs, validation_data= valid_gen, callbacks=callbacks, verbose=2, initial_epoch=epochs)\n    model.set_weights(LRA.best_weights) # load the best weights saved during training\n    e_dict=model.evaluate( test_gen, batch_size=test_batch_size, steps=test_steps, verbose=1,  return_dict=True)\n    acc=display_eval_metrics(e_dict)\n    msg=f'accuracy on the test set is {acc:5.2f} %'\n    print_in_color(msg, (0,255,0),(55,65,80))\n    save_id=str (model_type +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n    save_loc=os.path.join(save_dir, save_id)\n    model.save(save_loc)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### make predictions on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict (test_gen, batch_size=test_batch_size, steps=test_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### display confusion matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=test_gen.labels\nsamples=len(preds)\ngood_preds=0\ny_true=[]\ny_pred=[]\nfor i,  p in enumerate (preds):\n    pred_index=np.argmax(p)\n    true_index=labels[i]\n    if pred_index== true_index:\n        good_preds +=1\n    y_true.append(true_index)\n    y_pred.append (pred_index)\nacc=100* good_preds/samples\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (255,255,0), (55,65,80))\ncm = confusion_matrix(y_true, y_pred )\nclr = classification_report(y_true, y_pred, target_names=classes)\nlength=class_count\nif length<8:\n    fig_width=8\n    fig_height=8\nelse:\n    fig_width=length/3\n    fig_height=length/3\nplt.figure(figsize=(fig_width, fig_height))\nsns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \nplt.xticks(np.arange(length)+.5, classes, rotation= 90)\nplt.yticks(np.arange(length)+.5, classes, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()    \nprint(\"Classification Report:\\n----------------------\\n\", clr)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}