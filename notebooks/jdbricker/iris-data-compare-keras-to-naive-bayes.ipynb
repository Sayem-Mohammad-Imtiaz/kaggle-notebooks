{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#immport all my toolz plus a few extra just in case:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport pandas as pd\nimport numpy as np\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load up my dataset:\niris = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check it:\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scale my features, don't really need to do this for random forest but whatever:\nct = make_column_transformer(\n(StandardScaler(),['sepal_length','sepal_width','petal_length','petal_width'])\n)\n\nX = ct.fit_transform(iris)\n#too lazy to import one hot from sklearn, so let's use the built-in pandas method:\ny = pd.get_dummies(iris['species']).values\n#split and make sure we have balanced classes:\nX_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's bring a gun to a knife fight and see what happens:\nmodel = keras.Sequential()\n\nmodel.add(keras.layers.Input(4))\nmodel.add(keras.layers.Dense(4,activation=\"relu\"))\nmodel.add(keras.layers.Dense(3,activation=\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compile and train\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train,y_train,epochs=500, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check accuracy on the test set, gee I wonder...\n#my only surprise is that we only got to 95% accuracy\ny_act = y_test.argmax(axis=1)\ny_pred = model.predict(X_test).argmax(axis=1)\n\nprint(confusion_matrix(y_act,y_pred))\nprint(accuracy_score(y_act,y_pred))\nprint(precision_recall_fscore_support(y_act,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#minimal gainz after around 200 epochs\npd.DataFrame(history.history,index=history.epoch).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#and naive Bayes gets the same accuracy! Score 1 for the Rev. Bayes!\nnb_model = GaussianNB()\nnb_model.fit(X_train,y_train.argmax(axis=1))\n\ny_pred = nb_model.predict(X_test)\nprint(confusion_matrix(y_act,y_pred))\nprint(accuracy_score(y_act,y_pred))\nprint(precision_recall_fscore_support(y_act,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}