{"cells":[{"metadata":{},"cell_type":"markdown","source":"If you want to see more visualizations, please go to [notebook](https://www.kaggle.com/jswxhd/eda-strategies-for-different-data-attributtes). :)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nimport xgboost as xgb \nfrom xgboost import XGBClassifier\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\nraw_data.drop(columns=['CLIENTNUM',\n                       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],\n          axis=1,inplace=True)\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Ordinal Encoding and one-hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"Ordinal features like 'Education_Level' can be encoded with ordinal values to reduce dimension.\nNominal features like 'Marital_Status' is encoded with one-hot encoding."},{"metadata":{},"cell_type":"markdown","source":"For unknown values in the feature 'Education_Level' and 'Income_Category', I encode them as np.nan for further imputation. For unknown values in the feature 'Marital_Status', I treat it as a separate category."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_data.copy()\ndata['Attrition_Flag'] = data['Attrition_Flag'].apply(lambda x: 1 if x=='Attrited Customer' else 0)\ndata['Gender'] = data['Gender'].apply(lambda x: 1 if x=='F' else 0)\n\nedu_dict = {'Unknown':np.nan,'Uneducated':0,'High School':1,'College':2,\n             'Graduate':3,'Post-Graduate':4,'Doctorate':5}\ndata['Education_Level'] = data['Education_Level'].apply(lambda x: edu_dict[x])\n\nmarital_onehot = pd.get_dummies(data['Marital_Status'],prefix='Marital_Status')\ndata = pd.concat([data,marital_onehot],axis=1)\ndata.drop(columns=['Marital_Status','Marital_Status_Unknown'],axis=1,inplace=True)\n\nincome_dict = {'Unknown':np.nan,'Less than $40K':0,'$40K - $60K':1,\n                '$60K - $80K':2,'$80K - $120K':3,'$120K +':4}\ndata['Income_Category'] = data['Income_Category'].apply(lambda x: income_dict[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split train and test data in order to use further encoding and imputation methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns='Attrition_Flag',axis=1)\ny = data['Attrition_Flag']\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Encoding "},{"metadata":{},"cell_type":"markdown","source":"I think count encoding is suitable for the feature 'Card_Category' because value 'Blue' take the dominant place."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_enc = ce.CountEncoder(cols='Card_Category')\ncount_encoded = count_enc.fit(train_x['Card_Category'])\ntrain_x = train_x.join(count_encoded.transform(train_x['Card_Category']).add_suffix('_count'))\ntest_x = test_x.join(count_encoded.transform(test_x['Card_Category']).add_suffix('_count'))\ntrain_x.drop(columns='Card_Category',axis=1,inplace=True)\ntest_x.drop(columns='Card_Category',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Impute Missing Values with KNNImputer"},{"metadata":{},"cell_type":"markdown","source":"From my points of view, null values exist in features 'Education_Level' and 'Income_Category' because they were not recorded. Hence, dropping them directly is not a wise choice. Instead we should use imputing method."},{"metadata":{"trusted":true},"cell_type":"code","source":"Imputer = KNNImputer(missing_values = np.nan, n_neighbors=1, weights='uniform').fit(train_x)\nimputed_train_x = Imputer.transform(train_x)\nimputed_test_x = Imputer.transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(np.isnan(imputed_train_x)),np.sum(np.isnan(imputed_test_x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing is done successfully."},{"metadata":{},"cell_type":"markdown","source":"## Check Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_train = np.column_stack((imputed_train_x,train_y))\ndf_imputed_train = pd.DataFrame(imputed_train)\ndf_imputed_train.columns = list(train_x.columns) + ['Attrition_Flag']\n\ncorr = df_imputed_train.corr()\nfig,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr,ax=ax,vmin=-1, vmax=1, cmap='coolwarm', annot=False)\nplt.yticks(rotation=0,fontsize=13)\nplt.xticks(rotation=90,fontsize=13)\nax.set_title('Correlation Heatmap',fontsize=15,fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There is highly positively correlation between 'Avg_Open_To_Buy' and 'Credit_Limit'.\n2. There is highly negatively correlation betweern 'Marital_Status_Single' and 'Marital_Status_Married'."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = np.full(shape=(corr.shape[0],), fill_value=True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if abs(corr.iloc[i,j]) >= 0.9:\n            if columns[j]:\n                columns[j] = False\nsel_columns = df_imputed_train.columns[columns]\ndf_imputed_train = df_imputed_train[sel_columns]\ndf_imputed_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 'Avg_Open_To_Buy' is dropped because highly correlate with the feature 'Credit_Limit'."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train_x = df_imputed_train.drop(columns='Attrition_Flag',axis=1)\ndf_imputed_test_x = pd.DataFrame(imputed_test_x)\ndf_imputed_test_x.columns = test_x.columns\ncleaned_test_x = df_imputed_test_x.drop(columns='Avg_Open_To_Buy',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"## SMOTE + XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_before = train_y.value_counts()\ntrace = go.Pie(labels = y_cnt_before.index, \n               values = y_cnt_before.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Attrition Flag',\n               titlefont = dict(size=15),\n               hole = 0.5,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combine oversampling for minor class with undersampling for major class."},{"metadata":{"trusted":true},"cell_type":"code","source":"over_sampling = SMOTE(sampling_strategy=0.4, k_neighbors=5, random_state=42)\nunder_sampling = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\npipeline = Pipeline(steps=[('o',over_sampling),('u',under_sampling)])\nou_train_x,ou_train_y = pipeline.fit_resample(cleaned_train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cnt_after = ou_train_y.value_counts()\ntrace = go.Pie(labels = y_cnt_after.index, \n               values = y_cnt_after.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Attrition Flag',\n               titlefont = dict(size=15),\n               hole = 0.5,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customize evaluation function 'reversed recall'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rev_recall(pred,dtrain):\n    labels = dtrain.get_label()\n    pred = np.round(1.0 / (1.0 + np.exp(-pred)))\n    cm = confusion_matrix(labels, pred)\n    recall =float(cm[1][1]) / float(cm[1][0]+cm[1][1])\n    reversed_recall = 1-recall\n    return 'Reversed-Recall',reversed_recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"baseline_params = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'random_state': 42}\n\nbaseline_xgb = xgb.XGBClassifier(**baseline_params)\nbaseline_xgb.fit(ou_train_x,ou_train_y, eval_metric=rev_recall,\n                 eval_set=[(ou_train_x,ou_train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = baseline_xgb.predict(cleaned_test_x)\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,pred_y),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall is 92% in baseline model :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,8))\nxgb.plot_importance(baseline_xgb,ax=ax,importance_type='gain',height=0.6, max_num_features=None)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nax.set_title('Feature Importance',fontsize=15,fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost + Change Class Weight"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"baseline_params_2 = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'scale_pos_weight':3,\n                   'random_state': 42}\n\nxgb_2 = xgb.XGBClassifier(**baseline_params_2)\nxgb_2.fit(cleaned_train_x,train_y, eval_metric=rev_recall,\n                 eval_set=[(cleaned_train_x,train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y_2 = xgb_2.predict(cleaned_test_x)\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,pred_y_2),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the recall is the same as I get in SMOTE+XGBoost, the precision goes down a little. It is the side effect of changing class weight when modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,8))\nxgb.plot_importance(xgb_2,ax=ax,importance_type='gain',height=0.6, max_num_features=None)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nax.set_title('Feature Importance',fontsize=15,fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost + Threshold Moving"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"baseline_params_3 = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'random_state': 42}\n\nxgb_3 = xgb.XGBClassifier(**baseline_params_3)\nxgb_3.fit(cleaned_train_x,train_y, eval_metric=rev_recall,\n                 eval_set=[(cleaned_train_x,train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precision Recall curve and F-score** are chosen to find the best threshold because they reveal the model performance well when the dataset is imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y_3 = xgb_3.predict_proba(cleaned_test_x)\nclass_1_proba = pred_y_3[:,1]\nprecision, recall, thresholds = precision_recall_curve(test_y, class_1_proba)\nf_score = (2*precision*recall)/(precision+recall)\nidx = np.argmax(f_score)\n\nno_skill = np.sum(test_y) / len(test_y)\nfig,ax = plt.subplots(figsize=(8,6))\nax.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\nax.plot(recall, precision, marker='.', label='XGBoost',alpha=0.7)\nax.scatter(recall[idx], precision[idx], s=100, marker='o', color='black', label='Best')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_title('Precision Recall Curve')\nax.legend(loc='center left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('best threshold is %f' % (thresholds[idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_threshold = thresholds[idx]\nfinal_pred_y_3 = [1 if p>=best_threshold else 0 for p in class_1_proba]\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,final_pred_y_3),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_recalls = [float(confusion_matrix(test_y,y_hat)[1][1]/np.sum(confusion_matrix(test_y,y_hat),axis=1)[1]) \n               for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_precisions = [float(confusion_matrix(test_y,y_hat)[1][1]/np.sum(confusion_matrix(test_y,y_hat),axis=0)[1]) \n                  for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_acc = [float((confusion_matrix(test_y,y_hat)[1][1]+confusion_matrix(test_y,y_hat)[0][0])/len(test_y)) \n           for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_methods = ['SMOTE','Change Class Weight','Move Threshold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the result shown below, Recall for all three methods are almost the same. Precision and accuracy is the highest when using the method of moving threshold. From my point of view, resampling data and moving threshold are both good choices when dealing with imbalanced data. Drawback of Changing class weight is that precison will be sacrificed. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Table(header=dict(values=['Method','Recall', 'Precision','Accuracy'], \n                               fill_color='yellow', line_color='black'),\n                               cells=dict(values=[all_methods,all_recalls,all_precisions,all_acc], \n                               fill_color='lavender', line_color='black'))])\nfig.update_layout(width=800,height=700)\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}