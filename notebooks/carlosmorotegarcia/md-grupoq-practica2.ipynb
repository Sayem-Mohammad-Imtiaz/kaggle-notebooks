{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación\\*\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jiménez\n* José Antonio Gámez Martín\n\n### Alumnos:\n\n* Sara López Matilla\n* Carlos Morote García\n\n\\* Adaptado de las prácticas de Jacinto Arias Martínez y Enrique González Rodrigo"},{"metadata":{},"cell_type":"markdown","source":"## Introducción\n\nEn esta libreta se han estudiado y analizado los modelos más utilizados en `scikit-learn`. Se ha buscado el mejor modelo con sus respectivos hiperparámetros, con el objeto de obtener el mejor clasificador posible para los conjuntos de datos de `pima-indians-diabetes` y `breast-cancer-wisconsin-data`.\n\n* Carga de datos\n* Modelos de clasificación supervisada\n* Evaluación de modelos\n* Selección de modelos\n* Construcción y validación del modelo final\n\nSe analizará haciendo una replicación parcial del Kernel: [Logistic Regression & Data Preprocessing por faressayah](https://www.kaggle.com/faressayah/logistic-regression-data-preprocessing)\n\nSin embargo, antes hay que realizar una preparación del entorno.\n"},{"metadata":{},"cell_type":"markdown","source":"## Preparación del entorno\n\nAntes de entrar en materia con los datos es necesario disponer de las herramientas oportunas para hacerlo. Esto es lo que se puede encontrar en este apartado, siendo la parte más fundamental la de importar todos los paquetes de Python que se van a usar."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Third party\nfrom sklearn.base import clone\nfrom sklearn.model_selection import train_test_split\n\n# Preprocessing\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, Normalizer\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.impute import SimpleImputer\n\n# Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Validation\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n# Scripts\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con el objeto de que los experimentos sean reproducibles se ha establecido una semilla para la generación de números aleatorios. La semilla ha sido generada de forma aleatoria también. Se ha obtenido haciendo uso de la página: [random.org](http://random.org)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"seed = 337839890","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pima Indians Diabetes Database\n\nEn esta sección se va a analizar el conjunto de datos `pima_diabetes`. Como ya vimos en la práctica anterior, está compuesto por ocho variables predictoras y una objetivo.\n\nLas variables predictoras son:\n* `Pregnancies`: número de veces que ha estado embarazada.\n* `Glucose`: concentración de la glucosa en dos horas en un test de tolerancia oral a la glucosa.\n* `BloodPressure`: presión arterial diastólica (mmHg).\n* `SkinThickness`: grosor del pliegue de la piel del tríceps (mm).\n* `Insulin`: insulina sérica en dos horas ($\\mu$U/ml).\n* `BMI`: índice de masa corporal (peso(kg)/(altura(m))^2).\n* `DiabetesPedigreeFunction`: función de pedigrí de la diabetes. Esta función analiza las relaciones genealógicas de un ser vivo en el contexto de determinar cómo una cierta característica o fenotipo se hereda y manifiesta, en este caso, la diabetes.\n* `Age`: edad de la paciente.\n\nLa variable objetivo es Outcome. Tiene dos posibles valores: 0 significa que no se tiene diabetes y 1 significa que sí se tiene diabetes."},{"metadata":{},"cell_type":"markdown","source":"### Carga de datos\n\nPrimero se va a cargar el conjunto de datos. Se debe poner el parámetro `index` a `None` debido a que esta base de datos no tiene una columna índice."},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a visualizar una muestra del conjunto de datos, concretamente, cinco instancias aleatorias."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Primero procedemos a partir el conjunto de datos en conjunto de datos de entrenamiento y conjunto de datos de test. Estos dos subconjuntos que se obtienen se subdividen a su vez en `X`, siendo las variables predictoras, y en `y`, siendo la variable objetivo. Se ha optado por dividir el conjunto de datos en un 70% para el conjunto de entrenamiento y un 30% para el conjunto de test.\n\nDebido a que en ocasiones se utilizará el conjunto de entrenamiento y de test sin distinguir entre variables predictoras y objetivo, y en otras ocasiones se distinguirá, se van a obtener los conjuntos dividos y sin dividir."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(train, tests, X_train, X_test, y_train, y_test) = utils.split_data(data, target, seed, train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a comprobar que se ha realizado satisfactoriamente la partición del conjunto de datos en entrenamiento y test."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Numeros de instancias train: {train.shape[0]}\")\nprint(f\"Numeros de variables train: {train.shape[1]}\")\ntrain.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Numeros de instancias test: {tests.shape[0]}\")\nprint(f\"Numeros de variables test: {tests.shape[1]}\")\ntests.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos de clasificación supervisada"},{"metadata":{},"cell_type":"markdown","source":"### Preprocesamiento\n\nSe va a usar el preprocesamiento que se realizó en la práctica anterior para realizar el pipeline, aunque se van a realizar modificaciones que se irán indicando."},{"metadata":{},"cell_type":"markdown","source":"#### Selección de variables"},{"metadata":{},"cell_type":"markdown","source":"Se van a usar todas las variables, excepto SkinThickness y Insulin, como ya hicimos en la práctica anterior. Como ya se comentó, esta selección de variables se va a realizar en el make_column_selector al imputar los valores perdidos implícitamente."},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Eliminación de outliers"},{"metadata":{},"cell_type":"markdown","source":"Para realizar la eliminación de *outliers* se va a usar el mismo método que usamos en la práctica anterior, que se realizaba mediante un `IsolationForest`."},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_outliers = FunctionSampler(func=utils.outlier_rejection, kw_args={\"random_state\":seed})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Imputación de valores perdidos"},{"metadata":{},"cell_type":"markdown","source":"Como se vio en la práctica anterior, hay que imputar los valores perdidos de las variables `Glucose`, `BloodPresure` y `BMI`. Todas estas variables se van a imputar por la media, a diferencia de la práctica anterior, en la que las variables con valores enteros se imputaron por la moda.\n\nVolvemos a resaltar que en este paso se realiza la eliminación de variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_imputing = make_pipeline(SimpleImputer(strategy=\"mean\", missing_values=0))\n\n\nnum = 'BMI|Glucose|BloodPressure'\nnon = 'Pregnancies|DiabetesPedigreeFunction|Age'\n\nimpute_missing = make_column_transformer(\n(numerical_imputing, make_column_selector(pattern=num)),\n('passthrough', make_column_selector(pattern=non)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Discretización"},{"metadata":{},"cell_type":"markdown","source":"En este caso, vamos a variar los discretizadores para ver qué configuración nos permite alcanzar mejores resultados. Sin embargo, eso lo haremos en el grid, así que ahora crearemos solo el discretizador que se usó en la práctica anterior.\n\nSolo vamos a modificar el hiperparámetro `n_bins`. En esta ocasión, se va a juntar la discretización con la codificación, debido a que no hay problemas con la representación del árbol de decisión, que fue lo que hizo que se usara por separado la discretización y la codificación en la práctica anterior. La estrategia será siempre `kmeans` debido a que en la práctica anterior observamos que no había una partición clara por igual anchura o igual frecuencia que permitiese una buena discretización."},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=4, encode=\"onehot\", strategy=\"kmeans\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Normalización\n\nPara esta práctica, va a ser necesario normalizar los datos en algunos algoritmos, como es el caso del algoritmo de los vecinos más cercanos. Para ello, vamos a crear un método que nos permite normalizar los datos."},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = Normalizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos\n\nPrimeros vamos a crear los diferentes pipelines que se van a necesitar para evaluar cada algoritmo. A todos los modelos se les va a eliminar los *outlayers* y se les va a imputar los valores perdidos.\n\nEs necesario tener en cuenta que estos pipelines se van a crear solo para la evaluación de modelos mediante una validación cruzada, la optimización de hiperparámetros y su explicación de por qué se usan o no se hará en la sección de selección de modelos."},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos\n\nEste algoritmo necesita que las variables numéricas estén normalizadas en la misma escala, para ello le vamos a aplicar el normalizador. Este algoritmo no necesita un discretizador.\n\nPara crear este algoritmo, el único parámetro que vamos a incluir es el número de vecinos y cómo se evalua el voto de cada vecino. En optimización de parámetros se modificarán más parámetros."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 5\nweights = 'distance'\n\nk_neighbors_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        normalize,\n        KNeighborsClassifier(n_neighbors,\n                             weights=weights)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Árboles de decisión\n\nComo ya se hizo en la práctica anterior, se le va a aplicar un discretizador como preprocesamiento adicional.\n\nLos hiperparámetros que se van a introducir en este ejemplo de árbol de decisión son la máxima profundidad, que se va a fijar a 5, al no parecer una profundidad demasiado pequeña ni grande, se va a usar el critero de la entropía, se va a aplicar una post-poda con un alfa de 0.1 y se va a restringir el árbol para que cada hoja tenga como mínimo 20 ejemplos."},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        discretizer,\n        DecisionTreeClassifier(random_state=seed,\n                               max_depth=5,\n                               criterion='entropy',\n                               ccp_alpha=0.1,\n                               min_samples_leaf=20)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Adaboost\n\nAdaboost es una versión del ensemble Boosting. Se asignan pesos a las instancias y se aprenden de forma secuencial una serie de árboles débiles y con mucho sesgo. Si un ejemplo es clasificado bien en una iteración, en la siguiente iteración su peso será menor. Si es clasificado mal en una iteración, en la siguiente iteración tendrá mayor peso. La agregación de resultados se realiza por un voto ponderado por la importancia del clasificador.\n\nEn este caso no se va a aplicar ningún preprocesamiento adicional, debido a que la documentación de `scikit-learn` recomienda no discretizar, debido a que se obtienen mejores resultados, así que seguiremos esta recomendación.\n\nEl único hiperparámetro que vamos a modificar de este ensemble en esta ocasión va a ser la profundidad máxima, debido a que para este ensemble se requieren modelos débiles. Por ello, se va a realizar un *Uno-R*."},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        AdaBoostClassifier(random_state=seed,\n                           base_estimator=DecisionTreeClassifier(max_depth=1))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Bagging\n\nEste ensemble aprende un modelo en cada iteración. La particularidad de este algoritmo está en que en cada iteración usa un subconjunto de las instancias de entrenamiento. La elección de este subconjunto se realiza mediante muestreo aleatorio con reemplazo. La agregación de resultados se realiza mediante un voto por la mayoría. Este modelo reduce la varianza al computar los votos.\n\nA este ensemble se le va a pasar el discretizador como preprocesamiento adicional.\n\nEl estimador que se va a usar para este hiperparámetro va a ser un árbol de decisión de máxima profundidad, debido a que este modelo usa árboles muy profundos y muy ramificados."},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        discretizer,\n        BaggingClassifier(random_state=seed,\n                          base_estimator=DecisionTreeClassifier(max_depth=None))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Random Forest\n\nEste ensemble es una combinación de modelos con forma de árbol. Cada árbol se contruye de forma independiente de forma aleatoria mediante una selección de las instancias y las características que componen el árbol. No se lleva a cabo poda, pero se puede limitar el tamaño. Este ensemble reduce la varianza en el modelo base y más aún por la agregación.\n\nComo a todos los modelos basados en árboles que hemos visto hasta ahora, se le aplicará un discretizador como paso adiccional en el preprocesamiento.\n\nEn este caso, vamos a utilizar el árbol por defecto que construye `scikit-learn` si no modificamos sus hiperparámetros."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        discretizer,\n        RandomForestClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Gradient Boosting\n\nEste ensemble mezcla la técnica de Boosting con el gradiente descendiente. En cada iteración se usan los residuos que se han obtenido mediante el gradiente descendiente en la iteración anterior. En cada iteración se usan todos los modelos aprendidos anteriormente.\n\nA este ensemble se le va a aplicar el discretizador en el preprocesamiento.\n\nPara construir este modelo inicial, vamos a dejar el árbol que construye `scikit-learn` por defecto."},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        discretizer,\n        GradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Histogram Gradient Boosting\n\n\nEste ensemble es una versión de Gradient Boosting. Construye un histograma, es decir, agrupa los valores en diferentes rangos, y considera esos rangos para la construcción del árbol. Es mucho más rápido que la versión estándar, sobre todo con un conjunto de datos significativo.\n\nEn este caso no se va a aplicar discretización, ya que al agrupar los valores por rangos se está haciendo una discretización.\n\nAl igual que en Gradient Boosting, se va a utilizar el árbol por defecto."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = make_pipeline(\n        remove_outliers,\n        impute_missing,\n        HistGradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluación de modelos\n\nPara evaluar todos los modelos que hemos construido, vamos a realizar una validación cruzada. En este caso será una validación cruzada de diez carpetas con cinco repeticiones. Es importante remarcar que será una validación cruzada estratificada. Primero creamos esta validación cruzada."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Antes de empezar a evaluar nuestros modelos vamos a  explicar las métricas que se van a usar. Al igual que en la práctica anterior nos centramos en el recall en esta se buscará lo mismo. Para ello se han propuesto dos métricas, como son ` balanced_accuracy` y ` recall_weighted`. \n\n` balanced_accuracy` es una métrica usada en problemas binarios de clasificación cuando las clases están desbalanceadas. Esta métrica esta basada en otras dos métricas: la *sensibilidad* o el *recall* (ratio de verdaderos positivos) y *especificidad* siendo el ratio de verdaderos negativos. \n \n`recall_weighted` es una métrica idéntica al recall sin embargo el peso de cada valor dependerá del soporte que tenga. Para variables con mayor soporte tendrán menor peso y viceversa. \n\nPuesto que queremos centrarnos en obtener el mejor clasificador posible, tanto para detectar casos positivos como negativos evaluaremos nuestro modelo en base a la métrica `balanced_accuracy`."},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = ['balanced_accuracy', 'recall_weighted']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(k_neighbors_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este modelo podemos observar una gran varianza entre el resultado obtendio con el conjunto de entrenamiento y el conjunto de test, tanto para el `balanced_accuracy` como para el `recall_weighted`. En el conjunto de entrenamiento obtiene puntuaciones muy altas, de aproximadamente algo menos de un 95% para ambas métricas, mientras que el conjunto de test no alcanza ni siquiera un 70% en ambas métricas."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Árbol de decisión"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(decision_tree_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que este modelo obtiene una puntuación más baja que el modelo de los vecinos más cercanos, sin embargo, no se obtiene tanta diferencia entre los resultados del entrenamiento y los del test, lo cual es un buen dato. Sin embargo, este resultado no es demasiado vinculante, debido a que los hiperparámetros que hemos usado para la construcción del modelo pueden no ser los óptimos."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Adaboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(adaboost_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se puede observar que este modelo obtiene mejores resultados que los dos modelos anteriores, sin embargo, podemos observar que la varianza entre los resultados obtenidos en el entrenamiento y los obtenidos en el test es bastante alta."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(bagging_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se puede ver que obtenemos unos resultados peores que con Boosting y con los vecinos más cercanos. Se debe remarcar una gran variación entre los resultados obtenidos con el conjunto de entrenamiento y los obtenidos con el conjunto de test."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(random_forest_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este ensemble obtiene peores resultados que Boosting y unos resultados parecidos al algoritmo de los vecinos más cercanos. Volvemos a observar la gran variación entre los resultados obtenidos con el conjunto de entrenamiento y el conjunto de test."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(gradient_boosting_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este ensemble obtiene un resultado peor que Boosting, sin embargo, obtiene un `recall_weigthed` muy parecido. Se vuelve a observar la gran diferencia entre los resultados de entrenamiento y los resultados de test."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Histogram Grading Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(hist_gradient_boosting_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este modelo obtiene unos resultados muy parecidos a Boosting, de hecho, se obtienen resultados mejores en las milésimas, sin embargo, no es un márgen lo suficientemente amplio para poder decir que se han obtenido unos resultados mejores."},{"metadata":{},"cell_type":"markdown","source":"Con esta evaluación de modelos mediante una validación cruzada hemos obtenido que los mejores modelos son Boosting e Histogram Gradient Boosting. Si hacemos caso solamente a los resultados numéricos que hemos obtenido, diremos que el mejor modelo es Histogram Gradient Boosting. No obstante, si tenemos en cuenta el principio de la navaja de Ockam, por el cual si tenemos dos modelos iguales debemos escoger el menos complejo, podemos decir que el mejor modelo es Boosting.\n\nSin embargo, estas conclusiones no tienen una gran validez, ya que no hemos usado los hiperparámetros óptimos para cada modelo. Por ello, en la siguiente sección vamos a seleccionar los mejores hiperparámetros para cada modelo."},{"metadata":{},"cell_type":"markdown","source":"## Selección de modelos\n\nVamos a empezar a buscar en un grid la mejor configuración de parámetros para cada algoritmo. Se va a usar la métrica `balanced_accuracy` para escoger los parámetros óptimos. No obstante, también se va a mostrar el `recall_weighted` para poder comparar."},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos\n\nDe este algoritmo se va a elegir tres parámetros para optimizar:\n\n* `n_neighbors`: el resultado que se va a obtener dependerá mucho del número de vecinos más cercanos que se elijan, debido a que si el número de vecinos es demasiado bajo, el algoritmo tenderá a sobreajustar y tendrá mucha varianza, pero si es demasiado alto, el resultado será muy parecido al `Zero-R`, porque clasificará por la clase mayoritaria. Se van a elegir números impares de vecinos para no encontrar el caso de empate en los resultados, desde el 1 al 13.\n* `weights`: este hiperparámetro indica si la clasificación se realiza por el voto por la mayoría (`uniform`) o por el voto pesado por la inversa de la distancia (`distance`). El valor que tome puede influir en la configuración encontrada.\n* `p`: este hiperparámetro nos indica si la métrica que se elige para calcular la distancia entre vecinos es la distancia de Manhattan (p=1) o si es la distancia euclídea (p=2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = k_neighbors_model\n\nn_neighbors = [1, 3, 5, 7, 9, 11, 13]\nweights = [\"uniform\", \"distance\"]\np = [1,2]\n\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__p=p,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los mejores parámetros que se obtienen son con 11 vecinos, usando la distancia de Manhattan y el voto mediante la mayoría.\n \nSi miramos las cinco mejores configuraciones, podemos observar que tienen entre 9 y 13 vecinos y todos utilizan la distancia de Manhattan. Sin embargo, no parece hay homogeneidad en cuanto a qué estrategia de voto es mejor.\n\nEl número óptimo de vecinos que se ha encontrado es un valor moderado, debido a que no es un valor demasiado próximo a 1 ni demasiado alto, teniendo en cuenta que se suele calcular el número de vecinos entre 1 y $\\sqrt{N}$, siendo este valor 23 en este conjunto de entrenamiento. Este número nos proporciona robustez contra el ruido, es decir, reduce la varianza que este algoritmo puede tener con una cantidad baja de vecinos.\n\nPodemos observar que si cogemos la métrica `recall_weighted`, los resultados son bastante similares, solo encontramos algunas permutaciones."},{"metadata":{},"cell_type":"markdown","source":"---\n### Árbol de decisión\n\nPara este modelo vamos a evaluar los siguientes hiperparámetros:\n* `criterion`: se va a probar qué criterio es mejor, si la entropía o gini.\n* `max-depth`: indica la máxima profundidad que alcanza el árbol. Un árbol demasiado ramificado tenderá a sobreajustar, y por tanto a tener mucha varianza, mientras que un árbol con demasiadas pocas ramas tenderá a subajustar, y por tanto, tener mucho sesgo. Sin embargo, un árbol con la profundidad adecuada tenderá a generalizar correctamente y así se evitará el sobreajuste y el subajuste. Se van a usar los valores 3, 5, 7 y sin profundidad máxima.\n* `min_samples_split`: el mínimo número de ejemplos que debe tener un nodo interno para generar otra rama. Puede ayudar a combatir el sobreajuste, ya que no permite hojas con muy poco ejemplos en ellas. Vamos a usar los valores 2 (para evitar tener hojas con un solo ejemplo), 10, 20 y 30.\n* `ccp_alpha`: este hiperparámetro se usa para la post-poda, lo cual también ayuda a evitar el sobreajuste.\n\nAdemás, también se van a modificar los hiperparámetros del discretizador que se le pasa mediante el pipeline, concretamente, se va a modificar el número de bins."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = decision_tree_model\n\ncriterion = ['entropy', 'gini']\nmax_depth = [3, 5, 7, None]\nmin_samples_split = [2, 10, 20, 30]\nccp_alpha = [0.0, 0.05, 0.1]\n\nn_bins = [2, 4, 6]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        decisiontreeclassifier__criterion=criterion,\n                                        decisiontreeclassifier__max_depth=max_depth,\n                                        decisiontreeclassifier__min_samples_split=min_samples_split,\n                                        decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                        kbinsdiscretizer__n_bins = n_bins,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Primero vamos a analizar la mejor configuración mirando la tabla. Podemos observar que en esta ocasión se ha obtenido un empate en las mejor configuración. Todas estas configuraciones tienen en común que discretizan en dos intervalos. Esto es normal, debido a que el árbol de decisión hace particiones binarias para cada rama que crea. En cuanto al resto de parámetros, no se observa un gran consenso. Parece que el mejor criterio es la entropía. Parece que es positivo realizar una post-poda. Sin embargo, no parece claro cuál es la mejor profundidad máxima para el árbol, debido a que encontramos árboles de todas las profundidades posibles. Esto se puede deber a que no es necesaria tanta profundidad, ya que se obtiene el mismo resultado con una menor profundidad. Podemos observar mediante la poda que se tiende a reducir la varianza.\n\nEn el mejor resultado que nos ofrece el método podemos observar que tiene una máxima profundidad de 3, que es la mínima que podía tener, una poda de 0.05, un mínimo de dos hojas para generar otra rama, que es el número mínimo y el criterio de la entropía. Si comparamos esto con las conclusiones generales que hemos obtenido, podemos remarcar que este método ha elegido el modelo más simple para desempatar, siguiendo el principio de la navaja de Ockam.\n\nEn cuanto al ranking obtenido con el recall_weigthed, podemos observar que también parece haber empates. En la última posición de este ranking podemos observar los mismos hiperparámetros que en el caso de balanced_accuracy. Sin embargo, los que hemos obtenido como mejores, están en la posición 97."},{"metadata":{},"cell_type":"markdown","source":"---\n### Adaboost\n\nEn este caso solo nos interesa modificar un hiperparámetro:\n* `learning_rate`: reduce la contribución de cada clasificador. Se va a probar si un es mejor un valor alto, un valor medio o un valor alto.\n\nEn los árboles que se van a aprender en este ensemble, se van a modificar los siguientes hiperparámetros:\n* `max_depth`: como se quieren obtener modelos débiles y con mucho sesgo, se va a optar por controlar la máxima profundidad que pueden alcanzar. Se va a optar por profundidades de 1, 2 y 3.\n* `criterion`: se va a volver a buscar si es mejor el criterio de la entropía o el criterio gini."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\nlearning_rate = [0.3, 0.5, 1]\n\nmax_depth = [1, 2, 3]\ncriterion = ['entropy', 'gini']\n\nadaboost_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        adaboostclassifier__learning_rate=learning_rate,\n                                        adaboostclassifier__base_estimator__max_depth=max_depth,\n                                        adaboostclassifier__base_estimator__criterion=criterion,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n### Bagging\n\nPara este ensemble se van a modificar dos hiperparámetros:\n* `max_samples`: se va a observar si es mejor elegir un conjunto mayor o menor de ejemplos. Se optará por mirar el 25%, 50%, 75% y 100%.\n* `oob_score`: se va a observar la diferencia entre usar el error *Out of Bag* o no usarlo.\nSe podría modificar también el número máximo de atributos que se pueden escoger, pero no se va a implementar debido a que el algoritmo que vimos en la clase de teoría no lo hacía.\n\nDel árbol de decisión solo se va a comprobar un hiperparámetro, que va a ser `criterion`.\n\nTambién se va a comprobar la discretización."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = bagging_model\n\nmax_samples = [0.25, 0.5, 0.75, 1]\noob_score = [True, False]\n\ncriterion = ['entropy', 'gini']\n\nn_bins = [2, 4, 6]\n\nbagging_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv,\n                                     baggingclassifier__max_samples=max_samples,\n                                     baggingclassifier__oob_score=oob_score,\n                                     baggingclassifier__base_estimator__criterion=criterion,\n                                     kbinsdiscretizer__n_bins = n_bins,\n                                     scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Si observamos la tabla, podemos notar que obtenemos los mismos resultados con el error *Out of Bag* que sin él, por tanto, podemos concluir que este hiperparámetro no influye en el resultado. La elección entre el criterio de entropía y el criterio gini no parece influir demasiado, debido a que no podemos observar una clara mejoría de uno contra el otro. En cuanto al número de intervalos, parece mejor la configuración de 4 o 6 intervalos. Relativo al número de ejemplos máximos que podemos elegir, parece que elegir el 100% de los ejemplos no es una buena elección, ya que tendría más opciones de instancias para escoger, y se asemejaría más al algoritmo de un árbol de decisión tradicional. Además, sería más probable los árboles que construye el ensemble estén relacionados entre sí al haber mayor probabilidad de escoger los mismos ejemplos. Podemos observar que reduce la varianza.\n\nSi comparamos los dos rankings que obtenemos con las dos métricas, podemos observar que los peores resultados están clasificados en las partes inferiores de la tabla en ambos rankings. Sin embargo, las primeras posiciones varían de un ranking a otro."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Random Forest\n\nLos hiperparámetros que vamos a modificar en este algoritmos son:\n* `criterion`: como en todos los demás árboles, vamos a comprobar si es mejor el criterio de la entropía o el criterio gini.\n* `max_depth`: en este ensemble, al construir árboles bastante más profundos de lo que haría un `C4.5`, se puede limitar su profundidad. Por tanto, probaremos con una máxima profundidad que sea bastante profunda, para no limitar demasiado su crecimiento. Se probará con 5, 7, 9 y sin máxima profundidad.\n* `max_features`: se mirará el número máximo de atributos que se van a evaluar para construir el mejor árbol. Se va a probar con el 50%, el 75% y el 100%. \n* `max_samples`: el máximo número de instancias que van usar también es un hiperparámetro importante para no obtener árboles muy dependientes. Se va a mirar el 25% y el 50%, debido a que en Bagging hemos observado que es mejor que este porcentaje no sea muy alto, y ambos modelos están relacionados.\nEn este caso no se va a probar con el conjunto *Out of Bag*, debido a que antes hemos obtenido en Bagging que era indiferente usar este conjunto o no.\n\nDe nuevo, se va a probar la configuración del discretizador."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\n\ncriterion = ['entropy', 'gini']\nmax_depth = [5, 7, 9, None]\nmax_features = [0.5, 0.75, 1]\nmax_samples = [0.25, 0.5]\n\nn_bins = [2, 4, 6]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv,\n                                               randomforestclassifier__criterion=criterion,\n                                               randomforestclassifier__max_depth=max_depth,\n                                               randomforestclassifier__max_features=max_features,\n                                               randomforestclassifier__max_samples=max_samples,\n                                               kbinsdiscretizer__n_bins=n_bins,\n                                               scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observando los cinco mejores resultados de la tabla, podemos observar algo curioso, y es que aunque el que obtiene mejor puntuación usa el criterio gini, los otros cuatro usan el criterio de la entropía, por tanto, no podemos obtener conclusiones sólidas sobre qué criterio es mejor. Se puede observar que el número óptimo de bins en el que realizar la discretización es 6. En cuanto a la máxima profundidad que deben tener los árboles, se obtienen mejores resultados con árboles más profundos, incluso sin limitar su profundidad. En cuanto el máximo número de atributos, podemos observar que los mejores resultados son el 50% y el 75%, siendo el 100% el que peores resultado obtiene. El mejor número de ejemplos, como ya intuíamos, está entre el 25% y el 50%. Los resultados del máximo número de atributos e instancias se deben a que con estos rangos obtenemos árboles menos relacionados entre sí, que es lo que hace que los ensembles obtengan mejores resultados. Todo esto reduce la varianza, y además la agregación hace que se reduzca aún más la varianza.\n\nComo en la mayoría de los demás modelos, al comparar las dos métricas, observamos que los resultados que se encuentran al final de la tabla son los mismos, mientras que los resultados que se encuentran al principio varían."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Gradient Boosting\n\nPara este ensemble vamos a considerar los siguientes hiperparámetros:\n* `loss`: se va a evaluar si se consiguen mejores resultados con la regresión logística (`deviance`) o con el algoritmo AdaBoost (`exponential`). \n* `learning_rate`: la tasa de aprendizaje se va calcular entre 0.05 y 0.1.\n* `subsample`: si esta métrica es menor que 1, se usa el Gradient Boosting estocástico. Vamos a probar si es mejor el ensemble normal o el estocástico. Se usará 0.5, 0.75 y 1.\n* `max_depth`: su valor por defecto es 3, por tanto, probaremos con 2, 3 y 4.\n\nComo anteriormente, se optimizará el parámetro de la discretización."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\n\nloss = ['deviance', 'exponential']\nlearning_rate = [0.05, 0.1]\nsubsample = [0.5, 0.75, 1]\nmax_depth = [2, 3, 4]\n\nn_bins = [2, 4, 6]\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv,\n                                               gradientboostingclassifier__loss = loss,\n                                               gradientboostingclassifier__learning_rate = learning_rate,\n                                               gradientboostingclassifier__subsample = subsample,\n                                               gradientboostingclassifier__max_depth=max_depth,\n                                               kbinsdiscretizer__n_bins=n_bins,\n                                               scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Si nos fijamos en las cinco primeras configuraciones, parece que la función de pérdida del algoritmo Adaboost funciona mejor que la logística, debido a que para la misma configuración de los demás hiperparámetros, obtenemos mejores resultados con la exponencial. En cuanto a la tasa de aprendizaje, para mejor la tasa de 0.1. Parece que funciona mejor la versión estocástica, lo cual es razonable, debido a que esta versión es una mejora respecto a la versión normal. Si observamos el criterio de la máxima profundidad, no es posible sacar ninguna conclusión precisa, debido a que todas las profundidades que hemos seleccionado están recogidas. Podemos observar que al tener poca profundidad, estos árboles tienen sesgo, lo cual se reduce mediante el ensemble. Respecto al número de intervalos en los que realizamos la discretización, parece claro que el mejor número es 6, y que 2 es el peor.\n\nSi comparamos las dos métricas, observamos que los resultados varían."},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Histogram Gradient Boosting\n\nLos hiperparámetros que se van a optimizar son diferentes a los que hemos usado en el Gradient Boosting, debido a que no todos los hiperparámetros presentes en dicho algoritmo no están implementados en Histogram Gradient Boosting. Se van a optimizar los siguientes:\n* `learning_rate`: vamos a observar si es mejor una tasa de aprendizaje más alta o más baja.\n* `max_depth`: vamos a modificar la máxima profundidad para observar cómo influye en los resultados.\n\nEn un principio se había pensado en cambiar la función de pérdida, pero solo exiten dos funciones implementadas:  `binary_crossentropy`, que es la pérdida logística, y `categorical_crossentropy`, que es para variables multiclase. Como en este conjunto de datos solo tenemos una clase binaria, no podemos modificar estos valores.\n\nEs importante remarcar que no se va a realizar la discretización en el preprocesamiento, debido a que este ensemble realiza una discretización interna."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\n\nlearning_rate = [0.025, 0.05, 0.075, 0.1]\nmax_depth = [1, 2, 3, 4, 5]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv,\n                                               histgradientboostingclassifier__learning_rate = learning_rate,\n                                               histgradientboostingclassifier__max_depth=max_depth,\n                                               scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que la peor tasa de aprendizaje es 0.025, mientras que la mejor tasa no puede especificarse muy bien, debido a que en los mejores resultados obtenemos las otras tres tasas. En relación a la máxima profundidad, podemos observar que los mejores números son 2, 3 y 4, es decir, se obtienen modelos individuales con sesgo, que se soluciona mediante la agregación de los resultados.\n\nEn cuanto a la comparación de las dos tasas de rendimiento, podemos observar que la posición del ranking varía mucho según se usa una u otra."},{"metadata":{},"cell_type":"markdown","source":"## Construcción del modelo final\n\nFinalmente, vamos a comprobar qué modelo es mejor. Para ello vamos a comprobar su rendimiento repecto al conjunto de test."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como hemos utilizado el `balanced_accuracy` durante la selección de parámetros para escoger el mejor modelo, debemos escoger también esta métrica para elegir el modelo final. En esta tabla está representada en la columna *score*. Podemos observar que el modelo que mayor puntuación obtiene en esta métrica es Gradient Boosting, aunque Adaboost le sigue con un resultado muy parecido, solo diferenciado por cinco milésimas. Como se puede observar, hemos obtenido unos resultados diferentes a los que habíamos obtenido mediante la validación cruzada. Esto se debe a que hemos usado los mejores parámetros para cada modelo, y no unos arbitrarios.\n\nComo en el caso de la validación cruzada, debemos plantearnos si la mejora de Gradient Boosting frente a Adaboost es suficiente teniendo en cuenta la complejidad que Gradient Boosting introduce y si cinco milésimas se considera una diferencia lo suficientemente alta para considerarla mejor. Como ya hicimos en el caso de la validación cruzada, vamos a tener en cuenta el principio de la navaja de Ockam y vamos a escoger el modelo más sencillo. Por tanto, podemos concluir que el mejor modelo es Adaboost.\n\nEl modelo que peor resultado ha obtenido ha sido el algoritmo de los vecinos más cercanos, con casi una décima menos de puntuación respecto al mejor modelo. El siguiente peor modelo ha sido Bagging, siendo superado incluso por el árbol de decisión. Esto no suele ser lo normal. Este hecho se puede deber a que los parámetros no se han optimizado todo lo bien que deberían o que por un casual al ajustar la semilla hemos hallado una combinación que llevaba a este resultado. El siguiente ha sido el árbol de decisión, seguido con muy poca diferencia por el Histogram Gradient Boosting. El tercer mejor modelo ha sido Random Forest.\n\nComo última reflexión, podemos observar que Histogram Gradient Boosting es peor que su versión estándar, a pesar de que debería ser mejor. Esto se puede deber a varios factores. El primero de ellos se puede deber a que no hemos usado los mismos parámetros en ambos modelos, debido a que no estaban implementados en la versión del histograma. El segundo factor se puede deber a que la versión de histograma es mejor cuando el tamaño de la base de datos es muy grande, con una base de datos pequeña, como puede ser este caso, no se ve la diferencia o puede empeorar.\n\nFinalmente, vamos a observar las matrices de confusión de cada modelo para tener una visión más gráfica."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('k_neighbors_clf')\nutils.evaluate(k_neighbors_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('decision_tree_clf')\nutils.evaluate(decision_tree_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('adaboost_clf')\nutils.evaluate(adaboost_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('bagging_clf')\nutils.evaluate(bagging_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('random_forest_clf')\nutils.evaluate(random_forest_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('gradient_boosting_clf')\nutils.evaluate(gradient_boosting_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('hist_gradient_boosting_clf')\nutils.evaluate(hist_gradient_boosting_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n---"},{"metadata":{},"cell_type":"markdown","source":"## Breast cancer wisconsin data\n\nEn esta sección de la práctica 2 se va a realizar un estudio similar al anterior. En este caso usaremos el dataset de cáncer de pecho en Wisconsin (`breast-cancer-wisconsin-data`).\n\nEl dataset de `Wisconsin` está compuesto por diez variables que a su vez esta dividida en otras tres variables. A estas 30 variables hay que sumarle una variable `Unnamed: 32` que no aporta nada al estar vacía y la variable clase `diagnosis` (M = maligno, B = benigno).\n\nLas diez variables predictoras son:\n* `radius` Distancia del centro al perímetro\n* `texture`: Desviación estándar de los valores en la escala de grises\n* `perimeter`: Tamaño del núcleo del tumor\n* `area`: pi*radius^2\n* `smoothness`: Variación local en las longitudes de los radios\n* `compactness`: Perímetro^2 / área -1\n* `concavity`: Gravedad de las porciones cóncavas del contorno\n* `concave points`: Número de proporciones cóncavas en el contorno\n* `symmetry`: Simetría del contorno\n* `fractal dimension`: “coastline approximation” -1\n\nTodas estas variables se subdividen con su correspondiente:\n* `mean`: Media del parámetro a medir\n* `standard error` (`se`): Standard error de la media (`mean`)\n* `worst`: Peor o media de mayor valor de la media"},{"metadata":{},"cell_type":"markdown","source":"### Carga de datos \n\nEn este apartado se realizará la carga de datos y su posterior separación en sets de entrenamiento (`train`) y testeo (`test`). También dentro de estos propios conjuntos de datos se separarán los datos en variables predictoras (`X`) variables objetivo (`y`).\n\nComo ya se ha mencionado anteriormente, en esta sección de la práctica se utilizará el dataset: `breast-cancer-wisconsin-data`. Este conjunto de datos está compuesto por características obtenidas a partir de imágenes digitalizadas de bultos ubicados en el pecho."},{"metadata":{"trusted":true},"cell_type":"code","source":"index = \"id\"\ntarget = \"diagnosis\" # M = malignant, B = benign\n\ndata = utils.load_data(\"../input/breast-cancer-wisconsin-data/data.csv\",index, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividiremos el conjunto de datos en variables predictoras y variable objetivo (`diagnosis`). Posteriormente separemos los datos, quedándonos con un 75% para la muestra de entrenamiento, y el resto (25%) para la muestra de testeo. \n\nPara realizar esto se hará uso del método `split_data` del sript `utils`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.75\ntrain, tests, X_train, X_test, y_train, y_test = utils.split_data(data, target, seed, train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobaremos que los datos han sido cargados correctamente mostrando una muestra aleatoria."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Numeros de instancias train: {train.shape[0]}\")\nprint(f\"Numeros de variables train: {train.shape[1]}\")\ntrain.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Numeros de instancias test: {tests.shape[0]}\")\nprint(f\"Numeros de variables test: {tests.shape[1]}\")\ntests.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos de clasificación supervisada\n\nEn este apartado se expondrán todos los modelos que se van a usar, así como una descripción y sus parámetros. También se especificarán los parámetros que se consideren más importantes para este problema y que posteriormente se utilizarán para realizar una búsqueda en grid.\n\nSin embargo, primero tendremos que definir todos los procesos de preprocesamiento que han de ser introducidos en el pipeline."},{"metadata":{},"cell_type":"markdown","source":"### Preprocesamiento"},{"metadata":{},"cell_type":"markdown","source":"#### Eliminación de variables "},{"metadata":{},"cell_type":"markdown","source":"Como ya se comentaba en la práctica anterior, la selección de variables no era del todo correcta, ya que solo se estudiaba el poder predictivo de las variables predictoras de forma individual y no conjunta. Esto es un error, ya que, aunque una variable por sí sola no pueda clasificar la variable clase, puede darse el caso, que trabajando junto a otras variables sí se obtenga una buena clasificación. \n\nEs por esto que para esta práctica se ha decidido eliminar únicamente las variables cuya correlación sea de un 90% o superior. Lo que nos deja en eliminar las variables que se muestran a continuación.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables con fuertes correlaciones (90%)\ncorrelations = [\n    'perimeter_mean',\n    'perimeter_se',\n    'area_mean',\n    'area_se',\n    'area_worst',\n    'area_worst',\n    'concavity_worst',\n    'concavity_mean',\n]\n\nto_drop_columns_names = ['Unnamed: 32'] + correlations\nto_drop_columns = [train.columns.get_loc(column) for column in to_drop_columns_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_transformer = utils.drop_columns(to_drop_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Eliminación de outliers"},{"metadata":{},"cell_type":"markdown","source":"Para la eliminación de outliers se usará, igual que en la práctica anterior, el método `IsolationForest` implementado en `sklearn`. Este preprocesamiento se le aplicará a todos los métodos indistintamente, igual que la eliminación de variables predictoras indicadas anteriormente. Sus hiperparámetros estarán fijos y no se jugará con ellos en el Grid Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_outliers = FunctionSampler(func=utils.outlier_rejection, kw_args={\"random_state\":seed})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Discretización"},{"metadata":{},"cell_type":"markdown","source":"Al igual que en la práctica anterior aplicaremos una discretización. Sin embargo, no será aplicada a todos los modelos como se verá posteriormente. \n\nA diferencia de la anterior práctica, donde se separaba la codificación como otro proceso del pipeline, en esta se unificará para permitirnos mayor flexibilidad en el Grid Search.\n\nLa discretización será la etapa del preprocesamiento con la que más se tendrá que probar en el Grid Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"quantile\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### Normalización \n\nLa normalización es añadida en esta práctica pues consideramos que es necesaria, si no obligatoria, para algunos de los algoritmos. Por ejemplo, los vecinos más cercanos. De esta forma se ajustarán todas las variables predictoras para que tengan el mismo peso, es decir, que todas las variables aporten la misma información al algoritmo."},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = Normalizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos\n\nEn este apartado definiremos los modelos a usar. Aportaremos una breve explicación de cada método. \n\nCada modelo tendrá un preprocesamiento propio donde se busca adaptar los datos de la mejor forma para trabajar en armonía con el modelo. Sin embargo, todos los métodos tendrán la eliminación de variables y de outliers. \n"},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos\n\nEn la construcción del pipeline que usará el algoritmo de los vecinos más cercanos se ha de hacer una modificación con respecto al pipeline creado para la práctica anterior. En dicho pipeline no se normalizaba o estandarizaba, lo cual, es un paso fundamental para que todas las variables predictoras tengan el mismo peso. Es por eso que en este pipeline se añadirá un paso para escalar todas las variables predictoras a la misma proporción.\n\nLa función de peso usada en las predicciones es por defecto `uniform`. Inicialmente nosotros lo estableceremos a `distance`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vecinos más cercanos\n\nn_neighbors = 5\nweights = 'distance'\n\nk_neighbors_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        normalize,\n        KNeighborsClassifier(n_neighbors, weights=weights)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Árbol de decisión\n\nSiguiendo la tendencia de la práctica anterior se ha inicializado un árbol de decisión con una profundidad de tres para que no sobreajuste. Sin embargo, nos arriesgamos a no dejar al árbol crecer lo suficiente dejando el modelo sesgado. \n\nSe han añadido dos nuevos hiperparámetros para permitir la post-poda en caso necesario. De igual manera se ha especificado que las hojas deberán de tener mínimo 25 instancias, de esta forma se evitarán nodos hoja con pocos datos que solo conllevarán sobreajuste. \n\nEl resto de los hiperparámetros usados serán explicados en la búsqueda en Grid donde tendrán mayor importancia. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Árbol de decisión\n\ndecision_tree_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        discretizer,\n        DecisionTreeClassifier(random_state=seed, \n                               max_depth=3,\n                               criterion='entropy',\n                               ccp_alpha=0.1,\n                               min_samples_leaf=25)\n) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### AdaBoost\n\nAdaboost es un ensemble cuyo funcionamiento consta de generar muchos modelos pequeños de forma secuencial modificando los datos con los que se aprenden. Las predicciones son el resultado de todos estos modelos débiles por el peso de cada modelo. Dicho peso será calculado en tiempo de entrenamiento. A más peso más contará dicho modelo en el voto final.\n\nEs sabido que los ensembles a un mayor número de iteraciones y por lo tanto más estimadores internos mejor resultado dará. No obstante, en adaboost habrá que tener precaución para evitar los sobreajustes. \n\n<br>\n\nEn este pipeline se ha eliminado la discretización pues se ha comprobado que se obtienen mejores resultados más rápido. \n\n![error rate n_estimators adaboost](https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png)\n\n\nComo modelo base se ha usado un 1-R, siendo el modelo por defecto en sklearn.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost\nadaboost_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        AdaBoostClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=1))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Bagging\n\nEl Bootstrap Aggregating o Bagging es un ensemble utilizado para reducir la varianza y combatir el overfitting. Esto se logra por medio de muchos modelos complejos, como los usados en esta práctica, árboles completamente ramificados. Posteriormente, mediante un voto y / o agregación se obtendrá el ensemble.\n\nCon el objeto de que todos los modelos no sean iguales se utilizará el muestreo con remplazo (indicado en el hiperparámetro `Bootstrap`, el cual por defecto será `True`). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bagging\nbagging_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        discretizer,\n        BaggingClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=None))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Random Forest\n\nEl ensemble Random Forest, al igual que el Bagging, es un ensemble creado para reducir la varianza. Este modelo se basa en la hipótesis de independencia, suponiendo que todos los árboles creados en el bosque han sido entrenados con datos independientes. Esto se intenta lograr por medio de un muestreo con remplazo aleatorio. Por lo tanto, se puede decir que los árboles son construidos de forma semi-aleatoria. \n\nEs un hecho que los árboles de decisión que crecen sin restricciones tienen una alta varianza y se sobreajustan a los datos. Es por eso que creando un bosque, de media, los errores cometidos por árboles individuales son compensados por los árboles que aciertan. Combinando los árboles se consigue reducir dicha varianza, a veces aumentando ligeramente el sesgo. En la práctica la reducción de la varianza es **muy** significativa. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\nrandom_forest_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        discretizer,\n        RandomForestClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Gradient Boosting\n\nGradient Boosting es una generalización del modelo de Boosting combinado con el descenso del gradiente o una función de pérdida. Como en el Boosting, este ensemble generará un conjunto de meta-estimadores de forma secuencial. No obstante, en este modelo, todos los meta-estimadores tendrán la misma importancia y en cada iteración se tendrán en cuenta los modelos anteriores.  \n\nLa clasificación está restringida a casos binarios. Se trabajará con un ratio de probabilidad. Éste será el hiperparámetro que se buscará optimizar en la búsqueda Grid.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting\ngradient_boosting_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        discretizer,\n        GradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Histogram gradient boosting\n\nHistogram Gradient Boosting es una variante del Gradient Boosting pensada para ser más eficiente cuando se trabajan con instancias del orden de $10^4$ o superiores. Aunque este no sea el caso podemos seguir haciendo uso de dicho algoritmo.\n\nNo se ha usado la discretización pues la hace el algoritmo internamente.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram gradient boosting \nhist_gradient_boosting_model = make_pipeline(\n        drop_transformer,\n        remove_outliers,\n        HistGradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluación de modelos\n\n"},{"metadata":{},"cell_type":"markdown","source":"En este apartado se evaluarán los modelos. Se hará uso de una k-validación cruzada. De esta forma pretendemos obtener unos valores fiables tanto de la varianza como del sesgo usando únicamente el conjunto de train. \n\nNuestros modelos se evaluarán siguiendo varias métricas que se explicarán posteriormente. \n\nPrimero se evaluarán los modelos con los hiperparámetros con los que se definieron en el apartado anterior. Posteriormente se aplicará una búsqueda en grid para intentar obtener la mejor combinación posible de hiperparámetros.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Antes de empezar a evaluar nuestros modelos se tendrá que explicar las métricas que se van a usar. Al igual que en la práctica anterior nos centramos en el recall en esta se buscará lo mismo. Para ello se han propuesto dos métricas, como son ` balanced_accuracy`, ` recall_weighted` y auxiliarmente `roc_auc`. \n\n` balanced_accuracy` es una métrica usada en problemas binarios de clasificación cuando las clases están desbalanceadas. Esta métrica esta basada en otras dos métricas: la *sensibilidad* o el *recall* (ratio de verdaderos positivos) y *especificidad* siendo el ratio de verdaderos negativos. \n\n ![balanced.png](https://statisticaloddsandends.files.wordpress.com/2020/01/metrics2.png?w=584&h=267)\n \n \n`recall_weighted` es una métrica idéntica al recall sin embargo el peso de cada valor dependerá del soporte que tenga. Para variables con mayor soporte tendrán menor peso y viceversa. \n\n`roc_auc` (curva de característica operativa del recepto) es un gráfico que muestra el rendimiento de un modelo de clasificación en todos los umbrales de clasificación. Esta curva representa dos parámetros: Tasa de verdaderos positivos y tasa de falsos positivos.\nUna curva ROC representa TPR frente a FPR en diferentes umbrales de clasificación. Reducir el umbral de clasificación clasifica más elementos como positivos, por lo que aumentarán tanto los falsos positivos como los verdaderos positivos. En la siguiente figura, se muestra una curva ROC típica.\n \n ![roc.png](attachment:image.png)\n\n\nPuesto que queremos centrarnos en obtener el mejor clasificador posible, tanto para detectar casos positivos como negativos evaluaremos nuestro modelo en base a la métrica `balanced_accuracy`.\n\nLa métrica `roc_auc` la descartamos pues no consideramos que sea una métrica que nos interese para este problema. De forma intuitiva, el área bajo la curva nos indica cuan bien generaliza el modelo. Dado que nuestra muestra no está balanceada no serían resultados concluyentes.","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = ['balanced_accuracy', 'recall_weighted']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(k_neighbors_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estos resultados son bastante prometedores, esperamos poder mejorarlos realizando la apropiada búsqueda en Grid en la siguiente sección.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Árbol de decisión"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(decision_tree_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El árbol de decisión obtiene unos resultados bastante malos en comparación con los vecinos más cercanos. En este modelo podría estar pasando justo lo contrario y que nuestro modelo esté sesgado. Recordemos que la profundidad del árbol es 3 con post-poda y esto podría limitar el aprendizaje del mismo. También podría ocurrir que un árbol de decisión no pueda extraer más información de nuestra base de conocimientos. \n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Adaboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(adaboost_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como era de esperar de un ensemble, obtiene mejores resultados que un solo árbol de decisión. Cabe destacar la diferencia en la varianza del conjunto de `train` y de `test`."},{"metadata":{},"cell_type":"markdown","source":"### Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(bagging_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Buenos resultados, siendo semejantes al Adaboost. Sin embargo, son peores que el Adaboost pero mejores que los árboles de decisión, como cabía esperar de un ensemble. "},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(random_forest_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Teóricamente un modelo Random Forest obtendrá mejores resultados que el ensemble Bagging. Estos resultados a priori corroboran dicha afirmación."},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(gradient_boosting_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorprende que estos resultados se encuentren entre los peores de todos los modelos pues Gradient Boosting, teóricamente, debería obtener mejores resultados. Se espera optimizar sus hiperparámetros en la búsqueda en Grid."},{"metadata":{},"cell_type":"markdown","source":"### Histogram gradient boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimator(hist_gradient_boosting_model, X_train, y_train, cv, scoring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selección de modelos\n\nEn este apartado se efectuarán las búsquedas Grid para iterar entre todas las posibles combinaciones de los hiperparámetros y así obtener el mejor modelo. Se seguirá usando la métrica `balanced_accuracy` para elegir que modelo será el mejor. Sin embargo, también se clasificará con `recall_weighted` para saber si ambas métricas difieren mucho."},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos\n\n* `weights`: Como se ha estudiado usar la distancia como medida de peso para los vecinos nos proporciona una mayor fiabilidad siempre y cuando las fronteras de decisión sean claras y nuestra muestra está balanceada y no tenga ruido. Es por eso que también usaremos los pesos de cada instancia de manera uniforme de esta forma se reduce la sensibilidad al ruido, aunque se aumenta los errores en muestras desbalanceadas cuando nuestra instancia a predecir está cerca de la frontera de decisión. \n* `n_neighbors`: Éste es el hiperparámetro más importante a optimizar. Muy pocos vecinos generaría un sobreajuste muy grande y dado la enorme varianza que eso supondría en caso de ruido. Por otra parte, un exceso de vecinos puede llevar al modelo a comportarse como un zero-R eligiendo siempre la clase mayoritaria del conjunto de datos. <br>Se ha elegido siempre números impares para no generar nunca casos de empate. \n* `p`:  Este hiperparámetro nos especificará cómo se eligen los vecinos más cercanos. Ya sea, por la distancia de Manhattan o la distancia euclídea. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = k_neighbors_model\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [1, 3, 5, 9, 11, 15, 19]\np = [1,2] # 1 = Distancía de Manhattan; 2 = Distancía euclidea \n\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kneighborsclassifier__p=p,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A pesar de que hemos comentado que los casos con pocos vecinos pueden derivar en sobreajuste observamos que hemos obtenido un modelo con simplemente cinco vecinos. Observamos también que tanto la métrica `balanced_acuracy` como `recall_weighted` coinciden en que el modelo seleccionado es el mejor. \n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Árbol de decisión\n\nEn esta búsqueda destaca con respecto a la anterior que se buscará optimizar de manera conjunta tanto los hiperparámetros de la discretización como del modelo, en este caso un árbol de decisión. \nLa discretización aparecerá siempre con los mismos hiperparámetros en el resto de Grid, cuando corresponda. \n\n* `strategy`: Indica al discretizador qué método ha de usar para generar las particiones. Probaremos tanto con la supervisada por igual frecuencia `quantile` pues fue por la que se optó en la práctica anterior debido a las distribuciones de los datos. También se usará el método no supervisado por la k-medias `kmeans`.\n* `n_bins`: Indicará en cuantas particiones se particionará la variable predictora. Se usarán únicamente números pares.\n\nEn el árbol de decisión se modificarán tres hiperparámetros:\n* `max_depth`: Con el objeto de que el árbol no sobreajuste se intentará buscar la profundidad mínima que nos garantice los mejores resultados. De la práctica anterior sabemos que el árbol crece hasta una profundidad máxima de 7.\n* `criterion`: Buscará qué forma de medir las particiones es mejor a este problema. Solo puede tomar dos valores: `entropy` y `gini`.\n* `ccp_alpha`: Ya se está limitando el sobreajuste del árbol, ya sea por medio de la profundidad máxima y de los casos mínimos por hoja (establecida en 25). No obstante, daremos la posibilidad al árbol de una post-poda. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = decision_tree_model\n\nmax_depth = [3, 5, 7]\ncriterion = ['entropy', 'gini']\nccp_alpha = [0.0, 0.05, 0.1]\n\n\nstrategy = ['kmeans','quantile']\nn_bins = [2, 4, 6]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        decisiontreeclassifier__max_depth=max_depth,\n                                        decisiontreeclassifier__criterion=criterion,\n                                        decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                        kbinsdiscretizer__strategy = strategy,\n                                        kbinsdiscretizer__n_bins = n_bins,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tras realizar la búsqueda en Grid se han obtenido unos resultados muy parecidos al caso base del que se partía. Cabe destacar las diferencias, siendo estas en `criterion` que ha pasado de `entropy` a `gini`. Y en `ccp_alpha` pasando de 0.1 a 0.0 pues no se requiere de una post-poda al estar el árbol ya restringido por la profundidad y las hojas mínimas por nodo.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Adaboost\n\nEl número de iteraciones o estimadores no será controlado en esta búsqueda, dejándolo por defecto (50). Esto es debido a que se sabe que un ensemble obtendrá mejores resultados a más iteraciones o estimadores. Sin embargo, sí que se modificarán los siguientes hiperparámetros:\n* `learning_rate`: Controla la contribución de los estimadores internos en la combinación final. Default = 1.0. Se probarán valores más pequeños con el objeto de intentar reducir el sobreajuste.\n* ` min_samples_split`: Este hiperparámetro afecta a los 1-R que con los que se genera el ensemble. Determina el número mínimo de casos necesarios para considerar una partición. Esto se hace con el objeto de no tener particiones donde haya muy poco datos y por lo tanto, sobreajuste. Por defecto, este valor es 2.\n* `criterion`: Este hiperparámetro es interno de los 1-R. Decidirá cual es la mejor métrica para generar las particiones.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\nmin_samples_split = [2, 20, 50]\ncriterion = ['entropy', 'gini']\n\nlearning_rate = [0.3, 0.5, 1]\n\n\nadaboost_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        adaboostclassifier__base_estimator__min_samples_split=min_samples_split,\n                                        adaboostclassifier__base_estimator__criterion=criterion,\n                                        adaboostclassifier__learning_rate=learning_rate,\n                                        scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las conclusiones que podemos sacar tras obtener los resultados de la búsqueda Grid son variadas. Primero, cabe destacar que el hiperparámetro `min_samples_split` no tiene ninguna relevancia en nuestro problema. Esto es debido a que al trabajar sobre un 1-R siempre tendremos todo el conjunto de entrenamiento que es claramente superior a los parámetros puestos. \n\nEn cuanto al `learning_rate` podemos percatarnos que si se utiliza el `criterion` de entropía obtiene mejores resultados con 1, el valor por defecto. Mientras que si se utiliza el Gini obtendrá mejores resultados con un 0.5. Esto será cuando nos fijemos en ` mean_test_balanced_accuracy`.\n"},{"metadata":{},"cell_type":"markdown","source":"### Bagging\n\nAl igual que en Boosting, no tendremos en cuenta el número de meta-modelos dejando el valor por defecto, 10.\n\nComo ya se explicó anteriormente, los meta-modelos no aprenden con la base de conocimientos completa, sino con una muestra extraída de forma aleatoria y con remplazo. Éstos serán los hiperparámetros que se buscarán optimizar por medio de la búsqueda Grid.\n* `max_samples`: Número de instancias extraídas del conjunto completo. Default = 1.0\n* `max_features`: Número de variables predictoras usadas para entrenar el meta-modelo. Default = 1.0\n* `oob_score`: Instancias *Out-Of-Bag* para estimar el error. \n\nLa búsqueda de hiperparámetros para la discretización será igual que en el árbol de decisión. "},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = bagging_model\n\nmax_samples = [0.25, 0.5, 0.75]\nmax_features = [0.25, 0.5, 0.75]\noob_score = [True, False]\n\nstrategy = ['kmeans','quantile']\nn_bins = [2, 4, 6]\n\nbagging_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv,\n                                     baggingclassifier__max_samples=max_samples,\n                                     baggingclassifier__max_features=max_features,\n                                     baggingclassifier__oob_score=oob_score,\n                                     kbinsdiscretizer__strategy = strategy,\n                                     kbinsdiscretizer__n_bins = n_bins,\n                                     scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tras obtener los resultados de la búsqueda Grid podemos observar:\n\n * El hiperparámetro `oob_score` no aporta ningún tipo de información. Esto se puede deber a la validación cruzada que se realiza para validar cada modelo. \n * La discretización tampoco parece afectarle. También podría deberse a que los hiperparámetros proporcionados no son los apropiados para el algoritmo.\n * `max_features` y `max_samples` parecen ser los hiperparámetros más importantes entre todos los que se utilizan en la búsqueda Grid. \n \nSe ha concluido que con el 75% de las variables predictoras y la mitad de las instancias es la mejor combinación para entrenar cada estimador.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest\n\nEl Random Forest esta formado de árboles sin limite de profundidad, es decir, árboles con alta varianza y mucho sobreajuste. Por esto mismo no realizaremos modificaciones en el Grid con respecto a los hiperparámetros de los árboles que conforman el bosque. \n\nNo obstante, realizaremos una búsqueda en los hiperparámetros del bosque, concretamente en las instancias y variables que se seleccionaran de forma aleatoria para entrenar cada árbol. \n* `max_samples`: Número de instancias que se seleccionarán para entrenara un árbol. Puede tomar como parámetros un `int` indicando de manera explícita el número de instancias a usar del conjunto original, `float` siendo el porcentaje en tanto por 1 de las instancias que se usarán.\n* `max_features`: Número de variables que se considerarán cuando se entrene un árbol. Puede tomar como parámetros `sqrt` siendo la raíz cuadrada, `log2` siendo el logaritmo en base 2, `int` indicando de manera explicita cuantas variables se han de usar, `float` indicando en tanto por uno el porcentaje de variables a usar. \n\nAl igual que en todos los casos anteriores donde se aplicaba la discretización, se realizará la misma búsqueda. "},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\n\nmax_samples = [0.25, 0.5, 0.75]\nmax_features = ['sqrt', 'log2']\n\nstrategy = ['kmeans','quantile']\nn_bins = [2, 4, 6]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv,\n                                               randomforestclassifier__max_samples=max_samples,\n                                               randomforestclassifier__max_features=max_features,\n                                               kbinsdiscretizer__strategy=strategy,\n                                               kbinsdiscretizer__n_bins=n_bins,\n                                               scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nTras realizar la búsqueda en Grid observamos que:\n* Hemos obtenido los mismos resultados en los hiperparámetros de la discretización que para el modelo bagging. Esto se puede deber a que se ha configurado el bagging de forma muy similar al comportamiento de un Random Forest. Es por eso que es normal obtener los mismos resultados o por lo menos unos muy parecidos. \n  No serán igual dado que el Random Forest tiene 100 estimadores mientras que bagging simplemente se le han especificado 10 estimadores. También hay que tener en cuenta que ambos modelos usan la aleatoriedad para crear el muestreo con remplazo, por lo tanto, eso también variará.\n* `max_samples` será el 50% de las instancias, al igual que en Bagging. \n\n\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting\n\nComo ya se ha hecho con los modelos anteriores que usaban árboles como meta-estimador y se le realizaba una discretización se usarán los mismos parámetros en la búsqueda en Grid en el Gradient Boosting. \nTambién restringiremos la profundidad máxima a la que puede crecer cada árbol (meta-estimador).\n\nComo ya se ha explicado anteriormente, Gradient Boosting usa una función de perdidad. Esto se controla en sklearn con el hiperparámetro `loss`. Este hiperparámetro puede tomar dos valores: `deviance` siendo la regresión logística y `exponential`.\n\nPara especificar cuanto pesa cada árbol se usará el hiperparámetro `learning_rate`. Teniendo un valor por defecto de 0.1.\n\nEl hiperparámetro `subsample` controla la fracción del conjunto original de entrenamiento que será usada para entrenar cada árbol. El valor por defecto es 1.0. En el caso de que el valor de este hiperparámetro sea menor a 1 el algoritmo pasará a ser Stochastic Gradient Boosting. Además se obtendrá una reducción de la varianza y un incremento del sesgo.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\n\nloss = ['deviance', 'exponential'] # ‘deviance’ = logistic regression\nlearning_rate = [0.035, 0.1]\nsubsample = [0.35, 1]\n\nmax_depth = [1, 3, 7]\n\nstrategy = ['kmeans','quantile']\nn_bins = [2, 4, 6]\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv,\n                                               gradientboostingclassifier__loss = loss,\n                                               gradientboostingclassifier__learning_rate = learning_rate,\n                                               gradientboostingclassifier__subsample = subsample,\n                                               gradientboostingclassifier__max_depth=max_depth,\n                                               kbinsdiscretizer__strategy=strategy,\n                                               kbinsdiscretizer__n_bins=n_bins,\n                                               scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tras realizar la búsqueda en Grid podemos observar los siguientes resultados:\n* La discretización, al igual que en casos anteriores, funcionan mejor seis particiones. Podemos observar que los peores casos son con dos particiones.\n  <br>A diferencia de los modelos anteriores esta discretización no usará `kmeans` si no que usará `quantile`.\n* El hiperparámetro `subsample` ha resultado ser menor que uno. Como ya se ha comentado antes, esto generará una menor varianza a cambio de un mayor sesgo. \n* La profundidad de los árboles (`max_depth`) parece afectar mucho al algoritmo, siendo árboles pequeños más efectivos que grandes árboles sobreajustados.\n* Se ha elegido la regresión logística (`deviance`) como función de perdida `loss`.\n\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Histogram Gradient Boosting\n\nA pesar de el modelo Histogram Gradient Boosting está diseñado con el objeto de ser eficiente para grandes bases de datos (más de 10 000 instancias) haremos uso de el igualmente pues sigue aportando buenos resultados.\n\nEn esta búsqueda en Grid solamente se explorarán dos hiperparámetros: \n* `min_samples_leaf`: Los casos mínimos requeridos para considerar un nodo hoja. Para datasets pequeños es recomendable que sea menos de unos pocos cientos. Es recomendable establecer este valor bajo puesto solo se van a construir árboles muy profundos. \n* `learning_rate`: La tasa de aprendizaje o también conocida como *shrinkage*. Es usada como factor multiplicativo para los nodos hojas. Por defecto es 0.1. Establecer este hiperparámetro como 1 será lo mismo que establecer que no haya *shrinkage*.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\n\nmin_samples_leaf = [20, 50, 70]\nlearning_rate = [0.05, 0.1, 0.2]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                                  X_train, y_train, cv,\n                                                  histgradientboostingclassifier__learning_rate=learning_rate,\n                                                  histgradientboostingclassifier__min_samples_leaf=min_samples_leaf,\n                                                  scoring=scoring, refit='balanced_accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tras realizar la búsqueda en Grid podemos observar los siguientes resultados:\n* El `learning_rate` menor que uno siempre ha generado peores resultados. \n* Aparentemente  con `min_sample_leaf` altos se obtienen mejores resultados. No obstante, como se han establecido valores pequeños en la búsqueda Grid no quiere decir que valores superiores vayan a aportar mejores resultados siempre. \n"},{"metadata":{},"cell_type":"markdown","source":"## Construcción y validación del modelo final\n\n\nUna vez hemos obtenido por medio de la búsqueda en Grid todos los modelos podemos validarlos contra el conjunto de test para saber cuál ha resultado ser el mejor modelo."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la tabla superior podemos observar los resultados del conjunto de test contra todos los modelos. Nos fijaremos en especial sobre `M – recall` pues como ya comentábamos en la anterior práctica, siempre será mejor un falso positivo que un falso negativo, para el caso de `M`.\n\nEl modelo “ganador” sería el `Gradient Boosting` con un `M - recall` del 94,3% y un score del 95,5%. \n\n`Histogram Gradient Boosting` tiene una puntuación del 94,55% y un `M - recall` idéntico a los `Vecinos más cercanos`.\n\nEl siguiente modelo, ordenado por score, sería el `Random Forest` con un 94% similar a `Histogram Gradient Boosting`. Empero, `M - recall` desciende hasta un 90%.  \nPosteriormente se encuentra el `Decision tree` con unas puntuaciones muy similares, descendiendo el score un 1%.\n\nCabe destacar los resultados del `AdaBoost` y del `Bagging`. Ambos siendo ensembles formados por árboles de decisiones deberían obtener mejores resultados que un simple árbol de decisión. En este caso no es así, incluso llegando a obtener un 4% menos de diferencia en `M - recall`. Esto pensamos que ha podido ocurrir por la partición de los datos y la semilla y no siendo un caso extrapolable a todos los casos.\n\nPor último, observamos que los vecinos más cercanos es el peor modelo tanto en `score` como en `M – recall` con unas diferencias de un 10%.\n\n<br>\n\nPara una mayor legibilidad de los resultados se han representado todos los modelos en una matriz de confusión clasificando el conjunto de test. **Advertencia**: La puntuación junto a las matrices de confusión es la `accuracy` siendo completamente diferente del score de la tabla superior. Dicho score hace referencia a la métrica `balanced_accuracy`.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('k_neighbors_clf')\nutils.evaluate(k_neighbors_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('decision_tree_clf')\nutils.evaluate(decision_tree_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('adaboost_clf')\nutils.evaluate(adaboost_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('bagging_clf')\nutils.evaluate(bagging_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('random_forest_clf')\nutils.evaluate(random_forest_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('gradient_boosting_clf')\nutils.evaluate(gradient_boosting_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('hist_gradient_boosting_clf')\nutils.evaluate(hist_gradient_boosting_clf.best_estimator_, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n---"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Project (Predict Ad click)\n\n---\n\n### **Cambios**\n\n* Se ha encontrado una importante fuga de datos. Se ha ido indicando donde se encontraba mediante `fuga de datos`.\n* Se ha remplazado todo el preprocesamiento por un pipeline de sklearn.\n* Con el objeto de mantener un idioma común en toda la libreta se ha traducido completamente la libreta original del inglés al español.\n* Para mantener una libreta más limpia se han movido las declaraciones de los métodos al script de `utils`.\n\n---\n\nLa Regresión Logística se utiliza comúnmente para estimar la probabilidad de que una instancia pertenezca a una clase particular. Si la probabilidad estimada de que una instancia sea mayor del 50%, entonces el modelo predice que la instancia pertenece a esa clase 1, o bien predice que no es así. Esto lo convierte en un clasificador binario. En esta libreta veremos la teoría detrás de la Regresión Logística y la usaremos para indicar si un usuario particular de Internet hizo clic o no en un anuncio. Intentaremos crear un modelo que prediga si harán clic o no en un anuncio basado en las características de ese usuario.\n\nEste conjunto de datos contiene las siguientes características:\n\n* '`Daily Time Spent on Site`': tiempo del consumidor en el sitio en minutos\n* '`Age`': edad del consumidor en años\n* '`Area Income`': Promedio de ingresos de la zona geográfica del consumidor\n* '`Daily Internet Usage`': La media de minutos al día que el consumidor está en Internet\n* '`Ad Topic Line`': Cabecera del anuncio\n* '`City`': Ciudad del consumidor\n* '`Male`': Si el consumidor era hombre o no\n* '`Country`': Pais del consumidor\n* '`Timestamp`': Hora en la que el consumidor pulsó en el anuncio o cerró la ventana\n* '`Clicked on Ad`': 0 o 1 indicando si se hizo click en el anuncio"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import make_pipeline\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\n\n# Scripts\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variación: No se había declarado una semilla\nseed = 337839890","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Obtener los datos\n\n*En la libreta original no se dividia el conjunto hasta mucho más adelante. Nos hallamos ante una fuga de datos debido a que vamos a visualizar información de todos los datos.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/advertising/advertising.csv\")\n\n# ERROR: Mostrar el head no es una muestra siginificativa del conjunto\n# data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUEVO\ntrain, test, X_train, X_test, y_train, y_test = utils.split_data(data, 'Clicked on Ad', seed=seed, train_size=0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUEVO\nprint(train.shape)\ntrain.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUEVO\nprint(test.shape)\ntest.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: data.info()\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: data.describe()\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Análisis exploratorio de los datos\n\n*En la libreta original no se ha explicado nada de ninguna gráfica. Sería recomendable explicar qué representa cada una.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: data.Age.hist(bins=data.Age.nunique())\nplt.figure(figsize=(10, 8))\ntrain.Age.hist(bins=train.Age.nunique())\nplt.xlabel('Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La distribución de las edades sigue una distribución normal con tendencia a la derecha. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: sns.jointplot(data[\"Area Income\"], data.Age)\nsns.jointplot(train[\"Area Income\"], train.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la gráfica superior se puede observar la correlación entre las variables `Area Income` y `Age`. Podemos observar una mayor concentración de altos ingresos en edades alrededor de los 30 años."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: sns.jointplot(data[\"Daily Time Spent on Site\"], data.Age, kind='kde')\nsns.jointplot(train[\"Daily Time Spent on Site\"], train.Age, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la gráfica superior se puede observar la correlación entre las variables `Daily Time Spent on Site ` y `Age`. Podemos observar que alrededor de los 30 años se tiende a pasar más tiempo en una página web."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: sns.jointplot(data[\"Daily Time Spent on Site\"], data[\"Daily Internet Usage\"])\nsns.jointplot(train[\"Daily Time Spent on Site\"], train[\"Daily Internet Usage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la gráfica superior se puede observar la correlación entre las variables `Daily Time Spent on Site ` y `Daily Internet Usage`. Podemos observar que existen dos partes diferenciadas. Por una parte, tendremos las personas que dedican poco tiempo a internet y por lo tanto dedican poco tiempo en cada página web. Por otra parte, observamos el opuesto, gente que pasa mucho tiempo en internet y dedica más tiempo a cada página web."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: sns.pairplot(data, hue='Clicked on Ad')\nsns.pairplot(train, hue='Clicked on Ad')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la gráfica superior se puede observar la correlación entre todas las variables representables. Dichas gráficas diferenencian por la variable `Clicked on Ad` que es nuestra variable objetivo. Podemos observar que hay una clara diferenciación entre los valores de la variable clase. En especial para `Daily Internet Usage`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: data['Clicked on Ad'].value_counts()\ntrain['Clicked on Ad'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fuga de datos: sns.heatmap(data.corr(), annot=True)\nplt.figure(figsize=(12, 8))\nsns.heatmap(train.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El mapa de calor superior nos muestra que no existe una gran correlación entre variables. Las únicas excepciones serían las variables: `Clicked on Ad` con `Daily Time Spent on Site` y con `Dailiy Internet Usage`. Ambas muestran una correlación inversa."},{"metadata":{},"cell_type":"markdown","source":"## 2. Teoría de Regresión Logistica\n\n*En este apartado no se han realizado modificaciones por errores si no por estética*\n\nLa regresión logística es el algoritmo de clasificación lineal para problemas de dos clases. Es fácil de implementar, fácil de entender y obtiene grandes resultados en una amplia variedad de problemas, incluso cuando se incumplen las expectativas que el método tiene para sus datos.\n\n### Descripción\n\n#### Regresión logística\n\nLa regresión logística se denomina así por la función utilizada en el núcleo del método, la [función logística](https://en.wikipedia.org/wiki/Logistic_function).\n\nLa función logística, también llamada la **`función sigmoide`** fue desarrollada por los estadísticos para describir las propiedades del crecimiento de la población en la ecología, aumentando rápidamente y maximizando la capacidad de carga del medio ambiente. Es una curva en forma de S que puede tomar cualquier número de valor \n\n$$\\frac{1}{1 + e^{-x}}$$\n\n$e$ es la base del logaritmo natural y $x$ es el valor que quieres transformar por medio de la función logística."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(10, 6))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.title(\"Sigmoid Function\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La ecuación de regresión logística tiene una representación muy similar a la de la regresión lineal. La diferencia es que el valor de salida que se modela es de naturaleza binaria.\n\n$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n\no\n\n$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n\n$\\beta_0$ es el término de intercepción\n\n$\\beta_1$ es el coeficiente para $x_1$\n\n$\\hat{y}$ es la salida prevista con un valor real entre 0 y 1. Para convertirla en una salida binaria de 0 o 1, sería necesario redondearla a un valor entero o proporcionar un punto de corte para especificar el punto de segregación de la clase.\n***\n### Aprendiendo el modelo de regresión logística\n\nLos coeficientes (valores Beta b) del algoritmo de regresión logística deben ser estimados a partir de los datos de entrenamiento. Esto se hace usando la [estimación de máxima probabilidad](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n\nLa estimación de máxima probabilidad es un algoritmo de aprendizaje común utilizado por una variedad de algoritmos de Machine Learning, aunque hace suposiciones sobre la distribución de sus datos (más sobre esto cuando hablamos de la preparación de sus datos).\n\nLos mejores coeficientes darían como resultado un modelo que predeciría un valor muy cercano a 1 (por ejemplo, masculino) para la clase por defecto y un valor muy cercano a 0 (por ejemplo, femenino) para la otra clase. La intuición de la máxima probabilidad para la regresión logística es que un procedimiento de búsqueda busca valores para los coeficientes (valores Beta) que minimicen el error en las probabilidades predichas por el modelo a las de los datos (por ejemplo, probabilidad de 1 si los datos son de la clase primaria).\n\nNo vamos a entrar en la matemática de la máxima probabilidad. Basta decir que se utiliza un algoritmo de minimización para optimizar los mejores valores de los coeficientes de sus datos de entrenamiento. Esto se implementa a menudo en la práctica utilizando un algoritmo de optimización numérica eficiente (como el método Quasi-newton).\n\nCuando se está aprendiendo logística, se puede implementar desde cero utilizando el algoritmo de gradiente descendiente mucho más sencillo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Se ha trasladado al utils la función 'print_score'.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Razones para utilizar `scikit-learn` (no Pandas) para el preprocesamiento de ML:\n1. Puedes hacer validación cruzada de todo el flujo de trabajo.\n2. Puedes cuadricular el modelo de búsqueda y los hiperparámetros de preprocesamiento.\n3. Evita añadir nuevas columnas al DataFrame de origen.\n4. Pandas carece de pasos de ajuste/transformación separados para evitar la fuga de datos."},{"metadata":{},"cell_type":"markdown","source":"*Fuga de Datos: La separación entre conjuntos de test y train no se había realizado hasta este apartado.*\n\n*Se va a construir un Pipeline para el preprocesamiento de los datos.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_columns_names = ['Timestamp', 'Ad Topic Line', 'Country', 'City']\nto_drop_columns = [X_train.columns.get_loc(column) for column in to_drop_columns_names]\ndrop_transformer = utils.drop_columns(to_drop_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']\n\nct = make_column_transformer(\n    (MinMaxScaler(), num_columns),\n    (StandardScaler(), num_columns),\n    remainder='passthrough'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Preparación de los datos para la regresión logística\n\nLas suposiciones hechas por la regresión logística sobre la distribución y las relaciones en sus datos son muy parecidas a las suposiciones hechas en la regresión lineal.\n\nSe han realizado muchos estudios para definir estas suposiciones y se utiliza un lenguaje probabilístico y estadístico preciso. Mi consejo es usarlos como pautas o reglas generales y experimentar con diferentes esquemas de preparación de datos.\n\nEn última instancia, en los proyectos Machine Learning de modelado predictivo, el objetivo se centra en hacer predicciones precisas en lugar de interpretar los resultados. Como tal, puedes romper algunas suposiciones siempre y cuando el modelo sea robusto y tenga un buen rendimiento.\n\n- **Variable de salida binaria:** Esto podría ser obvio como ya lo hemos mencionado, pero la regresión logística está pensada para problemas de clasificación binaria (de dos clases). Predecirá la probabilidad de que una instancia pertenezca a la clase por defecto, que puede ser encajada en una clasificación de 0 o 1.\n- **Eliminar el ruido:** La regresión logística no supone ningún error en la variable de salida (y), hay que considerar la posibilidad de eliminar los valores atípicos y las posibles instancias mal clasificadas de sus datos de entrenamiento.\n- **Distribución Gaussiana:** La regresión logística es un algoritmo lineal (con una transformación no lineal en la salida). Supone una relación lineal entre las variables de entrada y las de salida. Las transformaciones de datos de sus variables de entrada que exponen mejor esta relación lineal pueden dar lugar a un modelo más preciso. Por ejemplo, puede usar log, root, Box-Cox y otras transformaciones univariantes para exponer mejor esta relación.\n- **Eliminar entradas correlacionadas:** Como la regresión lineal, el modelo puede sobreajustarse si se tienen múltiples entradas altamente correlacionadas. Considera el cálculo de las correlaciones por pares entre todas las entradas y elimina las entradas altamente correlacionadas.\n- **Fallar en la convergencia:** Es posible que el proceso de estimación de la verosimilitud esperada que aprende los coeficientes no converja. Esto puede suceder si hay muchas entradas altamente correlacionadas en sus datos o si los datos son muy escasos (por ejemplo, muchos ceros en sus datos de entrada)."},{"metadata":{},"cell_type":"markdown","source":"*Se ha añadido un Pipeline*"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = make_pipeline(\n    drop_transformer,\n    ct,\n    LogisticRegression(solver='liblinear')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = make_pipeline(\n    drop_transformer,\n    ct,\n    RandomForestClassifier(n_estimators=1000)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Implementación de la regresión logística en Scikit-Learn\n\n*No explicaban nada de los resultados obtenidos.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nutils.print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nutils.print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que hemos obtenido una mayor accuracy con el conjunto de test que con el conjunto de entrenamiento. Esto es debido a la varianza que presentan los datos. \n\nHemos obtenido una accuracy muy alta, por lo que se espera obtener unas buenas predicciones reales. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf_clf = RandomForestClassifier(n_estimators=1000)\nrf_clf.fit(X_train, y_train)\n\nutils.print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nutils.print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que los resultados del conjunto de entrenamiento (train) es de un 100%. Mientras que en el conjunto de prueba (test) se ha obtenido un 97.67%. Por lo tanto, observamos un sobreajuste de los datos. "},{"metadata":{},"cell_type":"markdown","source":"## 5. Medida del rendimiento\n\n#### 1. Matriz de la confusión\n\n- Cada fila: clase real\n\n- Cada columna: clase prevista\n\nPrimera fila: Anuncios sin clic, la clase negativa:\n\n* 146 fueron clasificados correctamente como anuncios no seleccionados. **Verdaderos negativos**.\n\n* Los 4 restantes fueron erróneamente clasificados como anuncios en los que se ha hecho clic. **Falsos positivos**.\n\nSegunda fila: Los anuncios clicados, la clase positiva:\n\n* 4 se clasificaron incorrectamente como Anuncios sin clic. **Falsos negativos**\n\n* 143 se clasificaron correctamente en los anuncios de clic. **Verdaderos positivos**.\n\n### 2. Precisión\n\nLa *precisión* mide la exactitud de las predicciones positivas. También llamada la precisión del clasificador ==> `97.33%`\n\n$$\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n\n#### 3. Recall\n\n\"Precisión\" se usa típicamente con \"recall\" (\"Sensibilidad\" o \"Tasa positiva real\"). La proporción de instancias positivas que son detectadas correctamente por el clasificador.\n\n$$\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$ ==> `97.33%`\n\n#### 4. F1 Score\n\n$F_1$ score es el medio armónico de la precisión y el recall. La media regular da el mismo peso a todos los valores. La media armónica da más peso a los valores bajos.\n\n\n$$F_1=\\frac{2}{\\frac{1}{\\textrm{precision}}+\\frac{1}{\\textrm{recall}}}=2\\times \\frac{\\textrm{precision}\\times \\textrm{recall}}{\\textrm{precision}+ \\textrm{recall}}=\\frac{TP}{TP+\\frac{FN+FP}{2}}$$ ==> `97.33%`\n\nEl $F_1$ favorece a los clasificadores que tienen una precisión y un recall similares.\n\n#### 5. Precisión / Recall Tradeoff\n\nEl aumento de la precisión reduce el recall y viceversa."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Se ha trasladado al utils la función 'plot_precision_recall_vs_threshold'.\nutils.plot_precision_recall_vs_threshold(lr_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con este gráfico, puede seleccionar el valor de umbral que le da la mejor precisión/recall para su tarea.\n\nAlgunas tareas pueden requerir una mayor precisión (exactitud de las predicciones positivas). Como diseñar un clasificador que recoja contenidos para adultos para proteger a los niños. Esto requerirá que el clasificador establezca una barra alta para permitir que cualquier contenido sea consumido por los niños.\n\nAlgunas tareas pueden requerir un mayor recall (proporción de instancias positivas que son detectadas correctamente por el clasificador). Como la detección de ladrones/intrusos en las imágenes de vigilancia - Cualquier cosa que remotamente se parezca a las instancias \"positivas\" debe ser recogida.\n\n***"},{"metadata":{},"cell_type":"markdown","source":"### La curva ROC\n\nEn lugar de trazar la precisión frente al recall, la curva ROC traza la `tasa positiva verdadera` (otro nombre para el recall) contra la `tasa positiva falsa`. La `tasa positiva falsa` (FPR) es la proporción de instancias negativas que se clasifican incorrectamente como positivas. Es igual a uno menos la `tasa negativa verdadera`, que es la proporción de instancias negativas que se clasifican correctamente como negativas.\n\nEl TNR también se llama `especificidad`. Por lo tanto, la curva ROC traza la `sensibilidad` (recall) frente a `1 - especificidad`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Se ha trasladado al utils la función 'plot_roc_curve'.\nutils.plot_roc_curve(lr_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, lr_clf.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usa la curva PR cuando la clase **positiva es rara** o cuando te preocupas más por los falsos positivos que por los falsos negativos\n\nUsa la curva ROC cuando la clase **negativa es rara** o cuando te importan más los falsos negativos que los falsos positivos\n\n\nEn el ejemplo anterior, la curva ROC parecía sugerir que el clasificador es bueno. Sin embargo, cuando se mira la curva PR, se puede ver que hay espacio para mejorar."},{"metadata":{},"cell_type":"markdown","source":"## 6. Logistic Regression Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"penalty = ['l1', 'l2']\nC = [0.5, 0.6, 0.7, 0.8]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nlr_cv = utils.optimize_params(estimator=lr_clf, X=X_train, y=y_train, cv=10,\n                        logisticregression__penalty=penalty, logisticregression__C=C, logisticregression__class_weight=class_weight, logisticregression__solver=solver,\n                        scoring='accuracy')\n\nutils.print_score(lr_cv.best_estimator_, X_train, y_train, X_test, y_test, train=True)\nutils.print_score(lr_cv.best_estimator_, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez obtenido el mejor modelo por la búsqueda en Grid observamos que el accuracy entre el conjunto entrenamiento y el conjunto de test apenas varía. "},{"metadata":{},"cell_type":"markdown","source":"## 7. Resumen\nEn esta libreta se ha descubierto el algoritmo de regresión logística para Machine Learning y el modelado predictivo. Se ha cubierto mucho terreno y se aprendió:\n\n- Qué es la función logística y cómo se utiliza en la regresión logística.\n- Que la representación clave en la regresión logística son los coeficientes, al igual que la regresión lineal.\n- Que los coeficientes en la regresión logística se estiman usando un proceso llamado estimación de máxima probabilidad.\n- Que hacer predicciones usando la regresión logística es tan fácil que se puede hacer en Excel.\n- Que la preparación de los datos para la regresión logística es muy similar a la regresión lineal.\n- Cómo evaluar un problema de clasificación de aprendizaje de una máquina.\n- Cómo ajustar los hiperparámetros de la regresión logística.\n\n### Referencias:\n- [Scikit Learn Library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n- [Logistic Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}