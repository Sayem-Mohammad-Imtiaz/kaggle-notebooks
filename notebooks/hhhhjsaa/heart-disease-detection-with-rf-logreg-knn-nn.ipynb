{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Data Set\n\nby Theodor Lanzer\n\nTask: Find out if Heart Disease is present at the current patient. \n\nClassification Problem","metadata":{"id":"3eokBXR0FPJ2"}},{"cell_type":"markdown","source":"#1. Problem Definition and description of data\n\nThe selected dataset consists of physical attributes of a human. Some features are measured values, others are subjectively determined by the patient. The features will be discussed in more detail in the following. The important target variable is whether a heart disease is present in the patient or not. \n\nThis is a binary classification problem. A heart disease is either present or not. \n\nThe dataset was published by the Medical Center, Long Beach and Cleveland Clinic Foundation. \n\nNow lets look at the attributes of this data set.\n\nAttribute Information:\n\nThe Problem has the following Inputs:\n\n1.   **age**: in years\n2.   **sex**:\n\n  *   Value 1: male\n  *   Value 0: female\n\n\n3.   **cp**: chest pain \n\n  *   Value 1: typical angina\n  *   Value 2: atypical angina\n  *   Value 3: non-anginal pain\n  *   Value 4: asymptomatic \n\n\n4.   **trestbps**: resting blood pressure (in mm HG)\n\n5.   **chol**: serum cholestoral in mg/dl\n6.   **fbs**: fasting blood sugar > 120 mg/ml\n\n   *    Value 1: true\n   *    Value 0: false\n\n\n\n7.   **restecg**:  resting electrocardiographic results\n\n  *   Value 0: normal\n  *   Value 1: having ST-T wave abnormality\n  *   Value 2: showing probale or  definite left ventricular hypertrophy\n\n\n8.   **thalach**: maximum heart rate achieved\n\n\n\n9.   **exang**: exercise induced angina\n\n *   Value 1: yes\n  *   Value 0: no\n\n\n10.   **oldpeak**: ST depression induced by exercise relative to rest\n\n\n11.   **slope**: the slope of the peak exercise ST segment\n \n  *   Value 1: upsloping\n  *   Value 2: flat\n  *   Value 3: downsloping\n\n\n \n12.   **ca**: number of major vessels (Values from 0-3)\n\n\n13.   **thal**: A blood disorder called 'Thalassemia':\n\n  *   Value 3: normal\n  *   Value 6: fixed detected\n  * Value 7: reversable detected\n\nOutput:\n\n\n\n1.  **Heartdisease**:\n\n  *   Value 1: present\n  *   Value 0: not present\n\n\nTo start with the topic, I researched which factors promote heart disease.\nI found out that risk factors for developing heart disease are the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking. I would like to find out if these hypotheses  can be confirmed with the present data set. (1).\n\nTo measure my success, I decided to use accuracy. This gives me a value of how many percent were correctly classified. Additionally I look at the Area under curve and Precision and Recall.\n\nSource:\n(1) https://www.nhs.uk/conditions/cardiovascular-disease/\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"aSQ25-MpkWQC"}},{"cell_type":"markdown","source":"#2. Preparing the enviroment","metadata":{"id":"angpPZzHHJAM"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\nimport matplotlib as mpl\nimport plotly.express as px\nimport matplotlib.cm as cm\nimport seaborn as sns\nsns.set_theme()\nimport os","metadata":{"id":"ESy71IIEHRwN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preparation\nfrom sklearn import preprocessing as pp\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n","metadata":{"id":"dw14KTneHTeZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ML Algorithms to be used\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as LGBMClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import optimizers, models, layers, regularizers\ntf.__version__\n","metadata":{"id":"GKCOafuZHVmE","outputId":"efa98174-3794-4173-97ba-976215752279","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1 Import Data from Kaggle\n\nBefore I start the research, I want to get an overview of the data. And whether there are any discrepancies. As a first look I take a look at the correlation matrix. Looking at the feature Chest Pain (cp) it has values from 0 to 3, but at the description it takes values from 1 to 4.","metadata":{"id":"PSByRZgDzC72"}},{"cell_type":"code","source":"\ndata = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndata.head()","metadata":{"id":"QCEZth5x1zR_","outputId":"418159bb-0af0-4a89-804e-0e0891e30a46","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1.2 Is Kaggle Data Set right? A look at the correlation matrix\n","metadata":{"id":"MvF-a72G2A5r"}},{"cell_type":"code","source":"correlationMatrix = data.corr() \n\nf = plt.figure(figsize=(15, 8))\nplt.matshow(correlationMatrix, fignum=f.number, cmap='viridis')\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=75)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\ncorrelationMatrix.style.background_gradient(cmap='viridis').set_precision(2)\nplt.show()","metadata":{"id":"TGm1H1tb2IdG","outputId":"a5941086-26c2-4c93-bbb4-3224e0fa8334","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlationMatrix.style.background_gradient(cmap='viridis').set_precision(2)","metadata":{"id":"uCxpMnr82VHL","outputId":"5ea994ae-576b-4745-b6bd-0ef5124b6098","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After looking at the correlation matrix, I noticed that the feature exang is negatively correlated with target (heartdisease). Which means that a angina induced by exercise would reduce the risk of heart disease. Angina is a type of chest pain caused by reduced blood flow to the heart(1). This makes no sense! In addition, younger people are more likely to be affected by heart diseases. The feature ca has different values than discribed. Something seems to be wrong. So I did some more research and found out that the target values are reversed. Now I have decided to import the data set from the original source and prepare it myself.\n\n\nSorce:\n\n(1) https://www.mayoclinic.org/diseases-conditions/angina/symptoms-causes/syc-20369373#:~:text=But%20when%20you%20increase%20the,arteries%20slow%20down%20blood%20flow.","metadata":{"id":"LQwsIfAp4jYq"}},{"cell_type":"markdown","source":"#3.2 Import Data from original source \nThe Kaggle Dataset has a few inconsistencies. Therefore I import the dataset from the original website (https://archive.ics.uci.edu/ml/datasets/Heart+Disease) and prepare it myself.","metadata":{"id":"TlAy14K9HYfh"}},{"cell_type":"code","source":"url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\nnew_names = ['age','sex','cp','trestbps','chol','fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'heartdisease']\ndataframe = pd.read_csv(url, names=new_names)\ndataframe.head()","metadata":{"id":"5lFWUv02HapT","outputId":"dcb61032-a0d9-484a-d401-227414ed9ba9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#4. Preprocessing Data ","metadata":{"id":"eZlDZtIvI1Zt"}},{"cell_type":"markdown","source":"At first I check for missing values and datatypes.","metadata":{"id":"YKBo6FQDIE5K"}},{"cell_type":"code","source":"dataframe.info()","metadata":{"id":"B2nB-IY0IRkE","outputId":"d247b07e-bf56-44f5-b6ef-ffb9033f09dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features **ca** and **thal** displayed as object-type, which is wrong. They should be numeric as the other features. After an investigation I found out that there are some lines with **?**. I decicded to remove those lines. ","metadata":{"id":"CcEtZzAtIVSJ"}},{"cell_type":"markdown","source":"Remove Questionmarks from **ca**\n1. Find those indices with Questionmark and remove whole lines\n2. Change datatype to float","metadata":{"id":"7QTcRNPKJhOE"}},{"cell_type":"code","source":"index_invalid_ca = dataframe[dataframe.ca == '?'].index\ndataframe.drop(index_invalid_ca, inplace = True)\ndataframe.ca = pd.to_numeric(dataframe.ca, downcast = 'float')","metadata":{"id":"9Wa3QZ_kJKu6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove Questionmarks from **thal**","metadata":{"id":"KN5wf-orKYjF"}},{"cell_type":"code","source":"index_invalid_thal = dataframe[dataframe.thal == '?'].index\ndataframe.drop(index_invalid_thal, inplace = True)\ndataframe.thal = pd.to_numeric(dataframe.thal, downcast = 'float')","metadata":{"id":"kJwf3oEjJlb5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the original dataset, the target takes integer values between 0 and 4. Here, 0 means no heart disease and 1 to 4 means the severity of the heart disease. Since we only want to find out whether a disease is present or not, the values greater than 0 are combined to 1. This leads to a binary classification problem.\n","metadata":{"id":"aCfZFjCmMl37"}},{"cell_type":"code","source":"dataframe.heartdisease = dataframe.heartdisease.where(dataframe.heartdisease < 1, 1)\n\n#update index\ndataframe = dataframe.reset_index()","metadata":{"id":"BBY9SgbINlN7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring Data and Visualization","metadata":{"id":"Gb-gjZ4uPFqT"}},{"cell_type":"markdown","source":"First, I look at the histograms to see how the data is distributed. There are more men than women. 160 participants have no heart disease and 137 have one. This is a balanced problem.","metadata":{"id":"Huqyk8fc-S6S"}},{"cell_type":"code","source":"dataframe = dataframe.copy().drop(['index'], axis = 1)","metadata":{"id":"b2qwAM46zAID","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe.describe()","metadata":{"id":"QxBKoVAYhJ0m","outputId":"93faf4ee-98d9-4520-a808-dab17713aba7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe.hist(bins = 15,figsize= (20,20))\nplt.show()","metadata":{"id":"VKfRGjHjPPcD","outputId":"e63cb655-7797-4845-944f-0257242e53da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For clarity in the next plots I replace here the values of the attributes sex and heartdisease. Afterwards I change the values back again.","metadata":{"id":"UbBgpM-u_eAV"}},{"cell_type":"code","source":"dataframe['sex'].replace({1:'Male',0:'Female'},inplace = True)\ndataframe['heartdisease'].replace({1:'Heart_attack - Yes',0:'Heart_attack - No'},inplace = True)","metadata":{"id":"zI2wF77_D_x6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot below, the age range of women is greater for no heart disease than for men. The range of age for heart disease is larger for men.","metadata":{"id":"nezazOtk_1Td"}},{"cell_type":"code","source":"sns.catplot(x ='age', y ='heartdisease', col = 'sex', data = dataframe, color = 'crimson', kind = 'box')","metadata":{"id":"q2r8ibinCjQS","outputId":"a0abb383-aeb9-4930-9f58-f2c9cdd051e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table shows the number of heart disease cases by gender and age. This shows that most heart disease is present in men between the ages of 57 and 59. ","metadata":{"id":"eqOgHG6HA607"}},{"cell_type":"code","source":"s= dataframe.groupby(['sex','age'])['heartdisease'].count().reset_index().sort_values(by='heartdisease',ascending=False)\ns.head(10).style.background_gradient(cmap='Purples')","metadata":{"id":"bIs2gUtZDqG4","outputId":"0fc1b150-d12e-4c56-963c-c44abae487fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next plot shows the patient's age on the X-axis and the chest pain attribute on the Y-axis. In addition, the plot shows whether a heart disease is present or not. It can be seen that with a value of 4 ( asymptotic) most of the heart diseases are present. ","metadata":{"id":"PEcB_-vbCKi_"}},{"cell_type":"code","source":"p1 = sns.scatterplot(data = dataframe, x = 'age', y = 'cp', hue = \"heartdisease\", s = 200)\np1.set(xlabel='Age [Years]', ylabel='Chest Pain')","metadata":{"id":"xo5q3ttx8ceR","outputId":"522e8bb5-dcb4-42ff-9cb2-2459e862531f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next plot shows the relationship between thalassemia and age. Having reversable detected thalassemia seems as a pretty strong indicator for heart disease. (Value of 7).","metadata":{"id":"RiqBgHM4GGYA"}},{"cell_type":"code","source":"p3 = sns.scatterplot(data = dataframe, x = 'age', y = 'thal', hue = \"heartdisease\", s = 200)\np3.set(xlabel='Age [Years]', ylabel='thalassemia')","metadata":{"id":"TckmLbcOFKyq","outputId":"c7a1136d-0e38-45b7-ce2d-3f4621a561da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next plot shows the relationship between resting blood pressure and age. In this plot, no relationship is apparent with respect to heart disease.","metadata":{"id":"HqXl0AeX9LgC"}},{"cell_type":"code","source":"p2 = sns.scatterplot(data = dataframe, x = 'age', y = 'trestbps', hue = \"heartdisease\", s = 200)\np2.set(xlabel='Age [Years]', ylabel='resting blood pressure')","metadata":{"id":"y3yExN_D8Hes","outputId":"64423d1a-9d71-4ed3-f9ab-04846983f5ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe['sex'].replace({'Male':1,'Female':0},inplace = True)\ndataframe['heartdisease'].replace({'Heart_attack - Yes':1,'Heart_attack - No':0},inplace = True)","metadata":{"id":"QIDxT8k6F-b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation Matrix:\n\nFrom the correlation matrix, age, chest pain, exang, slope, oldpeak, ca, thal correlate positively with heartdisease. Thalach, restecg are negatively correlated. \n\nResting blood pressure and cholesterol correlate poorly with heart disease. In my hypthosis from the beginning, however, I assumed this.\n\nThe attribute fbs does not correlate with heart disease at all. Therefore, I decide to take it out for the further calculations.","metadata":{"id":"ILS8VBV0kdUH"}},{"cell_type":"code","source":"correlationMatrix = dataframe.corr() \n\nf = plt.figure(figsize=(15, 8))\nplt.matshow(correlationMatrix, fignum=f.number, cmap='viridis')\nplt.xticks(range(dataframe.shape[1]), dataframe.columns, fontsize=15, rotation=65)\nplt.yticks(range(dataframe.shape[1]), dataframe.columns, fontsize=15)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=15)\nplt.show()","metadata":{"id":"utWMjJLzkT2D","outputId":"4331340d-2a7c-4cb5-cdb4-4be83e9bd567","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlationMatrix.style.background_gradient(cmap='viridis').set_precision(2)","metadata":{"id":"cs3IMtAukW2p","outputId":"a66af740-52b3-4e5f-f910-fb4af6126a4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Dataset for ML","metadata":{"id":"Sa1FshuakiK_"}},{"cell_type":"markdown","source":"### Creating Feature Matrix\n\nCreating arrays for X and Y data. As I said, I drop **fbs** on X Data \n","metadata":{"id":"mlgxN6-JgI6n"}},{"cell_type":"code","source":"dataX = dataframe.copy().drop(['heartdisease', 'fbs'],axis=1)\ndataY = dataframe['heartdisease'].astype(int).copy()","metadata":{"id":"104Ppxf4iDeo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataY.value_counts()","metadata":{"id":"vzeNq4scihTg","outputId":"b0e368f1-22a0-4167-b285-b41009620b2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rescaling data because most ML algorithms work better if the data is normalized around zero.","metadata":{"id":"0g0wwmuHiZHf"}},{"cell_type":"code","source":"featuresToScale = dataX.columns\nsX = pp.StandardScaler(copy=True)\ndataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])","metadata":{"id":"iX9lPV1SjDxt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split Data into training and test set. First I chose 20% as test set, but some ML Methods had way better results on test data than on trainings data, so I increased test data to 30%","metadata":{"id":"8Wpb-zOzkrxe"}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dataX,\ndataY, test_size=0.3,\nrandom_state=2021, stratify=dataY)\n\ny_test.value_counts()","metadata":{"id":"WqoBdyyYkhpx","outputId":"8ae34beb-b349-4f96-d8aa-f0e72f6f1545","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis\n\nSince I have 12 attributes I am trying out dimensional reduction to see if it improves my accuracy.","metadata":{"id":"cRXqza89lR14"}},{"cell_type":"markdown","source":"#### Functions used for PCA","metadata":{"id":"Rs06b8SfnrwT"}},{"cell_type":"code","source":"def anomalyScore (originalDF, reducedDF):\n  loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n  loss = pd.Series(data=loss,index=originalDF.index)\n  loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n  return loss","metadata":{"id":"pmdck0OSnuzx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotResults(trueLabels, anomalyScore, returnPreds = False, plotting = True):\n  preds = pd.concat([trueLabels, anomalyScore], axis=1)\n  preds.columns = ['trueLabel', 'anomalyScore']\n  \n  precision, recall, thresholds = \\\n  precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n  \n\n  average_precision = average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n  if plotting:\n    plt.step(recall, precision, color='b', alpha=0.7, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.3, color='b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.1])\n    plt.xlim([0.0, 1.0])\n    plt.title('Average Precision = {0:0.2f}'.format(average_precision))\n\n  if returnPreds==True:\n    return preds, average_precision","metadata":{"id":"bHC2LMiHn97W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparameters for PCA","metadata":{"id":"NTcw3YBQlYNB"}},{"cell_type":"code","source":"n_components =11\nsvd_solver = 'auto'\nrandom_state = 2021","metadata":{"id":"w74ANGJQlS_4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import KernelPCA\npca = KernelPCA(n_components=n_components,kernel = 'rbf', fit_inverse_transform = True ,random_state= 2021)","metadata":{"id":"xrL5ehq4mZwm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Implementation on X and Y Data","metadata":{"id":"BqUGco5dnDkJ"}},{"cell_type":"code","source":"X_train_PCA = pca.fit_transform(X_train)\nX_test_PCA = pca.fit_transform(X_test)","metadata":{"id":"iJCMrctInGTa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Organizing data into pd Framework","metadata":{"id":"9DOmvHrVnQ5A"}},{"cell_type":"code","source":"X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\nX_test_PCA = pd.DataFrame(data=X_test_PCA, index=X_test.index)","metadata":{"id":"JTalwBR2nU7P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(pca.explained_variance_ratio_)","metadata":{"id":"JXeUiiPo5LNL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.bar(range(len(pca.explained_variance_ratio_)),pca.explained_variance_ratio_)","metadata":{"id":"vvUza-e858IQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Anomaly Score","metadata":{"id":"gv_RXFC4nbOG"}},{"cell_type":"code","source":"X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\nX_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)","metadata":{"id":"HVik_j-ynfPH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomalyScorePCA = anomalyScore(X_train, X_train_PCA_inverse)\npredsPCA = plotResults(y_train, anomalyScorePCA, False, True)","metadata":{"id":"_SlUY9Hsnocw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Desicion if PCA or not.","metadata":{"id":"TRXVH4yPpqZz"}},{"cell_type":"code","source":"trigger_PCA = True\nif trigger_PCA == True:\n  X_train = X_train_PCA\n  X_test = X_test_PCA","metadata":{"id":"AP5sjeP9pvH_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross Validation","metadata":{"id":"c-31lggOqXQf"}},{"cell_type":"markdown","source":"To validate my models I use Cross Validation.","metadata":{"id":"lBtrYFA_Is0C"}},{"cell_type":"code","source":"k_fold = StratifiedKFold(n_splits=4, shuffle=True, random_state=2021)","metadata":{"id":"qY3ISbQPqaup","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#5. Model Selection \n\nI have chosen the following as my baseline models, which are well suited for classification problems:\n*   Logistic Regression \n*   Random Forests \n* K-Nearest Neighbors\n\nAfter testing the baseline models I build up a Neural Network\n*   Neural Network\n*   Fine tuning hyperparameters\n\nWorkflow:\n\nMy workflow with the Baseline models is always the same. I start with Gridsearch to find the best parameters. Then to validate the model I look at the accuracy in cross validation. After that I train the model with all the test data and look at the results using the correlation matrix, precision and recall, and ROC curve.\n\n\n\n\n","metadata":{"id":"fIYqZDBwoVzb"}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{"id":"6ohnc_rzp3SR"}},{"cell_type":"markdown","source":"Hyperparamters","metadata":{"id":"Tr0JW06TqT74"}},{"cell_type":"code","source":"penalty = 'l2' \nC = 0.1\nrandom_state = 2021\nsolver = 'liblinear'\nlogReg = LogisticRegression(penalty=penalty, C=C,random_state=random_state, solver=solver)","metadata":{"id":"PV6Pk8HTooa6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grid Search to find best result.","metadata":{"id":"g3JzHRuyCN6q"}},{"cell_type":"code","source":"penalty = ['l2']\nC = np.arange(0.01, 1, 0.1 )\nrandom_state = 2021\nsolver = ['lbfgs', 'liblinear', 'saga']\ngrid = {'penalty': penalty,'C':C, 'solver': solver}\n\ngridSearch = GridSearchCV(logReg, grid, scoring='accuracy', cv=k_fold, refit=True)\ngridSearch.fit(X_train, y_train)\nresults = gridSearch.cv_results_\n\nprint('Best accuracy obtained:', gridSearch.best_score_)\nprint('C value for the best case:', gridSearch.best_estimator_.C)\nprint('Penalty value for the best case:', gridSearch.best_estimator_.penalty)\nprint('Solver value for the best case:', gridSearch.best_estimator_.solver)","metadata":{"id":"B4bzOrEOoRbA","outputId":"5ecf08d7-9c5d-4182-d38a-026cb06aedfc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Parameters to the values from GridSearch\nlogReg.set_params(C = gridSearch.best_estimator_.C, solver = gridSearch.best_estimator_.solver )\n","metadata":{"id":"nVYS-YGOKfSv","outputId":"dbe59008-7de6-4b44-cc50-7055b12097c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross-validation for validating estimator performance","metadata":{"id":"R9Achu2zocyL"}},{"cell_type":"code","source":"#Lists for storing scores\ntrainingScores = []\ncvScores = []\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n  #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n\n  #Fitting Model\n  logReg.fit(X_train_fold, Y_train_fold)\n\n  #Checking how good the model is on trainingsdata\n  accuracy_score_Training = accuracy_score(Y_train_fold,logReg.predict(X_train_fold))\n  print('--------------------------------------------------------')\n  print('Training accuracy_score: ', accuracy_score_Training)\n  #Checking how good the model is on cv data\n  accuracy_score_cv = accuracy_score(Y_cv_fold,logReg.predict(X_cv_fold))\n  print('CV accuracy_score: ', accuracy_score_cv)\n\n  trainingScores.append(accuracy_score_Training)\n  cvScores.append(accuracy_score_cv)\n\nprint('--------------------------------------------------------')\nprint('--------------------------------------------------------')\nmean_accuracy_score_training = np.array(trainingScores).mean()\nprint('mean Accuracy_score Training:', mean_accuracy_score_training )\nprint('--------------------------------------------------------')\nmean_accuracy_score_cv = np.array(cvScores).mean()\nprint('mean Accuracy_score cv:', mean_accuracy_score_cv )","metadata":{"id":"0RaBS7E4sybc","outputId":"640c3079-d9bb-47f0-ee31-5317f5e6abe3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train model with all trainings data and hyperparameters from above","metadata":{"id":"vIR8seH8AsDt"}},{"cell_type":"code","source":"logReg.fit(X_train, y_train)","metadata":{"id":"5wKjbBYhwutB","outputId":"970cbb5d-bef3-42df-d8ac-e579bd2a65d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test model with training and test data set.","metadata":{"id":"If58D4JWpq6B"}},{"cell_type":"code","source":"# Prediction and accuracy on trainings data\ny_pred_train_proba_lg = logReg.predict_proba(X_train)\ny_pred_train_proba_lg = pd.DataFrame(data = y_pred_train_proba_lg, index = X_train.index)\n\ny_train_preds_lg = logReg.predict(X_train)\naccuracy_training_ges_lg = accuracy_score(y_train,y_train_preds_lg)","metadata":{"id":"DnprVcphp6YO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction and accuracy on test data\ny_pred_proba_lg = logReg.predict_proba(X_test)\ny_pred_proba_lg = pd.DataFrame(data = y_pred_proba_lg, index = X_test.index)\n\ny_preds_lg = logReg.predict(X_test)\naccuracy_test_ges_lg = accuracy_score(y_test,y_preds_lg)\n","metadata":{"id":"izT3sD4zBBSV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy for training and test data:","metadata":{"id":"bWuz1yJrL5qg"}},{"cell_type":"code","source":"print('--------------------------------------------------------')\nprint('accuracy_score whole trainings set', accuracy_training_ges_lg )\nprint('--------------------------------------------------------')\nprint('accuracy_score whole test set', accuracy_test_ges_lg )\nprint('--------------------------------------------------------')","metadata":{"id":"NqFiOxr3MAxp","outputId":"f3a44868-4f77-43d3-89de-d9f968d16675","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n## Evaluate the results\n\n\n\n","metadata":{"id":"e43WlAKwBLIF"}},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{"id":"PjMPbTejKdOu"}},{"cell_type":"code","source":"cm1 = confusion_matrix(y_test,y_preds_lg)\n#storing false negatives\nfn_lg = cm1[1,0]","metadata":{"id":"CWhD9OEm8Pw6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(logReg,X_test,y_test,cmap='Blues')","metadata":{"id":"ZIfWcA-bBXT5","outputId":"2ded9570-f939-4fd9-8811-874ff054ab82","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Precision Recall Curve:\n\n**Precision** \n\n*   Precision = True Positive / ( True Positive  + False Positive)\n*   captures how often, when a model makes a positive predeiction, this prediction turns out to be correct.\n\n**Recall** \n\n\n*   Recall  = True Positive / (True Positive + False Negative)\n*   tells us how confident we can be that all instances with the positive target level have been found the model\n\ngo through the treshold as in ROC. Count the Values for every treshold","metadata":{"id":"KWrF3iSgCdz3"}},{"cell_type":"code","source":"preds = pd.concat([y_test,y_pred_proba_lg.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))","metadata":{"id":"cq4q1nXUCh51","outputId":"1187d637-150d-412e-b6cb-8741281b8756","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC Curve","metadata":{"id":"YA28id1xFX5c"}},{"cell_type":"markdown","source":"The True Positives and False Negatives are calculated for each threshold. The curve results from all these points. The closer the curve is to the upper left corner, the better the solution.","metadata":{"id":"c4O9VtV1yKlr"}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"id":"Fhd_N1k3FdyR","outputId":"d4b6bf1c-77bd-40db-b8a9-c5b1d700c571","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{"id":"nQgnYnAbGshh"}},{"cell_type":"markdown","source":"Hyperparameters","metadata":{"id":"vboIzPiEGyLN"}},{"cell_type":"code","source":"n_estimators = 200\nrandom_state = 2021\ncriterion = 'gini'\n\nmax_depth = 3\nmax_leaf_nodes = None\nmin_samples_split = 2\n\nmax_features = 'sqrt'\n\nRFC = RandomForestClassifier(n_estimators= n_estimators,  random_state= random_state, criterion=criterion, max_features = max_features, max_depth = max_depth, max_leaf_nodes=max_leaf_nodes, min_samples_split=min_samples_split)","metadata":{"id":"jUYGuoWNGwwy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators = [ 100, 150, 200]\ncriterion = ['gini', 'entropy']\nrandom_state = 2021\nmax_depth = range(1, 5)\nmax_features = ['sqrt', 'log2']\ngrid = {'n_estimators': n_estimators,'criterion':criterion, 'max_depth': max_depth, 'max_features':max_features}\n\ngridSearch = GridSearchCV(RFC, grid, scoring='accuracy', cv=k_fold, refit=True)\ngridSearch.fit(X_train, y_train)\nresults = gridSearch.cv_results_\n\nprint('Best accuracy obtained:', gridSearch.best_score_)\nprint('n_estimators value for the best case:', gridSearch.best_estimator_.n_estimators)\nprint('criterion value for the best case:', gridSearch.best_estimator_.criterion)\nprint('max_depth value for the best case:', gridSearch.best_estimator_.max_depth)\nprint('max_features value for the best case:', gridSearch.best_estimator_.max_features)\n\n","metadata":{"id":"ArTR1dzPqqfJ","outputId":"8125baf4-8cde-48d9-96d9-658b816f0164","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Parameters to the values from GridSearch\nRFC.set_params(n_estimators = gridSearch.best_estimator_.n_estimators, criterion = gridSearch.best_estimator_.criterion, max_depth = gridSearch.best_estimator_.max_depth, max_features  =  gridSearch.best_estimator_.max_features)","metadata":{"id":"vktFA6LCMOoV","outputId":"f7ae327b-7574-48ac-8fbd-d994e673e09c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate Hyperparameters with CV","metadata":{"id":"xdt-TFdGJCa5"}},{"cell_type":"code","source":"#Storing Scores\ntrainingScores = []\ncvScores = []\n\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n  #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n\n  #Fitting Model\n  RFC.fit(X_train_fold, Y_train_fold)\n\n  #Checking how good the model is on trainingsdata\n  accuracy_score_Training = accuracy_score(Y_train_fold,RFC.predict(X_train_fold))\n  print('--------------------------------------------------------')\n  print('Training accuracy_score: ', accuracy_score_Training)\n  #Checking how good the model is on cv data\n  accuracy_score_Test = accuracy_score(Y_cv_fold,RFC.predict(X_cv_fold))\n  print('cv accuracy_score: ', accuracy_score_cv)\n \n  trainingScores.append(accuracy_score_Training)\n  cvScores.append(accuracy_score_cv)\n\nprint('--------------------------------------------------------')\nprint('--------------------------------------------------------')\nmean_accuracy_score_training = np.array(trainingScores).mean()\nprint('Mean Accuracy_score Training:', mean_accuracy_score_training )\nprint('--------------------------------------------------------')\nmean_accuracy_score_cv = np.array(cvScores).mean()\nprint('Mean Accuracy_score cv:', mean_accuracy_score_cv )\n","metadata":{"id":"UEGMwzGPJGTc","outputId":"990e02e0-0040-44ae-c01b-959e9da68cd7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train this model with training and test data set","metadata":{"id":"abPPQ6SsJw1C"}},{"cell_type":"code","source":"RFC.fit(X_train, y_train)","metadata":{"id":"WpkK32aZJrEe","outputId":"4e17808e-f9eb-4b2b-d46b-e3eaa409914e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict and Accuracy on trainings data\ny_pred_train_proba_rf = logReg.predict_proba(X_train)\ny_pred_train_proba_rf = pd.DataFrame(data = y_pred_train_proba_rf, index = X_train.index)\n\n\ny_train_preds_rf = logReg.predict(X_train)\naccuracy_score_training_ges_rf = accuracy_score(y_train,y_train_preds_rf)","metadata":{"id":"jvO-5e0qaw5q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict and Accuracy on test data\ny_pred_proba_rf = RFC.predict_proba(X_test)\ny_pred_proba_rf = pd.DataFrame(data = y_pred_proba_rf, index = X_test.index)\n\ny_preds_rf = RFC.predict(X_test)\naccuracy_test_ges_rf = accuracy_score(y_test,y_preds_rf)","metadata":{"id":"uMIQq1W0J6Vf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy for training and test set: ","metadata":{"id":"xIJE1NjrM7fu"}},{"cell_type":"code","source":"print('--------------------------------------------------------')\nprint('accuracy_score whole trainings set', accuracy_score_training_ges_rf )\nprint('--------------------------------------------------------')\nprint('accuracy_score whole test set', accuracy_test_ges_rf )\nprint('--------------------------------------------------------')","metadata":{"id":"Q6vbdrPVM-1R","outputId":"8f11dfb1-e0c7-4e35-c55b-69fba84b2581","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Results","metadata":{"id":"F_nS6sYmK2nG"}},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{"id":"0dvya6tXKTGm"}},{"cell_type":"code","source":"cm2 = confusion_matrix(y_test,y_preds_rf)\n#storing false negatives\nfn_rf = cm2[1,0]","metadata":{"id":"YfzF9vaL8IBa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(RFC,X_test,y_test,cmap='Blues')","metadata":{"id":"s3-6kREXKRqE","outputId":"c0f59bab-7ff7-4b1e-ce18-c278f8cf7f34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Recall Precision Curve","metadata":{"id":"rjQDmEUuKhoV"}},{"cell_type":"code","source":"preds = pd.concat([y_test,y_pred_proba_rf.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))","metadata":{"id":"KQ3BJk8mKeFE","outputId":"41d68325-c639-4f49-862b-bbab0d3e4b20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC-Curve","metadata":{"id":"JdqCJh1NKplo"}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"id":"BbstZQDrKnWe","outputId":"972e5356-0029-44f3-e8d9-1c9a28ed1504","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K Nearest Neighbors","metadata":{"id":"yqiZ-XxG_odL"}},{"cell_type":"code","source":"n_neighbors = 13\nweights = 'uniform'\nalgorithm = 'auto'\nknn = KNeighborsClassifier(n_neighbors =  n_neighbors, algorithm = algorithm, weights = weights)","metadata":{"id":"6EdB8pggB-MY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_neighbors = np.arange(1,30, 2)\nweights = ['uniform', 'distance']\nalgorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\ngrid = {'n_neighbors': n_neighbors,'weights':weights, 'algorithm': algorithm}\n\ngridSearch = GridSearchCV(knn, grid, scoring='accuracy', cv=k_fold, refit=True)\ngridSearch.fit(X_train, y_train)\nresults = gridSearch.cv_results_\n\nprint('Best accuracy obtained:', gridSearch.best_score_)\nprint('n_neighbors value for the best case:', gridSearch.best_estimator_.n_neighbors)\nprint('weights value for the best case:', gridSearch.best_estimator_.weights)\nprint('algorithm for the best case:', gridSearch.best_estimator_.algorithm)","metadata":{"id":"YgWPO8jX_9j5","outputId":"0096bc05-5012-4960-e3ff-477c655de1c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Parameters to the values from GridSearch\nknn.set_params(n_neighbors = gridSearch.best_estimator_.n_neighbors,  weights = gridSearch.best_estimator_.weights , algorithm = gridSearch.best_estimator_.algorithm )","metadata":{"id":"722dDQjmNXib","outputId":"0c3ee4da-89cd-4fe3-9592-7e56cc9dbd41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingScores = []\ncvScores = []\n\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n  #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n\n  #Fitting Model\n  knn.fit(X_train_fold, Y_train_fold)\n\n  #Checking how good the model is on trainingsdata\n  accuracy_score_Training = accuracy_score(Y_train_fold,knn.predict(X_train_fold))\n  print('--------------------------------------------------------')\n  print('Training accuracy_score: ', accuracy_score_Training)\n  #Checking how good the model is on cv data\n  accuracy_score_cv = accuracy_score(Y_cv_fold,knn.predict(X_cv_fold))\n  print('CV accuracy_score: ', accuracy_score_cv)\n  trainingScores.append(accuracy_score_Training)\n  cvScores.append(accuracy_score_cv)\n\nprint('--------------------------------------------------------')\nprint('--------------------------------------------------------')\ngesamt_accuracy_score_training = np.array(trainingScores).mean()\nprint('Mean accuracy_score Training:', gesamt_accuracy_score_training )\nprint('--------------------------------------------------------')\nmean_accuracy_score_cv = np.array(cvScores).mean()\nprint('Mean accuracy_score CV:', mean_accuracy_score_cv )","metadata":{"id":"NNqEJThrB7f4","outputId":"d2c92920-7fc9-451e-e00f-2b557fb82949","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and test model with training and test set.","metadata":{"id":"Uho0c7K1Nyis"}},{"cell_type":"code","source":"knn.fit(X_train, y_train)","metadata":{"id":"BkXBW9HlEyUz","outputId":"41c0b35c-dbbd-4573-de90-205bb4ee8805","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction and Accuracy on trainings data\ny_pred_train_proba_knn = knn.predict_proba(X_train)\ny_pred_train_proba_knn = pd.DataFrame(data = y_pred_train_proba_knn, index = X_train.index)\n#print(y_preds)\n\ny_train_preds_knn = knn.predict(X_train)\naccuracy_training_ges_knn = accuracy_score(y_train,y_train_preds_knn)","metadata":{"id":"xV4E3ycPE6Sh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction and Accuracy on test data\ny_pred_proba_knn = knn.predict_proba(X_test)\ny_pred_proba_knn = pd.DataFrame(data = y_pred_proba_knn, index = X_test.index)\n#print(y_preds)\n\ny_preds_knn = knn.predict(X_test)\naccuracy_test_ges_knn = accuracy_score(y_test,y_preds_knn)","metadata":{"id":"NdIQ63PHFCrb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy on training und test set:","metadata":{"id":"5Wnuod0NN4wb"}},{"cell_type":"code","source":"print('--------------------------------------------------------')\nprint('accuracy_score whole trainings set', accuracy_training_ges_knn )\nprint('--------------------------------------------------------')\nprint('accuracy_score whole test set', accuracy_test_ges_knn )\nprint('--------------------------------------------------------')","metadata":{"id":"BZU7Jjy6N8Hy","outputId":"4e7ff22a-fbba-44d1-e266-1920eaa63c1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{"id":"FrnV3BIOODyE"}},{"cell_type":"code","source":"cm3 = confusion_matrix(y_test,y_preds_knn)\n#storing false negatives\nfn_knn = cm3[1,0]","metadata":{"id":"VJHtIs7p75Ri","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(knn,X_test,y_test,cmap='Blues')","metadata":{"id":"FHuomCgTFMhq","outputId":"547e01bf-6c0a-477a-db5f-b58080c4ef52","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Recall Precision Curve","metadata":{"id":"l-VOKC2nOHZz"}},{"cell_type":"code","source":"preds = pd.concat([y_test,y_pred_proba_knn.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))","metadata":{"id":"CHDlVl1ZFPpR","outputId":"3aa696be-9ed6-416e-dde2-779d009ba7d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC-Curve","metadata":{"id":"Z7nhAaheOSwj"}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"id":"GjGhLwQ_FS6q","outputId":"cc4d5861-3ba7-47bf-b2bd-6312c50ceaa6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network\n\n\nFirst, I started with an NN with 2 layers and 32 neurons each and an output layer with one neuron.  As activation function I used **relu** for the first two layers and **sigmoid** for the Output Layer. Since it is a binary classification problem I decided to use **binary_crossentropy** as loss. And **accuracy** I used as metric. As optimizer I took **adam**.","metadata":{"id":"g0SYwbwFUgZn"}},{"cell_type":"code","source":"def build_model():\n  #Sequential API\n  model = models.Sequential()\n  #Defining the first hidden layer:\n  model.add(layers.Dense(units = 32, activation='relu', input_shape=(X_train.shape[1],)))\n  model.add(layers.Dense(units = 32, activation='relu'))\n  #Sigmoid for values between 0 and 1 (good for binary classification).\n  model.add(layers.Dense(units = 1,activation='sigmoid'))\n\n\n  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"id":"NVfE51m2LA6M","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking into model structure:\nbuild_model().summary()","metadata":{"id":"CJjlAMbiXddm","outputId":"24493111-12f3-4c48-eaf3-cac0d5ec475d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get started i trained the model with 200 epochs.","metadata":{"id":"hMTNL0zyTXsr"}},{"cell_type":"code","source":"#Model Training\n#--------------------------------------------------------------------------\n#Hyperparameters\nnum_epochs = 200\n\nbatch_size = 10","metadata":{"id":"cnRuVylZXfzM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lists for storing scores\ntrainingScores = []\ncvScores = []\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n  #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n  #Building the model\n  model = build_model()\n  #Fitting Model\n  #model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, verbose=0)\n  history =  model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, validation_data=(X_cv_fold, Y_cv_fold) ,verbose=0)\n\n  \n  #Evaluating the training performance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_train_fold, Y_train_fold, verbose=0)\n  trainingScores.append(val_binary_crossentropy)\n  print('--------------------------------------------------------')\n  print('Training accuracy: ', val_accuracy)\n\n  #Evaluating the CV performance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_cv_fold, Y_cv_fold, verbose=0)\n  cvScores.append(val_binary_crossentropy)\n  print('CV accuracy: ', val_accuracy)","metadata":{"id":"sI6jzzV2tQTN","outputId":"f8b0876c-b8ac-440e-d4d0-5aa6599930ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training accuracy here is at 1 and CV accuracy well below. The network is overfitted. To confirm this, I look at the loss in the learning curve. This also shows strong overfitting.","metadata":{"id":"Vtr2OuhtAT8_"}},{"cell_type":"code","source":"def plot_learning_curves(history):\n  #We will omit the first 10 points for a better visualization:\n  plt.plot(history.epoch,history.loss, \"k--\", linewidth=1.5, label=\"Training\")\n  plt.plot(history.epoch,history.val_loss, \"b-.\", linewidth=1.5, label=\"CV test\")\n  plt.legend()\n  plt.ylim(0.,1,10)\n  #plt.yscale(\"log\")\n  plt.xlabel(\"Epochs\"),  plt.ylabel(\"loss\")","metadata":{"id":"tVjOjU1xbv-e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = pd.DataFrame(history.history)\n#Adding epoch column:\nhist['epoch'] = history.epoch\n# As you can see, we have the losses as well as mae for both training and CV data:\n#hist.sample(3)","metadata":{"id":"e7T9-4I1btrr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curves(hist)","metadata":{"id":"MEZ75NFobx7u","outputId":"38c96376-e3a9-4531-fe47-fb8a775306f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = history.model","metadata":{"id":"kv6XYrq1xMqC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_binary_crossentropy_training, val_accuracy_total_training = model.evaluate(X_train, y_train, verbose=0)\nprint('--------------------------------------------------------')\nprint('accuracy for the entire trainings dataset: ', val_accuracy_total_training)\nprint('--------------------------------------------------------')","metadata":{"id":"GocZoQSyv8U3","outputId":"6485ab09-e65b-4de5-8dc2-30d391434329","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_binary_crossentropy_test, val_accuracy_total_test = model.evaluate(X_test, y_test, verbose=0)\nprint('--------------------------------------------------------')\nprint('accuracy for the entire test dataset: ', val_accuracy_total_test)\nprint('--------------------------------------------------------')","metadata":{"id":"hv-SmvoTv6id","outputId":"619a9823-bf58-4690-b826-3172fd631e91","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = model.predict_classes(X_test)","metadata":{"id":"Mm7vBHOwwJRr","outputId":"7987bd41-80d4-4d9c-d1a2-803dad2c3be3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm1 = confusion_matrix(y_test,y_preds)","metadata":{"id":"XvNkEJyGwL0a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cm = pd.DataFrame(cm1, range(2), range(2))\n# plt.figure(figsize=(10,7))\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap = 'crest') # font size\n\nplt.show()\n\n","metadata":{"id":"NzUvRE7BdWiw","outputId":"ed59c9de-f756-4783-e031-4953edb67318","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine Tuning Parameters","metadata":{"id":"T-XJ7-9Ns4Dx"}},{"cell_type":"markdown","source":"First, I added a dropout layer between the two layers. This helped, but the network still overfits. Next, I reduced the number of neurons to 16 and added a second layer after the dropout layer. A 4th layer and a second dropout layer did not improve the accuracy of the model. Therefore I have commented them out. Additionally I added a kernel regularizer. As regularizers I tried **l1**, **l2**, and **l1_l2** and got the best results with **l2**. I did not change the loss, the metric and the solver.","metadata":{"id":"JR9GmTMByMMk"}},{"cell_type":"code","source":"def build_model():\n  #Sequential API\n  model = models.Sequential()\n  #Defining the first hidden layer:\n  model.add(layers.Dense(units = 16,  kernel_regularizer=regularizers.l2(0.001), kernel_initializer=\"he_uniform\", activation='relu', input_shape=(X_train.shape[1],)))\n # model.add(layers.Dense(units = 16,  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n # model.add(layers.Dropout(0.3))\n  model.add(layers.Dense(units = 16,  kernel_regularizer=regularizers.l2(0.001),kernel_initializer=\"he_uniform\", activation='relu'))\n  model.add(layers.Dropout(0.3))\n  model.add(layers.Dense(units = 16,  kernel_regularizer=regularizers.l2(0.001),kernel_initializer=\"he_uniform\",activation='relu'))\n  #Sigmoid for values between 0 and 1 (good for binary classification).\n  model.add(layers.Dense(units = 1,activation='sigmoid'))\n\n\n  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"id":"Bi_wVPoYM8HP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking into model structure:\nbuild_model().summary()","metadata":{"id":"uf77pKwH0OOH","outputId":"3bd23847-4ccf-4ff2-f587-aeedaaf948ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lists for storing scores\ntrainingScores = []\ncvScores = []\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n    #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n  #Building the model\n  model = build_model()\n  #Fitting Model\n  #model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, verbose=0)\n  history =  model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, validation_data=(X_cv_fold, Y_cv_fold) ,verbose=0)\n\n\n  #Evaluating the training pperformance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_train_fold, Y_train_fold, verbose=0)\n  trainingScores.append(val_binary_crossentropy)\n  print('--------------------------------------------------------')\n  print('Training accuracy: ', val_accuracy)\n\n  #Evaluating the CV pperformance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_cv_fold, Y_cv_fold, verbose=0)\n  cvScores.append(val_binary_crossentropy)\n  print('CV accuracy: ', val_accuracy)\n","metadata":{"id":"Vx-qFrH5NW1r","outputId":"82003c26-f169-4160-ef92-769e602623c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = pd.DataFrame(history.history)\n#Adding epoch column:\nhist['epoch'] = history.epoch","metadata":{"id":"6hZC8y_BNbY5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curves(hist)","metadata":{"id":"FLnikEdQOXjP","outputId":"e0605516-6125-44c0-c325-82f3b6bb3d97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the learning curve shows the Model still overfits. To avoid overfitting, I have added early stopping. The training is stopped if no improvement has taken place over further epochs. To get the best model from cross validation I put in a checkpoint.","metadata":{"id":"d6nQ0Kut2hky"}},{"cell_type":"code","source":"myCheckpoint= keras.callbacks.ModelCheckpoint(\"my_best_model1.h5\", save_best_only=True)\nmyEarly_stopping = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)","metadata":{"id":"WzogwjmSgon_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run model with checkpoint and early stopping\n#Lists for storing scores\n\ntrainingScores = []\ncvScores = []\n\nfor train_index, cv_index in k_fold.split(X_train,y_train):\n\n    #Filtering data based on indices\n  X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n  Y_train_fold, Y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n  #Building the model\n  model = build_model()\n  #Fitting Model\n  #model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, verbose=0)\n  history =  model.fit(X_train_fold, Y_train_fold, epochs=num_epochs, batch_size=batch_size, validation_data=(X_cv_fold, Y_cv_fold) ,callbacks=[myCheckpoint,myEarly_stopping],verbose=0)\n\n\n  #Evaluating the training pperformance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_train_fold, Y_train_fold, verbose=0)\n  trainingScores.append(val_binary_crossentropy)\n  print('--------------------------------------------------------')\n  print('Training accuracy: ', val_accuracy)\n\n  #Evaluating the CV pperformance:\n  val_binary_crossentropy, val_accuracy = model.evaluate(X_cv_fold, Y_cv_fold, verbose=0)\n  cvScores.append(val_binary_crossentropy)\n  print('CV accuracy: ', val_accuracy)\n\n#Load best model from Checkpoint\nmodel = keras.models.load_model(\"my_best_model1.h5\")","metadata":{"id":"g7Qa4CJEX1vo","outputId":"51449806-e03c-4c71-dad5-1231dd15eaf6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_binary_crossentropy_test, val_accuracy_total_test = model.evaluate(X_test, y_test, verbose=0)\nprint('--------------------------------------------------------')\nprint('accuracy for the entire test dataset: ', val_accuracy_total_test)\nprint('--------------------------------------------------------')","metadata":{"id":"PT_JOmu79LTc","outputId":"5ff886c9-d46f-4023-d87a-26242fdd1f50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = model.predict_classes(X_test)\n","metadata":{"id":"AALFaz8T9na0","outputId":"44910bda-807a-4516-ab4a-4148f8640a6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm2 = confusion_matrix(y_test,y_preds)\n#storing false negatives\nfn_nn = cm2[1,0]","metadata":{"id":"C-JOBCGn6IAW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndf_cm = pd.DataFrame(cm2, range(2), range(2))\n# plt.figure(figsize=(10,7))\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},  cmap = 'crest') # font size\n\nplt.show()\n\n","metadata":{"id":"ihkmfeqtcX75","outputId":"315a8196-4702-448b-aa2d-e0929a94b7bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary \n\nThe neural network has the potential to have the greatest accuracy. Depending on how the weights are initialized, the model has an accuracy between 81% and 86%. The false positives are the lowest in the best NN-case.","metadata":{"id":"CqeD4Igf6OkB"}},{"cell_type":"markdown","source":"#6. Evaluation of the model predictions","metadata":{"id":"vhr-w9x52VFB"}},{"cell_type":"markdown","source":"If you look at the accuracies, you can see that all 4 models have about the same accuracy. This is about 84%. To judge the models I look at the number of false negatives declared. Since this is about detecting heart disease, the worst case is when the patient has heart disease but it is not detected. In this case, Logistic Regression and Neural Network are best. With the neural network it depends, as said above, on the initilized weights. In my experiments, the neural network classified between 6 in the best case and 12 in the worst case false negatives (from 30% Testdata size).","metadata":{"id":"pcskOZzhBe8K"}},{"cell_type":"code","source":"#Storing all scores \nscores = [accuracy_test_ges_lg, accuracy_test_ges_rf, accuracy_test_ges_knn, val_accuracy_total_test]\nfales_negatives_count = [fn_lg, fn_rf, fn_knn,fn_nn]\nalgorithms = ['Logisitc Regression', 'Random Forests',  'K-Nearest Neighbors', 'Neural Network']","metadata":{"id":"YmID_c9V2lXz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,8)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")\n\nsns.barplot(x = algorithms, y = scores)\nprint('Accuracies: logReg:' ,scores[0], 'Random Forest:' ,scores[1],  'K-Nearest-Neighbors:' ,scores[2],  'Neural Network:' ,scores[3])","metadata":{"id":"u-Fjqljh4EeW","outputId":"73234158-cf87-4ac5-808c-984b4c6d707b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,8)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Count of false negatives\")\n\nsns.barplot(x = algorithms, y = fales_negatives_count)","metadata":{"id":"nB4kw4D28uW6","outputId":"be8f34ab-e530-43df-c58a-5f0a971e92d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#7. Lessons Learnt and Conclusions\n\n* The classification whether heart disease is present or not can be classified with an accuracy around 84%.\n* Baseline models predict less false positives than false negatives. The other way around would be better.\n*   With none of the models it is possible to achieve an accuracy significantly above 85%. Since all models have similar accuracies and I have not found any better on Kaggle, this is probably the best possible result.\n*   I think that the size of the data set is not enough to get even better results.\n* Since we are dealing with human measurements and subjective values like chest pain, it is hard to get good results when the data set may be inconsistent. \n* After some research, I still found out that other attributes are also important in the development of heart disease. For example, smoking, obesity, stress and alcohol consumption. These data would certainly also be helpful in the detection.\n* In the beginning, I hypothesized that the high cholesterol and blood pressure have an influence on heart disease. This data set did not show that.\n\n\n\n","metadata":{"id":"n5FrBFVH4T1t"}}]}