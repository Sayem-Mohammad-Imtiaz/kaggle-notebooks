{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# csv 파일 불러오기\ndef load_csv(path):\n    df = pd.read_csv(path)\n    return df\n\n# col 확인하기\ndef show_col(df):\n    for col in df.columns: \n        print(col)\n    return None\n\n# 결측값이 있는지 확인하기\ndef check_missed_data(df):\n    print(df.isnull().sum())\n    return None\n\n# 히스토그램 그리기\ndef hist(df):\n    df.hist(edgecolor='black', linewidth=1.2)\n    fig = plt.gcf()\n#     fig.set_size_inches(12,10)\n    fig.set_size_inches(24,20)\n    plt.show()\n    return None\n\n# boxplot('owner', 'height')\ndef boxplot(df, a, b):\n    f, sub = plt.subplots(1, 1,figsize=(8,6))\n    sns.boxplot(x=df[a],y=df[b], ax=sub)\n    sub.set(xlabel=a, ylabel=b);\n    return None\n    \n# violin_plot(data_f, 'owner', 'leaf_length')\ndef violin_plot(df, _x, _y):\n    plt.figure(figsize=(8,6))\n    plt.subplot(1,1,1)\n    sns.violinplot(x=_x,y=_y,data=df)\n    return None\n    \n# plot_3d('day', 'leaf_length', 'leaf_width')\ndef plot_3d(a, b, c):\n    from mpl_toolkits.mplot3d import Axes3D\n\n    fig=plt.figure(figsize=(12,8))\n\n    ax=fig.add_subplot(1,1,1, projection=\"3d\")\n    ax.scatter(data_f[a],data_f[b],data_f[c],c=\"blue\",alpha=.5)\n    ax.set(xlabel=a,ylabel=b,zlabel=c)\n    return None\n\n# heatmap(df, ['day', 'height', 'leaf_width', 'leaf_length', 'owner'])\n#     plt.figure(figsize=(12,8))\ndef heatmap(df, cols):\n    plt.figure(figsize=(36,24))\n    sns.heatmap(df[cols].corr(),annot=False)\n\n# 문자로 된 값 숫자로 인코딩하기(라벨 인코딩)\ndef label2value(df, col):\n    #Labeling the object datas\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder=LabelEncoder()\n    for dataset in [df]:\n        dataset.loc[:,col]=labelencoder.fit_transform(dataset.loc[:,col].values)\n    print(col,' : ', labelencoder.classes_) # 어떤 값이 어떤 값으로 매칭되어 인코딩이 어떻게 되었는지 확인하기\n    return None    \n\n# 원-핫 인코딩\ndef one_hot_encoding(df, cols):\n    data_f_with_dummies = df\n    data_f_with_dummies = pd.get_dummies(data_f_with_dummies, columns = cols)\n    return data_f_with_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f = load_csv('/kaggle/input/mushroom-classification/mushrooms.csv')\n\ndata_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_col(data_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 숫자값으로 인코딩해주기\n\nfor col in data_f.columns:\n    label2value(data_f,col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist(data_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# veil-type 제거하기(모든 값이 'p'로 결과에 미치는 영향 없음.)\n\ndata_f.drop(['veil-type'], axis='columns', inplace=True)\n\nshow_col(data_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap(data_f,list(data_f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 원핫인코딩\ndata_f2 = one_hot_encoding(data_f, list(data_f))\n\ndata_f2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_col(data_f2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class_0 과 class_1 은 한 가지가 나머지 속성의 값을 표현할 수 있음.\ndata_f2.drop(['class_1'], axis='columns', inplace=True)\nshow_col(data_f2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap(data_f2,list(data_f2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 다양한 예측 알고리즘 패키지를 임포트함.              \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef run_4_regressors(a, b, c, d):\n    # [1] 결정트리 예측기 머신러닝 알고리즘 학습\n    gildong = DecisionTreeRegressor(random_state = 0)\n    gildong.fit(train_X, train_y) #학습용 문제, 학습용 정답  \n    score1 = gildong.score(test_X, test_y) # 시험 문제, 시험 정답\n    # score의 의미: 정확하게 예측하면 1, 평균으로 예측하면 0, 더 못 예측하면 음수  \n\n    # [2] 랜덤 포레스트 예측기 머신러닝 알고리즘\n    youngja = RandomForestRegressor(n_estimators=28,random_state=0)\n    youngja.fit(train_X, train_y)\n    score2 = youngja.score(test_X, test_y)\n\n    # [3] K근접이웃 예측기 머신러닝 알고리즘\n    cheolsu = KNeighborsRegressor(n_neighbors=2)\n    cheolsu.fit(train_X, train_y)\n    score3 = cheolsu.score(test_X, test_y)\n\n    # [4] 선형회귀 머신러닝 알고리즘\n    minsu = LinearRegression()\n    minsu.fit(train_X, train_y)\n    score4 = minsu.score(test_X, test_y)\n\n    plt.plot(['DT','RF','K-NN','LR'], [score1, score2, score3, score4])\n    print('스코어: {0:.2f}, {1:.2f}, {2:.2f}, {3:.2f}'.format(score1, score2, score3, score4))\n\n\n# 다양한 분류 알고리즘 패키지를 임포트함.\nfrom sklearn.linear_model import LogisticRegression  # Logistic Regression 알고리즘\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n\ndef run_4_classifiers(train_X, train_y, test_X, test_y):\n    gildong = svm.SVC() # 애기 \n    gildong.fit(train_X,train_y) # 가르친 후\n    prediction = gildong.predict(test_X) # 테스트\n    rate1 = metrics.accuracy_score(prediction,test_y) * 100\n\n    cheolsu = LogisticRegression()\n    cheolsu.fit(train_X,train_y)\n    prediction = cheolsu.predict(test_X)\n    rate2 = metrics.accuracy_score(prediction,test_y) * 100\n\n    youngja = DecisionTreeClassifier()\n    youngja.fit(train_X,train_y)\n    prediction = youngja.predict(test_X) # 테스트\n    rate3 = metrics.accuracy_score(prediction,test_y) * 100\n\n    minsu = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\n    minsu.fit(train_X,train_y)\n    prediction = minsu.predict(test_X) # 테스트\n    rate4 = metrics.accuracy_score(prediction,test_y) * 100\n\n    plt.plot(['SVM','Logistic','DTree','K-NN'], [rate1, rate2, rate3, rate4])\n    print('인식률: {0:.2f}, {1:.2f}, {2:.2f}, {3:.2f}'.format(rate1, rate2, rate3, rate4))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_4_parts(df, li, dap_col):\n    # 학습용(문제, 정답), 테스트용(문제, 정답)으로 데이터 나누기\n    train, test = train_test_split(df, train_size = 0.8)\n\n\n    # 학습용 문제와 정답\n    a = train[li]\n    b = train[dap_col]\n\n    # 시험 문제와 정답\n    c = test[li]\n    d = test[dap_col]\n\n    return a, b, c, d\n\ndef split_4_parts2(df, li, dap_col):\n    # 학습용(문제, 정답), 테스트용(문제, 정답)으로 데이터 나누기\n    train, test = train_test_split(df, train_size = 0.2)\n\n\n    # 학습용 문제와 정답\n    a = train[li]\n    b = train[dap_col]\n\n    # 시험 문제와 정답\n    c = test[li]\n    d = test[dap_col]\n\n    return a, b, c, d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_y, test_X, test_y = split_4_parts(data_f2, list(data_f2)[1:], ['class_0'])\n\nprint(train_X)\nprint(train_y)\nprint(test_X)\nprint(test_y)\n\ntrain_X2, train_y2, test_X2, test_y2 = split_4_parts2(data_f2, list(data_f2)[1:], ['class_0'])\n\nprint(train_X2)\nprint(train_y2)\nprint(test_X2)\nprint(test_y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_4_classifiers(train_X, train_y, test_X, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_4_classifiers(train_X2, train_y2, test_X2, test_y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}