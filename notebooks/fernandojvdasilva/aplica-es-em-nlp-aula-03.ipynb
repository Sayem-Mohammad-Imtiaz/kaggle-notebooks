{"cells":[{"metadata":{"_uuid":"d59e1714618222ce70d6049a2280bca3e71a7b38"},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_uuid":"a89f11e17dec224a567a99fe4d45d2ee9f52c196"},"cell_type":"markdown","source":"<h1 align=\"center\"> Aplicações em Processamento de Linguagem Natural </h1>\n<h2 align=\"center\"> Aula 03 - Técnicas de Pré-Processamento de Texto e Similaridade</h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.</h3>","execution_count":null},{"metadata":{"_uuid":"402e01a1ec45619f8553d15a85ba73d099e93e19"},"cell_type":"markdown","source":"<h2> Técnicas para Pré-Processamento - Parte 2</h2>","execution_count":null},{"metadata":{"_uuid":"e70ddf470536e6f629a2e5bac309e1296a881238"},"cell_type":"markdown","source":"<p>Uma vez que o texto já foi devidamente tratado, removendo stopwords e pontuações, e aplicando stemming ou lemmatization, agora precisamos contar a frequência das palavras (ou n-grams) que utilizaremos em seguida como atributos para as técnicas de aprendizado de máquina.</p>","execution_count":null},{"metadata":{"_uuid":"32726855db9b2f9adec59f35af9062eb7a6fd0b3"},"cell_type":"markdown","source":"<b>1. TF-IDF (Term Frequency - Inverse Document Frequency)</b>","execution_count":null},{"metadata":{"_uuid":"ec5fa0cecd2471b52b43a81e26626a7d9d172b94"},"cell_type":"markdown","source":"<p><b>Term Frequency:</b> um termo que aparece muito em um documento, tende a ser um termo importante. Em resumo, divide-se o número de vezes em que um termo apareceu pelo maior número de vezes em que algum outro termo apareceu no documento.</p>\n\ntf<sub>wd</sub> = f<sub>wd</sub> / m<sub>wd</sub>\n\nonde:<br>\nf<sub>wd</sub> é o número de vezes em que o termo <i>w</i> aparece no documento <i>d</i>.<br>\nm<sub>wd</sub> é o maior valor de f<sub>wd</sub> obtido para algum termo do documento <i>d</i><br>\n\n<p><b>Inverse Document Frequency:</b> um termo que aparece em poucos documentos pode ser um bom descriminante. Obtem-se dividindo o número de documentos pelo número de documentos em que o termo aparece.</p>\n\nidf<sub>w</sub> = log<sub>2</sub>(n / n<sub>w</sub>)\n\nonde:<br>\nn é o número de documentos no corpus\nn<sub>w</sub> é o número de documentos em que o termo <i>w</i> aparece.\n\nNa prática, usa-se:\n\ntf-idf = tf<sub>wd</sub> * idf<sub>w</sub>\n\nPodemos calcular o TF-IDF de um corpus usando o pacote <b>scikit-learn</b>. Primeiramente, vamos abrir novamente o texto de Hamlet e armazenar as sentenças em uma ndarray do numpy (como se cada sentença fosse um documento do corpus):","execution_count":null},{"metadata":{"trusted":true,"_uuid":"33063b14e207a18933105ced5c4dfc40d2905fa5"},"cell_type":"code","source":"import nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\n\nhamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n\nsents = sent_tokenize(hamlet_raw)\n\nhamlet_np = np.array(sents)\n\nprint(hamlet_np.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaf4eabeb753a08246129fbb94435da7e34b966d"},"cell_type":"markdown","source":"<p>Agora vamos definir uma função para tokenização pelo scikit-learn.</p>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"b54f728cb8a002a4601d17e940720501eb44a000"},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b61e5963b929b8c62b1caa94b29f497a3e453c2"},"cell_type":"markdown","source":"E essa função será chamada pelo objeto TfidfVectorizer","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ef6bcf9f1bb0448308dccf8a3a36e1789bce1a9b"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nhamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n\nsents = sent_tokenize(hamlet_raw)\n\nhamlet_np = np.array(sents)\n\ntfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\ntfs = tfidf_vectorizer.fit_transform(hamlet_np)\n\nprint(tfs.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6a407bd2272880203b2ce42a74e6818d3d18c3a5"},"cell_type":"code","source":"print([k for k in tfidf_vectorizer.vocabulary_.keys()][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8b7f5724b7e641cd5a0656c7ea77dc40d6e7dae"},"cell_type":"code","source":"print(tfs[:50,:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pprint\nprint(tfs[0,:60].toarray())\nprint(tfs[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in tfidf_vectorizer.vocabulary_.items():\n    print(item)\n    if item[1] == 17:\n        print(item[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfs[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b891d5d8811a6149e10d816c5214f26c8672e7b1"},"cell_type":"markdown","source":"<b>2. TF-IDF de N-gramas</b>","execution_count":null},{"metadata":{"_uuid":"515cfc8aedda7741940d22adbdf6169654a94139"},"cell_type":"markdown","source":"Opcionalmente, podemos obter os atributos tf-idf de n-grams, combinando as classes CountVectorizer e TfidfTransformer. Em nosso exemplo, vamos utilizar apenas trigramas:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6e48fff7b4ce29a920bae36331361214b5eb2e5d"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ncount_vect = CountVectorizer(ngram_range=(3,3))\n\nn_gram_counts = count_vect.fit_transform(hamlet_np)\n\ntfidf_transformer = TfidfTransformer()\n\ntfs_ngrams = tfidf_transformer.fit_transform(n_gram_counts)\n\nprint(tfs_ngrams.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in count_vect.vocabulary_.items():\n    print(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfs_ngrams[0,:17].toarray())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1269c71145cd2f3ce22961fc945bf9761646249c"},"cell_type":"markdown","source":"<b>3. Redução de Dimensionalidade</b>","execution_count":null},{"metadata":{"collapsed":true,"_uuid":"7fab659cec6d0f4b34a6a2dd9b1aacee2a46100b"},"cell_type":"markdown","source":"<p>A transformação do corpus em atributos contendo as frequências TF-IDF em geral resultará numa ndarray bastante esparsa, ou seja, com muitas dimensões. Porém, além de isso tornar o treinamento de algoritmos mais demorado e custoso (computacionalmente falando), muitas dessas dimensões provavelmente são pouco representativas ou mesmo podem causar ruído durante o treinamento. Para resolver esse problema, podemos aplicar uma técnica de redução de dimensionalidade simples chamada <b>Singular Value Decomposition (SVD)</b>. \n\n<p>Essa técnica transformará os vetores da matriz original, rotacionando e escalando-os, resultando em novas representações. A redução de dimensionalidade é feita ao manter apenas as <i>k</i> dimensões mais representativas que escolhermos. Outra vantagem dessa técnica é que as dimensões originais são, de certa forma, \"combinadas\", o que resulta em uma nova forma de representar a combinação de termos. No contexto de PLN, essa técnica é conhecida como <b>Latent Semantic Analysis (LSA)</b></p>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"25fe4a4a7f1b74b4835c3d270612e533ffef9d20"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd_transformer = TruncatedSVD(n_components=1000)\n\nsvd_transformer.fit(tfs)\n\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1][:30])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c1f5922d00b2ba009a2be6138222c55fd48fd06"},"cell_type":"markdown","source":"<p>Agora vamos manter as dimensões até que a variância acumulada seja maior ou igual a 0.50.</p>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"5887fbc34a108d6af7522f44c36b26087d1c090d"},"cell_type":"code","source":"cummulative_variance = 0.0\nk = 0\nfor var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n    cummulative_variance += var\n    if cummulative_variance >= 0.70:\n        break\n    else:\n        k += 1\n        \nprint(k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d5ec027d93824e292e763918cd6c2b419315b34"},"cell_type":"markdown","source":"<p>Transformarmos novamente, mas desta vez com o número de k componentes que obtemos anteriormente.</p>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"2b97f42f08b4cf665647883836e392ae65627786"},"cell_type":"code","source":"svd_transformer = TruncatedSVD(n_components=k)\nsvd_data = svd_transformer.fit_transform(tfs)\nprint(sorted(svd_transformer.explained_variance_ratio_)[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0269f8c3eb95595a7c56cfe6211f70c849c80332"},"cell_type":"code","source":"print(svd_data.shape)\nprint(svd_data)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"cc989f8172e55bde24f72daa5cbe67c1998dec43"},"cell_type":"markdown","source":"<h2> Similaridade de Cosseno</h2>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ec435da433c7fcc0dd52bd295370c6c454acb1e3"},"cell_type":"markdown","source":"<p>Uma vez que dois documentos são representados como vetores numéricos, é possível comparar a similaridade entre os documentos ao calcular o cosseno do ângulo entre esses documentos - uma medida de distância, não de amplitude. Para isso, basta resolver a equação do produto escalar entre os vetores, para encontrar o cosseno.</p>\n\n<img src=\"http://s0.wp.com/latex.php?latex=++%5Cdisplaystyle++%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D+%3D+%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%5Ccos%7B%5Ctheta%7D+%5C%5C+%5C%5C++%5Ccos%7B%5Ctheta%7D+%3D+%5Cfrac%7B%5Cvec%7Ba%7D+%5Ccdot+%5Cvec%7Bb%7D%7D%7B%5C%7C%5Cvec%7Ba%7D%5C%7C%5C%7C%5Cvec%7Bb%7D%5C%7C%7D++&amp;bg=ffffff&amp;fg=000000&amp;s=0\" alt=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" title=\"  \\displaystyle  \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos{\\theta} \\\\ \\\\  \\cos{\\theta} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}  \" class=\"latex\">","execution_count":null},{"metadata":{"trusted":true,"_uuid":"bdf38b4d45935b61f2d3c75b3460623ffd64a490"},"cell_type":"markdown","source":"<img class=\" wp-image-2582 \" title=\"vector_space\" src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/vector_space.png\" alt=\"\" width=\"504\" height=\"378\">\n<p fontsize=8>Fonte: http://blog.christianperone.com</p>","execution_count":null},{"metadata":{"trusted":true,"_uuid":"be6d306294f78eb9ccedda02351df5298c45761c"},"cell_type":"code","source":"    documents = (\n    \"The sky is blue\",\n    \"The sun is bright\",\n    \"The sun in the sky is bright\",\n    \"We can see the shining sun, the bright sun\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d31152584e3abd32b862fcc961c911d3e59122d8"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\nprint(tfidf_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41abb374235f50b25623af7cd9284ed7167a45d6"},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nquery_vect = tfidf_vectorizer.transform([\"The sun is red in the sky\"])\n#query_vect = tfidf_vectorizer.transform([\"The sun in the sky is bright\"])\n\ncosine_similarity(query_vect, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"944cb088e94e6fefe9ada5fd2983496a873a9f39"},"cell_type":"markdown","source":"<p><b>Exercício 3:</b>Escreva um chatbot que, dado uma pergunta em Inglês, encontre uma pergunta mais parecida no corpus de perguntas e respostas disponível no Kaggle (https://www.kaggle.com/rtatman/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a pergunta original e a resposta.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c21902ee0a762a1d233e5697a4c66045164a9433"},"cell_type":"code","source":"qa_file = open('../input/S08_question_answer_pairs.txt', 'r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_file.read()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}