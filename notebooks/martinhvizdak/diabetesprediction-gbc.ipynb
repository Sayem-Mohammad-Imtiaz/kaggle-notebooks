{"cells":[{"metadata":{},"cell_type":"markdown","source":"# More complex and accurate voting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data in this dataset is 100% complete and very easy to work with, therefore it doesn't need any modification\n# There also aren't any categorical data\n# We were working with a dataset with 768 entries, which can be enough considering there are no missing entries\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the pairplot we've suprisingly found out that there almost isn't a correlation between age and other values(apart of the pregnancies as to be  expected)\n# Unsuprisingly the skin thickness and BMI levels are depending(with increasing weight) as well as rising glucose levels are forcing body to produce more insulin\n# From the observation, insulin levels are decreasing much more with the number of pregnancies as with age\n\n# It's important to remove the outliers that will be missleading the ML models\n\nsns.pairplot(data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the outliers, that have impossible values, with mean or median can lead to more accurate models as well as better overall analytics \n\nfor column in df:\n    count = 0\n    for i in df[column]:\n        if(i == 0):\n            count += 1\n    print(f'{column}: {count}') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The columns where we need to get rid of the outliers are:\n# Glucose, BloodPressure, SkinThickness, Insulin and BMI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision to choose the median or the mean can be of very importance as it can improve the model \n\nfor column in df:\n    print(f'{column}:')\n    print(\"Max = \", end=\"\")\n    print(df[column].max())\n    print(\"Min = \", end=\"\")\n    print(df[column].min())\n    print(\"Mean = \", end=\"\")\n    print(df[column].mean())\n    print(\"Median = \", end=\"\")\n    print(df[column].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We've decided to use the mean for all of the values as it will be more accurate when training the models\n\ndf.loc[(df.Glucose==0)& (df.Outcome==0), 'Glucose']=int(df[(df.Outcome==0)]['Glucose'].mean())\ndf.loc[(df.Glucose==0)& (df.Outcome==1), 'Glucose']=int(df[(df.Outcome==1)]['Glucose'].mean())\n\ndf.loc[(df.BloodPressure==0)& (df.Outcome==0), 'BloodPressure']=int(df[(df.Outcome==0)]['BloodPressure'].mean())\ndf.loc[(df.BloodPressure==0)& (df.Outcome==1), 'BloodPressure']=int(df[(df.Outcome==1)]['BloodPressure'].mean())\n\ndf.loc[(df.SkinThickness<5)& (df.Outcome==0), 'SkinThickness']=int(df[(df.Outcome==0)]['SkinThickness'].mean())\ndf.loc[(df.SkinThickness<5)& (df.Outcome==1), 'SkinThickness']=int(df[(df.Outcome==1)]['SkinThickness'].mean())\n\ndf.loc[(df.Insulin==0)& (df.Outcome==0), 'Insulin']=int(df[(df.Outcome==0)]['Insulin'].mean())\ndf.loc[(df.Insulin==0)& (df.Outcome==1), 'Insulin']=int(df[(df.Outcome==1)]['Insulin'].mean())\n\ndf.loc[(df.BMI==0)& (df.Outcome==0), 'BMI']=int(df[(df.Outcome==0)]['BMI'].mean())\ndf.loc[(df.BMI==0)& (df.Outcome==1), 'BMI']=int(df[(df.Outcome==1)]['BMI'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a really high propability of having diabetes with higher amount of pregnancies(starting with 5)\n# For glucose, the safe limit seems to be around 100, after this level the probability for having diabetes rapidly increases\n# There doesn't seem to be a safe level of blood pressure, but people with lower blood pressure tend to have diabetes less\n# The same goes for the skin thickness\n# As for insulin the range is large(hitting every value) but around 70, the probabylity for diabetes rises\n# Statistically, BMI has an impact on the probability of diabetes, but the as the increasing weight doesn't mean increasing fat, the BMI is not that much valid(apart from the higher values), it is possible, that the FMI(Fat mass index) could provide a better results, but is harder to calculate\n#  As to be expected, the Pedigree function has a correlation with the probability of outcome\n# According to age, after 27, the probability of having diabetes rapidly increases\n\nfor column in df:\n    sns.violinplot(x=df.Outcome, y=df[column])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple model using Decision tree classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain, test = train_test_split(df)\n\nx = train[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]]\ny = train[\"Outcome\"]\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = test[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]]\ntestY = test[\"Outcome\"]\n\npredictions = clf.predict(testX)\n\nx = 0\nl = len(predictions)\n\nyList = []\n\nfor i in testY:\n    yList.append(i)\n\nfor i in range(l):\n    if(predictions[i] == yList[i]):\n        x += 1\nacc = x/l*100\n\nprint(\"Accuracy:\", f\"{acc}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More accurate and complex voting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\ntree_clf = DecisionTreeClassifier()\nknn_clf= KNeighborsClassifier()\nbgc_clf=BaggingClassifier()\ngbc_clf=GradientBoostingClassifier()\nabc_clf= AdaBoostClassifier()\n\nx = train[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]]\ny = train[\"Outcome\"]\n\n\nvoting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('tree', tree_clf),('knn', knn_clf),('bg', bgc_clf), ('gbc', gbc_clf),('abc', abc_clf)],voting='hard')\n\nvoting_clf.fit(x, y)\n\nfor clf in  (log_clf, rnd_clf, svm_clf,tree_clf,knn_clf,bgc_clf,gbc_clf,abc_clf,voting_clf):\n    clf.fit(x,y)\n    predictions = clf.predict(testX)\n    print(clf.__class__.__name__, accuracy_score(testY, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the best accuracy was shown on the Gradient bosting classifier(even more than the voting classifier) after multiple runs, we've decided to use this ML model in the end\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier(random_state=0)\ngbc.fit(x, y)\npred=gbc.predict(testX)\nprint(\"Accuracy for GradientBoosting data: \",gbc.score(testX, testY))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}