{"cells":[{"metadata":{"id":"R8trrfMNp_N8"},"cell_type":"markdown","source":"#Using a dataset from [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) to predict breast cancer diagnosis\n\n## What will this notebook cover?\nThis notebook will have EDA peppered in and a Random Forest as well as a KNN model.\n\n## Goal of the project\nMy main objective is to learn by doing.  I want to expand my skill set and I believe Random Forests and KNN models are two of the classic models to build.\n\n\n#Data Dictionary\n\n| Column | Meaning |\n| ------ | ------- |\n| ID | The unique identifier for each person |\n| Diagnosis | The diagnosis of breast tissue | \n| Radius_mean | Mean of distances from center to points on the perimeter | \n| Texture_mean | Standard deviation of grey-scale-values | \n| Perimeter_mean | Mean size of the core tumor | \n| Area_mean | The mean of the area | \n| Smoothness_mean | Mean of local variation in radius lengths | \n| Compactness_mean | Mean of perimeter<sup>2</sup> | \n| Concavity_mean | Mean of severity of concavr portions of the contour | \n| Concave points | Mean for number of concave portions of the contour | \n| Concave_points_mean | | \n| Fractal_dimension_mean | | \n| Radius_se | \n| Texture | The texture of the tumor | \n| Perimeter_se | | \n| Area_se | | \n| Smoothness_se | | \n| Compactness_se | | \n| Concavity_se | | \n| Concave points_se | | \n| Symmetry_se | | \n| Fractual_dimensions_se | | \n| Radius_worst | | \n| Texture_worst | | \n| Perimeters_wosrt | | \n| Area_worst | | \n| Smoothness | |\n| Compactness_worst | |\n| Concavity_worst | |\n| Concave points_worst | |\n| Symmetry_worst | |\n| Fractal_dimension_worst | |\n| Unname: 32 | |"},{"metadata":{"id":"IG8-75siwxw1"},"cell_type":"markdown","source":"#Let's import the libarires"},{"metadata":{"id":"zJ54in9UpPwy","executionInfo":{"status":"ok","timestamp":1616173521283,"user_tz":240,"elapsed":377,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"51b7ab6c-66c2-4381-f770-d169e56bd8ea","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"7UD6FY_6xpg_","executionInfo":{"status":"ok","timestamp":1616168187855,"user_tz":240,"elapsed":29040,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"#Use pandas to take the uploaded file and make it into a useful dataframe\nos.chdir(\"/kaggle/input/breast-cancer-wisconsin-data\")\ndataset = pd.read_csv(\"data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_lcaf_Aox2Rm","executionInfo":{"status":"ok","timestamp":1616168187856,"user_tz":240,"elapsed":29035,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"c7ad26e8-0267-4eab-957e-1fb5e0e62b59","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"GUKV4q6_i3o6"},"cell_type":"markdown","source":"# Let's familize ourself with the data\n\nThis is a method I commonly use to find a lot of quick and import information about the dataset I am using. \n\nThe methods I use here are:\n`head()`, `info()`, `describe()`, `tail()`, `columns()`, `dtypes()`, `shape()`"},{"metadata":{"id":"VYB9FKegzEEl","executionInfo":{"status":"ok","timestamp":1616175404128,"user_tz":240,"elapsed":373,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def background_check(dataframe):\n  \"\"\"\n    Summary: This function give us a quick look at the key components of our data\n\n    Description: These are pandas functions I use all the time and I figured I may\n    as well make it all in one function.  This is typically in a utils file and imported\n    in order to be conveinent.\n\n    Paramters: Dataframe\n\n    Return: None\n  \"\"\"\n  print(\"#\" * 100)\n  print(dataframe.head())\n\n  print(\"#\" * 100)\n  print(dataframe.info())\n\n  print(\"#\" * 100)\n  print(dataframe.describe)\n\n  print(\"#\" * 100)\n  print(dataframe.tail())\n\n  print(\"#\" * 100)\n  print(dataframe.columns)\n\n  print(\"#\" * 100)\n  print(dataframe.dtypes)\n\n  print(\"#\" * 100)\n  print(dataframe.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"R2iJkRifj6KN","executionInfo":{"status":"ok","timestamp":1616175419029,"user_tz":240,"elapsed":585,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"5c25bc00-3b4c-405f-d6c7-4fc5cc96844c","trusted":true},"cell_type":"code","source":"#Call the background check\nbackground_check(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"RRCk3bel27fv"},"cell_type":"markdown","source":"> **Note!**: The only categorical column is the diagnosis which we can handle.  Let's check our value counts to see how much we have of each.\n\n> **Note!**: The other value counts do not help as much since there are a lot of different values so I removed them for the sake of keeping this notebook clean"},{"metadata":{"id":"hQxG4xUM3coL","executionInfo":{"status":"ok","timestamp":1616175458663,"user_tz":240,"elapsed":343,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"7eace8a8-9ae2-4656-d83a-fd1eaa92bdf1","trusted":true},"cell_type":"code","source":"dataset.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"VPKrWePM4PTg"},"cell_type":"markdown","source":"What can we tell from this?  We can see we have 357 benign and 212 malignant.  This is an imbalance but it is okay for now."},{"metadata":{"id":"bei-VgGZ4mXZ"},"cell_type":"markdown","source":"# It is time to visualize this data!\n\nWhat type of graphs should we make?\n\nHow about we use these:\n- Correlations (to help see which columns influence the other)\n- Count plots (to help visualize the value counts)\n- Distplots (to help see the data distribution and see if we have outliers)\n- Violin plots (to help see the data distribution in a different light)\n\nThere are others we could try: pair plots, bar charts, pie charts, etc., but let's keep it simple!\n\nAs Andrej Karpathy said, \"Become one with the data.\"  Let's go"},{"metadata":{"id":"SwfLygcynIAC"},"cell_type":"markdown","source":"# Correlations and Heatmaps\n\nWhy correlations and heatmaps?\n\nWell, a correlation and heatmap are a great way to familiarize ourselves with our data.  We can see based on the color and the numbers what columns have a meaning too us and which do not."},{"metadata":{"id":"T7kkaG6838R9","executionInfo":{"status":"ok","timestamp":1616175905814,"user_tz":240,"elapsed":345,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def correlations(dataframe):\n  \"\"\"\n  Summary: This will plot the correlations in a heatmap\n\n  Description: This function will plot the correlations in a heatmap format for us.\n  We have about 30 columns so it will be a big graph.  Let's take it step by step\n\n  Parameters:  A dataframe\n\n  Return: None\n  \"\"\"\n\n  #We need to drop the useless Unnamed 32 column\n  dataframe = dataframe.drop([\"Unnamed: 32\"], axis = 1)\n\n  #Create the correlation and the cmap\n  corr = dataframe.corr(method = \"pearson\")\n  cmap = sns.diverging_palette(230, 20, as_cmap = True) #Set a nice color pattern for us to see\n\n  #This is our figure we will put the heatmap on\n  fig, ax = plt.subplots(figsize = (20, 15))\n\n  #This is the actual heatmap making with the arguments specifically chosen.  There are comments next to each to see what it does\n  sns_corr = sns.heatmap(corr,                                   #The data to correlate\n                         annot = True,                           #The numbers in the boxes\n                         fmt = \".1g\",                            #A formating option for the numbers in the boxes\n                         vmin = -1,                              #Take a glance at the right and see the bar?  We are adjusting that minimum dark blue value\n                         vmax = 1,                               #This is the same as before but the deep red\n                         center = 0,                             #This is the central value for the color.  Think of this as neutral\n                         cmap = \"coolwarm\",                      #The color scheme of the heatmap\n                         linecolor = \"black\",                    #The dark line clearly sepearting the value\n                         linewidths = 3,                         #The thickness of the black line\n                         cbar_kws = {\"orientation\": \"vertical\"}) #The bar on the right is set to vertical\n\n  #The column names are very close so make the rotation 90 degress to have them fit.  I am sorry - you have to turn your head to see them all.\n  sns_corr.set_xticklabels(sns_corr.get_xticklabels(), rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"id":"cI1TKJ506bhp","executionInfo":{"status":"ok","timestamp":1616175911292,"user_tz":240,"elapsed":5063,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"39daf6fd-96ba-4d91-e680-34abbac6f319","trusted":true},"cell_type":"code","source":"correlations(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"pLjqKrHv86WO"},"cell_type":"markdown","source":"This is a lot to take in.  We can take our time going through to develop our own intuition about the data.  Let's keep going!"},{"metadata":{"id":"W7wrSuImnFVM"},"cell_type":"markdown","source":"# Countplots\n\nWhy countplots\n\nWell, a countplot can show us the value counts method as a graph.  I used the value counts above and I wanted to use this since I learn more with the help of visuals and doing."},{"metadata":{"id":"L03wfcl-6dHz","executionInfo":{"status":"ok","timestamp":1616176138797,"user_tz":240,"elapsed":371,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def countplots(dataframe):\n  \"\"\"\n  Summary:  This will show us the countplot of the diagnosis\n\n  Description:  This shows us the countplot of only one column but I am trying to\n  modularize my code so I put it in its own method\n\n  Parameters: Dataframe\n\n  Return: None\n  \"\"\"\n  ax = sns.countplot(x = \"diagnosis\", data = dataframe)","execution_count":null,"outputs":[]},{"metadata":{"id":"uaOz5GhUCnLG","executionInfo":{"status":"ok","timestamp":1616176139671,"user_tz":240,"elapsed":450,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"171b08b9-b4da-4f03-aa4e-e3f0c9d80d76","trusted":true},"cell_type":"code","source":"countplots(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"HL0IzIx6ELT0"},"cell_type":"markdown","source":"This can visualize the value counts for us.  We can easily see our class imbalance now"},{"metadata":{"id":"JqcbfRV7nCcz"},"cell_type":"markdown","source":"# Distplots\n\nWhy distplots?\n\nDistplots can help us see the distribution of the data.  We can easily see what needs to be standardized."},{"metadata":{"id":"o4uhe-JBCpgl","executionInfo":{"status":"ok","timestamp":1616168193265,"user_tz":240,"elapsed":34383,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def distplots(dataframe):\n  \"\"\"\n  Summary: This will make a distribution plot for all of the columns\n\n  Description: This will make distribution plots for all of the data.  This will \n  help us see what needs to be standardized and normalized.\n\n  Parameters:  Dataframe\n\n  Return: None\n  \"\"\"\n  fig, ax = plt.subplots(nrows = 10, ncols = 3, figsize = (30, 30))\n\n  id_distplot = sns.distplot(dataframe.id, ax = ax[0][0])\n  radius_mean_distplot = sns.distplot(dataframe.radius_mean, ax = ax[0][1])\n  texture_mean_distplot = sns.distplot(dataframe.texture_mean, ax = ax[0][2])\n  perimeter_mean_distplot = sns.distplot(dataframe.perimeter_mean, ax = ax[1][0])\n  area_mean_distplot = sns.distplot(dataframe.area_mean, ax = ax [1][1])\n  smoothness_mean_distplot = sns.distplot(dataframe.smoothness_mean, ax = ax[1][2])\n  compactness_mean_distplot = sns.distplot(dataframe.compactness_mean, ax = ax[2][0])\n  concavity_mean_distplot = sns.distplot(dataframe.concavity_mean, ax = ax[2][1])\n  concave_points_mean_distplot = sns.distplot(dataframe[\"concave points_mean\"], ax = ax[2][2])\n  symmetry_mean_distplot = sns.distplot(dataframe.symmetry_mean, ax = ax[3][0])\n  fractal_dimension_mean_distplot = sns.distplot(dataframe.fractal_dimension_mean, ax = ax[3][1])\n  radius_se_distplot = sns.distplot(dataframe.radius_se, ax = ax[3][2])\n  texture_se_distplot = sns.distplot(dataframe.texture_se, ax = ax[4][0])\n  perimeter_se_distplot = sns.distplot(dataframe.perimeter_se, ax = ax[4][1])\n  area_se_distplot = sns.distplot(dataframe.area_se, ax = ax[4][2])\n  smoothness_se_distplot = sns.distplot(dataframe.smoothness_se, ax = ax[5][0])\n  compactness_se_distplot = sns.distplot(dataframe.compactness_se, ax = ax[5][1])\n  concavity_se_distplot = sns.distplot(dataframe.concavity_se, ax = ax[5][2])\n  concave_points_se_distplot = sns.distplot(dataframe[\"concave points_se\"], ax = ax[6][0])\n  symmetry_se_distplot = sns.distplot(dataframe.symmetry_se, ax = ax[6][1])\n  fractal_dimensions_se_distplot = sns.distplot(dataframe.fractal_dimension_se, ax = ax[6][2])\n  radius_worst_distplot = sns.distplot(dataframe.radius_worst, ax = ax[7][0])\n  texture_worst_distplot = sns.distplot(dataframe.texture_worst, ax = ax[7][1])\n  perimeter_worst_distplot = sns.distplot(dataframe.perimeter_worst, ax = ax[7][2])\n  area_worst_distplot = sns.displot(dataframe.area_worst, ax = ax[7][0])\n  smoothness_worst_displot = sns.displot(dataframe.smoothness_worst, ax = ax[7][1])\n  compactness_worst_distplot = sns.distplot(dataframe.compactness_worst, ax = ax[7][2])\n  concavity_worst_distplot = sns.distplot(dataframe.concavity_worst, ax = ax[8][0])\n  concave_points_worst_distplot = sns.distplot(dataframe[\"concave points_worst\"], ax = ax[8][1])\n  symmetry_worst_distplot = sns.distplot(dataframe.symmetry_worst, ax = ax[8][2])\n  fractal_dimension_worst_distplot = sns.distplot(dataframe.fractal_dimension_worst, ax = ax[9][0])\n\n  plt.show()\n  fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"F3FAxe3jExRc","executionInfo":{"status":"ok","timestamp":1616168200994,"user_tz":240,"elapsed":42105,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"bdc5621d-9f95-4879-837d-8f2aed021622","trusted":false},"cell_type":"code","source":"distplots(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"tB0z6arDNhM_"},"cell_type":"markdown","source":"# What can we see\n\nThe distributions give us an idea of what needs to be normalzied and standardized.\n\nI am not sure why these last two are not in the subplots, however, if you know and want to let me know feel free to say it!"},{"metadata":{"id":"JYD70UNrn969"},"cell_type":"markdown","source":"# Violin plots\n\nWhy violin plots?\n\nViolin plots help us see what a distribution plot can show us but it is even easier to see the distributions (at least I think so)."},{"metadata":{"id":"m4CX5vv9oKAH","executionInfo":{"status":"ok","timestamp":1616177482148,"user_tz":240,"elapsed":585,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def violin_plots(dataframe):\n  \"\"\"\n  Summary: This will make violin plots for the data\n  \n  Description: This will make a bunch of violin plots for the data to help us visualize\n  \n  Parameters: Dataframe\n  \n  Return: None\n  \"\"\"\n  fig, ax = plt.subplots(nrows = 10, ncols = 3, figsize = (30, 20))\n\n  id_violin = sns.violinplot(x = dataframe.id, y = dataframe[\"diagnosis\"], ax = ax[0][0])\n  radius_mean_violin = sns.violinplot(x = dataframe[\"radius_mean\"], y = dataframe[\"diagnosis\"], ax = ax[0][1])\n  texture_mean_violin = sns.violinplot(x = dataframe.texture_mean, y = dataframe[\"diagnosis\"], ax = ax[0][2])\n  perimeter_mean_violin = sns.violinplot(x = dataframe.perimeter_mean, y = dataframe[\"diagnosis\"], ax = ax[1][0])\n  area_mean_violin = sns.violinplot(x = dataframe.area_mean, y = dataframe[\"diagnosis\"], ax = ax [1][1])\n  smoothness_mean_violin = sns.violinplot(x = dataframe.smoothness_mean, y = dataframe[\"diagnosis\"], ax = ax[1][2])\n  compactness_mean_violin = sns.violinplot(x = dataframe.compactness_mean, y = dataframe[\"diagnosis\"], ax = ax[2][0])\n  concavity_mean_violin = sns.violinplot(x = dataframe.concavity_mean, y = dataframe[\"diagnosis\"], ax = ax[2][1])\n  concave_points_mean_violin = sns.violinplot(x = dataframe[\"concave points_mean\"], y = dataframe[\"diagnosis\"], ax = ax[2][2])\n  symmetry_mean_violin = sns.violinplot(x = dataframe.symmetry_mean, y = dataframe[\"diagnosis\"], ax = ax[3][0])\n  fractal_dimension_mean_violin = sns.violinplot(x = dataframe.fractal_dimension_mean, y = dataframe[\"diagnosis\"], ax = ax[3][1])\n  radius_se_violin = sns.violinplot(x = dataframe.radius_se, y = dataframe[\"diagnosis\"], ax = ax[3][2])\n  texture_se_violin = sns.violinplot(x = dataframe.texture_se, y = dataframe[\"diagnosis\"],  ax = ax[4][0])\n  perimeter_se_violin = sns.violinplot(x = dataframe.perimeter_se, y = dataframe[\"diagnosis\"], ax = ax[4][1])\n  area_se_violin = sns.violinplot(x = dataframe.area_se, y = dataframe[\"diagnosis\"], ax = ax[4][2])\n  smoothness_se_violin = sns.violinplot(x = dataframe.smoothness_se, y = dataframe[\"diagnosis\"], ax = ax[5][0])\n  compactness_se_violin = sns.violinplot(x = dataframe.compactness_se, y = dataframe[\"diagnosis\"], ax = ax[5][1])\n  concavity_se_violin = sns.violinplot(x = dataframe.concavity_se, y = dataframe[\"diagnosis\"],  ax = ax[5][2])\n  concave_points_se_violin = sns.violinplot(x = dataframe[\"concave points_se\"], y = dataframe[\"diagnosis\"], ax = ax[6][0])\n  symmetry_se_violin = sns.violinplot(x = dataframe.symmetry_se, y = dataframe[\"diagnosis\"],  ax = ax[6][1])\n  fractal_dimensions_se_violin = sns.violinplot(x = dataframe.fractal_dimension_se, y = dataframe[\"diagnosis\"], ax = ax[6][2])\n  radius_worst_violin = sns.violinplot(x = dataframe.radius_worst, y = dataframe[\"diagnosis\"], ax = ax[7][0])\n  texture_worst_violin = sns.violinplot(x = dataframe.texture_worst, y = dataframe[\"diagnosis\"], ax = ax[7][1])\n  perimeter_worst_violin = sns.violinplot(x = dataframe.perimeter_worst, y = dataframe[\"diagnosis\"], ax = ax[7][2])\n  area_worst_violin = sns.violinplot(x = dataframe.area_worst, y = dataframe[\"diagnosis\"], ax = ax[7][0])\n  smoothness_worst_violin = sns.violinplot(x = dataframe.smoothness_worst, y = dataframe[\"diagnosis\"],  ax = ax[7][1])\n  concavity_worst_viloin = sns.violinplot(x = dataframe.concavity_worst, y = dataframe[\"diagnosis\"], ax = ax[8][0])\n  concave_points_worst_violin = sns.violinplot(x = dataframe[\"concave points_worst\"], y = dataframe[\"diagnosis\"], ax = ax[8][1])\n  symmetry_worst_violin = sns.violinplot(x = dataframe.symmetry_worst, y = dataframe[\"diagnosis\"],  ax = ax[8][2])\n  fractal_dimension_worst_violin = sns.violinplot(x = dataframe.fractal_dimension_worst, y = dataframe[\"diagnosis\"], ax = ax[9][0])\n\n  fig.tight_layout()\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"PUpuy3Hloo1x","executionInfo":{"status":"ok","timestamp":1616177487214,"user_tz":240,"elapsed":5437,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"2b69ffbc-7417-47f4-ba96-c511c062646d","trusted":true},"cell_type":"code","source":"violin_plots(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZVctNkVUUL5g"},"cell_type":"markdown","source":"# Models!\n\nLet's split the data then try a random forest and a clustering algorithm"},{"metadata":{"id":"UK4K72DjU2QP","executionInfo":{"status":"ok","timestamp":1616174899933,"user_tz":240,"elapsed":382,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"#Split the data\nX = dataset.drop([\"Unnamed: 32\", \"diagnosis\"], axis = 1)\ny = pd.get_dummies(dataset[\"diagnosis\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"Rp9-KoU7gT59"},"cell_type":"markdown","source":"# Random forest with no normalization\n\nLet's see how it does with no normalization and then we can experiment."},{"metadata":{"id":"oKdfHjHJQ1Tq","executionInfo":{"status":"ok","timestamp":1616177819402,"user_tz":240,"elapsed":462,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def random_forest_model():\n  \"\"\"\n  Summary: This will create the random forest\n\n  Description: This will create, fit, and evaluate the random forest.  We also print a\n  classification report to see the precision, recall, f1-score, and support\n\n  Parameters: None\n\n  Return: None\n  \"\"\"\n  try:\n    random_forest = RandomForestClassifier()\n    \n    random_forest.fit(X_train, y_train)\n    \n    random_forest_preds = random_forest.predict(X_test)\n    \n    print(\"Accuracy is: \", accuracy_score(y_test, random_forest_preds) * 100)\n    print(classification_report(y_test, random_forest_preds))\n  except: AttributeError","execution_count":null,"outputs":[]},{"metadata":{"id":"yEorEMRkRWc6","executionInfo":{"status":"ok","timestamp":1616177820229,"user_tz":240,"elapsed":594,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"82e378c9-6fa9-4b48-aedb-2f98eb604af5","trusted":true},"cell_type":"code","source":"random_forest_model()","execution_count":null,"outputs":[]},{"metadata":{"id":"v-h6A14atJu-"},"cell_type":"markdown","source":"# KNN with no normalization\n\nLet's see how this does with no normalization and then we can experiment"},{"metadata":{"id":"Xf7U5GPnVZLb","executionInfo":{"status":"ok","timestamp":1616177925169,"user_tz":240,"elapsed":475,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"trusted":true},"cell_type":"code","source":"def knn():\n  \"\"\"\n  Summary: This is the K Nearest Neighbor's example\n\n  Description: This is the creation, fitting, and evaluating of the KNN model\n\n  Parameters: None\n\n  Return: None\n  \"\"\"\n  neighbors = KNeighborsClassifier(n_neighbors=3)\n  neighbors.fit(X_train, y_train)\n  neighbors_preds = neighbors.predict(X_test)\n\n  print(\"Accuracy is: \", accuracy_score(y_test, neighbors_preds) * 100)\n  print(classification_report(y_test, neighbors_preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"sMI7JsNVXC5o","executionInfo":{"status":"ok","timestamp":1616177925581,"user_tz":240,"elapsed":457,"user":{"displayName":"Alex Antaki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjlfMxpV7WDxIeySkiRiCNfJjOsmE3GHZf9Pbx-=s64","userId":"08201606946902658363"}},"outputId":"da747151-7630-4245-dac4-19fb7d9684a8","trusted":true},"cell_type":"code","source":"knn()","execution_count":null,"outputs":[]},{"metadata":{"id":"EGdMF5KcthxZ"},"cell_type":"markdown","source":"# What can we see from this?\n\nThe random forest is the winner.  I used all defaults too and it is in the high 90s.  It is not even normalized.  The RF is the clear winner to the clustering algorithm.\n\nWhat can we do differnetly?  Well we can noramlize the data as a start...  We can also try a different algorithm: Logistic regression, SVM, etc.  We can mess with the hyperparameters of the models: more estimators of clustes, depth of the tree, etc\n\nI am going to put this on my GitHub() with a README.md if you wish to view this on GitHub feel free to check it out"},{"metadata":{"id":"RmW7bhOttguu","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}