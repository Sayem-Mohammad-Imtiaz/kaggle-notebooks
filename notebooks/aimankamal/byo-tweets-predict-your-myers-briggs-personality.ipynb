{"cells":[{"metadata":{"_uuid":"d42dbe67549b6bb80d80d2ff8b613733f1c84d30","_cell_guid":"8eb608e3-4a08-4962-be7e-f53ea5aa1421"},"cell_type":"markdown","source":"# BYO Tweets and predict your Myers-Briggs Personality Type\n\n---\n\n**Why yet another kernel that explores the Myers-Briggs Personality Type Dataset?**\n\n- Exploring use of NLTK and XGBoost. \n- Practicing with the learnings from \"[XGBoost With Python Mini-Course](https://machinelearningmastery.com/xgboost-python-mini-course/)\".\n- Trying out to build a model for each type indicator individually.\n- And finally, predicting my own Myers-Briggs Personality Type.\n\n\n### Outline\n\n1. Data preview - distribution and correlation.\n1. Data preparation - process posts, vectorize with count and tf-idf.\n1. Apply steps of the \"[XGBoost With Python Mini-Course](https://machinelearningmastery.com/xgboost-python-mini-course/)\".\n1. Predict own Myers-Briggs Personality Type base on a few tweets and blog post. \n\n---\n**My playground:**\n[Utilizing-the-Kaggle-Python-Docker-Container-image](https://github.com/stefan-bergstein/Utilizing-the-Kaggle-Python-Docker-Container-image)\n\n---\n\nThis notebook is a fork from [Multiclass and multi-output classification](https://www.kaggle.com/depture/multiclass-and-multi-output-classification), [RNN mbti predictor](https://www.kaggle.com/prnvk05/rnn-mbti-predictor/) and [MBTI - Study personality\n](https://www.kaggle.com/laowingkin/mbti-study-personality).","execution_count":null},{"metadata":{"_uuid":"a0f7e95a8994cea779577c48c3d22cde3fdf2cc5","_cell_guid":"0e10c6de-685a-4e93-9925-7d73cb27497c","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read data\ndata = pd.read_csv('../input/mbti_1.csv') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840ae5aa20a5c29b983be8c45928862c299b9792","_cell_guid":"376de180-026c-46c1-8dfa-e2550edd0955"},"cell_type":"markdown","source":"## Data preview","execution_count":null},{"metadata":{"_uuid":"03a18534be1141ebff5875c48f7075995f23ca60","_cell_guid":"315f710c-371f-4c9d-87f0-13fbc0e29047","trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d145f588ec28c76d0c757d53e3b657e21c4e7a9","_cell_guid":"bda2db8c-c9c6-446b-a40e-f86241d60619"},"cell_type":"markdown","source":"### List of posts","execution_count":null},{"metadata":{"_uuid":"14c6ff28cd51d9969324fecb6a53b05ca5584069","_cell_guid":"05abe1d2-7acd-4f20-9539-be413c76822b","trusted":true},"cell_type":"code","source":"[p.split('|||') for p in data.head(2).posts.values]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79b9bb46f0aeba9d734a35bfdffedb7ad90ff51c","_cell_guid":"24c37003-e27e-4278-aa92-0884c595f1b3"},"cell_type":"markdown","source":"## Distribution of the MBTI personality types","execution_count":null},{"metadata":{"_uuid":"9ed4e6e21c4ff484425bb8ba73fa901253e78e2c","_cell_guid":"de987f70-aab9-45f0-958c-e40118a044e1","trusted":true},"cell_type":"code","source":"cnt_types = data['type'].value_counts()\n\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_types.index, cnt_types.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Types', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90bc9f8eade59e0b71f0a0601c58ac239dafc84b","_cell_guid":"f06f1a50-5d6b-4f57-b191-c65a1ec5e73b"},"cell_type":"markdown","source":"#### ... unbalanced occurrences. ","execution_count":null},{"metadata":{"_uuid":"f00d268818a332dd819e3eaa54cf1d8bd4523cb5","_cell_guid":"85d7b45f-d1bc-4d1e-a179-73bfe427fdd4"},"cell_type":"markdown","source":"### Add columns for the type Indicators","execution_count":null},{"metadata":{"_uuid":"56607eaad0911f4987c919ca9935203db90550ea","_cell_guid":"94fe02d5-753c-4a2c-bd10-dc3efb44ad80","trusted":true},"cell_type":"code","source":"def get_types(row):\n    t=row['type']\n\n    I = 0; N = 0\n    T = 0; J = 0\n    \n    if t[0] == 'I': I = 1\n    elif t[0] == 'E': I = 0\n    else: print('I-E incorrect')\n        \n    if t[1] == 'N': N = 1\n    elif t[1] == 'S': N = 0\n    else: print('N-S incorrect')\n        \n    if t[2] == 'T': T = 1\n    elif t[2] == 'F': T = 0\n    else: print('T-F incorrect')\n        \n    if t[3] == 'J': J = 1\n    elif t[3] == 'P': J = 0\n    else: print('J-P incorrect')\n    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n\ndata = data.join(data.apply (lambda row: get_types (row),axis=1))\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb604f55d0de8d78f8f8739673661eb2c9976303","_cell_guid":"f64d512c-d2b2-4494-832b-79c32eeba584","trusted":true},"cell_type":"code","source":"print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\nprint (\"Intuition (N) – Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\nprint (\"Thinking (T) – Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\nprint (\"Judging (J) – Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71300a00fb90466c0fe86e131d0f1dc158cb4765","_cell_guid":"106d4032-2cb6-424e-95af-ee73643c4ea9","trusted":true},"cell_type":"code","source":"N = 4\nbut = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\ntop = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n\nind = np.arange(N)    # the x locations for the groups\nwidth = 0.7      # the width of the bars: can also be len(x) sequence\n\np1 = plt.bar(ind, but, width)\np2 = plt.bar(ind, top, width, bottom=but)\n\nplt.ylabel('Count')\nplt.title('Distribution accoss types indicators')\nplt.xticks(ind, ('I/E',  'N/S', 'T/F', 'J/P',))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b015bb2f565b95c85bbcfbc57f9b1ed8d64c2d1","_cell_guid":"1cbe5f98-511c-4732-b7eb-058a17827726"},"cell_type":"markdown","source":"### Pearson Features Correlation","execution_count":null},{"metadata":{"_uuid":"f40a9b83141b40e8dd820045a5240d378851b3a2","_cell_guid":"abe4c46c-0f10-400d-ba5e-6e9a061d1a16","trusted":true},"cell_type":"code","source":"data[['IE','NS','TF','JP']].corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"119725c5f75ff48f02366b94c7c02f58e8e1ee6a","_cell_guid":"9609cbe7-3991-4253-9667-444e79465708"},"cell_type":"markdown","source":"\n\nUnclear if the matrix shows anything valuable for interpretation.","execution_count":null},{"metadata":{"_uuid":"5033669fbfadff1fd6efac9d5d35440e6fa29dd3","_cell_guid":"008a46e0-73dc-46ed-94af-0272a518fb65","trusted":true},"cell_type":"code","source":"cmap = plt.cm.RdBu\ncorr = data[['IE','NS','TF','JP']].corr()\nplt.figure(figsize=(12,10))\nplt.title('Pearson Features Correlation', size=15)\nsns.heatmap(corr, cmap=cmap,  annot=True, linewidths=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fa544cd379bc6524ee8c6158d08dd3ce9d6a7fd","_cell_guid":"2f84a821-2f1f-44eb-821e-a384b617abc2"},"cell_type":"markdown","source":"### Prep data","execution_count":null},{"metadata":{"_uuid":"1f25930f84536cbefb11a9c015e25e2bddf88468","_cell_guid":"cf133eb4-f85c-42f5-9820-91c70f130bbb"},"cell_type":"markdown","source":"Binarize Type Indicator (better implemenation than mine above)","execution_count":null},{"metadata":{"_uuid":"5c0185231d1dec0773ac8574677dec7a641d5409","_cell_guid":"8ab0a44e-70c7-4876-aa9c-fd6a552ecf94","trusted":true},"cell_type":"code","source":"b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    # transform mbti to binary vector\n    \n    return [b_Pers[l] for l in personality]\n\ndef translate_back(personality):\n    # transform binary vector to mbti personality\n    \n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\n# Check ...\nd = data.head(4)\nlist_personality_bin = np.array([translate_personality(p) for p in d.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d320caec234f9b36f1eb5221764f94b73ec608","_cell_guid":"0cb4844b-3205-4eed-9af5-949c79de3fcd"},"cell_type":"markdown","source":"### Preprocessing posts\n* Remove urls \n* Keep only words and put everything lowercase\n* Lemmatize each word \n* __Remove MBTI profiles strings. Too many appear in the posts!__","execution_count":null},{"metadata":{"_uuid":"77420dbbc4a8f7ff8984f342562b505571dfe677","_cell_guid":"e1421b79-1fa2-4a4a-bccf-e234223a9bd6","collapsed":true,"trusted":true},"cell_type":"code","source":"##### Compute list of subject with Type | list of comments \nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize\n\n# We want to remove these from the psosts\nunique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n  \nunique_type_list = [x.lower() for x in unique_type_list]\n\n\n# Lemmatize\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer()\n\n# Cache the stop words for speed \ncachedStopWords = stopwords.words(\"english\")\n\ndef pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n\n    list_personality = []\n    list_posts = []\n    len_data = len(data)\n    i=0\n    \n    for row in data.iterrows():\n        i+=1\n        if (i % 500 == 0 or i == 1 or i == len_data):\n            print(\"%s of %s rows\" % (i, len_data))\n\n        ##### Remove and clean comments\n        posts = row[1].posts\n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n        temp = re.sub(' +', ' ', temp).lower()\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n            \n        if remove_mbti_profiles:\n            for t in unique_type_list:\n                temp = temp.replace(t,\"\")\n\n        type_labelized = translate_personality(row[1].type)\n        list_personality.append(type_labelized)\n        list_posts.append(temp)\n\n    list_posts = np.array(list_posts)\n    list_personality = np.array(list_personality)\n    return list_posts, list_personality","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"501d30b844b84cd6e1fa54545b792183b73b0b51","_cell_guid":"44464e11-d570-4a70-8044-e4631917c89e","trusted":true},"cell_type":"code","source":"list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efa4ec02751f1fd1258b95050807780a61d0e3a8","_cell_guid":"5028348f-b571-4112-8045-1befb2a22001","trusted":true},"cell_type":"code","source":"print(\"Num posts and personalities: \",  list_posts.shape, list_personality.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4739e84cc06276fa69bdd4f34e160de36d57bcde","_cell_guid":"2ecc8222-0c70-43c6-87b6-d488eff5d5f2","trusted":true},"cell_type":"code","source":"list_posts[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f2a00a7e5f5fa837093b4d0fdf6dd05f6c61e7","_cell_guid":"4fc7a295-2381-4974-b0bc-6af5a5d7b047","trusted":true},"cell_type":"code","source":"list_personality[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e834d2d4594b7b9e200784033b15b94d02afb54","_cell_guid":"c4c7a734-bffc-44dd-81b8-865b7161cd53"},"cell_type":"markdown","source":"### Vectorize with count and tf-idf\n\nKeep words appearing in 10% to 70 % of the posts.","execution_count":null},{"metadata":{"_uuid":"86f08653912e5643b959751ea17b794e4d3cbcdc","_cell_guid":"fe473d3a-9f1d-48b0-8d97-0ff576ad4f10","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\n\n# Posts to a matrix of token counts\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1500, \n                             tokenizer=None,    \n                             preprocessor=None, \n                             stop_words=None,  \n                             max_df=0.7,\n                             min_df=0.1) \n\n# Learn the vocabulary dictionary and return term-document matrix\nprint(\"CountVectorizer...\")\nX_cnt = cntizer.fit_transform(list_posts)\n\n# Transform the count matrix to a normalized tf or tf-idf representation\ntfizer = TfidfTransformer()\n\nprint(\"Tf-idf...\")\n# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d072c06cb9c16e1e1f74215edaf5297a21a050","_cell_guid":"cf28feb1-2933-4226-9d5f-dcf81ec29a1f","trusted":true},"cell_type":"code","source":"feature_names = list(enumerate(cntizer.get_feature_names()))\nfeature_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d0c4c1f0d3c6ed93a29a30a34428888f108653d","_cell_guid":"789c0833-d2fe-4427-a05d-a5bb83d9239b","trusted":true},"cell_type":"code","source":"X_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f503cbcba4cc35a4efa1756befe38e541fbc705","_cell_guid":"e437c97c-1c4d-41bd-bd2c-53ae20f05a31"},"cell_type":"markdown","source":"## Train XGBoost classifiers\n\nPracticing \"XGBoost With Python Mini-Course\" by jason@machinelearningmastery.com\n\n[[1]](https://machinelearningmastery.com/xgboost-python-mini-course/) https://machinelearningmastery.com/xgboost-python-mini-course/","execution_count":null},{"metadata":{"_uuid":"d3573b9179fcad6e2efdde9663c983e4972cf613","_cell_guid":"cbc14fa8-056e-4892-abb9-3adfe7062d8e"},"cell_type":"markdown","source":"### X / Y data\n- X: Posts in tf-idf representation\n- Y: Binarized MBTI \n\nNote, here I trying out to build a model for each type indicator individually because I recall the multi-class / multi-out models in other notebooks weren't that accurate, right?\n\n","execution_count":null},{"metadata":{"_uuid":"f4f1ac0959bb779f16eec050cc4c9559d6ac4c0e","_cell_guid":"29a1d526-613b-4435-97ae-d268938cc157","trusted":true},"cell_type":"code","source":"print(\"X: Posts in tf-idf representation \\n* 1st row:\\n%s\" % X_tfidf[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8d7a87e8c469b24f53ce471196aef5dfe07a792","_cell_guid":"0833bf69-8f6c-4daf-915b-32b7402262f8","trusted":true},"cell_type":"code","source":"type_indicators = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) – Sensing (S)\", \n                   \"FT: Feeling (F) - Thinking (T)\", \"JP: Judging (J) – Perceiving (P)\"  ]\n\nfor l in range(len(type_indicators)):\n    print(type_indicators[l])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19586851e36b2606ea650ef657bd77dd114aa2a2","_cell_guid":"fa3c1056-06ae-4e75-ba98-b0456ca4b4c0","trusted":true},"cell_type":"code","source":"print(\"MBTI 1st row: %s\" % translate_back(list_personality[0,:]))\nprint(\"Y: Binarized MBTI 1st row: %s\" % list_personality[0,:])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d5d28276fbe18fe0eb02af1d00ebfddec43bb29","_cell_guid":"1ed7e296-7741-457c-89d6-091f7de0cb93"},"cell_type":"markdown","source":"## First XGBoost model for MBTI dataset\n","execution_count":null},{"metadata":{"_uuid":"76dd80d8b1ab630afde0a0a86b3fac5aa6c2c366","_cell_guid":"dbbed077-130a-4860-b0bd-80f328611bd4","trusted":true},"cell_type":"code","source":"# First XGBoost model for MBTI dataset\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Posts in tf-idf representation\nX = X_tfidf\n\n# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    # Let's train type indicator individually\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier()\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"* %s Accuracy: %.2f%%\" % (type_indicators[l], accuracy * 100.0))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac88eefd05911a98c832e78cbfcbc984b2a4fe8b","_cell_guid":"ff11f94b-a58a-4e6f-a106-7c7dc4065e79"},"cell_type":"markdown","source":"## Monitor Performance and Early Stopping\n\n*\"XGBoost model can evaluate and report on the performance on a test set for the model during training. It supports this capability by specifying both a test dataset and an evaluation metric on the call to model.fit() when training the model and specifying verbose output (verbose=True). For example, we can report on the binary classification error rate (error) on a standalone test set (eval_set) while training an XGBoost model.\" [[1]](https://machinelearningmastery.com/xgboost-python-mini-course/)*","execution_count":null},{"metadata":{"_uuid":"f73f66f97981081ce06a2b5ad4b9a5c0a438fe1e","_cell_guid":"19b328df-febe-446d-bcc9-fa6d90bae18e","trusted":true},"cell_type":"code","source":"# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier()\n    eval_set = [(X_test, y_test)]\n    model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"* %s Accuracy: %.2f%%\" % (type_indicators[l], accuracy * 100.0))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83cfd3b2eeab49cb22a7b6b2d3094728a1a54d82","_cell_guid":"2bd23c47-29d4-47cf-ad69-a368992f00c9"},"cell_type":"markdown","source":"## Feature Importance with XGBoost\n\n*\"A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model. A trained XGBoost model automatically calculates feature importance on your predictive modeling problem.\" [[1]](https://machinelearningmastery.com/xgboost-python-mini-course/)*\n\n#### Show feature importance plot and list for the first indicator:","execution_count":null},{"metadata":{"_uuid":"875cd02eb43ff88ffc5736dce112d2e429483705","_cell_guid":"47190dab-9c61-446e-be5d-d344eef60b7d","trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\n# Only the 1st indicator\ny = list_personality[:,0]\n# fit model on training data\nmodel = XGBClassifier()\nmodel.fit(X, y)\n# plot feature importance\nax = plot_importance(model, max_num_features=25)\n\nfig = ax.figure\nfig.set_size_inches(15, 20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03a460ac630d758cfa64fd2cb89ade4c3943f9ac","_cell_guid":"26acec59-e08a-4f0c-97cf-db45f0406fa0","trusted":true},"cell_type":"code","source":"features = sorted(list(enumerate(model.feature_importances_)), key=lambda x: x[1], reverse=True)\nfor f in features[0:25]:\n    print(\"%d\\t%f\\t%s\" % (f[0],f[1],cntizer.get_feature_names()[f[0]]))\n    \n# Save xgb_params for late discussuin\ndefault_get_xgb_params = model.get_xgb_params()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"850e3748060bd2c8c3da00866cdf7d4274caddba","_cell_guid":"9687d41c-15e2-42c9-95d1-079f7c077a77"},"cell_type":"markdown","source":"## How to Configure Gradient Boosting\n\n*\"A number of configuration heuristics were published in the original gradient boosting papers. \nThey can be summarized as:*\n\n- *Learning rate or shrinkage (learning_rate in XGBoost) should be set to 0.1 or lower, and smaller values will require the addition of more trees.*\n- *The depth of trees (tree_depth in XGBoost) should be configured in the range of 2-to-8, where not much benefit is seen with deeper trees.*\n- *Row sampling (subsample in XGBoost) should be configured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\" [[1]](https://machinelearningmastery.com/xgboost-python-mini-course/)*\n\nSee also [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n","execution_count":null},{"metadata":{"_uuid":"549735e9e451517c903973cda8e9d2c018822c70","_cell_guid":"310792d7-ce3b-4e7d-8dd7-76e5a03f14ca","trusted":true},"cell_type":"code","source":"# Save xgb_params for later discussuin\ndefault_get_xgb_params = model.get_xgb_params()\nprint (default_get_xgb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def plot_confusion_matrix(\n        cm, \n        classes,\n        normalize=False,\n        title=\"Confusion Matrix\",\n        cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    if normalize: \n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print('Normalized confusion matrix')\n    else:\n        print('Confusion matrix, without normalization')\n        \n    print(cm)\n    \n    thresh = cm.max() / 2\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment='center',\n                 color='white' if cm[i,j] > thresh else 'black')\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4d18a0728d1d5a88756838810569b9bc949ff51","_cell_guid":"cf65a6b7-cc0e-427e-985f-72f7cce6c939","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\nimport re\n\n# setup parameters for xgboost\nparam = {}\n\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\n# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"* %s Accuracy: %.2f%%\" % (type_indicators[l], accuracy * 100.0))\n    \n    #confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    cm_class = re.split(' / | – |: | - ', type_indicators[l])\n    plot_confusion_matrix(cm, [cm_class[1], cm_class[2]], title=\"Confusion Matrix\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9aa3cefd0fc2e27b66a276dec108eed7eae8449","_cell_guid":"d73069a6-5a53-402c-83ac-9fdad4673132"},"cell_type":"markdown","source":"## XGBoost Hyperparameter Tuning\n\n*\"The scikit-learn framework provides the capability to search combinations of parameters. This capability is provided in the GridSearchCV class and can be used to discover the best way to configure the model for top performance on your problem.*    \n\n*The parameters to consider tuning are:*\n- *The number and size of trees (n_estimators and max_depth).*\n- *The learning rate and number of trees (learning_rate and n_estimators).*\n- *The row and column subsampling rates (subsample, colsample_bytree and colsample_bylevel).\" [[1]](https://machinelearningmastery.com/xgboost-python-mini-course/)*\n    \nSee also [Introduction to gradient-boosted trees and XGBoost hyperparameters tuning](https://www.apprendimentoautomatico.it/en/introduction-to-gradient-boosted-trees-and-xgboost-hyperparameters-tuning-with-python/)    ","execution_count":null},{"metadata":{"_uuid":"bb69ead05063fe85c8187a32525cfd8180f3a413","_cell_guid":"166157f6-c857-4ca7-8202-524ccc79eae8","trusted":true},"cell_type":"code","source":"# Tune learning_rate\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# Posts in tf-idf representation\nX = X_tfidf\n\n# setup parameters for xgboost\nparam = {}\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\n\n# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    Y = list_personality[:,l]\n    model = XGBClassifier(**param)\n    # learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n    # param_grid = dict(learning_rate=learning_rate)\n    \n    print(\"hello\")\n    param_grid = {\n        'n_estimators' : [ 200, 300],\n        'learning_rate': [ 0.2, 0.3]\n        # 'learning_rate': [ 0.01, 0.1, 0.2, 0.3],\n        # 'max_depth': [2,3,4],\n    }\n    \n    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n    grid_result = grid_search.fit(X, Y)\n    # summarize results\n    print(\"* Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"* %f (%f) with: %r\" % (mean, stdev, param))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"479b0141e11762e3548f930a681de7cd11d8d362","_cell_guid":"9ca7571e-08f4-4cce-ba69-7b3365565298"},"cell_type":"markdown","source":"## Predict own Myers-Briggs Personality Type \n\nUsing a few tweets and blog post, let's try to predict my own Myers-Briggs Personality Type.\n\n1. Prep data\n2. Fit and predict the 4 type indicators\n3. Show result\n\n\n#### Prep data:","execution_count":null},{"metadata":{"_uuid":"efb774df6b5a342253f662189c1673c20813913a","_cell_guid":"6868eb62-4dea-4eb1-bba2-b97a893f0354","trusted":false,"collapsed":true},"cell_type":"code","source":"# A few few tweets and blog post\nmy_posts  = \"\"\"Getting started with data science and applying machine learning has never been as simple as it is now. There are many free and paid online tutorials and courses out there to help you to get started. I’ve recently started to learn, play, and work on Data Science & Machine Learning on Kaggle.com. In this brief post, I’d like to share my experience with the Kaggle Python Docker image, which simplifies the Data Scientist’s life.\nAwesome #AWS monitoring introduction.\nHPE Software (now @MicroFocusSW) won the platinum reader's choice #ITAWARDS 2017 in the new category #CloudMonitoring\nCertified as AWS Certified Solutions Architect \nHi, please have a look at my Udacity interview about online learning and machine learning,\nVery interesting to see the  lessons learnt during the HP Operations Orchestration to CloudSlang journey. http://bit.ly/1Xo41ci \nI came across a post on devopsdigest.com and need your input: “70% DevOps organizations Unhappy with DevOps Monitoring Tools”\nIn a similar investigation I found out that many DevOps organizations use several monitoring tools in parallel. Senu, Nagios, LogStach and SaaS offerings such as DataDog or SignalFX to name a few. However, one element is missing: Consolidation of alerts and status in a single pane of glass, which enables fast remediation of application and infrastructure uptime and performance issues.\nSure, there are commercial tools on the market for exactly this use case but these tools are not necessarily optimized for DevOps.\nSo, here my question to you: In your DevOps project, have you encountered that the lack of consolidation of alerts and status is a real issue? If yes, how did you approach the problem? Or is an ChatOps approach just right?\nYou will probably hear more and more about ChatOps - at conferences, DevOps meet-ups or simply from your co-worker at the coffee station. ChatOps is a term and concept coined by GitHub. It's about the conversation-driven development, automation, and operations.\nNow the question is: why and how would I, as an ops-focused engineer, implement and use ChatOps in my organization? The next question then is: How to include my tools into the chat conversation?\nLet’s begin by having a look at a use case. The Closed Looped Incidents Process (CLIP) can be rejuvenated with ChatOps. The work from the incident detection runs through monitoring until the resolution of issues in your application or infrastructure can be accelerated with improved, cross-team communication and collaboration.\nIn this blog post, I am going to describe and share my experience with deploying HP Operations Manager i 10.0 (OMi) on HP Helion Public Cloud. An Infrastructure as a Service platform such as HP Helion Public Cloud Compute is a great place to quickly spin-up a Linux server and install HP Operations Manager i for various use scenarios. An example of a good use case is monitoring workloads across public clouds such as AWS and Azure.\n\"\"\"\n\n# The type is just a dummy so that the data prep fucntion can be reused\nmydata = pd.DataFrame(data={'type': ['INFJ'], 'posts': [my_posts]})\n\nmy_posts, dummy  = pre_process_data(mydata, remove_stop_words=True)\n\nmy_X_cnt = cntizer.transform(my_posts)\nmy_X_tfidf =  tfizer.transform(my_X_cnt).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc8128f03e16622d6a340db9fb654b48d2dacef0","_cell_guid":"09a3383d-8e7d-4bad-9e22-e66251c1abe4"},"cell_type":"markdown","source":"#### Fit and predict the 4 type indicators:","execution_count":null},{"metadata":{"_uuid":"2a365ca5f617519491f2eb0b2d909b18e8758de8","_cell_guid":"8a75daf8-c835-473b-809f-5afba651d6f1","trusted":false,"collapsed":true},"cell_type":"code","source":"# setup parameters for xgboost\nparam = {}\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\nresult = []\n# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    \n    # make predictions for my  data\n    y_pred = model.predict(my_X_tfidf)\n    result.append(y_pred[0])\n    # print(\"* %s prediction: %s\" % (type_indicators[l], y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e27b7f6379e1246d68c8c6542327becca15f02be","_cell_guid":"f05c1173-e4a8-4260-afc4-a2ffe6df68ea","collapsed":true},"cell_type":"markdown","source":"#### Show result","execution_count":null},{"metadata":{"_uuid":"ea10e21f587244beff55e25d0986d8912e02ac36","_cell_guid":"2ffb4395-a9f2-408e-a7b4-d7d91281e51e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"The result is: \", translate_back(result))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6e8f8a5825e562b25b8cd6251027535ea691605","_cell_guid":"edc0a8bd-9f0a-414c-b13a-4fc777691167"},"cell_type":"markdown","source":"Wow, the result is very, very close to the real Myers-Briggs assessment that I did a few years back. Only one indicator is different. Any guess? ;-)\n","execution_count":null},{"metadata":{"_uuid":"c438fa213ace6947606d098e0f1b6aa6ab27e0cf","_cell_guid":"1a5c85fb-5cce-4220-943b-820bb741b531","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}