{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake and Real News","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Overview\n\nCan you use this data set to make an algorithm able to determine if an article is fake news or not ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Description\nFake.csv file contains a list of articles considered as \"fake\" news. True.csv contains a list of articles considered as \"real\" news. Both the files contain\n\n* The title of the article\n* The text of the article\n* The subject of the article\n* The date that this article was posted at","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Files\n\n* Fake.csv\n* True.csv","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## So let’s begin here…","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom string import punctuation\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will add a new column for both real and fake dataframe. This column will have 0 and 1. 1 for real news and 0 for fake news.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real['category']=1\nfake['category']=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will concatenate both the dataframe in a single dataframe and we will use this for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([real,fake])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.subject.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now concatenate Text, Title and Subject in Text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'] + \" \" + df['title'] + \" \" + df['subject']\ndel df['title']\ndel df['subject']\ndel df['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npnc = list(punctuation)\nstop.update(pnc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\ndef stem_text(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            word = stemmer.stem(i.strip())\n            final_text.append(word)\n    return \" \".join(final_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(stem_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting dataset in train set and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df['text'],df['category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(min_df=0,max_df=1,ngram_range=(1,2))\n\ncv_train = cv.fit_transform(X_train)\ncv_test = cv.transform(X_test)\n\nprint('Train shape: ',cv_train.shape)\nprint('Test shape: ',cv_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.fit(cv_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_nb = nb.predict(cv_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = metrics.accuracy_score(y_test, pred_nb)\nprint(\"Accuracy Score: \",score)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}