{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam Detection with NLTK (98.1% accuracy)"},{"metadata":{},"cell_type":"markdown","source":"# Overview\n1. Importing Libraries\n2. Reading the Dataset\n3. Exploratory Data Analysis (EDA)\n     - Mapping Labels\n     - Dropping Duplicates\n     - Adding \"length\" column\n     - Adding \"contain\" column\n4. Data Preprocessing\n     - Removing Punctuations & Digits\n     - Tokenization & Lower Case\n     - Removing Stopwords\n     - Lemmatization \n     - Merging Tokens\n     - Count Vectorization\n     - TFIDF\n5. Model Training\n    - Multinomial Naive Bayes \n    - Decision Trees\n    - Random Forest"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport seaborn as sns\nimport string \nimport re\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Reading the Dataset\nDataset has 3 empty columns (Unnamed: 2, Unnamed 3, Unnamed: 4}. Dropping those columns.\n\nRenaming v1 and v2 columns as 'label' and 'text' respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the dataset \n#dataset: https://www.kaggle.com/uciml/sms-spam-collection-data\nmsg=pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\nmsg.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nmsg.rename(columns={'v1':'label','v2':'text'},inplace=True)\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### Mapping Labels\nMapping ham to 0 and spam to 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#mapping ham=0 and spam=1\nfor i in msg.index:\n  if msg['label'][i]=='ham':\n    msg['label'][i]=0\n  else:\n    msg['label'][i]=1\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping Duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"#category count plot (count of spam and ham)\nsns.countplot(msg.label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data description grouped by labels \nmsg.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 4852 ham messages (4516 unique) and 747 spam messages (653 unique)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping duplicate rows\nmsg=msg.drop_duplicates()\nmsg.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding \"length\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding length column to the dataset \nmsg['length']=msg['text'].apply(len)\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg[msg.label==0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=msg[msg['label']==0].length,kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg[msg.label==1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=msg[msg['label']==1].length,kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above outputs and graphs, we notice that\n* Most of the ham messages have length<100 (mean around 70)\n* Most of the spam messages have a length of 150 (mean around 132)\n\nSo, we have discovered that spam messages generally have more characters than ham messages."},{"metadata":{},"cell_type":"markdown","source":"### Adding \"contain\" column\nLet us examine the spam messages and see if we can find any trends."},{"metadata":{"trusted":true},"cell_type":"code","source":"#examining spam texts\nfor i in range(50):\n  if msg['label'][i]==1:\n    print(msg['text'][i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that spam texts are more likely to contain numbers (charges, phone numbers), emails, links, and symbols!\n\nLet us add a column named \"contain\" denoting whether a text contains numbers, emails, links, or symbols!"},{"metadata":{"trusted":true},"cell_type":"code","source":"msg['contain']=msg['text'].str.contains('£').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('%').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('€').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains('\\$').map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"T&C\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"www|WWW\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"http|HTTP\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"https|HTTPS\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"@\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"email|Email|EMAIL\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"SMS|sms|FREEPHONE\").map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{11}\",regex=True).map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{10}\",regex=True).map({False:0,True:1})\nmsg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{5}\",regex=True).map({False:0,True:1})\n\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=msg[msg['label']==0].contain,kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=msg[msg['label']==1].contain,kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graphs confirm our observation that spam texts have a high occurrence of numbers, emails, links, and symbols as compared to ham texts."},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preprocessing\n\nThe goal of Text Preprocessing is to convert the text in a form that is easy to process and analyze. \n\nIt helps us get rid of unwanted data & noise by removing punctuations/digits/stopwords, converting to lower case, etc."},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuation & digits\nUsing inbuilt functions string.punctuation and.isdigit() to check for punctuations and digits and remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning/preprocessing - removing punctuation and digits \nmsg['cleaned_text']=\"\"\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['text'][i])):\n    if msg['text'][i][j] not in string.punctuation:\n      if msg['text'][i][j].isdigit()==False:\n        updated_list.append(msg['text'][i][j])\n  updated_string=\"\".join(updated_list)\n  msg['cleaned_text'][i]=updated_string\n\nmsg.drop(['text'],axis=1,inplace=True)\nmsg.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing & converting to lower case \nUsing re.split() to split text into words(tokens) and using .lower() to convert them into lower case."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning/preprocessing - tokenization and convert to lower case \nmsg['token']=\"\"\n\nfor i in msg.index:\n  msg['token'][i]=re.split(\"\\W+\",msg['cleaned_text'][i].lower())\n\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Stopwords\nStopwords refer to the most commonly used words in a language. For English, some of stopwords are \"on\",\"in\",\"a\",\"the\".\n\nMore on stopwords: https://www.tutorialspoint.com/python_text_processing/python_remove_stopwords.htm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning/preprocessing - stopwords\nmsg['updated_token']=\"\"\nstopwords=nltk.corpus.stopwords.words('english')\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['token'][i])):\n    if msg['token'][i][j] not in stopwords:\n      updated_list.append(msg['token'][i][j])\n  msg['updated_token'][i]=updated_list\n\nmsg.drop(['token'],axis=1,inplace=True)\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization \nLemmatization is the process in which different forms of a word are converted to its root word.\nFor example,\neating->eat, \nran->run, \nruns->run, \nbooks->book\n\nMore on Lemmatization: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning/preprocessing - lemmatization \nmsg['lem_text']=\"\"\nwordlem=nltk.WordNetLemmatizer()\n\nfor i in msg.index:\n  updated_list=[]\n  for j in range(len(msg['updated_token'][i])):\n    updated_list.append(wordlem.lemmatize(msg['updated_token'][i][j]))\n  msg['lem_text'][i]=updated_list \n\nmsg.drop(['updated_token'],axis=1,inplace=True)\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging Tokens\nMerging tokens to form the final text string."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning/preprocessing - merging token\nmsg['final_text']=\"\"\n\nfor i in msg.index:\n  updated_string=\" \".join(msg['lem_text'][i])\n  msg['final_text'][i]=updated_string\n\nmsg.drop(['cleaned_text','lem_text'],axis=1,inplace=True)\nmsg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's separate the targets & features, and then let's split them into training and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating target and features\ny=pd.DataFrame(msg.label)\nx=msg.drop(['label'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data (80:20 ratio)\nx_train,x_val,y_train,y_val=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count Vectorization\nIt involves counting the number of occurrences of each word/token in a given text.\n\nMore on Count Vectorization: https://www.educative.io/edpresso/countvectorizer-in-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"#count vectorization \ncv=CountVectorizer(max_features=5000)\ntemp_train=cv.fit_transform(x_train['final_text']).toarray()\ntemp_val=cv.transform(x_val['final_text']).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TFIDF\nIt tells us how important a word is to a text in a group of text. It is calculated by multiplying the frequency of a word, and the inverse document frequency (how common a word is, calculated by log(number of text/number of text which contains the word)) of the word across a group of text.\n\nMore on TFIDF: https://monkeylearn.com/blog/what-is-tf-idf/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidf\ntf=TfidfTransformer()\ntemp_train=tf.fit_transform(temp_train)\ntemp_val=tf.transform(temp_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging temp datafram with original dataframe\ntemp_train=pd.DataFrame(temp_train.toarray(),index=x_train.index)\ntemp_val=pd.DataFrame(temp_val.toarray(),index=x_val.index)\nx_train=pd.concat([x_train,temp_train],axis=1,sort=False)\nx_val=pd.concat([x_val,temp_val],axis=1,sort=False)\n\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the final_text column\nx_train.drop(['final_text'],axis=1,inplace=True)\nx_val.drop(['final_text'],axis=1,inplace=True)\n\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the labels to int datatype (for model training)\ny_train=y_train.astype(int)\ny_val=y_val.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model Training"},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multinomial Naive Bayes\nmodel=MultinomialNB()\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Multinomial Naive Bayes:\",accuracy_score(y_val,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree\nmodel=DecisionTreeClassifier(random_state=0)\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Decision Tree:\",accuracy_score(y_val,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest\nmodel=RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x_train,y_train)\ny_preds=model.predict(x_val)\nprint(\"Random Forest:\",accuracy_score(y_val,y_preds))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}