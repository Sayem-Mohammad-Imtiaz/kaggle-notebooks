{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip install torchaudio","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n    # Wrapper for the UrbanSound8K dataset\n    # Argument List\n    # path to the UrbanSound8K csv file\n    # path to the UrbanSound8K audio files\n    # list of folders to use in the dataset\n\n    def __init__(self, csv_path, file_path, folderList):\n        csvData = pd.read_csv(csv_path)\n        # initialize lists to hold file names, labels, and folder numbers\n        self.file_names = []\n        self.labels = []\n        self.folders = []\n        # loop through the csv entries and only add entries from folders in the folder list\n        for i in range(0, len(csvData)):\n            if csvData.iloc[i, 5] in folderList:\n                self.file_names.append(csvData.iloc[i, 0])\n                self.labels.append(csvData.iloc[i, 6])\n                self.folders.append(csvData.iloc[i, 5])\n\n        self.file_path = file_path\n        self.folderList = folderList\n\n    def __getitem__(self, index):\n        # format the file path and load the file\n        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n        sound, sample_rate = torchaudio.load(path, out=None, normalization=True)\n        soundData = torch.mean(sound, dim=0, keepdim=True)\n        tempData = torch.zeros([1, 160000])  # tempData accounts for audio clips that are too short\n\n        if soundData.numel() < 160000:\n            tempData[:, :soundData.numel()] = soundData\n        else:\n            tempData = soundData[:, :160000]\n\n        soundData = tempData\n\n        mel_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(soundData)  # (channel, n_mels, time)\n        mel_specgram_norm = (mel_specgram - mel_specgram.mean()) / mel_specgram.std()\n        mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(soundData)  # (channel, n_mfcc, time)\n        mfcc_norm = (mfcc - mfcc.mean()) / mfcc.std()\n        # spectogram = torchaudio.transforms.Spectrogram(sample_rate=sample_rate)(soundData)\n        feature = torch.cat([mel_specgram, mfcc], axis=1)\n        return feature[0].permute(1, 0), self.labels[index]\n\n    def __len__(self):\n        return len(self.file_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AudioLSTM(nn.Module):\n\n    def __init__(self, n_feature=5, out_feature=5, n_hidden=256, n_layers=2, drop_prob=0.5):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.n_feature = n_feature\n\n        self.lstm = nn.LSTM(self.n_feature, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n\n        self.dropout = nn.Dropout(drop_prob)\n\n        self.fc = nn.Linear(n_hidden, out_feature)\n\n    def forward(self, x, hidden):\n        # x.shape (batch, seq_len, n_features)\n        l_out, l_hidden = self.lstm(x, hidden)\n\n        # out.shape (batch, seq_len, n_hidden*direction)\n        out = self.dropout(l_out)\n\n        # out.shape (batch, out_feature)\n        out = self.fc(out[:, -1, :])\n\n        # return the final output and the hidden state\n        return out, l_hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n\n        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        return hidden\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n\n        model.zero_grad()\n        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n        \n        loss = criterion(output, target)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        if batch_idx % log_interval == 0: #print training stats\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, epoch):\n    model.eval()\n    correct = 0\n    y_pred, y_target = [], []\n    for data, target in test_loader:\n        data = data.to(device)\n        target = target.to(device)\n        \n        output, hidden_state = model(data, model.init_hidden(hyperparameters[\"batch_size\"]))\n        \n        pred = torch.max(output, dim=1).indices\n        correct += pred.eq(target).cpu().sum().item()\n        y_pred = y_pred + pred.tolist()\n        y_target = y_target + target.tolist()\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperparameters = {\"lr\": 0.01, \"weight_decay\": 0.0001, \"batch_size\": 128, \"in_feature\": 168, \"out_feature\": 10}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ncsv_path = '/kaggle/input/urbansound8k/UrbanSound8K.csv'\nfile_path = '/kaggle/input/urbansound8k/'\n\ntrain_set = UrbanSoundDataset(csv_path, file_path, range(1, 10))\ntest_set = UrbanSoundDataset(csv_path, file_path, [10])\nprint(\"Train set size: \" + str(len(train_set)))\nprint(\"Test set size: \" + str(len(test_set)))\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}  # needed for using datasets on gpu\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=hyperparameters[\"batch_size\"], shuffle=True, drop_last=True, **kwargs)\n\nmodel = AudioLSTM(n_feature=hyperparameters[\"in_feature\"], out_feature=hyperparameters[\"out_feature\"])\nmodel.to(device)\nprint(model)\n\noptimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr'], weight_decay=hyperparameters['weight_decay'])\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\ncriterion = nn.CrossEntropyLoss()\nclip = 5  # gradient clipping\n\nlog_interval = 10\nfor epoch in range(1, 41):\n    # scheduler.step()\n    train(model, epoch)\n    test(model, epoch)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}