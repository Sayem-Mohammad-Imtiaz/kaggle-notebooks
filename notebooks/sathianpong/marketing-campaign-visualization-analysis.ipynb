{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Note :<br>\n\n- Major update : (18-Jan-2021) <br>\n    1) Added content in section2 \"significant factor related to the number of store purchases.\" <br>\n    2) Add highlight color in dataframe for better visibility  <br>\n    3) Added KNN imputation explanation\n- I'll keep updating some little details that I miss. Once I think this notebook is perfectly done, I will delete this note. <br>\n- However, if I made some mistakes or you want to leave some advice, please do not hesitate to comment in the comment section below. <br>\n- Any comments are welcomed :)\n___"},{"metadata":{},"cell_type":"markdown","source":"**Section 01: Exploratory Data Analysis**\n\n- Are there any **null values or outliers**? How will you wrangle/handle them? <br>\n-> There're nullvalues and outliers in 'Income' column. They will be imputed by sklearn's KNNImputer\n\n- Are there any variables that warrant **transformations**? <br>\n-> We can transform year columns into age columns.\n\n- Are there any useful variables that you can engineer with the given data? <br>\n-> Total_product_purchases, Conversion rate of campaign acception and many others will be shown inside the notebook.\n\n- Do you notice any **patterns or anomalies** in the data? Can you plot them? <br>\n-> There're anomaly in customers' age and extramly high value in income columns. There also are highly right-skewed distribution in product purchases, income, channel purchase columns\n\n\n\n**Section 02: Statistical Analysis**\n\n- What factors are significantly related to the **number of store purchases?** <br>\n-> MntWines\n\n- Does **US** fare significantly better than the Rest of the World in terms of **total purchases?** <br>\n-> *No, we can't conclude like that*\n\n- Your supervisor insists that people who buy gold are more conservative. \nTherefore, people who spent an **above average amount on gold** in the last 2 years would have **more in store purchases**. Justify or refute this statement using an appropriate statistical test <br>\n-> *Yes*\n- Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do **\"Married PhD candidates\"** have a significant relation with **amount spent on fish**? What **other factors** are significantly related to amount spent on fish? <br>\n-> *Not at all*\n\n- Is there a significant relationship between **geographical regional** and success of a **campaign**? <br>\n-> *Yes, many campaigns are most successful in Spain*\n\n\n**Section 03: Data Visualization**\n\nThis notebook will not explicitly have this section. However, throughout this notebook, I did quite a lot of visualization to answer the questions of all the sections.\n\n- Which marketing **campaign** is most successful? <br>\n-> *Campaign4 have the greatest number of acceptance while canpaign2 have the least.*\n- What does the average customer look like for this company? <br>\n-> *Middle-age, Spainish, Have degree(s), Have family, Mostly purchase wine*\n- Which products are performing best? <br>\n-> *wine*\n- Which channels are underperforming? <br>\n-> *Deal(Purchase by discount deal)*\n\n\n___"},{"metadata":{},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{},"cell_type":"markdown","source":"We're gonna using these libraries."},{"metadata":{},"cell_type":"markdown","source":"*Make sure the version of seaborn is 0.11.0"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport warnings\nimport re\nimport os\nsns.set()\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\nwarnings.filterwarnings(\"ignore\")\n\nsns.__version__","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import our data."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/marketing-data/marketing_data.csv', index_col='ID', parse_dates=['Dt_Customer'])\n\ndata = data.rename(columns={\n    'Dt_Customer':'Enrollment date',\n    'Recency':'Days since last purchase',\n    ' Income ':'Income'})\n\ndata['ID'] = data.index\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of columns :',data.shape[1])\nprint('Number of records :',data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"Change \"Income\" column format from \"\\\\$84,835.00\" (String) to 84835.00 (Float) <br>\nBy replacing comma(\",\") and \"$\" with empty string(\"\")."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Another way\ndata['Income'].str.replace(r'[$,]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract(x):\n    if x is np.nan: return np.nan\n    return float(re.sub(r'[$,]', \"\", str(x)))\n\ndata['Income'] = data['Income'].apply(extract)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create table for each section of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store customer's information\nCustomers = data.loc[:,:'Days since last purchase'].join(data[['Country']])  \n\n# Store product's information\nProducts = data.loc[:,'MntWines':'MntGoldProds']     \n\n# Store Purchases' information\nPurchases = data.loc[:,'NumDealsPurchases':'NumWebVisitsMonth']    \n\n# Store campaign's information\nCampaigns = data.loc[:,'AcceptedCmp3':'AcceptedCmp2']     \nMisc = data.loc[:,['Response','Complain']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create table for each type of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"category = data.select_dtypes(include='object')\nnumeric = data.select_dtypes(exclude='object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Section 01: Exploratory Data Analysis\n- Are there any null values or outliers? How will you wrangle/handle them?\n- Are there any variables that warrant transformations?\n- Are there any useful variables that you can engineer with the given data?\n- Do you notice any patterns or anomalies in the data? Can you plot them?\n\nI'll do my best to answer above questions. This first section is separated into 3 parts: <br>\n    **1) Null values** : In this part, I'll try to find all the missing values and impute them. <br>\n    **2) Outliers** : In this part, I'll plot all the distribution of numerical data in order to detect some outliers and try to make sense of them.<br> \n    **3) Analysis** in each feature : In this part, I'll try to seek some insights into the data by creating perceptive pivot tables, graphs, and other visualizations. \n\n"},{"metadata":{},"cell_type":"markdown","source":"\n## 1) Null values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"pd.DataFrame(data.isnull().sum(), columns=['#Null values']).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There're 24 detectable null values in \"Income\" column. However, we still need to check for other missing values since sometimes missing values are denoted as, for example, \"Unknown\" for categorical data or -1 for numerical data. <br>\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for f in category.columns:\n    print(category[f].value_counts())\n    print('***********************************')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspecting each value counts in categorical columns -> there're no more missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = numeric.describe()\n\ndef custom_style(row):\n    \n    color = 'white'\n    if row.name == 'min' or row.name == 'max':\n        color = 'darkkhaki'\n\n    return ['background-color: %s' % color]*len(row.values)\n\ndf.style.apply(custom_style, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspecting each numerical columns -> There's no weirdly low or high value.\n\n\n### We can conclude that there are only have missing values in \"Income\" column"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"'''\nfrom sklearn.impute import KNNImputer\n\nnumeric_before_impute = numeric.drop(['Year_Birth','Enrollment date','ID'], axis=1).copy()\n\nimputer = KNNImputer(missing_values=np.nan)\nnumeric_imputed = imputer.fit_transform(numeric_before_impute)\n\nnumeric_imputed = pd.DataFrame(numeric_imputed, \n                       index=numeric_before_impute.index, \n                       columns=numeric_before_impute.columns).join(numeric[['Year_Birth','Enrollment date','ID']])\n\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Outliers <br>\nCreate report() function to describe and visualize the numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def report(feature):\n    fig, ax = plt.subplots(1,2)\n    fig.set_size_inches(16,4)\n    fig.suptitle(feature, fontsize=16)\n    sns.histplot(data=numeric, x=feature, kde=True, ax=ax[0])\n    sns.boxplot(data=numeric, x=feature, ax=ax[1])\n    plt.show()\n\n    print(numeric[feature].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll create new column named \"Age\" and \"Enroll_at_age\" derived from \"Year_Birth\" and \"Enrollment date\" respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\n\nAge = date.today().year-numeric['Year_Birth']\nnumeric.insert(1, 'Age', Age,)\n\nEnroll_at_age = numeric['Enrollment date'].dt.year - numeric['Year_Birth']\nnumeric.insert(6, 'Enroll_at_age', Enroll_at_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, print the distribution report for numerical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in numeric.columns:\n    if col in ['Year_Birth','Enrollment date']: continue\n    if col == 'AcceptedCmp3' : break\n    report(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we get some sense of distribution of each numerical column, next, we'll analyze them. <br>\n___\n\n\n**-> Age** <br>\nThere're 3 people with ages 128, 122, and 121 whose Enrollment_at_age are 113, 114, and 121 respectively which is quite impossible."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric[numeric['Age']>80]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to [Wikipedia](https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people), there're no person alives at that age in 20 century. <br>\nWe can conclude that there were some mistakes in these records. So, I'll mark their \"Year_Birth\", \"Age\", \"Enroll_at_age\" Null."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = numeric[numeric['Age']>120].index\nnumeric.loc[temp, [\"Year_Birth\", \"Age\", \"Enroll_at_age\"]] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**-> Income**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric[numeric['Income']>160000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one person with ID 9432 that has very high income which is not impossible. Moreover, when consider age and enrollment date, it seems OK. So, I decide to do not thing with him.\n\n**Amount of product, number of purchases features** are exponentially distributed which not unusual. <br>"},{"metadata":{},"cell_type":"markdown","source":"### 3) Analysis <br>\nAfter we have looked into missing values and outliers, we then analyze all of the features to get some insight into them.\n\nConsidering **Enrollment date**, we can see the total number of enrollment in each year in the following bar graph."},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.bar(height = numeric['Enrollment date'].dt.year.value_counts()[[2012,2013,2014]], x=['2012','2013','2014'])\nplt.title('Number of enrollment in each year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However in 2012, the data is gathered from 2012/08 and ,in 2014, data is gathered until 2014/07. Thus, that makes sense that 2013 have the greatest number of enrollment. The graph below shows the average number of enrollment over time."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df = pd.pivot_table(numeric, values='ID', index='Enrollment date', aggfunc='count')\ndf['count'] = df['ID'].rolling(10).mean()\ndf['Year'] = df.index.year.astype('category')\n\nfig, ax = plt.subplots()\nfig.set_size_inches(20,6)\n\nsns.lineplot(data=df, x='Enrollment date', y='count', ax=ax, hue='Year')\nax.set_ylabel('Average enrollments')\nax.xaxis.set_major_locator(mdates.MonthLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nplt.xticks(rotation=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Income <br>\n**Mean and Median of Income** in **each country**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# This time, we need to exclude the outlier in \"Income\" column before caculating any statistics.\nnumeric_analysis = numeric[numeric['Income']!=666666]\n\ndf = pd.pivot_table(numeric_analysis.join(category[['Country']]), \n                     values='Income', \n                     index='Country', \n                     aggfunc={'Income':['mean','median']})\ndf.plot(kind='bar')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.ylabel('Income')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll go deeper by looking into relationship between **age** and **income** in **each country**. <br>\nWe noticed that \"ME\" country have only 3 observations. We won't drop them. Instead, we'll keep that caution in mind while we're doing analysis."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = numeric_analysis[['Income','Age']].join(category[['Country']])\nsns.lmplot(data=temp, y='Income', x='Age', col='Country', col_wrap=4, line_kws={'color': 'darkorange'}, scatter_kws={'color':'teal'})\nplt.ylim(0,200000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, **Income** in each **education level**."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df = pd.pivot_table(numeric_analysis.join(category[['Education']]), \n               values='Income', \n               index='Education', \n               aggfunc={'Income':['count','mean','median']})\n\ndf[['mean','median']].plot(kind='bar')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.ylabel('Income')\nplt.xlabel('')\nplt.title('Income in each education level')\nplt.show()\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del numeric_analysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Purchase"},{"metadata":{},"cell_type":"markdown","source":"I'll create new column **\"Total products amount\"** : define a total number of products purchased by each customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric['Total products amount'] = np.sum(Products, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll looking into **proportion** of the number of **purchases in each channel** ('Deal', 'Web', 'Catalog', 'Store') to see the performance of each channel. <br>\nWe see that 39% of all purchase is in store, 27.5% in web, 15.6% in deal, and 17.9% by catalog. We can conclude that ,from the data, more than half of the customers purchased in store and website. "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"total_purchase_each = np.sum(Purchases.iloc[:,:-1], axis=0)\n\npercent_purchase_each = total_purchase_each/np.sum(total_purchase_each)*100\n\ndef plot_pie_chart(labels, sizes: pd.Series, title):\n    fig, ax = plt.subplots()\n    fig.suptitle(title, fontsize=16)\n    fig.set_size_inches(5,5)\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    ax.axis('equal')  \n    plt.show()\n    print(sizes.sort_values(ascending=False))\n    \n\nplot_pie_chart(labels=['Deal','Web','Catalog','Store'], sizes=percent_purchase_each, title='Purchases in each channel')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, looking into total number of **purchases** in each **country**. <br>\nWe see that most of the customers are from Spain"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"if 'Total purchase' not in Purchases.columns:\n    Purchases['Total purchase'] = np.sum(Purchases, axis=1)\n\nPurchase_category = Purchases.join(category)\n\nPurchase_country_summary = pd.pivot_table(Purchase_category, \n                                          values='Total purchase', \n                                          index='Country', \n                                          aggfunc={'Total purchase':['sum']})\n\nPurchase_country_summary.plot(kind='bar')\nplt.title('Total purchases in each country')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Product"},{"metadata":{},"cell_type":"markdown","source":" We wanted to know which product was the most popular. So, we'll look into **Overall proportion of products purchased**. <br>\n We see that 50.2% of all products purchased by all customers is wine and the second place(27.6%) is meat."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"sum_each_product = np.sum(Products, axis=0)\n\nplot_pie_chart(sizes=sum_each_product/np.sum(sum_each_product)*100, \n               labels=['Wine','Fruit','Meat','Fish','Sweet','Gold'],\n              title='Overall %Products')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moreover, we want to know the average purchase behavior of each customer. We'll looking into **Average proportion of each product purchased by one ID** <br>\nFrom 100% of all product each person puurchased*, we see that 45.8% will be wine, 24.95% meat product, 12% gold ptoduct, 7% fish product, 5% sweet, 4.9% fruit product."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"if 'Total' not in Products.columns:\n    Products['Total'] = np.sum(Products, axis=1)\n\nEach_ID_Products = Products.apply(lambda x:x/x[-1]*100, axis=1)\n\nAvg_Each_ID_Products = np.mean(Each_ID_Products, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plot_pie_chart(sizes = Avg_Each_ID_Products[:-1], \n               labels=['Wine','Fruit','Meat','Fish','Sweet','Gold'],\n               title='Average Product for each ID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## - Campaign <br>\nWe wanted to know the performance of each campaign we conducted. Below, the bar graph and pie-chart show the number of acceptance and success rates in each campaign. <br>\nWe see that camapign2 might have some problems because it's very less accepted while other campaigns are accepted at a similar rate."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"each_campaign = np.sum(Campaigns,axis=0)\n\nCR_each_canpaign = each_campaign/len(Campaigns)*100","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(15,4)\n\ncam_color = ['steelblue','peru','olivedrab','teal','sienna']\n\nax[0].barh(y=['Campaign 3','Campaign 4','Campaign 5','Campaign 1','Campaign 2'], \n           width=each_campaign.values, color=cam_color)\nax[0].set_xlabel('# success')\n\nax[1].pie(x=CR_each_canpaign, labels=['Campaign 3','Campaign 4','Campaign 5','Campaign 1','Campaign 2'],\n         autopct='%1.1f%%', shadow=True, startangle=90, colors=cam_color)\nax[1].axis('equal')\nax[1].set_xlabel('Overall each campaign\\'s success rate')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to know if 'Age' have some noticeable effect on campaign acceptance. <br>\nAlthough there's no obvious trend, we still can see that campaign3 is likely to be accepted in younger customers than campaign4 while other campaigns are uniformly accepted in different ages."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,len(Campaigns.columns), sharey=True)\ni=0\nfig.set_size_inches(20,6)\n\nfor campaign in Campaigns.columns:\n    sns.histplot(data=numeric[numeric[campaign]==1], x = 'Age', ax=ax[i])\n    ax[i].set_ylim(0,40)\n    ax[i].set_title(campaign)    \n    i+=1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll see if there's that effect in different countries. The table below shows the **overall conversion rate** in **each country**. <br>\n\nThe overall conversion rate is **around 30%**. <br>\nWe see that '*ME'* has the best conversion rate but there is only 3 observation in this country. So, *it's not significant*. <br>\nOther than 'ME', campaigns in **'SP' and 'CA'** get the **best conversion rate(32.4%)**. <br>\nThe worst rate is in 'AUS' which is 21.8%."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"Campaigns_category = Campaigns.join(category)\nCampaigns_category['Total accept'] = np.sum(Campaigns, axis=1)\n\nsummary_country = pd.pivot_table(Campaigns_category, \n                                   values='Total accept', \n                                   index='Country', \n                                   aggfunc={'Total accept':['sum','count']})\n\nsummary_country['CR'] = summary_country['sum']/summary_country['count']\nsummary_country.rename(columns={'count':'#customers', 'sum':'Total accept'}).style.background_gradient(sns.light_palette('khaki', as_cmap=True), \n                                                                                                       subset=pd.IndexSlice[:, ['CR']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll see the **average performance** of **each campaign** in **each country**. <br>\nKeep in mind that *\\\"ME\\\" has only 3 observations*."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = Campaigns_category.groupby(by='Country').agg(['mean'])\ntemp.drop(['ME'], axis=0, inplace=True)  # since \"ME\" have only 3 observations, I decided to drop it in this table.\ntemp.style.background_gradient(sns.light_palette('green', as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In each column, the darkest green shade one is the country that performed the best in each campaign. <br>\nWe see that:\n- In campaign 3, \"GER\", \"IND\", \"SP\", \"US\" perform quite well.\n- In campaign 4, \"SP\", \"IND\", \"GER\", \"CA\" perform quite well.\n- In campaign 5, \"AUS\", \"CA\", \"SP\" perform quite well.\n- In campaign 1, \"SP\", \"CA\" perform quite well.\n- Campaign 2 was doing not very well in any country.\n- For the overall average performance, \"CA\", \"GER\", \"SP\" are the best."},{"metadata":{},"cell_type":"markdown","source":"# Section 02: Statistical Analysis\n\n- What **factors** are significantly related to the **number of store purchases?** <br>\nI'll figure out this question by trying some feature selection methods including L1-Regularization, ANOVA F-test, Recursive feature elimination."},{"metadata":{},"cell_type":"markdown","source":"**First**, We'll see the *linear* effect of numerical variables on \"number of store purchases\" by using statistical models such as regression and F-test. <br>\nBut before that we need to immpute the missing values. <br>\n\nI'll impute these null values by KNN imputation. KNN imputation is an approach to fill the missing data by using a model to predict the missing values. A range of different models can be used, although a simple k-nearest neighbor (KNN) model has proven to be effective in experiments. The use of a KNN model to predict or fill missing values is referred to as “Nearest Neighbor Imputation” or “KNN imputation.” <br>\n\n[this link](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/) provides you a great stuff about KNN imputation. Make sure you check it out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building analysis dataset\nnumeric_analsis = numeric[numeric['Income']!=666666]\n\n# Let's clear redundant features\nX_numeric = numeric_analsis.drop(['NumStorePurchases','Enrollment date','Year_Birth','ID','Total products amount'], axis=1)\n\n# Focus on number of store purchases\ny = numeric_analsis[['NumStorePurchases']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.feature_selection import f_regression, f_classif, chi2, RFE, VarianceThreshold, SelectPercentile\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.impute import KNNImputer\n\n# 1) Preoprocess\n# Impute null values\nImputer = KNNImputer(missing_values=np.nan)\n\n# Standardize numeric columns\nScaler = StandardScaler()\n\nnumeric_pipe = Pipeline([(\"Impute\", Imputer),\n                         (\"Scale\", Scaler)])\n\nX_numeric_preprocessed = numeric_pipe.fit_transform(X_numeric)\n\n# 2) Building the models\nlasso = Lasso(alpha=0.01).fit(X_numeric_preprocessed, y)\nridge = Ridge().fit(X_numeric_preprocessed, y)\nF, p = f_regression(X_numeric_preprocessed, y)  # Statistically check how each predictor & target are linearly correlated","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table below shows a summary of the models we have built. LASSO's coefficient and F-value can tell the importance of each variable. <br>\nI also included the variance of each variable to see the spread of value in each variable."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"index = [\n    ['Features','Variance','Lasso','Ridge','F-test','F-test'],\n    ['','','Coef','Coef','F-value','p-value']\n]\n\nStorePurchases_effect_num = pd.DataFrame(list(zip(X_numeric.columns,\n                                          X_numeric.var()/np.nanmean(X_numeric, axis=0),\n                                          lasso.coef_, \n                                          ridge.coef_[0], \n                                          F, p))\n                                          ,columns=index).set_index('Features')\n\nStorePurchases_effect_num.sort_values(by=('F-test','p-value')).style.background_gradient(sns.light_palette('khaki', as_cmap=True), \n                                                                                                       subset=pd.IndexSlice[:, [('Lasso','Coef'),\n                                                                                                                                ('Ridge','Coef'),\n                                                                                                                                ('F-test','F-value')]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For simplicity, we plot the **coefficient of LASSO** and **F-value of F-test** in the bar graphs below."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nStorePurchases_effect_num['Lasso']['Coef'].sort_values().plot(kind='barh')\nplt.xlabel('Lasso coef')\nplt.title('LASSO analysis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering Lasso coefficient, we see that \n- Features having positive effect to the number of store purchases in decreasing order is *'MntWines', 'NumDealsPurchases', 'NumWebPurchases', 'MntFruits', 'MntFishProducts'*.\n- Features having negative effect to the number of store purchases in decreasing order is *NumWebVisitsMonth, Kidhome, Response*."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nStorePurchases_effect_num['F-test']['F-value'].sort_values().plot(kind='barh')\nplt.xlabel('F-value')\nplt.title('F-test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apart from statistical methods(LASSO and F-test) that measure te linear effect, we'll also perform Recursive Feature Elimination which is a feature selection method based on ML."},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\n\nrfe = RFE(estimator = rf, n_features_to_select = 1.0, verbose=1).fit(X_numeric_preprocessed, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(dict(zip(X_numeric.columns, rfe.ranking_)), name='Rank').to_frame().sort_values(by='Rank')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude from many methods we performed before that **'MntWines' is the most significant factor related to the number of store purchases.**"},{"metadata":{},"cell_type":"markdown","source":"Once we see the effect of our numerical variables on the number of store purchases, we also have to see the effect of categorical variables.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"category_NumStore = numeric[['NumStorePurchases']].join(category)\ncategory_NumStore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(kind='box', data=category_NumStore.query(\"Country!='ME'\"), col='Country', x='Education', y='NumStorePurchases',  col_wrap=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously, we can see that **'Basic' education level** seems to have **the lowest number of store purchases in all countries**. \n\nOn the other hand, there is no obvious trend in marital status in each country as shown in the plot below. However, from the plot below, we might conclude that **widows in the US** are **less likely to purchase in-store** while those in AUS, IND, GER are tended to purchase in-store."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(kind='bar', data=category_NumStore.query(\"(Country!='ME') and (Marital_Status not in ['YOLO','Alone','Absurd'])\"), \n            col='Country', x='Marital_Status', y='NumStorePurchases',  col_wrap=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we have to look up to the average number of store purchases in each country and in each education level to see if there's some bias in the number of samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, sharey=True)\nfig.set_size_inches(15,5)\n\nfor i,col in list(enumerate(['Education', 'Marital_Status', 'Country'])):\n    sns.countplot(data=data, x='Education',  ax=ax[i])\n\nfor ax_i in ax:\n    ax_i.xaxis.set_label_coords(0.5, 1.05)\n    plt.setp( ax_i.xaxis.get_majorticklabels(), rotation=40 )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't confidentially tell that **'Basic' education level** have **the lowest number of store purchases in all countries** because we don't have enough data to say so. But these visualization give us a roughly say. If we have more data, we'll have more confident to that."},{"metadata":{},"cell_type":"markdown","source":"- Does **US** fare significantly better than the Rest of the World in terms of **total purchases?** <br>\n\nI'll do **X-test** to test whether mean of total purchases in US is significantly higher than other country by stating null-hypothesis and alternative hypothesis under the significance level of 0.05 as following.<br>\nIn each country : $C$ <br>\n$H_0 : \\mu_{us} \\leq \\mu_{c}$ <br>\n$H_a : \\mu_{us} > \\mu_{c}$\n$, \\alpha = 0.05$\n\nLet's see the distribution of purchases in each country and then <br>\na table that summary important statistics of purchases in each country."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = Purchases.join(data[['Country','ID']])\n\nfig, ax = plt.subplots(2,4)\nfig.set_size_inches(20,7)\ni=0\n\nimport itertools \naxes = list(itertools.chain(ax[0],ax[1]))\n\nfor c in temp.Country.unique():\n    sns.histplot(data=temp[temp['Country'] == c], x='Total purchase', label=c, ax=axes[i], bins=15)\n    axes[i].legend()\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_purchase_country = np.sum(Purchases, axis=1).to_frame('Total purchases').join(category[['Country']]).groupby(by='Country').agg(['count','sum','mean','median','std'])\n\nsummary_purchase_country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\n\nUS_mean = summary_purchase_country.loc['US', ('Total purchases','mean')]\nUS_std = summary_purchase_country.loc['US', ('Total purchases','std')]\nUS_n = summary_purchase_country.loc['US', ('Total purchases','count')]\n\nfor country in summary_purchase_country.index:\n    if country == 'US':break\n    other_mean = summary_purchase_country.loc[country, ('Total purchases','mean')]\n    x = (US_mean-other_mean)/(US_std/US_n**0.5)\n    print('mean \"US\" > mean \"'+country.upper()+'\" p-value = '+str(norm.sf(x)))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we ignore \"ME\", because of its too few observation, we still can not conclude that US has significantly better than the Rest of the World in term of total purchase since p-value of $\\mu_{us}>\\mu_{ca}$ is 0.0554 which is not less than 0.05."},{"metadata":{},"cell_type":"markdown","source":"- people who spent an above average amount on gold in the last 2 years would have more in store purchases.\n\nFrom the histogram and boxplot below, we can conclude that people who spent an **above-average amount on gold** have more **in-store purchases**."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Gold_avg = np.mean(Products['MntGoldProds'])\n\nmask = Products['MntGoldProds']>=Gold_avg\n\nAbove_gold = Products[mask].index\nBelow_gold = Products[~mask].index\n\nfig,ax = plt.subplots(1,2)\nfig.set_size_inches(15,5)\nsns.histplot(ax=ax[0], data=Purchases[mask], x='NumStorePurchases', kde=True, label='Purchase Gold Above Avg.', color='indigo',element='step')\nsns.histplot(ax=ax[0], data=Purchases[~mask], x='NumStorePurchases', kde=True, label='Below', color='darkorange', element='step')\n\ntemp = Purchases.join(mask)\ntemp['MntGoldProds'] = temp['MntGoldProds'].replace({True:'Gold above avg.', False:'Gold below avg.'})\nsns.boxplot(ax=ax[1], data=temp, y='NumStorePurchases', x='MntGoldProds', palette={'Gold above avg.':'indigo','Gold below avg.':'darkorange'})\n\nax[0].legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- do **\"Married PhD candidates\"** have a significant relation with amount spent on **fish**? What other factors are significantly related to amount spent on fish?"},{"metadata":{"trusted":true},"cell_type":"code","source":"PhD_Married = (category['Education']=='PhD') & (category['Marital_Status']=='Married')\n\nnumeric.loc[PhD_Married,['MntFishProducts']].describe().join(numeric.loc[~PhD_Married,['MntFishProducts']].describe(),\n                                                            lsuffix='_PhD&Marrid',\n                                                            rsuffix='_Not')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the summary table above, we clearly see that \"Married PhD candidates\" doesn't have a significant relation with amount spent on fish. <br>"},{"metadata":{},"cell_type":"markdown","source":"So, what is the key factor related to amount spent on fish? Let's first see the correlation between other numerical features and 'MntFishProducts'."},{"metadata":{"trusted":true},"cell_type":"code","source":"Customers.drop(['Year_Birth','Enrollment date'], axis=1)\\\n.join([Purchases, Products, Campaigns])\\\n.corr()[['MntFishProducts']]\\\n.style.background_gradient(sns.light_palette('green', as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation table above, the darker the color shade is the greater relation on 'MntFishProducts' each variable has."},{"metadata":{},"cell_type":"markdown","source":"Moreover, we need to see if there are some categorical variables that can be related to 'MntFishProducts'."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, sharey=True)\nfig.set_size_inches(20,7)\nsns.boxplot(data=data[data['Country']!='ME'], \n            x='Country', y='MntFishProducts',\n            ax=ax[0])\n\nsns.boxplot(data=data.query('Marital_Status not in  [\"YOLO\",\"Alone\",\"Absurd\"]'), \n            x='Marital_Status', y='MntFishProducts',\n            ax=ax[1])\n\nsns.boxplot(data=data, x='Education', y='MntFishProducts',ax=ax[2])\n\nfig.tight_layout()\nfig.suptitle('Relationship between each categorical variable\\nand amount spent on Fish', fontsize=18, y=1.07)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the **\"Relationship between each categorical variable and amount spent on Fish\"** graphs we see that:\n- In 'Country' variable, we see no difference in each country on 'MntFishProducts'.\n- In 'Marital_Status' variable, Widow tends to have more 'MntFishProducts'.\n- In 'Education' variable, 'Graduation' and '2n Cycle' tends to have more 'MntFishProducts'."},{"metadata":{},"cell_type":"markdown","source":"Once we know that 'Marital_Status' and 'Education' have some relation to 'MntFishProducts', we'll go deeper to find more specific insight.\n\nFirst, see 'Marital_Status' in each country."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.catplot(kind='box', data = data.query('(Marital_Status not in [\"YOLO\",\"Absurd\",\"Alone\"]) and (Country != \"ME\")'), \n            x ='Marital_Status', \n            y ='MntFishProducts', \n            col='Country', col_wrap=4)\nplt.suptitle('In each country, \\nAmount spent on Fish in each marital status', fontsize=18, y=1.07)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the **\"In each country, Amount spent on Fish in each marital status\"** graphs we see that:\n- **Widows** from **\"GER\" and \"IND\"** tend to spend on fish significantly more than others. While the **Divorced** from those country have a opposite trend."},{"metadata":{},"cell_type":"markdown","source":"And then, see 'Education' in each country."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.catplot(kind='box', data = data.query('(Marital_Status not in [\"YOLO\",\"Absurd\",\"Alone\"]) and (Country != \"ME\")'), \n            x ='Education', \n            y ='MntFishProducts', \n            col='Country', col_wrap=4)\nplt.suptitle('In each country, \\nAmount spent on Fish in each education level', fontsize=18, y=1.07)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the **\"In each country, Amount spent on Fish in each education level\"** graphs we see that:\n\n- Only 'Graduation' and '2n Cycle' in 'SP', 'US', and 'SA' spend on fish more than others. But not for other countries."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}