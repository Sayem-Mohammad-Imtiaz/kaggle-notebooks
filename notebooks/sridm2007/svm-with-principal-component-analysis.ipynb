{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bd98a3c1-c211-b180-3155-005d4bc4f860"},"source":"This is my first machine learning algorithm in Kaggle. I will explore this dataset and try Logisctic regression, and Support Vector Machine classification algorithm on the diabetes dataset. The plan in the mind is as follows:\n1. Apply principal Principal Component Analysis and use fewer components to train and test a logistic regression classifier. If the accuracy is not good, it probably means that the decision boundary is nonlinear, hence go to SVM.\n2. Train and test a SVM algorithm and optimize the hyperparameters using grid search. Apply PCA and see how the number of principal components influence the accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5c51373-c1b2-77c3-262c-036d592c6d79"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\ndata = pd.read_csv(\"../input/diabetes.csv\")\ndata.head(5)\n\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d6a23c6-c419-dda3-48aa-b1a4e47df573"},"outputs":[],"source":"# Total number of rows in the dataset\nprint(len(data)) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee7cc999-e3bf-03c7-a169-bf9136640f8a"},"outputs":[],"source":"#Separate features and labels\nX = data.iloc[:,0:8].values\ny = data.iloc[:,8].values\n\n#Test train split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\n#Standard scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n#Applying PCA here\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= None) #We will set it none so that we can see the variance explained and then choose no of comp.\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance"},{"cell_type":"markdown","metadata":{"_cell_guid":"1577177c-b3fc-50b1-b6c6-5167c6d02cd4"},"source":"Let us try with 2 principal components and fit a logistic regression to see the performance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a820838-696d-98ae-4806-c543bf224830"},"outputs":[],"source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\npca = PCA(n_components= 2) # here you can change this number to play around\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n# Create the classifier and train using training data\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\n#Predict the test set values\ny_pred = classifier.predict(X_test)\n\n#Compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\ncm"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a0f9d84-a60a-fffd-6556-cfee6dfcae28"},"source":"Let us try to play with the number of components"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f1d3ce6-385b-d52e-6fea-817152203cee"},"outputs":[],"source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\npca = PCA(n_components= 4) #I have tried different no of components here\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n# Create the classifier and train using training data\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\n#Predict the test set values\ny_pred = classifier.predict(X_test)\n\n#Compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\ncm"},{"cell_type":"markdown","metadata":{"_cell_guid":"f8d0c668-7ee9-94b9-b80f-05be08b889b0"},"source":"Logistic classification is a linear classification. Let us proceed to try some nonlinear classifiers such as Support vector machines."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b41870c-40ea-68c3-ab2f-7a4398df6c26"},"outputs":[],"source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n#Create classifier object\nfrom sklearn.svm import SVC\nclassifier_svm_kernel = SVC(C=5.0,kernel='rbf', gamma=0.12,tol=0.00001)\nclassifier_svm_kernel.fit(X_train,y_train)\n\n#Predict the result for test values\ny_pred = classifier_svm_kernel.predict(X_test)\n\n#Compute confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\ncm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b678946f-7814-2740-65f4-9935f724b39e"},"outputs":[],"source":"#Comparing the predictions with the actual results\ncomparison = pd.DataFrame(y_test,columns=['y_test'])\ncomparison['y_predicted'] = y_pred\ncomparison.head(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ebe1a0df-65e7-f9b9-1923-aa4bcd62897c"},"source":"Let us now try to implement a k-fold cross validation with a grid search routine to figure the best hyperparameters along with the statistics of our accuracy, precision and recall. Applying a ten-fold cross validation."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5b7a222-1db8-1cf7-16fb-642a4082773a"},"outputs":[],"source":"#Apply k-fold validation here\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier_svm_kernel,X=X_train,y=y_train,cv=10)\naccuracies"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f5101bd-c398-82d9-57cd-fd62f41a4113"},"outputs":[],"source":"plt.hist(accuracies)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"523a41c9-4104-00a0-794e-97ef35bebbfe"},"source":"From the plot of accuracy histogram, we can see that the variance is less where as the accuracy is centered around 0.76.  Note that we have used all the features in this SVM classifier. However, the hyperparameters were chosen at random. Let us try to pass a grid-search method to figure out optimal values for hyperparameters."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38519078-0074-a5a8-9e47-5c76116922e8"},"outputs":[],"source":"#Applying grid search for optimal parameters and model after k-fold validation\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = [{'C':[0.01,0.1,1,10,50,100,500,1000], 'kernel':['rbf'], 'gamma': [0.1,0.125,0.15,0.17,0.2]}]\ngrid_search = GridSearchCV(estimator=classifier_svm_kernel, param_grid=parameters, scoring ='accuracy',cv=10,n_jobs=-1)\ngrid_search = grid_search.fit(X_train,y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8088661c-162f-e55f-1aa7-4f9f655acdcd"},"outputs":[],"source":"best_accuracy = grid_search.best_score_\nbest_accuracy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"664af3bb-d764-1895-11db-5bf0ae86fb82"},"outputs":[],"source":"opt_param = grid_search.best_params_\nopt_param"},{"cell_type":"markdown","metadata":{"_cell_guid":"7216fd70-2f84-10d8-74fb-a5a5fa5c0fdb"},"source":"It seems, that even with the optimal parameters, the accuracy is still around 0.76 with a support vector classifier.  It might be interesting to study if using a different classifier might be able to give us better accuracy. \n\nOne last thing before I end this notebook is to see whether using just few principal components can give us similar accuracy. Hence I will apply PCA and then use SVM on the few components."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c4e79fa-9a87-d2a3-5a72-0fb3e5138089"},"outputs":[],"source":"#Reloading the features and labels and normalizing them\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n#Choosing 2 principal components\npca = PCA(n_components= 2) # here you can change this number to play around\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n#Create classifier object\nclassifier_svm_kernel = SVC(C=5.0,kernel='rbf', gamma=0.12,tol=0.00001)\nclassifier_svm_kernel.fit(X_train,y_train)\n\n# Grid search and k fold validation libraries already imported. So start the grid search\ngrid_search = GridSearchCV(estimator=classifier_svm_kernel, param_grid=parameters, scoring ='accuracy',cv=10,n_jobs=-1)\ngrid_search = grid_search.fit(X_train,y_train)\n\nbest_accuracy = grid_search.best_score_\nbest_accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"abd160ad-0c11-8ae8-e774-cd88a286d13e"},"source":"Using just 2 principal components gives you an accuracy of about 0.5 less than all the features. Thus, it seems that most of the variance is explained by just 2 components. Let us just try different values of n_components."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f0e0dd7-5d3f-7ea3-361b-7e1126924fe1"},"outputs":[],"source":"#Reloading the features and labels and normalizing them\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n#Choosing different principal components\npca = PCA(n_components= 7) # here you can change this number to play around\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\n#Create classifier object\nclassifier_svm_kernel = SVC(C=5.0,kernel='rbf', gamma=0.12,tol=0.00001)\nclassifier_svm_kernel.fit(X_train,y_train)\n\n# Grid search and k fold validation libraries already imported. So start the grid search\ngrid_search = GridSearchCV(estimator=classifier_svm_kernel, param_grid=parameters, scoring ='accuracy',cv=10,n_jobs=-1)\ngrid_search = grid_search.fit(X_train,y_train)\n\nbest_accuracy = grid_search.best_score_\nbest_accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"89a76f90-5bd6-ebed-243f-026d9dffe0d8"},"source":"The results were as follows: \n    n = 3 gave best_accuracy = 0.752\n    n = 4 gave best_accuracy = 0.715\n    n = 5 gave best_accuracy = 0.749\n    n = 6 gave best_accuracy = 0.767\n    n = 7 gave best_accuracy = 0.754\n    \nOfcourse, i these accuracies are dependent on randomness and I am just showing the best accuracy than the mean accuracy. It still looks like n=3 (after several tries) gives a satisfactory accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93699331-9755-0db9-7bcd-2969567186b4"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}