{"cells":[{"metadata":{"collapsed":true,"_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport nltk\nimport random\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n%matplotlib inline","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"0aa2e083-6b14-40e1-98d7-dc361933ec8d","_uuid":"83b27eb12aac44db763d748a9aedab2d719f32c9"},"cell_type":"markdown","source":"# Sms classification - feature engineering vs nlp."},{"metadata":{"_cell_guid":"b2bd374e-7142-4049-8378-63e29e572abd","_uuid":"8b454a650d430bb0fab235b63c3a758f77a6ff6f"},"cell_type":"markdown","source":"## 1. Data preparation"},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sms = pd.read_csv('../input/spam.csv', encoding='latin-1', engine='python')","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"d442668f-22bd-4997-847c-8c7e4ff084a8","_uuid":"2361f72d275c9d46b0fe4b0467ed072105ff06e0","trusted":true},"cell_type":"code","source":"sms.head()","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0181c9f8-38d9-4ee0-8570-dc5a3d297700","_uuid":"3b2582909881695002ee3587040d062ba69a054d","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sms = sms[[\"v1\", \"v2\"]]\nsms.columns = [\"class\", \"message\"]","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"9fcd2087-1b0d-4e13-911b-e4469d206e3f","_uuid":"f8d48a03658ec24df24fb7d287ae15a06cddcc82","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.DataFrame(data=sms.sum().isnull(), columns=[\"Has null?\"])","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"83f0c034-2f3e-4021-aad3-39979f9d67d5","_uuid":"c1a34a19c78dfd44fcfb8eda1901ad136cd709c6","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cats = list(set(sms[\"class\"]))\nsms.loc[:,(\"class\")] = sms[\"class\"].apply(lambda x: cats.index(x))\ncats","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"5a19aa39-80a9-4e6a-8e45-76190561babe","_uuid":"405a85836701b8942dfa001fe4aed84c2293a32a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sms.head()","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"17331ef5-b795-4b3a-abfb-3d3e2f4c097c","_uuid":"e72ad363240968cb94a0900f2367ba68d2b96d29"},"cell_type":"markdown","source":"## 2. Visualizations\n\nI'm going to compare the most frequently used words across the two classes."},{"metadata":{"collapsed":true,"_cell_guid":"53aa7247-ac70-4847-a371-ad52febe56ba","_uuid":"ca2e1de749d5b7da09f8961d9414b0a50a8aec96","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def word_count(clas):\n    \"\"\" This function counts most frequent words \n    that occur in one class and plots a bar plot.    \n    clas: 0 - legit, 1 - spam. \"\"\"\n    \n    stop_words = set(stopwords.words(\"english\"))    \n    all_words = []\n    rgtok = RegexpTokenizer(r'\\w+')\n    lem = WordNetLemmatizer()\n\n    for msg in list(sms[sms[\"class\"] == clas ][\"message\"]):\n        for word in (rgtok.tokenize(msg)):\n                if word not in stop_words and not word.isdigit():        \n                    all_words.append( lem.lemmatize(word.lower()) )\n\n    all_words_d = nltk.FreqDist(all_words)\n    most_common = pd.DataFrame(data=all_words_d.most_common(30), columns=[\"Word\", \"Count\"])            \n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax = sns.barplot(y=\"Word\", x=\"Count\", data=most_common, ax=ax, orient=\"h\")","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"a92a906b-6aed-493a-b7c9-600cf78fff1e","_uuid":"49b8b9c50c685c7a60d843c0a646810a9aed738d"},"cell_type":"markdown","source":"### Most frequent words in legit messages."},{"metadata":{"_cell_guid":"9a0265ea-7d80-4178-9463-10065dc10972","_uuid":"d33edb1d5eaf1ddc35852a026c5f2cdb4a9cfc36","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"word_count(1)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"ea28b42b-b7f7-4c77-9c92-526a037cca86","_uuid":"b3e564f82a58b005f7d643881d1aed8a9c821762"},"cell_type":"markdown","source":"### Most frequent words in spam."},{"metadata":{"_cell_guid":"24e49e9c-bac9-4acf-9ad6-7e161fa99289","_uuid":"cbe61195c8f79a63b7020d5076bb5e88895b5414","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"word_count(0)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"9633eb4e-e7a8-4b10-b620-a6a64c58b410","_uuid":"32793d8c160edc5ae60b6cac60df3770d8b54e10"},"cell_type":"markdown","source":"As you can see, there is a great difference between words that are frequently used."},{"metadata":{"_cell_guid":"3c6707dd-6353-4d62-b855-827effbfffaf","_uuid":"9b660c9a22c8b8a352498c92186568bfb1a16a57"},"cell_type":"markdown","source":"## 3. Features & more vis\n\nLet's make up some features and check if they are any good."},{"metadata":{"collapsed":true,"_cell_guid":"937feeac-7db8-4738-b9d2-3d798864c5c1","_kg_hide-input":true,"_uuid":"ee214fd6e77f6e322e8fb3888f2f7ff7548b0609","trusted":true},"cell_type":"code","source":"sms[\"no_letters\"] = sms[\"message\"].apply(lambda x: len(x))\nsms[\"no_words\"] = sms[\"message\"].apply(lambda x: len(str(x).split(' ' )))\nsms[\"no_spaces\"] = sms[\"message\"].apply(lambda x: sum( l == \" \" for l in str(x)))\nsms[\"no_alnum\"] = sms[\"message\"].apply(lambda x: sum( l.isalnum() for l in str(x)))\nsms[\"no_notalnum\"] = sms[\"message\"].apply(lambda x: sum( not l.isalnum() for l in str(x)))\nsms[\"no_alnum\"] = sms[\"message\"].apply(lambda x: sum( l.isalnum() for l in str(x)))\nsms[\"no_digits\"] = sms[\"message\"].apply(lambda x: sum( l.isdigit() for l in str(x)))\nsms[\"no_capital\"] = sms[\"message\"].apply(lambda x: sum (l.isupper() for l in str(x)))\nsms[\"no_unique\"] = sms[\"message\"].apply(lambda x: len(set(str(x).split(' '))))\nsms[\"no_punct\"] = sms[\"message\"].apply(lambda x: sum(str(x).count(punct) for punct in \".,:;\" ))\nsms[\"no_excl\"] = sms[\"message\"].apply(lambda x: sum(str(x).count(punct) for punct in \"!\" ))\nsms[\"no_quest\"] = sms[\"message\"].apply(lambda x: sum(str(x).count(punct) for punct in \"?\" ))","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"97b33bb8-865e-475a-b1d4-b3f08c723761","_uuid":"3859534c8a04ef3f4496db07def5d5cb4549d3a7"},"cell_type":"markdown","source":"### Correlation matrix"},{"metadata":{"_cell_guid":"2af8c6b1-7486-4775-a114-a4b7a904945d","_kg_hide-input":true,"_uuid":"106c583c3c7f29ec99c734b6841af1a7b2d1dd2e","trusted":true},"cell_type":"code","source":"sms_corr = sms.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(sms_corr, cmap=cmap)\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"a70addb4-3db4-4329-a396-f1602243490b","_uuid":"3877bc445144ca7ee5256d192948ac727ad4995c"},"cell_type":"markdown","source":"### Visualize per class distributions of *promising* features."},{"metadata":{"scrolled":false,"_uuid":"b3f9766bfb33c983427c522c6c1cc49b6582bdf7","_cell_guid":"c7b2eea2-8e69-4c50-b9e9-c2d19edaf529","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,1, figsize=(14, 14))\nsns.distplot(sms[sms[\"class\"] == 1][\"no_letters\"], color=\"green\", label=\"ham\", norm_hist=True, kde=False, ax=ax[0])\nsns.distplot(sms[sms[\"class\"] == 0][\"no_letters\"], color=\"red\", label=\"spam\", norm_hist=True, kde=False, ax=ax[0])\nax[0].legend()\n\nsns.distplot(sms[sms[\"class\"] == 1][\"no_digits\"], color=\"green\", label=\"ham\", norm_hist=True, kde=False, ax=ax[1])\nsns.distplot(sms[sms[\"class\"] == 0][\"no_digits\"], color=\"red\", label=\"spam\", norm_hist=True, kde=False, ax=ax[1])\nax[1].legend()\n\nsns.distplot(sms[sms[\"class\"] == 1][\"no_excl\"], color=\"green\", label=\"ham\", norm_hist=True, kde=False, ax=ax[2])\nsns.distplot(sms[sms[\"class\"] == 0][\"no_excl\"], color=\"red\", label=\"spam\", norm_hist=True, kde=False, ax=ax[2])\nax[2].legend()\n\nplt.show()","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"8c46dc0c-e9d3-4598-83b9-56fc9dab9f7a","_uuid":"a253243fe77994b5aa4c45b2b06bc4be52353461"},"cell_type":"markdown","source":"Based on the plots above - when people text each other they use less letters than spammers. The amount of numbers used in a text also seems to be some kind of indication of the nature of the text. Lastly the number of exclamation marks - kinda makes sense since as a spammer you would want to emphasize eg. winning a prize etc.\n\nSo it seemes to me that these 3 features are the most promising and I shall use them to train models to compare against nlp."},{"metadata":{"_cell_guid":"43bfdd4d-8f3a-4cea-8ea1-ba5776d14e3f","_uuid":"88600c02efe5e3f7b90f73d4d7e06e65789200e4"},"cell_type":"markdown","source":"## 4. Model"},{"metadata":{"_cell_guid":"d15a1345-ba83-464d-a091-75b3c5476215","_uuid":"3e9cdc5946394f289e6681bf244e33b1acd70af7"},"cell_type":"markdown","source":"### Tokenize messages, lemmatize words\nI tokenized and lemmatized the with the nltk RegexpTokenizer and WordNetLemmatizer in such a way that all punctuation marks  where dropped from the texts."},{"metadata":{"collapsed":true,"_cell_guid":"abd79c8e-2366-4b6b-8c0a-a9b07c3bb2f0","_uuid":"eb3d450ddfea5e11abf72098008cef42295889c9","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def tokenize_sms(df):\n    \"\"\" Tokenization of sms messages with nltk. \"\"\"\n    \n    rgtok = RegexpTokenizer(r'\\w+')    \n    lem = WordNetLemmatizer()    \n    \n    df[\"message\"] = df[\"message\"].apply(lambda x: rgtok.tokenize(x))\n    df[\"message\"] = df[\"message\"].apply(lambda x: [lem.lemmatize(y.lower()) for y in x])\n    df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))    \n        \n    return df","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"49100788-4fb1-4ac1-b5d7-6f84e0b71683","_uuid":"7e4970bdfc1b779dd3be9f8362174c0b2e9c8d82","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sms = tokenize_sms(sms)\nsms[['class', 'message']].head()","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"71ee835a-3ffa-43c4-97fc-24fc6f409026","_uuid":"96a02661dc461845226c6d5e2e5adc81cff768c8"},"cell_type":"markdown","source":"### Test train split\n\nThe standard 80-20 split was conducted on the data. Quick glance at the train data:"},{"metadata":{"_cell_guid":"b0a51a06-fb98-4e27-836b-6f81d3d5cdf5","_uuid":"671e0f0dfd5e336c3b2b623505b0978c9262f18b","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rs = 111\nc_to_keep = [\"message\", \"no_letters\", \"no_digits\", \"no_excl\"]\nX_train, X_test, y_train, y_test = train_test_split(sms[c_to_keep], sms[\"class\"], \n                                                    test_size=0.2, train_size=0.8, \n                                                    random_state=rs)\n\nc_to_keep = [\"message\", \"no_letters\", \"no_digits\", \"no_excl\"]\n\nX_train, X_test, y_train, y_test = train_test_split(sms[c_to_keep], sms[\"class\"], \n                                                    test_size=0.2, train_size=0.8, \n                                                    random_state=rs)\n\nX_train.head()","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"40a43106-05b1-4ee9-9cab-16fcd318ec67","_uuid":"ef8f51e2f73a9506f674591682e2711d0f0c51b4"},"cell_type":"markdown","source":"### Training\nTo make things easier I wrote a helper function."},{"metadata":{"_cell_guid":"c4f23413-32fd-4013-ac3d-c38cd3da8ea5","_uuid":"771bb7c06a57b246846e538115ffe2048a055126","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def go(model, params, X_train, X_test, y_test):\n    \"\"\" This function fits piplines and gets info about cv and classification report. \n    \n    Parameters\n    model:   an estimator.\n    params:  parameters for grid search.\n    X_train: data to train estimator.\n    X_test:  data to test estimator.\n    y_pred:  used to generate classification report. \n    \n    Returns clf.score. \"\"\"\n    \n    clf = GridSearchCV(model, params, cv=5, n_jobs=6)\n    clf.fit(X_train, y_train)\n    \n    print(model.named_steps['clf'])\n\n    print(\"\\n\")    \n    \n    for fold in zip(clf.cv_results_['mean_test_score'], clf.cv_results_['std_test_score'], clf.cv_results_['params']):\n        print(\"M: %8.5f. Sd: %8.5f. %s\" % (fold[0], fold[1], fold[2]))  \n        \n    print(\"\\nBest params: %s \\n\" % str(clf.best_params_))\n        \n    print(\"\\n\")\n        \n    y_pred = clf.predict(X_test)\n    print(classification_report(y_test, y_pred))        \n    \n    return clf.score(X_test, y_test)","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"497f65ff-1037-49af-8adf-f1e0e33726f8","_uuid":"c825636915ebc0b72e1a6599a9d8f2068b47c183"},"cell_type":"markdown","source":"#### 1. Text only - words and bi-grams."},{"metadata":{"_cell_guid":"1400be5c-e2c1-46f6-9808-68d0629c3ae7","_uuid":"d268c7def6a47d91560defd4590e8678ce9ff5e1","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mnb = Pipeline([('vect', CountVectorizer(analyzer=\"word\", \n                                          tokenizer=str.split, \n                                          ngram_range=(1,2),\n                                          stop_words=\"english\",\n                                          strip_accents=\"unicode\",\n                                          lowercase=True)),\n               ('tfidf', TfidfTransformer()),\n               ('clf',  MultinomialNB(alpha=10))])\n\nmnb_params = {\"clf__alpha\": [10, 1, 0.1, 0.01, 0.001]}\n\nmnb_score = go(model=mnb, params=mnb_params, X_train=X_train[\"message\"], X_test=X_test[\"message\"], y_test=y_test)","execution_count":34,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"084bdc2a-9d44-45ac-8cdd-6238b372ea8d","_uuid":"fedec63859f7f3368ab5e68cb72a7ef866ecf657","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"rfc = Pipeline([('vect', CountVectorizer(analyzer=\"word\", \n                                          tokenizer=str.split, \n                                          ngram_range=(1,2),\n                                          stop_words=\"english\",\n                                          strip_accents=\"unicode\",\n                                          lowercase=True)),\n               ('tfidf', TfidfTransformer()),\n               ('clf',  RandomForestClassifier())])\n\nrfc_params = {\"clf__n_estimators\": range(1, 100, 10)}\n\nrfc_score = go(model=rfc, params=rfc_params, X_train=X_train[\"message\"], X_test=X_test[\"message\"], y_test=y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"260d45b1-4eec-445b-9837-4ae96db7a610","_uuid":"01db6fa8e02f1bdbb4c25fac49134d06c54dde20"},"cell_type":"markdown","source":"#### 2. Engineered features only."},{"metadata":{"collapsed":true,"_cell_guid":"5e15f8ce-600b-4a98-9afd-f6efc8e48bc4","_uuid":"08c2305327c0a26a47fbdb35028cff1c9fe9dc24","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"t_cols = [\"no_letters\", \"no_digits\", \"no_excl\"]\n\nmnb_ef = Pipeline([('clf', MultinomialNB())])\nmnb_ef_params = {\"clf__alpha\": [10, 1, 0.1, 0.01, 0.001]}\nmnb_ef_score = go(model=mnb_ef, params=mnb_ef_params, X_train=X_train[t_cols],X_test=X_test[t_cols], y_test=y_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"348c03d0-acad-4bdf-baad-4a83ff71aba8","_uuid":"43483e37a53c9d38d069b6a2f4e0b02d8e9c4ecf","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"rfc_ef = Pipeline([('clf', RandomForestClassifier())])\nrfc_ef_params = {\"clf__n_estimators\": range(1, 100, 10)}\nrfc_ef_score = go(model=rfc_ef, params=rfc_ef_params, X_train=X_train[t_cols],X_test=X_test[t_cols], y_test=y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fba9d262-0c0a-48d1-a11b-90304f59234c","_uuid":"30f153c006a39a803c3e8f94cb71b5c59251697f"},"cell_type":"markdown","source":"## 5. Summary\n\nThe Multinomial Bayes classifier performed worse when trained on the features I came up with. The Random Forest classifier performed the same. Going in to this little experiment I really thought the performance hit would be worse."},{"metadata":{"collapsed":true,"_cell_guid":"eda99906-a94e-4c14-bb42-61f87e339fe4","_uuid":"67f568139558a80dfeef37cc9d254729c46b54c8","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"summary = pd.DataFrame(data=np.array([[mnb_score, rfc_score],[mnb_ef_score,rfc_ef_score]]), \n                       columns=[\"Multinomial Bayes\", \"Random Forest\"],\n                    index=[\"nlp\", \"engineered\"])\n\nsummary.round(2)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}