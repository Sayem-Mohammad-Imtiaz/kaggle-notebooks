{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Netflix is one of the world's leading entertainment services with 204 million paid memberships in over 190 countries enjoying TV series, documentaries and feature films across a wide variety of genres and languages. In this notebook we will explore the data on TV Shows and Movies available on Netflix worldwide. First we will do some Exploratory Data Analysis (EDA) to know and describe the data in a better way through interactive graphs and visualizations. Then we will build a multilabel classifier using BERT and MLP to predict the Genre of a show using its description.\n\n\nPlease upvote the notebook if you like it and suggest improvements in the comments.\n\n\n\n![](https://haasentertainment.com/wp-content/uploads/2019/11/ReflectingNeglectedBug-size_restricted.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget 'https://raw.githubusercontent.com/SarthakV7/covid_dashboard/master/data/country_to_iso.csv'\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv('../input/netflix-shows/netflix_titles.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's begin by taking a look at the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The dataset has 7787 rows and 12 columns:\n* **show_id:** unique id of each show (not much of a use for us in this notebook)\n* **type:** The category of a show, can be either a Movie or a TV Show\n* **title:** Name of the show\n* **director:** Name of the director(s) of the show\n* **cast:** Name of actors and other cast of the show\n* **country:** Name of countries the show is available to watch on Netflix\n* **date_added:** Date when the show was added on Netflix\n* **release_year:** Release year of the show\n* **rating:** Show rating on netflix\n* **duration:** Time duration of the show\n* **listed_in:** Genre of the show\n* **description:** Some text describing the show"},{"metadata":{},"cell_type":"markdown","source":"## How about the number of shows available worldwide in each country?"},{"metadata":{"trusted":true},"cell_type":"code","source":"iso = pd.read_csv('country_to_iso.csv')[['Country','Alpha-3 code']]\ndf_map = pd.DataFrame()\nx = np.hstack([np.array(i.split(',')) for i in df.country.dropna()])\nunique, counts = np.unique(x, return_counts=True)        \ndf_map['Country'] = unique\ndf_map['count'] = counts\ndf_map = df_map.merge(iso, how='left', on='Country').dropna()\ndf_map['Alpha-3 code'] = df_map['Alpha-3 code'].apply(lambda x:x[2:-1])\n\nfig = go.Figure(data=go.Choropleth(locations=df_map['Alpha-3 code'],\n                                    z=df_map['count'].astype(float),\n                                    colorscale='viridis',\n                                    text=df_map['Country'],\n                                    marker_line_color='black',\n                                    colorbar_title = 'number of shows'))\n\n# fig.update_geos(projection_type=\"orthographic\")\n                \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(df_map.sort_values('count', ascending=False).iloc[:20], values='count', \n             names='Country', title='Shows available in different countries (top 20)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A majority of shows are available in the US (2883 shows), followed by India (956 shows), UK (577 shows), Canada (259 shows), Japan (237 shows), France(196 shows)..."},{"metadata":{},"cell_type":"markdown","source":"## Number of Movies or TV Shows?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(df, x='type')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are 5377 Movies and 2410 TV Shows available on Netflix all across the world."},{"metadata":{},"cell_type":"markdown","source":"## Number of shows (Movies + TV Shows) released every year"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(df, x='release_year')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The number of shows have been rapidly increasing each year achieving a maximum in 2018 (1121 Shows!) followed by a dip in 2019 (996 Shows) and 2020 (868 Shows)."},{"metadata":{},"cell_type":"markdown","source":"## Number of shows (Movies + TV Shows) added to Netflix every year"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram([int(i.split(', ')[1]) for i in df.date_added.dropna()], orientation='h', labels={'value':'year'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The number of shows have been rapidly increasing each year achieving a maximum in 2019 (2153 Shows!) followed by a dip 2020 (2009 Shows)."},{"metadata":{},"cell_type":"markdown","source":"## Here are the top 10 oldest shows available on Netflix"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values('release_year')[['title', 'type', 'country', 'director', 'cast', 'release_year']].dropna().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Content ratings of the shows"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.funnel(df.rating.value_counts(), labels={'index':'rating type'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The largest count of shows are made with the 'TV-MA' rating (2863 shows)**\n\"TV-MA\": For mature audiences only.\n\n**Second largest is the 'TV-14' rating (1931 shows)**\n\"TV-14\": May be inappropriate for children younger than 14 years of age.\n\n**Third largest is the 'TV-PG' rating (806 shows)**\n\"TV-PG\": Parental guidance suggested\n\n**Fourth largest is the very popular 'R' rating (665 shows)**\n\"R\": May be unsuitable for children under the age of 17 (Under 17 requires accompanying parent or adult guardian\")"},{"metadata":{},"cell_type":"markdown","source":"## How long is a movie?\n### Histogram of duration of movies."},{"metadata":{"trusted":true},"cell_type":"code","source":"grp = df.groupby('type')\nmovie = grp.get_group('Movie')\nmovie['duration'] = [int(i.split(' ')[0]) for i in movie.duration.dropna()]\nfig = px.histogram(movie, x='duration', nbins=60, labels={'duration':'duration (in mins)'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a majority of the shows are between 90 and 99 mins. The distribution takes a bell shaped curve, let's use violin plot and box-plot to get a better idea about the data"},{"metadata":{},"cell_type":"markdown","source":"### Violin and box-plot representation of movie duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = grp.get_group('Movie')\nmovie['duration'] = [int(i.split(' ')[0]) for i in movie.duration.dropna()]\nfig = px.violin(movie, x='duration', box=True, points=\"all\", labels={'duration':'duration (in mins)'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright! The median movie length is 97 mins, 50% of the movies have a duration between 86 mins and 114 mins.\nShortest movie available on netflix is 3 mins long whereas the longest movie available on netflix is 312 mins long!"},{"metadata":{},"cell_type":"markdown","source":"### Top 20 shortest movies available on Netflix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#longest movie\nfig = px.bar(movie.sort_values('duration')[['title', 'duration']].iloc[:20], x='title', y='duration',\n             labels={'index':'Director', 'value':'movie count'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 13 movies that are less than 15 mins long, the shortest movie being **Silent** (3 mins) followed by **Sol Levante** (5 mins) and **Cops and Robbers** (8 mins).\n\nBelow are some more details on these 3 movies from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.sort_values('duration')[['title', 'director', 'country', 'rating', 'duration', 'description']].iloc[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 20 longest movies available on Netflix"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(movie.sort_values('duration')[['title', 'duration']].iloc[-20:], x='title', y='duration',\n             labels={'title':'Movie name'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 15 movies that are more than 200 mins long, the longest movie being **Black Mirror: Bandersnatch** (312 mins) followed by **The School of Mischief** (253 mins) and **No Longer kids** (237 mins).\n\nBelow are some more details on these 3 movies from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.sort_values('duration')[['title', 'director', 'country', 'rating', 'duration', 'description']].iloc[-3:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How many seasons do TV Shows on Netflix have?\n### Histogram of number of seasons of TV Shows."},{"metadata":{"trusted":true},"cell_type":"code","source":"tv_series = grp.get_group('TV Show')\ntv_series['duration'] = [int(i.split(' ')[0]) for i in tv_series.duration]\nfig = px.histogram(tv_series, x='duration', nbins=20, labels={'duration':'number of seasons'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the TV Shows (1608 shows) have a single season whereas much lesser shows have more than 1 seasons on Netflix."},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at the top 25 TV Shows on Netflix with the most number of seasons"},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = df[df['type']=='TV Show'][['title', 'duration']]\ndd['duration'] = [int(i.split(' ')[0]) for i in dd.duration]\nfig = px.bar(dd.sort_values('duration', ascending=False).iloc[:25], x='title', y='duration',\n             labels={'title':'TV series name', 'duration':'number of seasons'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gery's Anatomy leads with 16 seasons available on Netflix followed by NCIS and Supernatural (15 seasons both)."},{"metadata":{},"cell_type":"markdown","source":"## Who has directed most number of movies?\n### Top 20 directors who've directed the most number of movies."},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = pd.DataFrame()\ndd['director'] = [f'{i} ({j})' for i,j in df[['director', 'country']].dropna().values]\nfig = px.bar(dd.director.value_counts()[:20], labels={'index':'Director', 'value':'movie count'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Marcus Raboy has 15 shows listed on Netflix followed by Jay Karas (14 shows) and then Cathy Garcia-Molina (13 shows)."},{"metadata":{},"cell_type":"markdown","source":"![](https://m.media-amazon.com/images/M/MV5BNTgyNzk2Njc4Ml5BMl5BanBnXkFtZTgwOTM0ODEyMjE@._V1_UY317_CR53,0,214,317_AL__QL50.jpg)\nSince the early 1990s, Raboy has amassed a large number music video credits directing music videos for Mary J. Blige, Dixie Chicks, Rihanna, Luther Vandross, Shakira, Santana, and Westlife among other notable artists.\nHis feature film credits are Friday After Next (2002) and Janky Promoters (2009) both starring Ice Cube and Mike Epps.\nRaboy grew up in New York City and attended New York University. He is managed by David Naylor & Associates and currently resides in Los Angeles, California.\n\n![](https://resizing.flixster.com/85hGP2DnnYs2Fy299eGAA3g5j_g=/300x300/v2/http://media.baselineresearch.com/images/1238751/1238751_full.jpg)\nJay Karas's resume primarily consists of directing live telecasts and stand-up comedy specials. In recent years he moved on to directing episodic television, directing episodes of Parks and Recreation, Raising Hope, Awkward, The Fosters, Brooklyn Nine-Nine and Workaholics. In 2014, Karas made his feature film directing debut with the film Break Point, starring Jeremy Sisto and David Walton.\nKaras's episode of Dice was one of The Hollywood Reporter's \"Critics' Picks: The 15 Best TV Episodes of 2016\" and Teachers made Vanity Fair's \"5 Underrated TV Shows You Should Watch Right Now\".\n\n![](https://assets.mubicdn.net/images/cast_member/181323/image-w240.jpg)\nCatherine Rosales Garcia-Molina (born November 28, 1971) is a Filipino film and television director best known for directing romantic comedy films produced and distributed by Star Cinema. She has also directed a few TV series, which aired on ABS-CBN. Some of her films were considered top-grossers in the Philippines.\nHer 2019 film, Hello, Love, Goodbye, starring Kathryn Bernardo and Alden Richards, was released in July and has become the highest grossing Philippine film of all time."},{"metadata":{},"cell_type":"markdown","source":"## Who has acted in most number of shows?\n#### Top 25 actors who've casted the most in shows."},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = pd.DataFrame()\ndd['cast'] = np.hstack([np.array(i.split(',')) for i in df.cast.dropna()])\nfig = px.bar(dd.cast.value_counts()[:25], labels={'index':'cast', 'value':'movie count'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anupam Kher has casted in most number of shows (38 shows) on Netflix, followed by Takahiro Sakurai (28 shows) and then Shah Rukh Khan (27 shows)."},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/b/b9/AnupamKher2.jpg)\nAnupam Kher (born 7 March 1955) is an Indian actor and the former Chairman of Film and Television Institute of India. He is the recipient of two National Film Awards and eight Filmfare Awards. He has appeared in over 500 films predominantly in Hindi language, and many plays. He won the Filmfare Award for Best Actor for his performance in Saaransh (1984). He holds the record for winning the Filmfare Award for Best Comedian five times in total for: Ram Lakhan (1989), Lamhe (1991), Khel (1992), Darr (1993) and Dilwale Dulhaniya Le Jayenge (1995).\n\n![](https://m.media-amazon.com/images/M/MV5BZmIyMGYyOTYtMGQ5OS00MWM3LWIxNzUtYTkyOWExOWQ4YjhiXkEyXkFqcGdeQXVyNDQxNjcxNQ@@._V1_UY317_CR21,0,214,317_AL__QL50.jpg)\nTakahiro Sakurai (櫻井 孝宏, Sakurai Takahiro, born June 13, 1974) is a Japanese voice actor associated with Intention. Notable roles that Sakurai has portrayed include Giyū Tomioka from Demon Slayer: Kimetsu no Yaiba, Tentomon from the Digimon Adventure series, Zombieman from One Punch Man, Sting Eucliffe from Fairy Tail, Sasori from Naruto Shippuden, Cloud Strife in Compilation of Final Fantasy VII, Licht/Patry from Black Clover, and Osomatsu Matsuno from Osomatsu-san.\n\n![](https://in.bmscdn.com/iedb/artist/images/website/poster/large/shah-rukh-khan-2092-12-09-2017-02-10-43.jpg)\nShah Rukh Khan (born 2 November 1965), also known by the initialism SRK, is an Indian actor, film producer, and television personality. Referred to in the media as the \"Baadshah of Bollywood\" (in reference to his 1999 film Baadshah), \"King of Bollywood\" and \"King Khan\", he has appeared in more than 80 Hindi films, and earned numerous accolades, including 14 Filmfare Awards. The Government of India has awarded him the Padma Shri, and the Government of France has awarded him the Ordre des Arts et des Lettres and the Legion of Honour. Khan has a significant following in Asia and the Indian diaspora worldwide. In terms of audience size and income, he has been described as one of the most successful film stars in the world."},{"metadata":{},"cell_type":"markdown","source":"## Genre of the shows available on Netflix\n### Bar plot of show genre and number of corresponding shows"},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = pd.DataFrame()\ndd['listed_in'] = np.hstack([np.array(i.split(', ')) for i in df.listed_in.dropna()])\nfig = px.bar(dd.listed_in.value_counts(), labels={'index':'genre', 'value':'movie count'}, orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the shows belong to International Movies category followed by comedy, Documentaries and TV Dramas."},{"metadata":{},"cell_type":"markdown","source":"## Top words used in show description\n### Wordcloud of words used in show descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(df.description.dropna().values)\nwordcloud = WordCloud(background_color = 'black').generate(text)\nplt.figure(figsize=(15, 6))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like **life, family, find take, new, world** are some of the most common words used in describing the shows."},{"metadata":{},"cell_type":"markdown","source":"## Top words used in show title\n### Wordcloud of words used in show title"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(df.title.dropna().values)\nwordcloud = WordCloud(background_color = 'black').generate(text)\nplt.figure(figsize=(15, 6))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like **Christmas, man, love, life, story, girl** are some of the most common words used in describing the shows."},{"metadata":{},"cell_type":"markdown","source":"## Let's use some deep learning to predicting a show's genre based on its description\n\n![](https://i.ibb.co/vxbyh33/Screen-Shot-2021-02-15-at-6-27-29-PM.png)\n\n### I suggest that before proceeding, check out [my blog on BERT](https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef) to understand the futher steps better.\n\nIn this section, we'll be using transformers and MLP to predict a show's genre from its description text.\nThis will be a multilabel classification task.\n\nMulti-label classification originated from the investigation of text categorisation problem, where each document may belong to several predefined topics simultaneously.\n\nMulti-label classification of textual data is an important problem. Examples range from news articles to emails. For instance, this can be employed to find the genres that a movie belongs to, based on the summary of its plot.\n\n### Approach:\nWe'll be using transfer learning to get embeddings from pretrained BERT for each show's description. \nNext, we'll be training a MLP from the generated embeddings to predict the show genre.\n\n### Data:\nThe input data here will be show description and the output labels will be the 42 Genres.\n\n**Input data:** This will be the text describing a show\n\n**Output labels:** This will be a vector of size 42 (since there are 42 genres) having values 0 or 1. 0 corresponding to a genre not present and 1 corresponding a genre present"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for creating output labels from show genre (the feature 'listed_in' in the dataset)\nlabels = np.hstack([np.array(i.split(', ')) for i in df.listed_in.dropna()])\nunique = np.unique(labels)\ndef generate_label(x):\n    genres = x.split(', ')\n    label = np.zeros(shape=unique.shape)\n    for i in genres:\n        for j in range(len(unique)):\n            if unique[j]==i:\n                label[j]=1\n    return label.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the prepared dataset that we'll be using in this task.\nThe column text holds show description text and the column label holds the 42 dimensional vector to be used as output labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\ndata = pd.DataFrame()\ndata['text'] = df.dropna()['description']\ndata['title'] = df.dropna()['title']\ndata['label'] = [generate_label(x) for x in tqdm(df.dropna()['listed_in'])]\ndata[['text', 'label']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's begin by checking the word length of show descriptions so that we can trim and pad the texts to the same length for easy processing in the future steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordlen(x): \n    return len(x.split())\n\ndata['len'] = data.text.apply(get_wordlen)\ndata['len'].plot(kind='hist')\nplt.title('histogram of show description word length')\nplt.xlabel('word length')\nplt.show()\nfor i in np.arange(0.9,1,0.01):\n    p = data.len.quantile(i)\n    print(f'word length at {int(i*100)} percentile:',p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the texts (98%) are less than a length of 31, we'll be using 31 as the threshold value and making all the text of length 31 by truncating larger texts or by padding smaller texts."},{"metadata":{},"cell_type":"markdown","source":"### Let's divide our data into train, validation and test sets (70:15:15)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data[['text', 'title']], data['label'], test_size=0.3, random_state=33)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape, y_val.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary libraries\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom transformers import BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input\ntf.get_logger().setLevel('ERROR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a transformer model from pretrained BERT_EN_UNCASED"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nmax_seq_length = 31\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\" )\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False, name='BERT')\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\nbert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model.summary(), bert_model.output, bert_model.input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(bert_model, show_shapes=False, show_dtype=False,\n                          show_layer_names=True, rankdir='TB', \n                          expand_nested=False, dpi=96)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using tokenization to convert the description text into a format understood by BERT\nWe'll be creating 3 type of inputs from a given test:\n\n* token_ids: The token embeddings are numerical representations of words in the input sentence.\n* token_masks: The mask tokens that help BERT to understand what all input words are relevant and what all are just there for padding.\n* token_segments: The segment embeddings are used to help BERT distinguish between the different sentences in a single input."},{"metadata":{"trusted":true},"cell_type":"code","source":"from bert import tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef text_to_tokens(x):\n    t = np.asarray(tokenizer.tokenize(x))\n    if len(t)>max_seq_length-2:\n        t = t[:max_seq_length-2]\n    padding = np.asarray(['[PAD]']*(max_seq_length-t.shape[0]-2))\n    pre, post = np.asarray(['[CLS]']), np.asarray(['[SEP]'])\n    final = np.concatenate((pre,t,post,padding))\n    ids = np.asarray(tokenizer.convert_tokens_to_ids(final))\n    mask = (ids != 0)*1\n    segment = np.zeros_like(ids)\n    return ids, mask, segment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n# initializing lists to collect the generated tokens, masks and segments. \nX_train_tokens, X_val_tokens, X_test_tokens = [], [], []\nX_train_mask, X_val_mask, X_test_mask = [], [], []\nX_train_segment, X_val_segment, X_test_segment = [], [], []\n# Generating and storing tokens, masks, segments values for X_train texts\nfor i,x in tqdm(enumerate(X_train.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_train_tokens.append(t) \n    X_train_mask.append(m) \n    X_train_segment.append(s)\n    \n# Generating and storing tokens, masks, segments values for X_val texts\nfor i,x in tqdm(enumerate(X_val.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_val_tokens.append(t) \n    X_val_mask.append(m) \n    X_val_segment.append(s)\n    \n# Generating and storing tokens, masks, segments values for X_test texts\nfor i,x in tqdm(enumerate(X_test.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_test_tokens.append(t) \n    X_test_mask.append(m) \n    X_test_segment.append(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Obtaining the numerical embeddings of text from BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the tokens lists to array type\nX_train_tokens = np.row_stack(X_train_tokens) \nX_val_tokens = np.row_stack(X_val_tokens) \nX_test_tokens = np.row_stack(X_test_tokens)\n# converting the masks lists to array type\nX_train_mask = np.row_stack(X_train_mask) \nX_val_mask = np.row_stack(X_val_mask) \nX_test_mask = np.row_stack(X_test_mask)\n# converting the segment lists to array type\nX_train_segment = np.row_stack(X_train_segment) \nX_val_segment = np.row_stack(X_val_segment) \nX_test_segment = np.row_stack(X_test_segment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pooled_output = bert_model.predict([X_train_tokens, X_train_mask, X_train_segment])\nX_val_pooled_output = bert_model.predict([X_val_tokens, X_val_mask, X_val_segment])\nX_test_pooled_output = bert_model.predict([X_test_tokens, X_test_mask, X_test_segment])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pooled_output.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a MLP model which can take the BERT embeddings as input and generate predictions.\nSince the embeddings are of a dimension (31, 768), we will forst initiale an Input layer of the same dimension, next we'll add a GlobalAveragePooling1D layer to extract 768 embeddings from the input (average for each of the 31 rows) and finally we'll be initializing a Dense layer as the output layer with 42 units and sigmoid as the activation function.\nSigmoid is used because each of the 42 outputs could take a value between 0 and 1.\n\nFor training the model, we'll use binary_crossentropy as the loss function and Adaptive Momentum (ADAM) as the optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\n\ninput_layer = Input((None, 768))\ngpa = GlobalAveragePooling1D()(input_layer)\n# x = Dense(units=64, activation='elu')(gpa)\n# x = Dense(units=64, activation='elu')(x)\noutput_layer = Dense(units=42, activation='sigmoid')(gpa)\n\nmlp = Model(input_layer, output_layer)\nmlp.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(mlp, show_shapes=False, show_dtype=False,\n                          show_layer_names=True, rankdir='TB', \n                          expand_nested=False, dpi=96)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining metric for testing the predicted and actual genres.\nThe metric first calculates the accuracy on each predicted output vector by comparing it with the actual output vector. If all the 42 elements of predicted and actual vectors are the same, the accuracy is 100%. The accuracy decreases as more elements mismatch in the predicted and the actual vector.\n\nThe accuracy score is calculated as the mean of all the accuracies obtained on the batch of data passed.\n\n![](https://miro.medium.com/max/1400/1*_qeJQPY9CKki2xouAQr6fg.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndef get_accuracy(y, y_pred):\n    acc = []\n    for i,j in zip(y, y_pred):\n        acc.append(accuracy_score(i,j))\n    return np.mean(acc)\n\ndef accuracy(y, y_pred):\n    return tf.py_function(get_accuracy, (y, tf.cast((y_pred>0.5), tf.float32)), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import optimizers\nmetrics = [accuracy]\nmlp.compile(optimizer=optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_output = np.vstack(y_train.values)\ny_test_output = np.vstack(y_test.values)\ny_val_output = np.vstack(y_val.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = mlp.fit(X_train_pooled_output, y_train_output, epochs=40, \n                  validation_data=(X_val_pooled_output, y_val_output))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the performance of our trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metric = pd.DataFrame()\ndf_metric['epoch'] = np.arange(len(history.history['loss']))\ndf_metric['loss'] = history.history['loss']\ndf_metric['val_loss'] = history.history['val_loss']\ndf_metric['accuracy'] = history.history['accuracy']\ndf_metric['val_accuracy'] = history.history['val_accuracy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy score on train and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df_metric, x=\"epoch\", y=[\"accuracy\", 'val_accuracy'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary_crossentropy loss on train and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df_metric, x=\"epoch\", y=[\"loss\", 'val_loss'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the model has preformed pretty well and was able to achieve an accuracy over 96% and the losses also reduced to similar values meaning there's no bias or variance issue."},{"metadata":{},"cell_type":"markdown","source":"### Below are some samples from the test dataset which has not been seen by our model till now. Let's see how the model performs on these samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred = (mlp.predict(X_test_pooled_output)>0.5)*1\nacc = [accuracy_score(i,j) for i,j in zip(y_pred, y_test)]\nidx = np.argsort(acc)[::-1]\ndef show(i):\n    print(f'movie: {X_test.title.values[i]}')\n    print(f'description: {X_test.text.values[i]}')\n    y_act_idx = unique[np.where(y_test_output[i]==1)]\n    y_pred_idx = unique[np.where(y_pred[i]==1)]\n    print(f'metric score: {acc[i]}')\n    print(f'actual genre: {y_act_idx}')\n    print(f'predicted genre: {y_pred_idx}')\n    print('\\n', '*'*50, '\\n')\n    \nfor i in idx[:10]:\n    show(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below is the histogram of accuracy scores achieved by the model on the test/unseen dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(acc, nbins=20, labels={'value':'Accuracy score'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Please upvote if you liked this notebook and suggest improvements in the comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}