{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"    The objective of this exercise is to use machine learning techniques to perform supervised learning on Breast Cancer Wisconsin data set provided here. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Lets import required libraries before we proceed further\nimport numpy as np \nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, validation_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score , classification_report , confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c689dfd2b3e91cec405ec3f9c22f5a04b18f760"},"cell_type":"code","source":"#Load data into dataframe\ndfBCancer = pd.read_csv(\"../input/data.csv\")\ndfBCancer.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"076db5454329e8de36375244602c9d24c8a47eba"},"cell_type":"code","source":"dfBCancer.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7c84763f47f5953dc87349af971121ebdae79e9"},"cell_type":"code","source":"dfBCancer.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61b1d1f62aa60ce06f8d460588676a74d44681ae"},"cell_type":"markdown","source":"Few observations after initial scan of the dataset:\n* Dataset contains: 33 columns: 32 of which are numericals where as one is object.\n* Column diagnosis represents the label vector.\n* Column Unnamed: 32 has all the records as Nan ( count for this column is showing zero when called describe function).\n* Column ID has unique value for all the rows and a simple scan of each value of this column suggests that its an unique identifier for each row.\n* Rest 30 column can be divided into blocks of 10 features each representing Mean, Worst and Standard Error values.\n* Its also evident that scale of features representing Mean and Worst values is significantly different from respective \"se\" features."},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"caf08b25a8da5caef4f42ada1e58dbeee7ca4e53"},"cell_type":"code","source":"#Drop Unnamed: 32 as it only contains Nan\n# axis=1: represents column\n#inplace = True : represents whether we want to delete column from this dataframe instance, in inplace=False is specified it will return a new\n#dataframe having column \"Unnamed: 32\" deleted but will not change the original datafram\ndfBCancer.drop([\"Unnamed: 32\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a50fc863946b39b1502eee44d3dda892f418474"},"cell_type":"code","source":"fig, axs = plt.subplots(6, 5, figsize=(16,20))\ndf=dfBCancer.drop([\"id\"], axis=1)\ng = sns.FacetGrid(df)\nk=1\nfor i in range(6):\n    for j in range(5):\n        axs[i][j].set_title (df.columns[k])\n        g.map(sns.boxplot, \"diagnosis\",  df.columns[k],  ax=axs[i][j])\n        k=k+1\nfig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n                    hspace=0.4, wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddf0bb52adcd90816348d7661162dd285f2a8128"},"cell_type":"code","source":"# As we mentioned earlier we can see that above data frame contains block of features representing mean, worst and se\n# In order to analyse these features separately we shall be dividing dfBCancer dataframe into 3 dataframes. \n# This will help us in performing further analysis on this dataset\n\ndfCancer_mean = dfBCancer.drop([\"id\", \"diagnosis\"], axis=1).iloc[:, 0:10]\ndfCancer_se = dfBCancer.drop([\"id\", \"diagnosis\"], axis=1).iloc[:, 10:20]\ndfCancer_worst = dfBCancer.drop([\"id\", \"diagnosis\"], axis=1).iloc[:, 20:30]\n\nprint(dfCancer_mean.columns)\nprint(\"----------------\")\nprint(dfCancer_se.columns)\nprint(\"----------------\")\nprint(dfCancer_worst.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bff46b6ee8d9eee67a2d7ffcf396003e46f1b261"},"cell_type":"markdown","source":"I am interested in analysing if there is any correlation between columns in respective feature buckets. Dividing data into 3 sets makes it easier to perform analysis.\nNote that correlation does not necessary mean, that there is a causal relationship between these features. We shall delve deep into this later."},{"metadata":{"trusted":true,"_uuid":"8e0583652c37f9563f08119e5dd291180b227065"},"cell_type":"code","source":"#Lets draw histogram\nfig, axs = plt.subplots(2, 5, figsize=(12, 8))\n\ng = sns.FacetGrid(dfCancer_mean)\nk=0\nfor i in range(2):\n    for j in range(5):\n        g.map(sns.distplot, dfCancer_mean.columns[k], ax=axs[i][j])\n        k=k+1\nfig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n                    hspace=0.4, wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"837ed2c7ff8245906756e28e245b68f3a61c20d1"},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = dfCancer_mean.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n # Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(14, 14))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, annot= True, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d889f972ce56620315a68816c453447da62e0b8"},"cell_type":"markdown","source":"Wow...we see that radius_mean, perimeter_mean and area_mean feature are highly positvely correlated (.99 & 1). Is there any causal relationship ? Well, if we go by the mathematical definition of radius, mean & area we know that mean and area are dependent on radius and therefore its expected that these features will be highly correlated.\n\nSince these features are highly correlated and there is a causal relationship, we can safely drop 2 of these columns , hoping that these features will not add a lot of information in prediction.\n\nAre there any other features which are correlated as well ?\n\nWe see that concavity_mean is highly correlated with concave points_mean. 0.92.\nwhile concave points_mean seem to be highly positively correlated to radius, area and perimeter features; concavity mean does not.\nConcavity_mean and concave points_mean are also highly positvely correlated to compactness_mean\n\nBasis above analysis, shall be dropping following columns from the bCancer_mean dataframe:\n* perimeter_mean\n* area_mean\n* concave points_mean\n* compactness_mean\n\nLets check if similar relationship exists in other dataframes as well:"},{"metadata":{"trusted":true,"_uuid":"96e03e3f2e5df63efbde941cd5024d396cda109a"},"cell_type":"code","source":"#Lets draw histogram\nfig, axs = plt.subplots(2, 5, figsize=(12, 8))\n\ng = sns.FacetGrid(dfCancer_se)\nk=0\nfor i in range(2):\n    for j in range(5):\n        g.map(sns.distplot, dfCancer_se.columns[k], ax=axs[i][j])\n        k=k+1\nfig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n                    hspace=0.4, wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"864c005f48d381a5acf4ae1e598a7c8ed5783880"},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = dfCancer_se.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n # Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(14, 14))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(150, 275, s=80, l=55, n=9, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, annot= True, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4776c157da5699ec1c19210b91c4cab30b431b8"},"cell_type":"markdown","source":"In the dCancer_se dataframe we do see radius, perimeter and area highly positvely correlated >= 0.95 however in this dataset concavity_se and compactness_se are having correlation =0.8; though compactness_se is correlated to fractal_dimension_se with a correlation of 0.8 , fractal_dimension_se correlated to concavity_se with correlation of 0.73. Concave points_se's correlation to concavity_se and compctness_se is of the order of 0.74/0.77.\n\nBasis above analysis, shall be dropping following columns from the bCancer_mean dataframe:\n* perimeter_se\n* area_se\n* concave points_se\n* compactness_se\n"},{"metadata":{"trusted":true,"_uuid":"374b6350b744e18333950050d87626bcd7dd0907"},"cell_type":"code","source":"#Lets draw histogram\nfig, axs = plt.subplots(2, 5, figsize=(12, 8))\n\ng = sns.FacetGrid(dfCancer_worst)\nk=0\nfor i in range(2):\n    for j in range(5):\n        g.map(sns.distplot, dfCancer_worst.columns[k], ax=axs[i][j])\n        k=k+1\nfig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n                    hspace=0.4, wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"561fba983f22ddfaabeb0d5f39c1c78275db5f5a","collapsed":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = dfCancer_worst.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n # Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(14, 14))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 50, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, annot= True, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0609ecc6c177e38bc74386f58ff5c434f0f792c1"},"cell_type":"markdown","source":"Similar relationship exists in the dCancer_worst datafame therefore we shall be dropping following columns from the bCancer_mean dataframe:\n* perimeter_worst\n* area_worst\n* concave points_worst\n* compactness_worst"},{"metadata":{"trusted":true,"_uuid":"9e25ecadda563aaba7b4315835e3ed6a34738253","collapsed":true},"cell_type":"code","source":"# I shall be combining all these dataframes back into one result dataframe after droppoing these columns, this time I shall not be using \n#inplace =True, as i dont want original dataframe's values to be lost lets concatenate it back\nresult = pd.concat([dfCancer_worst.drop([\"area_worst\", \"perimeter_worst\", \"concave points_worst\" , \"compactness_worst\"], axis=1),\n                   dfCancer_se.drop([\"area_se\", \"perimeter_se\", \"concave points_se\" , \"compactness_se\"], axis=1),\n                   dfCancer_mean.drop([\"area_mean\", \"perimeter_mean\", \"concave points_mean\" , \"concavity_mean\"], axis=1)],\n                   axis=1)\n# check if resulting dataframe as all the dataset combined except the ones which we didnt want\nresult.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f6a1e8c0b7500fd0d323c54baf515502a2872ca"},"cell_type":"markdown","source":"Ok good, we have 18 columns / features in the above dataframe. Lets start learning process now"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd85b3054bf4ad0d71b427dad1f3b40e4279dae6"},"cell_type":"code","source":"# First convert categorical label into qunatitative values for prediction\nfactor = pd.factorize( dfBCancer.diagnosis)\ndiagnosis = factor[0]\ndefinitions = factor[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93f3632b2e9fe6bf1429fae5266630ce389d38cb"},"cell_type":"code","source":"# Split dataset into test and train data\ntrainX, testX, trainY, testY = train_test_split(result, diagnosis, test_size=0.35, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14b3efb60c1a4d6ddd63bbf2b83453db40b1a171"},"cell_type":"markdown","source":"I shall be first calling Logistic Regressor for prediction, however before that do you remember that our dataset had different scales for Mean, worst and se columns; therefore we should standardise data as well. \n\nShould you normalize or Standardize: http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html\nScikit Learn : http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler"},{"metadata":{"trusted":true,"_uuid":"08120c34d69bcd27482a414830fb54404a9592ed","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstdScalar= StandardScaler().fit(trainX)\ntrainX = stdScalar.transform(trainX)\ntestX= stdScalar.transform(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ab74194bc3661d6730a316ca6152f37cdf392fc","collapsed":true},"cell_type":"code","source":"print(\"Mean of trainX: \", trainX.mean(axis=0), \" and standard deviation of trainX: \", trainX.std(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b69903a10aa80da4efd0342fd3b1238792e8784a","collapsed":true},"cell_type":"code","source":"print(\"Mean of testX: \", testX.mean(axis=0), \" and standard deviation of testX: \", testX.std(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bcbe4d6f387cdb9683012484e0ec3b95f3146e2","collapsed":true},"cell_type":"code","source":"regression = LogisticRegression()\nregression.fit(trainX, trainY)\n\npredtrainY = regression.predict(trainX)\n\nprint('Accuracy {:.2f}%'.format(accuracy_score(trainY, predtrainY) * 100))\nprint(classification_report(trainY, predtrainY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5e7830f230c51c710fc1207dbb2e7f3d93870d8"},"cell_type":"markdown","source":"Great We have 98% Accuracy on training dataset. Lets check confusion matrix"},{"metadata":{"trusted":true,"_uuid":"2f8f4f7fdbdde95f823531a93750e71959b89ede","collapsed":true},"cell_type":"code","source":"#Create a Confusion matrix\n\n#Reverse factorize\nreversefactor = dict(zip(range(len(definitions)),definitions))\ny_test = np.vectorize(reversefactor.get)(trainY)\ny_pred = np.vectorize(reversefactor.get)(predtrainY)\ncm = confusion_matrix(y_test, y_pred)\n\n# plot\nfig, ax = plt.subplots()\nax.matshow(cm, cmap=plt.cm.Greens, alpha=0.3)\nax.grid(False)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, s=cm[i,j], va='center', ha='center', fontsize=9)\n\nplt.xlabel('True Predictions')\nplt.ylabel('False Predictions')\nplt.xticks(range(len(definitions)), definitions.values, rotation=90, fontsize=8)\nplt.yticks(range(len(definitions)), definitions.values, fontsize=8)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b17826add003444cd78c5489bccfcc4d6bb266d8","collapsed":true},"cell_type":"code","source":"# use this on the test dataset\npredtestY = regression.predict(testX)\nprint('Accuracy {:.2f}%'.format(accuracy_score(testY, predtestY) * 100))\nprint(classification_report(testY, predtestY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfa59be8b37ef5e9efb751372daa36a502ed4f81"},"cell_type":"markdown","source":"Great we have 98% accuracy Test and Train both dataset. Lets print confusion matrix for test dataset as well"},{"metadata":{"trusted":true,"_uuid":"5058484e1fb52b3d017bd66428db50480beea8b4","collapsed":true},"cell_type":"code","source":"#Create a Confusion matrix\n\n#Reverse factorize\nreversefactor = dict(zip(range(len(definitions)),definitions))\ny_test = np.vectorize(reversefactor.get)(testY)\ny_pred = np.vectorize(reversefactor.get)(predtestY)\ncm = confusion_matrix(y_test, y_pred)\n\n# plot\nfig, ax = plt.subplots()\nax.matshow(cm, cmap=plt.cm.Greens, alpha=0.3)\nax.grid(False)\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, s=cm[i,j], va='center', ha='center', fontsize=9)\n\nplt.xlabel('True Predictions')\nplt.ylabel('False Predictions')\nplt.xticks(range(len(definitions)), definitions.values, rotation=90, fontsize=8)\nplt.yticks(range(len(definitions)), definitions.values, fontsize=8)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7331d308e607e1107231a08dc979e1afdefe0d2d"},"cell_type":"markdown","source":"Prediction using Random Forest, Since Random Forest is a Decision Tree based estimator; we don't need to standardize features:"},{"metadata":{"trusted":true,"_uuid":"9c2d7267b561c84474b12654979666cde7bc2eb6","collapsed":true},"cell_type":"code","source":"randclassifier = RandomForestClassifier(max_depth=13,max_features ='sqrt', n_estimators=50,class_weight=\"balanced\", random_state=42)\n\nrandclassifier.fit(trainX,trainY)\npredtrainY = randclassifier.predict(trainX)\n\nprint('Accuracy {:.2f}%'.format(accuracy_score(trainY, predtrainY) * 100))\nprint(classification_report(trainY, predtrainY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a72d4d999edbfe60d8b5348c13d5951fe49e35d2"},"cell_type":"markdown","source":"Great, We have 100% accuracy on train data , lets check accuracy on the test dataset"},{"metadata":{"trusted":true,"_uuid":"f1e0672f227fb1a0937132d27023fac455e2bd7f","collapsed":true},"cell_type":"code","source":"#use this on the test dataset\n\npredtestY = randclassifier.predict(testX)\n\nprint('Accuracy {:.2f}%'.format(accuracy_score(testY, predtestY) * 100))\nprint(classification_report(testY, predtestY))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecc5a325f94c1b6615035884f51ed986e1d23d94","collapsed":true},"cell_type":"code","source":"#Lets draw validation curve now for Random Forest Model\n\nplt.figure(figsize=(10,8))\n\nparam_range = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n\ntrain_scores, test_scores = validation_curve(\n    randclassifier, result, diagnosis, param_name=\"n_estimators\", param_range=param_range,\n    cv=10, scoring=\"accuracy\", n_jobs=1)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nlw = 2\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(param_range, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,alpha=0.1,\n                     color=\"r\")\nplt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n\n# Create plot\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\n\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.ylim(0.8, 1.0)\nplt.grid(True)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0fe49dfa8e47d3820b9e7f0d40c59cfef48a33b","collapsed":true},"cell_type":"code","source":"svmClassifier = svm.SVC()\nsvmClassifier.fit(trainX,trainY)\npredtrainY=svmClassifier.predict(trainX)\n\nprint('Accuracy {:.2f}%'.format(accuracy_score(trainY, predtrainY) * 100))\nprint(classification_report(trainY, predtrainY))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d1cf1c1f556498107576ec251644414138c3af8","collapsed":true},"cell_type":"code","source":"# use this on the test dataset\npredtestY = svmClassifier.predict(testX)\n\nprint('Accuracy {:.2f}%'.format(accuracy_score(testY, predtestY) * 100))\nprint(classification_report(testY, predtestY))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"35976cab4a64b726240dd66978605be48ae9b832"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}