{"metadata":{"language_info":{"file_extension":".py","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{"_uuid":"93f5620e1a5bd8d3a819f99950a33cee7ebad5af","_cell_guid":"7e912fd5-932c-4bcf-a1a4-2eb4f34635cd"},"cell_type":"markdown","source":"We will use various clustering techniques on the world happiness data. Below are the various clustering techniques we will use.\n\n1.KMeans\n\n2.MeanShift\n\n3.MiniBatchKMeans\n\n4.SpectralClustering\n\n5.DBSCAN\n\n6.Affinity Propagation\n\n7.Birch\n\n8.GaussianMixture"},{"execution_count":null,"metadata":{"_uuid":"87476055ecf8dbf7e3cb444cb98e8bf79c79701f","_cell_guid":"9e3be276-669c-401e-be9e-dfe436195627"},"cell_type":"code","outputs":[],"source":"%reset -f\nimport time                   # To time processes\nimport warnings               # To suppress warnings\n\nimport numpy as np            # Data manipulation\nimport pandas as pd           # Dataframe manipulatio \nimport matplotlib.pyplot as plt                   # For graphics\n\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset\n\nimport os                     # For os related operations\nimport sys \n\nfrom sklearn import metrics\n\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n# %matplotlib inline\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"9cac551ab05acb0c9444af0c0285f6bc7771112a","_cell_guid":"e40715a0-bd6e-4508-9bac-2ac447474c9c"},"cell_type":"code","outputs":[],"source":"X= pd.read_csv(\"../input/2017.csv\", header = 0)"},{"execution_count":null,"metadata":{"_uuid":"7ba7eed83ac92320f5587b2b995908dbbe9a1726","_cell_guid":"1669a0d2-93df-45e7-9225-54f311746129"},"cell_type":"code","outputs":[],"source":"X.columns.values"},{"execution_count":null,"metadata":{"_uuid":"dacb0be4f4bd98d55103df68ce31546c412a4104","_cell_guid":"409b1610-8f33-4e87-8edd-523c6f4240ff"},"cell_type":"code","outputs":[],"source":"X.shape"},{"execution_count":null,"metadata":{"_uuid":"a0fe183bce9b18fda597a774d740969e0a879fc5","_cell_guid":"1f3001f1-258b-4688-88ba-7a770444af77"},"cell_type":"code","outputs":[],"source":"X.dtypes"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"edf214395c21171d96108974456312685287b1ca","_cell_guid":"5f830b13-ce96-4d5a-8599-9ecfcfddfe84"},"cell_type":"code","outputs":[],"source":"X_copy = X\nX = X.iloc[:, 2: ]"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6e66433f6c2e1729e3a7b0ea96916d59b05d51fa","_cell_guid":"ad81d679-7f0c-4f8e-97cc-b013096e6341"},"cell_type":"code","outputs":[],"source":"## Function for various clusters\ndef compute_cluster (clType,df= X):\n    if clType=='KMeans':\n        result = cluster.KMeans(n_clusters= 2).fit_predict(df) \n    elif clType == 'MeanShift' :\n        result = cluster.MeanShift(bandwidth=0.2).fit_predict(df)    \n    elif clType == 'MiniBatchKMeans':\n        result = cluster.MiniBatchKMeans(n_clusters=2).fit_predict(df)        \n    elif clType == 'SpectralClustering':\n        result = cluster.SpectralClustering(n_clusters=2).fit_predict(df)\n    elif clType == 'DBSCAN':\n        result = cluster.DBSCAN(eps=0.3).fit_predict(df)\n    elif clType == 'Affinity Propagation':\n        result = cluster.AffinityPropagation(damping=0.9, preference=-200).fit_predict(df)\n    elif clType == 'Birch':\n        result = cluster.Birch(n_clusters= 2).fit_predict(df)\n    elif clType == 'GaussianMixture' :\n        gmm = mixture.GaussianMixture( n_components=2, covariance_type='full')\n        gmm.fit(df)\n        result = gmm.predict(df)  \n    else:\n        print(\"exit\")\n    \n    cl_df.loc[cl_df.Name == clType, 'Silhouette-Coeff'] = metrics.silhouette_score(df, result, metric='euclidean')\n    cl_df.loc[cl_df.Name == clType, 'Calinski-Harabaz'] = metrics.calinski_harabaz_score(df, result)\n    \n    return result"},{"metadata":{"_uuid":"2c21a23107f91e05de116ac75ffee3e79582ec28","_cell_guid":"0363c474-0ecd-40a9-8f38-0fe001f13953"},"cell_type":"markdown","source":"Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)."},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f3ae172e584c036828f129604679e427ba34ff5a","_cell_guid":"1c7895bb-7d54-4d3d-a01c-d03de6ca89dd"},"cell_type":"code","outputs":[],"source":"ss = StandardScaler().fit_transform(X)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"56b79f60074cfba8553832a0cc88e0f55dec53d0","_cell_guid":"c6970304-7716-4f23-af97-6d912659c484"},"cell_type":"code","outputs":[],"source":"cl_dist = {'Name' : ['KMeans','MeanShift','MiniBatchKMeans','SpectralClustering','DBSCAN','Affinity Propagation','Birch','GaussianMixture']}\ncl_df = pd.DataFrame(cl_dist)\ncl=pd.Series(['KMeans','MeanShift','MiniBatchKMeans','SpectralClustering','DBSCAN','Affinity Propagation','Birch','GaussianMixture'])"},{"execution_count":null,"metadata":{"_uuid":"bfb854f987ca16eda4d32713e1c297da82e36bff","_cell_guid":"889ce439-8f09-41a2-830d-108148eb8546"},"cell_type":"code","outputs":[],"source":"X.head(5)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"1d1696eda48873f415aa5fcf79078c0f584f78a5","_cell_guid":"6b0c2fd6-cb08-437e-9aac-cba7edd88843"},"cell_type":"code","outputs":[],"source":"for i in range(0,cl.size) :\n    result = compute_cluster(clType=cl[i])\n    X[cl[i]] = pd.DataFrame(result)"},{"execution_count":null,"metadata":{"_uuid":"33d390391ddf0a67d9c38f4ab7060f77a981ee4f","_cell_guid":"3ac8245e-93d9-4b52-ae38-4f8705efa2e3"},"cell_type":"code","outputs":[],"source":"cl_df"},{"metadata":{"_uuid":"6a6701de7e2a90f9f9ad80fb6254b5217feabffd","_cell_guid":"c23b3fb2-32f4-4c36-890c-642f48711a46"},"cell_type":"markdown","source":"The **Silhouette Coefficient** (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n\na: The mean distance between a sample and all other points in the same class.\nb: The mean distance between a sample and all other points in the next nearest cluster.\nThe Silhouette Coefficient s for a single sample is then given as:\ns = {b - a}/{max(a, b)}\n\nThe score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\nThe score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n\nthe **Calinski-Harabaz index** (sklearn.metrics.calinski_harabaz_score) can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.\n\nFor k clusters, the Calinski-Harabaz score s is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion.\nThe score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."},{"execution_count":null,"metadata":{"_uuid":"45f42ac93eb6238d69b7aea08907652c302fe842","_cell_guid":"be7ce5d4-cdeb-4d5b-90d3-310b21da9481"},"cell_type":"code","outputs":[],"source":"rows = 4    # No of rows for the plot\ncols = 2    # No of columns for the plot\n\n# 4 X 2 plot\nfig,ax = plt.subplots(rows,cols, figsize=(10, 10)) \nx = 0\ny = 0\nfor i in cl:\n    ax[x,y].scatter(X.iloc[:, 6], X.iloc[:, 5],  c=X.iloc[:, 12+(x*y)])\n    ax[x,y].set_title(i + \" Cluster Result\")\n    y = y + 1\n    if y == cols:\n        x = x + 1\n        y = 0\n        plt.subplots_adjust(bottom=-0.5, top=1.5)\nplt.show()"}],"nbformat_minor":1,"nbformat":4}