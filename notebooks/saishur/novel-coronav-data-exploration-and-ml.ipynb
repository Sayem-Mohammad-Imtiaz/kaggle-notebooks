{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"CoronaV = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/2019_nCoV_data.csv')\nprint(CoronaV.head(10))\nprint('\\n')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CoronaV.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize the dataset\nCoronaV = CoronaV.drop('Sno', axis = 1)\nCoronaV.columns = ['State', 'Country', 'Date', 'Confirmed', 'Deaths', 'Recovered']\nCoronaV['Date'] = CoronaV['Date'].apply(pd.to_datetime).dt.normalize() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV[['State','Country','Date','Confirmed']].drop_duplicates().shape[0] == CoronaV.shape[0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV[['Country','State']][CoronaV['State'].isnull()].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV[CoronaV['Country'].isin(list(CoronaV[['Country','State']][CoronaV['State'].isnull()]['Country'].unique()))]['State'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV.State.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV.Country.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CoronaV[CoronaV['Country'].isin(['China', 'Mainland China'])].groupby('Country')['State'].unique())\nprint(CoronaV[CoronaV['Country'].isin(['China', 'Mainland China'])].groupby('Country')['Date'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV['Country'] = CoronaV['Country'].replace(['Mainland China'], 'China') #set 'Mainland China' to 'China'\nsorted(CoronaV.Country.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CoronaV.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = CoronaV[CoronaV['Country']=='China']\nchina.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (12,9)\nax1 = china[['Date','Confirmed']].groupby(['Date']).sum().plot()\nax1.set_ylabel(\"Total Number of Confirmed Cases\")\nax1.set_xlabel(\"Date\")\n\nax2 = china[['Date','Deaths', 'Recovered']].groupby(['Date']).sum().plot()\nax2.set_ylabel(\"Total N\")\nax2.set_xlabel(\"Date\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=3, specs=[[{\"type\" : \"pie\"}, {\"type\" : \"pie\"},{\"type\" : \"pie\"}]],\n                    subplot_titles=(\"number of provience in countries\", \"Deaths\", \"Recovers\"))\n\nfig.add_trace(\n    go.Pie(labels=CoronaV.groupby('Country')['State'].nunique().sort_values(ascending=False)[:10].index,\n           values=CoronaV.groupby('Country')['State'].nunique().sort_values(ascending=False)[:10].values),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(labels=CoronaV[CoronaV.Deaths > 0].groupby('Country')[\"Deaths\"].sum().index,\n           values=CoronaV[CoronaV.Deaths > 0].groupby('Country')[\"Deaths\"].sum().values),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Pie(labels=CoronaV.groupby('Country')[\"Recovered\"].sum().sort_values(ascending=False).index[:4],\n           values=CoronaV.groupby('Country')[\"Recovered\"].sum().sort_values(ascending=False).values[:4]),\n    row=1, col=3\n)\n\nfig.update_layout(height=400, showlegend=True)\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV['Date'] = pd.to_datetime(CoronaV['Date'])\nCoronaV['Day'] = CoronaV['Date'].apply(lambda x : x.day)\nCoronaV['Hour'] = CoronaV['Date'].apply(lambda x : x.hour)\n\nCoronaV = CoronaV[CoronaV['Confirmed'] != 0]\nCoronaV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_case = CoronaV.groupby('Country')['Confirmed','Deaths','Recovered'].sum().reset_index()\nglobal_case.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CoronaV.groupby(['Date','Country']).agg({\n    'Confirmed': pd.Series.nunique,\n}).reset_index().pivot(index='Date',columns='Country',values='Confirmed').plot.barh(stacked=True,figsize=(26,10),colormap='gist_rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (17,10)\nnums = china.groupby([\"State\"])['Confirmed'].aggregate(sum).reset_index().sort_values('Confirmed', ascending= False)\nax = sns.barplot(x=\"Confirmed\", y=\"State\", order = nums['State'], data=china, ci=None) \nax.set_xlabel(\"Total Confirmed Cases\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ci(N,p):\n    lci = (p - 1.96*(((p*(1-p))/N) ** 0.5))*100\n    uci = (p + 1.96*(((p*(1-p))/N) ** 0.5))*100\n    return str(np.round(lci,3)) + \"% - \" + str(np.round(uci,3)) + '%'\n\nfinal = CoronaV[CoronaV.Date==np.max(CoronaV.Date)]\nfinal = final.copy()\n\nfinal['CFR'] = np.round((final.Deaths.values/final.Confirmed.values)*100,3)\nfinal['CFR 95% CI'] = final.apply(lambda row: get_ci(row['Confirmed'],row['CFR']/100),axis=1)\nglobal_cfr = np.round(np.sum(final.Deaths.values)/np.sum(final.Confirmed.values)*100, 3)\nfinal.sort_values('CFR', ascending= False).head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tops = final.sort_values('CFR', ascending= False)\ntops = tops[tops.CFR >0]\ndf = final[final['CFR'] != 0]\nplt.rcParams[\"figure.figsize\"] = (10,5)\nax = sns.barplot(y=\"CFR\", x=\"State\", order = tops['State'], data=df, ci=None) \nax.axhline(global_cfr, alpha=.5, color='r', linestyle='dashed')\nax.set_title('Case Fatality Rates (CFR) as of 30 Jan 2020')\nax.set_ylabel('CFR %')\nprint('Average CFR % = ' + str(global_cfr))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nscaler = StandardScaler()\nscd = scaler.fit_transform(final[['Confirmed','Deaths','Recovered']])\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1) #LOF is very sensitive to the choice of n_neighbors. Generally, n_neighbors = 20 works better\nclf.fit(scd)\nlofs = clf.negative_outlier_factor_*-1\nfinal['LOF Score'] = lofs\ntops = final.sort_values('LOF Score', ascending= False)\nplt.rcParams[\"figure.figsize\"] = (20,12)\nax = sns.barplot(x=\"LOF Score\", y=\"State\", order = tops['State'], data=final, ci=None) \nax.axvline(1, alpha=.5, color='g', linestyle='dashed')\nax.axvline(np.median(lofs), alpha=.5, color='b', linestyle='dashed')\nax.axvline(np.mean(lofs) + 3*np.std(lofs), alpha=.5, color='r', linestyle='dashed')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.sort_values('LOF Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nplt.rcParams[\"figure.figsize\"] = (5,5)\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=1897)\n    kmeans.fit(scd)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within Cluster Sum of Squares')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=1897)\nclusters = np.where(kmeans.fit_predict(scd) == 0, 'Cluster 1', 'Cluster 2')\nclusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import decomposition\npca = decomposition.PCA(n_components=3)\npca.fit(scd)\nX = pca.transform(scd)\nprint(pca.explained_variance_ratio_.cumsum())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (7,7)\nax = sns.scatterplot(X[:,0], X[:,1], marker = 'X', s = 80, hue=clusters)\nax.set_title('K-Means Clusters of States')\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(final.State.values, clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = CoronaV['Deaths'].values.reshape(-1,1)\ny = CoronaV['Recovered'].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To retrieve the intercept:\nprint(regressor.intercept_)#For retrieving the slope:\nprint(regressor.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as seabornInstance\nplt.figure(figsize=(15,10))\nplt.tight_layout()\nseabornInstance.distplot(CoronaV['Recovered'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split training feature and target sets into training and validation subsets\nfrom sklearn.model_selection import train_test_split\n\nX_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train_scale, y_train, random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import machine learning algorithms\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)\nX_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential([Dense(32, activation='relu', input_shape=(1,)), Dense(32, activation='relu'), Dense(1, activation='sigmoid'),])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(X_train, Y_train, batch_size=32, epochs=500, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, Y_test)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2 = Sequential([Dense(1000, activation='relu', input_shape=(1,)), Dense(1000, activation='relu'), Dense(1000, activation='relu'), Dense(1000, activation='relu'),    Dense(1, activation='sigmoid'),])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_2 = model_2.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist_2.history['accuracy'])\nplt.plot(hist_2.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\nfrom keras import regularizers\n\nmodel_3 = Sequential([Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(1,)),    Dropout(0.3),    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),    Dropout(0.3),    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)), Dropout(0.3), Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)), Dropout(0.3), Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),])\nmodel_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhist_3 = model_3.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist_3.history['loss'])\nplt.plot(hist_3.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.ylim(top=1.2, bottom=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist_3.history['accuracy'])\nplt.plot(hist_3.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_3.evaluate(X_test, Y_test)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nclf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\ny_pred = cross_val_predict(clf, X, y, cv=10)\nconf_mat = confusion_matrix(y, y_pred)\n\nconf_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint(conf_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(conf_mat, annot=True, annot_kws={\"size\": 10}) # font size\nplt.ylabel('Actual')\nplt.xlabel('Predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score \n# True Positives\nTP = confusion[1, 1]# True Negatives\nTN = confusion[0, 0]# False Positives\nFP = confusion[0, 1]# False Negatives\nFN = confusion[1, 0]\n\nprint((TP + TN) / float(TP + TN + FP + FN))\nprint(accuracy_score(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nprint(\"Precision Score : \",precision_score(y, y_pred, \n                                           pos_label='positive',\n                                           average='micro'))\nprint(\"Recall Score : \",recall_score(y, y_pred, \n                                           pos_label='positive',\n                                           average='micro'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}