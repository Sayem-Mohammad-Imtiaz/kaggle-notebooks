{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing DataFrame\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None) \ndf_ = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf = df_.drop(['id'], axis=1)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform a sweetviz analysis as that will give you most EDA information in one line\n<!-- # Analyzing data -->\nreport=sv.analyze(df)\n<!-- # Generating report -->\nreport.show_html('report.html')","metadata":{}},{"cell_type":"markdown","source":"> There is one \"Other\" gender since it does give us much info get rid of it\n> > Do the dep and indep variable split","metadata":{}},{"cell_type":"code","source":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)\ny = df['stroke']\ndf.drop(['stroke'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# There are null values in BMI so do the mean Imputing","metadata":{}},{"cell_type":"code","source":"# Impute BMI\ndf['bmi'].fillna((df['bmi'].mean()), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# See a Pairplot and understand the Data distribution\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(data=df_, hue=\"Residence_type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Doing One hot Encoding always with a Sklearn package is not a good thing because we miss some level of fine tuning.\nHere I decided to drop \"never_worked\" from \"work_type\" and \"Unknown\" from \"Smoking\" rather than dropping the first column always.\n\nDo the minMaxScaling on Age, Glucose_level, BMI\n\nChange Gender, Maried, Residence_type from Object to label encoded Int\n\nThen drop the unnecessary columns","metadata":{}},{"cell_type":"code","source":"# Reframing and removing Gender\ndf.loc[(df.gender == 'Male'),'gender'] = 1\ndf.loc[(df.gender == 'Female'),'gender'] = 0\n\n\n# Reframing and removing Married\ndf.loc[(df.ever_married == 'Yes'),'ever_married'] = 1\ndf.loc[(df.ever_married == 'No'),'ever_married'] = 0\n\n# Reframing and removing residenceType\ndf.loc[(df.Residence_type == 'Urban'),'Residence_type'] = 1\ndf.loc[(df.Residence_type == 'Rural'),'Residence_type'] = 0\n\nfrom sklearn.preprocessing import MinMaxScaler\nmin_max_scaler = MinMaxScaler()\n\n# Scale Age, GlucoseLevel, Bmi\ndf[['age', 'avg_glucose_level', 'bmi']] = min_max_scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])\n\n# Create columns for work type\ndf[['children', 'govt_job', 'private', 'self_employed']] = 0\n\n\n# Filling based on condition\nfor index, row in df.iterrows():\n    if row['work_type'] == 'Private':\n        row['private'] = 1\n    elif row['work_type'] == 'Self-employed':\n        row['self_employed'] = 1\n    elif row['work_type'] == 'children':\n        row['children'] = 1\n    elif row['work_type'] == 'Govt_job':\n        row['govt_job'] = 1\n        \ndf.drop(['work_type'], axis=1, inplace=True)        \n        \n# Create columns for smoking status\ndf[['never_smoked', 'formerly_smoked', 'smoked']] = 0\n\n\n# Filling based on condition\nfor index, row in df.iterrows():\n    if row['smoking_status'] == 'never smoked':\n        row['never_smoked'] = 1\n    elif row['smoking_status'] == 'formerly smoked':\n        row['formerly_smoked'] = 1\n    elif row['smoking_status'] == 'smokes':\n        row['smoked'] = 1\n        \ndf.drop(['smoking_status'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Although you change the data column data type will not change so we have to apply those changes also.**","metadata":{}},{"cell_type":"code","source":"df[[\"gender\", \"ever_married\", \"Residence_type\"]] = df[[\"gender\", \"ever_married\", \"Residence_type\"]].apply(pd.to_numeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Since we are working with highly imbalanced data we have to do startified split to understand the dynamics","metadata":{}},{"cell_type":"code","source":"# Test train Split with Stratify\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=0, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's implement and see various algorithms to pick the best one","metadata":{}},{"cell_type":"markdown","source":"**Implementing KNN**\n","metadata":{}},{"cell_type":"code","source":"# Testing with KNN Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\nneigh.fit(x_train, y_train)\n\nknn_pred = neigh.predict(x_test)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(neigh, x_test, y_test))\nprint(metrics.classification_report(y_test, knn_pred, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN report\n\nAccuracy is high.. But this is an imbalanced set so focus on other aspects like Precision, Recall, and F1 Scores...\n\nFor 0's it's ok But that's not the case for 1's\n\nSo, this is not recommended","metadata":{}},{"cell_type":"markdown","source":"# Implementing Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Testing with GNB Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nGNB = GaussianNB()\nGNB.fit(x_train, y_train)\n\ngnb_pred = GNB.predict(x_test)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(GNB, x_test, y_test))\nprint(metrics.classification_report(y_test, gnb_pred, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**More or Less GNB is also falling in the same category. So, there is something critically wrong with the data**\n\n**It's not our steps of preprocessing but the distribution of data.**\n\nSo, we need to correct this imbalance and we can do that using SMOTE from imblearn package","metadata":{}},{"cell_type":"code","source":"# Since its doing worst lets try smote and retrain\n!pip install imblearn\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_resampled, y_resampled = oversample.fit_resample(df, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we have the Smoted data Let's see its work on six Algorithms","metadata":{}},{"cell_type":"code","source":"# Test train Split with Stratify\n\nfrom sklearn.model_selection import train_test_split\n\nx_train_resampled, x_test_resampled, y_train_resampled, y_test_resampled = train_test_split(X_resampled, y_resampled,\n                                                    test_size=0.33,\n                                                    random_state=0,\n                                                    stratify=y_resampled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with KNN Algorithm after smote\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh2 = KNeighborsClassifier()\nneigh2.fit(x_train_resampled, y_train_resampled)\n\nknn_pred_resampled = neigh2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(neigh2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, knn_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can immediately notice that there is little reduction in Accuracy**\n**Along with that we also need to make note that there is a significant raise in Precision, Recall, and F1-Scores. So, this is much more sophisticated than the previous models**","metadata":{}},{"cell_type":"markdown","source":"# Applying GNB on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with GNB Algorithm after smote\nfrom sklearn.naive_bayes import GaussianNB\nGNB2 = GaussianNB()\nGNB2.fit(x_train_resampled, y_train_resampled)\n\ngnb_pred_resampled = GNB2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(GNB2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, gnb_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Logistic Regression on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with Logistic Regression Algorithm after smote\nfrom sklearn.linear_model import LogisticRegression\nLR2 = LogisticRegression()\nLR2.fit(x_train_resampled, y_train_resampled)\n\nlr_pred_resampled = LR2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(LR2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, lr_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Decision Tree on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with Decision Tree Algorithm after smote\nfrom sklearn.tree import DecisionTreeClassifier\nDT2 = DecisionTreeClassifier()\nDT2.fit(x_train_resampled, y_train_resampled)\n\ndt_pred_resampled = DT2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(DT2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, dt_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying random Forest on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with Random Forest Algorithm after smote\nfrom sklearn.ensemble import RandomForestClassifier\nRF2 = RandomForestClassifier()\nRF2.fit(x_train_resampled, y_train_resampled)\n\nrf_pred_resampled = RF2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(RF2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, rf_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Support Vector Machine on Smoted Data","metadata":{}},{"cell_type":"code","source":"# Testing with SVC Algorithm after smote\nfrom sklearn.svm import SVC\nSVC2 = SVC()\nSVC2.fit(x_train_resampled, y_train_resampled)\n\nsvc_pred_resampled = SVC2.predict(x_test_resampled)\nfrom sklearn import metrics\nprint(metrics.plot_confusion_matrix(SVC2, x_test_resampled, y_test_resampled))\nprint(metrics.classification_report(y_test_resampled, svc_pred_resampled, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# By seeing all the Confusion matrices and Classification Reports we cacn strongly say that Decision Tree and random Forest are performing great.\n**On top of that Random forest is False Negatives So it's better to opt Random Forest. Because it is better to alert a normal person than not alerting a patient**\n\n**Lets place all the matrices in one plot and consider this for review**","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn import metrics\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,10))\n\nclassifiers = [neigh2, GNB2, LR2, DT2, RF2, SVC2]\n\nfor cls, ax in zip(classifiers, axes.flatten()):\n    metrics.plot_confusion_matrix(cls, x_test_resampled, y_test_resampled, \n                          ax=ax)\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# the End","metadata":{}},{"cell_type":"code","source":"|","metadata":{},"execution_count":null,"outputs":[]}]}