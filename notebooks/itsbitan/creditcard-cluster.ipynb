{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check the missing data\ncreditcard_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets deal with missing values\ncreditcard_df['MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].fillna(creditcard_df['MINIMUM_PAYMENTS'].mean())\ncreditcard_df['CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].fillna(creditcard_df['CREDIT_LIMIT'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We note that there are no missing data","metadata":{}},{"cell_type":"code","source":"# Let's see if we have duplicated entries in the data\ncreditcard_df.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's drop Customer ID since it has no meaning here \ncreditcard_df.drop('CUST_ID', axis =1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(creditcard_df.columns)\nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check the correlation and heat map\ncorr = creditcard_df.corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize = (15,12))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            annot=True,fmt='.2f',linewidths=0.30,\n            cmap = colormap, linecolor='white')\nplt.title('Correlation of df Features', y = 1.05, size=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's scale the data first\nsc = StandardScaler()\ncreditcard_df_scaled = sc.fit_transform(creditcard_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df_scaled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creditcard_df_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_1 = []\n\nrange_values = range(1, 20)\n\nfor i in range_values:\n  kmeans = KMeans(n_clusters = i)\n  kmeans.fit(creditcard_df_scaled)\n  scores_1.append(kmeans.inertia_) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(scores_1, 'bx-')\nplt.title('Finding the right number of cluster')\nplt.xlabel('Cluster')\nplt.ylabel('Scores')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(8)\nkmeans.fit(creditcard_df_scaled)\nlabels = kmeans.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.cluster_centers_.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [creditcard_df.columns])\ncluster_centers   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_kmeans = kmeans.fit_predict(creditcard_df_scaled)\ny_kmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate the clusters labels to our original dataframe\ncreditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of various clusters\nfor i in creditcard_df.columns:\n  plt.figure(figsize = (25, 5))\n  for j in range(8):\n    plt.subplot(1,8,j+1)\n    cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{}    \\nCluster {} '.format(i,j))\n  \n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(creditcard_df_scaled)\nprincipal_comp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data=principal_comp, columns=['pca1', 'pca2'])\npca_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink','yellow','gray','purple', 'black'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom keras.optimizers import SGD\n\nencoding_dim = 7\n\ninput_df = Input(shape=(17,))\n\n\n# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution \n\nx = Dense(encoding_dim, activation='relu')(input_df)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nencoded = Dense(10, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\ndecoded = Dense(17, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n#encoder - used for our dimention reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer= 'adam', loss='mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder.fit(creditcard_df_scaled, creditcard_df_scaled, batch_size = 128, epochs = 25,  verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder.save_weights('autoencoder.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = encoder.predict(creditcard_df_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_2 = []\n\nrange_values = range(1, 20)\n\nfor i in range_values:\n  kmeans = KMeans(n_clusters= i)\n  kmeans.fit(pred)\n  scores_2.append(kmeans.inertia_)\n\nplt.plot(scores_2, 'bx-')\nplt.title('Finding right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('scores') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(scores_1, 'bx-', color = 'r')\nplt.plot(scores_2, 'bx-', color = 'g')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(4)\nkmeans.fit(pred)\nlabels = kmeans.labels_\ny_kmeans = kmeans.fit_predict(creditcard_df_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cluster_dr = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)\ndf_cluster_dr.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\nprin_comp = pca.fit_transform(pred)\npca_df = pd.DataFrame(data = prin_comp, columns =['pca1','pca2'])\npca_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','deeppink'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}