{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Fake news classification\n\nKaggle dataset: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\n\nGiven some news informations, can we predict if the news is fake or real ?\n\nFor each news, we have its content (text), its title, the date it's been published, and its subject.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 7)\n\ndf_fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\ndf_true = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\n\ndf_fake.date = pd.to_datetime(df_fake.date, errors='coerce')  # Some dates are links to article\ndf_true.date = pd.to_datetime(df_true.date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the data\nWe first take a look at the data, to see what we're dealing with.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Some fake news:')\nfor title, content in zip(df_fake.title[:5], df_fake.text[:5]):\n    print(f'Title: {title}')\n    print(f'Content: {content[:100]}...\\n')\n    \nprint('\\n\\nSome true news:')\nfor title, content in zip(df_true.title[:5], df_true.text[:5]):\n    print(f'Title: {title}')\n    print(f'Content: {content[:100]}...\\n')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"num_fake = len(df_fake)\nnum_true = len(df_true)\nprint(f'Total fake news: {num_fake:,}')\nprint(f'Total real news: {num_true:,}')\n\nmin_date = min(df_fake.date.min(), df_true.date.min())\nmax_date = max(df_fake.date.max(), df_true.date.max())\nprint(f'News published between {min_date} and {max_date}.')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import math  # math.isnan\nfrom collections import defaultdict\n\n# Fake news count\ndate_count = defaultdict(int)\nfor date in df_fake.date:\n    date_count[(date.year, date.month//4)] += 1  # Group months by 4\n\nX, Y = [], []\nfor year, month in sorted(date_count.keys()):\n    if math.isnan(year) or math.isnan(month):\n        continue  # Some dates are invalid\n        \n    X.append(f'{year} - {month+1}')\n    Y.append(date_count[(year, month)])\n\nplt.bar(X, Y, label='Fake news', alpha=0.8)\n\n# Real news count\ndate_count = defaultdict(int)\nfor date in df_true.date:\n    date_count[(date.year, date.month//4)] += 1  # Group months by 4\n\nX, Y = [], []\nfor year, month in sorted(date_count.keys()):\n    if math.isnan(year) or math.isnan(month):\n        continue  # Some dates are invalid\n        \n    X.append(f'{year} - {month+1}')\n    Y.append(date_count[(year, month)])\n\nplt.bar(X, Y, label='Real news', alpha=0.7)\n\nplt.title('Total news over time')\nplt.xticks(rotation='45')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(min_df=100, stop_words='english')\npylab.rcParams['figure.figsize'] = (20, 14)\npylab.rcParams['font.size'] = 14\n\n# Fake news\nvect.fit(df_fake.title)\nwc = WordCloud(width=1920, height=1080).generate_from_frequencies(vect.vocabulary_)\n\nplt.subplot(2, 1, 1)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most frequent words in fake news titles')\n\n# Real news\nvect.fit(df_true.title)\nwc = WordCloud(width=1920, height=1080).generate_from_frequencies(vect.vocabulary_)\n\nplt.subplot(2, 1, 2)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most frequent words in true news titles')\n\nplt.show()\npylab.rcParams['figure.figsize'] = (10, 7)  # Default parameters\npylab.rcParams['font.size'] = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification using only the title\n\nThe goal is to see if the titles alone can help the classification process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we create the train and test sets. We use 80% of the whole dataset to train our model, and 20% to test it.\n\nWe also define some score functions, to evaluate our performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\n# Create train and test datasets\nrandom.seed(0)\n\nX = list(df_fake.title) + list(df_true.title)\ny = [1 for _ in range(len(df_fake))] + [0 for _ in range(len(df_true))]\n\nindex = [i for i in range(len(X))]\nrandom.shuffle(index)\nX, y = [X[i] for i in index], [y[i] for i in index] # Shuffle both lists adequatly\n\nprct_train = 0.8\ncut = int(len(X) * prct_train)\nX_train, y_train = X[:cut], y[:cut]\nX_test, y_test = X[cut:], y[cut:]\n\nprint(f'Training on {len(X_train):,} exemples ({int(100*prct_train)}% of the dataset).')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval functions\n\ndef accuracy(y_pred, y_test):\n    score = [1 if pred == test else 0 for pred, test in zip(y_pred, y_test)]\n    return sum(score) / len(y_pred)\n\ndef recall(y_pred, y_test):\n    score = [1 if pred == 1 and test == 1 else 0 for pred, test in zip(y_pred, y_test)]\n    return sum(score) / sum(y_test)\n\ndef F1(y_pred, y_test):\n    acc, rec = accuracy(y_pred, y_test), recall(y_pred, y_test)\n    return 2 * acc * rec / (acc + rec)\n\ndef print_scores(y_pred, y_test):\n    acc = accuracy(y_pred, y_test)\n    rec = recall(y_pred, y_test)\n    f1 = F1(y_pred, y_test)\n    \n    print('Accuracy\\tRecall\\t\\tF1')\n    print(f'{round(acc, 2)}\\t\\t{round(rec, 2)}\\t\\t{round(f1, 2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the first classifier, we will use the multinomial naive bayes algorithm, which is powerful for basic document classification.\n\nOur data will be vectorized using the CountVectorizer and the tf-idf representation.\n\nWe create a pipeline for the tf-idf vectorization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('count', CountVectorizer()),\n                 ('tfid', TfidfTransformer())]).fit(X_train)\n\ncount_train = pipe['count'].transform(X_train)\ncount_test = pipe['count'].transform(X_test)\n\ntfidf_train = pipe.transform(X_train)\ntfidf_test = pipe.transform(X_test)\n\nprint(f'Vocabulary size: {tfidf_train.shape[1]}')\nprint(f'Matrix shape: {tfidf_train.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# CountVectorizer\nclassifier = MultinomialNB()\nclassifier.fit(count_train, y_train)\n\ny_pred = classifier.predict(count_test)\nprint('CountVectorizer scores:')\nprint_scores(y_pred, y_test)\n\n# Tf-idf\nclassifier = MultinomialNB()\nclassifier.fit(tfidf_train, y_train)\n\ny_pred = classifier.predict(tfidf_test)\nprint('\\nTf-idf scores:')\nprint_scores(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**95%** of F1 score is pretty good for such a basic model !\n\nWe could think that because the tf-idf representation is more complex than the simple count vector, it would have better performance. But we can see that here, it is pointless to go for the tf-idf representation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = classifier.feature_log_prob_\nprob_true = np.exp(probs[0, :])  # True news features prob\nprob_fake = np.exp(probs[1, :])  # Fake news features prob\nfeatures = pipe['count'].get_feature_names()\n\npylab.rcParams['figure.figsize'] = (20, 14)\npylab.rcParams['font.size'] = 14\n\n\ntrue_features = {word: prob for prob, word in zip(prob_true, features)}\nwc = WordCloud(width=1920, height=1080).generate_from_frequencies(true_features)\n\nplt.subplot(2, 1, 1)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most impactful words for true news prediction')\n\nfake_features = {word: prob for prob, word in zip(prob_fake, features)}\nwc = WordCloud(width=1920, height=1080).generate_from_frequencies(fake_features)\n\nplt.subplot(2, 1, 2)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most impactful words for fake news prediction')\npylab.rcParams['figure.figsize'] = (10, 7)  # Default parameters\npylab.rcParams['font.size'] = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I draw what deduced the multinomial naive bayes classifier after training. It is what seems to be the most impactful words when making the prediction for one class or the other.\nAccording to the naive bayes algorithm, those words are here because of their frequencies in each classes and because of their overall frequencies.\n\nI'm not sure of what I did though, if you have a better explaination of what I draw, please tell me.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Classification using the news text\n\nNow we want to see if the text alone is useful for the classification process, and if it does better than the model trained with the titles only.\nWe'll use the same model.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X = list(df_fake.text) + list(df_true.text)\ny = [1 for _ in range(len(df_fake))] + [0 for _ in range(len(df_true))]\n\nindex = [i for i in range(len(X))]\nrandom.shuffle(index)\nX, y = [X[i] for i in index], [y[i] for i in index] # Shuffle both lists adequatly\n\nprct_train = 0.8\ncut = int(len(X) * prct_train)\nX_train, y_train = X[:cut], y[:cut]\nX_test, y_test = X[cut:], y[cut:]\n\nprint(f'Training on {len(X_train):,} exemples ({int(100*prct_train)}% of the dataset).')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because our texts are much bigger compared to titles, our vocabulary size is also bigger. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('count', CountVectorizer()),\n                 ('tfid', TfidfTransformer())]).fit(X_train)\n\ncount_train = pipe['count'].transform(X_train)\ncount_test = pipe['count'].transform(X_test)\n\ntfidf_train = pipe.transform(X_train)\ntfidf_test = pipe.transform(X_test)\n\nprint(f'Vocabulary size: {tfidf_train.shape[1]:,}')\nprint(f'Matrix shape: {tfidf_train.shape}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# CountVectorizer\nclassifier = MultinomialNB()\nclassifier.fit(count_train, y_train)\n\ny_pred = classifier.predict(count_test)\nprint('CountVectorizer scores:')\nprint_scores(y_pred, y_test)\n\n# Tf-idf\nclassifier = MultinomialNB()\nclassifier.fit(tfidf_train, y_train)\n\ny_pred = classifier.predict(tfidf_test)\nprint('\\nTf-idf scores:')\nprint_scores(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the text based model has approximatly the same performance as the title based one. So here the best model would be the title based one, since it uses lower dimension matrices, and therefore use less memory and needs less computation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis dataset seems pretty easy to use with the simplest document classification algorithm.\nWhen rapidly looking at other kernels, it seems that a more sophiscated approach can give better results up to 99% accuracy. Those methods try to actually have a better understanding of the news content.\nHere, with methods based only on word frequencies, I don't think we can gain much precision over our results.\n\nHere, we trained the model using any 80% of the whole dataset. We could try to train only the oldest news, and try our predictions on the later news. It would give a better point of view (I guess) if we had to try our algorithm on nowadays news.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}