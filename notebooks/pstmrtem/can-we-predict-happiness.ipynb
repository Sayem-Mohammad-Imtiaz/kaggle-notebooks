{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello ! So for my first kernel ever, I will try to do some data visualization and to predict happiness with the given columns. There's a first time for everything, and after reading a lot on data science, I will try to do some work. Let's go !"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 7)\n\ndf = pd.read_csv(\"../input/happiness-and-alcohol-consumption/HappinessAlcoholConsumption.csv\", sep=\",\", engine=\"python\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the data first."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print(\"{} countries\".format(len(df)))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the happiness distribution, and some of the most/least happy country and others."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"HappinessScore\"]\nplt.hist(X, bins=30)\nplt.title(\"Happiness Score Histogram\")\nplt.xlabel(\"Hapiness Score\")\nplt.ylabel(\"# of rows\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Most happy country:\")\nmax_happiness = df[\"HappinessScore\"].max()\ndf[df[\"HappinessScore\"] == max_happiness]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Least happy country:\")\nmin_happiness = df[\"HappinessScore\"].min()\ndf[df[\"HappinessScore\"] == min_happiness]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Most GDP per capita country:\")\nmax_GDP = df[\"GDP_PerCapita\"].max()\ndf[df[\"GDP_PerCapita\"] == max_GDP]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Most beer per capita country:\")\nmax_beer = df[\"Beer_PerCapita\"].max()\ndf[df[\"Beer_PerCapita\"] == max_beer]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so now we want to see if there's any correlation between columns. Can be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"pylab.rcParams['figure.figsize'] = (7, 6)\nsn.heatmap(df.corr(), annot=True)\nplt.title(\"Correlation matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some curves to show whether or not there is a trivial correlation between some interesting columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"pylab.rcParams['figure.figsize'] = (10, 8)\n\ndf_sorted = df.sort_values(by=[\"Beer_PerCapita\"])\nX, y = df_sorted[\"Beer_PerCapita\"], df_sorted[\"HappinessScore\"]\n\nplt.subplot(5, 1, 1)\nplt.plot(X, y)\nplt.xlabel(\"Beer_PerCapita\")\nplt.ylabel(\"HappinessScore\")\nplt.title(\"Relation between beer consumption and happiness\")\n\n\ndf_sorted = df.sort_values(by=[\"GDP_PerCapita\"])\nX, y = df_sorted[\"GDP_PerCapita\"], df_sorted[\"HappinessScore\"]\n\nplt.subplot(5, 1, 3)\nplt.plot(X, y)\nplt.xlabel(\"GDP_PerCapita\")\nplt.ylabel(\"HappinessScore\")\nplt.title(\"Relation between GDP and happiness\")\n\n\ndf_sorted = df.sort_values(by=[\"HDI\"])\nX, y = df_sorted[\"HDI\"], df_sorted[\"HappinessScore\"]\n\nplt.subplot(5, 1, 5)\nplt.plot(X, y)\nplt.xlabel(\"HDI\")\nplt.ylabel(\"HappinessScore\")\nplt.title(\"Relation between HDI and happiness\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there seem to be a small correlation between HDI, GDP and happiness, but it appears that beer consumption isn't really related to happiness (and I'm pretty sorry for that, as I wanted to predict happiness only by using beer consumption)."},{"metadata":{},"cell_type":"markdown","source":"Ok so, just for the kicks, let's look at the mean values when we look at the data grouped by Region."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_group_by = df.groupby([\"Region\"]).mean()\ndf_group_by","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enought is enought, let's predict happiness score of a country.\nWe first try without any localization feature and for the rest of the kernel, we will only use a LinearRegression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting happiness using linear model\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nreg = linear_model.LinearRegression()\n\nX_columns = [\"HDI\", \"GDP_PerCapita\", \"Beer_PerCapita\", \"Wine_PerCapita\", \"Spirit_PerCapita\"]\n\ntrain = df.sample(frac=0.8,random_state=200)\ntest = df.drop(train.index)\n\nX_train = train[X_columns]\ny_train = train[\"HappinessScore\"]\nX_test = test[X_columns]\ny_test = test[\"HappinessScore\"]\n\nreg.fit(X_train, y_train)\nprint(\"R^2 score:\", reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well,I know 0.6 isn't a good value, but I don't know how bad it is. Maybe I have not enought features ? Let's try to add some polynomial features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting happiness using linear model, adding polynomial features\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nreg = linear_model.LinearRegression()\npoly = PolynomialFeatures(2)\n\nX_columns = [\"HDI\", \"GDP_PerCapita\", \"Beer_PerCapita\", \"Wine_PerCapita\", \"Spirit_PerCapita\"]\n\ntrain = df.sample(frac=0.8,random_state=200)\ntest = df.drop(train.index)\n\nX_train = train[X_columns]\ny_train = train[\"HappinessScore\"]\nX_test = test[X_columns]\ny_test = test[\"HappinessScore\"]\nX_train = poly.fit_transform(X_train)\nX_test = poly.fit_transform(X_test)\n\nreg.fit(X_train, y_train)\nprint(\"R^2 score with polynomial features:\", reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok so now we are obviously overfitting. Let's forget about polynomial features, and now try adding a one-hot encoding on the Region values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding region to X_columns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nreg = linear_model.LinearRegression()\n\ndf_one_hot = pd.get_dummies(df, prefix=\"region\", columns=[\"Region\"])\nX_columns = set(df_one_hot.columns.values) - {\"Country\", \"HappinessScore\", \"Hemisphere\"}\n\ntrain = df_one_hot.sample(frac=0.8,random_state=200)\ntest = df_one_hot.drop(train.index)\n\nX_train = train[X_columns]\ny_train = train[\"HappinessScore\"]\nX_test = test[X_columns]\ny_test = test[\"HappinessScore\"]\n\nreg.fit(X_train, y_train)\nprint(\"R^2 score with region infos:\", reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.7 is better ! Maybe with a one-hot encoding on the Region and Hemisphere feature ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding region and hemisphere to X_columns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nreg = linear_model.LinearRegression()\n\ndf_one_hot = pd.get_dummies(df, prefix=\"region\", columns=[\"Region\"])\ndf_one_hot[\"Hemisphere\"] = df_one_hot[\"Hemisphere\"].replace(\"noth\", \"north\") # Correcting some misspells\ndf_one_hot = pd.get_dummies(df_one_hot, prefix=\"hemisphere\", columns=[\"Hemisphere\"])\nX_columns = set(df_one_hot.columns.values) - {\"Country\", \"HappinessScore\"}\n\ntrain = df_one_hot.sample(frac=0.8,random_state=200)\ntest = df_one_hot.drop(train.index)\n\nX_train = train[X_columns]\ny_train = train[\"HappinessScore\"]\nX_test = test[X_columns]\ny_test = test[\"HappinessScore\"]\n\nreg.fit(X_train, y_train)\nprint(\"R^2 score with region and hemisphere infos:\", reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemisphere does not really bring that much. We could select a few features just by looking at the correlation matrix ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# New correlation matrix\ndf_one_hot = pd.get_dummies(df, prefix=\"region\", columns=[\"Region\"])\nselect_columns = set(df_one_hot.columns) - {\"Country\", \"HDI\", \"GDP_PerCapita\", \"Beer_PerCapita\", \"Spirit_PerCapita\", \"Wine_PerCapita\"}\ndf_corr = df_one_hot[select_columns]\n\npylab.rcParams['figure.figsize'] = (7, 6)\nsn.heatmap(df_corr.corr(), annot=True)\nplt.title(\"Correlation matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok so now we can try one last time, with only HDI, GPD, Sub-Saharan Africa and Western Europe features (the one with the most happiness correlation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final try, with selected columns, and polynomial features\n# columns are selected according to the correlation matrices\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nreg = linear_model.LinearRegression()\npoly = PolynomialFeatures(2)\n\ndf_one_hot = pd.get_dummies(df, prefix=\"region\", columns=[\"Region\"])\ntrain = df_one_hot.sample(frac=0.8,random_state=200)\ntest = df_one_hot.drop(train.index)\n\nX_columns = [\"HDI\", \"GDP_PerCapita\", \"region_Sub-Saharan Africa\", \"region_Western Europe\"]\nX_train = train[X_columns]\ny_train = train[\"HappinessScore\"]\nX_test = test[X_columns]\ny_test = test[\"HappinessScore\"]\nX_train = poly.fit_transform(X_train)\nX_test = poly.fit_transform(X_test)\n\nreg.fit(X_train, y_train)\nprint(\"R^2 score with selected polynomial features:\", reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, 0.78 is better, but I guess this is not the 0.99 I usually find in books. There may be some better model / data preparation to increase the R², but I feel like with those features, we are not able to completly define the happiness score.\nAnyway, it is interesting to see that we ended up predicting happiness score without using alcohol consumption !\nMaybe we could add a summary of all alcohol columns into one (like a sum, ponderated by the alcohol rate ?)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}