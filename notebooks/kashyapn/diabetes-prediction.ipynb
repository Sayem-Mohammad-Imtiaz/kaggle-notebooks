{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=9 :     # as there are 9 columns in the data\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BMI'] = df['BMI'].replace(0,df['BMI'].mean())\ndf['BloodPressure'] = df['BloodPressure'].replace(0,df['BloodPressure'].mean())\ndf['Glucose'] = df['Glucose'].replace(0,df['Glucose'].mean())\ndf['Insulin'] = df['Insulin'].replace(0,df['Insulin'].mean())\ndf['SkinThickness'] = df['SkinThickness'].replace(0,df['SkinThickness'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in df:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(df[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=df, width= 0.5,ax=ax,  fliersize=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q = df['Pregnancies'].quantile(0.98)\n# we are removing the top 2% data from the Pregnancies column\ndata_cleaned = df[df['Pregnancies']<q]\nq = data_cleaned['BMI'].quantile(0.99)\n# we are removing the top 1% data from the BMI column\ndata_cleaned  = data_cleaned[data_cleaned['BMI']<q]\nq = data_cleaned['SkinThickness'].quantile(0.99)\n# we are removing the top 1% data from the SkinThickness column\ndata_cleaned  = data_cleaned[data_cleaned['SkinThickness']<q]\nq = data_cleaned['Insulin'].quantile(0.95)\n# we are removing the top 5% data from the Insulin column\ndata_cleaned  = data_cleaned[data_cleaned['Insulin']<q]\nq = data_cleaned['DiabetesPedigreeFunction'].quantile(0.99)\n# we are removing the top 1% data from the DiabetesPedigreeFunction column\ndata_cleaned  = data_cleaned[data_cleaned['DiabetesPedigreeFunction']<q]\nq = data_cleaned['Age'].quantile(0.99)\n# we are removing the top 1% data from the Age column\ndata_cleaned  = data_cleaned[data_cleaned['Age']<q]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data_cleaned, width= 0.5,ax=ax,  fliersize=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats     \nimport numpy as np\nz = np.abs(stats.zscore(data_cleaned))      #Using Z-score for removing some more outliers\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 3\nprint(np.where(z > 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data_cleaned[(z < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data1, width= 0.5,ax=ax,  fliersize=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data1['Outcome']\nX = data1.drop('Outcome',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in data1:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.distplot(data1[column])\n        plt.xlabel(column,fontsize=20)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,25), facecolor='white')\nplotnumber = 1\n\nfor column in X:\n    if plotnumber<=9 :\n        ax = plt.subplot(3,3,plotnumber)\n        sns.stripplot(y,X[column])\n    plotnumber+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standard-Scaling\nfrom sklearn.preprocessing import StandardScaler   \nscalar = StandardScaler()                               \nX_scaled = scalar.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"vif\"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]\nvif[\"Features\"] = X.columns\n\n#let's check the values\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No multicollinearity as such(Since vif values are below 5)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_scaled,y, test_size= 0.2, random_state = 60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model  import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = log_reg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nconf_mat = confusion_matrix(y_test,y_pred)\nconf_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_positive = conf_mat[0][0]\nfalse_positive = conf_mat[0][1]\nfalse_negative = conf_mat[1][0]\ntrue_negative = conf_mat[1][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Breaking down the formula for Accuracy\nAccuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)\nAccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precison\nPrecision = true_positive/(true_positive+false_positive)\nPrecision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recall\nRecall = true_positive/(true_positive+false_negative)\nRecall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F1 Score\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\nF1_Score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area Under Curve\nauc = roc_auc_score(y_test, y_pred)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems the model is highly overfitting, lets perform hyperparameter tuning using Grid Search CV and see whether the accuracy improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_param = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,32,1),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'splitter' : ['best', 'random']\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator=clf,\n                     param_grid=grid_param,\n                     cv=5,\n                    n_jobs =-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters = grid_search.best_params_\nprint(best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion = 'entropy', max_depth =10, min_samples_leaf= 9, min_samples_split= 2, splitter ='random')\nclf.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy improved marginally.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Area under the curve\nauc = roc_auc_score(y_test, y_pred)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrand_clf = RandomForestClassifier(random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_clf.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_clf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_param = {\n    \"n_estimators\" : [90,100,115],\n    'criterion': ['gini', 'entropy'],\n    'max_depth' : range(2,20,2),\n    'min_samples_leaf' : range(1,10,1),\n    'min_samples_split': range(2,10,1),\n    'max_features' : ['auto','log2']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_clf = RandomForestClassifier(criterion= 'entropy',\n max_depth = 10,\n max_features = 'auto',\n min_samples_leaf = 3,\n min_samples_split= 8,\n n_estimators = 100,random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_clf.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting almost the same accuracy after hyperparameter tuning, maybe it needed more parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_clf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rand_clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, y_pred)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { 'algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n               'leaf_size' : [18,20,25,27,30,32,34],\n               'n_neighbors' : [3,5,7,9,10,11,12,13,15,17,19]\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch = GridSearchCV(knn, param_grid,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size =18, n_neighbors =13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems before hyperparameter tuning, the model was overfitting, now it is better.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, y_pred)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier(objective='binary:logistic')\nmodel.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cheking training accuracy\ny_pred = model.predict(x_train)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_train,predictions)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cheking initial test accuracy\ny_pred = model.predict(x_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test,predictions)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is highly overfitting. Need to perform hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid={\n   \n    'learning_rate':[1,0.5,0.1,0.01,0.001],\n    'max_depth': [3,5,10,20],\n    'n_estimators':[10,50,100,200]\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid= GridSearchCV(XGBClassifier(objective='binary:logistic'),param_grid, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model=XGBClassifier(learning_rate=0.01, max_depth= 3, n_estimators= 200)\nnew_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_new = new_model.predict(x_test)\npredictions_new = [round(value) for value in y_pred_new]\naccuracy_new = accuracy_score(y_test,predictions_new)\naccuracy_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy has increased for the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(y_test, y_pred_new)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Considering the accuracy and AUC scores of all the models taken here, Random Forest Classifier and XGBoost Classifier models perform the best","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}