{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Question: \n## Based upon customer data provided by a global bank, please provide a Exiting Customer list to Marketing Team.\n## The Exiting Customer list will be used to retain customer who is going to exit the bank."},{"metadata":{},"cell_type":"markdown","source":"# Import Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Checking Missing Value and Data Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Exited').agg({'CustomerId':'count'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1) There is no missing value\n#### 2) Numerical Data: Age, Estimated Salary, Tenure, Balance, CreditScore, \n#### 3a) Categorical Data: Gender, Geography\n#### 3b) HasCrDard, IsActiveMember and NumofProducts should be Categorical/ Binary Data rather than int64\n#### 4a) Label: Exited should be Categorical data\n#### 4b) Label: Exited is imbalance, the proportion of Exit:Not Exit around 20%:80%"},{"metadata":{},"cell_type":"markdown","source":"## Prepare Dataset for EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA_Data=df.copy()\nEDA_Data['Exited']=EDA_Data['Exited'].apply(lambda x: 'Exit' if x==1 else 'Not Exit')\nEDA_Data['HasCrCard']=EDA_Data['HasCrCard'].apply(lambda x: 'Has Credit Card' if x==1 else 'Do not have Credit Card')\nEDA_Data['IsActiveMember']=EDA_Data['IsActiveMember'].apply(lambda x: 'Active Member' if x==1 else 'Inactive Member')\nEDA_Data['NumOfProducts']=EDA_Data['NumOfProducts'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA_Data.drop(['RowNumber','CustomerId'],axis=1).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Relationship between Numerical Data and Exited"},{"metadata":{},"cell_type":"markdown","source":"### 1) Relationship between Age and Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='Age',data=EDA_Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Overall, customer who exited is older than those not exit. \n#### The median of exited customer is around 45 years old.\n#### The median of exited customer is already older than 75% not exit customer."},{"metadata":{},"cell_type":"markdown","source":"### 2) Relationship between Tenure and Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='Tenure',data=EDA_Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare to Not Exist Customer, Customer who join less than 3 years or above 7 years are more likely to exit."},{"metadata":{},"cell_type":"markdown","source":"### 3) Relationship between Balance and Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='Balance',data=EDA_Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Customer whose balance less than $40000 are more likely not exit\n#### The top 25% exit customer has higher balance than those not exit"},{"metadata":{},"cell_type":"markdown","source":"### 4) Relationship between Credit Score and Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='CreditScore',data=EDA_Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Credit Score only slightly correlated to Exited. \n#### Customer with lower credt score slightly exit.\n#### There are some outliers whose score fall around 350. "},{"metadata":{},"cell_type":"markdown","source":"### 5) Relationship between EstimatedSalary and Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='EstimatedSalary',data=EDA_Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Relationship between Categorical Data and Exited"},{"metadata":{},"cell_type":"markdown","source":"#### Estimated Salary do not correlated to Exited."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(3, 2, figsize=(10,15),)\naxis[0,0].set_title(\"Relationship between Gender and Exited\")\naxis[0,1].set_title(\"Relationship between Geography and Exited\")\naxis[1,0].set_title(\"Relationship between Has Credit Card and Exited\")\naxis[1,1].set_title(\"Relationship between Is Active and Exited\")\naxis[2,0].set_title(\"Relationship between No. of Product and Exited\")\n\nsns.countplot(x='Gender',hue='Exited',data=EDA_Data,ax=axis[0,0])\nsns.countplot(x='Geography',hue='Exited',data=EDA_Data,ax=axis[0,1])\nsns.countplot(x='HasCrCard',hue='Exited',data=EDA_Data,ax=axis[1,0])\nsns.countplot(x='IsActiveMember',hue='Exited',data=EDA_Data,ax=axis[1,1])\nsns.countplot(x='NumOfProducts',hue='Exited',data=EDA_Data,ax=axis[2,0])\n\naxis[2,1].remove()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above diagram, we can see that:\n#### 1) Female is easier exit than Male\n#### 2) Customer in Germany more likely to exit\n#### 3) Inactive Member has higher proportion to exit than Active Member\n#### 4) Credit Card is not correlated to Exit\n#### 5) Customer with 2 products have higher proportion not exit"},{"metadata":{},"cell_type":"markdown","source":"### Correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nEDA2=df.iloc[:,3:-1]\nEDA2=pd.concat([df.iloc[:,-1],EDA2,],axis=1)\nEDA2=pd.get_dummies(EDA2,columns=['Geography','Gender','NumOfProducts','HasCrCard','IsActiveMember'])\nsns.heatmap(EDA2.corr(),vmin=-1,vmax=1,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Features with color close to 0 means no correlation, \n#### Features with color close to -1 or 1 means having strong negative or postive relationship to each other respectively. \n#### From the above heat map, we can see that all features only have weak relationship to each other.\n#### Features that have relative slightly relationship with Exited include:\n#### Age, NumOfProduct, Geography, IsActiveMember,Balance,Gender"},{"metadata":{},"cell_type":"markdown","source":"## Train Data without Feature Selected - Set 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"X0=df.iloc[:,3:-1]\nX0_c=X0.loc[:,['Geography','Gender','NumOfProducts','HasCrCard','IsActiveMember']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply OneHotEncoding to Categorical Data: (Geography,Gender,NumOfProducts,HasCrCard,IsActiveMember)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X0_c=pd.get_dummies(X0_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX0_n=X0.loc[:,['CreditScore','Age','Balance','Tenure','EstimatedSalary']]\nscaler = StandardScaler()\nX0_n1=scaler.fit_transform(X0_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X0_n1=pd.DataFrame(X0_n1,columns=['CreditScore','Age','Balance','Tenure','EstimatedSalary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X0_c,X0_n1],axis=1)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Generating new data by oversampling\n#### As mentioned before, the data is imbalnce, so, we increase the number of samples by SMOTE technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"#conda install -c conda-forge imbalanced-learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts(),'\\n',y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Start to Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns1_1_y_pred=y_pred\nclf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC (Support Vector Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(random_state=42)\nsvc=clf.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\ns1_2_y_pred=y_pred\nsvc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns1_3_y_pred=y_pred\nclf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':6, 'eta':0.3, 'objective':'binary:hinge' } #Use default value in the first time\nnum_round = 2\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\ns1_4_y_pred=y_pred\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set 1 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary1=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),columns=['Logistic Reg.','SVC','Random Forest','XGB'],index=ind_name)\nsummary1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Data with Feature Selected (Based on EDA) - Set 2 \n### [Keep all hyperparameter same as Set 1]"},{"metadata":{},"cell_type":"markdown","source":"### Remove Estimated Salary, Has Credit Card, Credit Score and Tenure refer to EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=df.iloc[:,3:-1]\nX1_c=X1.loc[:,['Geography','Gender','NumOfProducts','IsActiveMember']]\nX1_n=X1.loc[:,['Age','Balance']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 1: Distribution of Numerical Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA_Data[EDA_Data['Exited']=='Exit'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA_Data[EDA_Data['Exited']=='Not Exit'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=EDA_Data['Age'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Age1=EDA_Data[EDA_Data['Exited']=='Exit']\nsns.distplot(a=Age1['Age'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Age1=EDA_Data[EDA_Data['Exited']=='Not Exit']\nsns.distplot(a=Age1['Age'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_gp(a):\n    if a>=18 and a<30:\n        return 'Gp1'\n    elif a>=30 and a<40:\n        return 'Gp2'\n    elif a>=40 and a<50:\n        return 'Gp3'\n    elif a>=50:\n        return 'Gp4'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_c['Age_group']=X1_n['Age'].apply(age_gp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_c.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=EDA_Data['Balance'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_c['Balance_Group']=X1_n['Balance'].apply(lambda x: 'Without Balance' if x<50000 else 'With Balance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## As All of X's are Catergorical Data, only need transfer them to Binary Dataa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_c2=pd.get_dummies(X1_c)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1_c2, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Generating new data by oversampling\n#### Same as before, as the data is imbalnce, so, we increase the number of samples by SMOTE technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts(),'\\n',y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Start to Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns2_1_y_pred=y_pred\nclf.score(X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC (Support Vector Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(random_state=42)\nsvc=clf.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\ns2_2_y_pred=y_pred\nsvc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns2_3_y_pred=y_pred\nclf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':6, 'eta':0.3, 'objective':'binary:hinge' }\nnum_round = 2\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\ns2_4_y_pred=y_pred\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set 2 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary2=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),columns=['Logistic Reg.','SVC','Random Forest','XGB'],index=ind_name)\nsummary2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = clf.predict_proba(X_test)\npreds = probs[:,1]\n\n\nfpr11, tpr11, threshold = metrics.roc_curve(y_test, s1_1_y_pred)\nroc_auc11 = metrics.auc(fpr11, tpr11)\nfpr12, tpr12, threshold = metrics.roc_curve(y_test, s1_2_y_pred)\nroc_auc12 = metrics.auc(fpr12, tpr12)\nfpr13, tpr13, threshold = metrics.roc_curve(y_test, s1_3_y_pred)\nroc_auc13 = metrics.auc(fpr13, tpr13)\nfpr14, tpr14, threshold = metrics.roc_curve(y_test, s1_4_y_pred)\nroc_auc14 = metrics.auc(fpr14, tpr14)\n\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Set 1 Model AUC-ROC')\nplt.plot(fpr11, tpr11, 'b', label = 'AUC = %0.2f logistic' % roc_auc11)\nplt.plot(fpr12, tpr12, 'r', label = 'AUC = %0.2f svc' % roc_auc12)\nplt.plot(fpr13, tpr13, 'y', label = 'AUC = %0.2f RF' % roc_auc13)\nplt.plot(fpr14, tpr14, 'g', label = 'AUC = %0.2f XGB' % roc_auc14)\n\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = clf.predict_proba(X_test)\npreds = probs[:,1]\n\n\nfpr21, tpr21, threshold = metrics.roc_curve(y_test, s2_1_y_pred)\nroc_auc21 = metrics.auc(fpr21, tpr21)\nfpr22, tpr22, threshold = metrics.roc_curve(y_test, s2_2_y_pred)\nroc_auc22 = metrics.auc(fpr22, tpr22)\nfpr23, tpr23, threshold = metrics.roc_curve(y_test, s2_3_y_pred)\nroc_auc23 = metrics.auc(fpr23, tpr23)\nfpr24, tpr24, threshold = metrics.roc_curve(y_test, s2_4_y_pred)\nroc_auc24 = metrics.auc(fpr24, tpr24)\n\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Set 2 Model AUC-ROC')\nplt.plot(fpr11, tpr21, 'b', label = 'AUC = %0.2f logistic' % roc_auc21)\nplt.plot(fpr12, tpr22, 'r', label = 'AUC = %0.2f svm' % roc_auc22)\nplt.plot(fpr13, tpr23, 'y', label = 'AUC = %0.2f RF' % roc_auc23)\nplt.plot(fpr14, tpr24, 'g', label = 'AUC = %0.2f XGB' % roc_auc24)\n\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that the best model among Set 1 and Set 2 is 'Random Forest in Set 1'\n#### All Models have higher score in Set 1 than Set 2\n#### However, for building a Churn Model, it is more important to predict customer who will leave correctly rather than the overall accuracy of the model.\n#### Let's think carefully: \n#### For Type I error, which is the error to predict the customer who exit, but actually he/she doesn't. \n#### For Type II error, which is the error to predict the customer who not exist, but actually he/she does.\n#### Which error is more serious? It should be Type II error.\n#### For Type I error case, if we predict wrongly, we may waste cost/resource to retain a customer who actually will stay.\n#### For Type II error case, if we predict wrongly, we may take no action to the customer and the customer will therefore leave.\n#### Therefore, Recall Score is much more important than Accuracy Score.\n#### Before select which model to be used, let's fine tune our model!"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning - Set 1\nI will try to fine tune the Hyperparameter and hope to obtain a better Accuracy and Recall Rate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X0_c,X0_n1],axis=1)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(y_train).groupby('Exited').agg({'Exited':'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf =  LogisticRegression(random_state=42)\nparam_grid = [\n    {'C':[0.01,0.1,1,10]}]\n     #'max_iter':[100,150,200,1000]}]\n    #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2']}] \n    #{'solver': ['liblinear','saga'],'penalty':['l1']}]\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy',cv=5)\nlr=search.fit(X_train, y_train)\n\nlr=clf.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf =  LogisticRegression(random_state=42)\n\nparam_grid = [\n    {'C':[0.01,0.1,1,10]}]\n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2']}]\n  #{'solver': ['liblinear','saga'],'penalty':['l1']}]\nsearch = GridSearchCV(clf, param_grid,scoring='recall', cv=5)\nlr=search.fit(X_train, y_train)\n\n#lr=clf.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC (Support Vector Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(random_state=42, kernel='rbf')\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10]}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\nsvc.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(random_state=42, kernel='rbf')\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10],\n   'gamma':['scale', 'auto']}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='recall', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\nsvc.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  {'n_estimators' : [140,150,160,170,180,190,200]}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\nclf.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  {'n_estimators' : [140,150,160,170,180,190,200]}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\nclf.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':4, 'eta':0.6, 'objective':'binary:hinge'}\nnum_round = 20\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\naccuracy_score(y_test,y_pred)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set 3 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary3=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),\n                      columns=['Logistic Reg.(Accuracy)','Logistic Reg.(Recall)',\n                               'SVC (Accuracy)','SVC (Recall)',\n                               'Random Forest (Accuracy)','Random Forest (Recall)'\n                               ,'XGB'],index=ind_name)\nsummary3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning - Set 2\nAlso, I will try to fine tune the Hyperparameter and hope to obtain a better Accuracy and Recall Rate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_c2=pd.get_dummies(X1_c)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1_c2, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(y_train).groupby('Exited').agg({'Exited':'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42)\n\n\nparam_grid = [\n  {'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['newton-cg','sag','lbfgs'],'C':[0.1,1,10],'max_iter':[1000,10000]}] \n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2'],'C':[0.1,1,10],'max_iter':[1000,10000]}] #1\n  #{'solver': ['liblinear','saga'],'penalty':['l1'],'max_iter':[1000,10000]}]\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n\n\nlr=search.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  (No much Change)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42)\n\nparam_grid = [\n    {'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['newton-cg','sag','lbfgs' ],'C':[0.1,1,10],'max_iter':[1000,10000]}] \n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2'],'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['liblinear','saga'],'penalty':['l1'],'C':[0.1,1,10],'max_iter':[1000,10000]}]\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n\n\nlr=search.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  (No much Change)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC (Support Vector Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf = SVC(random_state=42)#,C=1,kernel='rbf',gamma='scale')\n\n\nparam_grid = [\n    {'C': [0.01,0.1,1,10]}]    \n   #{'C': [0.01,0.1,1,10], 'kernel': ['rbf'],'gamma':['scale','auto']}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\n\n\n#svc=clf.fit(X_train, y_train)\n#y_pred=svc.predict(X_test)\n#s2_2_y_pred=y_pred\n\nsvc.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf = SVC(random_state=42)#,C=1,kernel='rbf',gamma='scale')\n\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10]}]    \n  #{'C': [0.01,0.1,1,10], 'kernel': ['rbf'],'gamma':['scale','auto']}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\n\n\n#svc=clf.fit(X_train, y_train)\n#y_pred=svc.predict(X_test)\n#s2_2_y_pred=y_pred\n\nsvc.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  #{'n_estimators' : [100,150,200],'max_depth':[5,10,15]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[5,6,7,8,9,10]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[9,10]}]\n  #{'n_estimators' : [110,111,112,113,114,115,116,117,118,119],'max_depth':[5,10,15]}]\n   {'n_estimators' : [117,118,119],'max_depth':[9,10,11],'criterion':['gini','entropy']}]\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\n#clf.fit(X_train, y_train)\n#y_pred=clf.predict(X_test)\nclf.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  #{'n_estimators' : [100,150,200],'max_depth':[5,10,15]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[5,6,7,8,9,10]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[9,10]}]\n  #{'n_estimators' : [105,106,107,108,109,110,111,112,113,114,115,116,117,118,119],'max_depth':[9,10,11]}]\n   {'n_estimators' : [108,109],'max_depth':[9]}]\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\n#clf.fit(X_train, y_train)\n#y_pred=clf.predict(X_test)\nclf.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scale_pos_weight=sum(negative instances) / sum(positive instances)\nimport xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':5, 'eta':0.07,'objective':'binary:hinge'}\nnum_round = 500\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\naccuracy_score(y_test,y_pred)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set 4 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary4=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),\n                      columns=['Logistic Reg.(Accuracy)','Logistic Reg.(Recall)',\n                               'SVC (Accuracy)','SVC (Recall)',\n                               'Random Forest (Accuracy)','Random Forest (Recall)'\n                               ,'XGB'],index=ind_name)\nsummary4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After hyperparameter tuning, we can see that:\n1) For Accuracy, All score in set 1 Model have been improved, while in set 2 Model, only Logistic Regression, SVC and XGB have been improved.\n\n2) For Recall rate, Logistic Regression, SVC and Random Forest have been improved in set 1 while only SVC have been improved in set 2.\n\nSo, which model should be used?\nIn my opinion, XGB with hyperparameter tuning in set 1 is recommended to use.\nAs mentioned before, our business question is find out customer who will exit the bank.\nTherefore, Recall Rate is more important than Accuracy Score.\n\nRecap that:\nTP (True Positive): Predict customer will exit while the customer really exit\nTN (True Negative): Predict customer will not exit while the customer really not exit\nFP (False Positive): Predict customer will exit while actually the customer will not exit \nFN (False Negative): Predict customer will not exit while actually the customer will exit \n\nAccuracy Rate = TP/(TP+TN)\nRecall Rate = TP/(TP+FN)\nPrecision = TP/(TP+FP)\nF1 Score = 2TP/(2TP+FP+FN)\n\nIn our case, we want predict Exit customer for retention.\nTherefore, which ones is more important?\nA) Finding a exit customer from a base with exiting customer and not exiting customer correctly? OR\nB) Finding a exit customer from a base with exiting customer and 'I guess the customer will not leave, but actually the customer will leave'? OR\nC) Finding a exit customer from a base with exiting customer and 'I guess the customer will leave, but actually the customer will not leave'?\n\nThe answer should be B, right? \nWe should minimize the % of 'I guess the customer will not leave, but actually the customer will leave', i.e. Use the highest recall rate.\n\nTherefore XGBoost in set 1 seems the best ones we are going to use.\nWe should the ones with hyperparameter tuning because the F1_Score is higher than the ones without tuning.\nF1 score becomes high only when both precision and recall are high.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}