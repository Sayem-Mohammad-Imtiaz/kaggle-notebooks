{"cells":[{"cell_type":"markdown","metadata":{},"source":"%matplotlib inline"},{"cell_type":"markdown","metadata":{},"source":"## Introduction\n\nWe will make an early investigation of the keywords of several job titles, how they affect\nsalaries and what range of salaries we are looking at. The methodology should be applicable\nfor many similar metrics.\n\n## Step 1. Data loading\n\nWe open the dataset and have a quick look..."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\n\n\n\nsrc = pd.read_csv(\"../input/Salaries.csv\", low_memory=False)\nsrc.describe()\nsrc.head(10)"},{"cell_type":"markdown","metadata":{},"source":"## Step 2. Having a look at titles\n\nIn this steps I get data only for 2014 and have a look at job titles. There are\n997 distinct job titles and most of them have just a few words in them. This is\ngood because it means that even algorithms with relatively high complexity will run fine."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from nltk.corpus import stopwords\nen_stopwords = stopwords.words('english')\n#stopwords quita las palabras básicas: i, as, how, etc..\n\nsrc2014 = src[src[\"Year\"]==2014]\n\njob_titles = src2014[\"JobTitle\"]\nunique_job_titles = job_titles.unique()\n#obtiene los jobs únicos, como agrupar por jobs\n\nprint(\"Job titles: \", job_titles.count(), \" unique job titles \", len(unique_job_titles))\n\nimport re\ndef tokenize(title):\n    return filter(lambda w: w and (w not in en_stopwords), re.split('[^a-z]*', title.lower()))\n\n\ntokenized_titles = list(filter(None, [list(tokenize(title)) for title in unique_job_titles]))\n\nlengths = list(map(len, tokenized_titles))\n\npd.DataFrame({\"number or keywords per title\": lengths}).hist(figsize=(16, 4));\n#pd.DataFrame({\"number or keywords per title\": lengths}).plot(figsize=(16, 4));"},{"cell_type":"markdown","metadata":{},"source":"## Step 3. Extracting n-order tuples from titles\nLet's create a little algorithm to create all combinations of the keywords from titles.\nWe are going to use this in a bit. For example, `deputy chief` breaks down to 3\ncombinations: `deputy`, `chief` and `deputy chief`."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"tokenized_title = tokenized_titles[0]\n\nprint(\"original: \", tokenized_title)\n\nimport itertools\n\ndef keywords_for_title(tokenized_title):\n    for keyword_len in range(1, len(tokenized_title)+1):\n        for keyword in itertools.combinations(tokenized_title, keyword_len):\n            yield keyword\n\nprint(\"keywords: \", list(keywords_for_title(tokenized_title)))"},{"cell_type":"markdown","metadata":{},"source":"## Step 4. Extracting n-order tuples from titles\n\nLet's have a look on distinct keyword combinations from titles. As we can see\nthere's a great variety of titles that include \"supervisor\", \"senior\", \"assistant\" etc."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from collections import Counter\nkeywords_stats = Counter()\nfor title in tokenized_titles:\n    for keyword in keywords_for_title(title):\n        keywords_stats[keyword] += 1\n\nprint(\"distinct keywords: \", len(keywords_stats))\nkeywords_stats.most_common(1115)"},{"cell_type":"markdown","metadata":{},"source":"## Step 5. Do the data extraction and aggregation\n\nThis is a relatively slow step because we go through the initial dataset and we\ncreate primitives that will help us compute more advanced statistics in the next step.\nEssentially it's just increasing a few counters per keyword combination."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"salaries = Counter()\ncounts = Counter()\nmin_salaries = {}\nmax_salaries = {}\n\nfor index, row in src2014.iterrows():\n    job_title = row[\"JobTitle\"]\n    salary_plus_benefits = row[\"TotalPayBenefits\"]\n    \n    # Remove temporary jobs\n    if salary_plus_benefits < 10000:\n        continue\n    \n    tokenized_title = list(tokenize(job_title))\n    if not tokenized_title:\n        continue\n    for keyword in keywords_for_title(tokenized_title):\n        salaries[keyword] += salary_plus_benefits\n        counts[keyword] += 1\n        if keyword in max_salaries:\n            if salary_plus_benefits < min_salaries[keyword]:\n                min_salaries[keyword] = salary_plus_benefits\n            if salary_plus_benefits > max_salaries[keyword]:\n                max_salaries[keyword] = salary_plus_benefits\n        else:\n            min_salaries[keyword] = salary_plus_benefits\n            max_salaries[keyword] = salary_plus_benefits"},{"cell_type":"markdown","metadata":{},"source":"## Step 6. We create the two statistics; shift and variance.\n\nFor a given keyword, shift is the amount of shift in average salary for titles\nthat have a keyword in contrast to the ones that don't have it. For example\nif jobs with \"senior\" have an average of USD 1100 and jobs without have an\naverage of USD 900 and the average is USD 1000 then the shift is (1100 - 900) / 1000 =\n200/1000 = 0.2 = 20%.\n\nThe variance is the max-min salary / average salary for the given keywords. Thos are\ntwo interesting statistics to investigate."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"s_all = src2014[\"TotalPayBenefits\"].sum()\nn_all = src2014[\"TotalPayBenefits\"].count()\navg_salary = s_all / n_all\n\nshifts = Counter()\nvariances = Counter()\n\nfor keyword in salaries:\n    s_with = salaries[keyword]\n    n_with = counts[keyword]\n    \n    # Skip ill-cases\n    if min_salaries[keyword] == 0:\n        continue\n    \n    if n_with < 5:\n        continue\n    \n    if len(keyword) > 2:\n        continue\n\n    avg_salary_with = s_with / n_with\n        \n    shifts[keyword] = (avg_salary_with - ((s_all-s_with)/(n_all-n_with))) / avg_salary\n    variances[keyword] = max_salaries[keyword] / min_salaries[keyword]\n\nprint('shifts: ', shifts.most_common(15))\nprint()\nprint('variances: ', variances.most_common(15))"},{"cell_type":"markdown","metadata":{},"source":"## Step 7. Scatter-plot the results\n\nWe compile a `DataFrame` that helps us plot and gain intuition on the results."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"keys=[]\nshifts_v=[]\nvariances_v=[]\n\nfor i in shifts.keys():\n    keys.append(i)\n    shifts_v.append(shifts[i])\n    variances_v.append(variances[i])\n\nstats = pd.DataFrame({\"keys\": keys, \"shifts\": shifts_v, \"variances\": variances_v})\nstats.plot(\"shifts\", \"variances\", \"scatter\")"},{"cell_type":"markdown","metadata":{},"source":"## Step 8. Focus on above average\n\nFocus only on above average salaries"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Keep only the above average salaries\nkeys=[]\nshifts_v=[]\nvariances_v=[]\n\nfor i in shifts.keys():\n    if shifts[i] <= 1:\n        continue\n    keys.append(i)\n    shifts_v.append(shifts[i])\n    variances_v.append(variances[i])\n\nstats = pd.DataFrame({\"keys\": keys, \"shifts\": shifts_v, \"variances\": variances_v})\nstats.plot(\"shifts\", \"variances\", \"scatter\")"},{"cell_type":"markdown","metadata":{},"source":"## Step 9. Clustering with DBSCAN.\n\nLet's try to identify some clusters using the DBSCAN algorithm. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nX = stats[['shifts', 'variances']].values\n#X = StandardScaler().fit_transform(X)\n#db = DBSCAN(eps=0.4, min_samples=10).fit(X)\ndb = KMeans(n_clusters=3).fit(X)\n\n##############################################################################\n# From http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n##############################################################################\n\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n#core_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\n\n##############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = 'k'\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"## Step 10. Print general cluster information\n\nPrint the identified number of clusters and the number of data-points in each cluster.\nThe -1 cluster is the noize one and can be ignored."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from collections import defaultdict\nlabels_per_class = defaultdict(list)\nfor label in set(labels):\n    for cnt, i_label in enumerate(labels):\n        if i_label == label:\n            labels_per_class[label].append(cnt)\n\n[(k, len(labels_per_class[k])) for k in labels_per_class.keys()]"},{"cell_type":"markdown","metadata":{},"source":"## Step 11. Print cluster results\n\nFor each interesting class of datapoints, print us details on the samples"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"for i_class in range(len(labels_per_class)):\n    print(\"class: \", i_class)\n    for ikey in labels_per_class[i_class]:\n        dp = [\n            (\"shift\", format(shifts_v[ikey], '.2f')),\n            (\"variance\", format(variances_v[ikey], '.2f')),\n            (\"key\", keys[ikey]),\n        ]\n        print(\"  -\", dp)\n"},{"cell_type":"markdown","metadata":{},"source":"## Step 12. Summary\n\nWe can see here a segmentation between highly-payed and conservative and highly-payed\nand more open-market related jobs. Professions that have to compete with the private sector\ne.g. 'attorney', 'architect', 'physician', 'nurse' and some other managerial\npositions have large variance indicating performance-driven incentives. Contrary jobs with\nkeywords such as 'sheriff', 'civil', 'commander' etc. are very well paid but they are more\nconservative in terms of performance incentives. This might or might not be the right\nthing to do."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}