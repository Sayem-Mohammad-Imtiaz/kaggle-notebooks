{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bank Churners\n\nIntro: In this notebook I'll use some ordinary ML algos in order to predict predict bank churners","metadata":{}},{"cell_type":"markdown","source":"Imports:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:06:39.103267Z","iopub.execute_input":"2021-07-21T09:06:39.103858Z","iopub.status.idle":"2021-07-21T09:06:39.109195Z","shell.execute_reply.started":"2021-07-21T09:06:39.103822Z","shell.execute_reply":"2021-07-21T09:06:39.108136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us read the data.\n\nNote: I'll delete the last two columns as the dataset description states.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv (\"/kaggle/input/credit-card-customers/BankChurners.csv\")\ncolumns = df.columns.tolist()\ncols_to_use = columns[:len(columns) - 2]  # drop the last two columns\ndf = df[cols_to_use]\nprint(\"The data shape is : {} \".format(df.shape))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:06:40.163813Z","iopub.execute_input":"2021-07-21T09:06:40.164443Z","iopub.status.idle":"2021-07-21T09:06:40.296763Z","shell.execute_reply.started":"2021-07-21T09:06:40.164375Z","shell.execute_reply":"2021-07-21T09:06:40.295651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:06:41.607427Z","iopub.execute_input":"2021-07-21T09:06:41.607779Z","iopub.status.idle":"2021-07-21T09:06:41.634164Z","shell.execute_reply.started":"2021-07-21T09:06:41.607749Z","shell.execute_reply":"2021-07-21T09:06:41.633461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look over the categorical columns:","metadata":{}},{"cell_type":"code","source":"cat_columns = df.select_dtypes(include = ['object']).nunique(dropna=False)\nprint(cat_columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:06:42.810865Z","iopub.execute_input":"2021-07-21T09:06:42.811231Z","iopub.status.idle":"2021-07-21T09:06:42.832324Z","shell.execute_reply.started":"2021-07-21T09:06:42.811201Z","shell.execute_reply":"2021-07-21T09:06:42.831315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_columns.index:\n    print(\"Feature: \", col)\n    print(\"   Vals: \", df[col].unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:25:34.182893Z","iopub.execute_input":"2021-07-21T09:25:34.183236Z","iopub.status.idle":"2021-07-21T09:25:34.198226Z","shell.execute_reply.started":"2021-07-21T09:25:34.183207Z","shell.execute_reply":"2021-07-21T09:25:34.197197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's handle categorical cols that can be interpreted as ordinal features:","metadata":{}},{"cell_type":"code","source":"# Education level\n# Change 'College'=14 'Doctorate'=21 'Graduate'=16 'High School'=12 'Post-Graduate'=18 'Uneducated'=8 'Unknown'= Mode\ndf.loc[df['Education_Level'] == 'College',       'Education_Level'] = 14\ndf.loc[df['Education_Level'] == 'Doctorate',     'Education_Level'] = 21\ndf.loc[df['Education_Level'] == 'Graduate',      'Education_Level'] = 16\ndf.loc[df['Education_Level'] == 'High School',   'Education_Level'] = 12\ndf.loc[df['Education_Level'] == 'Post-Graduate', 'Education_Level'] = 18\ndf.loc[df['Education_Level'] == 'Uneducated',    'Education_Level'] = 8\ndf.loc[df['Education_Level'] == 'Unknown',       'Education_Level'] = df['Education_Level'].mode()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:43:34.781342Z","iopub.execute_input":"2021-07-21T09:43:34.781727Z","iopub.status.idle":"2021-07-21T09:43:34.805455Z","shell.execute_reply.started":"2021-07-21T09:43:34.781689Z","shell.execute_reply":"2021-07-21T09:43:34.804399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Income \ndf.loc[df['Income_Category'] == 'Less than $40K', 'Income_Category'] = 30\ndf.loc[df['Income_Category'] == '$40K - $60K',    'Income_Category'] = 50\ndf.loc[df['Income_Category'] == '$60K - $80K',    'Income_Category'] = 70\ndf.loc[df['Income_Category'] == '$80K - $120K',   'Income_Category'] = 100\ndf.loc[df['Income_Category'] == '$120K +',        'Income_Category'] = 200\ndf.loc[df['Income_Category'] == 'Unknown',        'Income_Category'] = 0\ndf.loc[df['Income_Category'] == 0, 'Income_Category'] = df['Income_Category'].mode()[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:47:47.543387Z","iopub.execute_input":"2021-07-21T09:47:47.543763Z","iopub.status.idle":"2021-07-21T09:47:47.571386Z","shell.execute_reply.started":"2021-07-21T09:47:47.543729Z","shell.execute_reply":"2021-07-21T09:47:47.570393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Card Category\ndf.loc[df['Card_Category'] == 'Blue', 'Card_Category'] = 1\ndf.loc[df['Card_Category'] == 'Silver', 'Card_Category'] = 2\ndf.loc[df['Card_Category'] == 'Gold', 'Card_Category'] = 3\ndf.loc[df['Card_Category'] == 'Platinum', 'Card_Category'] = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's divide the data to features and label and split it to train-test accordingly:","metadata":{}},{"cell_type":"code","source":"Y = df['Attrition_Flag'].to_numpy()\nY[Y=='Existing Customer'] = 1\nY[Y=='Attrited Customer'] = 2\nY = Y.astype('int')\n\nX = pd.get_dummies(df.drop(['CLIENTNUM', 'Attrition_Flag'], axis=1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:56:31.943253Z","iopub.execute_input":"2021-07-21T09:56:31.943649Z","iopub.status.idle":"2021-07-21T09:56:31.979032Z","shell.execute_reply.started":"2021-07-21T09:56:31.943618Z","shell.execute_reply":"2021-07-21T09:56:31.977956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check target balance:","metadata":{}},{"cell_type":"code","source":"print(np.unique(Y, return_counts=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:53:22.589162Z","iopub.execute_input":"2021-07-21T09:53:22.589731Z","iopub.status.idle":"2021-07-21T09:53:22.599686Z","shell.execute_reply.started":"2021-07-21T09:53:22.589682Z","shell.execute_reply":"2021-07-21T09:53:22.598657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The label is imbalanced. Let's use SMOTE algo in order to balance the data:","metadata":{}},{"cell_type":"code","source":"sm = SMOTE(random_state=42)\nx_res, y_res = sm.fit_resample(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:57:14.091436Z","iopub.execute_input":"2021-07-21T09:57:14.091905Z","iopub.status.idle":"2021-07-21T09:57:14.144087Z","shell.execute_reply.started":"2021-07-21T09:57:14.091865Z","shell.execute_reply":"2021-07-21T09:57:14.143017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check that the data is now more balanced:","metadata":{}},{"cell_type":"code","source":"print(np.unique(y_res, return_counts=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T09:57:49.9699Z","iopub.execute_input":"2021-07-21T09:57:49.970305Z","iopub.status.idle":"2021-07-21T09:57:49.976746Z","shell.execute_reply.started":"2021-07-21T09:57:49.970252Z","shell.execute_reply":"2021-07-21T09:57:49.975731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\nI'll use serveral common classification ML algos. I'll use the Recall score as my main performance measurement since it's much more meaningful for this specific task of churn prediction.","metadata":{}},{"cell_type":"code","source":"def ClassPrediction(classifier, mdl):\n  model = classifier.fit(x_res, y_res)\n  y_hat = model.predict(x_train)\n  acc = recall_score(y_train, y_hat, pos_label=2)\n  results.loc[mdl, 'Train'] = acc\n  y_hat = model.predict(x_test)\n  acc = recall_score(y_test, y_hat, pos_label=2)\n  results.loc[mdl, 'Test'] = acc\n\n# Storing Results\nresults = pd.DataFrame()\n\n# Models\nstage = 'Classification Model'\nClassPrediction(DecisionTreeClassifier(), 'Decision Tree')\nClassPrediction(RandomForestClassifier(), 'Random Forest')\nClassPrediction(GradientBoostingClassifier(), 'Gradient Boost')\nClassPrediction(AdaBoostClassifier(), 'AdaBoost')\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T10:10:29.632846Z","iopub.execute_input":"2021-07-21T10:10:29.63324Z","iopub.status.idle":"2021-07-21T10:10:36.931598Z","shell.execute_reply.started":"2021-07-21T10:10:29.633204Z","shell.execute_reply":"2021-07-21T10:10:36.930187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: Gradient Boost had achieved the best Recall Score of ~88.9% with respect to the test set with no visible overfitting.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}