{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Let me check your Diabetese***\n\n![image.png](https://res.cloudinary.com/grohealth/image/upload/c_fill,f_auto,fl_lossy,h_650,q_auto,w_1085/v1581695681/DCUK/Content/causes-of-diabetes.png)\n\nDiabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy.","metadata":{}},{"cell_type":"markdown","source":"## Data Description\n\n**Context**\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n**Content**\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n**Acknowledgements**\n\nSmith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.","metadata":{}},{"cell_type":"markdown","source":"### **Importing Required libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.019926Z","iopub.execute_input":"2021-07-07T10:47:33.02034Z","iopub.status.idle":"2021-07-07T10:47:33.854397Z","shell.execute_reply.started":"2021-07-07T10:47:33.020257Z","shell.execute_reply":"2021-07-07T10:47:33.853327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Load the Datasets**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.855909Z","iopub.execute_input":"2021-07-07T10:47:33.856224Z","iopub.status.idle":"2021-07-07T10:47:33.899375Z","shell.execute_reply.started":"2021-07-07T10:47:33.856194Z","shell.execute_reply":"2021-07-07T10:47:33.898307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Shape of Datasets**","metadata":{}},{"cell_type":"code","source":"print(\"Shape of datasets:\",df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.903064Z","iopub.execute_input":"2021-07-07T10:47:33.903388Z","iopub.status.idle":"2021-07-07T10:47:33.910782Z","shell.execute_reply.started":"2021-07-07T10:47:33.903352Z","shell.execute_reply":"2021-07-07T10:47:33.909608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Checking Null Value**","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.912428Z","iopub.execute_input":"2021-07-07T10:47:33.912812Z","iopub.status.idle":"2021-07-07T10:47:33.927241Z","shell.execute_reply.started":"2021-07-07T10:47:33.912769Z","shell.execute_reply":"2021-07-07T10:47:33.926113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **correlation**\n\nIn statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related.","metadata":{}},{"cell_type":"code","source":"corr=df.corr()\ncorr","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.929019Z","iopub.execute_input":"2021-07-07T10:47:33.929501Z","iopub.status.idle":"2021-07-07T10:47:33.953709Z","shell.execute_reply.started":"2021-07-07T10:47:33.929423Z","shell.execute_reply":"2021-07-07T10:47:33.952784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **correlation using Heatmap**\n\n***Heatmap***\n\nA heat map is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions. The variation in color may be by hue or intensity, giving obvious visual cues to the reader about how the phenomenon is clustered or varies over space.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.heatmap(corr,annot=True,cmap='BuPu')","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:33.955007Z","iopub.execute_input":"2021-07-07T10:47:33.955405Z","iopub.status.idle":"2021-07-07T10:47:34.655977Z","shell.execute_reply.started":"2021-07-07T10:47:33.955318Z","shell.execute_reply":"2021-07-07T10:47:34.655162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let’s look at how much each attribute correlates with the **Outcome**","metadata":{}},{"cell_type":"code","source":"corr['Outcome'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.657192Z","iopub.execute_input":"2021-07-07T10:47:34.657625Z","iopub.status.idle":"2021-07-07T10:47:34.666978Z","shell.execute_reply.started":"2021-07-07T10:47:34.657583Z","shell.execute_reply":"2021-07-07T10:47:34.665911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation coefficient ranges from –1 to 1. When it is close to 1,When the coefficient is close to –1, it means that there is a strong negative correlation. coefficients close to zero mean that there is no\nlinear correlation.","metadata":{}},{"cell_type":"markdown","source":"![image.png](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png)","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.671112Z","iopub.execute_input":"2021-07-07T10:47:34.671788Z","iopub.status.idle":"2021-07-07T10:47:34.715805Z","shell.execute_reply.started":"2021-07-07T10:47:34.671751Z","shell.execute_reply":"2021-07-07T10:47:34.714855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"look at the columns ('Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI') minimum value. which is zero. \n\nCan minimum value of above listed columns be zero ?\n\nNo. this value doesn't make any sense\n\n","metadata":{}},{"cell_type":"code","source":"col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.717853Z","iopub.execute_input":"2021-07-07T10:47:34.718155Z","iopub.status.idle":"2021-07-07T10:47:34.722409Z","shell.execute_reply.started":"2021-07-07T10:47:34.718128Z","shell.execute_reply":"2021-07-07T10:47:34.721465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in col:\n    df[i].replace(0,df[i].mean(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.723899Z","iopub.execute_input":"2021-07-07T10:47:34.724262Z","iopub.status.idle":"2021-07-07T10:47:34.74444Z","shell.execute_reply.started":"2021-07-07T10:47:34.72423Z","shell.execute_reply":"2021-07-07T10:47:34.743418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.746142Z","iopub.execute_input":"2021-07-07T10:47:34.74647Z","iopub.status.idle":"2021-07-07T10:47:34.80504Z","shell.execute_reply.started":"2021-07-07T10:47:34.746439Z","shell.execute_reply":"2021-07-07T10:47:34.803833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Exploratory Data Analysis***\n\n![image.png](https://devopedia.org/images/article/75/4833.1523705164.jpg)\n\n**Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. ... It can also help determine if the statistical techniques you are considering for data analysis are appropriate.**\n\n","metadata":{}},{"cell_type":"markdown","source":"### **Let's Explore Independent variable**","metadata":{}},{"cell_type":"code","source":"df.hist(figsize=(20,15))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:34.807002Z","iopub.execute_input":"2021-07-07T10:47:34.807298Z","iopub.status.idle":"2021-07-07T10:47:36.472099Z","shell.execute_reply.started":"2021-07-07T10:47:34.807262Z","shell.execute_reply":"2021-07-07T10:47:36.470994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A histogram is a chart that shows frequencies for. intervals of values of a metric variable. Such intervals as known as “bins” and they all have the same widths.","metadata":{}},{"cell_type":"markdown","source":"Above histograme clearly show the data is widely dispersed. so firstly we check the varience of each columns","metadata":{}},{"cell_type":"code","source":"df.var()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:36.473611Z","iopub.execute_input":"2021-07-07T10:47:36.474059Z","iopub.status.idle":"2021-07-07T10:47:36.48334Z","shell.execute_reply.started":"2021-07-07T10:47:36.474015Z","shell.execute_reply":"2021-07-07T10:47:36.482229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we use **StandardScaler**\n\nStandardScaler is useful for the features that follow a Normal distribution.\n\nStandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way. StandardScaler can be influenced by outliers (if they exist in the dataset) since it involves the estimation of the empirical mean and standard deviation of each feature.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX =  pd.DataFrame(ss.fit_transform(df.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:36.485496Z","iopub.execute_input":"2021-07-07T10:47:36.485951Z","iopub.status.idle":"2021-07-07T10:47:36.635972Z","shell.execute_reply.started":"2021-07-07T10:47:36.485905Z","shell.execute_reply":"2021-07-07T10:47:36.635008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['Age'],color='green')","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:36.637369Z","iopub.execute_input":"2021-07-07T10:47:36.637656Z","iopub.status.idle":"2021-07-07T10:47:36.911625Z","shell.execute_reply.started":"2021-07-07T10:47:36.637627Z","shell.execute_reply":"2021-07-07T10:47:36.910642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.countplot(x='Age',data=df,hue='Outcome')","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:36.91315Z","iopub.execute_input":"2021-07-07T10:47:36.913569Z","iopub.status.idle":"2021-07-07T10:47:37.906023Z","shell.execute_reply.started":"2021-07-07T10:47:36.913526Z","shell.execute_reply":"2021-07-07T10:47:37.904999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Boxplot***\n\nn descriptive statistics, a box plot or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending from the boxes indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='Outcome',y='Age',data=df)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:37.907349Z","iopub.execute_input":"2021-07-07T10:47:37.90765Z","iopub.status.idle":"2021-07-07T10:47:38.050045Z","shell.execute_reply.started":"2021-07-07T10:47:37.907619Z","shell.execute_reply":"2021-07-07T10:47:38.049254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x='Pregnancies',hue='Outcome',data=df)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:38.051037Z","iopub.execute_input":"2021-07-07T10:47:38.051427Z","iopub.status.idle":"2021-07-07T10:47:38.400328Z","shell.execute_reply.started":"2021-07-07T10:47:38.051398Z","shell.execute_reply":"2021-07-07T10:47:38.399594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Pairplot***\n\npairplot() : To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. This shows the relationship for (n, 2) combination of variable in a DataFrame as a matrix of plots and the diagonal plots are the univariate plots.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df,hue='Outcome')","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:38.401311Z","iopub.execute_input":"2021-07-07T10:47:38.401749Z","iopub.status.idle":"2021-07-07T10:47:57.17921Z","shell.execute_reply.started":"2021-07-07T10:47:38.401716Z","shell.execute_reply":"2021-07-07T10:47:57.178147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X\ny=df['Outcome']","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:57.180479Z","iopub.execute_input":"2021-07-07T10:47:57.180801Z","iopub.status.idle":"2021-07-07T10:47:57.184757Z","shell.execute_reply.started":"2021-07-07T10:47:57.180763Z","shell.execute_reply":"2021-07-07T10:47:57.1838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Models***\n\n![image.png](https://miro.medium.com/max/1920/1*saKqNtSnrKdVVEC772ewzw.png)","metadata":{}},{"cell_type":"markdown","source":"***Split the Data Set***","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.3,random_state=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:57.186121Z","iopub.execute_input":"2021-07-07T10:47:57.186424Z","iopub.status.idle":"2021-07-07T10:47:57.266426Z","shell.execute_reply.started":"2021-07-07T10:47:57.186395Z","shell.execute_reply":"2021-07-07T10:47:57.265388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Logistic regression**\n\nthe logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.\n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression(C=1,penalty='l2')\nLR.fit(X_train,Y_train)\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,LR.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,LR.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:57.267904Z","iopub.execute_input":"2021-07-07T10:47:57.268208Z","iopub.status.idle":"2021-07-07T10:47:57.386008Z","shell.execute_reply.started":"2021-07-07T10:47:57.268167Z","shell.execute_reply":"2021-07-07T10:47:57.384978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Random Forest Classifier**\n\nRandom forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object.\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF=RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nRF.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,RF.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,RF.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:47:57.388753Z","iopub.execute_input":"2021-07-07T10:47:57.389102Z","iopub.status.idle":"2021-07-07T10:47:57.784394Z","shell.execute_reply.started":"2021-07-07T10:47:57.389067Z","shell.execute_reply":"2021-07-07T10:47:57.783575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **KNearestNeighbors**","metadata":{}},{"cell_type":"markdown","source":"K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,knn.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,knn.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T11:15:08.043189Z","iopub.execute_input":"2021-07-07T11:15:08.043722Z","iopub.status.idle":"2021-07-07T11:15:08.09801Z","shell.execute_reply.started":"2021-07-07T11:15:08.043675Z","shell.execute_reply":"2021-07-07T11:15:08.097155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Support vector machine**\n\nIn machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n\nA support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text.\n\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\n**The advantages of support vector machines are:**\n\n1. Effective in high dimensional spaces.\n\n2. Still effective in cases where number of dimensions is greater than the number of samples.\n\n3. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n4. Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\n**The disadvantages of support vector machines include:**\n\n1. If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n\n2. SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,svm.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,svm.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T11:46:13.422008Z","iopub.execute_input":"2021-07-07T11:46:13.422361Z","iopub.status.idle":"2021-07-07T11:46:13.463556Z","shell.execute_reply.started":"2021-07-07T11:46:13.422331Z","shell.execute_reply":"2021-07-07T11:46:13.462422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **DecisionTreeClassifier**\n\nA decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.\n\nclass sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDTC= DecisionTreeClassifier(criterion='entropy',max_depth=5)\nDTC.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,DTC.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,DTC.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T11:52:07.00248Z","iopub.execute_input":"2021-07-07T11:52:07.002835Z","iopub.status.idle":"2021-07-07T11:52:07.019874Z","shell.execute_reply.started":"2021-07-07T11:52:07.002798Z","shell.execute_reply":"2021-07-07T11:52:07.018683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **GradientBoostingClassifier**\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\nclass sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nGD=GradientBoostingClassifier()\nGD.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,GD.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,GD.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T11:57:10.232779Z","iopub.execute_input":"2021-07-07T11:57:10.233158Z","iopub.status.idle":"2021-07-07T11:57:10.391793Z","shell.execute_reply.started":"2021-07-07T11:57:10.233127Z","shell.execute_reply":"2021-07-07T11:57:10.390686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **XGBClassifier**\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\n","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nXGB=XGBClassifier(booster = 'gbtree', learning_rate = 0.1, max_depth=6,n_estimators = 10)\nXGB.fit(X_train,Y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,XGB.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,XGB.predict(X_test))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:03:07.943565Z","iopub.execute_input":"2021-07-07T12:03:07.944013Z","iopub.status.idle":"2021-07-07T12:03:08.004942Z","shell.execute_reply.started":"2021-07-07T12:03:07.943977Z","shell.execute_reply":"2021-07-07T12:03:08.003826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Thank you so much for Reading this Notebook***\n**Please review and kindly comments the issue**\n\n**all type of review are welcomed**\n\n![image.png](https://static.vecteezy.com/system/resources/thumbnails/000/412/892/small_2x/964.jpg)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}