{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Step by step EDA and statistical analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is work in progress...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If there are any questions, comments, suggestions feel free to point out :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing libraires","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport math\n\nimport matplotlib.pyplot as plt \nimport seaborn as sb\nfrom matplotlib import style\n# style.use('fivethirtyeight')\nstyle.use('ggplot')\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport os ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATADIR = '../input/summer-products-and-sales-in-ecommerce-wish'\nos.listdir(DATADIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(DATADIR + '/summer-products-with-rating-and-performance_2020-08.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Theme, crawl month contains only one unique value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_missing_data(df):\n    columns_with_null = df.columns[df.isna().sum() > 0]\n    null_pct = (df[columns_with_null].isna().sum() / df.shape[0]).sort_values(ascending=False) * 100\n    plt.figure(figsize=(8,6));\n    sb.barplot(y = null_pct.index, x = null_pct, orient='h')\n    plt.title('% Na values in dataframe by columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_missing_data(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets take a look at the columns with more than 50% null values and check whether they are useful or not**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['merchant_profile_picture'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"merchant_profile_picture contains the url to merchants profile picture where more than 80% of data is missing, for now let's check the others","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values: \", df['has_urgency_banner'].unique())\nprint(\"Value counts: \", df['has_urgency_banner'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"has_urgency_banner is a binary column which tells us whether the product has an urgency banner or not, so we can replace nan's with 0's to get rid of the nan's ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['has_urgency_banner'] = df['has_urgency_banner'].replace(np.nan,0)\nprint(\"Unique values: \", df['has_urgency_banner'].unique())\nprint(\"Value counts: \", df['has_urgency_banner'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['urgency_text'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Quantité limitée !' translates to 'Quantity Limited' and 'Réduction sur les achats en gros' means 'discount on wholesale purchases' rest are all nan, let's make them right","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['urgency_text']=df['urgency_text'].replace({'Quantité limitée !':'QuantityLimited',\n                                               'Réduction sur les achats en gros':'WholesaleDiscount',\n                                               np.nan:'noText'})\nprint(df['urgency_text'][:5])\nprint(df['urgency_text'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's process the columns with ratings, all the rating count columns has same number of values missing i.e. 45 missing values but the rating_count column has no na values, lets check the ratin_count where values are mising in other rating count columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_columns = ['rating_one_count','rating_two_count','rating_three_count','rating_four_count','rating_five_count']\ndf[rating_columns] = df[rating_columns].fillna(value=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['rating_five_count']==-1,'rating_count'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"all values in the rating_count column are 0 where there are na values in other rating count columns so lets fill 0 in place of the na values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[rating_columns]=df[rating_columns].replace(-1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's move to the remaining columns product_variation_size_id, merchant_name, merchant_name_info_subtitle, origin_country","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['origin_country'].unique())\nprint(df['product_color'].unique())\nprint(df['product_variation_size_id'].unique())\nprint(df['merchant_name'].unique())\nprint(df['merchant_info_subtitle'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data is categorical, so we can replace the nan values with an unknown token 'Unknown'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_cat_cols = ['origin_country','product_color','product_variation_size_id','merchant_name','merchant_info_subtitle']\ndf[nan_cat_cols] = df[nan_cat_cols].replace(np.nan,'Unknown')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can check if we handled the nan values properply","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns[df.isna().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All null values were handled except for the 'merchant_profile_picture' which we will look after later","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check for identical rows and eliminated them for the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.drop_duplicates()\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for products a unique identifier is product_id and same goes for merchant_id ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Duplicate product_id :\",df['product_id'].duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This represents that even after removing identical values still there duplicates of 'product_id' are present, this is because same product can be soold by different merchants on different price, hence this seems to a clear indicator that merchant_id influences the price of product","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsb.distplot(df['price'], color='red', label='Price')\nsb.distplot(df['retail_price'], color='blue', label='Retail price')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot indicates a right skewed distribution but not very Clear.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kwargs = {'cumulative':True}\nf, axes = plt.subplots(1,2, figsize=(14,6))\nf.suptitle('CDF of Price and Retail Price')\nsb.distplot(df['price'].values,kde_kws=kwargs, hist_kws=kwargs, color='red', label='Price', ax=axes[0]);\nsb.distplot(df['retail_price'].values,kde_kws=kwargs, hist_kws=kwargs, color='blue', label='Retail Price', ax=axes[1]);\naxes[0].set(xlabel='Price');\naxes[1].set(xlabel='Retail Price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CDFs are more useful inorder to visualize the data more efficiently.\n* CDF of price reveals that 97% of products are listed for less than aproximate price 19-20\n* CDF of Price closely represents the CDF curve of Normal distribution which can be summarized efficiently except for the 3% data\n* Incase of Retail price the distribution is not very much smooth and contains price gaps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(x=df['retail_price'], name='Retail Price'))\nfig.add_trace((go.Box(x=df['price'], name='Price')))\nfig['layout']['title'] = 'Distribution of Price and Retail Price'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With Boxplots we can easily spot the outliers and quartiles\n* The Upper fence of Price is at 18 i.e most of the data is priced less tha 18\n* There an item wiht price of 49 i.e clearly an oulier as it is far away from the Inter Quartile Range (Q3 - Q1) \n* Box plot of Retail price is much more spread out,  there is huge difference of 195 between the upper fence and max data point","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's now explore these outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outliers = df[df['price'] > 18]\nprint(\"Number of outliers: \",df_outliers.shape[0])\nprint(\"Outlier: \", df_outliers[df_outliers['price']==49])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one is a premium item with price much higher than the average price and number of units sold are very low compared to average units sold (4422), with an above average rating of 4.67 and rating density of 6% as only 6 poeple posted a review out of 100","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df, x='units_sold', y='price',marginal_x='box', title='Price vs Units Sold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There a clear relationship of price and units sold.\n* Higher the price lesser the units sold\n* There are some cases where the price is low still the units sold are below average, possible reasons the product might not be upto the mark as per the buyers or there are some other factors affecting the price we haven't touched yet\n* units sold seems be a range not continous values \n* median of units sold is 1000, by this we can consider that products with units sold below 1000 (inclusive) were below average and products with units sold are very successfull.\n* It totally depends on your business goals which price range you want to focus on, for now I will take the unsupervised approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#range for units sold\nsorted(df['units_sold'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nclusters = {}\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i).fit(df[['units_sold']])\n    clusters[i] = kmeans.inertia_\n    \nplt.plot(list(clusters.keys()), list(clusters.values()));\nplt.xlabel('no. of clusters');\nplt.ylabel('kmeans inertia');   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By performing clustering we can see that units_sold can be clustered in 3 categories (optimal) as the inertia curve smooths out after 3 clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#order cluster method\ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['units_sold_cluster'] = KMeans(n_clusters=3).fit(df[['units_sold']]).predict(df[['units_sold']])\ndf = order_cluster('units_sold_cluster','units_sold',df,True)\ndf.groupby(['units_sold_cluster'])['units_sold'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have a clear picture of top selling, and price range of products","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df,x='units_sold',y='rating', color='units_sold_cluster', marginal_y ='box',title='Rating vs units sold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Median for rating is 3.85 and the products in top selling cluster has rating between 3.35 to 4.1 seems very reasonable\n* Rating is very important to determine the potential of product\n* Still there are some products with 5 star rating yet unable to cross the 100-1000 unit sold line\n* there are some really bad performing products with rating below 3","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"px.scatter(df,x='rating',y='merchant_rating', color='units_sold_cluster', marginal_y ='box',title='Merchant Rating vs units sold', opacity=0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df,x='rating', y='product_variation_inventory', color='units_sold_cluster', title='Product variation vs Rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df,x='rating_count',y='rating', color='units_sold_cluster', title='Rating vs Rating count')\nfig.add_trace(go.Scatter(x=np.ones((len(df)))*1103,y=df['rating'],name='Threshold 1'))\nfig.add_trace(go.Scatter(x=np.ones((len(df)))*7773, y=df['rating'],name='Threshold 2'))\nfig.update_layout(showlegend=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above visualization we can conclude that products sold by merchants belonging to cluster 2 and 1 are Top selling,most liked and trusted by buyers\n* There's some kind of thresholding that can be done on rating and rating count to separate the 3 categories of products\n* still there are few overlapping data points","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df,x='retail_price', y='price',color='units_sold_cluster',marginal_y='box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the top selling products seems be concentrated to the left where the price difference is much siginificant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df, x='price', y='shipping_option_price', color= 'units_sold_cluster', title='Shipping price vs Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People always prefer paying less shipping charges \nwe can see that most selling products has low shipping charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features= ['price','retail_price','units_sold','rating','rating_count','shipping_option_price','product_variation_inventory','merchant_rating','merchant_rating_count']\ncorr = df[features].corr(method='spearman')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8));\nsb.heatmap(corr,annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at other binary attributes which might affect the sales of product","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['uses_ad_boosts'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['uses_ad_boosts'])['units_sold'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Consider these two groups of products one uses ad boost other dosen't\n* There is very small difference between the means of the two groups\n* Does using ad boost results in more success of products\n* How big the difference is bwetween these two two groups?\n* Is the effect statistically significant?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.query('uses_ad_boosts == 0')['units_sold'].values, df.query('uses_ad_boosts == 1')['units_sold'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking how big the effect is between two groups","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![Cohen's d](https://tien-nguyen.github.io/images/cohen-d.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the effect size \n\ndef EffectSize(group1, group2):\n    diff = group1.mean()- group2.mean() \n    var1 = group1.var()\n    var2 = group2.var()\n    n1,n2 = len(group1), len(group2)\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff/math.sqrt(pooled_var)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g1,g2 = data\nprint(\"Difference in means: \",g1.mean()-g2.mean())\nEffectSize(g1,g2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The effect size is too small to make any difference","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's define a null hypothesis that there is no effect of using ad boosting on units sold with threshold of 0.05","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class HypothesisTest(object):\n    \n    def __init__(self, data):\n        self.data = data\n        self.MakeModel()\n        self.actual = self.TestStatistic(data)\n        \n    def PValue(self, iters=1000):\n        self.test_stats = [self.TestStatistic(self.RunModel()) for _ in range(iters)]\n        \n        count = sum(1 for x in self.test_stats if x > self.actual)\n        return count/iters\n    \n    def TestStatistic(self, data):\n        raise UnimplementedMethodException()\n    def MakeModel(self):\n        pass\n    def RunModel(self):\n        raise UnimplementedMethodException()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiffMeans(HypothesisTest):\n    def TestStatistic(self, data):\n        group1,group2 =data\n#         test_stat = abs(group1.mean() - group2.mean())\n        test_stat = abs(EffectSize(group1, group2))\n        return test_stat\n    def MakeModel(self):\n        group1, group2 = self.data\n        self.n, self.m = len(group1), len(group2)\n        self.pool = np.hstack((group1,group2))\n        \n    def RunModel(self):\n        np.random.shuffle(self.pool)\n        data = self.pool[:self.n], self.pool[self.n:]\n        return data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = DiffMeans(data)\ntest.PValue()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The resulting P-value is much greater than the threshold we set hence we accept the null hypothesis that there is no statistically significant effect of ad boosting\n* It is possible that there might be an significant effect but we didn't see it here because of less amount of data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets see discount applied","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['difference'] = df['retail_price'] - df['price']\ndf['discount'] = df['difference']/df['retail_price'] *100\nplt.figure(figsize=(12,6))\nsb.distplot(df['discount']);\nplt.title('Distribution of Discount');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we can there are some products which were sold at discount of 95%(thats some crazy sale going on)\n* There are products which were sold at price more than the retail price, must some products with huge demand as people are buying it at greater prices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df,x='discount', y='rating_count', color='units_sold_cluster')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be no specific relation between units sold and discount provided ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rating_score'] = df['rating']*df['rating_count']\ndf['rating_score'] =df['rating_score']/df['rating_score'].max()\nplt.figure(figsize=(12,6))\nsb.distplot(df['rating_score']);\nplt.title('Distribution of Rating Score');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df,x='rating_score',y='units_sold', color='units_sold', title='Units Sold vs Rating score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* By combining the product rating and rating count we defined a metric rating score which seems to be very useful to understand the sales of a product","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_clusters(df,column):\n    clusters = {}\n    for i in range(1,8):\n        kmeans = KMeans(n_clusters=i).fit(df[[column]])\n        clusters[i] = kmeans.inertia_\n\n    plt.plot(list(clusters.keys()), list(clusters.values()));\n    plt.title(f'{column} clusters')\n    plt.xlabel('no. of clusters');\n    plt.ylabel('kmeans inertia');   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_clusters(df,'rating_score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 clusters for rating score will be optimal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3).fit(df[['rating_score']])\ndf['rating_score_cluster'] = kmeans.predict(df[['rating_score']])\ndf= order_cluster(df=df,cluster_field_name='rating_score_cluster',target_field_name='rating_score',ascending=True)\ndf.groupby('rating_score_cluster')[['rating','rating_count','units_sold']].describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have an even more clear picture of whats selling, liked by people and what are some below average products","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['overall_score'] = df['rating_score_cluster'] + df['units_sold_cluster']\nmake_clusters(df,'overall_score');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here 2 clusters seems to be optimal ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans= KMeans(n_clusters=2).fit(df[['overall_score']])\ndf['overall_score_cluster'] = kmeans.predict(df[['overall_score']])\ndf = order_cluster(df=df,target_field_name='overall_score', cluster_field_name='overall_score_cluster', ascending=True)\ndf.groupby('overall_score_cluster')[['rating_score','price','units_sold']].describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With this overall score we have identified the groups of top selling, most liked products which are the ones generating high revenue and products performing below average\n* There 213 successfull products with range of units sold from 10K to 100K at a mean price of 8.45\n* In the other cluster the mean price is 8.34 but mean units sold are much low\n* another thing to notice is that people prefer a reasonable price as in successfull cluster the max price is 19, products in this cluster must be worth the price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['title_orig','units_sold','price','rating_score','units_sold_cluster','rating_score_cluster','overall_score_cluster']].sample(frac=.25).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}