{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:01:58.148995Z","iopub.execute_input":"2021-08-13T05:01:58.149391Z","iopub.status.idle":"2021-08-13T05:01:58.156035Z","shell.execute_reply.started":"2021-08-13T05:01:58.149359Z","shell.execute_reply":"2021-08-13T05:01:58.155194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataFrame\n\ndata = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\nprint (type(data))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:31.902284Z","iopub.execute_input":"2021-08-13T05:06:31.902814Z","iopub.status.idle":"2021-08-13T05:06:31.934755Z","shell.execute_reply.started":"2021-08-13T05:06:31.902762Z","shell.execute_reply":"2021-08-13T05:06:31.933558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (data.info())\n\nprint (data['bmi'].describe()) # For Statistical Info\n\n# We can calculate IQR and remove outlier from the column of 'bmi'\n\nQ1 = data['bmi'].quantile(0.25)\nQ3 = data['bmi'].quantile(0.75)\nIQR = Q3 - Q1\nprint (\"The interquartile range between Q3 and Q1 >>\" , IQR)\nfiltered_data = data.query('(@Q1 - 1.5 * @IQR) <= bmi <= (@Q3 + 1.5 * @IQR)') # query function\n\nfiltered_data # This is filtered DataFrame\n                    # We now calculate the mean(),mode(),median() value from the filtered_data\n                    # What we should do is calculate the mean ( ) from only X_train not from the whole dataset ( which will posses risk of overfit on test_data)\n                        \nbmi_mean = filtered_data['bmi'].mean()\nprint (bmi_mean)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:33.398136Z","iopub.execute_input":"2021-08-13T05:06:33.398656Z","iopub.status.idle":"2021-08-13T05:06:33.467683Z","shell.execute_reply.started":"2021-08-13T05:06:33.398624Z","shell.execute_reply":"2021-08-13T05:06:33.466406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the total number of missing values contained in our DataFrame\n# We must choose which features to use as training and which for labeled data\n\nX = data.drop('stroke',axis = 1) # training features\ny = data.stroke # labeled features # Pandas Series","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:33.629066Z","iopub.execute_input":"2021-08-13T05:06:33.629444Z","iopub.status.idle":"2021-08-13T05:06:33.635583Z","shell.execute_reply.started":"2021-08-13T05:06:33.629414Z","shell.execute_reply":"2021-08-13T05:06:33.634831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum() # We can see bmi column contain","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:33.796182Z","iopub.execute_input":"2021-08-13T05:06:33.796703Z","iopub.status.idle":"2021-08-13T05:06:33.809333Z","shell.execute_reply.started":"2021-08-13T05:06:33.79666Z","shell.execute_reply":"2021-08-13T05:06:33.808459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-Test split stage\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,test_size = 0.2,random_state = 0)\n\n\n# Creating the list of categorical data\ncategorical_col  = [i for i in X_train.columns if X_train[i].dtype == \"object\"]\nprint (categorical_col)\n\n# Safely ordinal encoded\n# This step is not necessary right now but when we working with large data set we \ngood_label_col = [col for col in categorical_col if \n                   set(X_test[col]).issubset(set(X_train[col]))]\n\nprint (\"Good_label\",good_label_col)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:33.904626Z","iopub.execute_input":"2021-08-13T05:06:33.9052Z","iopub.status.idle":"2021-08-13T05:06:33.920596Z","shell.execute_reply.started":"2021-08-13T05:06:33.905158Z","shell.execute_reply":"2021-08-13T05:06:33.919449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder # use this for categorical data when there is no relationship\nfrom sklearn.preprocessing import OrdinalEncoder # use ordinal encoder for ordered relationship data\nfrom sklearn.preprocessing import LabelBinarizer # use this for binary classification (Yes or No)\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Training features contain categorical data and need encoding\n\n\n# Defining Encoders \nOH_imputer = OneHotEncoder(handle_unknown='ignore',sparse = False)\n# binary_imputer = LabelBinarizer() # Here is the problem\nordinal_imputer = OrdinalEncoder()\nsimple_imputer = SimpleImputer(strategy='constant',fill_value = 28.313586163784137) # we imputed with data\n\n\n# Defining the features that would be imputed with respect to suitable Encoder (Corresponding features)\nbinary_feature_col = [col for col in X_train[categorical_col] if X_train[col].nunique() < 3] # Notice iteration only use for categorical_col\none_hot_col = [col for col in X_train[categorical_col] if X_train[col].nunique() >= 3] # Notice gender has unique = 3\nsimple_impute_col = ['bmi']\n# Since there is no ordinal data column in our dataset we leave this step\n\n\n# Creating column transformer\nimputer = ColumnTransformer([\n    ('ordinal_imputer',ordinal_imputer,binary_feature_col), # we use binary_feature_col here for ordinal encode because our previous LabelBinarizer() gives us error\n    ('one_hot_imputer',OH_imputer,one_hot_col),\n    ('Simple_imputer',simple_imputer,simple_impute_col)\n                                                ],remainder = 'passthrough')\n\n\n# Creating pipeline\npipeline = Pipeline(steps = [(['preprocessor',imputer])])","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:34.075662Z","iopub.execute_input":"2021-08-13T05:06:34.076018Z","iopub.status.idle":"2021-08-13T05:06:34.097924Z","shell.execute_reply.started":"2021-08-13T05:06:34.075986Z","shell.execute_reply":"2021-08-13T05:06:34.096892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation\nimputed_X_train = pipeline.fit_transform(X_train)\nimputed_X_test = pipeline.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:34.521647Z","iopub.execute_input":"2021-08-13T05:06:34.522193Z","iopub.status.idle":"2021-08-13T05:06:34.560117Z","shell.execute_reply.started":"2021-08-13T05:06:34.522144Z","shell.execute_reply":"2021-08-13T05:06:34.559269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Models\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate= 0.1,n_estimators= 100)\nclf.fit(imputed_X_train,y_train)\nprediction = clf.predict(imputed_X_test)\n\n\n\nfrom sklearn.metrics import accuracy_score\nprint (accuracy_score(prediction,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:06:34.907783Z","iopub.execute_input":"2021-08-13T05:06:34.908305Z","iopub.status.idle":"2021-08-13T05:06:35.644797Z","shell.execute_reply.started":"2021-08-13T05:06:34.908267Z","shell.execute_reply":"2021-08-13T05:06:35.643721Z"},"trusted":true},"execution_count":null,"outputs":[]}]}