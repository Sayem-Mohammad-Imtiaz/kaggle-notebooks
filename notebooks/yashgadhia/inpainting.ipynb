{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install pyamg\nimport pyamg\nimport sys\nimport cv2\nimport time\nimport glob\nimport scipy.sparse\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image \nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch.autograd.variable import Variable\nfrom torchvision import datasets, transforms","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(sys.version)\ndevice='cuda'\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_mask(sample_batch, img_size, mask_type):\n    np.random.seed(seed=int(time.time())) \n    masks = np.ones((sample_batch, img_size, img_size), dtype=np.float32)\n\n    if mask_type == 'center':\n        scale = 0.25\n        low, upper = int(img_size * scale), int(img_size * (1.0 - scale))\n        masks[:, low:upper, low:upper] = 0.\n    elif mask_type == 'random':\n        ratio = 0.8\n        masks[np.random.random((sample_batch, img_size, img_size)) <= ratio] = 0.\n    elif mask_type == 'half':\n        half_types = np.random.randint(4, size=sample_batch)\n        masks = [half_mask(half_types[idx], img_size) for idx in range(sample_batch)]\n        masks = np.asarray(masks)\n    elif mask_type == 'pattern':\n        masks = [pattern_mask(img_size) for _ in range(sample_batch)]\n        masks = np.asarray(masks)\n    else:\n        raise NotImplementedError\n\n    return masks\n\n\ndef half_mask(half_type, img_size):\n    mask = np.ones((img_size, img_size), dtype=np.float32)\n    half = int(img_size / 2.)\n\n    if half_type == 0:  # top mask\n        mask[:half, :] = 0.\n    elif half_type == 1:  # bottom mask\n        mask[half:, :] = 0.\n    elif half_type == 2:  # left mask\n        mask[:, :half] = 0.\n    elif half_type == 3:  # right mask\n        mask[:, half:] = 0.\n    else:\n        raise NotImplementedError\n\n    return mask\n\n\ndef pattern_mask(img_size):\n    num_points, ratio = 3, 0.25\n    mask = np.zeros((img_size, img_size), dtype=np.float32)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n\n    for num in range(num_points):\n        coordinate = np.random.randint(img_size, size=2)\n        mask[coordinate[0], coordinate[1]] = 1.\n        mask = cv2.dilate(mask, kernel, iterations=1)\n\n    while np.sum(mask) < ratio * img_size * img_size:\n        flag = True\n        while flag:\n            coordinate = np.random.randint(img_size, size=2)\n            if mask[coordinate[0], coordinate[1]] == 1.:\n                mask2 = np.zeros((img_size, img_size), dtype=np.float32)\n                mask2[coordinate[0], coordinate[1]] = 1.\n                mask2 = cv2.dilate(mask2, kernel, iterations=1)\n\n                mask[mask + mask2 >= 1.] = 1.\n                flag = False\n\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    return 1. - mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rescale(image,image_size):\n  rescaled = np.resize(image,(image_size,image_size))\n  return torch.from_numpy(rescaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_data = glob.glob(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/*.jpg\")\nlen_data = len(folder_data)\nprint(len_data)\n\ntest_image_paths = folder_data[200000:200512]\n\n\nclass TestDataset(Dataset):\n  def __init__(self, image_paths, mask_type, mask_generator, scale):\n    self.image_paths = image_paths\n    self.mask_type = mask_type\n    self.transforms = transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.CenterCrop(64),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ])\n    self.mask_generator = mask_generator\n    self.scale = scale\n\n  def __getitem__(self, index):\n    image = Image.open(self.image_paths[index])\n    mask = self.mask_generator(1,64,self.mask_type)\n    t_image = self.transforms(image)\n    t_mask = self.scale(mask,64)\n    return t_image,t_mask\n\n  def __len__(self):\n    return len(self.image_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset_center = TestDataset(test_image_paths,mask_type='center',mask_generator=gen_mask,scale=rescale)\ntest_dataset_pattern = TestDataset(test_image_paths,mask_type='pattern',mask_generator=gen_mask,scale=rescale)\ntest_loader_center = torch.utils.data.DataLoader(test_dataset_center, batch_size=16, shuffle=False)\ntest_loader_pattern = torch.utils.data.DataLoader(test_dataset_pattern, batch_size=16, shuffle=False)\nprint(len(test_dataset_center))\nprint(len(test_loader_center))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weights(masks,window_size):\n\n  assert (window_size+1)%2 == 0\n\n  pad_value = (window_size-1)/2\n  padded_masks = F.pad(1-masks, (int(pad_value),int(pad_value),int(pad_value),int(pad_value)), \"constant\", 0)\n  padded_masks.unsqueeze_(1)\n  kernel = torch.ones((window_size,window_size))\n  kernel = kernel.view(1, 1, window_size, window_size)\n  kernel = kernel.to(device)\n  temp = F.conv2d(padded_masks,kernel)\n  temp.squeeze_(1)\n  weights = (1/window_size)*torch.mul(temp,masks)\n  return weights\n\ndef context_loss(outputs,images,masks,window_size):\n  batch_size = images.size()[0]\n  W = get_weights(masks,window_size)\n  W.unsqueeze_(1)\n  W = torch.cat((W,W,W),dim=1)\n  closs = (1/batch_size)*(torch.sum(torch.abs(torch.mul(outputs-images,W))))\n  return closs\n\ndef prior_loss(discriminator_outputs,Lambda):\n  batch_size = discriminator_outputs.size()[0]\n  ploss = (1/batch_size)*(Lambda*torch.sum(-torch.log(discriminator_outputs)))\n  return ploss\n\ndef random_z(batch_size):\n  Z = torch.randn((batch_size,100,1,1), requires_grad=True,device=\"cuda\")\n  return Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_mask(mask):\n    if type(mask[0][0]) is np.ndarray:\n        result = np.ndarray((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n        for i in range(mask.shape[0]):\n            for j in range(mask.shape[1]):\n                if sum(mask[i][j]) > 0:\n                    result[i][j] = 1\n                else:\n                    result[i][j] = 0\n        mask = result\n    return mask\n\n\n\ndef blend(img_target, img_source, img_mask, offset=(0, 0)):\n    # compute regions to be blended\n    region_source = (\n            max(-offset[0], 0),\n            max(-offset[1], 0),\n            min(img_target.shape[0]-offset[0], img_source.shape[0]),\n            min(img_target.shape[1]-offset[1], img_source.shape[1]))\n    region_target = (\n            max(offset[0], 0),\n            max(offset[1], 0),\n            min(img_target.shape[0], img_source.shape[0]+offset[0]),\n            min(img_target.shape[1], img_source.shape[1]+offset[1]))\n    region_size = (region_source[2]-region_source[0], region_source[3]-region_source[1])\n\n    # clip and normalize mask image\n    img_mask = img_mask[region_source[0]:region_source[2], region_source[1]:region_source[3]]\n    img_mask = prepare_mask(img_mask)\n    img_mask[img_mask==0] = False\n    img_mask[img_mask!=False] = True\n\n    # create coefficient matrix\n    A = scipy.sparse.identity(np.prod(region_size), format='lil')\n    for y in range(region_size[0]):\n        for x in range(region_size[1]):\n            if img_mask[y,x]:\n                index = x+y*region_size[1]\n                A[index, index] = 4\n                if index+1 < np.prod(region_size):\n                    A[index, index+1] = -1\n                if index-1 >= 0:\n                    A[index, index-1] = -1\n                if index+region_size[1] < np.prod(region_size):\n                    A[index, index+region_size[1]] = -1\n                if index-region_size[1] >= 0:\n                    A[index, index-region_size[1]] = -1\n    A = A.tocsr()\n    \n    # create poisson matrix for b\n    P = pyamg.gallery.poisson(img_mask.shape)\n\n    # for each layer (ex. RGB)\n    for num_layer in range(img_target.shape[2]):\n        # get subimages\n        t = img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer]\n        s = img_source[region_source[0]:region_source[2], region_source[1]:region_source[3],num_layer]\n        t = t.flatten()\n        s = s.flatten()\n\n        # create b\n        b = P * s\n        for y in range(region_size[0]):\n            for x in range(region_size[1]):\n                if not img_mask[y,x]:\n                    index = x+y*region_size[1]\n                    b[index] = t[index]\n\n        # solve Ax = b\n        x = pyamg.solve(A,b,verb=False,tol=1e-10)\n\n        # assign x to target image\n        x = np.reshape(x, region_size)\n        x[x>255] = 255\n        x[x<0] = 0\n        x = np.array(x, img_target.dtype)\n        img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer] = x\n\n    return img_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GeneratorNet(torch.nn.Module):\n  def __init__(self):\n    super(GeneratorNet, self).__init__()\n    self.main = nn.Sequential(\n        nn.ConvTranspose2d(100, 1024, kernel_size = 4, stride = 1, padding = 0, bias = False),\n        nn.BatchNorm2d(1024),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1, bias =False),\n        nn.BatchNorm2d(512),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        nn.BatchNorm2d(256),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        nn.BatchNorm2d(128),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(128, 3, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        nn.Tanh()\n    )\n    \n\n  def forward(self, x):\n    #print(x)\n    x = self.main(x)\n    #print(x.shape)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiscriminatorNet(torch.nn.Module):\n  def __init__(self):\n    super(DiscriminatorNet, self).__init__()\n    self.main = nn.Sequential(\n      nn.Conv2d(3, 128, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(128, 256, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.BatchNorm2d(256),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(256, 512, kernel_size = 5, stride = 2, padding =2, bias = False),\n      nn.BatchNorm2d(512),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(512, 1024, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.BatchNorm2d(1024),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(1024, 1, kernel_size = 4, stride = 1, padding = 0, bias = False),\n      nn.Sigmoid()\n    )\n    \n  def forward(self, x):\n    x = self.main(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Generator = GeneratorNet()\nGenerator.load_state_dict(torch.load(\"../input/pretrained-gan-pytorch/g_epoch-30.pth\"))\nGenerator = Generator.to(device)\nGenerator.eval()\nfor param in Generator.parameters():\n  param.requires_grad = False\n\nDiscriminator = DiscriminatorNet()\nDiscriminator.load_state_dict(torch.load(\"../input/pretrained-gan-pytorch/d_epoch-30.pth\"))\nDiscriminator = Discriminator.to(device)\nDiscriminator.eval()\nfor param in Discriminator.parameters():\n  param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (images, masks) in enumerate(test_loader_center):\n  losses = []\n  images = images.to(device)\n  masks = masks.to(device)\n  Z = random_z(batch_size=16)\n  optimizer = optim.Adam([Z])\n  if (i+1)%10==0 or i==0:\n    print(\"Starting Training for Batch \"+str(i+1)+\"...\")\n  for iter in range(1500):\n    outputs = Generator(Z)\n    discriminator_outputs = Discriminator(outputs)\n    closs = context_loss(outputs,images,masks,window_size=7)\n    ploss = prior_loss(discriminator_outputs,Lambda=0.003)\n    inpainting_loss = closs + ploss\n    optimizer.zero_grad()\n    inpainting_loss.backward()\n    optimizer.step()\n    losses.append(inpainting_loss)\n    if (i+1)%10==0 or i==0:\n      if (iter+1)%500==0:\n        print(\"Iteration No. = \"+str(iter+1))\n        print(\"Loss = \"+str(inpainting_loss))\n  if (i+1)%10==0 or i==0:\n    plt.figure(figsize=(10,5))\n    plt.title(\"Inpainting Loss During Training\")\n    plt.plot(losses)\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n  Z_optimal = Z.detach()\n  generated_images = Generator(Z_optimal)\n  generated_images = generated_images.detach().cpu().numpy()\n  generated_images = np.transpose(generated_images,(0,2,3,1))\n  generated_images = ((generated_images+1.)/2.)*255.\n  generated_images = generated_images.astype(np.uint8)\n  images_for_blend = images.detach().cpu().numpy()\n  images_for_blend = np.transpose(images_for_blend,(0,2,3,1))\n  images_for_blend = ((images_for_blend+1.)/2.)*255.\n  images_for_blend = images_for_blend.astype(np.uint8)\n  masks_for_blend = (1.-masks).detach().cpu().numpy()\n  masks_for_blend = (masks_for_blend*255.).astype(np.uint8)\n  masks.unsqueeze_(1)\n  masks = torch.cat((masks,masks,masks),dim=1)\n  temp_mask = masks.detach().cpu().numpy().astype(np.uint8)\n  temp_mask = np.transpose(temp_mask,(0,2,3,1))\n  for j in range(generated_images.shape[0]):\n    masked_image = images_for_blend[j,:,:,:]*temp_mask[j,:,:,:]\n    original_image = Image.fromarray(np.uint8(images_for_blend[j,:,:,:]))\n    masked_image = Image.fromarray(np.uint8(masked_image)) \n    original_image.save(\"{}_Original_Image.png\".format(1+(i*generated_images.shape[0])+j),\"PNG\")\n    masked_image.save(\"{}_Center_Masked_Image.png\".format(1+(i*generated_images.shape[0])+j),\"PNG\")\n    inpainting_result = blend(images_for_blend[j,:,:,:],generated_images[j,:,:,:],masks_for_blend[j,:,:],offset=(0,0))\n    #inpainting_result = (1-temp_mask[j,:,:,:])*generated_images[j,:,:,:] + (temp_mask[j,:,:,:])*images_for_blend[j,:,:,:]\n    inpainted_image = Image.fromarray(np.uint8(inpainting_result))\n    inpainted_image.save(\"{}_Center_Inpainted_Image.png\".format(1+(i*generated_images.shape[0])+j),\"PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (images, masks) in enumerate(test_loader_pattern):\n  losses = []\n  images = images.to(device)\n  masks = masks.to(device)\n  Z = random_z(batch_size=16)\n  optimizer = optim.Adam([Z])\n  if (i+1)%10==0 or i==0:\n    print(\"Starting Training for Batch \"+str(i+1)+\"...\")\n  for iter in range(1500):\n    outputs = Generator(Z)\n    discriminator_outputs = Discriminator(outputs)\n    closs = context_loss(outputs,images,masks,window_size=7)\n    ploss = prior_loss(discriminator_outputs,Lambda=0.003)\n    inpainting_loss = closs + ploss\n    optimizer.zero_grad()\n    inpainting_loss.backward()\n    optimizer.step()\n    losses.append(inpainting_loss)\n    if (i+1)%10==0 or i==0:\n      if (iter+1)%500==0:\n        print(\"Iteration No. = \"+str(iter+1))\n        print(\"Loss = \"+str(inpainting_loss))\n  if (i+1)%10==0 or i==0:\n    plt.figure(figsize=(10,5))\n    plt.title(\"Inpainting Loss During Training\")\n    plt.plot(losses)\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n  Z_optimal = Z.detach()\n  generated_images = Generator(Z_optimal)\n  generated_images = generated_images.detach().cpu().numpy()\n  generated_images = np.transpose(generated_images,(0,2,3,1))\n  generated_images = ((generated_images+1.)/2.)*255.\n  generated_images = generated_images.astype(np.uint8)\n  images_for_blend = images.detach().cpu().numpy()\n  images_for_blend = np.transpose(images_for_blend,(0,2,3,1))\n  images_for_blend = ((images_for_blend+1.)/2.)*255.\n  images_for_blend = images_for_blend.astype(np.uint8)\n  masks_for_blend = (1.-masks).detach().cpu().numpy()\n  masks_for_blend = (masks_for_blend*255.).astype(np.uint8)\n  masks.unsqueeze_(1)\n  masks = torch.cat((masks,masks,masks),dim=1)\n  temp_mask = masks.detach().cpu().numpy().astype(np.uint8)\n  temp_mask = np.transpose(temp_mask,(0,2,3,1))\n  for j in range(generated_images.shape[0]):\n    masked_image = images_for_blend[j,:,:,:]*temp_mask[j,:,:,:]\n    original_image = Image.fromarray(np.uint8(images_for_blend[j,:,:,:]))\n    masked_image = Image.fromarray(np.uint8(masked_image)) \n    #original_image.save(\"{}_Original_Image\".format(1+(i*generated_images.shape[0])+j),\"PNG\")\n    masked_image.save(\"{}_Pattern_Masked_Image.png\".format(1+(i*generated_images.shape[0])+j),\"PNG\")\n    inpainting_result = blend(images_for_blend[j,:,:,:],generated_images[j,:,:,:],masks_for_blend[j,:,:],offset=(0,0))\n    #inpainting_result = (1-temp_mask[j,:,:,:])*generated_images[j,:,:,:] + (temp_mask[j,:,:,:])*images_for_blend[j,:,:,:]\n    inpainted_image = Image.fromarray(np.uint8(inpainting_result))\n    inpainted_image.save(\"{}_Pattern_Inpainted_Image.png\".format(1+(i*generated_images.shape[0])+j),\"PNG\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}