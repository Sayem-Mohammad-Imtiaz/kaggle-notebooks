{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch.autograd.variable import Variable\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(sys.version)\ndevice='cuda'\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 64\ndataset = torchvision.datasets.ImageFolder(root=\"../input/celeba-dataset/img_align_celeba\",\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n                                         shuffle=True, num_workers=2)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GeneratorNet(torch.nn.Module):\n  def __init__(self):\n    super(GeneratorNet, self).__init__()\n    self.main = nn.Sequential(\n        nn.ConvTranspose2d(100, 1024, kernel_size = 4, stride = 1, padding = 0, bias = False),\n        #nn.BatchNorm2d(1024),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1, bias =False),\n        #nn.BatchNorm2d(512),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        #nn.BatchNorm2d(256),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        #nn.BatchNorm2d(128),\n        nn.ReLU(inplace = True),\n\n        nn.ConvTranspose2d(128, 3, kernel_size = 4, stride = 2, padding = 1, bias=False),\n        nn.Tanh()\n    )\n    \n\n  def forward(self, x):\n    #print(x)\n    x = self.main(x)\n    #print(x.shape)\n    return x\n\ngenerator = GeneratorNet()\ngenerator.float()\ngenerator.to(device)\n\ngenerator.apply(weights_init)\n\nprint(generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiscriminatorNet(torch.nn.Module):\n  def __init__(self):\n    super(DiscriminatorNet, self).__init__()\n    self.main = nn.Sequential(\n      nn.Conv2d(3, 128, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(128, 256, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.BatchNorm2d(256),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(256, 512, kernel_size = 5, stride = 2, padding =2, bias = False),\n      nn.BatchNorm2d(512),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(512, 1024, kernel_size = 5, stride = 2, padding = 2, bias = False),\n      nn.BatchNorm2d(1024),\n      nn.LeakyReLU(0.2, inplace=True),\n\n      nn.Conv2d(1024, 1, kernel_size = 4, stride = 1, padding = 0, bias = False)\n    )\n    \n  def forward(self, x):\n    x = self.main(x)\n    return x\n\ndiscriminator = DiscriminatorNet()\ndiscriminator.float()\ndiscriminator.to(device)\n\ndiscriminator.apply(weights_init)\n\nprint(discriminator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizerG = optim.Adam(generator.parameters(), lr = 0.0002, betas = (0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr = 0.0002, betas = (0.5, 0.999))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def noise(size):\n  n = Variable(torch.randn(size, 100, 1, 1))\n  return n.to(device)\n\nsamples = 16\nfixed_noise = noise(samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lossesD = []\nlossesG = []\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n  discriminator.train()\n  generator.train()\n  lossD = 0\n  lossG = 0\n  prob_real = 0\n  prob_fake = 0 \n  for num_iter, (real_batch, _) in enumerate(dataloader):\n\n    x_real = Variable(real_batch).to(device)\n    optimizerD.zero_grad()\n    pred_real = discriminator(x_real)\n    pred_real.to(device)\n    loss_real = criterion(pred_real.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_real.backward()\n    z = noise(x_real.size(0)).to(device)\n    x_fake = generator(z).to(device)\n    x_fake.detach()\n    pred_fake = discriminator(x_fake)\n    pred_fake.to(device)\n    loss_fake = criterion(pred_fake.view(-1,1), (torch.zeros(x_real.size(0),1)).to(device))\n    loss_fake.backward()\n    optimizerD.step()\n    lossD = lossD + loss_real + loss_fake\n    #prob_real = prob_real + np.mean(pred_real.detach().cpu().numpy())\n    #prob_fake = prob_fake + np.mean(pred_fake.detach().cpu().numpy())\n    #binary_pred_real = np.zeros((x_real.size(0),1))\n    #binary_pred_fake = np.zeros((x_real.size(0),1))\n    #binary_pred_real[pred_real.detach().cpu().numpy()>0.5]=1\n    #binary_pred_fake[pred_fake.detach().cpu().numpy()>0.5]=1\n    #accuracy_real = accuracy_real + accuracy_score(np.ones((x_real.size(0),1)),binary_pred_real)\n    #accuracy_fake = accuracy_fake + accuracy_score(np.zeros((x_real.size(0),1)),binary_pred_fake)\n\n    #zg = random_noise(x_real.size(0)).to(device)\n    fake_x = generator(z).to(device)\n    optimizerG.zero_grad()\n    fake_pred = discriminator(fake_x)\n    loss_gen = criterion(fake_pred.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_gen.backward()\n    optimizerG.step()\n    lossG = lossG + loss_gen\n\n  lossesD.append(lossD/len(dataloader))\n  lossesG.append(lossG/len(dataloader))\n  print(\"Epoch No. = \"+ str(epoch+1))\n  print(\"Discriminator Loss = \"+ str(lossesD[epoch].item()), \"Generator Loss = \"+ str(lossesG[epoch].item()))\n  #print(\"Discriminator Confidence on Real Data = \"+ str(prob_real/len(train_dl)), \"Discriminator Confidence on Fake Data = \"+str(prob_fake/len(train_dl)))\n\n  with torch.no_grad():\n    generated_images = generator(fixed_noise.detach())\n    for i in range(16):\n      plt.subplot(4, 4, 1 + i)\n      plt.axis('off')\n      plt.imshow(np.transpose(generated_images.cpu().numpy()[i],(1,2,0)))\n    plt.show()  \n\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(lossesG,label=\"G\")\nplt.plot(lossesD,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_noise = noise(100)\nwith torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(100):\n\t  plt.subplot(10, 10, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_noise = noise(4)\nwith torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(4):\n\t  plt.subplot(2, 2, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ntorch.save(generator.state_dict(),'g_epoch-{}.pth'.format(15))\ntorch.save(discriminator.state_dict(), 'd_epoch-{}.pth'.format(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lossesD = []\nlossesG = []\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n  discriminator.train()\n  generator.train()\n  lossD = 0\n  lossG = 0\n  prob_real = 0\n  prob_fake = 0 \n  for num_iter, (real_batch, _) in enumerate(dataloader):\n\n    x_real = Variable(real_batch).to(device)\n    optimizerD.zero_grad()\n    pred_real = discriminator(x_real)\n    pred_real.to(device)\n    loss_real = criterion(pred_real.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_real.backward()\n    z = noise(x_real.size(0)).to(device)\n    x_fake = generator(z).to(device)\n    x_fake.detach()\n    pred_fake = discriminator(x_fake)\n    pred_fake.to(device)\n    loss_fake = criterion(pred_fake.view(-1,1), (torch.zeros(x_real.size(0),1)).to(device))\n    loss_fake.backward()\n    optimizerD.step()\n    lossD = lossD + loss_real + loss_fake\n    #prob_real = prob_real + np.mean(pred_real.detach().cpu().numpy())\n    #prob_fake = prob_fake + np.mean(pred_fake.detach().cpu().numpy())\n    #binary_pred_real = np.zeros((x_real.size(0),1))\n    #binary_pred_fake = np.zeros((x_real.size(0),1))\n    #binary_pred_real[pred_real.detach().cpu().numpy()>0.5]=1\n    #binary_pred_fake[pred_fake.detach().cpu().numpy()>0.5]=1\n    #accuracy_real = accuracy_real + accuracy_score(np.ones((x_real.size(0),1)),binary_pred_real)\n    #accuracy_fake = accuracy_fake + accuracy_score(np.zeros((x_real.size(0),1)),binary_pred_fake)\n\n    #zg = random_noise(x_real.size(0)).to(device)\n    fake_x = generator(z).to(device)\n    optimizerG.zero_grad()\n    fake_pred = discriminator(fake_x)\n    loss_gen = criterion(fake_pred.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_gen.backward()\n    optimizerG.step()\n    lossG = lossG + loss_gen\n\n  lossesD.append(lossD/len(dataloader))\n  lossesG.append(lossG/len(dataloader))\n  print(\"Epoch No. = \"+ str(epoch+1+15))\n  print(\"Discriminator Loss = \"+ str(lossesD[epoch].item()), \"Generator Loss = \"+ str(lossesG[epoch].item()))\n  #print(\"Discriminator Confidence on Real Data = \"+ str(prob_real/len(train_dl)), \"Discriminator Confidence on Fake Data = \"+str(prob_fake/len(train_dl)))\n  torch.save(generator.state_dict(),'g_epoch-{}.pth'.format(epoch+1+15))\n  torch.save(discriminator.state_dict(), 'd_epoch-{}.pth'.format(epoch+1+15))\n\n  with torch.no_grad():\n    generated_images = generator(fixed_noise.detach())\n    for i in range(16):\n      plt.subplot(4, 4, 1 + i)\n      plt.axis('off')\n      plt.imshow(np.transpose(generated_images.cpu().numpy()[i],(1,2,0)))\n    plt.show()  \n\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(lossesG,label=\"G\")\nplt.plot(lossesD,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(4):\n\t  plt.subplot(2, 2, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lossesD = []\nlossesG = []\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n  discriminator.train()\n  generator.train()\n  lossD = 0\n  lossG = 0\n  prob_real = 0\n  prob_fake = 0 \n  for num_iter, (real_batch, _) in enumerate(dataloader):\n\n    x_real = Variable(real_batch).to(device)\n    optimizerD.zero_grad()\n    pred_real = discriminator(x_real)\n    pred_real.to(device)\n    loss_real = criterion(pred_real.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_real.backward()\n    z = noise(x_real.size(0)).to(device)\n    x_fake = generator(z).to(device)\n    x_fake.detach()\n    pred_fake = discriminator(x_fake)\n    pred_fake.to(device)\n    loss_fake = criterion(pred_fake.view(-1,1), (torch.zeros(x_real.size(0),1)).to(device))\n    loss_fake.backward()\n    optimizerD.step()\n    lossD = lossD + loss_real + loss_fake\n    #prob_real = prob_real + np.mean(pred_real.detach().cpu().numpy())\n    #prob_fake = prob_fake + np.mean(pred_fake.detach().cpu().numpy())\n    #binary_pred_real = np.zeros((x_real.size(0),1))\n    #binary_pred_fake = np.zeros((x_real.size(0),1))\n    #binary_pred_real[pred_real.detach().cpu().numpy()>0.5]=1\n    #binary_pred_fake[pred_fake.detach().cpu().numpy()>0.5]=1\n    #accuracy_real = accuracy_real + accuracy_score(np.ones((x_real.size(0),1)),binary_pred_real)\n    #accuracy_fake = accuracy_fake + accuracy_score(np.zeros((x_real.size(0),1)),binary_pred_fake)\n\n    #zg = random_noise(x_real.size(0)).to(device)\n    fake_x = generator(z).to(device)\n    optimizerG.zero_grad()\n    fake_pred = discriminator(fake_x)\n    loss_gen = criterion(fake_pred.view(-1,1), (torch.ones(x_real.size(0),1)).to(device))\n    loss_gen.backward()\n    optimizerG.step()\n    lossG = lossG + loss_gen\n\n  lossesD.append(lossD/len(dataloader))\n  lossesG.append(lossG/len(dataloader))\n  print(\"Epoch No. = \"+ str(epoch+1+20))\n  print(\"Discriminator Loss = \"+ str(lossesD[epoch].item()), \"Generator Loss = \"+ str(lossesG[epoch].item()))\n  #print(\"Discriminator Confidence on Real Data = \"+ str(prob_real/len(train_dl)), \"Discriminator Confidence on Fake Data = \"+str(prob_fake/len(train_dl)))\n  torch.save(generator.state_dict(),'g_epoch-{}.pth'.format(epoch+1+20))\n  torch.save(discriminator.state_dict(), 'd_epoch-{}.pth'.format(epoch+1+20))\n\n  with torch.no_grad():\n    generated_images = generator(fixed_noise.detach())\n    for i in range(16):\n      plt.subplot(4, 4, 1 + i)\n      plt.axis('off')\n      plt.imshow(np.transpose(generated_images.cpu().numpy()[i],(1,2,0)))\n    plt.show()  \n\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(lossesG,label=\"G\")\nplt.plot(lossesD,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(4):\n\t  plt.subplot(2, 2, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_noise = noise(4)\nwith torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(4):\n\t  plt.subplot(2, 2, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_noise = noise(4)\nwith torch.no_grad():\n  test_images = generator(test_noise.detach())\n  for i in range(4):\n\t  plt.subplot(2, 2, 1 + i)\n\t  plt.axis('off')\n\t  plt.imshow(np.transpose(test_images.cpu().numpy()[i],(1,2,0)))\n  plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}