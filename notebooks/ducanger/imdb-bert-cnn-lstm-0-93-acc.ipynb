{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install bert-for-tf2","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:25:48.849741Z","iopub.execute_input":"2021-08-10T07:25:48.850166Z","iopub.status.idle":"2021-08-10T07:26:02.057406Z","shell.execute_reply.started":"2021-08-10T07:25:48.850083Z","shell.execute_reply":"2021-08-10T07:26:02.056396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:02.059192Z","iopub.execute_input":"2021-08-10T07:26:02.059517Z","iopub.status.idle":"2021-08-10T07:26:02.904752Z","shell.execute_reply.started":"2021-08-10T07:26:02.059478Z","shell.execute_reply":"2021-08-10T07:26:02.903914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport transformers\nimport bert \nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer \nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:02.908348Z","iopub.execute_input":"2021-08-10T07:26:02.908622Z","iopub.status.idle":"2021-08-10T07:26:08.596202Z","shell.execute_reply.started":"2021-08-10T07:26:02.908594Z","shell.execute_reply":"2021-08-10T07:26:08.595304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset\nTrain / Validation / Test = 7 / 1 / 2","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/imdb-dataset/train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('../input/imdb-dataset/val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('../input/imdb-dataset/test.csv', usecols = ['review','sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:08.597861Z","iopub.execute_input":"2021-08-10T07:26:08.598204Z","iopub.status.idle":"2021-08-10T07:26:10.266708Z","shell.execute_reply.started":"2021-08-10T07:26:08.598168Z","shell.execute_reply":"2021-08-10T07:26:10.265788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:10.268245Z","iopub.execute_input":"2021-08-10T07:26:10.268634Z","iopub.status.idle":"2021-08-10T07:26:10.319505Z","shell.execute_reply.started":"2021-08-10T07:26:10.268591Z","shell.execute_reply":"2021-08-10T07:26:10.318526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_val.info())\ndf_val","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:10.320872Z","iopub.execute_input":"2021-08-10T07:26:10.321261Z","iopub.status.idle":"2021-08-10T07:26:10.348267Z","shell.execute_reply.started":"2021-08-10T07:26:10.321212Z","shell.execute_reply":"2021-08-10T07:26:10.347315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:10.349817Z","iopub.execute_input":"2021-08-10T07:26:10.350427Z","iopub.status.idle":"2021-08-10T07:26:10.381411Z","shell.execute_reply.started":"2021-08-10T07:26:10.35038Z","shell.execute_reply":"2021-08-10T07:26:10.380365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('F1_SCORE: ', round(f1_score(y_test, y_pred_proba >= 0.5, average = \"macro\"), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:10.385737Z","iopub.execute_input":"2021-08-10T07:26:10.386178Z","iopub.status.idle":"2021-08-10T07:26:10.393931Z","shell.execute_reply.started":"2021-08-10T07:26:10.386142Z","shell.execute_reply":"2021-08-10T07:26:10.392336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\nRef: https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning","metadata":{}},{"cell_type":"code","source":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndf_train['review'] = df_train['review'].apply(lambda z: remove_punctuations(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_html(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_url(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_emoji(z))\n\ndf_val['review'] = df_val['review'].apply(lambda z: remove_punctuations(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_html(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_url(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_emoji(z))\n\ndf_test['review'] = df_test['review'].apply(lambda z: remove_punctuations(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_html(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_url(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_emoji(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:10.396248Z","iopub.execute_input":"2021-08-10T07:26:10.397065Z","iopub.status.idle":"2021-08-10T07:26:20.195952Z","shell.execute_reply.started":"2021-08-10T07:26:10.397026Z","shell.execute_reply":"2021-08-10T07:26:20.194943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"donå«t\", \"do not\", data)  \n    return data\n    \ndf_train['review'] = df_train['review'].apply(lambda z: remove_abb(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_abb(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_abb(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:20.197659Z","iopub.execute_input":"2021-08-10T07:26:20.198062Z","iopub.status.idle":"2021-08-10T07:26:29.894639Z","shell.execute_reply.started":"2021-08-10T07:26:20.198019Z","shell.execute_reply":"2021-08-10T07:26:29.89381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_train.head(5))\nprint(df_val.shape)\nprint(df_val.head(5))\nprint(df_test.shape)\nprint(df_test.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.896023Z","iopub.execute_input":"2021-08-10T07:26:29.896358Z","iopub.status.idle":"2021-08-10T07:26:29.909742Z","shell.execute_reply.started":"2021-08-10T07:26:29.896323Z","shell.execute_reply":"2021-08-10T07:26:29.908897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding ","metadata":{}},{"cell_type":"code","source":"class IntentDetectionData:\n    DATA_COLUMN,  LABEL_COLUMN  = \"review\",\"sentiment\"\n\n    def __init__(self, train, val, test, tokenizer: FullTokenizer, classes, max_seq_len):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        ((self.train_x, self.train_y), (self.val_x, self.val_y), (self.test_x, self.test_y)) = map(self._prepare, [train, val, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.val_x, self.test_x = map(self._pad, [self.train_x, self.val_x, self.test_x])\n\n    def _prepare(self, df):\n        x, y = [], []\n    \n        for non, row in tqdm(df.iterrows()):\n            text, label =\\\n                row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"] ## Tokens beigning and ending specified by separation of tokens.\n\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens) ## Convert Tokens to IDs\n\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)] ## -2 as ignoring tokens provided by bert\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids)) ## padding by zeros\n            x.append(np.array(input_ids))\n        \n        return np.array(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.911083Z","iopub.execute_input":"2021-08-10T07:26:29.911486Z","iopub.status.idle":"2021-08-10T07:26:29.923672Z","shell.execute_reply.started":"2021-08-10T07:26:29.911449Z","shell.execute_reply":"2021-08-10T07:26:29.922301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"def LSTM_V0(bert_output):\n    #...\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.925253Z","iopub.execute_input":"2021-08-10T07:26:29.925616Z","iopub.status.idle":"2021-08-10T07:26:29.935397Z","shell.execute_reply.started":"2021-08-10T07:26:29.925578Z","shell.execute_reply":"2021-08-10T07:26:29.934658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"def CNN_V0(bert_output):\n    net = Conv1D(128, 7, activation='relu',padding='same')(bert_output)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.936788Z","iopub.execute_input":"2021-08-10T07:26:29.937188Z","iopub.status.idle":"2021-08-10T07:26:29.944494Z","shell.execute_reply.started":"2021-08-10T07:26:29.937153Z","shell.execute_reply":"2021-08-10T07:26:29.943377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiLSTM","metadata":{}},{"cell_type":"code","source":"def BiLSTM_V0(bert_output):\n    net = Bidirectional(LSTM(units=32, return_sequences=True,))(bert_output)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.945943Z","iopub.execute_input":"2021-08-10T07:26:29.946482Z","iopub.status.idle":"2021-08-10T07:26:29.95589Z","shell.execute_reply.started":"2021-08-10T07:26:29.946447Z","shell.execute_reply":"2021-08-10T07:26:29.955048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN + LSTM","metadata":{}},{"cell_type":"code","source":"def CNN_LSTM_V0(bert_output):\n    net = Dropout(0.3)(bert_output)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.957381Z","iopub.execute_input":"2021-08-10T07:26:29.957832Z","iopub.status.idle":"2021-08-10T07:26:29.966297Z","shell.execute_reply.started":"2021-08-10T07:26:29.957794Z","shell.execute_reply":"2021-08-10T07:26:29.965406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CNN_LSTM_V1(bert_output):\n\n    # channel 1\n    net = Conv1D(filters=128, kernel_size=3*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    a = LSTM(128)(net)\n\n    # channel 2\n    net = Conv1D(filters=128, kernel_size=5*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    b = LSTM(128)(net)\n\n    # channel 3\n    net = Conv1D(filters=128, kernel_size=7*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    c = LSTM(128)(net)\n\n    # channel 4\n    net = Conv1D(filters=128, kernel_size=9*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    d = LSTM(128)(net)\n\n    merged = concatenate([a,b,c,d])\n    dense = Dense(100, activation='relu')(merged)\n    drop = Dropout(0.2)(dense)\n    outputs = Dense(1, activation='sigmoid')(merged)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.967447Z","iopub.execute_input":"2021-08-10T07:26:29.967965Z","iopub.status.idle":"2021-08-10T07:26:29.97878Z","shell.execute_reply.started":"2021-08-10T07:26:29.967923Z","shell.execute_reply":"2021-08-10T07:26:29.977934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM + CNN","metadata":{}},{"cell_type":"code","source":"def LSTM_CNN_V0(bert_output):\n    net = Bidirectional(LSTM(128, return_sequences=True))(bert_output)\n    net = Conv1D(128, 7, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.980351Z","iopub.execute_input":"2021-08-10T07:26:29.980771Z","iopub.status.idle":"2021-08-10T07:26:29.99662Z","shell.execute_reply.started":"2021-08-10T07:26:29.980701Z","shell.execute_reply":"2021-08-10T07:26:29.995824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose model","metadata":{}},{"cell_type":"code","source":"def create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file):\n\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read()) ## Reading bert config\n        bert_params = map_stock_config_to_params(bc) ## Mapping parameters \n        bert_params.adapter_size = None # Adapter size helps tune Bert model faster\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'CNN':{0: CNN_V0},\n                    'BiLSTM':{0: BiLSTM_V0},\n                    'CNN+LSTM':{0: CNN_LSTM_V0, 1: CNN_LSTM_V1},\n                    'LSTM+CNN':{0: LSTM_CNN_V0},}\n    \n    ## Specifying input\n    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n        \n    outputs = choose_model[model_name][model_ver](bert_output)\n\n    model = keras.Model(input_ids, outputs)\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_checkpnt_file) ##Loading the weights from bert chckpoint file\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:29.998053Z","iopub.execute_input":"2021-08-10T07:26:29.998506Z","iopub.status.idle":"2021-08-10T07:26:30.008012Z","shell.execute_reply.started":"2021-08-10T07:26:29.99847Z","shell.execute_reply":"2021-08-10T07:26:30.00695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT model","metadata":{}},{"cell_type":"code","source":"# Load BERT model\n# https://github.com/google-research/bert/blob/master/README.md\n\nbert_model_name = \"uncased_L-12_H-768_A-12\"\n# uncased_L-4_H-512_A-8\n# uncased_L-12_H-768_A-12\n\n!wget  https://storage.googleapis.com/bert_models/2020_02_20/{bert_model_name}.zip\n!unzip {bert_model_name}.zip","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:30.009405Z","iopub.execute_input":"2021-08-10T07:26:30.00991Z","iopub.status.idle":"2021-08-10T07:26:36.969127Z","shell.execute_reply.started":"2021-08-10T07:26:30.009874Z","shell.execute_reply":"2021-08-10T07:26:36.968107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_path = \"./\"\nbert_checkpnt_file = os.path.join(bert_model_path, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_model_path, \"bert_config.json\")\nbert_vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\nprint(bert_checkpnt_file)\nprint(bert_config_file)\nprint(bert_vocab_file)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:36.970837Z","iopub.execute_input":"2021-08-10T07:26:36.971183Z","iopub.status.idle":"2021-08-10T07:26:36.978593Z","shell.execute_reply.started":"2021-08-10T07:26:36.971143Z","shell.execute_reply":"2021-08-10T07:26:36.977614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pretrain","metadata":{}},{"cell_type":"code","source":"# Tokenize\ntokenizer = FullTokenizer(vocab_file=bert_vocab_file)\ntokens = tokenizer.tokenize(\"People say nothing is impossible, but I do nothing everyday\")\nprint(tokens)\ntokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:36.980145Z","iopub.execute_input":"2021-08-10T07:26:36.980799Z","iopub.status.idle":"2021-08-10T07:26:37.101231Z","shell.execute_reply.started":"2021-08-10T07:26:36.980748Z","shell.execute_reply":"2021-08-10T07:26:37.100256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = [0, 1]\nmax_seq_len = 384\ndata = IntentDetectionData(df_train, df_val, df_test, tokenizer, classes, max_seq_len)\nprint(data.max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:26:37.105152Z","iopub.execute_input":"2021-08-10T07:26:37.105496Z","iopub.status.idle":"2021-08-10T07:31:20.236254Z","shell.execute_reply.started":"2021-08-10T07:26:37.105467Z","shell.execute_reply":"2021-08-10T07:31:20.235183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creat model","metadata":{}},{"cell_type":"code","source":"model_name = \"LSTM+CNN\"\nmodel_ver = 0\nLR = 2e-5\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=LR)\nmetrics = tf.metrics.BinaryAccuracy()\n\nmodel = create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:31:20.237973Z","iopub.execute_input":"2021-08-10T07:31:20.238316Z","iopub.status.idle":"2021-08-10T07:31:32.015735Z","shell.execute_reply.started":"2021-08-10T07:31:20.238278Z","shell.execute_reply":"2021-08-10T07:31:32.014886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot architecture model\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=96) #to_file='model.jpeg'","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:31:32.018566Z","iopub.execute_input":"2021-08-10T07:31:32.018833Z","iopub.status.idle":"2021-08-10T07:31:32.558912Z","shell.execute_reply.started":"2021-08-10T07:31:32.018805Z","shell.execute_reply":"2021-08-10T07:31:32.554411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# Save model\nmodel_ckpt_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_binary_accuracy', mode='max',verbose=1, save_best_only=True, save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with {bert_model_name}_{model_name}_V{model_ver}_{max_seq_len}\\n\")\ntrain_history = model.fit(data.train_x, data.train_y, validation_data=(data.val_x,data.val_y), epochs=3, batch_size=16, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T07:31:32.560525Z","iopub.execute_input":"2021-08-10T07:31:32.560934Z","iopub.status.idle":"2021-08-10T09:44:14.853488Z","shell.execute_reply.started":"2021-08-10T07:31:32.560892Z","shell.execute_reply":"2021-08-10T09:44:14.852254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:44:14.858678Z","iopub.execute_input":"2021-08-10T09:44:14.858973Z","iopub.status.idle":"2021-08-10T09:44:15.247014Z","shell.execute_reply.started":"2021-08-10T09:44:14.858942Z","shell.execute_reply":"2021-08-10T09:44:15.24605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save architecture model\nconfig = model.to_json()\nmodel_config_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.json\"\nwith open(model_config_path, \"w\") as outfile:\n    json.dump(config, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:44:15.248372Z","iopub.execute_input":"2021-08-10T09:44:15.24871Z","iopub.status.idle":"2021-08-10T09:44:15.259665Z","shell.execute_reply.started":"2021-08-10T09:44:15.248672Z","shell.execute_reply":"2021-08-10T09:44:15.258749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"model.load_weights(model_ckpt_path)\ny_pred_proba = model.predict(data.test_x)\nget_metrics(data.test_y, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:44:15.261015Z","iopub.execute_input":"2021-08-10T09:44:15.261482Z","iopub.status.idle":"2021-08-10T09:47:08.403074Z","shell.execute_reply.started":"2021-08-10T09:44:15.261436Z","shell.execute_reply":"2021-08-10T09:47:08.402123Z"},"trusted":true},"execution_count":null,"outputs":[]}]}