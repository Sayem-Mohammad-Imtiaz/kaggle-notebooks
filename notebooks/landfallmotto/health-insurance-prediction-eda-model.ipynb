{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Health Insurance Cross Sell Prediction**\n\nVehicle insurance (also known as car insurance, motor insurance, or auto insurance) is insurance for cars, trucks, motorcycles, and other road vehicles. Its primary use is to provide financial protection against physical damage or bodily injury resulting from traffic collisions and against liability that could also arise from incidents in a vehicle. Vehicle insurance may additionally offer financial protection against theft of the vehicle, and against damage to the vehicle sustained from events other than traffic collisions, such as keying, weather or natural disasters, and damage sustained by colliding with stationary objects. The specific terms of vehicle insurance vary with legal regulations in each region.<p>\n \nReference: [https://en.wikipedia.org/wiki/Vehicle_insurance](https://en.wikipedia.org/wiki/Vehicle_insurance)\n    \n    \nOur goal is to build a model from Health insurance customer data to predict whether they interest in purchasing vehicle insurance policy.\n[https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction](https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction) <p>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, auc, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, learning_curve, cross_validate, train_test_split, KFold, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\ndata.head(2)\ntest_df=pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns='id',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Helper Functions**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def count_plot(df,feat,palette='rainbow'):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n\n    labels=df[feat].value_counts().index\n    values=df[feat].value_counts().values\n    \n    plt.figure(figsize=(15,5))\n\n    ax = plt.subplot2grid((1,2),(0,0))\n    sns.barplot(x=labels, y=values,palette=palette, alpha=0.75)\n    for i, p in enumerate(ax.patches):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha=\"center\")\n    plt.title('Response of Customer', fontsize=15, weight='bold')    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Target Variable**\n\n**Respone** is our target variable where 1 means customers interested in vehichle insurance or 0 when not interested. \nSo this is a classification task. Also when we look at the target distribution it's clear that we have imbalance between labels.\nWe can try to up or down sample data for increasing accuracy. <p>\n\n\nOversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set (i.e. the ratio between the different classes/categories represented). These terms are used both in statistical sampling, survey design methodology and in machine learning.<p>\nReference: [https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"count_plot(data,'Response')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Missing Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = data.isnull().sum()\nmissing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Gender**\n\nGender distribution in data looks balanced."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"count_plot(data,'Gender','Purples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Age Groups**\n\nLet's see if Age has any effects on response target variable."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nfig=px.histogram(data, x=\"Age\", color=\"Response\", marginal=\"violin\",title =\"Distribution of Age vs Response\", \n                   labels={\"Age\": \"Age\"},\n                   template=\"plotly_dark\",\n                   color_discrete_map={\"0\": \"Not Buy\", \"1\": \"Buy\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some Age ranges have more interest in Vehicle insurance. So, it will be better to group ages regarding to above distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [20, 30, 40, 50, 60, 70, 80,90]\nlabels = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79','80+']\ndata['AgeClass']=pd.cut(data.Age, bins, labels = labels,include_lowest = True)\n\ntest_df['AgeClass']=pd.cut(test_df.Age, bins, labels = labels,include_lowest = True)\n\ndata[['Age','AgeClass']].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Age vs Vehicle Damage**\n\nOlder people having more damaged cars."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.factorplot(\"Vehicle_Damage\", \"Age\", \"Gender\", data=data, kind=\"box\")\n    g.set_axis_labels(\"Vehicle_Damage\", \"Age\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Features Cat vs Num**\n\nLet's decide which features are categorical and numeric. This will be later used for encoding purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cats=['Gender','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Policy_Sales_Channel','Vintage','AgeClass']\ndata_nums=['Age','Annual_Premium']\ndata_all=data_cats+data_nums","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Outlier Detection**\n\nIn statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses. Reference: [https://en.wikipedia.org/wiki/Outlier](https://en.wikipedia.org/wiki/Outlier)\n\n<p>\n    \nThe interquartile range (IQR) is often used to find outliers in data. Outliers here are defined as observations that fall below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. In a boxplot, the highest and lowest occurring value within this limit are indicated by whiskers of the box (frequently with an additional bar at the end of the whisker) and any outliers as individual points. Reference: [https://en.wikipedia.org/wiki/Interquartile_range#Outliers](https://en.wikipedia.org/wiki/Interquartile_range#Outliers) <p>\n\n![https://upload.wikimedia.org/wikipedia/commons/1/1a/Boxplot_vs_PDF.svg](https://upload.wikimedia.org/wikipedia/commons/1/1a/Boxplot_vs_PDF.svg)\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def detect_outliers(df,feat):\n    Q1 = data[feat].quantile(0.25)\n    Q3 = data[feat].quantile(0.75)\n    IQR = Q3 - Q1\n    #data[~ ((data['Annual_Premium'] < (Q1 - 1.5 * IQR)) |(data['Annual_Premium'] > (Q3 + 1.5 * IQR))) ]\n    return df[((df[feat] < (Q1 - 1.5 * IQR)) |(data[feat] > (Q3 + 1.5 * IQR))) ].shape[0]\n\ndef clean_outliers(df,feat):\n    Q1 = data[feat].quantile(0.25)\n    Q3 = data[feat].quantile(0.75)\n    IQR = Q3 - Q1\n    return df[~ ((df[feat] < (Q1 - 1.5 * IQR)) |(data[feat] > (Q3 + 1.5 * IQR))) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in data_nums:\n    res=detect_outliers(data,feat)\n    if (res>0):\n        print('%d Outlier detected in feature %s' % (res,feat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data=clean_outliers(data,'Annual_Premium')\nclean_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train Test Split**\n\nWe split data into 33% test and rest for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(clean_data[data_cats+data_nums], clean_data.Response, test_size=0.33, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Encode Categorical Values**\n\n**OrdinalEncoder/LabelEncoder:** When order is important for categorical variables, it's important to use sklearn OrdinalEncoder or LabelEncoder. eg. cold, warm, hot <p>\n**One Hot Encoding:** When order is NOT important we can use sklearn OneHotEncoder or pandas get_dummies function. eg. Gender is an example Female,Male<p>\n    \n\nThere is two rows in test data which has different Policy Sales Channel not exists in train data. It's 141 and 142. Just 2 of them so we replace them with 140.\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prepare_inputs(train):\n    oe = OrdinalEncoder()\n    oe.fit(train)\n    return oe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oe=prepare_inputs(data[data_cats])\n\nX_train_enc=oe.transform(X_train[data_cats])\nX_test_enc=oe.transform(X_test[data_cats])\n\n# there is 2 unknown new Policy_Sales_Channel values in test 141 and 142\n# we replace them with 140\n\ntest_df.loc[test_df['Policy_Sales_Channel']==141.0, 'Policy_Sales_Channel']=140.0\ntest_df.loc[test_df['Policy_Sales_Channel']==142.0, 'Policy_Sales_Channel']=140.0\n\ntest_df_enc=oe.transform(test_df[data_cats])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_enc=np.concatenate((X_train_enc, X_train[data_nums].values), axis=1)\nall_test_enc=np.concatenate((X_test_enc, X_test[data_nums].values), axis=1)\n\nall_test_df_enc=np.concatenate((test_df_enc, test_df[data_nums].values), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Select Features**"},{"metadata":{},"cell_type":"markdown","source":"SelectKBest score functions:\n\nFor Regression: f_regression, mutual_info_regression<br>\nFor Classification: chi2, f_classif, mutual_info_classif <p>\n    \nChi2 in general for categorical variables. We use mutual_info_classif which is suitable for mixed variables not just categorical or numerical ones 👍 <br>\nHere we see adding age groups as new feature doest not bring any improvements. Age and Ageclass have same feature importance 😒\nDepending of the kscores we can drop some non useful features form dataset for example vintage here is the lowers k-score we may drop it if we want.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n\n# chi2 for categorical variables\n# mutual_info_classif for mixed variables\n   \nfs = SelectKBest(score_func=mutual_info_classif, k='all')\nfs.fit(all_train_enc, y_train)\nX_train_fs = fs.transform(all_train_enc)\n\n\n\nfor i in range(len(fs.scores_)):\n    print('%s: %f' % (data_all[i], fs.scores_[i]))\n\nplt.figure(figsize=(18,8))\nsns.barplot(data_all, fs.scores_, orient='v')\nplt.title('Categorical Feature Selection with mutual_info_classif')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **OverSampling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler \nfrom imblearn.over_sampling import ADASYN\n\n#ros = RandomOverSampler(random_state=42, sampling_strategy='minority')\n#all_train_enc_over_sampled, y_train_over_sampled = ros.fit_resample(all_train_enc, y_train)\n\nada = ADASYN(random_state=42)\nall_train_enc_over_sampled, y_train_over_sampled = ada.fit_resample(all_train_enc, y_train)\n\ny_train=y_train_over_sampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualize**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.decomposition import PCA\nn_components = 2\n\npca = PCA(n_components=n_components)\ncomponents = pca.fit_transform(all_train_enc_over_sampled)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\n\nfig = px.scatter(components, x=0, y=1, color=y_train, title=f'Total Explained Variance: {total_var:.2f}%',)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Scale Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(all_train_enc)\nX_train_transformed = scaler.transform(all_train_enc_over_sampled)\nX_test_transformed = scaler.transform(all_test_enc)\nall_test_df_transformed = scaler.transform(all_test_df_enc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter \n\n#calculate class weight for XGBoost\ncounter = Counter(y_train)\nweight_estimate = counter[0] / counter[1]\nprint('Estimate: %.3f' % weight_estimate)\n# this is mainly for scale_pos_weight in xgboost since it's not support class_weight='balanced' like option\n# weights is manual in xgboost\n# eg. xgtest=XGBClassifier(random_state=55,  scale_pos_weight=weight_estimate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(random_state=55, n_jobs=-1)\nlr=LogisticRegression(random_state=55, n_jobs=-1)\nsv = SVC(probability=True,random_state=55,)\nlogreg = LogisticRegression(solver='newton-cg',random_state=55, n_jobs=-1) \ngb = GradientBoostingClassifier(random_state=55)\ngnb = GaussianNB()\nxgb = XGBClassifier(random_state=55, nthread=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[rf, lr, logreg, gb, gnb, xgb]\ncv = StratifiedKFold(5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Run Models**\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_results = pd.DataFrame()\nrow_number = 0\nresults = []\nnames = []\n\nfor ml in models:\n    model_name=ml.__class__.__name__\n    print('Training %s model ' % model_name)\n    cv_results = cross_validate(ml, X_train_transformed, y_train, cv=cv, scoring='roc_auc', return_train_score=True, n_jobs=-1 )\n    model_results.loc[row_number,'Model Name']=model_name\n    model_results.loc[row_number, 'Train roc_auc  Mean']=cv_results['train_score'].mean()\n    model_results.loc[row_number, 'Test roc_auc  Mean']=cv_results['test_score'].mean()\n    model_results.loc[row_number, 'Fit Time Mean']=cv_results['fit_time'].mean()\n    results.append(cv_results)\n    names.append(model_name)\n    \n    row_number+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results_array = []\nfor tt in results:\n    cv_results_array.append(tt['test_score'])\n\nfig = plt.figure(figsize=(18, 6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(cv_results_array)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(model_results.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RandomForest Classifier** looks overfit so XGBoost looks better"},{"metadata":{},"cell_type":"markdown","source":"Let's also see xgb eval metrics here"},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_set = [(X_train_transformed, y_train), (X_test_transformed,y_test)]\nxgtest=XGBClassifier(random_state=55, nthread=-1)\nxgtest.fit(X_train_transformed, y_train, eval_metric=[\"auc\", \"logloss\", \"error\"], eval_set=eval_set, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores=xgtest.predict(X_test_transformed)\nroc_auc_score(y_test, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\nresults = xgtest.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\npyplot.ylabel('Log Loss')\npyplot.title('XGBoost Log Loss')\npyplot.show()\n# plot classification error\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\n# plot auc\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Test')\nax.legend()\npyplot.ylabel('AUC')\npyplot.title('XGBoost AUC Score')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ROC Curve**\n\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The method was developed for operators of military radar receivers, which is why it is so named.The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). <p>\n    \n<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/ROC_space-2.png\" width=\"500px\">\n\nReference: [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gb_proba=xgtest.predict_proba(X_test_transformed)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds  = roc_curve(y_test, gb_proba)\n\n\nplt.title('XGBoost ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', format(round(auc(fpr,tpr),5)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Insights of Best Model**\n\nAnalyze internals of best model. **XGBoost** in this case."},{"metadata":{},"cell_type":"markdown","source":"## **Classification Report**\n\nThe classification report visualizer displays the precision, recall, F1, and support scores for the model. In order to support easier interpretation and problem detection, the report integrates numerical scores with a color-coded heatmap. All heatmaps are in the range (0.0, 1.0) to facilitate easy comparison of classification models across different classification reports.<p>\n    \n[https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html](https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\n\n\ndef view_report(model,X,y):\n    visualizer = ClassificationReport(\n        model, classes=['0', '1'],\n        cmap=\"YlGn\", size=(600, 360)\n    )\n    visualizer.fit(X,y)\n    visualizer.score(X,y)\n    visualizer.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model = xgtest\nview_report(model,X_train_transformed, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Class Prediction Error**\n\nThe Yellowbrick ClassPredictionError plot is a twist on other and sometimes more familiar classification model diagnostic tools like the Confusion Matrix and Classification Report. Like the Classification Report, this plot shows the support (number of training samples) for each class in the fitted classification model as a stacked bar chart. Each bar is segmented to show the proportion of predictions (including false negatives and false positives, like a Confusion Matrix) for each class. <p>\n    \n    \n[https://www.scikit-yb.org/en/latest/api/classifier/class_prediction_error.html](https://www.scikit-yb.org/en/latest/api/classifier/class_prediction_error.html)\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassPredictionError\n\ndef show_errors(model, X_train,y_train,X_test,y_test):\n    classes=['Not Responded','Responded']\n    visualizer = ClassPredictionError(model)\n\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model = xgtest\nshow_errors(model, X_train_transformed, y_train,X_test_transformed,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Discrimination Threshold**\n\nA visualization of precision, recall, f1 score, and queue rate with respect to the discrimination threshold of a binary classifier. The discrimination threshold is the probability or score at which the positive class is chosen over the negative class. Generally, this is set to 50% but the threshold can be adjusted to increase or decrease the sensitivity to false positives or to other application factors.<p>\n    \n[https://www.scikit-yb.org/en/latest/api/classifier/threshold.html](https://www.scikit-yb.org/en/latest/api/classifier/threshold.html)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.classifier import DiscriminationThreshold\n\nmodel = xgtest\n\nvisualizer = DiscriminationThreshold(model, n_trials=1,excludestr=['queue_rate'],random_state=55)\nvisualizer.fit(X_train_transformed, y_train)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tt=pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/sample_submission.csv')\nid=tt.id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model=XGBClassifier(random_state=55)\nbest_model.fit(X_train_transformed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=best_model.predict_proba(all_test_df_transformed)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(data = {'id': id, 'Response': preds})\nsubmission.to_csv('vehicle_insurance.csv', index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}