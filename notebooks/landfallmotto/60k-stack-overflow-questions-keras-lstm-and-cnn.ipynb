{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Import Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing import text\nfrom sklearn.metrics import classification_report\n\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.style import set_palette\nfrom yellowbrick.text import FreqDistVisualizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.test.gpu_device_name()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/60k-stack-overflow-questions-with-quality-rate/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Target Variable**\n\nWe have 3 target variables and classes looks balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Y.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **HTML To Text**\n\nLet's convert data from html to text using BeautifulSoup module"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['Text']=data.Body.apply(lambda x: BeautifulSoup(x, 'html.parser').text)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Lower Case The Text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Text']=data['Text'].str.lower()\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Document Word Counts Distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_FEATURES = 20000\nMAX_LEN = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X=data['Text'].values\n\nplt.style.use('seaborn')\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\n\nword_index = tokenizer.word_index\n\nresult = [len(x.split()) for x in X]\n\n\nplt.figure(figsize=(20,5))\nplt.title('Document size')\nplt.hist(result, 200, density=False, range=(0,np.max(result)))\nplt.show()\n\n\nprint('max length: %i / min length: %i / mean length: %i / limit length: %i' % (np.max(result), np.min(result), np.mean(result), MAX_LEN))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Token Frequency Distribution**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(X)\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='v',color='rb')\nvisualizer.fit(docs)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,validation=train_test_split(data,test_size=0.25, random_state=55)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Settings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 25\nBATCH_SIZE = 24\n#MAX_LEN = 192","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Encode Target Variable to Binary Categorical**"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(data.Y.values)\nencoded_Y_train = encoder.transform(train.Y.values)\nencoded_Y_valid = encoder.transform(validation.Y.values)\n\n\nx_train = train.Text.values\nx_valid = validation.Text.values\n\n\ny_train = np_utils.to_categorical(encoded_Y_train)\ny_valid = np_utils.to_categorical(encoded_Y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tokenize Words and Pad Max Sequences**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens=text.Tokenizer(num_words=MAX_FEATURES, lower=True)\ntokens.fit_on_texts(list(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=tokens.texts_to_sequences(x_train)\nx_valid=tokens.texts_to_sequences(x_valid)\n\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_valid = tf.keras.preprocessing.sequence.pad_sequences(x_valid, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{},"cell_type":"markdown","source":"# **LSTM MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\nx = layers.Embedding(MAX_FEATURES, 128)(inputs)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\n#x = layers.Flatten()(x)\n#x = layers.Dropout(0.5)(x)\noutputs = layers.Dense(3, activation=\"softmax\")(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CNN MODEL**\n\nThis is for demonstration only how to implement Text Classification in CNN."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_cnn = tf.keras.Input(shape=(None,), dtype=\"int32\")\nx_cnn = layers.Embedding(MAX_FEATURES, 128)(inputs_cnn)\nx_cnn = layers.Bidirectional(layers.GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x_cnn)\nx_cnn = layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x_cnn)\navg_pool = layers.GlobalAveragePooling1D()(x_cnn)\nmax_pool = layers.GlobalMaxPooling1D()(x_cnn)\nx_cnn = layers.concatenate([avg_pool, max_pool])\n\noutputs_cnn = layers.Dense(3, activation=\"softmax\")(x_cnn)\nmodel_cnn = tf.keras.Model(inputs_cnn, outputs_cnn)\nmodel_cnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD=tf.keras.optimizers.SGD(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy',optimizer=SGD,metrics=[tf.keras.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train Model**"},{"metadata":{},"cell_type":"markdown","source":"# **Callbacks**"},{"metadata":{"trusted":true},"cell_type":"code","source":"es_cb = EarlyStopping(monitor='val_loss', min_delta=0,  patience=10, verbose=0, mode='auto')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_valid, y_valid),callbacks = [es_cb,reduce_lr], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['auc'])\nplt.plot(history.history['val_auc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Testing Model**\n\nA random question from https://stackoverflow.com/questions/tagged/java"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_question = ['I have sql server management studio 14.0.17825.0 and would like to use group_concat function. But I get error when i try to use. The error is invalid column name group_concat Is there any other function that I could use? Could you provide a sample code which could achieve what function group_concat?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq = tokenizer.texts_to_sequences(test_question)\npadded = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_LEN)\npred = model.predict(padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=list(encoder.classes_)\nprint(np.argmax(pred), labels[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Prediction Test**\n\nThis part is to show how to use prediction and classificaion report. In normal cases, it's not right to use validation data for this purpose. We need another dataset that model never seen before.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(x_valid)\ny_pred=np.argmax(y_pred,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=np.argmax(y_valid,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = list(encoder.classes_)\n\nprint(classification_report(y_true, y_pred, target_names=target_names))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}