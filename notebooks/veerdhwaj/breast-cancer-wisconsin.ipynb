{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Read the data\nimport numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #to ignore the warnings\n\n#for model building\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\ndataset=pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\ndataset.head(2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the unwanted Unnamed:32 column\ndataset=dataset.drop(columns={'Unnamed: 32'})\n\ndataset['diagnosis']=dataset['diagnosis'].replace({'M':1,'B':0})\n# Finding the correraltion between the features\ncorr = dataset.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(dataset.corr(), cmap='YlGnBu', annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will now find out the features that have a higher correlation to diagnosis\ncorr[abs(corr['diagnosis']) > 0.59].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Local Outlier Factor\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n\n# split the data to X and y before Local Outlier Factorization\n\ny=dataset[\"diagnosis\"]\nX=dataset.drop([\"diagnosis\"],axis=1)\ncolumns= dataset.columns.tolist()\nlof= LocalOutlierFactor()\ny_pred=lof.fit_predict(X)\ny_pred[0:30]\nx_score= lof.negative_outlier_factor_\noutlier_score= pd.DataFrame()\noutlier_score[\"score\"]=x_score\n\nlofthreshold= -2.5\nloffilter= outlier_score[\"score\"]< lofthreshold\noutlier_index= outlier_score[loffilter].index.tolist()\nX= X.drop(outlier_index)\ny= y.drop(outlier_index).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Dont fit the scaler while standardizate X_test !\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\nkey = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue = [LogisticRegression(), KNeighborsClassifier(n_neighbors = 2, weights ='uniform'), SVC(kernel=\"rbf\",random_state=15), DecisionTreeClassifier(random_state=10), RandomForestClassifier(n_estimators=60, random_state=0), GradientBoostingClassifier(random_state=20), AdaBoostClassifier(), xgb.XGBClassifier(random_state=0,booster=\"dart\")]\nmodels = dict(zip(key,value))\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    predicted.append(acc)\n    print(name,acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we noticed ADaBoost Classifier have the best accuracy, we will be going ahead with that","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}