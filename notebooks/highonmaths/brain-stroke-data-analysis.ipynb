{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n!pip install statsmodels\nimport statsmodels.api as sm \nfrom scipy import stats","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:48.207026Z","iopub.execute_input":"2021-05-23T19:56:48.207625Z","iopub.status.idle":"2021-05-23T19:56:54.912426Z","shell.execute_reply.started":"2021-05-23T19:56:48.207572Z","shell.execute_reply":"2021-05-23T19:56:54.911297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:54.915661Z","iopub.execute_input":"2021-05-23T19:56:54.916222Z","iopub.status.idle":"2021-05-23T19:56:54.923823Z","shell.execute_reply.started":"2021-05-23T19:56:54.916168Z","shell.execute_reply":"2021-05-23T19:56:54.922323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:54.92585Z","iopub.execute_input":"2021-05-23T19:56:54.926317Z","iopub.status.idle":"2021-05-23T19:56:54.956646Z","shell.execute_reply.started":"2021-05-23T19:56:54.926269Z","shell.execute_reply":"2021-05-23T19:56:54.954962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:54.958394Z","iopub.execute_input":"2021-05-23T19:56:54.95898Z","iopub.status.idle":"2021-05-23T19:56:55.006122Z","shell.execute_reply.started":"2021-05-23T19:56:54.958926Z","shell.execute_reply":"2021-05-23T19:56:55.004918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.010369Z","iopub.execute_input":"2021-05-23T19:56:55.010763Z","iopub.status.idle":"2021-05-23T19:56:55.04206Z","shell.execute_reply.started":"2021-05-23T19:56:55.010731Z","shell.execute_reply":"2021-05-23T19:56:55.040668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns = ['id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.04519Z","iopub.execute_input":"2021-05-23T19:56:55.045617Z","iopub.status.idle":"2021-05-23T19:56:55.055941Z","shell.execute_reply.started":"2021-05-23T19:56:55.045575Z","shell.execute_reply":"2021-05-23T19:56:55.054637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bmi column has missing values, so we will try and deal with those missing values first of all of this column, for that we will need some additional informations, like how many missing value values are there, so:-\ndata.isnull().sum()\n# so now we can see, every other column except bmi zero missing values","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.057589Z","iopub.execute_input":"2021-05-23T19:56:55.057904Z","iopub.status.idle":"2021-05-23T19:56:55.077133Z","shell.execute_reply.started":"2021-05-23T19:56:55.057873Z","shell.execute_reply":"2021-05-23T19:56:55.075769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.078715Z","iopub.execute_input":"2021-05-23T19:56:55.079018Z","iopub.status.idle":"2021-05-23T19:56:55.100445Z","shell.execute_reply.started":"2021-05-23T19:56:55.078988Z","shell.execute_reply":"2021-05-23T19:56:55.099025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['bmi'].fillna(value=data['bmi'].mean(), inplace=True)\ndata['bmi'].isnull().sum()\n# here with above code what we have done is that we have replaced the missing values in the column 'bmi' with the mean values, so we need to calculate the mean by summing the values in the 'bmi' column and then dividing it by total number of rows.\n#  and then we replaced all the missing values with mean, there are many stratigies for the replacement which solely depends over the type of data you have.\n# and now we can see that we have zero missing values in our column","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.102012Z","iopub.execute_input":"2021-05-23T19:56:55.102435Z","iopub.status.idle":"2021-05-23T19:56:55.110943Z","shell.execute_reply.started":"2021-05-23T19:56:55.102389Z","shell.execute_reply":"2021-05-23T19:56:55.110028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.112682Z","iopub.execute_input":"2021-05-23T19:56:55.113144Z","iopub.status.idle":"2021-05-23T19:56:55.140143Z","shell.execute_reply.started":"2021-05-23T19:56:55.113109Z","shell.execute_reply":"2021-05-23T19:56:55.138863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data['stroke']\nx = data.drop(columns='stroke', axis=1)\n# here we have divided our dataset into dependent and independent variables, in 'x' we have all the independent variables and in 'y' we have all the depedent variables.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.141493Z","iopub.execute_input":"2021-05-23T19:56:55.141859Z","iopub.status.idle":"2021-05-23T19:56:55.153669Z","shell.execute_reply.started":"2021-05-23T19:56:55.141827Z","shell.execute_reply":"2021-05-23T19:56:55.152622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.info() # so now our independent data columns have no missing values and now we can deal further","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.155807Z","iopub.execute_input":"2021-05-23T19:56:55.156293Z","iopub.status.idle":"2021-05-23T19:56:55.181224Z","shell.execute_reply.started":"2021-05-23T19:56:55.156247Z","shell.execute_reply":"2021-05-23T19:56:55.180001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['gender'].value_counts() # so as we can see, only one column is there with 'other' value in our categorical column of gender, so we will replace that with male, or we can also delete the whole row , there can be different strategies. we will only have to delete one row if we decide to get rid of 'other' value.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.183382Z","iopub.execute_input":"2021-05-23T19:56:55.183893Z","iopub.status.idle":"2021-05-23T19:56:55.196137Z","shell.execute_reply.started":"2021-05-23T19:56:55.183844Z","shell.execute_reply":"2021-05-23T19:56:55.194726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['gender'] = x['gender'].replace('Other', 'Male') # we replaced with the 'male' value here\nx['gender'].value_counts() # and now checking the number of unique values","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.198227Z","iopub.execute_input":"2021-05-23T19:56:55.198599Z","iopub.status.idle":"2021-05-23T19:56:55.212168Z","shell.execute_reply.started":"2021-05-23T19:56:55.198564Z","shell.execute_reply":"2021-05-23T19:56:55.211328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.213435Z","iopub.execute_input":"2021-05-23T19:56:55.213715Z","iopub.status.idle":"2021-05-23T19:56:55.23888Z","shell.execute_reply.started":"2021-05-23T19:56:55.213688Z","shell.execute_reply":"2021-05-23T19:56:55.23801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the 'ever_married' column\nx['ever_married'].value_counts() # so here we don't have any value which is in very less amount.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.239945Z","iopub.execute_input":"2021-05-23T19:56:55.240368Z","iopub.status.idle":"2021-05-23T19:56:55.255832Z","shell.execute_reply.started":"2021-05-23T19:56:55.240337Z","shell.execute_reply":"2021-05-23T19:56:55.254572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['Residence_type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.25745Z","iopub.execute_input":"2021-05-23T19:56:55.257769Z","iopub.status.idle":"2021-05-23T19:56:55.273534Z","shell.execute_reply.started":"2021-05-23T19:56:55.257739Z","shell.execute_reply":"2021-05-23T19:56:55.272555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['smoking_status'].value_counts() # so here also we do not need to worry about the different data in very less amount.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.275042Z","iopub.execute_input":"2021-05-23T19:56:55.275335Z","iopub.status.idle":"2021-05-23T19:56:55.286383Z","shell.execute_reply.started":"2021-05-23T19:56:55.275307Z","shell.execute_reply":"2021-05-23T19:56:55.285157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['work_type'].value_counts() # here we can replace 'Never_worked' values with 'Govt_job'","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.288164Z","iopub.execute_input":"2021-05-23T19:56:55.288878Z","iopub.status.idle":"2021-05-23T19:56:55.301123Z","shell.execute_reply.started":"2021-05-23T19:56:55.288826Z","shell.execute_reply":"2021-05-23T19:56:55.299712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will encode our categorical features.\n# as we donot have any ordinal categorical features, that's we can simply get the dummy variables rather than label encoding\ncategorical_features = ['gender','ever_married','work_type','Residence_type','smoking_status']","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.304834Z","iopub.execute_input":"2021-05-23T19:56:55.305158Z","iopub.status.idle":"2021-05-23T19:56:55.313475Z","shell.execute_reply.started":"2021-05-23T19:56:55.305129Z","shell.execute_reply":"2021-05-23T19:56:55.3125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy = pd.get_dummies(x[categorical_features])  # herer we converted these categorical features into dummy variables","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.315715Z","iopub.execute_input":"2021-05-23T19:56:55.317234Z","iopub.status.idle":"2021-05-23T19:56:55.341879Z","shell.execute_reply.started":"2021-05-23T19:56:55.317193Z","shell.execute_reply":"2021-05-23T19:56:55.340686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.concat([x, dummy], axis=1)\nx.head() # here we concatenated the two data frames.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.343284Z","iopub.execute_input":"2021-05-23T19:56:55.343623Z","iopub.status.idle":"2021-05-23T19:56:55.379208Z","shell.execute_reply.started":"2021-05-23T19:56:55.343591Z","shell.execute_reply":"2021-05-23T19:56:55.377982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will drop all the categorical features which are there in dataframes which will not be usedful in the prediction now as we already have out set of dummy encoded set of categorical features.\nx.drop(columns=categorical_features, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.380746Z","iopub.execute_input":"2021-05-23T19:56:55.381061Z","iopub.status.idle":"2021-05-23T19:56:55.387941Z","shell.execute_reply.started":"2021-05-23T19:56:55.381031Z","shell.execute_reply":"2021-05-23T19:56:55.386887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.391064Z","iopub.execute_input":"2021-05-23T19:56:55.391632Z","iopub.status.idle":"2021-05-23T19:56:55.421625Z","shell.execute_reply.started":"2021-05-23T19:56:55.391582Z","shell.execute_reply":"2021-05-23T19:56:55.420669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.422973Z","iopub.execute_input":"2021-05-23T19:56:55.423356Z","iopub.status.idle":"2021-05-23T19:56:55.445024Z","shell.execute_reply.started":"2021-05-23T19:56:55.423318Z","shell.execute_reply":"2021-05-23T19:56:55.443573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will see if there are outliers present in the data columns, for that we will make a function\ndef outlier(x , column):\n    q1 = x[column].quantile(0.25)\n    q3 = x[column].quantile(0.75)\n    itr = q3 -q1\n    upper_limit = q3 + 1.5 * itr\n    lower_limit = q1 - 1.5 * itr\n    return upper_limit, lower_limit","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.446398Z","iopub.execute_input":"2021-05-23T19:56:55.446727Z","iopub.status.idle":"2021-05-23T19:56:55.452943Z","shell.execute_reply.started":"2021-05-23T19:56:55.446698Z","shell.execute_reply":"2021-05-23T19:56:55.45202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will check for outliers in the numerical columns one by one.\nul , ll = outlier(x, 'age')\nul2, ll2 = outlier(x, 'avg_glucose_level')\nul3, ll3 = outlier(x, 'bmi')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.453961Z","iopub.execute_input":"2021-05-23T19:56:55.454239Z","iopub.status.idle":"2021-05-23T19:56:55.477057Z","shell.execute_reply.started":"2021-05-23T19:56:55.454211Z","shell.execute_reply":"2021-05-23T19:56:55.475735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will plot the graph to see the outliers present in the data visually using box plot.\nnumercal_columns = ['age', 'avg_glucose_level', 'bmi']\nplt.boxplot(x['bmi']) # here we can see , we have lot of outliers which we need to remove or deal with some techniques","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.478313Z","iopub.execute_input":"2021-05-23T19:56:55.478617Z","iopub.status.idle":"2021-05-23T19:56:55.643461Z","shell.execute_reply.started":"2021-05-23T19:56:55.478588Z","shell.execute_reply":"2021-05-23T19:56:55.642545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.boxplot(x['avg_glucose_level']) # here too we have lot of data outside of the box, which we call outliers","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.644862Z","iopub.execute_input":"2021-05-23T19:56:55.645168Z","iopub.status.idle":"2021-05-23T19:56:55.769933Z","shell.execute_reply.started":"2021-05-23T19:56:55.645138Z","shell.execute_reply":"2021-05-23T19:56:55.768606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.boxplot(x['age']) # here there is no outliers present","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.771655Z","iopub.execute_input":"2021-05-23T19:56:55.771981Z","iopub.status.idle":"2021-05-23T19:56:55.891724Z","shell.execute_reply.started":"2021-05-23T19:56:55.771948Z","shell.execute_reply":"2021-05-23T19:56:55.890514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.893362Z","iopub.execute_input":"2021-05-23T19:56:55.893828Z","iopub.status.idle":"2021-05-23T19:56:55.981293Z","shell.execute_reply.started":"2021-05-23T19:56:55.89379Z","shell.execute_reply":"2021-05-23T19:56:55.980096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# therfore now we will make a function for treating the outliers\ndef outliers_deal(value):\n    if value > ul2:\n        return ul2\n    elif value < ll2:\n        return ll2\n    else:\n        return value","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.986913Z","iopub.execute_input":"2021-05-23T19:56:55.987433Z","iopub.status.idle":"2021-05-23T19:56:55.992565Z","shell.execute_reply.started":"2021-05-23T19:56:55.987367Z","shell.execute_reply":"2021-05-23T19:56:55.99168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['avg_glucose_level'] = x['avg_glucose_level'].apply(outliers_deal)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:55.994207Z","iopub.execute_input":"2021-05-23T19:56:55.994556Z","iopub.status.idle":"2021-05-23T19:56:56.015061Z","shell.execute_reply.started":"2021-05-23T19:56:55.994522Z","shell.execute_reply":"2021-05-23T19:56:56.013917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# therfore now we will make another function for treating the outliers\ndef outliers_deal(value):\n    if value > ul3:\n        return ul3\n    elif value < ll3:\n        return ll3\n    else:\n        return value","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.016181Z","iopub.execute_input":"2021-05-23T19:56:56.016535Z","iopub.status.idle":"2021-05-23T19:56:56.029284Z","shell.execute_reply.started":"2021-05-23T19:56:56.016503Z","shell.execute_reply":"2021-05-23T19:56:56.028217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['bmi'] = x['bmi'].apply(outliers_deal)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.030841Z","iopub.execute_input":"2021-05-23T19:56:56.031147Z","iopub.status.idle":"2021-05-23T19:56:56.050327Z","shell.execute_reply.started":"2021-05-23T19:56:56.031118Z","shell.execute_reply":"2021-05-23T19:56:56.049285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.describe() # now we can see the max and min values have been limited after we dealt with the outliers","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.051909Z","iopub.execute_input":"2021-05-23T19:56:56.052438Z","iopub.status.idle":"2021-05-23T19:56:56.138811Z","shell.execute_reply.started":"2021-05-23T19:56:56.052276Z","shell.execute_reply":"2021-05-23T19:56:56.137461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.140406Z","iopub.execute_input":"2021-05-23T19:56:56.140897Z","iopub.status.idle":"2021-05-23T19:56:56.161586Z","shell.execute_reply.started":"2021-05-23T19:56:56.140846Z","shell.execute_reply":"2021-05-23T19:56:56.160346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if we apply tree based algorithms, multicolinearity will not be a problem in that case, but if we apply distance based algorithms, multicolinearity will effect our predictions. thereby we will try to remove multicolinearity and in the process the feature variables will also be reduced which effect the model predcition in a positive way\ncorr = x.corr()\ncorr","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.163054Z","iopub.execute_input":"2021-05-23T19:56:56.163456Z","iopub.status.idle":"2021-05-23T19:56:56.217616Z","shell.execute_reply.started":"2021-05-23T19:56:56.163423Z","shell.execute_reply":"2021-05-23T19:56:56.21511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the heat map of correlation matrix using seaborn library\nplt.figure(dpi=150)\nsns.heatmap(x.corr(), xticklabels= corr.columns, yticklabels= corr.columns)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.219164Z","iopub.execute_input":"2021-05-23T19:56:56.219529Z","iopub.status.idle":"2021-05-23T19:56:56.917562Z","shell.execute_reply.started":"2021-05-23T19:56:56.219493Z","shell.execute_reply":"2021-05-23T19:56:56.916463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(dpi=100)\n# sns.pairplot(x,height=2,palette='OrRd')\n# plt.show() # one can observe the correlation between different variable","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.919068Z","iopub.execute_input":"2021-05-23T19:56:56.919696Z","iopub.status.idle":"2021-05-23T19:56:56.923972Z","shell.execute_reply.started":"2021-05-23T19:56:56.919649Z","shell.execute_reply":"2021-05-23T19:56:56.922708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x.rename(columns={'smoking_status_never smoked':'smoking_status_never_smoked','smoking_status_formerly smoked':'smoking_status_formerly_smoked'})","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.925285Z","iopub.execute_input":"2021-05-23T19:56:56.925587Z","iopub.status.idle":"2021-05-23T19:56:56.939871Z","shell.execute_reply.started":"2021-05-23T19:56:56.925558Z","shell.execute_reply":"2021-05-23T19:56:56.938553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use VIF to calculate and remove the features which are highly correlated\ncolumns = ['age','hypertension','heart_disease','avg_glucose_level','bmi','gender_Female','gender_Male',\t'ever_married_No','ever_married_Yes','work_type_Govt_job','work_type_Never_worked','work_type_Private',\t'work_type_Self-employed',\t'work_type_children',\t'Residence_type_Rural'\t,'Residence_type_Urban'\t'smoking_status_Unknown'\t,'smoking_status_formerly smoked'\t,'smoking_status_never smoked',\t'smoking_status_smokes' ]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.941727Z","iopub.execute_input":"2021-05-23T19:56:56.942209Z","iopub.status.idle":"2021-05-23T19:56:56.957389Z","shell.execute_reply.started":"2021-05-23T19:56:56.942158Z","shell.execute_reply":"2021-05-23T19:56:56.956205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if we are applying any kind of tree models, then we don't need to worry about the multicolinearity. feature scaling is also not required for tree based models\n# so now, we will split our dataset into train and test\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:56.958824Z","iopub.execute_input":"2021-05-23T19:56:56.959194Z","iopub.status.idle":"2021-05-23T19:56:57.126331Z","shell.execute_reply.started":"2021-05-23T19:56:56.95916Z","shell.execute_reply":"2021-05-23T19:56:57.125308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:57.128202Z","iopub.execute_input":"2021-05-23T19:56:57.128923Z","iopub.status.idle":"2021-05-23T19:56:58.01542Z","shell.execute_reply.started":"2021-05-23T19:56:57.128874Z","shell.execute_reply":"2021-05-23T19:56:58.014319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nmetrics.accuracy_score(y_train, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:58.016706Z","iopub.execute_input":"2021-05-23T19:56:58.017001Z","iopub.status.idle":"2021-05-23T19:56:58.027761Z","shell.execute_reply.started":"2021-05-23T19:56:58.016972Z","shell.execute_reply":"2021-05-23T19:56:58.026613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(x_test)\nmetrics.accuracy_score(y_test, y_pred)\n# if the absolute difference between the accuracy score of both prediction i.e, over training set and over test set would have  been very high, in that case, we would have needed the hyperparameter tuning because of overfitting, but here in this case, the difference is not that big, so we can easily proceed ahead.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:58.029033Z","iopub.execute_input":"2021-05-23T19:56:58.029531Z","iopub.status.idle":"2021-05-23T19:56:58.071513Z","shell.execute_reply.started":"2021-05-23T19:56:58.029492Z","shell.execute_reply":"2021-05-23T19:56:58.07033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now, we will visualize the theh feature importance\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(x_train,y_train)\npredict = clf.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:58.072808Z","iopub.execute_input":"2021-05-23T19:56:58.073104Z","iopub.status.idle":"2021-05-23T19:56:58.527968Z","shell.execute_reply.started":"2021-05-23T19:56:58.073076Z","shell.execute_reply":"2021-05-23T19:56:58.526876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nfeature_importace = pd.Series(clf.feature_importances_ , index=x.columns).sort_values(ascending=False)\nfeature_importace","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:58.529327Z","iopub.execute_input":"2021-05-23T19:56:58.529636Z","iopub.status.idle":"2021-05-23T19:56:58.551439Z","shell.execute_reply.started":"2021-05-23T19:56:58.529607Z","shell.execute_reply":"2021-05-23T19:56:58.550139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing feature importance\nplt.figure(dpi=100)\nsns.barplot(x = feature_importace, y = feature_importace.index )\nplt.xlabel('feature importance score')\nplt.ylabel('features')\nplt.title('visualizing important features')\nplt.show()\n# now what we can do is that we can retrain our model on selected features. the more the value the more the feature is important.","metadata":{"execution":{"iopub.status.busy":"2021-05-23T19:56:58.553251Z","iopub.execute_input":"2021-05-23T19:56:58.553732Z","iopub.status.idle":"2021-05-23T19:56:58.923331Z","shell.execute_reply.started":"2021-05-23T19:56:58.553684Z","shell.execute_reply":"2021-05-23T19:56:58.922259Z"},"trusted":true},"execution_count":null,"outputs":[]}]}