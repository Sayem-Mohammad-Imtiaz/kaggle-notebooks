{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/first-gop-debate-twitter-sentiment/Sentiment.csv')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_drop = ['candidate_gold','relevant_yn_gold','sentiment_gold',\n                   'subject_matter_gold','tweet_coord','tweet_location','user_timezone',\n                   'id','tweet_created','tweet_id','name']\ndf.drop(labels=columns_to_drop,axis=1,inplace=True)\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing Text Data**","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(doc):\n    words_filtered = [e.lower() for e in doc.split()]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'rt']\n    doc_without_stopwords = ' '.join([word for word in words_cleaned if not word in stopwords_set])\n    \n    return doc_without_stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function is used to remove the punctuation in the text data\ndef remove_punctuations(doc):\n    punctuations = \"\"\"!()-[]{};:'\"\\,“”<>./?@#$%^&*_~\"\"\"\n    #we add one more punctuation to our list as this punctuation mark was used multiple times in the text data\n    punctuations += '�' \n    for p in punctuations:\n      if p in doc:\n        doc = doc.replace(p,\"\")\n    return doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_punctuations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function will remove all the tokens which are not alphabatic\ndef remove_digits(doc):\n    tokens = doc.split()\n    result = ' '.join([i for i in tokens if i.isalpha()])\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_digits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing libraries for stemming\nimport re\nimport nltk\nfrom nltk.stem import SnowballStemmer #general stemmer\nprint(\" \".join(SnowballStemmer.languages))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will select the dutch language stemmer as out text is in dutch language\nstemmer = SnowballStemmer(\"english\")\n# stemmer.stem(df['text'].iloc[0])\ndf['text'] = df['text'].apply(stemmer.stem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting tweets with positive and negative sentiment\ndf_final = df[df['sentiment'] != 'Neutral']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final['sentiment'] = df_final['sentiment'].apply(lambda x : 1 if x == 'Positive' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating pradictor and target variable \nX = df_final['text']\ny = df_final['sentiment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preparing data for word2vec**\nhttps://www.kaggle.com/pierremegret/gensim-word2vec-tutorial","metadata":{}},{"cell_type":"code","source":"# tokenizing the proprocessed text data\nsent = [row.split() for row in df_final['text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models.phrases import Phrases, Phraser","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phrases = Phrases(sent, min_count=30, progress_per=10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigram = Phraser(phrases)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = bigram[sent]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     vector_size=300, \n                     alpha=0.03, \n                     min_alpha=0.0007)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.build_vocab(sentences, progress_per=10000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation\n\nhttps://www.kaggle.com/guichristmann/lstm-classification-model-with-word2vec\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM","metadata":{}},{"cell_type":"code","source":"w2v_model.wv.vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# w2v_model.wv.get_vecattr(word, \"count\")  #gives the count of words/occurence of the word.\ndef word2token(sentence):\n#     print(sentence)\n    words = sentence.split()\n#     print(words)\n    vec = []\n    for word in words:\n        try:\n            vec.append(w2v_model.wv.key_to_index[word])\n        # If word is not in index return 0. I realize this means that this\n        # is the same as the word of index 0 (i.e. most frequent word), but 0s\n        # will be padded later anyway by the embedding layer (which also\n        # seems dirty but I couldn't find a better solution right now)\n        except KeyError:\n            vec.append(0)\n    return vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = df_final['text'].apply(word2token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nX = pad_sequences(temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size,embedding_size = w2v_model.wv.vectors.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_weights = w2v_model.wv.vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_weights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\nmodel.add(LSTM(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting the dataset into test and train set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output size = 32\nbatch_size = 64\nepochs = 100\nmodel.fit(X_train, y_train, epochs = epochs, batch_size=batch_size)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print(y_pred[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,yp in enumerate(y_pred):\n    if yp >= 0.5:\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncf_mat = confusion_matrix(y_test, y_pred,labels=[0,1])\ncf_mat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}