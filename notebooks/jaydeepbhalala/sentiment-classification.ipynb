{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/first-gop-debate-twitter-sentiment/Sentiment.csv')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# id,tweet_created,tweet_id, can be deleted ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing data**","metadata":{}},{"cell_type":"code","source":"df.info() #candidate_gold,relevant_yn_gold,sentiment_gold,subject_matter_gold,tweet_coord,tweet_location,user_timezone ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_drop = ['candidate_gold','relevant_yn_gold','sentiment_gold',\n                   'subject_matter_gold','tweet_coord','tweet_location','user_timezone',\n                   'id','tweet_created','tweet_id','name']\ndf.drop(labels=columns_to_drop,axis=1,inplace=True)\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['candidate'].unique() # can be encoded using label encoder ,if needed.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['candidate_confidence'].describe() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['relevant_yn'].nunique() # can be encoded using label encoder ,if needed.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['relevant_yn_confidence'].describe() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment_confidence'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['subject_matter'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['retweet_count'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(doc):\n    words_filtered = [e.lower() for e in doc.split()]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'rt']\n    doc_without_stopwords = ' '.join([word for word in words_cleaned if not word in stopwords_set])\n    \n    return doc_without_stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function is used to remove the punctuation in the text data\ndef remove_punctuations(doc):\n    punctuations = \"\"\"!()-[]{};:'\"\\,“”<>./?@#$%^&*_~\"\"\"\n    #we add one more punctuation to our list as this punctuation mark was used multiple times in the text data\n    punctuations += '�' \n    for p in punctuations:\n      if p in doc:\n        doc = doc.replace(p,\"\")\n    return doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_punctuations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function will remove all the tokens which are not alphabatic\ndef remove_digits(doc):\n    tokens = doc.split()\n    result = ' '.join([i for i in tokens if i.isalpha()])\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(remove_digits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing libraries for stemming\nimport re\nimport nltk\nfrom nltk.stem import SnowballStemmer #general stemmer\nprint(\" \".join(SnowballStemmer.languages))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will select the dutch language stemmer as out text is in dutch language\nstemmer = SnowballStemmer(\"english\")\n# stemmer.stem(df['text'].iloc[0])\ndf['text'] = df['text'].apply(stemmer.stem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WordClouds of Positive and Negative Sentiments**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\nfrom matplotlib import pyplot as plt\n\ndf_pos = df[df['sentiment'] == 'Positive']\ndf_pos = df_pos['text']\ndf_neg = df[df['sentiment'] == 'Negative']\ndf_neg = df_neg['text']\n\ndef wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(words)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words\")\nwordcloud_draw(df_pos,'white')\nprint(\"Negative words\")\nwordcloud_draw(df_neg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final = df[df['sentiment'] != 'Neutral']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final['sentiment'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final['sentiment'] = df_final['sentiment'].apply(lambda x : 1 if x == 'Positive' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating pradictor and target variable\nX = df_final['text']\ny = df_final['sentiment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting the dataset into test and train set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ml algorithm work with numbers only so we will convert our text feature in numeric form\n#we will bag of words approach here\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X) #creating corpus using the whole data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming train data in numeric form with help of whole corpus\nX_train = vectorizer.transform(X_train)\n# print(vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming test data in numeric form with help of whole corpus\nX_test = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Trying naive byes**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nGNB_classifier = GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#fitting the train dataset\nGNB_classifier.fit(X_train.toarray(),y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#finding accuracy of the model\nGNB_classifier.score(X_test.toarray(), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Trying support vector classifier**","metadata":{}},{"cell_type":"code","source":"#importing support vector machine algorithm from sklearn library\nfrom sklearn.svm import SVC\nsvm_clf_model = SVC()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#fitting the train dataset\nsvm_clf_model.fit(X_train.toarray(),y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#finding accuracy of the model\nsvm_clf_model.score(X_test.toarray(), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Trying another support vector classifier**","metadata":{}},{"cell_type":"code","source":"#importing support vector machine algorithm from sklearn library\nfrom sklearn.svm import SVC\nsvm_clf_model1 = SVC(C=100,gamma=1,kernel='sigmoid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#fitting the train dataset\nsvm_clf_model1.fit(X_train.toarray(),y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#finding accuracy of the model\nsvm_clf_model1.score(X_test.toarray(), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Trying passive aggressive classifer**\n\nhttps://www.geeksforgeeks.org/passive-aggressive-classifiers/","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\npac=PassiveAggressiveClassifier(max_iter=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npac.fit(X_train.toarray(),y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pac.score(X_test.toarray(), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **By Selecting various number of features only**","metadata":{}},{"cell_type":"markdown","source":"**Expermenting**\n1. By changing number of features\n2. By doing n-grams and varying number of feature\n3. By topic modelling or so\n--Try doing some topic modelling like latent Dirichlet allocation or Probabilistic latent Semantic Analysis for the corpus using a specified number of topics - say 20. You would get a vector of 20 probabilities corresponding to the 20 topics for each document. You could use that vector as input for your classification or use it as additional features on top of what you already have from your base model enhanced with bigrams and trigrams.\n\nSource : https://datascience.stackexchange.com/questions/19276/improving-accuracy-of-text-classification","metadata":{}},{"cell_type":"code","source":"# spliting the dataset into test and train set\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=6000) #max_features=2000,\nvectorizer.fit(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming train data in numeric form with help of whole corpus\nX_train1 = vectorizer.transform(X_train1)\n# print(vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming train data in numeric form with help of whole corpus\nX_test1 = vectorizer.transform(X_test1)\n# print(vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_clf_model = SVC()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#fitting the train dataset\nsvm_clf_model.fit(X_train1.toarray(),y_train1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#finding accuracy of the model\nsvm_clf_model.score(X_test1.toarray(), y_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1 = svm_clf_model.predict(X_test1.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncf_mat = confusion_matrix(y_test1, y_pred1,labels=[0,1])\ncf_mat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'True 0 are {(y_test1==0).sum()} True predicted 0 are {cf_mat[0][0]}')\nprint(f'True 1 are {(y_test1==1).sum()} True predicted 1 are {cf_mat[1][1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mode is too bias towards negative class(class 0) as dataset is imbalance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((y==0).sum())\nprint((y[y==0].count()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Evaluation is Pending**\n--> Topics to be covered\n\n1. Classification accuracy\n2. Confusion matrix\n3. Precision and recall\n4. F1 score\n5. Sensitivity and specificity\n6. ROC curve and AUC","metadata":{}},{"cell_type":"markdown","source":"**MORE TO TRY**\nhttps://datascience.stackexchange.com/questions/19276/improving-accuracy-of-text-classification","metadata":{}},{"cell_type":"markdown","source":"**Source for Learn Word Embeddings**\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz\n\nhttps://developers.google.com/machine-learning/crash-course/embeddings/video-lecture\n\nhttps://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n\nhttps://kavita-ganesan.com/easily-access-pre-trained-word-embeddings-with-gensim/","metadata":{}},{"cell_type":"code","source":"# Another way to think of an embedding is as \"lookup table\". \n# pretrained word embedding provided by --> Gensim,Spacy.\n\n# The disadvantage of pre-trained word embeddings is that the words contained within may not capture \n# the peculiarities of language in your specific application domain\n\n# The vectors can be accessed directly using the .vector attribute of each processed token (word).\n# The mean vector for the entire sentence is also calculated simply using .vector, \n# providing a very convenient input for machine learning models based on sentences.\n\n# phrase detection in gensim using\n# from gensim.models.phrases import Phraser, Phrases\n\n# gensim appears to be a popular NLP package, and has some nice documentation and tutorials, including for word2vec.\n# Source : https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Sentiment Analysis using Deep Learning**","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c\n\nhttps://medium.com/deep-learning-with-keras/lstm-understanding-the-number-of-parameters-c4e087575756#:~:text=LSTM%20layer%20has%20%E2%80%9Cdimensionality%20of,vector%20with%20dimension%203%20(feature)\n\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.utils.np_utils import to_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_fatures = 3000\ntokenizer = Tokenizer(num_words=max_fatures)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(df_final['text'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df_final['text'].values)\ntype(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0],X[1],X[2],X[3],X[4],X[5],X[6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pad_sequences(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0],X[1],X[2],X[3],X[4],X[5],X[6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,y, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 128\nlstm_out = 32\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output size = 32\nbatch_size = 64\nepochs = 10\nmodel.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output size = 64\nbatch_size = 64\nepochs = 10\nmodel.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# output size = 128\nbatch_size = 64\nepochs = 10\nmodel.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size)","metadata":{}},{"cell_type":"markdown","source":"Source for below\nhttps://www.kaggle.com/jaydeepbhalala/gensim-word2vec-tutorial/edit","metadata":{}},{"cell_type":"markdown","source":"https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation\n\nhttps://www.kaggle.com/guichristmann/lstm-classification-model-with-word2vec","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}