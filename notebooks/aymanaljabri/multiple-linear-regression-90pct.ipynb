{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Ask the Right Question:\n\nThe purpose of this notebook is to investigate the linear relationship (if any) between [Height, Gender] as independent variables and Weigths (dependent variable). And build a model that resembles this relationship, validate it, interpert results. And then make some predictions.\n\n# 2. Data Collection:\nThe second step in data analysis is to collect the data. In our case we're using a simple Bivariate dataset contains the Weights in Lbs and Heights in inches of 10,000 observations that is split between Male and Female adults","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndf = pd.read_csv('../input/weight-height/weight-height.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:55:54.252802Z","iopub.execute_input":"2021-09-23T14:55:54.253516Z","iopub.status.idle":"2021-09-23T14:55:54.28273Z","shell.execute_reply.started":"2021-09-23T14:55:54.253471Z","shell.execute_reply":"2021-09-23T14:55:54.281776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Cleaning:\n**First order** of business is to take a quick look at our data set to make sure it is:   \n### 3.1 No missing values\n","metadata":{}},{"cell_type":"code","source":"# 1. No missing values\nprint(df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:43:08.122369Z","iopub.execute_input":"2021-09-23T14:43:08.123374Z","iopub.status.idle":"2021-09-23T14:43:08.133108Z","shell.execute_reply.started":"2021-09-23T14:43:08.123321Z","shell.execute_reply":"2021-09-23T14:43:08.13208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Outliers\n\nSometimes outliers are bad data and must be get rid of, and sometimes they're Michael Jordan or Shaquille O'Neal and must be kept!  \n\nEither way, they must be identified first. To do so, we define outliers as observations that are: \n1. More than 3 standard deviations away from the mean.\n2. Or more than 1.5 * Inter-Quartile-Range (IQR) away from either the 25th OR the 75th percentiles (ends of the boxplot).\n\nFor the current dataset; it contains one continuous and one categorical (discrete) variables. We can use boxplot to visualze outliers, using the builtin feature in matplotlib, searborn or pandas\n\n**More on outliers detection**   \nOutlier detection methods by data type include:  \n- Univariate data -> boxplot. outside of 1.5 times inter-quartile range is an outlier.  \n- Bivariate -> scatterplot with confidence ellipse. outside of, say, 95% confidence ellipse is an outlier.  \n- Multivariate -> Mahalanobis D2 distance   \n\nOnce defined, mark those observations as outliers. Then run a logistic regression to see if there are any systematic patterns.   ","metadata":{}},{"cell_type":"code","source":"# 2. Outliers: \nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,4))\nax1.grid()\nsns.boxplot(x=df.Weight, y=df.Gender, ax=ax1)\nax2.grid()\nsns.boxplot(x=df.Height, y=df.Gender, ax=ax2);","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:00:34.273002Z","iopub.execute_input":"2021-09-23T15:00:34.274043Z","iopub.status.idle":"2021-09-23T15:00:34.635445Z","shell.execute_reply.started":"2021-09-23T15:00:34.273997Z","shell.execute_reply":"2021-09-23T15:00:34.634647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't see a reason why a 270lbs male or a 55inch female should be excluded from the dataset. Thuse, I'm going to leave the dataset as is for now and revisit the outliers later, but from a predictive point of view. i.e. when plotting the residuals","metadata":{}},{"cell_type":"markdown","source":"### 3.3 Balacend between Genders\n\nIn general, having balanced data generates higher accuracy models. This is true for all regression types and machine learning algorithms.  ","metadata":{}},{"cell_type":"code","source":"# 3. Distribution across genders is perfect\ndf.groupby('Gender').Weight.nunique()/len(df)*100","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:43:21.934691Z","iopub.execute_input":"2021-09-23T14:43:21.935333Z","iopub.status.idle":"2021-09-23T14:43:21.945676Z","shell.execute_reply.started":"2021-09-23T14:43:21.935297Z","shell.execute_reply":"2021-09-23T14:43:21.9447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our case the data is split equally between Male and Female, which is ideal. But in case you're dealing with an imbalanced dataset, can use WLS (weighted least squared regression) instead of OLS. This is out of this Notebook's scope","metadata":{}},{"cell_type":"markdown","source":"### 3.4 Distribution of Variables (Optional)\nUnlike error terms, the dependent and/or independet variables don't need to be normally distributed. But if their distributions are very far off from normal, it'll be difficut to fit a line that results in a normally distributed erorrs (which is one of the assumption of linear regression 2.1.6). That's why I like to check the distribution upfront just to know what to expect. This is an optional step but extremely helpful   \n\nThe easiest way to do that in practice is to visualize the distribution using histogram:","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(2,1,figsize=(8,6))\nfig.suptitle('Distribution of Variables', fontsize=20)\nsns.histplot(data=df, x='Weight', hue='Gender', ax=ax[0], stat='percent')\nsns.histplot(data=df, x='Height', hue='Gender',ax=ax[1]);","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:43:29.386579Z","iopub.execute_input":"2021-09-23T14:43:29.38745Z","iopub.status.idle":"2021-09-23T14:43:30.395366Z","shell.execute_reply.started":"2021-09-23T14:43:29.387397Z","shell.execute_reply":"2021-09-23T14:43:30.394548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like both variables are normally distributed. This will make our life easier when investigating some of the typical problems of linear regression. ","metadata":{}},{"cell_type":"markdown","source":"## 4. Analyzing the Data\n\nThis step should focus on finding the answer to the question asked in step (1): Is there a linear relationship between the dependent and independent variables. That being said, and before we get to the regression itself, let's list the assumptions of a linear regression model:\n### 4.1 Assumptions of a Linear Regression\n1. There is a linear relationship between dependend and independent variables\n2. The independent variables are not random, and there is no exact linear relationship between them (No multicollinearity)\n3. The expected value of error term is zero $E(\\epsilon | x_i)=0$\n4. The variance of the error term is constanct for all observations (i.e. $E(\\epsilon^2_i)=\\sigma_{\\epsilon}^2$ (heteroscedasticity)\n5. The error term of one observation is not corrolated with that of another (no serial correlation)\n6. The error term is normally distributed\n\nWe are going to test each of these assumptions after regressing the data!","metadata":{}},{"cell_type":"markdown","source":"### 4.2 Pre-Processing\n#### 4.2.1 Split the Data\n\nIt's standard practice to split the data into training and test sets to see how the model behaves when presented with data it hasn't seens. I'm goign to use pandas to do this. \n\nAnother, very common way to do it is by using`train_test_split` from `sklearn.model_selection`, but I prefer pandas. As it retains both sets as DataFrames. This will make the regression outcome much more user friendly!  ","metadata":{}},{"cell_type":"code","source":"# Split the data to train and test using pandas\n\ndf_test = df.sample(frac=0.3)\ndf_train = pd.merge(df, df_test, how='outer', indicator=True).query('_merge ==\"left_only\"').drop(columns=['_merge'])\nprint(f'Size of training set:{len(df_train)}, size of test set: {len(df_test)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:51:31.483655Z","iopub.execute_input":"2021-09-23T14:51:31.48448Z","iopub.status.idle":"2021-09-23T14:51:31.525557Z","shell.execute_reply.started":"2021-09-23T14:51:31.484434Z","shell.execute_reply":"2021-09-23T14:51:31.524626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2.2 Let's make sure splitting didn't affect databalance between Genders:","metadata":{}},{"cell_type":"code","source":"print(\"Gender count across Train Set:\\n\",df_train.groupby('Gender').size())","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:51:31.799973Z","iopub.execute_input":"2021-09-23T14:51:31.800404Z","iopub.status.idle":"2021-09-23T14:51:31.808034Z","shell.execute_reply.started":"2021-09-23T14:51:31.800373Z","shell.execute_reply":"2021-09-23T14:51:31.80719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 OLS regression model","metadata":{}},{"cell_type":"code","source":"# Ordinary Least Square (OLS) regression\n\nols = smf.ols(formula=\"Weight ~ C(Gender) + Height\", data=df_train)\nres = ols.fit(cov_type='nonrobust')\nprint(res.summary())","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:09.108107Z","iopub.execute_input":"2021-09-23T13:00:09.108352Z","iopub.status.idle":"2021-09-23T13:00:09.190046Z","shell.execute_reply.started":"2021-09-23T13:00:09.108317Z","shell.execute_reply":"2021-09-23T13:00:09.18891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Interpretting Regression Results\n### 5.1 R-squared\nR-squared/Adj.R-squared: 90.4% of variability in dependent variable can be explained by the independed variables. This is high enough for training set, but let's see how well the model deals with data it hasn't seen (test_set)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_test_pred = res.predict(exog=df_test)\ntest_rsquare= r2_score(df_test.Weight, y_test_pred)\nprint(f\"Test R-squared:{test_rsquare:.3f} \")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:09.191769Z","iopub.execute_input":"2021-09-23T13:00:09.192245Z","iopub.status.idle":"2021-09-23T13:00:09.368155Z","shell.execute_reply.started":"2021-09-23T13:00:09.192198Z","shell.execute_reply":"2021-09-23T13:00:09.367206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test R-squared at 90.5% is as high as train R-squared, which means the model generalizes well\n\n\n#### 2.4.2 F-statistic|Prob(F)\nIt tests how well the independent variables as a group explain the variations of the dependent variable (i.e. tests the null hypothesis $H_0: coef_1=coef_2=\\dots coef_n=0$)   \nP-value of F-statistic, or the probability of type I error (observation happenning giving the null hypothesis is true)  \nIn this case F-statistic is very large and Prob(F) is zero, which is great!  \n\n#### 2.4.3 AIC:\nThe Akaike Information Criterion (AIC) measures overfit. It rewards the model for goodness-of-fit and penalize it if the model becomes overly complicated  \nIn this case AIC is large, which is good!  \n\n#### 2.4.4 Omnibus/Prob(Omnibus):\nOmnibus tests the skewness and kurtosis of the residuals.  \n**closer to zero the better***  \nProb(Omnibus):tests the probability the residuals are normally distributed.  \n**The closer to one the better***  \n\nIn this case Omnibus is relatively hight and the Prob (Omnibus) is relatively low so the data is far from normal. A linear regression approach would probably be better than random guessing but likely not as good as a nonlinear approach.  \n\n#### 2.4.5 Skew \nMeasures data symmetry. We want to see something close to zero, indicating the residual distribution is normal. Note that this value also drives the Omnibus.  \n**The closer to zero the better***   \n\n#### 2.4.6 Kurtosis\nMeasure of \"peakiness\", or curvature of the data. Higher peaks lead to greater Kurtosis. Greater Kurtosis can be interpreted as a tighter clustering of residuals around zero, implying a better model with few outliers.  \n**The higher the better**  \n\n#### 2.4.7 Durbin-Watson\nTests for homoscedasticity. We hope to have a value between 1.5 and 2.5. In this case, the data is close, but within limits.  \n\n#### 2.4.8 Jarque-Bera (JB)/Prob(JB)\nlike the Omnibus test in that it tests both skew and kurtosis. We hope to see in this test a confirmation of the Omnibus test.   \n\n#### 2.4.9 Condition Number\nMeasures the sensitivity of a function's output as compared to its input. When we have multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something below 30.  \n**The smaller below 30 the better**\n","metadata":{}},{"cell_type":"code","source":"res.resid.hist()\nres.resid.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:09.370955Z","iopub.execute_input":"2021-09-23T13:00:09.37126Z","iopub.status.idle":"2021-09-23T13:00:09.683679Z","shell.execute_reply.started":"2021-09-23T13:00:09.37122Z","shell.execute_reply":"2021-09-23T13:00:09.682681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = res.get_prediction().summary_frame().rename(columns={'mean':'Weight_P'})\npred['Weight'] = df_train.Weight\npred['Height'] = df_train.Height\npred['Gender'] = df_train.Gender","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:09.684905Z","iopub.execute_input":"2021-09-23T13:00:09.685212Z","iopub.status.idle":"2021-09-23T13:00:09.699561Z","shell.execute_reply.started":"2021-09-23T13:00:09.685175Z","shell.execute_reply":"2021-09-23T13:00:09.698293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = pred.Gender =='Male'\ncolor = None\nfig, ax = plt.subplots(1,2,figsize=(17,6), sharey=True)\nfig.suptitle(\"Train Set Observed vs Predicted\", fontsize=16)\nfor i,gender in enumerate(pred.Gender.unique()):\n    if i ==1:\n        mask = ~mask\n        color = 'orange'\n    ax[i].scatter(data=pred[mask], x='Height', y='Weight',s=2,alpha=0.5, label='True', color=color)\n    ax[i].plot('Height','Weight_P',data=pred[mask], linewidth=0.75, label='Predicted')\n    ax[i].plot('Height','obs_ci_upper',data=pred[mask], linestyle=':', linewidth=0.5, label='Upper Bound')\n    ax[i].plot('Height','obs_ci_lower',data=pred[mask], linestyle=':', linewidth=0.5,label='Lower Bound')\n    ax[i].set_title(gender, fontsize=12)\n    ax[i].set_xlabel('Height')\n    ax[i].set_ylabel('Weight')\n    ax[i].grid(True)\n    ax[i].legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:09.701092Z","iopub.execute_input":"2021-09-23T13:00:09.701685Z","iopub.status.idle":"2021-09-23T13:00:10.441284Z","shell.execute_reply.started":"2021-09-23T13:00:09.70164Z","shell.execute_reply":"2021-09-23T13:00:10.440345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res.predict(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:00:10.44268Z","iopub.execute_input":"2021-09-23T13:00:10.442998Z","iopub.status.idle":"2021-09-23T13:00:10.463877Z","shell.execute_reply.started":"2021-09-23T13:00:10.442958Z","shell.execute_reply":"2021-09-23T13:00:10.462869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nbonferroni` : one-step correction\n    - `sidak` : one-step correction\n    - `holm-sidak` :\n    - `holm` :\n    - `simes-hochberg` :\n    - `hommel` :\n    - `fdr_bh` : Benjamini/Hochberg\n    - `fdr_by` : Benjamini/Yekutieli\n\"\"\"\noutliers = res.outlier_test(\"bonferroni\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:31:22.253626Z","iopub.execute_input":"2021-09-23T13:31:22.253908Z","iopub.status.idle":"2021-09-23T13:31:39.616222Z","shell.execute_reply.started":"2021-09-23T13:31:22.253881Z","shell.execute_reply":"2021-09-23T13:31:39.615255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outidx = outliers.iloc[:,-1].nsmallest(5).index\ndf_train.loc[outidx]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:31:39.618125Z","iopub.execute_input":"2021-09-23T13:31:39.619003Z","iopub.status.idle":"2021-09-23T13:31:39.640141Z","shell.execute_reply.started":"2021-09-23T13:31:39.618955Z","shell.execute_reply":"2021-09-23T13:31:39.639286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.figure(figsize=(10,6))\nplt.scatter(df_train.Height, df_train.Weight, c=df_train.Gender.values=='Male', s=0.7)\nplt.scatter(df_train.loc[outidx].Height, df_train.loc[outidx].Weight, c=df_train.loc[outidx].Gender.values=='Male', marker='<');","metadata":{"execution":{"iopub.status.busy":"2021-09-23T13:31:39.64192Z","iopub.execute_input":"2021-09-23T13:31:39.642544Z","iopub.status.idle":"2021-09-23T13:31:40.053649Z","shell.execute_reply.started":"2021-09-23T13:31:39.642497Z","shell.execute_reply":"2021-09-23T13:31:40.052978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}