{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(r\"../input/creditcardfraud/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab a peek at the data \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe information about dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine number of missing values in dataset\ndf.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine number of fraud cases in dataset\ndf['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df[\"Class\"],data=df)\nplt.title(\"Class Distrubution\",fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate total data into non-fraud and fraud cases\nfraud = df[df.Class == 0] #save non-fraud df observations into a separate df\nnormal = df[df.Class == 1] #do the same for frauds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Only 0.17% fraudulent transaction out all the transactions. The data is highly Unbalanced. Lets first apply our models without balancing it and if we don’t get a good accuracy then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Amount details of the fraudulent transaction\") \nfraud.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"details of valid transaction\") \n\nnormal.Amount.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can clearly notice from this, the average Money transaction for the fraudulent ones is more. This makes this problem crucial to deal with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the histogram of each parameter\ndf.hist(figsize = (20, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=\"Amount\",y=\"Time\",data=df,hue=\"Class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation matrix\ncorrmat=df.corr()\nf,ax=plt.subplots(figsize=(50,30))\nsns.heatmap(corrmat,vmax=.8,square=True,cbar=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the HeatMap we can clearly see that most of the features do not correlate to other features but there are some features that either has a positive or a negative correlation with each other. For example, V2 and V5 are highly negatively correlated with the feature called Amount.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing the data into inputs parameters and outputs value format\nX=df.drop(\"Class\",axis=1)\ny=df['Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Skicit-learn to split data into training and testing sets \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix,f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us run Logistic regression and evaluate the performance metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(X_train,y_train)\npred=log.predict(X_test)\nprint(accuracy_score(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us run RandomForestClassifier and evaluate the performance metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(X_train,y_train)\npred1=random.predict(X_test)\nprint(accuracy_score(y_test,pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test,pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import NearMiss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nm=NearMiss()\nx_us,y_us=nm.fit_sample(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(x_us,y_us)\npred5=log.predict(X_test)\nprint(confusion_matrix(y_test,pred5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(x_us,y_us)\npred6=random.predict(X_test)\nprint(confusion_matrix(y_test,pred6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nos=RandomOverSampler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_os,y_os=os.fit_sample(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(x_os,y_os)\npred8=random.predict(X_test)\nprint(confusion_matrix(y_test,pred8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test,pred8)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}