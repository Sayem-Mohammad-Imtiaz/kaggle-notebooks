{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Practical Implementation Of PCA"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import  the libraries\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading The Dataset"},{"metadata":{},"cell_type":"markdown","source":"To import the dataset I have used the pandas library"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/glass.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_names=df.columns.tolist()\nprint(\"Columns names:\")\nprint(columns_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"df.corr() compute pairwise correlation of columns.Correlation shows how the two variables are related to each other.Positive values shows as one variable increases other variable increases as well. Negative values shows as one variable increases other variable decreases.Bigger the values,more strongly two varibles are correlated and viceversa."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualising correlation using Seaborn library"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different fearures')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### DataFrame.iloc\nPurely integer-location based indexing for selection by position.\n\n.iloc[ ] is primarily integer position based (from 0 to length-1 of the axis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:11].values\ny = df.iloc[:,0].values\nX\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Data Standardisation\nStandardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Computing Eigenvectors and Eigenvalues:\n"},{"metadata":{},"cell_type":"markdown","source":"### Covariance Matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(cov_mat, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different features')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Eigen decomposition of the covariance matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorting The Covariance matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n\nExplained Variance is a useful measure which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(10), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above clearly shows that maximum variance (somewhere around 30%) can be explained by the first principal component alone. Comparatively 7th and 8th components share less amount of information as compared to the rest of the Principal components."},{"metadata":{},"cell_type":"markdown","source":"### Projection Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"significance = [np.abs(i)/np.sum(eig_vals) for i in eig_vals]\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(significance))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Explained Variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The construction of the projection matrix that will be used to transform the Glass data onto the new feature subspace. From the above bargraph and curve we can observe that the first 5 principal components carry 90% of the data. Hence we can drop other components. Here, we are reducing the 10-dimensional feature space to a 5-dimensional feature subspace, by choosing the “top 5” eigenvectors with the highest eigenvalues to construct our d×k-dimensional eigenvector matrix W"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix_w = np.hstack((eig_pairs[0][1].reshape(10,1), \n                      eig_pairs[1][1].reshape(10,1),\n                      eig_pairs[2][1].reshape(10,1),\n                      eig_pairs[3][1].reshape(10,1),\n                      eig_pairs[4][1].reshape(10,1)\n                    ))\nprint('Matrix W:\\n', matrix_w)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(matrix_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = X_std.dot(matrix_w)\nY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this projected data can be further used to get accurate results as we have reduced the dimesnions by selecting only the principal components that have max information about data"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}