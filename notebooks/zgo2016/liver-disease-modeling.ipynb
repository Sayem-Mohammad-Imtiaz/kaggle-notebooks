{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import Library\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\nimport warnings \n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load Indian liver patient data \ndata=pd.read_csv('../input/indian_liver_patient.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preliminary Data Analyses\nIn this section, we will explore given data to find missing values and learn how many features/samles we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new DataFrame that includes Male, Female patient information\n\ndisease, no_disease = data['Dataset'].value_counts()\nmale, female = data['Gender'].value_counts()\n\ninfo=['Diognised with Liver Disease', 'Not Diognised with Liver Disease', 'Male', 'Female']\ncount=[disease, no_disease, male, female]\n\ndf_patient=pd.DataFrame({'Patient Info': info, 'Count': count})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_patient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling\nIn this section, we fill out missing values with median value for the given feature. Then, we will process categorical variable 'Gender' and will create two new features 'Male' and 'Female' from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=data['Dataset']\nsex=pd.get_dummies(data['Gender'])\n#data = data.join(sex)\ndata.insert(loc=0, column='Male', value=sex['Male'])\ndata.insert(loc=0, column='Female', value=sex['Female'])\ndata.drop(['Gender'], axis=1, inplace=True)\n#data.drop(['Dataset'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = data.columns\ncols = list(set(cols))\ndel cols[cols.index('Dataset')]\n#data.hist(column=cols, bins=10, figsize=(20,20), xlabelsize = 7, color='green', log=True)\ndel cols[cols.index('Male')]\ndel cols[cols.index('Female')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analyses (EDA)\nIn this section, we learn more abot data by vizualizing it on plots, which will help us in identifying outliers and other important details about the data. Also, we will use this knowledge to generate new features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(cols, data, plot_type):\n\n    fig = plt.figure(figsize = (25,25))\n    \n    sns.set(font_scale=1.5) \n    \n    for idx, val in enumerate(cols):\n            \n        plt.subplot(3, 3, idx+1)\n\n        if plot_type == 'hist':\n            disease = 'sns.distplot(data[data[\"Dataset\"] == 1].' + val + ', color=\"blue\", label=\"Liver disease\")'\n            healthy = 'sns.distplot(data[data[\"Dataset\"] == 2].' + val + ', color=\"orange\", label=\"Healthy liver\")'\n            exec (disease)\n            exec (healthy)\n            plt.legend()\n            plt.xlabel(val)\n            plt.ylabel(\"Frequency\")\n          \n        if plot_type == 'cdf':\n            a='plt.hist(data[data[\"Dataset\"] == 1].' + val + ',bins=50,fc=(0,1,0,0.5),label=\"Bening\",normed = True,cumulative = True)'\n            exec (a)\n            sorted_data = exec('np.sort(data[data[\"Dataset\"] == 1].' + val + ')')\n            #sorted_data = exec (sorted_d)\n            y = np.arange(len(sorted_data))/float(len(sorted_data)-1)\n            plt.plot(sorted_data,y,color='red')\n            plt.title('CDF of liver dicease bilirubin')\n            \n        if plot_type == 'swarm':\n            condition = 'sns.swarmplot(x=' +  \"'\" + 'Dataset' + \"'\" + ',y=' + \"'\" + val + \"'\" + ',data=data)'\n            print (condition)\n            exec (condition)\n              \n        if plot_type == 'box':\n            condition = 'sns.boxplot(x=' +  \"'\" + 'Dataset' + \"'\" + ',y=' + \"'\" + val + \"'\" + ',data=data)'\n            print (condition)\n            exec (condition)\n            \n        if plot_type == 'violin':\n            condition = 'sns.violinplot(x=' +  \"'\" + 'Dataset' + \"'\" + ',y=' + \"'\" + val + \"'\" + ',data=data)'\n            print (condition)\n            exec (condition)\n        \n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(cols, data, 'hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram plots, we observe that the healthy patients have higher frequency of small values in narrow ranges compared to unhealthy patients."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(cols, data, 'swarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Swarm plots show outliers for some of the features for both healthy and unhealthy patients. However, we can not claim that these outliers represent erroneous data points. We will apply feature engineering method to generate new features. Specifically, new hot encoded quantile features will be introduced for alkaline phosphotase, direct bilirubin, alamine aminotransferase, total bilirubin, aspartate aminotransferase, age and albumin and Globulin Ratio features since they revealed some hints on possible outliers in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(cols, data, 'box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above barplots, we can count outliers of some features for healthy patients to be within the whisker extend of unhealthy patients. For example, Aspartate Aminotransfertase max value for healthy patients is within whisker extent of the corresponding unhealthy patients. Thus, we will use this observation to generate new features for the analysis. Specifically, we will be setting sample value to 1 for the new feature, if healthy patient’s value from the original feature is above the whisker extend of the corresponding un-healthy patient’s value. Otherwise, we will set it to 0."},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define X and y for train/test split\n\nX = data.drop(['Dataset'], axis=1)\ny = data.Dataset\n\ncols = data.columns\ncols = list(set(cols))\ndel cols[cols.index('Dataset')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define Classifiers and Parameters for Xgboost, Logistic Regression, KNNeighbors and Random Forest Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGB(X_train, y_train, X_test, y_test):\n   \n    import xgboost as xgb\n\n    xgb_clf = xgb.XGBClassifier()\n\n    params={'max_depth': [2,3,4], 'subsample': [0.6, 1.0],'colsample_bytree': [0.5, 0.6],\n    'n_estimators': [500, 1000], 'reg_alpha': [0.03, 0.05]}\n\n    xgb = GridSearchCV(xgb_clf,param_grid=params, n_jobs=-1, cv=3, scoring='f1')\n    xgb.fit(X_train, y_train)\n\n    y_pred = xgb.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    # evaluate predictions\n    print('Accuracy of xgb classifier on test set: {:.2f}'.format(accuracy_score(y_test, predictions)))\n    print (\"Classification report:\\n{}\".format(classification_report(y_test,predictions)))\n    \n    precision,recall,fscore,support=score(y_test,predictions)\n    \n    return fscore, accuracy_score(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LR(X_train, y_train, X_test, y_test):\n    \n    clf = LogisticRegression()\n    grid_values = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n    \n    lr = GridSearchCV(clf, param_grid=grid_values, cv=3, n_jobs=-1, scoring=\"f1\")\n    lr.fit(X_train, y_train)\n\n    # make predictions on test data\n    y_pred = lr.predict(X_test)\n\n    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(lr.score(X_test, y_test)))\n    print (\"Classification report:\\n{}\".format(classification_report(y_test,y_pred)))\n    \n    precision,recall,fscore,support=score(y_test, y_pred)\n    \n    return fscore, accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def KNN(X_train, y_train, X_test, y_test):\n    \n    reg=KNeighborsClassifier(n_neighbors=8)\n\n    #lr = GridSearchCV(clf, param_grid=grid_values, scoring=\"f1\")\n\n    k_range = list(range(1, 31))\n    param_grid = dict(n_neighbors=k_range)\n\n    grid = GridSearchCV(reg, param_grid, cv=3, n_jobs=-1, scoring='f1')\n    grid.fit(X_train, y_train)\n       \n    print('Accuracy of KNeighbors classifier on test set: {:.2f}'.format(grid.score(X_test, y_test)))\n    print (\"Classification report:\\n{}\".format(classification_report(y_test,grid.predict(X_test))))\n    \n    precision,recall,fscore,support=score(y_test, grid.predict(X_test))\n    return fscore, accuracy_score(y_test, grid.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RF(X_train, y_train, X_test, y_test, flag=0):\n    \n    from sklearn.metrics import precision_recall_fscore_support as score\n    from sklearn.metrics import confusion_matrix\n    from sklearn import metrics\n\n    rfc = RandomForestClassifier(random_state=42, criterion='entropy', min_samples_split=5, oob_score=True)\n    parameters = {'n_estimators':[200, 400, 600, 800, 1000], 'min_samples_leaf':[4, 8, 16], \n                  'max_features': ['auto', 'sqrt']}\n\n    scoring = make_scorer(accuracy_score, greater_is_better=True)\n\n    cl_rand_fr = GridSearchCV(rfc, param_grid=parameters, cv=3, n_jobs=-1, scoring='f1')\n    cl_rand_fr.fit(X_train, y_train)\n    cl_rand_fr = cl_rand_fr.best_estimator_\n\n    # Show prediction accuracy score\n    print ('Accuracy of random forest classifier on test set: {:.2f}'.format(accuracy_score(y_test, cl_rand_fr.predict(X_test))))\n    print (\"Classification report:\\n{}\".format(classification_report(y_test,cl_rand_fr.predict(X_test))))\n    \n    if flag == 1:\n    \n        print(\"Confusion Matrix:\\n{}\".format(confusion_matrix(y_test, cl_rand_fr.predict(X_test))))\n    \n        from yellowbrick.classifier import ROCAUC\n        fig, ax=plt.subplots(1,1,figsize=(12,8))\n\n        auc=ROCAUC(cl_rand_fr, macro=False, micro=False)\n        auc.fit(X_train, y_train)\n        auc.score(X_test, y_test)\n        auc.poof()\n        \n        return 0\n    \n    else:\n        \n        precision,recall,fscore,support=score(y_test, cl_rand_fr.predict(X_test))\n        return fscore, accuracy_score(y_test, cl_rand_fr.predict(X_test)), cl_rand_fr.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [\"Xgboost\", \"KN Neighbors\", \"Logistic Regression\", \"Random Forest\"]\nfsc1=[]\nfsc2=[]\nacc=[]\nf1acc=[]\nresult = [\"Regular\", \"MinMaxScaled\", \"Quantile\", \"SMOTE\", \"Max\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Four Classifier Results for Regular Rest/Train Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X[cols], y, test_size = 0.3, random_state=42, stratify=y)\n\nfscore, accuracy = XGB(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = KNN(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = LR(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy, features = RF(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\ncoef = pd.Series(features, index = X_train.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(10, 10))\ncoef.head(11).plot(kind='bar')\nplt.title('Feature Significance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Four Classifier Results for MinMax Scaled Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=MinMaxScaler()\n\nX_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\nX_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n\nfscore, accuracy = XGB(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = KNN(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = LR(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy, features = RF(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Four Classifier Results with Additional Quantile Feautres for Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Introduce quantile features and add them to dataset\nscaler=MinMaxScaler()\n\nqcut_cols=['Alkaline_Phosphotase', 'Direct_Bilirubin', 'Alamine_Aminotransferase', 'Total_Bilirubin', 'Aspartate_Aminotransferase', 'Age', 'Albumin_and_Globulin_Ratio']\nimportant_fs=data[qcut_cols]\n\nfor i in range(len(qcut_cols)):\n    new_q_f1=pd.get_dummies(pd.qcut(data[qcut_cols[i]], 4, labels=[qcut_cols[i]+\"_Q0\", qcut_cols[i]+\"_Q1\", qcut_cols[i]+\"_Q2\", qcut_cols[i]+\"_Q3\"]))\n    important_fs=pd.concat([important_fs, new_q_f1], axis=1, sort=False)\n\nnew_cols=important_fs.columns.values\n\nX_train, X_test, y_train, y_test = train_test_split(important_fs[new_cols], y, test_size = 0.3, random_state=0, stratify=y)\n\nX_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\nX_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n\nfscore, accuracy = XGB(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = KNN(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = LR(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy, features = RF(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Four Classifier Results with SMOTE Oversamping for Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SMOTE oversampling \nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import make_scorer\n\nf1_scorer = make_scorer(f1_score)\n\nX_train, X_test, y_train, y_test = train_test_split(X[cols], y, test_size = 0.3, random_state=0, stratify=y)\n\nold_X_test = X_test\nold_y_test = y_test\n\nparameters = {'n_estimators':[200, 400, 600, 800, 1000], 'min_samples_leaf':[4, 8, 16], 'max_features': ['auto', 'sqrt']}\nrfc = RandomForestClassifier(random_state=42, min_samples_split=5, oob_score=True)\n\nsm = SMOTE(random_state=0, sampling_strategy=1.0, k_neighbors=7, n_jobs=-1)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\nX_test_res, y_test_res = sm.fit_sample(X_test, y_test.ravel())\n\nfscore, accuracy = XGB(X_train_res, y_train_res, X_test.values, y_test.values)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = KNN(X_train_res, y_train_res, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = LR(X_train_res, y_train_res, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy, features = RF(X_train_res, y_train_res, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Four Classifier Results with Additional Max Value Feautres for Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting sample value to 1 for the new feature, if healthy patient’s value from the original \n# feature is above the whisker extend of the corresponding un-healthy patient’s value.\n# Otherwise, we will set it to 0.\n\nimportant_max=data[qcut_cols]\nval = data[qcut_cols][data['Dataset'] == 2].max()\nfor i in range(len(qcut_cols)):\n    new_max_f1=qcut_cols[i]\n    important_max[new_max_f1+'_max']=np.where(important_max[new_max_f1]>val[new_max_f1], 1, 0)\n\nnew_cols_max=important_max.columns.values\n\nX_train, X_test, y_train, y_test = train_test_split(important_max[new_cols_max], y, test_size = 0.3, random_state=0, stratify=y)\n\nfscore, accuracy = XGB(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = KNN(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy = LR(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)\n\nfscore, accuracy, features = RF(X_train, y_train, X_test, y_test)\n\nfsc1.append(fscore[0])\nfsc2.append(fscore[1])\nacc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntable=pd.DataFrame({'Applied Method':result, 'f1 score for unhealthy patients from Xgboost':[fsc1[0], fsc1[4], fsc1[8], fsc1[12], fsc1[16]], 'f1 score for healthy patients from Xgboost':[fsc2[0], fsc2[4], fsc2[8], fsc2[12], fsc2[16]], 'f1 accuracy score for Xgboost':[acc[0], acc[4], acc[8], acc[12], acc[16]], 'f1 score for unhealthy patients from KNN':[fsc1[1], fsc1[5], fsc1[9], fsc1[13], fsc1[17]], 'f1 score for healthy patients from KNN':[fsc2[1], fsc2[5], fsc2[9], fsc2[13], fsc2[17]], 'f1 accuracy score for KNN':[acc[1], acc[5], acc[9], acc[13], acc[17]], 'f1 score for unhealthy patients from Logistic Regression':[fsc1[2], fsc1[6], fsc1[10], fsc1[14], fsc1[18]], 'f1 score for healthy patients from Logistic Regression':[fsc2[2], fsc2[6], fsc2[10], fsc2[14], fsc2[18]], 'f1 accuracy score for Logistic Regression':[acc[2], acc[6], acc[10], acc[14], acc[18]], 'f1 score for unhealthy patients from Random Forest':[fsc1[3], fsc1[7], fsc1[11], fsc1[15], fsc1[19]], 'f1 score for healthy patients from Random Forest':[fsc2[3], fsc2[7], fsc2[11], fsc2[15], fsc2[19]], 'f1 accuracy score for Random Forest':[acc[3], acc[7], acc[11], acc[15], acc[19]]})\ntable=table.set_index('Applied Method').T\ntable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"The best f1 accuracy score for Random Forest is with SMOTE oversampled train/test split:\", table['SMOTE'].loc[\"f1 accuracy score for Random Forest\"])\nprint (\"with f1 score for unhealthy patients:\", table['SMOTE'].loc[\"f1 score for unhealthy patients from Random Forest\"], \"and f1 score for healthy patients:\", table['SMOTE'].loc[\"f1 score for healthy patients from Random Forest\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF(X_train_res, y_train_res, old_X_test, old_y_test, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = data._get_numeric_data().columns\ncor = data[num_cols].corr()\n\nthreshold = 0.7\n\ncorlist = []\n\nfor i in range(0,len(num_cols)):\n    for j in range(i+1,len(num_cols)):\n        if (j != i and cor.iloc[i,j] <= 1 ) or (j != i and cor.iloc[i,j] >= -1):\n            corlist.append([cor.iloc[i,j],i,j]) \n\n#Sort higher correlations first            \nsort_corlist = sorted(corlist,key=lambda x: -abs(x[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(17,17))\n\ncorr_mat=data.corr()\nsns.heatmap(corr_mat,annot=True,linewidths=1, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_plot=[]\ny_plot=[]\nfor x,i,j in sort_corlist:\n    if num_cols[i] != 'Dataset' and num_cols[j] != 'Dataset':\n#        print (num_cols[i],num_cols[j],x)\n        x_plot.append(num_cols[i])\n        y_plot.append(num_cols[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Strongly correlated pairs\nx_max_plot=[]\ny_max_plot=[]\nfor x,i,j in sort_corlist:\n    if num_cols[i] != 'Dataset' and num_cols[j] != 'Dataset':\n        if x >= 0.60:\n            x_max_plot.append(num_cols[i])\n            y_max_plot.append(num_cols[j])\n            print (num_cols[i],num_cols[j],x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\naxes=axes.flatten()\nfor i in range(len(x_max_plot)):\n    sns.scatterplot(data=data, x=x_max_plot[i], y=y_max_plot[i], ax=axes[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"We investigated the Liver patients dataset and applied machine learning algorithms to predict the patient disease. Our observations revealed that the distribution of patients with and without disease significantly differ. Specifically, we observe that the healthy patients have higher frequency of small values in narrow ranges for total bilirubin, direct bilirubin, aspertate aminotransferace and alkaline phosphotase compared to unhealthy patients.\n\nSeveral ML algorithms were used to predict the outcome on test data. We used logistic regression, xgboost, random forest and knn classifier with gridsearch parameters.\n\nSeveral feature engineering methods were applied to generate new features. Specifically, new hot encoded quantile features were introduced for alkaline phosphotase, direct bilirubin, alamine aminotransferase, total bilirubin, aspartate aminotransferase, age and albumin and Globulin Ratio features since they revealed some hints of possible outliers in the data.\n\nAnother feature engineering is done by setting sample value to one for the new feature, if healthy patient’s value from the original feature is above the whisker extend of the corresponding un-healthy patient’s value, zero otherwise.\n\nThe SMOTE oversampling was applied to improve the imbalance Dataset feature for liver patient disease outcome, since it included more data for unhealthy patients than healthy ones. Our results demonstrated that random forest with SMOTE produced better f1 score on both classes.\n\nWe found 4 pairs of strongly correlated features: direct and total bilirubin, aspertate aminotransferace and alamine aminotransferace, albumin and total proteins, albumin and globulin ratio and albumin.\n\nOverall, these analysis and techniques can be applied for liver patient diagnoses and similar medical related problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}