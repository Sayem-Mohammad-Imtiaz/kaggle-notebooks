{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThis Notebook covers the following aspects:\n\n~1. Overview\n\n~2. Data Preprocessing\n\n    ~2.1 Encoding categorical variables\n    \n    \n~3. Classification Model\n\n~4. Tuning Model\n\n~5. Building the Final Model"},{"metadata":{},"cell_type":"markdown","source":"# **1. Overview**"},{"metadata":{},"cell_type":"markdown","source":"Input Variables:\n\n-**classes**: edible=e, poisonous=p)\n\n-**cap-shape**: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\n-**cap-surface**: fibrous=f,grooves=g,scaly=y,smooth=s\n\n-**cap-color**: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\n-**bruises**: bruises=t,no=f\n\n-**odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\n-**gill-attachment**: attached=a,descending=d,free=f,notched=n\n\n-**gill-spacing**: close=c,crowded=w,distant=d\n\n-**gill-size**: broad=b,narrow=n\n\n-**gill-color**: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\n-**stalk-shape**: enlarging=e,tapering=t\n\n-**stalk-root**: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\n-**stalk-surface-above-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n\n-**stalk-surface-below-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n\n-**stalk-color-above-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n-**stalk-color-below-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n-**veil-type: partial**=p,universal=u\n\n-**veil-color: brown**=n,orange=o,white=w,yellow=y\n\n-**ring-number**: none=n,one=o,two=t\n\n-**ring-type**: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\n-**spore-print-color**: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\n-**population**: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\n-**habitat**: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"},{"metadata":{},"cell_type":"markdown","source":"# **2. Data Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understanding the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.isna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isin([' ?']).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No need to use this code as this dataset no null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Encoding categorical variables"},{"metadata":{},"cell_type":"markdown","source":"Encoding categorical variables numerically for classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategorical_df = df.select_dtypes(include=['object'])\ncategorical_df.columns\n\n\nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\n\ncategorical_df = categorical_df.apply(enc.fit_transform)\ncategorical_df.head()\n\n\ndf = df.drop(categorical_df.columns, axis=1)\ndf = pd.concat([df, categorical_df], axis=1)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('class', axis=1)\ny = df['class']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Classification Model"},{"metadata":{},"cell_type":"markdown","source":"Split Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using default model\n\n1. Logistic Regression\n\n2. SVM\n\n3. Decision Tree\n\n4. Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndef_lr= LogisticRegression()\ndef_lr.fit(X_train, y_train)\n\nlr_pred = def_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, lr_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\ndef_svm = svm.SVC()\ndef_svm.fit(X_train, y_train)\n\nsvm_pred = def_svm.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(\"SVM accuracy: \", accuracy_score(y_test, svm_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision TreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndef_dt= DecisionTreeClassifier()\ndef_dt.fit(X_train, y_train)\n\ndt_pred = def_dt.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Decision Tree accuracy\", accuracy_score(y_test, dt_pred))\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ndef_rf = RandomForestClassifier()\ndef_rf.fit(X_train, y_train)\n\n\nrf_pred = def_rf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Random Forests accuracy\", accuracy_score(y_test, rf_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Tuning Model"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV , KFold\n\n\nlr= LogisticRegression()\n\n\ngs_grid  = {\n               'penalty': ['l1','l2','elasticnet'],\n               'class_weight': ['balanced', 'None'],\n               'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\n\nlr_CV = GridSearchCV(estimator = lr, param_grid=gs_grid, cv=5)\n\nresult = lr_CV.fit(X_train, y_train)\n\nprint(result.best_params_)\nprint(result.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Building the Final Model"},{"metadata":{},"cell_type":"markdown","source":"Input those parameter into Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\nfinal_lr=LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced')\nfinal_lr.fit(X_train, y_train)\n\nfinal_lr_pred = final_lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression accuracy: \", accuracy_score(y_test, final_lr_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 4 Model's accuary scoure: \n\nI will choose ***Random Forest*** as Final Model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}