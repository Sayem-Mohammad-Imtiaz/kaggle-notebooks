{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Personnal projet:** Stoke predictions with very unbalanced class.\n\n**Goal:** *Predict the stroke risk thanks to the healthcare-dataset-stroke*\n\nIn this notebook I will have to deal with very balanced target variable classes.\n\nSo I'm going to use several class rebalancing strategies. two **oversampling** methods, one **undersampling** method and **class_weight = \"balanced\"** for models that support this option.\n\nIs notebook consist in 5 parts:\n\n> **1. Exploratory data analysis**\n> \n> **2. Features engineering**\n> \n> **3. Class rebalancing strategies**\n> \n> **4. Classification**\n> \n>  **5.Let's Interpret the predictions**\n> \n> >I will use the **Shapley** method which explains the contribution of each feature in the prediction. This method explains how the model works globally, but also each prediction individually. \n\n* In the last section **(Bonus)** I propose my method to test the model on false data samples.\n\n***I hope you wiil enjoy readding :)***\n","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The dasaset have {} columns and  {} rows\".format(df.shape[1],df.shape[0]))\nprint(\"------------------------\")\nfor col in df.columns:\n    print(\"{} nan(s) in columns  {}\".format(df[col].isna().sum(),col))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 1.Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Stroke classes .\",size=16)\nsns.countplot(data=df, x=\"stroke\",edgecolor=\"black\",color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target classes are very unbalanced!","metadata":{}},{"cell_type":"markdown","source":"* **Smoking status**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"smoking_status classes repartion.\")\nsns.countplot(data=df, x=\"smoking_status\",hue=\"stroke\",edgecolor=\"black\",color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exact_percent(var):\n    #Extraction of the percent of stroke in each class of smoke status\n    percent=[]\n    lab=[]\n    for s in df[var].unique():\n        d=df.loc[df[var]==s]\n        N= d.shape[0]\n        n1=d.loc[d[\"stroke\"]==1].shape[0]\n        p =n1/N*100\n        percent.append(p)\n        lab.append(s)\n    return lab,percent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_percent(lab,percent,name):\n    fig=plt.figure(figsize=[8,6])\n    fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n    plt.title(\"Stroke rate according the {} .\".format(name))\n    plt.bar(np.arange(len(lab)),percent,edgecolor=\"black\",color=\"#b8c7e1\")\n    plt.xticks(np.arange(len(lab)),lab,rotation=45)\n    plt.ylabel(\"percent %\")\n    y=max(percent)+2\n    plt.ylim(0,y)\n    for i, p in enumerate(percent):\n        plt.text(i-0.1, p+0.5, \"{}%\".format(round(p,1)),size=12)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab,percent=exact_percent(\"smoking_status\")\nplot_percent(lab,percent,\"smoking_status\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  **GENDER and MARITAL STATUS**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Gender classes repartion.\")\nsns.countplot(data=df, x='gender',hue=\"stroke\",edgecolor=\"black\",color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab,percent=exact_percent(\"gender\")\nplot_percent(lab,percent,\"gender\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab,percent=exact_percent('ever_married')\nplot_percent(lab,percent,'ever_married')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a bias. the stroke rate is not directly related to whether you were married or not. People who have been married are on average much older (as we see on the next graph).\n\n**==>Beware of misinterpretation**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Age according the marital status classes.\")\nsns.boxplot(data=df,y='age',x='ever_married',width=0.4,showfliers=False,color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **work_type**","metadata":{}},{"cell_type":"code","source":"lab,percent=exact_percent('work_type')\nplot_percent(lab,percent,'work_type')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Residence_type'**","metadata":{}},{"cell_type":"code","source":"lab,percent=exact_percent('Residence_type')\nplot_percent(lab,percent,'Residence_type')\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab,percent=exact_percent('hypertension')\nplot_percent(lab,percent,'hypertension')\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab,percent=exact_percent('heart_disease')\nplot_percent(lab,percent,'heart_disease')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **'avg_glucose_level' and 'bmi'**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"avg_glucose_level repartion according the stoke classes.\")\nsns.boxplot(data=df,y='avg_glucose_level',x=\"gender\",hue=\"stroke\",width=0.4,showfliers=False,color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"avg_glucose_level repartion according the stoke classes.\")\nsns.boxplot(data=df,y='bmi',x=\"gender\",hue=\"stroke\",width=0.4,showfliers=False,color=\"#b8c7e1\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 2. Features ingineering","metadata":{}},{"cell_type":"code","source":"def smoke_encoder(val):\n    if val  in ['Unknown','never smoked']:\n        return 0\n    elif val==\"smokes\":\n        return 1\n    else:\n        return 2\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=df[[\"stroke\",\"avg_glucose_level\",\"bmi\",\"age\",\"hypertension\",\"heart_disease\",\"gender\",\"ever_married\"]].copy()\ndata[\"smoking_status\"]=df[\"smoking_status\"].apply(smoke_encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def work_type_encoder(val):\n    if val=='Self-employed':\n        return 2\n    elif val in ['Private','Govt_job']:\n        return 1\n    else: \n        return 0\n\ndata[\"word_type\"]=df[\"work_type\"].apply(work_type_encoder)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_gender(val):\n    if val==\"Male\":\n        return 1\n    elif val== \"Female\":\n        return 0\n    else:\n        return np.nan\ndata[\"gender\"]=data[\"gender\"].apply(encode_gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_married(val):\n    if val==\"Yes\":\n        return 1\n    else:\n        return 0\ndata[\"ever_married\"]=data[\"ever_married\"].apply(encode_married)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove the individual whose gender is \"Other\"","metadata":{}},{"cell_type":"code","source":"data=data.loc[data[\"gender\"].isna()==False]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.fillna(data.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Check the correlations**","metadata":{}},{"cell_type":"markdown","source":"the following function allows to keep all the individuals of class 1 for the target variable + a N sample of the others","metadata":{}},{"cell_type":"code","source":"\ndef get_sample(N,df):\n    d0=df.loc[df[\"stroke\"]==0]\n    d1=df.loc[df[\"stroke\"]==1]\n    \n    n =d0.shape[0]\n    \n    p =N/n\n    sample=d0.sample(frac=p, replace=True)\n    d=pd.concat([d1,sample])\n    print(\"A {} rows sample as been extracted.\".format(d.shape[0]))\n    \n    return d\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=get_sample(300,data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# calculate the correlation matrix\ncorr = sample.corr()\n\n# plot the heatmap\nfig=plt.figure(figsize=[12,9])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Correlations\",size=18)\nax=sns.heatmap(corr, vmin=-1, vmax=1,cmap=\"bwr\",\n        xticklabels=data.columns,\n        yticklabels=data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We take a sample for a better visualization\nsample=get_sample(800,data)\nsample.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 3.  **Class rebalancing strategies**","metadata":{}},{"cell_type":"markdown","source":"We use 3 class rebalancing strategies:\n\n* Two Oversampling strategies:\n\n**ADASYN** (Adaptive Synthetic): is an algorithm that generates synthetic data, and its greatest advantages are not copying the same minority data, and generating more data for “harder to learn” examples.\n\n**SMOTE** (Synthetic Minority Oversampling Technique): first selects a minority class instance **A** at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors **B** at random and connecting **A** and **B** to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances **A** and **B**.\n\n* One undersampling method:\n\n**Random Undersampling**: This method seeks to randomly select and remove samples from the majority class, consequently reducing the number of examples in the majority class in the transformed data.\n\n> *Warning*:This can be highly problematic, as the loss of such data can make the decision boundary between the minority and majority instances harder to learn, resulting in a loss in classification performance.\n\nWe also use the **class_weight=\"balanced\"** option for the randomforest and the logistic regression .\n","metadata":{}},{"cell_type":"code","source":"from sklearn import decomposition\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE\n\nX=sample.drop(columns=[\"stroke\"])\ny=sample[\"stroke\"]\n\npca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nX_vis = pca.fit_transform(X)\n\n# Apply the random over-sampling\nada = ADASYN()\n#ada.fit(X,y)\nX_resampled, y_resampled = ada.fit_resample(X, y)\nX_res_vis = pca.transform(X_resampled)\n\n# Two subplots, unpack the axes array immediately\nf, (ax1, ax2) = plt.subplots(1, 2)\n\nc0 = ax1.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], label=\"Class #0\",\n                 alpha=0.5)\nc1 = ax1.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], label=\"Class #1\",\n                 alpha=0.5)\nax1.set_title('Original set')\n\nax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n            label=\"Class #0\", alpha=.5)\nax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n            label=\"Class #1\", alpha=.5)\nax2.set_title('ADASYN')\n\n# make nice plotting\nfor ax in (ax1, ax2):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n   \nplt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n              ncol=2, labelspacing=0.)\nplt.tight_layout(pad=3)\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the random over-sampling\nsmo = SMOTE()\n#ada.fit(X,y)\nX_resampled, y_resampled = smo.fit_resample(X, y)\nX_res_vis = pca.transform(X_resampled)\n\n# Two subplots, unpack the axes array immediately\nf, (ax1, ax2) = plt.subplots(1, 2)\n\nc0 = ax1.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], label=\"Class #0\",\n                 alpha=0.5)\nc1 = ax1.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], label=\"Class #1\",\n                 alpha=0.5)\nax1.set_title('Original set')\n\nax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n            label=\"Class #0\", alpha=.5)\nax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n            label=\"Class #1\", alpha=.5)\nax2.set_title('SMOTE')\n\n# make nice plotting\nfor ax in (ax1, ax2):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n   \nplt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n              ncol=2, labelspacing=0.)\nplt.tight_layout(pad=3)\nplt.show()\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom imblearn.under_sampling import RandomUnderSampler\n\nros = RandomUnderSampler()\n#ada.fit(X,y)\nX_resampled, y_resampled = ros.fit_resample(X, y)\nX_res_vis = pca.transform(X_resampled)\n\n# Two subplots, unpack the axes array immediately\nf, (ax1, ax2) = plt.subplots(1, 2)\n\nc0 = ax1.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], label=\"Class #0\",\n                 alpha=0.5)\nc1 = ax1.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], label=\"Class #1\",\n                 alpha=0.5)\nax1.set_title('Original set')\n\nax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n            label=\"Class #0\", alpha=.5)\nax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n            label=\"Class #1\", alpha=.5)\nax2.set_title('Random under sampler')\n\n# make nice plotting\nfor ax in (ax1, ax2):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n   \nplt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n              ncol=2, labelspacing=0.)\nplt.tight_layout(pad=3)\nplt.show()\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 4. **Classification**","metadata":{}},{"cell_type":"markdown","source":"* **class rebalancing**","metadata":{}},{"cell_type":"markdown","source":"We use the **F1 score** as cross validation score for the rest of the notebook. \n\nUsing accuracy would be irrelevant on the test set where the classes are unbalanced(The models would look falsely efficient)","metadata":{}},{"cell_type":"code","source":"#under and over sampler \nada = ADASYN()\nsmo = SMOTE()\nrus = RandomUnderSampler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\ndef lets_try(train, y):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(train)\n    \n   \n    def test_model(clf):\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, train, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n\n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n    \n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, scaled_train, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    clf = SVC(kernel=\"linear\")\n    results[\"SVC\"] = test_model_scaler(clf)\n    print(\"SVC done\")\n    \n    clf = LogisticRegression()\n    results[\"Logistic Regression\"] = test_model_scaler(clf)\n    print(\"Logistic Regression done\")\n\n    clf = KNeighborsClassifier()\n    results[\"Kneighbors\"] = test_model(clf)\n    print(\"Kneighbors done\")\n\n    clf = SVC(kernel=\"poly\")\n    results[\"SVC poly\"] = test_model_scaler(clf)\n    print(\"SVC poly done.\")\n\n    clf = RandomForestClassifier()\n    results[\"Random Forest Classifier\"] = test_model(clf)\n    print(\"Random Forest Classifier done\")\n\n\n    clf =SVC(kernel='rbf')\n    results[\"SVC RBF\"] = test_model_scaler(clf)\n    print(\"SVC rbf done\")\n\n   \n    return results ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX=data.drop(columns=[\"stroke\"])\ny=data[\"stroke\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=ada.fit_resample(X, y)\nadasyn_results=lets_try(X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=smo.fit_resample(X, y)\nsmote_results=lets_try(X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=rus.fit_resample(X, y)\nrus_results=lets_try(X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **With the *class_weight=\"balanced\"* option**","metadata":{}},{"cell_type":"code","source":"def lets_try2(train, y):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(train)\n    \n   \n    def test_model(clf):\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, train, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n\n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n    \n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, scaled_train, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    \n    clf = LogisticRegression(class_weight=\"balanced\")\n    results[\"Logistic Regression\"] = test_model_scaler(clf)\n    print(\"Logistic Regression done\")\n\n    \n\n    clf = RandomForestClassifier(class_weight=\"balanced\")\n    results[\"Random Forest Classifier\"] = test_model(clf)\n    print(\"Random Forest Classifier done\")\n\n   \n    return results ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_results=lets_try2(X_train, y_train)","metadata":{"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(1,figsize=[20,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"CV results on the train set . 5 folds\",size=16)\n\nplt.subplot(1,4,1)\nplt.title(\"Adasyn oversampling\",size=16)\nplt.boxplot(adasyn_results.values(),labels=adasyn_results.keys(),showmeans=True)\nplt.ylabel(\"  Scores CV \\n (f1)\",size=14)\nplt.ylim(0.6,1)\nplt.xticks(rotation=90)\nplt.grid()\n\nplt.subplot(1,4,2)\nplt.title(\"Smote oversampling\",size=16)\nplt.boxplot(smote_results.values(),labels=smote_results.keys(),showmeans=True)\nplt.ylim(0.6,1)\nplt.xticks(rotation=90)\nplt.grid()\n\nplt.subplot(1,4,3)\nplt.title(\"Random Undersampling\",size=16)\nplt.boxplot(rus_results.values(),labels=rus_results.keys(),showmeans=True)\nplt.ylim(0.6,1)\nplt.xticks(rotation=90)\nplt.grid()\n\nplt.subplot(1,4,4)\nplt.title(\"Balanced class_weight\",size=16)\nplt.boxplot(balanced_results.values(),labels=balanced_results.keys(),showmeans=True)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.grid()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Check results on the testing set.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef lets_try_test(test, y,X_train,y_train):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(X_train)\n    scaled_test=ss.transform(test)\n    \n   \n    def test_model(clf):\n        clf.fit(X_train,y_train)\n        pred=clf.predict(test)\n        score=f1_score(y,pred)\n        return score\n\n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n        clf.fit(scaled_train,y_train)\n        pred=clf.predict(scaled_test)\n        score=f1_score(y,pred)\n        return score\n\n    \n    clf = SVC(kernel=\"linear\")\n    results[\"SVC\"] = test_model_scaler(clf)\n    print(\"SVC done\")\n    \n    clf = LogisticRegression()\n    results[\"Logistic Regression\"] = test_model_scaler(clf)\n    print(\"Logistic Regression done\")\n\n    clf = KNeighborsClassifier()\n    results[\"Kneighbors\"] = test_model(clf)\n    print(\"Kneighbors done\")\n\n    clf = SVC(kernel=\"poly\")\n    results[\"SVC poly\"] = test_model_scaler(clf)\n    print(\"SVC poly done.\")\n\n    clf = RandomForestClassifier()\n    results[\"Random Forest Classifier\"] = test_model(clf)\n    print(\"Random Forest Classifier done\")\n\n\n    clf =SVC(kernel='rbf')\n    results[\"SVC RBF\"] = test_model_scaler(clf)\n    print(\"SVC rbf done\")\n\n   \n    return results ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=ada.fit_resample(X, y)\nadasyn_results=lets_try_test(X_test,y_test,X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=smo.fit_resample(X, y)\nsmote_results=lets_try_test(X_test,y_test,X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=rus.fit_resample(X, y)\nrus_results=lets_try_test(X_test,y_test,X_train_res, y_train_res)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(1,figsize=[20,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.suptitle(\"F1 results on the testing set\",size=16)\nn=len(adasyn_results)\nplt.subplot(1,3,1)\nplt.title(\"Adasyn oversampling\",size=16)\nplt.bar(range(0,n),adasyn_results.values(),edgecolor=\"black\",color=\"#b8c7e1\")\nplt.ylabel(\"F1 score\",size=14)\nplt.ylim(0,1.01)\nplt.xticks(range(0,n),adasyn_results.keys(),rotation=90)\nplt.grid()\n\nplt.subplot(1,3,2)\nplt.title(\"Smote oversampling\",size=16)\nplt.bar(range(0,n),smote_results.values(),edgecolor=\"black\",color=\"#b8c7e1\")\nplt.ylim(0,1.01)\nplt.xticks(range(0,n),smote_results.keys(),rotation=90)\nplt.grid()\n\nplt.subplot(1,3,3)\nplt.title(\"Random Undersampling\",size=16)\nplt.bar(range(0,n),rus_results.values(),edgecolor=\"black\",color=\"#b8c7e1\")\nplt.ylim(0,1.01)\nplt.xticks(range(0,n),rus_results.keys(),rotation=90)\nplt.grid()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=ada.fit_resample(X, y)\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nmodel0=RandomForestClassifier()\nmodel0.fit(X_train_res, y_train_res)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model0,X_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the testing set \\n with ADASYN\",size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just take a look on the Kneigbors performances on the testing set (by using SMOTE)","metadata":{}},{"cell_type":"code","source":"X_train_res, y_train_res=smo.fit_resample(X, y)\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nmodel0=KNeighborsClassifier()\nmodel0.fit(X_train_res, y_train_res)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model0,X_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the KNeighbors on the testing set \\n with smote\",size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We need to convert the target variable in a categoriel one \n# for a better displayed summary_plot (shap )\ndef convert_variable(val):\n    return str(val)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_res, y_train_res=smo.fit_resample(X, y)\ny_train_res=y_train_res.apply(convert_variable)\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nrf_smote=RandomForestClassifier()\nrf_smote.fit(X_train_res, y_train_res)\n\ny_test=y_test.apply(convert_variable)\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(rf_smote,X_test, y_test\n                           , cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the testing set \\n with smothe\",size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. **Let's Interpret the predictions**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Feature importances of the Random forest using SMOTE.\",size=16)\nplt.barh(X_test.columns, rf_smote.feature_importances_,color=\"#28a2b4\",edgecolor='black')\nplt.grid()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **SHAP**:\n\nIn the following section,I will use **the shapley method** to interpret the model predictions\n\nSHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016)48 is a method to explain individual predictions. SHAP is based on the game theoretically optimal Shapley Values.\n\nMore informations? follow this link: https://christophm.github.io/interpretable-ml-book/shap.html#examples-4\n","metadata":{}},{"cell_type":"code","source":"import shap\n\nshap.initjs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(rf_smote)\nshap_values = explainer.shap_values(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values[1],X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features are displayed according to their impact on the prediction in descending order.\n\nFor example, age is the most impactful feature, a high age will tend to increase\n stroke risk score\n \n This summary_plot is globally consistent with the previous feature importances plot ","metadata":{}},{"cell_type":"code","source":"def display_gender(val):\n    if val ==1:\n        return \"Male\"\n    else:\n        return \"Female\"\ndef display_married(val):\n    if val==1:\n        return \"Ever married\"\n    else:\n        return \"Never married\"\n    \n    \ndef show_one_case(num):\n    print(\"--------------------\")\n    print(\"Case N° {}\".format(num))\n    print(\"--------------------\")\n    case=X_test.iloc[[num],:]\n    for i in case.columns:\n        if i==\"gender\":\n            print(i,\":  \", display_gender(case[i].values[0]))\n        elif i==\"ever_married\":\n            print(\"Martial status :  \", display_married(case[i].values[0]))\n        else:\n            print(i,\":  \", case[i].values[0])\n    pred=rf_smote.predict_proba(case)[0]\n    print(\"--------------------\")\n    print('True class :', y_test.iloc[num])\n    print(\"Stroke risks: {} %\".format(pred[1]*100))","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=1674\n\n#421\nshow_one_case(n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"case1 = X_test.iloc[[n],:]\nshap_values = explainer.shap_values(case1)\nshap.force_plot(explainer.expected_value[1], shap_values[1], case1)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the case of this 82-year-old woman, her age,her word type as well as her high BMI considerably increase her risk of stroke.\nRisk = 82%\n","metadata":{}},{"cell_type":"code","source":"n=89\nshow_one_case(n)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"case2 = X_test.iloc[[n],:]\nshap_values = explainer.shap_values(case2)\nshap.force_plot(explainer.expected_value[1], shap_values[1], case2)\n","metadata":{"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we observe the case of a child it is mainly his very young age (5 years old) which protected him from  stroke risk.","metadata":{}},{"cell_type":"markdown","source":"# **Bonus**: \"Test of fire\".\n\nWe  submit our model to two robustness tests by offering it a false sample to predict.\n\n**Test 1:** For this first test, we  generate false data by randomly choosing values among those present in the trainning set.\n\n**Test 2:** for the second test we generate  random numbers between 0 and 300  for each feature","metadata":{}},{"cell_type":"code","source":"from random import sample\nfrom random import choice\ndef generate_fake_sample1(N,Xtest,ytest):\n    X_fake_sample=pd.DataFrame(columns=Xtest.columns)\n    for feature in Xtest.columns:\n        list_values=list(Xtest[feature])\n        fake_values=sample(list_values,N)\n        X_fake_sample[feature]=pd.Series(fake_values)\n    l=[\"0\",\"1\"]    \n    y_fake=[choice(l) for i in range(0,N) ]\n    \n    return X_fake_sample,pd.Series(y_fake)\n    \n    ","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_fake_sample2(N,Xtest,ytest):\n    X_fake_sample=pd.DataFrame(columns=Xtest.columns)\n    for feature in Xtest.columns:\n        fake_values=randint(0,300,N)\n        X_fake_sample[feature]=pd.Series(fake_values)\n    l=[\"0\",\"1\"]    \n    y_fake=[choice(l) for i in range(0,N) ]\n    \n    return X_fake_sample,pd.Series(y_fake)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_fake,y_fake=generate_fake_sample1(800,X_train,y_train)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_pred=rf_smote.predict(x_fake)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(rf_smote,x_fake,y_fake\n                           ,normalize=\"true\", cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the Fake set N°1 \\n with smothe\",size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_fake,y_fake=generate_fake_sample1(800,X_train,y_train)\nfake_pred=rf_smote.predict(x_fake)\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(rf_smote,x_fake,y_fake\n                           ,normalize=\"true\", cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the Fake set N°2 \\n with smothe\",size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**On  fake samples, the performances are  not good. So we can reasonably think that scores on real data reflect a real ability to predict stroke risks.**","metadata":{}},{"cell_type":"markdown","source":"# Conclusion:\nThe random forest obtains perfect results regardless of the oversampling strategy used.\n**SMOTE** and **ADASYN** can therefore be used interchangeably.\n\nThe **age**, the **avg_glucose_level** and body **masse index** are the features who have the biggest impact on the predicition.\n\nWith shapley method we see that the **marital status** and the **gender** also impact the prediction","metadata":{}}]}