{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Using Fake and Real news datasets\nimport pandas as pd\nimport re\ndata_true =pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ndata_fake =pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Personal project:** Create a Fake News detector\n\nI'm going to use the two sets of fake-and-real-news-dataset to train models and detect fake new ones.\n\nI would mainly use newspaper article headlines.\n\nThis notebook consists of 3 parts\n\n**1. Exploring the Dataset**\n\n**2. feature engineering and preprocessing**\n\n**3. Classification**\n> * Compare models on the trainning set\n> * Compare the best models on the testing set\n> \n\n*I hope you will enjoy reading!!*","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upload the data sets.","metadata":{}},{"cell_type":"code","source":"print(data_true.shape)\nprint(data_true.columns)\ndata_true[\"Target\"]=\"True\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_true.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_fake.shape)\nprint(data_fake.columns)\ndata_fake[\"Target\"]=\"Fake\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fake.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df=pd.concat([data_true, data_fake], ignore_index=True)\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.  Exploring the data","metadata":{}},{"cell_type":"markdown","source":"* **\"Subject\" analysis**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=[12,9])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Subjects\",size=18)\nsns.countplot(data=df, x=\"subject\",hue=\"Target\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[\"subject\"].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Re-encode the label of the Subjects","metadata":{}},{"cell_type":"code","source":"def encode_subject(label):\n    if label  in [\"politicsNews\",'politics' ,'Government News','left-news']:\n        return \"politics\"\n    elif label  in ['worldnews' ,'News']:\n        return \"world news\"\n    else:\n        return \"US_News\"\n\ndf[\"subject\"]=df[\"subject\"].apply(encode_subject) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[10,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Subjects\",size=18)\nsns.countplot(data=df, x=\"subject\",hue=\"Target\", palette=\"Set3\",edgecolor=\"black\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we remove the rows whose subject is US_News\n","metadata":{}},{"cell_type":"code","source":"df=df.loc[df[\"subject\"]!=\"US_News\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Titles and texts lenght  analysis**","metadata":{}},{"cell_type":"markdown","source":"> **Question:** Is there a link between the title length and the target variable?","metadata":{}},{"cell_type":"code","source":"import nltk\n\ndef count_words(title):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    new_words = tokenizer.tokenize(title)\n    return len(new_words)\ndf[\"n_words in title\"]=df[\"title\"].apply(count_words)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,5])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Number of words in the title.\",size=18)\nsns.boxplot(data=df, x=\"Target\",y=\"n_words in title\",showfliers=False,width=0.4,color=\"#a5acce\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The fake publications seems to have  longer titles than the true ones.","metadata":{}},{"cell_type":"markdown","source":"Same approach with the all **text**","metadata":{}},{"cell_type":"code","source":"df[\"n_words in text\"]=df[\"text\"].apply(count_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[8,5])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\n\nplt.title(\"Number of words in the text.\",size=18)\nsns.boxplot(data=df, x=\"Target\",y=\"n_words in text\",showfliers=False,width=0.4,color=\"#a5acce\")\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"In this section i'll be worked with a 6000 rows sample.\n \nWe extract feature form the titles by using **TfidfVectorizer** .We only fit this algorithme on the trainning set, thus the test set is not used to build the model or the preprocessing methods.\n\nwe extract a large number of variables, so we apply a dimensional reduction with **PCA**\n\nFinnaly we encode the subject with **.get_dummies()**","metadata":{}},{"cell_type":"code","source":"def get_sample(N,df):\n    n =df.shape[0]\n    p =N/n\n    sample=df.sample(frac=p, replace=True)\n    print(\"A {} rows sample as been extracted.\".format(N))\n    return sample\nsample=get_sample(6000,df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\ny=sample[\"Target\"]\nX=sample.drop(columns=[\"Target\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the following function returns the cleaned text corpus. remove punctuation and stopwords","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\ndef a_number(word):\n    for letter in word:\n        if str(letter)in [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]:\n            return True\ndef not_a_word(word):\n    if len(word)==1:\n        return True\n    elif a_number(word):\n        return True\n    else:\n        return False\n\ndef extract_corpus(col,df):\n    snowstem = SnowballStemmer(language=\"english\")\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    stop_words = set(stopwords.words(\"english\"))\n    \n    stop_word2=[\"for\",\"is\",\"a\",\"of\",\"no\",\"not\",\"he\",\"she\",\n                \"this\",\"on\",\"it\",\"to\",\"in\",\"at\",\"is\", \"or\",\n                \"in\",\"not\",\"by\",\"if\",\"in\"]\n    stop_words= stop_words.union(stop_word2)\n    corpus = []\n    text_list=df[col].values\n    \n    for text in text_list:\n        text=tokenizer.tokenize(text)\n        review = [snowstem.stem(word.lower()) for word in text if not word in stop_words]\n        review=[word for word in review if not_a_word(word)==False]\n        review = ' '.join(review)\n        \n        corpus.append(review)\n    return corpus\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nthe function determines the number of principal components to generate to keep  **60% of the explained variance** \n\nReturns a PCA instance with de good number of PC.","metadata":{}},{"cell_type":"code","source":"from sklearn import decomposition\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef nbr_of_pca(features):\n    scaler=StandardScaler() \n    Xs=scaler.fit_transform(features)\n    \n\n    # apply PCA\n    pca = decomposition.PCA(n_components=min(features.shape[1],\n                                             features.shape[0])).fit(Xs)\n    nbr_pca=0\n    scree = pca.explained_variance_ratio_\n    for i in range(features.shape[1]):\n        a = scree.cumsum()[i]\n        if a >= 0.6:\n            print(\"{} principal components explaines  60% of the total variance\".format(i))\n            print(\"Sum of variance explained :{}%\".format(round(a*100,2)))\n            nbr_pca=i\n            break\n    pca = decomposition.PCA(n_components=nbr_pca)\n    \n    return pca,nbr_pca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is **preprocessing the train** Set and returns the **pca and the Vectorizer** fitted on this set + the processed train set","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport scipy.sparse\n\ndef preprocess_train_set(train):\n    ref=train[[\"subject\",\"n_words in title\",\"n_words in text\"]]\n    \n    #create corpus to fit vectorizer\n    corpus=extract_corpus(\"title\",train)\n    vectorizer = TfidfVectorizer()\n    features = vectorizer.fit_transform(corpus)\n    #features to pandas data frame\n    features=pd.DataFrame(features.todense())\n    \n    #Apply PCA with the right number of component to keep \n    #60% of de explained variance\n    scaler=StandardScaler() \n    Xs=scaler.fit_transform(features)\n    pca,N=nbr_of_pca(features)\n    d=pca.fit_transform(Xs)\n    \n    d=pd.DataFrame(d, columns=[\"Title PCA n°{}\".format(i+1) for i in range(0,N) ])\n    for i in d.columns:\n        ref[i]=d[i].values\n    ref=pd.get_dummies(data=ref, columns=[\"subject\"])\n    #Return the pca and vectoriver fitted ,\n    # + the number of principal component and the data preprocessed\n    return pca,N,vectorizer, ref\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function take the **pca and vectorizer** already fitted on the train set and returns de processed testing set .","metadata":{}},{"cell_type":"code","source":"def preprocess_test(vectorizer,pca,N,test):\n    ref=test[[\"subject\",\"n_words in title\",\"n_words in text\"]]\n    corpus=extract_corpus(\"title\",test)\n    #Vectorizer already fitted on the train set \n    features = vectorizer.transform(corpus)\n    features=pd.DataFrame(features.todense())\n    \n    scaler=StandardScaler() \n    Xs=scaler.fit_transform(features)\n    #PCA already fitted on the train set\n    d=pca.transform(Xs)\n    \n    d=pd.DataFrame(d, columns=[\"Title PCA n°{}\".format(i+1) for i in range(0,N) ])\n    for i in d.columns:\n        ref[i]=d[i].values\n    ref=pd.get_dummies(data=ref, columns=[\"subject\"])\n    return ref","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca,N,vectorizer, Xtrain=preprocess_train_set(X_train)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nXtest=preprocess_test(vectorizer,pca,N,X_test)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Trainning set:\")\nprint(Xtrain.shape[0],'Rows',Xtrain.shape[1],\"columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Testing set:\")\nprint(Xtest.shape[0],'Rows',Xtest.shape[1],\"columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  3.  Classification .","metadata":{}},{"cell_type":"markdown","source":" # 3.1 Compare model on the trainning set","metadata":{}},{"cell_type":"markdown","source":"the following function tests the models on the trainning set and returns the results as a dictionary.\nWe apply a cross validation with 10 splits","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\ndef lets_try(train, y):\n    results = {}\n    ss=StandardScaler()\n    scaled_train=ss.fit_transform(train)\n    \n   \n    def test_model(clf):\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, train, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n\n    #for the model which needed standardized data \n    def test_model_scaler(clf):\n    \n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf, scaled_train, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    clf = SVC(kernel=\"linear\")\n    results[\"SVC\"] = test_model_scaler(clf)\n    print(\"SVC done\")\n    \n    clf = LogisticRegression()\n    results[\"Logistic Regression\"] = test_model_scaler(clf)\n    print(\"Logistic Regression done\")\n\n    clf = KNeighborsClassifier()\n    results[\"Kneighbors\"] = test_model(clf)\n    print(\"Kneighbors done\")\n\n    clf = SVC(kernel=\"poly\")\n    results[\"SVC poly\"] = test_model_scaler(clf)\n    print(\"SVC poly done.\")\n\n    clf = RandomForestClassifier()\n    results[\"Random Forest Classifier\"] = test_model(clf)\n    print(\"Random Forest Classifier done\")\n\n\n    clf =SVC(kernel='rbf')\n    results[\"SVC RBF\"] = test_model_scaler(clf)\n    print(\"SVC rbf done\")\n\n   \n    return results ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_results=lets_try(Xtrain, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[10,10])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of Cross-validation score  on the trainning set. \\n 10 folds\",size=16)\nplt.boxplot(dic_results.values(),labels=dic_results.keys(),showmeans=True)\nplt.ylabel(\"  Scores CV \\n (Accuracy)\",size=14)\nplt.ylim(0.4,1)\nplt.xticks(rotation=90)\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n4 models seem very efficient on the training set:\n\n> *  **Random forest**\n> * **logistic regression**\n> * **SVC kernel rbf**\n> * **SVC kernel linear**\n\nlet's evaluate these models on the test set","metadata":{}},{"cell_type":"markdown","source":"# 3.2 Compare the best models on the testing set","metadata":{}},{"cell_type":"code","source":"ss=StandardScaler()\nscaled_test=ss.fit_transform(Xtest)\nscaled_train=ss.fit_transform(Xtrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nmodel0=RandomForestClassifier()\nmodel0.fit(Xtrain,y_train)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model0,Xtest, y_test\n                           , normalize='true', cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the Random forest on the testing set\",size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1=LogisticRegression()\nmodel1.fit(scaled_train,y_train)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model1,scaled_test, y_test\n                           , normalize='true', cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of logistic regression on the testing set\",size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=SVC(kernel='rbf')\n\nmodel.fit(scaled_train,y_train) \n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model, scaled_test, y_test\n                           , normalize='true', cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the SVC(rbf kernel) on the testing set\",size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=SVC(kernel='linear')\n\nmodel.fit(scaled_train,y_train) \n\nfig, ax = plt.subplots(figsize=(8, 6))\ndisp=plot_confusion_matrix(model, scaled_test, y_test\n                           , normalize='true', cmap=plt.cm.Blues, ax=ax)\ndisp.ax_.set_title(\"Results of the SVC( linear) on the testing set\",size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_y_test(test):\n    y_list=[]\n    for i in test:\n        if i==\"True\":\n            y_list.append(0)\n        else:\n            y_list.append(1)\n    return pd.Series(y_list)\nY=convert_y_test(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score,auc\n#reg logistic\npred_prob1 = model1.predict_proba(scaled_test)\nfpr1, tpr1, thresh1 = roc_curve(Y, pred_prob1[:,1])\n\n#random forest\npred_prob0 = model0.predict_proba(Xtest)\nfpr0, tpr0, thresh0 = roc_curve(Y, pred_prob0[:,1])\nauc0= auc(tpr0,fpr0 )\nauc1= auc(tpr1,fpr1 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=[6,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"ROC curve\",size=16)\nplt.plot(tpr1,fpr1,  linestyle='--', label='Logistic regression')\nplt.plot(tpr0,fpr0,  marker='.', label='RandomForest')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.text(0.75,0.18,\"AUC log:{} \\n AUC RF:{}\".format(round(auc1,3),round(auc0,3)))\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **HoW large have to be de train to obtain good prediction performance ?**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n    \ndef get_processed_set(n_rows):\n    sample=get_sample(n_rows,df)\n    y=sample[\"Target\"]\n    X=sample.drop(columns=[\"Target\"])\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    pca,N,vectorizer, Xtrain=preprocess_train_set(X_train)\n    Xtest=preprocess_test(vectorizer,pca,N,X_test)\n    return Xtrain, Xtest, y_train, y_test\n\ndef get_cv_score(train, test,y_train, y_test):\n    model=LogisticRegression()\n    ss=StandardScaler()\n    scaled_test=ss.fit_transform(test)\n    scaled_train=ss.fit_transform(train)\n    def test_model_scaled(clf):\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n        scores = cross_val_score(clf,scaled_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n        return scores.mean()\n    \n    cv_score=test_model_scaled(model)\n    model.fit(scaled_train,y_train)\n    test_pred=model.predict(scaled_test)\n    score_test=accuracy_score(y_test,test_pred)\n    return cv_score, score_test\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score=[]\ntest_score=[]\nlist_n=[200,500,1000,1500,3000,4500,6000,7000,8000]\ntrain_shape=[]\nfor n in list_n:\n    #get the sets\n    Xtrain, Xtest, y_train, y_test=get_processed_set(n)\n    N=Xtrain.shape[0]\n    #Get scores values  \n    cv_score, score_test=get_cv_score(Xtrain, Xtest, y_train, y_test)\n    train_shape.append(N)\n    train_score.append(cv_score)\n    test_score.append(score_test)\n","metadata":{"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display scores\nfig=plt.figure(figsize=[10,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Accuracy score according of the number \\n of rows in the trainning set \",size=16)\nplt.plot(train_shape,train_score,  linestyle='--', label='Train set ')\nplt.plot(train_shape,test_score,  marker='.', label='Test set')\nplt.xlabel(\"Number of rows\")\nplt.ylabel(\"Accuracy \")\nplt.ylim(0.7,1)\nplt.text(2500,0.72,\"The testing set represent 1/3 of the total dataset\")\nplt.grid()\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions :\nThe random Forest and the logistique regression are the most efficient models. The **logistic regression** being much faster we recommend to use it.\n\nFor the training set A sample of 4000 lines is enough to fit the TfidfVectorizer ,so that the models can then generalize in an efficient way.\n\n\nCross validation scores are much low on smaller samples","metadata":{}},{"cell_type":"markdown","source":"**Bonus: T-SNE visualisation .**","metadata":{}},{"cell_type":"code","source":"def convert_y_color(test):\n    y_list=[]\n    for i in test:\n        if i==\"True\":\n            y_list.append(\"#a11d31\")\n        else:\n            y_list.append(\"#4277b2\")\n    return pd.Series(y_list)\nY=convert_y_color(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2,learning_rate=100)\nx_new=tsne.fit_transform(Xtrain)\n\nplt.scatter(x_new[:,0],x_new[:,1],c=Y)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(n_components=3,learning_rate=100)\nx_new=tsne.fit_transform(Xtrain)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1, figsize=(8, 6))\nplt.title(\"3D Visualisation with t-SNE\")\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(x_new[:,0],x_new[:,1],x_new[:,2], c=Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}