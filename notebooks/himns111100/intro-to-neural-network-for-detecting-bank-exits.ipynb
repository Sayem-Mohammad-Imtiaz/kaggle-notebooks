{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Given Dataset consisting of Bank Customer Information, we need to create a model that will predict if a customer will leave the bank or not."},{"metadata":{},"cell_type":"markdown","source":"### Import the necessary Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n### Removes warnings that occassionally show up\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bank=pd.read_csv('/kaggle/input/bank-customer-churn-modeling/Churn_Modelling.csv')\ndata_bank.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EXPLORATORY DATA ANALYSIS"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bank.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bank.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bank.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above information indicates that there are no missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5 point summary\ndata_bank.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CORRELATION - HEAT MAP\ncolormap = plt.cm.plasma\nplt.figure(figsize=(17,10))\nplt.title('Correlation of Cutomer Exiting Bank', y=1.05, size=15)\nsns.heatmap(data_bank.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \n            linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Checking number of people exited from the bank via Geography. i.e. from mentioned Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Geography\", data=data_bank,hue=\"Exited\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gendermap = sns.FacetGrid(data_bank,hue = 'Exited')\n(gendermap.map(plt.hist,'Age',edgecolor=\"w\").add_legend())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop the columns which are unique for all users like IDs"},{"metadata":{},"cell_type":"markdown","source":"#### Removing CustomerId, RowNumber and Surname. Removing Surname is basically because One Hot Encoding will behave unusual if this column is retained(give error)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data_new = data_bank.drop(['RowNumber', 'CustomerId', 'Surname'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbank_data_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of Numerical Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_distribution = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\nfor i in numerical_distribution:\n    plt.hist(data_bank[i])\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Label Encoding the Bank.csv columns to make them integers"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Label Encoding of all the columns\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# Categorical boolean mask\ncategorical_feature_mask = bank_data_new.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = bank_data_new.columns[categorical_feature_mask].tolist()\nbank_data_new[categorical_cols] = bank_data_new[categorical_cols].apply(lambda col: le.fit_transform(col))\nprint(bank_data_new.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_scaled = bank_data_new.apply(zscore)\nX_columns =  df_scaled.columns.tolist()[1:10]\nY_Columns = bank_data_new.columns.tolist()[-1:]\n\nX = df_scaled[X_columns].values\ny = np.array(bank_data_new['Exited']) # Exited\n\nprint(y)\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state= 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the output class label (One-Hot Encoding)\ny_train=to_categorical(y_train,2)\ny_test=to_categorical(y_test,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nnormalize=Normalizer(norm=\"l2\")\nX_train=normalize.transform(X_train)\n\nprint(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test=normalize.transform(X_test)\nprint(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize Sequential Graph (model)\nmodel = tf.keras.Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(units=6, activation='relu', input_shape=(9,)))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train, y_train, batch_size=45, epochs=200, validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe above code indicates:\n\nX_train is the independent variable portion of the data which needs to be fitted with the model.\n\ny_train is the output portion of the data which the model needs to produce after fitting.\n\nbatch_size: How often we want to back-propogate the error values so that individual node weights can be adjusted.\n\nepochs: The number of times we want to run the entire test data over again to tune the weights. This is like the fuel of the algorithm.\n\nvalidation_split: 0.1 The fraction of data to use for validation data."},{"metadata":{},"cell_type":"markdown","source":"### Checking the Accuracy for Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test,verbose=1)\n\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscore = model.evaluate(X_train, y_train,verbose=1)\n\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix Calculation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfmatrx= confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confmatrx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.array(history.history['accuracy']) * 100)\nplt.plot(np.array(history.history['val_accuracy']) * 100)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'])\nplt.title('Accuracy over epochs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PRINT THE ACCURACY"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (((confmatrx[0][0]+confmatrx[1][1])*100)/(len(y_test)), '% of testing data was classified correctly')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the accuracy using accuracy_score as well to check if the above calculation is correct or not.\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n#### The Train and Test set are almost similar. It shows that the model did not overfit on the train set.\n\n#### On Compiling the Neural Network, I have used optimizer as \"adam\" as it is very efficient to Stochastic Gradient Decent. The loss function used is \"binary_crossentropy which is used within adam.\n#### The accuracy metrics which will be evaluated(minimized) by the model. The \"Accuracy\" is used as a criteria to improve model performance.\n\n#### On calculation of the accuracy based on the Confusion Matrix, it came out as 86.45(APPROX). which matches with the score calculated through various EPOCH\n\n#### We can used Optimizer as \"SGD\" as well to check if we get better results and accuracy.\n\n#### HOWEVER, FOR BETTER RESULT WE CAN YOUR HYPER PARAMETER TUNING FOR BETTER RESULT."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}