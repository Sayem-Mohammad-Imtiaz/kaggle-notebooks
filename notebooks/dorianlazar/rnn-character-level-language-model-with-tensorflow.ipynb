{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### A simple RNN language model implemented from scratch with TensorFlow\n#### It is similar to my previous notebook that used words vocabulary,\n#### but this time I tried a vocabulary of ASCII characters instead","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-24T16:22:38.2882Z","iopub.execute_input":"2021-06-24T16:22:38.288611Z","iopub.status.idle":"2021-06-24T16:22:39.871091Z","shell.execute_reply.started":"2021-06-24T16:22:38.288521Z","shell.execute_reply":"2021-06-24T16:22:39.870257Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom typing import Union\nfrom math import ceil\nfrom os import mkdir","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:36.973085Z","iopub.execute_input":"2021-07-01T15:23:36.973706Z","iopub.status.idle":"2021-07-01T15:23:38.842304Z","shell.execute_reply.started":"2021-07-01T15:23:36.973617Z","shell.execute_reply":"2021-07-01T15:23:38.841286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EOS = chr(10) # End of sentence\n\ndef build_vocabulary() -> list:\n    # builds a vocabulary using ASCII characters\n    vocabulary = [chr(i) for i in range(10, 128)]\n    return vocabulary\n\ndef word2index(vocabulary: list, word: str) -> int:\n    # returns the index of 'word' in the vocabulary\n    return vocabulary.index(word)\n\ndef words2onehot(vocabulary: list, words: list) -> np.ndarray:\n    # transforms the list of words given as argument into\n    # a one-hot matrix representation using the index in the vocabulary\n    n_words = len(words)\n    n_voc = len(vocabulary)\n    indices = np.array([word2index(vocabulary, word) for word in words])\n    a = np.zeros((n_words, n_voc))\n    a[np.arange(n_words), indices] = 1\n    return a\n\ndef sample_word(vocabulary: list, prob: np.ndarray) -> str:\n    # sample a word from the vocabulary according to 'prob'\n    # probability distribution (the softmax output of our model)\n    return np.random.choice(vocabulary, p=prob)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:41.682113Z","iopub.execute_input":"2021-07-01T15:23:41.682822Z","iopub.status.idle":"2021-07-01T15:23:41.693511Z","shell.execute_reply.started":"2021-07-01T15:23:41.682771Z","shell.execute_reply":"2021-07-01T15:23:41.692547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self, vocabulary: list = [], a_size: int = 0):\n        self.vocab = vocabulary\n        self.vocab_size = len(vocabulary)\n        self.a_size = a_size\n        self.combined_size = self.vocab_size + self.a_size\n        \n        # weights and bias used to compute the new a\n        # (a = vector that is passes to the next time step)\n        self.wa = tf.Variable(tf.random.normal(\n            stddev=1.0/(self.combined_size+self.a_size),\n            shape=(self.combined_size, self.a_size),\n            dtype=tf.double))\n        self.ba = tf.Variable(tf.random.normal(\n            stddev=1.0/(1+self.a_size),\n            shape=(1, self.a_size),\n            dtype=tf.double))\n        \n        # weights and bias used to compute y (the softmax predictions)\n        self.wy = tf.Variable(tf.random.normal(\n            stddev=1.0/(self.a_size+self.vocab_size),\n            shape=(self.a_size, self.vocab_size),\n            dtype=tf.double))\n        self.by = tf.Variable(tf.random.normal(\n            stddev=1.0/(1+self.vocab_size),\n            shape=(1, self.vocab_size),\n            dtype=tf.double))\n        \n        self.weights = [self.wa, self.ba, self.wy, self.by]\n        self.optimizer = tf.keras.optimizers.Adam()\n    \n    def __call__(self,\n                 a: Union[np.ndarray, tf.Tensor],\n                 x: Union[np.ndarray, tf.Tensor],\n                 y: Union[np.ndarray, tf.Tensor, None] = None) -> tuple:\n        \n        a_new = tf.math.tanh(tf.linalg.matmul(tf.concat([a, x], axis=1), self.wa)+self.ba)\n        y_logits = tf.linalg.matmul(a_new, self.wy)+self.by\n        if y is None:\n            # during prediction return softmax probabilities\n            return (a_new, tf.nn.softmax(y_logits))\n        else:\n            # during training return loss\n            return (a_new, tf.math.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(y, y_logits)))\n    \n    def fit(self,\n            sentences: list,\n            batch_size: int = 128,\n            epochs: int = 10) -> None:\n        \n        n_sent = len(sentences)\n        num_batches = ceil(n_sent / batch_size)\n        \n        for epoch in range(epochs):\n            \n            random.shuffle(sentences)\n            start = 0\n            batch_idx = 0\n            \n            while start < n_sent:\n                \n                print('Training model: %05.2f%%' %\n                      (100*(epoch*num_batches+batch_idx+1)/(epochs*num_batches),),\n                      end='\\r')\n                \n                batch_idx += 1\n                end = min(start+batch_size, n_sent)\n                batch_sent = sentences[start:end]\n                start = end\n                batch_sent.sort(reverse=True, key=lambda s: len(s))\n                \n                init_num_words = len(batch_sent)\n                a = np.zeros((init_num_words, self.a_size))\n                x = np.zeros((init_num_words, self.vocab_size))\n                \n                time_steps = len(batch_sent[0])+1\n                \n                with tf.GradientTape() as tape:\n                \n                    losses = []\n                    for t in range(time_steps):\n                        words = []\n                        for i in range(init_num_words):\n                            if t > len(batch_sent[i]):\n                                break\n                            if t == len(batch_sent[i]):\n                                words.append(EOS)\n                                break\n                            words.append(batch_sent[i][t])\n\n                        y = words2onehot(self.vocab, words)\n                        n = y.shape[0]\n                        a, loss = self(a[0:n], x[0:n], y)\n                        losses.append(loss)\n                        x = y\n                    \n                    loss_value = tf.math.reduce_mean(losses)\n                \n                grads = tape.gradient(loss_value, self.weights)\n                self.optimizer.apply_gradients(zip(grads, self.weights))\n\n    def sample(self) -> str:\n        # sample a new sentence from the learned model\n        sentence = ''\n        a = np.zeros((1, self.a_size))\n        x = np.zeros((1, self.vocab_size))\n        while True:\n            a, y_hat = self(a, x)\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            sentence += word\n            x = words2onehot(self.vocab, [word])\n        return sentence\n    \n    def predict_next(self, sentence: str) -> str:\n        # predict the next part of the sentence given as parameter\n        a = np.zeros((1, self.a_size))\n        for word in sentence.strip():\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        s = ''\n        while True:\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            s += word\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        return s\n    \n    def save(self, name: str) -> None:\n        mkdir(f'./{name}')\n        with open(f'./{name}/vocabulary.txt', 'w') as f:\n            f.write(','.join(self.vocab))\n        with open(f'./{name}/a_size.txt', 'w') as f:\n            f.write(str(self.a_size))\n        np.save(f'./{name}/wa.npy', self.wa.numpy())\n        np.save(f'./{name}/ba.npy', self.ba.numpy())\n        np.save(f'./{name}/wy.npy', self.wy.numpy())\n        np.save(f'./{name}/by.npy', self.by.numpy())\n    \n    def load(self, name: str) -> None:\n        with open(f'./{name}/vocabulary.txt', 'r') as f:\n            self.vocab = f.read().split(',')\n        with open(f'./{name}/a_size.txt', 'r') as f:\n            self.a_size = int(f.read())\n            \n        self.vocab_size = len(self.vocab)\n        self.combined_size = self.vocab_size + self.a_size\n        \n        self.wa = tf.Variable(np.load(f'./{name}/wa.npy'))\n        self.ba = tf.Variable(np.load(f'./{name}/ba.npy'))\n        self.wy = tf.Variable(np.load(f'./{name}/wy.npy'))\n        self.by = tf.Variable(np.load(f'./{name}/by.npy'))\n        self.weights = [self.wa, self.ba, self.wy, self.by]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:44.434088Z","iopub.execute_input":"2021-07-01T15:23:44.434455Z","iopub.status.idle":"2021-07-01T15:23:44.471158Z","shell.execute_reply.started":"2021-07-01T15:23:44.434425Z","shell.execute_reply":"2021-07-01T15:23:44.470126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/million-headlines/abcnews-date-text.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:46.733016Z","iopub.execute_input":"2021-07-01T15:23:46.733409Z","iopub.status.idle":"2021-07-01T15:23:49.008694Z","shell.execute_reply.started":"2021-07-01T15:23:46.733382Z","shell.execute_reply":"2021-07-01T15:23:49.007742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = build_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:52.6666Z","iopub.execute_input":"2021-07-01T15:23:52.666937Z","iopub.status.idle":"2021-07-01T15:23:52.671694Z","shell.execute_reply.started":"2021-07-01T15:23:52.66691Z","shell.execute_reply":"2021-07-01T15:23:52.670546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = df['headline_text'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:54.633956Z","iopub.execute_input":"2021-07-01T15:23:54.634335Z","iopub.status.idle":"2021-07-01T15:23:54.662145Z","shell.execute_reply.started":"2021-07-01T15:23:54.634302Z","shell.execute_reply":"2021-07-01T15:23:54.660856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(vocabulary, 1024)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:56.192259Z","iopub.execute_input":"2021-07-01T15:23:56.192603Z","iopub.status.idle":"2021-07-01T15:23:56.235368Z","shell.execute_reply.started":"2021-07-01T15:23:56.192575Z","shell.execute_reply":"2021-07-01T15:23:56.234458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(sentences, batch_size=4096, epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T15:23:57.61607Z","iopub.execute_input":"2021-07-01T15:23:57.616429Z","iopub.status.idle":"2021-07-01T16:33:14.859481Z","shell.execute_reply.started":"2021-07-01T15:23:57.6164Z","shell.execute_reply":"2021-07-01T16:33:14.858492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('news_headlines_model')\n# model.load('news_headlines_model')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T16:47:29.809322Z","iopub.execute_input":"2021-07-01T16:47:29.809773Z","iopub.status.idle":"2021-07-01T16:47:29.819771Z","shell.execute_reply.started":"2021-07-01T16:47:29.80974Z","shell.execute_reply":"2021-07-01T16:47:29.818946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    print(model.sample())","metadata":{"execution":{"iopub.status.busy":"2021-07-01T16:47:33.951487Z","iopub.execute_input":"2021-07-01T16:47:33.952018Z","iopub.status.idle":"2021-07-01T16:47:34.22249Z","shell.execute_reply.started":"2021-07-01T16:47:33.951975Z","shell.execute_reply":"2021-07-01T16:47:34.221354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = 'scientists just discovered'\ns += model.predict_next(s)\ns","metadata":{"execution":{"iopub.status.busy":"2021-07-01T16:47:57.588455Z","iopub.execute_input":"2021-07-01T16:47:57.588789Z","iopub.status.idle":"2021-07-01T16:47:57.610982Z","shell.execute_reply.started":"2021-07-01T16:47:57.588761Z","shell.execute_reply":"2021-07-01T16:47:57.610128Z"},"trusted":true},"execution_count":null,"outputs":[]}]}