{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-06T16:05:29.93941Z","iopub.execute_input":"2021-06-06T16:05:29.939818Z","iopub.status.idle":"2021-06-06T16:05:29.956904Z","shell.execute_reply.started":"2021-06-06T16:05:29.939785Z","shell.execute_reply":"2021-06-06T16:05:29.955285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport random\n\nnp.random.seed(1001)\nrandom.seed(1001)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:05:35.978648Z","iopub.execute_input":"2021-06-06T16:05:35.979023Z","iopub.status.idle":"2021-06-06T16:05:35.9853Z","shell.execute_reply.started":"2021-06-06T16:05:35.97899Z","shell.execute_reply":"2021-06-06T16:05:35.983751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/predict-test-scores-of-students/test_scores.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:06:38.217775Z","iopub.execute_input":"2021-06-06T16:06:38.218185Z","iopub.status.idle":"2021-06-06T16:06:38.282022Z","shell.execute_reply.started":"2021-06-06T16:06:38.218152Z","shell.execute_reply":"2021-06-06T16:06:38.280903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loaded fine. Also, all attributes are reasonably well balanced and there are no missing values. Nice tidy data.\n\n## Low-hanging fruit: the pre-test scores\n\nWe've got a pre-test score. These are usually very predictive (they are even sometimes used as a proxy for test results when a final test can't be sat).\n\nLet's see how well it correlates:","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=data, x='posttest',y='pretest')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:07:00.99627Z","iopub.execute_input":"2021-06-06T16:07:00.99672Z","iopub.status.idle":"2021-06-06T16:07:01.239747Z","shell.execute_reply.started":"2021-06-06T16:07:00.996634Z","shell.execute_reply":"2021-06-06T16:07:01.239044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is highly correlated. It's also one of the few real-valued attributes and it would be good to factor that out so any models we build can stick to fitting parameters to the difficult stuff.\n\nLet's move to predicting the difference from pre- to post-test results. Happily, minimising the MAD on the difference in the scores gives the same loss as MAD on the final score itself. This means we can build a delta-score predictor and just add each of its predictions to the pre-test value.\n\nIf we end up using decision trees, this also means we can avoid the use of hard thresholds at decision points which can be a weakness.\n","metadata":{}},{"cell_type":"code","source":"data = data.assign(testdelta=data['posttest']-data['pretest'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:07:23.016828Z","iopub.execute_input":"2021-06-06T16:07:23.01749Z","iopub.status.idle":"2021-06-06T16:07:23.040064Z","shell.execute_reply.started":"2021-06-06T16:07:23.017452Z","shell.execute_reply":"2021-06-06T16:07:23.039273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.testdelta.mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:02.898129Z","iopub.execute_input":"2021-06-06T16:28:02.898493Z","iopub.status.idle":"2021-06-06T16:28:02.905753Z","shell.execute_reply.started":"2021-06-06T16:28:02.898463Z","shell.execute_reply":"2021-06-06T16:28:02.904894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So on average, the pre-test is 12 points lower than the final test result.\n\n### An initial baseline model\nWe can now make a super-simple baseline model: add the average delta to the pre-test.\nNote that this is calculated across all data, so we're evaluating on the test data here, but it still gives a good feeling for where we start from:","metadata":{}},{"cell_type":"code","source":"sum(abs(x-12.14) for x in data.testdelta) / len(data.testdelta)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:05.755331Z","iopub.execute_input":"2021-06-06T16:28:05.755844Z","iopub.status.idle":"2021-06-06T16:28:05.76392Z","shell.execute_reply.started":"2021-06-06T16:28:05.755802Z","shell.execute_reply":"2021-06-06T16:28:05.763085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we should be looking to do (significantly) better than a MAD of 3.5.\n\n## Other real-valued attributes\nLet's take a look at the other real-valued attribute: the number of students. We'd expect this to be negatively correlated with final score (small classes doing better) but it's less clear whether this will correlate with the delta-scores.","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:08:24.182203Z","iopub.execute_input":"2021-06-06T16:08:24.182626Z","iopub.status.idle":"2021-06-06T16:08:24.190644Z","shell.execute_reply.started":"2021-06-06T16:08:24.182585Z","shell.execute_reply":"2021-06-06T16:08:24.189128Z"}}},{"cell_type":"code","source":"sns.scatterplot(data=data, x='testdelta',y='n_student', alpha=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:07.323313Z","iopub.execute_input":"2021-06-06T16:28:07.32367Z","iopub.status.idle":"2021-06-06T16:28:07.503897Z","shell.execute_reply.started":"2021-06-06T16:28:07.32364Z","shell.execute_reply":"2021-06-06T16:28:07.502614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatter plot shows no obvious correlations. Some care must be taken not to overfit on this attribute: consider the unusual distributions at class sizes 26 and 29.\n\nThese are clearly just artifacts as they shouldn't differ much from those at the adjacent class sizes. It's probably information leakage from the classroom attribute.\n\nWe'll leave this attribute out to begin with. Also note we're cheating a bit here as these plots are on all data. Once we've looked at the possible values of the discrete attributes we'll make the test/train split to ensure we're doing any further investigations properly.\n\n## Discrete attributes","metadata":{}},{"cell_type":"code","source":"data[['school_setting','school_type','teaching_method','gender','lunch']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:09.002648Z","iopub.execute_input":"2021-06-06T16:28:09.003044Z","iopub.status.idle":"2021-06-06T16:28:09.036484Z","shell.execute_reply.started":"2021-06-06T16:28:09.003011Z","shell.execute_reply":"2021-06-06T16:28:09.035115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.school_setting.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:11.967093Z","iopub.execute_input":"2021-06-06T16:28:11.967435Z","iopub.status.idle":"2021-06-06T16:28:11.975042Z","shell.execute_reply.started":"2021-06-06T16:28:11.967406Z","shell.execute_reply":"2021-06-06T16:28:11.973589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two attributes have been left out here.\n\nThe *classroom* attribute feels a bit too fine-grained. Also, it wouldn't be useful in a real model in which we'd want to predict scores on later years (rather than predicting classmates' scores on the exact same test).\n\nThe *school* attribute may be informative. At this point I'm assuming that the school setting+type+teaching method sufficiently captures what the school does. The plan is to first work without this attribute, then compare a final model with it in.","metadata":{}},{"cell_type":"markdown","source":"## Upper bound model\n\nWe have a baseline of 3.5, but we can also find a reasonable upper limit to what we could achieve.\n\nWe'll maximally overfit to the training data. In this case that means breaking the data up into the smallest subsets that have identical attribute values (so there will be 3 x 2 x 2 x 2 x 2 = 48 subsets). Using the mean of each subset as the prediction for its members will provide a minimal MAD.\n\nIf there were more features we can do this by learning a single decision tree with high depth, split nodes down to size 2, and permitting leaves of single items. But in this case we can just do the binning ourselves:","metadata":{}},{"cell_type":"code","source":"uniques = [list(data[x].unique()) for x in ['school_setting','school_type','teaching_method','gender','lunch']]\nuniques","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:15.801212Z","iopub.execute_input":"2021-06-06T16:28:15.801556Z","iopub.status.idle":"2021-06-06T16:28:15.811556Z","shell.execute_reply.started":"2021-06-06T16:28:15.801527Z","shell.execute_reply":"2021-06-06T16:28:15.810245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsets = [[]]\nfor feats in uniques:\n    subsets = [ fs + [x] for x in feats for fs in subsets]\nlen(subsets)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:23.468749Z","iopub.execute_input":"2021-06-06T16:28:23.469121Z","iopub.status.idle":"2021-06-06T16:28:23.477296Z","shell.execute_reply.started":"2021-06-06T16:28:23.469091Z","shell.execute_reply":"2021-06-06T16:28:23.476025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsets[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:25.125128Z","iopub.execute_input":"2021-06-06T16:28:25.125499Z","iopub.status.idle":"2021-06-06T16:28:25.134058Z","shell.execute_reply.started":"2021-06-06T16:28:25.125468Z","shell.execute_reply":"2021-06-06T16:28:25.132832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll sub-divide our data into the 48 sub-sets, get a mean, MAD, and number of items.","metadata":{}},{"cell_type":"code","source":"sub_data = [data[(data.school_setting==x[0]) & (data.school_type==x[1]) & (data.teaching_method==x[2]) & (data.gender==x[3]) & (data.lunch==x[4])] for x in subsets]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:26.490891Z","iopub.execute_input":"2021-06-06T16:28:26.491298Z","iopub.status.idle":"2021-06-06T16:28:26.651832Z","shell.execute_reply.started":"2021-06-06T16:28:26.491263Z","shell.execute_reply":"2021-06-06T16:28:26.650572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And do a quick sanity check that the number of items in the subsets matches the original data:","metadata":{}},{"cell_type":"code","source":"print(len(data), sum(len(d) for d in sub_data))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:27.517333Z","iopub.execute_input":"2021-06-06T16:28:27.517809Z","iopub.status.idle":"2021-06-06T16:28:27.523513Z","shell.execute_reply.started":"2021-06-06T16:28:27.517764Z","shell.execute_reply":"2021-06-06T16:28:27.522637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for d in sub_data[:5]:\n    print(len(d),d.testdelta.mean(), d.testdelta.mad())","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:29.250075Z","iopub.execute_input":"2021-06-06T16:28:29.250472Z","iopub.status.idle":"2021-06-06T16:28:29.264931Z","shell.execute_reply.started":"2021-06-06T16:28:29.250438Z","shell.execute_reply":"2021-06-06T16:28:29.262915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MAD we'd get for predicting across the whole dataset is just the weighted mean of the MAD of each sub-set of the data:","metadata":{}},{"cell_type":"code","source":"sum(d.testdelta.mad()*len(d) for d in sub_data)/len(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:31.28954Z","iopub.execute_input":"2021-06-06T16:28:31.289986Z","iopub.status.idle":"2021-06-06T16:28:31.334044Z","shell.execute_reply.started":"2021-06-06T16:28:31.289935Z","shell.execute_reply":"2021-06-06T16:28:31.33305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we'd like to approach 2.6 MAD. Let's see how close we can get to it. \n\nIt's possible that with the *school* and *n_students* attributes you could do a bit better, but we'll leave that for later.\n\n## Test/train split\n\nOur upper and lower bounds have been determined based on all data, as has the decision to predict the delta scores. These decisions could have been made based on training data only so it isn't a big issue. From now on we'll be more careful though.\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:15:25.590872Z","iopub.execute_input":"2021-06-06T16:15:25.591282Z","iopub.status.idle":"2021-06-06T16:15:25.598317Z","shell.execute_reply.started":"2021-06-06T16:15:25.59125Z","shell.execute_reply":"2021-06-06T16:15:25.596796Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nys = data.testdelta\ntrain, test, train_y, test_y = train_test_split(data,ys,train_size=0.8)\ntrain.school_setting.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:34.160446Z","iopub.execute_input":"2021-06-06T16:28:34.16089Z","iopub.status.idle":"2021-06-06T16:28:34.174548Z","shell.execute_reply.started":"2021-06-06T16:28:34.160856Z","shell.execute_reply":"2021-06-06T16:28:34.173334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If three values aren't present for school setting run it again with a new random seed and get a new split.\n\nOtherwise the training data we've got is badly skewed. For determining the one-hot encoding below we'll leave it to automatically infer the encoding so we want all values present.\n\n## Random forest with limited features\nWe'll try to keep the meta-heuristic search space small -- we've only got about 2000 instances and they'll get over-used fast.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.compose import ColumnTransformer\n\ntrain_x = train[['school_setting','school_type','teaching_method','gender','lunch']]\ntest_x = test[['school_setting','school_type','teaching_method','gender','lunch']]\n\n\n# one-hot encode the attribute with three values, the rest can use a single binary feature\nfeature_transform = ColumnTransformer(transformers=[\n    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'), ['school_setting']),\n    ('binary', preprocessing.OneHotEncoder(handle_unknown='error', drop='first'), ['school_type','teaching_method','gender','lunch']),\n])\n\n# consider forests with many small trees - not a very large feature space\nparams = {\n  'predictor__n_estimators':[20,80,240],\n  'predictor__min_samples_leaf':[8,16,32],\n  'predictor__min_samples_split':[8,16,32],\n  'predictor__max_depth':[2,3,4,5]\n}\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:41.788432Z","iopub.execute_input":"2021-06-06T16:28:41.788803Z","iopub.status.idle":"2021-06-06T16:28:41.802001Z","shell.execute_reply.started":"2021-06-06T16:28:41.788773Z","shell.execute_reply":"2021-06-06T16:28:41.800433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the random forest we'll use the mean absolute error criterion since that's what we want to optimise at the leaves. \n\nWe'll also bootstrap, since only toggling features won't give that many unique trees. Re-sampling the data will hopefully improve diversity.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn import ensemble\n\npredictor = Pipeline(steps=[\n    ('feature_transform',feature_transform),\n    ('predictor',ensemble.RandomForestRegressor(criterion='mae', bootstrap=True, n_estimators=20, min_samples_leaf=8,min_samples_split=8,max_depth=2))\n  ])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:48.289447Z","iopub.execute_input":"2021-06-06T16:28:48.289803Z","iopub.status.idle":"2021-06-06T16:28:48.296878Z","shell.execute_reply.started":"2021-06-06T16:28:48.289771Z","shell.execute_reply":"2021-06-06T16:28:48.295582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmeta_fit = GridSearchCV(predictor, params, scoring='neg_mean_absolute_error',n_jobs=-1)\nmeta_fit.fit(train_x,train_y)\nprint('Best score:',meta_fit.best_score_)\nprint('Best params',meta_fit.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:28:50.382939Z","iopub.execute_input":"2021-06-06T16:28:50.383584Z","iopub.status.idle":"2021-06-06T16:33:00.777766Z","shell.execute_reply.started":"2021-06-06T16:28:50.383535Z","shell.execute_reply":"2021-06-06T16:33:00.776514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(meta_fit.score(train_x,train_y), meta_fit.score(test_x,test_y))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:33:00.779597Z","iopub.execute_input":"2021-06-06T16:33:00.779961Z","iopub.status.idle":"2021-06-06T16:33:00.809192Z","shell.execute_reply.started":"2021-06-06T16:33:00.779916Z","shell.execute_reply":"2021-06-06T16:33:00.808225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAD of 2.6 on the training and test data is as good as we could hope for. Normally I'd be concerned about overfitting since we got right down to 2.6 on the training data, but if it holds on the test data then it may be generalising ok.\n\nWe may have an \"easy\" test set here (e.g. with few outliers) as it actually performed slightly better than on the training data.\n\n## Full features\nWe'll now throw in n_student, school, and pretest as additional features.\n\nSince this will then be feature-rich we'll turn off bootstrapping and try out some larger tree sizes.\n\nWe'll use the same test/training split as above, but with the extra features in.","metadata":{}},{"cell_type":"code","source":"train_x = train[['school','n_student','pretest','school_setting','school_type','teaching_method','gender','lunch']]\ntest_x = test[['school','n_student','pretest','school_setting','school_type','teaching_method','gender','lunch']]\n\nfeature_transform = ColumnTransformer(transformers=[\n    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'), ['school_setting','school']),\n    ('binary', preprocessing.OneHotEncoder(handle_unknown='error', drop='first'), ['school_type','teaching_method','gender','lunch']),\n  ],remainder='passthrough')\n\nparams = {\n  'predictor__n_estimators':[20,80,240],\n  'predictor__min_samples_leaf':[4,8,16],\n  'predictor__min_samples_split':[8,16,32],\n  'predictor__max_depth':[2,4,8,16]\n}\n\npredictor = Pipeline(steps=[\n    ('feature_transform',feature_transform),\n    ('predictor',ensemble.RandomForestRegressor(criterion='mae'))\n])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:41:09.380513Z","iopub.execute_input":"2021-06-06T16:41:09.380931Z","iopub.status.idle":"2021-06-06T16:41:09.394518Z","shell.execute_reply.started":"2021-06-06T16:41:09.380898Z","shell.execute_reply":"2021-06-06T16:41:09.393035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_fit = GridSearchCV(predictor, params, scoring='neg_mean_absolute_error',n_jobs=-1)\nmeta_fit.fit(train_x,train_y)\nprint('Best score:',meta_fit.best_score_)\nprint('Best params',meta_fit.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:41:19.959744Z","iopub.execute_input":"2021-06-06T16:41:19.960361Z","iopub.status.idle":"2021-06-06T16:56:51.500722Z","shell.execute_reply.started":"2021-06-06T16:41:19.960311Z","shell.execute_reply":"2021-06-06T16:56:51.499538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(meta_fit.score(train_x,train_y), meta_fit.score(test_x,test_y))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:56:58.280841Z","iopub.execute_input":"2021-06-06T16:56:58.281241Z","iopub.status.idle":"2021-06-06T16:56:58.345684Z","shell.execute_reply.started":"2021-06-06T16:56:58.281209Z","shell.execute_reply":"2021-06-06T16:56:58.344422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nThe full set of attributes reaches 2.4 MAD, which is a fair step from 2.01 on the training data. This is a good uplift from the simple discrete attributes and looks like a reasonable generalisation.\n\nTo find the r2 metric we have to convert from predicted delta scores into posttest values by adding the pretest back on.","metadata":{}},{"cell_type":"code","source":"import sklearn.metrics as metrics\npreds = meta_fit.predict(test_x)\npreds = [y+test_x.pretest.iat[i] for i,y in enumerate(preds)]\npost_ys = [y+test_x.pretest.iat[i] for i,y in enumerate(test_y)]\n\nmetrics.r2_score(preds,post_ys)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T16:58:02.15026Z","iopub.execute_input":"2021-06-06T16:58:02.150661Z","iopub.status.idle":"2021-06-06T16:58:02.198966Z","shell.execute_reply.started":"2021-06-06T16:58:02.150631Z","shell.execute_reply":"2021-06-06T16:58:02.197644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So overall:\n\n2.4 Mean absolute error\n\n95.0% R2 score","metadata":{}}]}