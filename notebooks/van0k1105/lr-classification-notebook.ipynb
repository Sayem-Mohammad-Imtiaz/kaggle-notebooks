{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1d67b898-bba4-f0c9-7bc4-ac042315b7a7"},"source":"# Breast Cancer Logistic Regression Classification"},{"cell_type":"markdown","metadata":{"_cell_guid":"48fd93ab-3ac6-a9f0-0a39-b0f1743511c7"},"source":"<h2> Reading Data </h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"e3eaf08d-b6c5-d805-9d3c-a709ee1bf1e6"},"source":"Importing libraries"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0da6bcf-e73d-f2c6-9f66-3f370f7976f7"},"outputs":[],"source":"import pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import log_loss,r2_score\nfrom sklearn.preprocessing import normalize\n\nimport numpy as np\nnp.set_printoptions(precision=4,suppress=True,linewidth=100)\n\nfrom IPython.display import display\n\nfrom scipy import stats\n\nimport math\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"8932ba87-7d06-d58c-796f-ad4c8a4a9a98"},"source":"Reading data from tables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a35ab85-7fa9-058f-ddbc-dbeb4f8e3e9a"},"outputs":[],"source":"#reading data\ndf = pd.read_csv('../input/data.csv')\n#changing symbolic class tags into numeric ones (M = 1, B = 0)\n#feature selection and transformation - will be expanded in the future\ndf['diagnosis'] = df['diagnosis'].apply(lambda x:1 if x == 'M' else 0)\ndf['concavity_mean'] = df['concavity_mean'].apply(lambda x: np.mean(df['concavity_mean']) if x<=0 else x)\ndf['log_concavity'] = df['concavity_mean'].apply(lambda x: math.log(x))\ndf['log_fd'] = df['fractal_dimension_mean'].apply(lambda x: math.log(x))\ndf = df[['diagnosis','radius_mean','texture_mean','log_concavity','smoothness_mean','symmetry_mean','log_fd']]\ndisplay(df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f83c437-dca9-f7c9-fb83-8239e6b64056"},"outputs":[],"source":"#assigning independent and dependent variables\nX = df[['radius_mean','texture_mean','log_concavity','smoothness_mean','symmetry_mean','log_fd']]\ny = df['diagnosis']"},{"cell_type":"markdown","metadata":{"_cell_guid":"4bd36cbb-6002-0c23-8acd-19d3870164c4"},"source":"<h2> Collinearity tests </h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"03681b45-9379-0873-7573-e69cf5656ef5"},"source":"Calculating VIFs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e395ff8-7430-6f46-e716-f540d39077ec"},"outputs":[],"source":"#calculating coefficients of determination for linear regression of each predictor from other ones\nr_squared = []\nlin_regr = LinearRegression()\nfor column in X.columns:\n    regr_var = X.ix[:, X.columns != column]\n    regr_target = X[column]\n    lin_regr.fit(regr_var,regr_target)\n    r_squared.append(r2_score(regr_target,lin_regr.predict(regr_var)))\n#calculating VIFs\nvif = [1/(1-r2) for r2 in r_squared]\n#output\nr2_data = pd.DataFrame(index = X.columns)\nr2_data['R^2'] = r_squared\nr2_data['VIF'] = vif\ndisplay(r2_data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ada6332-ae66-177d-1328-7a04e0ca4bf1"},"source":"Belsley, Kuh, and Welsch (BKW)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f460955c-4e65-5a4a-01e3-b06d84765494"},"outputs":[],"source":"#normalizing the matrix of independent variables before applying BKW\nX_bkw = normalize(X,norm='l2',axis=0)\n#calculating SVD of the matrix\nU, s, V = np.linalg.svd(X_bkw)\n#calculating condition indexes\ncond_indexes = np.max(s)/s\n#calculating variance-decomposition proportions\nvar_frac = np.matrix([[V[k,j]**2/s[j]**2 for k in range(len(s))] for j in range(len(s))])\nvar_frac = [var_frac[:,j]/np.sum(var_frac[:,j]) for j in range(np.shape(var_frac)[1])]\nvar_frac = np.transpose(np.reshape(a=var_frac,newshape=(6,6)))\n#output\ndf_bkw = pd.DataFrame()\ndf_bkw['Condition Index'] = cond_indexes\ndf_bkw['radius_mean'] = var_frac[:,0]\ndf_bkw['texture_mean'] = var_frac[:,1]\ndf_bkw['log_concavity'] = var_frac[:,2]\ndf_bkw['smoothness_mean'] = var_frac[:,3]\ndf_bkw['symmetry_mean'] = var_frac[:,4]\ndf_bkw['log_fd'] = var_frac[:,5]\ndisplay(df_bkw)"},{"cell_type":"markdown","metadata":{"_cell_guid":"72cd74de-32bc-dc2a-8111-9320e6e5fd0e"},"source":"<h2>Basic model building and evaluation</h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"3364fff3-3792-bd5f-4367-5e448a9668bd"},"source":"Building the model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82a8f707-9c80-40f7-060b-fb0091988605"},"outputs":[],"source":"#ะก is inverse regularization coefficient, we take a large value to suppress regularization for now\nlog_regr = LogisticRegression(C=100000)\n#Fitting the model\nlog_regr.fit(X,y)\n#Displaying coefficients\nprint('Coefficients:')\nprint('Constant: {} X1: {} X2: {} X3: {} X4: {} X5: {} X6: {}'.format(log_regr.intercept_[0],log_regr.coef_[0,0],log_regr.coef_[0,1],log_regr.coef_[0,2],log_regr.coef_[0,3],log_regr.coef_[0,4],log_regr.coef_[0,5]))\ncoefs = [log_regr.intercept_[0],log_regr.coef_[0,0],log_regr.coef_[0,1],log_regr.coef_[0,2],log_regr.coef_[0,3],log_regr.coef_[0,4],log_regr.coef_[0,5]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"735f1eca-5c94-dc26-70d3-d448b3470b79"},"source":"Calculating coefficients' variances"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62d88d19-385d-f6d3-ccd3-aa3d308699fe"},"outputs":[],"source":"#getting the probability scores\nprobs = log_regr.predict_proba(X)[:,1]\n#calculating the covariance matrix\nX_const = pd.DataFrame()\nX_const['Constant_term'] = [1]*len(X)\nX_const = pd.concat([X_const,X],axis=1)\nV = np.diag([x*(1-x) for x in probs])\ncovariance_matrix = np.linalg.inv(np.transpose(X_const).dot(V).dot(X_const))\ncovar_dataframe = pd.DataFrame(data=covariance_matrix,index=X_const.columns,columns=X_const.columns)\nprint('Covariance matrix:')\ndisplay(covar_dataframe)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8a680e9-62ab-4a0f-a9bc-efc5309da98b"},"outputs":[],"source":"#getting the variances\nvariances = np.diagonal(covariance_matrix)\nprint('Variances:')\nprint('Constant: {} X1: {} X2: {} X3: {} X4: {} X5: {} X6: {}'.format(*variances))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1378d8c-9fd4-5275-742a-0ed2fbf71923"},"source":"Wald test of significance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61f50dd0-1b61-0e7e-27ae-ce575a53263b"},"outputs":[],"source":"#calculating Wald statistics\nwald_stats = [coefs[i]**2 / variances[i] for i in range(len(coefs))]\nprint('Wald statistics:')\nprint('Constant: {} X1: {} X2: {} X3: {} X4: {} X5: {} X6: {}'.format(*wald_stats))\n#calculating p-values\np_values = [1 - stats.chi2.cdf(stat,1) for stat in wald_stats]\nprint('P-value:')\nprint('Constant: {} X1: {} X2: {} X3: {} X4: {} X5: {} X6: {}'.format(*p_values))"},{"cell_type":"markdown","metadata":{"_cell_guid":"cdb6e2a1-d108-b006-8779-dd6bd3faa6e8"},"source":"Basic model coefficients output"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebdc8b7c-42e9-233c-b6da-cf15a768306b"},"outputs":[],"source":"#building the output table\nbase_model_df = pd.DataFrame(index=X_const.columns)\nbase_model_df['Coefficient'] = coefs\nbase_model_df['Variance'] = variances\nbase_model_df['Wald-stat'] = wald_stats\nbase_model_df['P-value'] = p_values\npd.set_option('display.float_format', lambda x: '%.5f' % x)\ndisplay(base_model_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"14a57600-809a-4673-a729-af43fb69ca3f"},"source":"Calculating McFadden's pseudo R-squared"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbd53bd5-57da-3f6b-5040-2e107a0dfe46"},"outputs":[],"source":"#calculating log-likelihood for the basic model and the \"null\" model\nL_1 = -log_loss(y,probs)\nL_0 = -log_loss(y,[0]*len(y))\nprint('L1: {} L0: {}'.format(L_1,L_0))\n#calculating McFadden's R^2\nr2_macfadden = 1 - (L_1/L_0)\nprint('R^2 McFadden: {}'.format(r2_macfadden))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b3b880d-cafd-3dc7-25a2-41ddb49bbc0e"},"source":"Calculating model metrics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51b9799b-41b5-f724-d2bf-f940a024a2e1"},"outputs":[],"source":"#getting the values for the \"ground truth/predicted\" table\n#cross-validating over 10 folds\ntotal_TP = 0\ntotal_FP = 0\ntotal_TN = 0\ntotal_FN = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f_score = []\nkf = KFold(len(X),n_folds=10)\nfor train_index,test_index in kf:\n    x_train, x_test = X.iloc[train_index],X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    log_regr.fit(x_train,y_train)\n    pred = log_regr.predict(x_test)\n    TP, FP, TN, FN = 0, 0, 0, 0\n    y_test = y_test.values\n    for i in range(len(pred)):\n        if pred[i] == 1 and y_test[i] == 1:\n            TP += 1\n            total_TP += 1\n        if pred[i] == 1 and y_test[i] == 0:\n            FP += 1\n            total_FP += 1\n        if pred[i] == 0 and y_test[i] == 0:\n            TN += 1\n            total_TN += 1\n        if pred[i] == 0 and y_test[i] == 1:\n            FN += 1\n            total_FN += 1\n    #calculating metrics for the iteration\n    accuracy = float(TP + TN)/len(pred)\n    total_accuracy.append(accuracy)\n    precision = float(TP)/(TP + FP)\n    total_precision.append(precision)\n    recall = float(TP)/(TP + FN)\n    total_recall.append(recall)\n    f_score = 2*precision*recall/(precision+recall)\n    total_f_score.append(f_score)\n#displayig the \"true/predicted\" table\ntrue_pred_table = pd.DataFrame(index=['M','B','Total'],columns=['M','B','Total'])\ntrue_pred_table.columns.name = 'True\\Pred'\ntrue_pred_table['M']['M'] = total_TP\ntrue_pred_table['M']['B'] = total_FP\ntrue_pred_table['B']['M'] = total_FN\ntrue_pred_table['B']['B'] = total_TN\ntrue_pred_table['M']['Total'] = total_TP + total_FP\ntrue_pred_table['B']['Total'] = total_FN + total_TN\ntrue_pred_table['Total']['M'] = total_TP + total_FN\ntrue_pred_table['Total']['B'] = total_FP + total_TN\ntrue_pred_table['Total']['Total'] = total_TP + total_FP + total_FN + total_TN\ndisplay(true_pred_table)\n#displaying metrics\nmetric_df = pd.DataFrame(index = ['Accuracy','Precision','Recall','F Score'],columns=['Value'])\nmetric_df.columns.name = 'Quality metric'\nmetric_df['Value'] = [np.mean(total_accuracy),np.mean(total_precision),np.mean(total_recall),np.mean(total_f_score)]\ndisplay(metric_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bfcbeb5f-ab64-5a40-8a69-7ff46a10626c"},"source":"<h2>Testing for errors/variable dependencies</h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f79960d-3d80-6ebe-333a-9fec3e7323fb"},"source":"Ramsey's RESET adequacy test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6484717-d3e0-eb5d-8168-2ac208a81ca7"},"outputs":[],"source":"#building the variable matrix with new test variables\nX_reset = X.copy()\nX_reset['probs_squared'] = probs**2\nX_reset['probs_cubed'] = probs**3\n#Fitting the model and calculating variances\nlog_regr.fit(X_reset,y)\ncoefs_reset = [log_regr.coef_[0,6],log_regr.coef_[0,7]]\nprobs_reset = log_regr.predict_proba(X_reset)[:,1]\nX_const_reset = pd.DataFrame()\nX_const_reset['Constant_term'] = [1]*len(X)\nX_const_reset = pd.concat([X_const_reset,X_reset],axis=1)\nV_reset = np.diag([x*(1-x) for x in probs_reset])\ncovariance_matrix_reset = np.linalg.inv(np.transpose(X_const_reset).dot(V_reset).dot(X_const_reset))\ncovar_dataframe_reset = pd.DataFrame(data=covariance_matrix_reset,index=X_const_reset.columns,columns=X_const_reset.columns)\nprint('Covariance matrix:')\ndisplay(covar_dataframe_reset)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f13db82-eda5-8b1b-357b-225179206746"},"outputs":[],"source":"#calculating Wald statistics\nvariances_reset = np.diagonal(a=covariance_matrix_reset)[7:]\nwald_stats_reset = [coefs_reset[i]**2/variances_reset[i] for i in range(len(coefs_reset))]\np_values_reset = [1 - stats.chi2.cdf(stat,1) for stat in wald_stats_reset]\n#the final output\nreset_model_df = pd.DataFrame(index=['probs_squared','probs_cubed'])\nreset_model_df['Coefficient'] = coefs_reset\nreset_model_df['Variance'] = variances_reset\nreset_model_df['Wald-stat'] = wald_stats_reset\nreset_model_df['P-value'] = p_values_reset\ndisplay(reset_model_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f067620-3214-2850-e1f7-459450c8f2f8"},"source":"<h2>Model optimization</h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"aa905290-1cef-82ac-3226-279738c8126d"},"source":"Selecting the regularization coefficient"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d595f50a-e155-bb87-67e0-363d1e9ed07f"},"outputs":[],"source":"reg_strength = [1.1**i for i in range(-20,20)]\ncrossval_results = pd.DataFrame(columns = ['Accuracy', 'Precision', 'Recall', 'F Score'], index = reg_strength)\ncrossval_results.columns.name = 'Inverse reg. strength'\n#iterating over coefficient's values\nfor c in reg_strength:\n    log_regr = LogisticRegression(C=c)\n    total_accuracy = []\n    total_precision = []\n    total_recall = []\n    total_f_score = []\n    #cross-validating over 10 folds\n    kf = KFold(len(df),n_folds=10)\n    for train_index,test_index in kf:\n        x_train, x_test = X.loc[train_index],X.loc[test_index]\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\n        log_regr.fit(x_train,y_train)\n        pred = [1 if x>0.5 else 0 for x in log_regr.predict_proba(x_test)[:,1]]\n        TP, FP, TN, FN = 0, 0, 0, 0\n        y_test = y_test.values\n        for i in range(len(pred)):\n            if pred[i] == 1 and y_test[i] == 1:\n                TP += 1\n            if pred[i] == 1 and y_test[i] == 0:\n                FP += 1\n            if pred[i] == 0 and y_test[i] == 0:\n                TN += 1\n            if pred[i] == 0 and y_test[i] == 1:\n                FN += 1\n        #calculating metrics for the iteration\n        accuracy = float(TP + TN)/len(pred)\n        total_accuracy.append(accuracy)\n        precision = float(TP)/(TP + FP)\n        total_precision.append(precision)\n        recall = float(TP)/(TP + FN)\n        total_recall.append(recall)\n        f_score = 2*precision*recall/(precision+recall)\n        total_f_score.append(f_score)\n    crossval_results['Accuracy'][c]=np.mean(total_accuracy)\n    crossval_results['Precision'][c]=np.mean(total_precision)\n    crossval_results['Recall'][c]=np.mean(total_recall)\n    crossval_results['F Score'][c]=np.mean(total_f_score)\ndisplay(crossval_results)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"444b9b16-4314-de32-1e70-4ce5e7d16c0e"},"outputs":[],"source":"#displaying the best iteration\nbest_result = crossval_results.loc[crossval_results['Recall'].idxmax()]\nprint(best_result)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7164cfba-164e-2389-36e3-7939eb305dbb"},"outputs":[],"source":"#fitting the regularized model\nlog_regr = LogisticRegression(C=best_result.name)\nlog_regr.fit(X,y)\ncoefs = [log_regr.intercept_[0],log_regr.coef_[0,0],log_regr.coef_[0,1],log_regr.coef_[0,2],log_regr.coef_[0,3],log_regr.coef_[0,4],log_regr.coef_[0,5]]\nprobs_reg = log_regr.predict_proba(X)[:,1]\nX_const = pd.DataFrame()\nX_const['Constant_term'] = [1]*len(X)\nX_const = pd.concat([X_const,X],axis=1)\nV = np.diag([x*(1-x) for x in probs_reg])\ncovariance_matrix = np.linalg.inv(np.transpose(X_const).dot(V).dot(X_const))\nvariances = np.diagonal(covariance_matrix)\nwald_stats = [coefs[i]**2 / variances[i] for i in range(len(coefs))]\np_values = [1 - stats.chi2.cdf(stat,1) for stat in wald_stats]\n\n#displaying the regularized model's coefficients\nreg_model_df = pd.DataFrame(index=X_const.columns)\nreg_model_df['Coefficient'] = coefs\nreg_model_df['Variance'] = variances\nreg_model_df['Wald-stat'] = wald_stats\nreg_model_df['P-value'] = p_values\ndisplay(reg_model_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"19799b05-6841-016e-ca22-9ebbcea4b561"},"source":"Deleting the insignificant variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35e982a6-25f7-815f-b0f7-8500ab85dee2"},"outputs":[],"source":"#building the new variable matrix without insignificant variables\nX = X[['radius_mean','texture_mean','log_concavity']]\n#building the new model\nlog_regr.fit(X,y)\ncoefs = [log_regr.intercept_[0],log_regr.coef_[0,0],log_regr.coef_[0,1],log_regr.coef_[0,2]]\nprobs = log_regr.predict_proba(X)[:,1]\nX_const = pd.DataFrame()\nX_const['Constant_term'] = [1]*len(X)\nX_const = pd.concat([X_const,X],axis=1)\nV = np.diag([x*(1-x) for x in probs])\ncovariance_matrix = np.linalg.inv(np.transpose(X_const).dot(V).dot(X_const))\nvariances = np.diagonal(covariance_matrix)\nwald_stats = [coefs[i]**2 / variances[i] for i in range(len(coefs))]\np_values = [1 - stats.chi2.cdf(stat,1) for stat in wald_stats]\n\n#displaying the coefficients\nreg_model_df = pd.DataFrame(index=X_const.columns)\nreg_model_df['Coefficient'] = coefs\nreg_model_df['Variance'] = variances\nreg_model_df['Wald-stat'] = wald_stats\nreg_model_df['P-value'] = p_values\ndisplay(reg_model_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c30ecb8-2fc2-4fdd-67d1-2102a3210a06"},"source":"Minimizing Type-II errors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34f41fd6-cdba-c34b-19fc-ca8b04b74d24"},"outputs":[],"source":"#displaying the new model's metrics\n#getting the values for the \"ground truth/predicted\" table\n#cross-validating over 10 folds\ntotal_TP = 0\ntotal_FP = 0\ntotal_TN = 0\ntotal_FN = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f_score = []\nkf = KFold(len(X),n_folds=10)\nfor train_index,test_index in kf:\n    x_train, x_test = X.iloc[train_index],X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    log_regr.fit(x_train,y_train)\n    pred = log_regr.predict(x_test)\n    TP, FP, TN, FN = 0, 0, 0, 0\n    y_test = y_test.values\n    for i in range(len(pred)):\n        if pred[i] == 1 and y_test[i] == 1:\n            TP += 1\n            total_TP += 1\n        if pred[i] == 1 and y_test[i] == 0:\n            FP += 1\n            total_FP += 1\n        if pred[i] == 0 and y_test[i] == 0:\n            TN += 1\n            total_TN += 1\n        if pred[i] == 0 and y_test[i] == 1:\n            FN += 1\n            total_FN += 1\n    #calculating metrics for the iteration\n    accuracy = float(TP + TN)/len(pred)\n    total_accuracy.append(accuracy)\n    precision = float(TP)/(TP + FP)\n    total_precision.append(precision)\n    recall = float(TP)/(TP + FN)\n    total_recall.append(recall)\n    f_score = 2*precision*recall/(precision+recall)\n    total_f_score.append(f_score)\n#displayig the \"true/predicted\" table\ntrue_pred_table = pd.DataFrame(index=['M','B','Total'],columns=['M','B','Total'])\ntrue_pred_table.columns.name = 'True\\Pred'\ntrue_pred_table['M']['M'] = total_TP\ntrue_pred_table['M']['B'] = total_FP\ntrue_pred_table['B']['M'] = total_FN\ntrue_pred_table['B']['B'] = total_TN\ntrue_pred_table['M']['Total'] = total_TP + total_FP\ntrue_pred_table['B']['Total'] = total_FN + total_TN\ntrue_pred_table['Total']['M'] = total_TP + total_FN\ntrue_pred_table['Total']['B'] = total_FP + total_TN\ntrue_pred_table['Total']['Total'] = total_TP + total_FP + total_FN + total_TN\ndisplay(true_pred_table)\n#displaying metrics\nmetric_df = pd.DataFrame(index = ['Accuracy','Precision','Recall','F Score'],columns=['Value'])\nmetric_df.columns.name = 'Quality metric'\nmetric_df['Value'] = [np.mean(total_accuracy),np.mean(total_precision),np.mean(total_recall),np.mean(total_f_score)]\ndisplay(metric_df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2518187-2d4c-9c0d-7a57-b207bc59eebf"},"outputs":[],"source":"#cross-validation for an optimal classification threshold\nclassification_thresholds = np.arange(0.05,0.5,0.05)\ncrossval_results = pd.DataFrame(columns = ['Accuracy', 'Precision', 'Recall', 'F Score'], index = classification_thresholds)\ncrossval_results.columns.name = 'Threshold'\n#iterating over threshold values\nlog_regr = LogisticRegression(C=best_result.name)\nfor threshold in classification_thresholds:\n    total_accuracy = []\n    total_precision = []\n    total_recall = []\n    total_f_score = []\n    #cross-validating over 10 folds\n    kf = KFold(len(df),n_folds=10)\n    for train_index,test_index in kf:\n        x_train, x_test = X.loc[train_index],X.loc[test_index]\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\n        log_regr.fit(x_train,y_train)\n        pred = [1 if x>threshold else 0 for x in log_regr.predict_proba(x_test)[:,1]]\n        TP, FP, TN, FN = 0, 0, 0, 0\n        y_test = y_test.values\n        for i in range(len(pred)):\n            if pred[i] == 1 and y_test[i] == 1:\n                TP += 1\n            if pred[i] == 1 and y_test[i] == 0:\n                FP += 1\n            if pred[i] == 0 and y_test[i] == 0:\n                TN += 1\n            if pred[i] == 0 and y_test[i] == 1:\n                FN += 1\n        #calculating metrics for the iteration\n        accuracy = float(TP + TN)/len(pred)\n        total_accuracy.append(accuracy)\n        precision = float(TP)/(TP + FP)\n        total_precision.append(precision)\n        recall = float(TP)/(TP + FN)\n        total_recall.append(recall)\n        f_score = 2*precision*recall/(precision+recall)\n        total_f_score.append(f_score)\n    crossval_results['Accuracy'][threshold]=np.mean(total_accuracy)\n    crossval_results['Precision'][threshold]=np.mean(total_precision)\n    crossval_results['Recall'][threshold]=np.mean(total_recall)\n    crossval_results['F Score'][threshold]=np.mean(total_f_score)\ndisplay(crossval_results)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c19f3a6-7acd-97c1-8c00-db139c1113bc"},"source":"<h2>Comparing optimized and basic models</h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"c970a6c1-589d-479b-573b-2b0501ebaa8b"},"source":"Calculating the new McFadden's R2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61e299d3-5002-1718-09d3-183783c96abf"},"outputs":[],"source":"#calculating log-likelihood\nprobs_optimized = log_regr.predict_proba(X)[:,1]\nL_1 = -log_loss(y,probs_optimized)\nL_0 = -log_loss(y,[0]*len(y))\nprint('L1: {} L0: {}'.format(L_1,L_0))\n#calculating McFadden's R2\nr2_macfadden = 1 - (L_1/L_0)\nprint('R^2 McFadden: {}'.format(r2_macfadden))"},{"cell_type":"markdown","metadata":{"_cell_guid":"62f14fdf-d11f-e72e-f6dc-1cbe78cbfedd"},"source":"Calculating AIC for optimized and basic models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cac08a5a-e5d6-d782-b2f3-6a983275fe8a"},"outputs":[],"source":"#calculating log-likelihood\nL_basic = -log_loss(y,probs)\nL_optimized = -log_loss(y,probs_optimized)\n#Calculating AICs\naic_basic = 16 - 2*L_basic\naic_optimized = 10 - 2*L_optimized\nprint('Basic: {} Optimized: {}'.format(aic_basic,aic_optimized))\ninformation_loss_proba = math.exp((aic_optimized - aic_basic)/2)\nprint('Probability that basic model minimizes information loss: {}'.format(information_loss_proba))"},{"cell_type":"markdown","metadata":{"_cell_guid":"23fc5411-4d9a-2186-2553-59ae67e7e7e8"},"source":"Calculating BIC for optimized and basic models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05cd6b38-3d4e-bcb7-d5c1-134c8aeeb3a2"},"outputs":[],"source":"bic_basic = 8*math.log(len(df)) - 2*L_basic\nbic_optimized = 5*math.log(len(df)) - 2*L_optimized\nprint('Basic: {} Optimized: {}'.format(bic_basic,bic_optimized))"},{"cell_type":"markdown","metadata":{"_cell_guid":"29184a4f-ff65-497b-4922-7618c5aafb4c"},"source":"<h1>Confidence intervals and outlier tests</h1>"},{"cell_type":"markdown","metadata":{"_cell_guid":"e408d79b-d855-141d-6c58-eccd49671edd"},"source":"Confidence intervals"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"adc4b8db-3d9b-7793-440a-dba1bfd4e39a"},"outputs":[],"source":"#calculating confidence intervals for odds ratio\nCI_df = reg_model_df[['Coefficient','Variance']].loc[['radius_mean','texture_mean','log_concavity']]\nCI_df['Odds Ratio'] = CI_df.apply(lambda x: math.exp(x['Coefficient']),axis=1)\nCI_df['Lower CL'] = CI_df.apply(lambda x: x['Odds Ratio']+stats.norm.interval(0.95)[0]*math.sqrt(x['Variance'])*x['Odds Ratio'],axis=1)\nCI_df['Upper CL'] = CI_df.apply(lambda x: x['Odds Ratio']+stats.norm.interval(0.95)[1]*math.sqrt(x['Variance'])*x['Odds Ratio'],axis=1)\ndisplay(CI_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"73acc487-a35f-41f9-b604-b88d8eb43984"},"source":"Calculating residuals"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a96c32d8-55e0-40fe-9bad-1a9e63397813"},"outputs":[],"source":"residuals_df = pd.DataFrame()\n#calculating absolute deviation\nresiduals_df['Y'] = df['diagnosis']\nresiduals_df['Probability'] = probs_optimized\nresiduals_df['Error'] = residuals_df['Y'] - residuals_df['Probability']\n#calculating Pearson-normalized residuals\nresiduals_df['Pearson residual'] = residuals_df['Error']/(residuals_df['Probability']*(1 - residuals_df['Probability']))\n#calculating studentized Pearson-normalized residuals\n#calculating the projection matrix to get Pregibon leverages\nW = np.diag([prob*(1-prob) for prob in probs_optimized])\nH = (W**(1/2)).dot(X).dot(np.linalg.matrix_power(np.transpose(X).dot(W).dot(X),-1)).dot(np.transpose(X)).dot(W**(1/2))\nh = np.diagonal(H)\nresiduals_df['SPR'] = [residuals_df['Pearson residual'].iloc[i]/math.sqrt(1-h[i]) for i in range(len(df))]\n#calculating deviance residual\nresiduals_df['Deviance residual'] = [np.sign(residuals_df['Y'].iloc[i] - residuals_df['Probability'].iloc[i])*math.sqrt(-2*(residuals_df['Y'].iloc[i]*math.log(residuals_df['Probability'].iloc[i])+(1-residuals_df['Y'].iloc[i])*math.log(1-residuals_df['Probability'].iloc[i]))) for i in range(len(df))]\n#calculating Delta Chi2\nresiduals_df['Delta Chi2'] = residuals_df['SPR']**2\n#calculating deltas of deviation\nresiduals_df['Delta D'] = [(residuals_df['Deviance residual'].iloc[i])**2/(1-h[i]) for i in range(len(df))]\n#calculating deltas of regression coefficients\nresiduals_df['Delta Coefficients'] = [h[i]*(residuals_df['SPR'].iloc[i])**2/(1-h[i]) for i in range(len(df))]\n#displaying residual plots\nf,ax = plt.subplots(2,3,figsize=(15,10))\nplt.tight_layout()\nax[0,0].scatter(x=residuals_df['Probability'],y=residuals_df['Pearson residual'])\nax[0,0].set_title('PR')\nax[0,1].scatter(x=residuals_df['Probability'],y=residuals_df['SPR'])\nax[0,1].set_title('SPR')\nax[0,2].scatter(x=residuals_df['Probability'],y=residuals_df['Deviance residual'])\nax[0,2].set_title('DR')\nax[1,0].scatter(x=residuals_df['Probability'],y=residuals_df['Delta Chi2'])\nax[1,0].set_title('dChi2')\nax[1,1].scatter(x=residuals_df['Probability'],y=residuals_df['Delta D'])\nax[1,1].set_title('dD')\nax[1,2].scatter(x=residuals_df['Probability'],y=residuals_df['Delta Coefficients'])\nax[1,2].set_title('dCoef')\nplt.show()\n#displaying the table\ndisplay(residuals_df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d931488-9f7b-812e-6af6-eb1cd7585694"},"source":"Outlier Detection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db2e31a0-0221-b6fa-87bb-786cfeb93e31"},"outputs":[],"source":"#finding objects where Delta Chi2 and Delta D lie outside the 95-th percentile of Chi2 distribution with 1 df\n#and Delta Coefficients is more than 0.1\nchisq_thresh = stats.chi2.ppf(0.95,1)\noutliers1 = residuals_df.loc[(residuals_df['Delta Chi2'] > chisq_thresh) | (residuals_df['Delta D'] > chisq_thresh)]\ndisplay(outliers1)\nprint('Number of outliers by the Chi2 Test: {}'.format(len(outliers1)))\noutliers2 = outliers1[outliers1['Delta Coefficients'] > 0.1]\ndisplay(outliers2)\nprint('Number of outliers with big coefficient influence: {}'.format(len(outliers2)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46e01b63-7b0c-b9c9-4a95-f22fc423b8d1"},"outputs":[],"source":"#displaying the outliers\ndisplay(df[df.index.isin(outliers2.index.values)])"},{"cell_type":"markdown","metadata":{"_cell_guid":"86972242-15f7-359b-d820-76c72538826f"},"source":"Building the model without outliers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6537788d-8402-5c40-57ee-c9b13caa39e6"},"outputs":[],"source":"X = X[X.index.isin(outliers2.index.values) == False]\ny = y[y.index.isin(outliers2.index.values) == False]\nlog_regr.fit(X,y)\n#cross-validating over 10 folds\ntotal_TP = 0\ntotal_FP = 0\ntotal_TN = 0\ntotal_FN = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f_score = []\nkf = KFold(len(X),n_folds=10)\nfor train_index,test_index in kf:\n    x_train, x_test = X.iloc[train_index],X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    log_regr.fit(x_train,y_train)\n    pred = [1 if x>0.35 else 0 for x in log_regr.predict_proba(x_test)[:,1]]\n    TP, FP, TN, FN = 0, 0, 0, 0\n    y_test = y_test.values\n    for i in range(len(pred)):\n        if pred[i] == 1 and y_test[i] == 1:\n            TP += 1\n            total_TP += 1\n        if pred[i] == 1 and y_test[i] == 0:\n            FP += 1\n            total_FP += 1\n        if pred[i] == 0 and y_test[i] == 0:\n            TN += 1\n            total_TN += 1\n        if pred[i] == 0 and y_test[i] == 1:\n            FN += 1\n            total_FN += 1\n    #calculating metrics for the iteration\n    accuracy = float(TP + TN)/len(pred)\n    total_accuracy.append(accuracy)\n    precision = float(TP)/(TP + FP)\n    total_precision.append(precision)\n    recall = float(TP)/(TP + FN)\n    total_recall.append(recall)\n    f_score = 2*precision*recall/(precision+recall)\n    total_f_score.append(f_score)\n#displayig the \"true/predicted\" table\ntrue_pred_table = pd.DataFrame(index=['M','B','Total'],columns=['M','B','Total'])\ntrue_pred_table.columns.name = 'True\\Pred'\ntrue_pred_table['M']['M'] = total_TP\ntrue_pred_table['M']['B'] = total_FP\ntrue_pred_table['B']['M'] = total_FN\ntrue_pred_table['B']['B'] = total_TN\ntrue_pred_table['M']['Total'] = total_TP + total_FP\ntrue_pred_table['B']['Total'] = total_FN + total_TN\ntrue_pred_table['Total']['M'] = total_TP + total_FN\ntrue_pred_table['Total']['B'] = total_FP + total_TN\ntrue_pred_table['Total']['Total'] = total_TP + total_FP + total_FN + total_TN\ndisplay(true_pred_table)\n#displaying metrics\nmetric_df = pd.DataFrame(index = ['Accuracy','Precision','Recall','F Score'],columns=['Value'])\nmetric_df.columns.name = 'Quality metric'\nmetric_df['Value'] = [np.mean(total_accuracy),np.mean(total_precision),np.mean(total_recall),np.mean(total_f_score)]\ndisplay(metric_df)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}