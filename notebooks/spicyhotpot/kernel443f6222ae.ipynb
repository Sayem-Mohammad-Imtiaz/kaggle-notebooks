{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_hotel = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')\ndata_hotel.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_hotel.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_hotel.drop(['agent', 'company', 'arrival_date_week_number', 'arrival_date_day_of_month', \n                 'days_in_waiting_list', 'reservation_status_date', 'reservation_status_date', \n                 'reservation_status', ], inplace=True, axis=1)\ndata_hotel.dropna(subset=['country', 'children'], inplace=True)\ndata_hotel.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_month_map = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8,\n                   'September': 9, 'October': 10, 'November': 11, 'December': 12}\ndata_hotel['arrival_date_month'] = data_hotel['arrival_date_month'].map(class_month_map)\n\nclass_meal_map = {'BB': 1, 'HB': 2, 'FB': 3, 'SC': 0, 'Undefined': 0}\ndata_hotel['meal'] = data_hotel['meal'].map(class_meal_map)\n\ndata_hotel['adr'] = pd.cut(data_hotel['adr'], bins=3, labels=['low', 'median', 'high'])\ndummy = pd.get_dummies(data_hotel[['hotel', 'distribution_channel', 'reserved_room_type',\n                                  'assigned_room_type', 'deposit_type', 'customer_type', 'adr', 'country', 'market_segment']])\ndata_hotel = pd.concat([data_hotel, dummy],axis=1)\ndata_hotel.drop(['hotel', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type',\n                 'customer_type', 'adr', 'country', 'market_segment'], inplace=True, axis=1)\n\nhotel_labels = data_hotel['is_canceled']\nhotel_features = data_hotel.drop(['is_canceled'], axis=1)\nX_train, X_test, Y_train, Y_test = train_test_split(hotel_features.values, hotel_labels.values, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Find the best parameters\n\n# max_depth = range(2, 7)\n# min_samples_split = range(2, 9, 2)\n# min_samples_leaf = range(2, 11, 2)\n# tree_parameters = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n# grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=tree_parameters, cv=10)\n# grid_search.fit(X_train, Y_train)\n# grid_search.best_params_\n\ndecision_tree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=6, min_samples_split=2)\ndecision_tree.fit(X_train, Y_train)\ndecision_tree_pre = decision_tree.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, decision_tree_pre))\nprint('test-REC ', recall_score(Y_test, decision_tree_pre))\nprint('test-F1: ', f1_score(Y_test, decision_tree_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_score = decision_tree.predict_proba(X_test)[:, 1]\ntree_fpr, tree_tpr, tree_threshold = roc_curve(Y_test, decision_tree_score)\ndecision_tree_auc = auc(tree_fpr, tree_tpr)\nplt.title('DecisionTree')\nplt.stackplot(tree_fpr, tree_tpr, color='red', alpha=0.3)\nplt.plot(tree_fpr, tree_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(decision_tree_auc, accuracy_score(Y_test, decision_tree_pre), \n                 recall_score(Y_test, decision_tree_pre), f1_score(Y_test, decision_tree_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=150)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest_pre = random_forest.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, random_forest_pre))\nprint('test-REC ', recall_score(Y_test, random_forest_pre))\nprint('test-F1: ', f1_score(Y_test, random_forest_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_score = random_forest.predict_proba(X_test)[:, 1]\nrf_fpr, rf_tpr, rf_threshold = roc_curve(Y_test, random_forest_score)\nrandom_forest_auc = auc(rf_fpr, rf_tpr)\nplt.title('RandomForest')\nplt.stackplot(rf_fpr, rf_tpr, color='red', alpha=0.3)\nplt.plot(rf_fpr, rf_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(random_forest_auc, accuracy_score(Y_test, random_forest_pre), \n                 recall_score(Y_test, random_forest_pre), f1_score(Y_test, random_forest_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Find the best parameters\n\n# k = np.arange(10, 21)\n# accuracy_test = []\n# for i in k:\n#     cv_result = cross_val_score(KNeighborsClassifier(n_neighbors=i, weights='distance'), X_train, Y_train, cv=6, scoring='accuracy')\n#     accuracy_test.append(cv_result.mean())\n# arg_max = np.array(accuracy_test).argmax()\n# plt.plot(k, accuracy_test, marker='o')\n# plt.text(k[arg_max], accuracy_test[arg_max], 'The best k is {}'.format(k[arg_max]))\n# plt.show()\n\nknn_clf = KNeighborsClassifier(n_neighbors=18, weights='distance')\nknn_clf.fit(X_train, Y_train)\nknn_clf_pre = knn_clf.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, knn_clf_pre))\nprint('test-REC ', recall_score(Y_test, knn_clf_pre))\nprint('test-F1: ', f1_score(Y_test, knn_clf_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf_score = knn_clf.predict_proba(X_test)[:, 1]\nknn_fpr, knn_tpr, knn_threshold = roc_curve(Y_test, knn_clf_score)\nknn_auc = auc(knn_fpr, knn_tpr)\nplt.title('KNN')\nplt.stackplot(knn_fpr, knn_tpr, color='red', alpha=0.3)\nplt.plot(knn_fpr, knn_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(knn_auc, accuracy_score(Y_test, knn_clf_pre), \n                 recall_score(Y_test, knn_clf_pre), f1_score(Y_test, knn_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive_Bayes\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\nmnb_clf = MultinomialNB()\nmnb_clf.fit(X_train, Y_train)\nmnb_clf_pre = mnb_clf.predict(X_test)\nprint('mnb-test-ACC: ', accuracy_score(Y_test, mnb_clf_pre))\nprint('mnb-test-REC ', recall_score(Y_test, mnb_clf_pre))\nprint('mnb-test-F1: ', f1_score(Y_test, mnb_clf_pre))\n\nprint('--------------------------------------------------')\n\nbnb_clf = BernoulliNB()\nbnb_clf.fit(X_train, Y_train)\nbnb_clf_pre = bnb_clf.predict(X_test)\nprint('bnb-test-ACC: ', accuracy_score(Y_test, bnb_clf_pre))\nprint('bnb-test-REC ', recall_score(Y_test, bnb_clf_pre))\nprint('bnb-test-F1: ', f1_score(Y_test, bnb_clf_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_clf_score = mnb_clf.predict_proba(X_test)[:, 1]\nmnb_fpr, mnb_tpr, mnb_threshold = roc_curve(Y_test, mnb_clf_score)\nmnb_auc = auc(mnb_fpr, mnb_tpr)\nplt.figure('MultinomiaNB')\nplt.title('MultinomiaNB')\nplt.stackplot(mnb_fpr, mnb_tpr, color='red', alpha=0.3)\nplt.plot(mnb_fpr, mnb_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(mnb_auc, accuracy_score(Y_test, mnb_clf_pre), \n                 recall_score(Y_test, mnb_clf_pre), f1_score(Y_test, mnb_clf_pre)))\n\nbnb_clf_score = bnb_clf.predict_proba(X_test)[:, 1]\nbnb_fpr, bnb_tpr, bnb_threshold = roc_curve(Y_test, bnb_clf_score)\nbnb_auc = auc(bnb_fpr, bnb_tpr)\nplt.figure('BernoulliNB')\nplt.title('BernoulliNB')\nplt.stackplot(bnb_fpr, bnb_tpr, color='red', alpha=0.3)\nplt.plot(bnb_fpr, bnb_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(bnb_auc, accuracy_score(Y_test, bnb_clf_pre), \n                 recall_score(Y_test, bnb_clf_pre), f1_score(Y_test, bnb_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\n\n# Find the best parameters\n\n# kernel_para=['sigmoid', 'poly', 'linear', 'rbf']\n# c_para = [5000, 10000, 15000, 20000, 25000]\n# svm_parameters = {'C': c_para, 'kernel': kernel_para}\n# grid_SVC = GridSearchCV(estimator=SVC(), param_grid=svm_parameters, scoring='accuracy', cv=6)\n# grid_SVC.fit(X_train, Y_train)\n# grid_SVC.best_params_, grid_SVC.best_score_\n\nsvc_clf = SVC(C=10000, kernel='rbf', probability=True)\nsvc_clf.fit(X_train, Y_train)\nsvc_clf_pre = svc_clf.predict(X_test)\nsvc_clf_train_pre = svc_clf.predict(X_train)\nprint('test-ACC: ', accuracy_score(Y_test, svc_clf_pre))\nprint('test-REC ', recall_score(Y_test, svc_clf_pre))\nprint('test-F1: ', f1_score(Y_test, svc_clf_pre))\nprint('-----------------------------------------------------')\nprint('train-ACC: ', accuracy_score(Y_train, svc_clf_train_pre))\nprint('train-REC ', recall_score(Y_train, svc_clf_train_pre))\nprint('train-F1: ', f1_score(Y_train, svc_clf_train_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"svc_clf_score = svc_clf.predict_proba(X_test)[:, 1]\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(Y_test, svc_clf_score)\nsvc_auc = auc(svc_fpr, svc_tpr)\nplt.title('SVC')\nplt.stackplot(svc_fpr, svc_tpr, color='red', alpha=0.3)\nplt.plot(svc_fpr, svc_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(svc_auc, accuracy_score(Y_test, svc_clf_pre), \n                 recall_score(Y_test, svc_clf_pre), f1_score(Y_test, svc_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adaboost\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Find the best parameters\n\n# max_depth = range(2, 11)\n# min_samples_split = range(2, 9, 2)\n# min_samples_leaf = range(2, 9, 2)\n# ada_tree_parameters = {'base_estimator__max_depth': max_depth, \n#                        'base_estimator__min_samples_split': min_samples_split,\n#                        'base_estimator__min_samples_leaf': min_samples_leaf}\n# ada_grid_search = GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), \n#                                param_grid=ada_tree_parameters, cv=5, scoring='roc_auc')\n# ada_grid_search.fit(X_train, Y_train)\n# ada_grid_search.best_params_\n\n# n_estimators = range(100, 1600, 100)\n# learning_rate = [0.01, 0.05, 0.1, 0.15, 0.2]\n# ada_tree_params2 = {\n#     'n_estimators':n_estimators, 'learning_rate':learning_rate\n# }\n# ada_grid_search2 = GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5,min_samples_leaf=6,min_samples_split=2)),\n#                                  param_grid=gbdt_tree_params2,\n#                                  scoring='roc_auc',\n#                                  cv=5)\n# ada_grid_search2.fit(X_train, Y_train)\n# ada_grid_search2.best_params_\n\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5, min_samples_leaf=6, min_samples_split=2), n_estimators=1000)\nada_clf.fit(X_train, Y_train)\nada_clf_pre = ada_clf.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, ada_clf_pre))\nprint('test-REC ', recall_score(Y_test, ada_clf_pre))\nprint('test-F1: ', f1_score(Y_test, ada_clf_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_clf_score = ada_clf.predict_proba(X_test)[:, 1]\nada_fpr, ada_tpr, ada_threshold = roc_curve(Y_test, ada_clf_score)\nada_auc = auc(ada_fpr, ada_tpr)\nplt.title('Adaboost')\nplt.stackplot(ada_fpr, ada_tpr, color='red', alpha=0.3)\nplt.plot(ada_fpr, ada_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(ada_auc, accuracy_score(Y_test, ada_clf_pre), \n                 recall_score(Y_test, ada_clf_pre), f1_score(Y_test, ada_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBDT\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Find the best parameters\n\n# max_depth = range(2, 11)\n# min_samples_split = range(2, 9, 2)\n# min_samples_leaf = range(2, 9, 2)\n# n_estimators = range(100, 1600, 100)\n# learning_rate = [0.01, 0.05, 0.1, 0.15, 0.2]\n# gbdt_tree_parameters = {'max_depth': max_depth, \n#                        'min_samples_split': min_samples_split,\n#                        'min_samples_leaf': min_samples_leaf,\n#                        'n_estimators':n_estimators, \n#                         'learning_rate':learning_rate}\n# gbdt_grid_search = GridSearchCV(estimator=GradientBoostingClassifier(), \n#                                param_grid=gbdt_tree_parameters, cv=5, scoring='roc_auc')\n# gbdt_grid_search.fit(X_train, Y_train)\n# gbdt_grid_search.best_params_\n\ngbdt_clf = GradientBoostingClassifier(max_depth=5, min_samples_split=2, min_samples_leaf=6, learning_rate=0.05, n_estimators=1000)\ngbdt_clf.fit(X_train, Y_train)\ngbdt_clf_pre = gbdt_clf.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, gbdt_clf_pre))\nprint('test-REC ', recall_score(Y_test, gbdt_clf_pre))\nprint('test-F1: ', f1_score(Y_test, gbdt_clf_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbdt_clf_score = gbdt_clf.predict_proba(X_test)[:, 1]\ngbdt_fpr, gbdt_tpr, gbdt_threshold = roc_curve(Y_test, gbdt_clf_score)\ngbdt_auc = auc(gbdt_fpr, gbdt_tpr)\nplt.title('GBDT')\nplt.stackplot(gbdt_fpr, gbdt_tpr, color='red', alpha=0.3)\nplt.plot(gbdt_fpr, gbdt_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(gbdt_auc, accuracy_score(Y_test, gbdt_clf_pre), \n                 recall_score(Y_test, gbdt_clf_pre), f1_score(Y_test, gbdt_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost\nimport xgboost\n\n#Find the best parameters\n\n# max_depth = range(2, 11)\n# learning_rate = [0.01, 0.05, 0.1, 0.15, 0.2]\n# n_estimators = range(100, 1600, 100)\n# xgboost_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'n_estimators':n_estimators}\n# xgboost_grid = GridSearchCV(estimator=xgboost.XGBClassifier(), param_grid=xgboost_params, cv=5, scoring='roc_auc')\n# xgboost_grid.fit(X_train, Y_train)\n# xgboost_grid.best_params_\n\nxgboost_clf = xgboost.XGBClassifier(max_depth=10, learning_rate=0.05, n_estimators=1000)\nxgboost_clf.fit(X_train, Y_train)\nxgboost_clf_pre = xgboost_clf.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, xgboost_clf_pre))\nprint('test-REC ', recall_score(Y_test, xgboost_clf_pre))\nprint('test-F1: ', f1_score(Y_test, xgboost_clf_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost_clf_score = xgboost_clf.predict_proba(X_test)[:, 1]\nxgboost_fpr, xgboost_tpr, xgboost_threshold = roc_curve(Y_test, xgboost_clf_score)\nxgboost_auc = auc(xgboost_fpr, xgboost_tpr)\nplt.title('XGBoost')\nplt.stackplot(xgboost_fpr, xgboost_tpr, color='red', alpha=0.3)\nplt.plot(xgboost_fpr, xgboost_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(xgboost_auc, accuracy_score(Y_test, xgboost_clf_pre), \n                 recall_score(Y_test, xgboost_clf_pre), f1_score(Y_test, xgboost_clf_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#ANNs(Artificial Neural Networks)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import SGD\n\n\nmdl = Sequential()\nmdl.add(Dense(500, input_dim=len(X_train[0])))\nmdl.add(Activation('sigmoid'))\n# mdl.add(Dense(500))\n# mdl.add(Activation('sigmoid'))\nmdl.add(Dense(2))\nmdl.add(Activation('softmax'))\nsgd = SGD(lr=0.01)\nmdl.compile(loss='mean_squared_error', optimizer='sgd')\nmdl.fit(X_train, Y_train, epochs=100, batch_size=10000)\nmdl_pre = mdl.predict_classes(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, mdl_pre))\nprint('test-REC ', recall_score(Y_test, mdl_pre))\nprint('test-F1: ', f1_score(Y_test, mdl_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LR\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, Y_train)\nlr_pre = lr.predict(X_test)\nprint('test-ACC: ', accuracy_score(Y_test, lr_pre))\nprint('test-REC ', recall_score(Y_test, lr_pre))\nprint('test-F1: ', f1_score(Y_test, lr_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_score = lr.predict_proba(X_test)[:, 1]\nlr_fpr, lr_tpr, lr_threshold = roc_curve(Y_test, lr_score)\nlr_auc = auc(lr_fpr, lr_tpr)\nplt.title('LinearRegression')\nplt.stackplot(lr_fpr, lr_tpr, color='red', alpha=0.3)\nplt.plot(lr_fpr, lr_tpr, color='black')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--')\nplt.text(0.5, 0.25, 'AREA={:.2f}\\nACC={:.2f}\\nREC={:.2f}\\nF1={:.2f}'\n         .format(lr_auc, accuracy_score(Y_test, lr_pre), \n                 recall_score(Y_test, lr_pre), f1_score(Y_test, lr_pre)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can find that RandomForest model is the best one to predict the cancelling, \n# therefore we can see the importance of variable, and analyze the data.\n\n# find the importance of variable\nimportance_variable = random_forest.feature_importances_\nimportance_series = pd.Series(importance_variable, index = hotel_features.columns)\nimportance_series.sort_values(ascending=False, inplace=True)\ntop_impt = importance_series.head(10)\nplt.title('Importance_variable')\nsns.barplot(y=top_impt.index, x=top_impt.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analyze the lead_time\n\nplt.figure()\nplt.title('lead_time hist')\nsns.distplot(data_hotel['lead_time'], bins=20) #in this figure,we know that almost time is the period [0, 100]\n\nlead_time_part1 = data_hotel[data_hotel['lead_time'] >= 0][data_hotel['lead_time'] <= 100]\nlead_time_part2 = data_hotel[data_hotel['lead_time'] > 100][data_hotel['lead_time'] <= 200]\nlead_time_part3 = data_hotel[data_hotel['lead_time'] > 200][data_hotel['lead_time'] <= 300]\nlead_time_part4 = data_hotel[data_hotel['lead_time'] > 300]\n\nplt.figure()\nplt.bar(x=['0-100', '101-200', '201-300', '300-'], height=[lead_time_part1.shape[0], \n                                                           lead_time_part2.shape[0],\n                                                           lead_time_part3.shape[0], \n                                                           lead_time_part4.shape[0]], color='red', width=0.5, label='total')\n\nplt.bar(x=['0-100', '101-200', '201-300', '300-'], height=[lead_time_part1[lead_time_part1['is_canceled']==1].shape[0], \n                                                           lead_time_part2[lead_time_part2['is_canceled']==1].shape[0],\n                                                           lead_time_part3[lead_time_part3['is_canceled']==1].shape[0],\n                                                           lead_time_part4[lead_time_part4['is_canceled']==1].shape[0]], \n                                                           color='#0000A3', width=0.5, label='canceled')\nplt.ylabel('the number of samples')\nplt.xlabel('lead_time /d')\nplt.legend()\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analyze \"arrival_date_month\"\n\nplt.figure()\nplt.subplot(121)\nplt.pie(x=hotel_features['arrival_date_month'].value_counts(normalize=True).values,\n       labels=hotel_features['arrival_date_month'].value_counts(normalize=True).index,\n       autopct='%.1f%%')\n\nplt.subplot(122)\nhotel_features['quarters'] = pd.cut(hotel_features['arrival_date_month'], bins=4, labels=['1st', '2nd', '3rd', '4th']) \nplt.pie(x=hotel_features['quarters'].value_counts(normalize=True).values,\n       labels=hotel_features['quarters'].value_counts(normalize=True).index,\n       autopct='%.1f%%')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}