{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport gc\n\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/test.csv\")\n\nprint(f\"Train Data: {train.shape}\")\ndisplay(train.head())\nprint(f\"Test Data: {test.shape}\")\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hypothesis:\n1. Younger people tend to take Vehicle Insurance more than older people because the chance of getting a new vehicle in older age is less and also older people might already have insurance for their vehicles.\n2. Response will be positive for people who haven't insured previously.\n3. People with old vehicles(age) won't take vehicle insurance because of the high premium rates for older vehicles.\n4. If the age of the vehicle is less and if it had a previous damage, then those people will buy the vehicle insurance.\n\n\nThese are some of the hypothesis I want to check. There are more hypothesis which you can come up with and validate the same."},{"metadata":{},"cell_type":"markdown","source":"First let's change the datatype of some of the features to categorical since it is wrongly inferred as integer type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset info\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a copy of the train and test set\ntrainData = train.copy()\ntestData = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the datatypes\n# In order to reduce the repetitive process for test set, will combine both the train and test data and then change the data types\ntrainData[\"type\"] = \"train\"\ntestData[\"type\"] = \"test\"\ncombined = pd.concat([trainData, testData], axis=0)\nprint(combined.shape)\ncombined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing Region_code to category\ncombined[\"Region_Code\"] = combined[\"Region_Code\"].astype(\"category\")\n\n# Changing Policy_Sales_Channel to category\ncombined[\"Policy_Sales_Channel\"] = combined[\"Policy_Sales_Channel\"].astype(\"category\")\n\n# Changing Driving_License to category\ncombined[\"Driving_License\"] = combined[\"Driving_License\"].astype(\"category\")\n\n# Changing Previously_Insured to category\ncombined[\"Previously_Insured\"] = combined[\"Previously_Insured\"].astype(\"category\")\n\n# Changing Vehicle_Damage to category\ncombined[\"Vehicle_Damage\"] = combined[\"Vehicle_Damage\"].astype(\"category\")\n\n# Changing Vehicle_Age to category\ncombined[\"Vehicle_Age\"] = combined[\"Vehicle_Age\"].astype(\"category\")\n\n# Changing Response to category\ncombined[\"Response\"] = combined[\"Response\"].astype(\"category\")\n\n# Changing Gender to category\ncombined[\"Gender\"] = combined[\"Gender\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting it back to train and test\ntrainData = combined[combined[\"type\"] == \"train\"].drop(\"type\", axis=1)\ntestData = combined[combined[\"type\"] == \"test\"].drop([\"type\", \"Response\"], axis=1)\nprint(f\"Train: {trainData.shape}\")\nprint(f\"Test: {testData.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First check for the null values in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null Values in the Train Data\ntrainData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values in the Test data\ntestData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the Numerical Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Age\nfig = plt.figure(figsize=(10,5))\nax = fig.subplots(1,2)\nax[0].hist(trainData[\"Age\"], bins=20)\nax[0].axvline(trainData[\"Age\"].mean(), color=\"r\")\n\nax[1].boxplot(trainData[\"Age\"])\nfig.suptitle(\"Distribution of Age\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Annual Premium\nfig = plt.figure(figsize=(10,5))\nax = fig.subplots(1,2)\nax[0].hist(trainData[\"Annual_Premium\"], bins=20)\nax[0].axvline(trainData[\"Annual_Premium\"].mean(), color=\"r\")\n\nax[1].boxplot(trainData[\"Annual_Premium\"])\nfig.suptitle(\"Distribution of Annual Premium\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Vintage\nfig = plt.figure(figsize=(10,5))\nax = fig.subplots(1,2)\nax[0].hist(trainData[\"Vintage\"], bins=20)\nax[0].axvline(trainData[\"Vintage\"].mean(), color=\"r\")\n\nax[1].boxplot(trainData[\"Vintage\"])\nfig.suptitle(\"Distribution of Vintage\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the Categorical Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = trainData.select_dtypes(\"category\").columns\ncat_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will check the different categories in each of the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Gender\nsns.countplot(trainData[\"Gender\"])\ndisplay(trainData[\"Gender\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Driving_License\nsns.countplot(trainData[\"Driving_License\"])\ndisplay(trainData[\"Driving_License\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Region_Code\nsns.countplot(trainData[\"Region_Code\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Previously_Insured\nsns.countplot(trainData[\"Previously_Insured\"])\ndisplay(trainData[\"Previously_Insured\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Vehicle_Age\nsns.countplot(trainData[\"Vehicle_Age\"])\ndisplay(trainData[\"Vehicle_Age\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Vehicle_Damage\nsns.countplot(trainData[\"Vehicle_Damage\"])\ndisplay(trainData[\"Vehicle_Damage\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Policy_Sales_Channel\nsns.countplot(trainData[\"Policy_Sales_Channel\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Response\nsns.countplot(trainData[\"Response\"])\ndisplay(trainData[\"Response\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some key observations from the EDA:\n1. **The Target/Label is highly imbalanced**\n2. Most people considered in the data have license with them.\n3. There are very few people with vehicle age > 2 years are present in the data."},{"metadata":{},"cell_type":"markdown","source":"# **Answering the hypothesis through the data:**"},{"metadata":{},"cell_type":"markdown","source":"## **1. Are Younger people taking more vehicle insurance than the older ones?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age vs Response\n# Grouping the age into buckets\nbins = [1, 20, 40, 60,100]\ntrainData[\"Age_bucket\"] = pd.cut(trainData[\"Age\"], bins=bins, labels=[\"children\",\"young\", \"middle-aged\",\"old\"])\n\ncrosstab = pd.crosstab(trainData[\"Age_bucket\"], trainData[\"Response\"])\ncrosstab.plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2. Is there any positive response among people who haven't insured previously?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Previously_Insured vs Response\npd.crosstab(trainData[\"Previously_Insured\"], trainData[\"Response\"]).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3. Does vehicle age affects the response rate?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vehicle Age vs Response\npd.crosstab(trainData[\"Vehicle_Age\"], trainData[\"Response\"]).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4. Does the age of vehicle and any damage in the past affectsthe response?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vehicle Age vs Vehicle Damage vs Response\nt1 = trainData.copy()\nt1[\"Response\"] =trainData[\"Response\"].astype(int)\npd.pivot_table(t1, values=\"Response\",index=[\"Vehicle_Age\", \"Vehicle_Damage\"], aggfunc= np.sum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preparation:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting X and Y\nX = trainData.drop([\"id\", \"Age\", \"Response\"], axis=1)\ny= trainData[\"Response\"]\n\nprint(X.shape)\nprint(y.shape)\ndisplay(X.head())\ndisplay(y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Train and Test set\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\nprint(f\"Training set: {X_train.shape}\")\nprint(f\"Testing set: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a preprocessing pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\ncat_features_full = list(X.select_dtypes(\"category\").columns)\n\nnum_features_full = list(X.select_dtypes([\"int\", \"float\"]).columns)\n\npreprocess = make_column_transformer(\n                                        (OneHotEncoder(handle_unknown=\"ignore\"), cat_features_full),\n                                        (MinMaxScaler(), num_features_full)\n                                    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\npipe = make_pipeline(preprocess,logreg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Building:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate different metrics\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n\nscore_log = pd.DataFrame()\ndef score_model(model, name, x_train, y_train, x_test, y_test, position):\n    model = model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    y_pred_prob = model.predict_proba(x_test)[:,1]\n    score_log.loc[position, \"Model\"] = name\n    score_log.loc[position, \"Accuracy\"] = round(accuracy_score(y_test, y_pred), 3)\n    score_log.loc[position,\"F1-Score\"] = round(f1_score(y_test, y_pred), 3)\n    score_log.loc[position, \"Roc-Auc\"] = round(roc_auc_score(y_test, y_pred), 3)\n    score_log.loc[position, \"Roc-Auc (Proba)\"] = round(roc_auc_score(y_test, y_pred_prob), 3)\n    score_log.loc[position, \"Precision\"] = round(precision_score(y_test, y_pred), 3)\n    score_log.loc[position, \"Recall\"] = round(recall_score(y_test, y_pred), 3)\n    return score_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to test and create submission file\ndef submission(pipeline):\n    testData = test.copy()\n    \n    bins = [1, 20, 40, 60,100]\n    testData[\"Age_bucket\"] = pd.cut(testData[\"Age\"], bins=bins, labels=[\"children\",\"young\", \"middle-aged\",\"old\"])\n    testID = testData.id\n    testData = testData.drop([\"id\", \"Age\"], axis=1)\n    \n    preds = pipeline.predict_proba(testData)[:,1]\n    \n    submission_log = pd.DataFrame(zip(testID, preds), columns=[\"id\", \"Response\"])\n    \n    return submission_log\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(class_weight=\"balanced\")\npipe = make_pipeline(preprocess,logreg)\n# print(cross_val_score(pipe, X, y,cv=StratifiedKFold(3), scoring=\"accuracy\").mean())\nscore_model(pipe, \"Logistic Regression\", X_train, y_train, X_test, y_test, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Decision Tree model\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier(class_weight=\"balanced\")\npipe = make_pipeline(preprocess,dtree)\n# print(cross_val_score(pipe, X, y,cv=StratifiedKFold(3), scoring=\"accuracy\").mean())\nscore_model(pipe, \"Decision Tree\", X_train, y_train, X_test, y_test, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the weightage for the positive class to handle the imbalance\nweight = int(round(y.value_counts()[0]/y.value_counts()[1],2))\nweight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating XGBoost model\nimport xgboost as xgb\n\n\n# Hyperparameter values are arbitrary\nmodelXGB = xgb.XGBClassifier(n_estimators = 1500,\n                      scale_pos_weight=weight,\n                      learning_rate = 0.01,\n                      colsample_bytree = 0.4,\n                      subsample = 0.7,\n                      objective='binary:logistic', \n                      reg_lambda = 0.4,\n                      max_depth=4, \n                      gamma=10,\n                      n_jobs=-1,\n                      )\n\npipe = make_pipeline(preprocess,modelXGB)\n# %time print(cross_val_score(pipe, X, y,cv=StratifiedKFold(3), scoring=\"accuracy\").mean())\nscore_model(pipe, \"XGB Classifier\", X_train, y_train, X_test, y_test, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Light GBM Model\nimport lightgbm as lgb\n\nmodelLGB = lgb.LGBMClassifier(objective='binary',\n                             n_estimators = 1500,\n                             learning_rate = 0.01,\n                             n_jobs = -1,\n                             seed=123,\n                             max_depth = 4,\n                             subsample = 0.7,\n                             reg_lambda = 0.3,\n                             colsample_bytree = 0.4,\n                             scale_pos_weight=weight,\n                             num_leaves = 10)\n\npipe = make_pipeline(preprocess,modelLGB)\nscore_model(pipe, \"LGBM Classifier\", X_train, y_train, X_test, y_test, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an ensemble\nfrom sklearn.ensemble import VotingClassifier\n\nvotingCLF = VotingClassifier([\n                             (\"XGBoost\", modelXGB),\n                             (\"LGBM\", modelLGB)], voting=\"soft\", weights=[0.3, 0.7])\n\npipe = make_pipeline(preprocess,votingCLF)\nscore_model(pipe, \"Voting Ensemble\", X_train, y_train, X_test, y_test, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_log = submission(pipe)\n# sub_log.to_csv(\"submission_ensemble1_withProba.csv\", index=False)\nsub_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the model with the given features gives an AUC score of 0.851 with the Private leaderboard ranking of around 300, wheras the Top score for the hackathon is around 0.863.\n\nNow, let's perform the feature engineering and try building a new model on top of it to see whether adding new features impacts the performance of the model or not."},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataFeat = train.copy()\ntestDataFeat = test.copy()\n\ncombinedFeat = pd.concat([trainDataFeat, testDataFeat], axis=0)\nprint(f\"The size of the combined data: {combinedFeat.shape}\")\ncombinedFeat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedFeat.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the categorical variables {Gender, Vehicle_Age, Vehicle_Damage}\ncombinedFeat[\"Gender\"] = combinedFeat[\"Gender\"].map({\"Male\": 1,\n                                                   \"Female\": 0})\n\ncombinedFeat[\"Vehicle_Age\"] = combinedFeat[\"Vehicle_Age\"].map({\"< 1 Year\": 0,\n                                                             \"1-2 Year\": 1,\n                                                             \"> 2 Years\": 2})\n\ncombinedFeat[\"Vehicle_Damage\"] = combinedFeat[\"Vehicle_Damage\"].map({\"Yes\": 1,\n                                                                   \"No\": 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new features by combining and transforming the given set of features\n\n# Changing vintage to years from days\ncombinedFeat[\"Vintage\"] = combinedFeat[\"Vintage\"]/365\n\n# Policy channels per Region\ncombinedFeat[\"unique_policy_channel_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Policy_Sales_Channel\"].transform(\"nunique\")\n\n# Average age per Region\ncombinedFeat[\"avg_age_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Age\"].transform(\"mean\")\n\n# Total & Average Driving license per Region\ncombinedFeat[\"sum_license_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Driving_License\"].transform(\"sum\")\ncombinedFeat[\"avg_license_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Driving_License\"].transform(\"mean\")\n\n# Total & Average Insured persons per Region\ncombinedFeat[\"sum_insured_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Previously_Insured\"].transform(\"sum\")\ncombinedFeat[\"avg_insured_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Previously_Insured\"].transform(\"mean\")\n\n# Total & Average vehicle damager per Region\ncombinedFeat[\"sum_vehicle_damage_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Vehicle_Damage\"].transform(\"sum\")\ncombinedFeat[\"avg_vehicle_damage_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Vehicle_Damage\"].transform(\"mean\")\n\n# Average vintage per Region\ncombinedFeat[\"avg_vintage_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Vintage\"].transform(\"mean\")\n\n# Total, Average & Standard deviation of Annual premium paid by customers per Region\ncombinedFeat[\"sum_premium_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Annual_Premium\"].transform(\"sum\")\ncombinedFeat[\"avg_premium_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Annual_Premium\"].transform(\"mean\")\ncombinedFeat[\"std_premium_per_Region\"] = combinedFeat.groupby([\"Region_Code\"])[\"Annual_Premium\"].transform(\"std\")\n\n# Previously not insured & have vehicle damage\ncombinedFeat[\"not_insured_has_damage\"] = np.where((combinedFeat[\"Previously_Insured\"]== 0) & (combinedFeat[\"Vehicle_Damage\"]== 1), 1, 0)\n\n# Have Driving license & vehicle damage\ncombinedFeat[\"license_has_damage\"] = np.where((combinedFeat[\"Driving_License\"] == 1) & (combinedFeat[\"Vehicle_Damage\"] == 1), 1, 0)\n\n# Total, Average & Standard deviation of Annual premium by vehicle age\ncombinedFeat[\"sum_premium_per_vehicle_age\"] = combinedFeat.groupby([\"Vehicle_Age\"])[\"Annual_Premium\"].transform(\"sum\")\ncombinedFeat[\"avg_premium_per_vehicle_age\"] = combinedFeat.groupby([\"Vehicle_Age\"])[\"Annual_Premium\"].transform(\"mean\")\ncombinedFeat[\"std_premium_per_vehicle_age\"] = combinedFeat.groupby([\"Vehicle_Age\"])[\"Annual_Premium\"].transform(\"std\")\n\n# Total, Average & Standerd deviation of Annual premium by vehicle damage\ncombinedFeat[\"sum_premium_per_vehicle_damage\"] = combinedFeat.groupby([\"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"sum\")\ncombinedFeat[\"avg_premium_per_vehicle_damage\"] = combinedFeat.groupby([\"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"mean\")\ncombinedFeat[\"std_premium_per_vehicle_damage\"] = combinedFeat.groupby([\"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"std\")\n\n# Total, Average & Standerd deviation of Annual premium by Policy channel\ncombinedFeat[\"sum_premium_by_policy_channel\"] = combinedFeat.groupby([\"Policy_Sales_Channel\"])[\"Annual_Premium\"].transform(\"sum\")\ncombinedFeat[\"avg_premium_by_policy_channel\"] = combinedFeat.groupby([\"Policy_Sales_Channel\"])[\"Annual_Premium\"].transform(\"mean\")\ncombinedFeat[\"std_premium_by_policy_channel\"] = combinedFeat.groupby([\"Policy_Sales_Channel\"])[\"Annual_Premium\"].transform(\"std\")\n\n# Total, Average & Standerd deviation of Annual premium by vehicle damage & vehicle age\ncombinedFeat[\"sum_premium_per_vehicle_age_damage\"] = combinedFeat.groupby([\"Vehicle_Age\", \"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"sum\")\ncombinedFeat[\"avg_premium_per_vehicle_age_damage\"] = combinedFeat.groupby([\"Vehicle_Age\", \"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"mean\")\ncombinedFeat[\"std_premium_per_vehicle_age_damage\"] = combinedFeat.groupby([\"Vehicle_Age\", \"Vehicle_Damage\"])[\"Annual_Premium\"].transform(\"std\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combinedFeat.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\ntrainDataFeat = combinedFeat[~combinedFeat[\"Response\"].isnull()].drop(\"id\", axis=1).dropna()\ntestDataFeat = combinedFeat[combinedFeat[\"Response\"].isnull()].drop([\"Response\", \"id\"], axis=1).fillna(0)\n\nprint(f\"Train Data: {trainDataFeat.shape}\")\nprint(f\"Test Data: {testDataFeat.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the training data into train and validation\nfrom sklearn.model_selection import train_test_split\n\nX_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(trainDataFeat.drop(\"Response\", axis=1), trainDataFeat[\"Response\"], test_size=0.3, random_state=123, stratify=trainDataFeat[\"Response\"])\n\nprint(f\"Training set: {X_train_feat.shape}\")\nprint(f\"Testing set: {X_test_feat.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_feat_scaled = pd.DataFrame(scaler.fit_transform(X_train_feat), columns = X_train_feat.columns)\nX_test_feat_scaled = pd.DataFrame(scaler.transform(X_test_feat), columns = X_test_feat.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing the test data\nscaler = MinMaxScaler()\nscaler.fit(trainDataFeat.drop(\"Response\", axis=1))\ntestDataFeat_scaled = pd.DataFrame(scaler.transform(testDataFeat), columns = testDataFeat.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(class_weight=\"balanced\")\nscore_model(logreg, \"Logistic Regression (Features)\", X_train_feat_scaled, y_train_feat, X_test_feat_scaled, y_test_feat, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Creating XGBoost model\nimport xgboost as xgb\n\nmodelXGB = xgb.XGBClassifier(n_estimators = 1000,\n                      scale_pos_weight=weight,\n                      learning_rate = 0.01,\n                      colsample_bytree = 0.4,\n                      subsample = 0.7,\n                      objective='binary:logistic', \n                      reg_lambda = 0.4,\n                      max_depth=4, \n                      gamma=10,\n                      n_jobs=-1,\n                      )\n\n\nscore_model(modelXGB, \"XGB Classifier (features)\", X_train_feat_scaled, y_train_feat, X_test_feat_scaled, y_test_feat, 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Light GBM Model\nimport lightgbm as lgb\n\nmodelLGB = lgb.LGBMClassifier(objective='binary',\n                             n_estimators = 1000,\n                             learning_rate = 0.01,\n                             n_jobs = -1,\n                             seed=123,\n                             max_depth = 4,\n                             subsample = 0.7,\n                             reg_lambda = 2,\n                             colsample_bytree = 0.4,\n                             scale_pos_weight=weight,\n                             num_leaves = 10)\n\nscore_model(modelLGB, \"LGBM Classifier (features)\", X_train_feat_scaled, y_train_feat, X_test_feat_scaled, y_test_feat, 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an ensemble\nfrom sklearn.ensemble import VotingClassifier\n\nvotingCLF = VotingClassifier([\n                             (\"XGBoost\", modelXGB),\n                             (\"LGBM\", modelLGB)], voting=\"soft\", weights=[0.4, 0.6])\n\nscore_model(votingCLF, \"Voting Ensemble  (Features)\", X_train_feat_scaled, y_train_feat, X_test_feat_scaled, y_test_feat, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting on the test set\npreds = votingCLF.predict_proba(testDataFeat_scaled)\nsubmission_log = pd.DataFrame(zip(test.id, preds[:,1]), columns=[\"id\", \"Response\"])\nsubmission_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission_log.to_csv(\"submission_voting_feat_prob.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This final voting ensemble has an AUC score of 0.857 and private AUC score of 0.862 with the private leaderboard rank of 185.\n\nWe can still fine tune the hyperparameters of the individual models and also add CatBoost to the ensemble to try improve the score further. Try it out and let me know if you were able to improve the leaderboard score.\n\nHappy Learning!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}