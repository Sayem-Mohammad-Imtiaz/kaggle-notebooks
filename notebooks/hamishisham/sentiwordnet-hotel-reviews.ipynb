{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install numpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pandas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tweet-preprocessor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data[['Review']]\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to preprocess the tweets data\ndef preprocess_tweet_data(data,name):\n    # Proprocessing the data\n    data[name]=data[name].str.lower()\n    # Code to remove the Hashtags from the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    # Code to remove the links from the text\n    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Code to remove the Special characters from the text \n    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n    # Code to substitute the multiple spaces with single spaces\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n    # Code to remove all the single characters in the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n    # Remove the twitter handlers\n    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Function to tokenize and remove the stopwords    \ndef rem_stopwords_tokenize(data,name):\n      \n    def getting(sen):\n        example_sent = sen\n\n        stop_words = set(stopwords.words('english')) \n\n        word_tokens = word_tokenize(example_sent) \n\n        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n\n        filtered_sentence = [] \n\n        for w in word_tokens: \n            if w not in stop_words: \n                filtered_sentence.append(w) \n        return filtered_sentence\n    x=[]\n    for i in data[name].values:\n        x.append(getting(i))\n    data[name]=x\n# Making a function to lemmatize all the words\nlemmatizer = WordNetLemmatizer() \ndef lemmatize_all(data,name):\n    arr=data[name]\n    a=[]\n    for i in arr:\n        b=[]\n        for j in i:\n            x=lemmatizer.lemmatize(j,pos='a')\n            x=lemmatizer.lemmatize(x)\n            b.append(x)\n        a.append(b)\n    data[name]=a\n# Function to make it back into a sentence \ndef make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the preprocessing function to preprocess the tweet data\npreprocess_tweet_data(data,'Review')\n# Using tokenizer and removing the stopwords\nrem_stopwords_tokenize(data,'Review')\n# Converting all the texts back to sentences\nmake_sentences(data,'Review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting nlp from spacy.load\nnlp=spacy.load('en')\n# Making the function to get the sentiments out of the dataframe\ndef get_sentiment(data,name):\n    count=1\n    l=len(data)\n    positive_sentiments=[]\n    negative_sentiments=[]\n    for tex in data[name].values:\n        print('The current status is :',count*100/l,'%')\n        tex=nlp(tex)\n        noun=[]\n        verb=[]\n        adj=[]\n        adv=[]\n        for i in tex :\n            if i.pos_=='NOUN':\n                noun.append(i)\n            elif i.pos_ =='ADJ':\n                adj.append(i)\n            elif i.pos_ =='VERB':\n                verb.append(i)\n            elif i.pos_=='ADV':\n                adv.append(i)\n        clear_output(wait=True)\n        count+=1\n        neg_score=[]\n        pos_score=[]\n        for i in tex :\n            try:\n                if i in noun:\n                    x=swn.senti_synset(str(i)+'.n.01')\n                    neg_score.append(x.neg_score())\n                    pos_score.append(x.pos_score())\n                elif i in adj:\n                    x=swn.senti_synset(str(i)+'.a.02')\n                    neg_score.append(x.neg_score())\n                    pos_score.append(x.pos_score())\n                elif i in adv :\n                    x=swn.senti_synset(str(i)+'.r.02')\n                    neg_score.append(x.neg_score())\n                    pos_score.append(x.pos_score())\n                elif i in verb :\n                    x=swn.senti_synset(str(i)+'.v.02')\n                    neg_score.append(x.neg_score())\n                    pos_score.append(x.pos_score())\n\n            except:\n                pass\n        positive_sentiments.append(np.mean(pos_score))\n        negative_sentiments.append(np.mean(neg_score))\n\n    data['Positive Sentiment']=positive_sentiments\n    data['Negative Sentiment']=negative_sentiments\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sentiment(data,'Review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overall=[]\nfor i in range(len(data)):\n    if data['Positive Sentiment'][i]>data['Negative Sentiment'][i]:\n        overall.append('Positive')\n    elif data['Positive Sentiment'][i]<data['Negative Sentiment'][i]:\n        overall.append('Negative')\n    else:\n        overall.append('Neutral')\ndata['Overall Sentiment']=overall\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nX = vec.fit_transform(data['Review'])\ndf = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating a python object of the class CountVectorizer\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n                             ngram_range=(1,1)) # number of n-grams\n\nbow_data = bow_counts.fit_transform(data['Review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n                                                                    data['Overall Sentiment'], # Target variable\n                                                                    test_size = 0.2, # 20% test size\n                                                                    random_state = 0) # random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n### Training the model \nlr_model_all = LogisticRegression() # Logistic regression\nlr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n\n## Predicting the output\ntest_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n\n\n## Calculate key performance metrics\n\nfrom sklearn.metrics import classification_report\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdf = pd.DataFrame()\nnewdf['y_test_bow'] = y_test_bow\nnewdf['test_pred_lr_all'] = test_pred_lr_all\nnewdf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}