{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import and prepare data for model","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#python won't show a long unnecessary error message that comes up a lot\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"hitters=pd.read_csv('/kaggle/input/hitters/hitters_filtered').drop(columns='Unnamed: 0')\nhitters.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add full player name, rather than just having pleyer ID.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"names=pd.read_csv('/kaggle/input/the-history-of-baseball/player.csv')\nnames['name']=names['name_first']+' '+names['name_last']\nnames=names[['player_id','name']]\nnames=names[names['player_id'].isin(hitters['player_id'].tolist())]\nhitters=hitters.join(names.set_index('player_id'),on='player_id')\nhitters.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the entries in the 'percent' column are empty. We have to fill these with something or cut them out in order to carry on with the 'percent' column in our machine learning algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hitters2=hitters[-hitters['percent'].isnull()].reset_index().drop(columns='index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create decision tree regressor model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=hitters2['percent']\nfeatures=['g','ab','r','h','rbi','bb','hr']\nX=hitters2[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Make new dataframe with all testing data, add predictions, whether each prediction would warrant a hall of fame induction, and if the prediction was correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(val_X)\ndf=df.join(hitters2[['player_id','name','inducted','percent','threshold','year']])\ndf['prediction']=predictions\n\ndf['guess']=''\nfor index in df.reset_index()['index']:\n    if df['prediction'][index]>=df['threshold'][index]:\n        df['guess'][index]='Y'\n    else:\n        df['guess'][index]='N'\ndf['correct?']=df['guess']==df['inducted']\n\n#change the order of the columns\ndf=df[['name','player_id','g','ab','r','h','hr','rbi','bb','percent','threshold','year','inducted','prediction','guess',\n      'correct?']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['correct?'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"231/265 predictions correct, or 87.2%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at the actual hall of famers in the testing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', None)\nhof=df[df['inducted']=='Y']\nhof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hof['correct?'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 18 out of the 30 hall of famers were predicted to be hall of famers by the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualize the data, predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s30=range(1930,1940)\ns40=range(1940,1950)\ns50=range(1950,1960)\ns60=range(1960,1970)\ns70=range(1970,1980)\ns80=range(1980,1990)\ns90=range(1990,2000)\ns2000 = range(2000,2016)\n\ndecades=[s30,s40,s50,s60,s70,s80,s90,s2000]\n\nfig, axes = plt.subplots(nrows=4, ncols=2,figsize=(40, 20))\nfig.subplots_adjust(hspace=1)\nplt.suptitle('MLB HOF Voting results and predictions \\n green: incorrect- should be HOF \\n blue: incorrect- should not be HOF',fontsize=30)\nfor decade,ax in zip(decades,axes.flatten()):\n    frame=df[df['year'].isin(decade)]\n    \n    ax.plot(frame['name'],frame['percent'],'o',color='red',label = 'Actual Values')\n\n    ax.plot(frame['name'],frame['prediction'],'X',color='yellow',label = 'Predicted Values')\n  \n    incorrect=frame[frame['correct?'].isin([False])]\n    circle_rad = 10 \n    \n    overshoot=incorrect[incorrect['prediction']>incorrect['percent']]\n    ax.plot(overshoot['name'], overshoot['percent'], 'o',ms=circle_rad * 2, mec='b', mfc='none', mew=2)\n    ax.plot(overshoot['name'], overshoot['prediction'], 'o',ms=circle_rad * 2, mec='b', mfc='none', mew=2)\n    \n    undershoot=incorrect[incorrect['percent']>incorrect['prediction']]\n    ax.plot(undershoot['name'], undershoot['percent'], 'o',ms=circle_rad * 2, mec='g', mfc='none', mew=2)\n    ax.plot(undershoot['name'], undershoot['prediction'], 'o',ms=circle_rad * 2, mec='g', mfc='none', mew=2)\n    \n    ax.set_xlabel('Player')\n    ax.set_ylabel('Percent of HOF Votes')\n    ax.set_title(str(decade[0])+'-'+str(decade[-1]))\n    ax.legend(loc = 'upper right')\n    ax.set_xticklabels(labels=frame['name'],rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpreting the visuals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"These visuals help visualize what the model did well and what it didn't. What stands out are the number of blue circles- this being players that were predicted to make the HOF but didn't actually get voted in. Let's take a closer look at all of the incorrect predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"incorrect=df[df['correct?'].isin([False])]\nincorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overshoot=incorrect[incorrect['prediction']>incorrect['percent']]\nlen(overshoot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"22 of the incorrect predictions were the result of too high of a prediction (like a false positive).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"undershoot=incorrect[incorrect['percent']>incorrect['prediction']]\nlen(undershoot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"12 of the incorrect predictions were the result of too low of a prediction (like a false negative).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Takeaways:\n\n-87.2% accuracy seems pretty good for a start, but looking closer at the data we realize that there are many flaws in the model.\n-Only 18 out of the 30 hall of famers in the testing data were actually predicted to make the hall of fame\n-22 players that are not hall of famers were predicted to be\n\nThis means that most of the players our model predicted to be in the hall of fame are not, thus 87.2% accuracy is certainly not a fair way to summarize the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Future work","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are so many ways to improve this model. Some ideas include:\n\n\n1. Including player awards such as MVP, silver slugger, and many other baseball awards\n2. Include advanced stats such as slugging percentage and on base percentage\n3. Include postseason stats and other stats that weren't included in this model.\n4. Changing the paramaters to fine tune the model- I have some other machine learning notebooks that show how to do this\n5. Try a different type of model, ex. random forest regressor\n6. Instead of predicting percent of the vote, try making it a bunary classification problem. That is, rather than predict a numerical value and determine if that warrants a HOF induction, simply predict whether or not a player will get inducted into the HOF.\n7. The data used for this project is dated. Data through 2020 will help create a better model. It may be hard to find that data, however, and the data for this project was already avaialable on kaggle which made it easier to use\n\nThere are more ways to help improve the model as these are just what come to mind. The other notebooks show how I combined data from different datasets to create the data for this model, and how I decided which features to put in the model.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}