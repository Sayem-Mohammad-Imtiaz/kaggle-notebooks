{"cells":[{"metadata":{"_uuid":"13831611-3f39-426e-9362-1c7e549e0602","_cell_guid":"5c4d9088-ba1f-4e42-a605-6149e4f47291","trusted":true},"cell_type":"code","source":"# imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import data\n\npath = '/kaggle/input/titanic/train_and_test2.csv'\n\n# read data from csv\n\ncols = list(pd.read_csv(path, nrows=1))\ndata = pd.read_csv(path, usecols=[i for i in cols if i[:4] != 'zero'])\nn = len(data.columns) - 1\n\n# fill null values\n\ndata.Embarked.fillna(data.Embarked.mode()[0], inplace = True)\n\n# train-test split\n\ntrain = data.sample(frac=0.8)\ntest = data.drop(train.index)\n\n# standardize & normalize\n\ntrain = (train - train.mean()) / train.std()\ntrain = (train - train.min()) / (train.max() - train.min())\ntest = (test - test.mean()) / test.std()\ntest = (test - test.min()) / (test.max() - test.min())\n\n\n# convert to numpy\n\nm_train = train.shape[0]\nm_test = test.shape[0]\n\ntrain_y = (train['2urvived']).to_numpy().reshape(1, m_train)\ntrain_x = (train.drop(['2urvived'], axis=1)).to_numpy().T\n\ntest_y = (test['2urvived']).to_numpy().reshape(1, m_test)\ntest_x = (test.drop(['2urvived'], axis=1)).to_numpy().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# definition of the sigmoid function\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# methods for logistic regression\n\n# forwards and backwards propagation \n\ndef propagate_logistic(w, b, X, Y):\n    \n    m = X.shape[1]\n    \n    # current predictions made by the model\n    z = np.dot(w.T, X) + b\n    A = sigmoid(z) \n    # current loss of the model\n    loss = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))\n    \n    # compute dw \n    dw = (1 / m) * np.dot(X, (A - Y).T)\n    # compute db\n    db = (1 / m) * np.sum(A - Y)\n    \n    return dw, db, loss\n\n\n# gradient descent\n\ndef optimize_logistic(w, b, X, Y, n_iter, alpha):\n    \n    loss = []\n    \n    for i in range(n_iter):\n        \n        dw, db, cur_loss = propagate_logistic(w, b, X, Y)\n        w -= alpha * dw\n        b -= alpha * db\n        \n        if i % 100 == 0:\n            loss.append(cur_loss)\n            if i % 1000 == 0:\n                print(\"Loss at iteration %i: %f\" % (i, cur_loss))\n        \n    return w, b, loss\n\n# predict \n\ndef predict_logistic(w, b, X):\n    A = sigmoid(np.dot(w.T, X) + b)\n    ret = A > 0.5\n    return ret\n\n\ndef model_logistic(X_train, Y_train, X_test, Y_test, num_iter=2000, alpha=0.5):\n    # init. weights\n    w = np.random.rand(n, 1)\n    b = np.random.rand()\n    \n    w, b, loss = optimize_logistic(w, b, X_train, Y_train, num_iter, alpha)\n    \n    Y_hat_test = predict_logistic(w, b, X_test)\n    Y_hat_train = predict_logistic(w, b, X_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_hat_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_hat_test - Y_test)) * 100))\n    \n    return loss\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# methods for a shallow neural network\n\n# forwards and backwards propagation\n\ndef propagate_nn(w1, b1, w2, b2, X, Y):\n    \n    m = X.shape[1]\n    \n    # current predictions made by the model\n    z1 = np.dot(w1, X) + b1\n    A1 = sigmoid(z1)\n    z2 = np.dot(w2, A1) + b2\n    A2 = sigmoid(z2)\n    \n    # current loss of the model\n    loss = (- 1 / m) * np.sum(Y * np.log(A2) + (1 - Y) * (np.log(1 - A2)))\n    \n    # compute dw, db\n    dz2 = A2 - Y\n#     print('dz2 shape: ')\n#     print(dz2.shape)\n    dw2 = (1 / m) * np.dot(A1, dz2.T).T\n#     print('dw2 shape: ')\n#     print(dw2.shape)\n    db2 = (1 / m) * np.sum(dz2)\n#     print('db2 shape: ')\n#     print(db2.shape)\n    g_tag = A1 * (1 - A1)\n#     print('g_tag shape: ')\n#     print(g_tag.shape)\n    dz1 = np.dot(w2.T, dz2) * g_tag\n#     print('dz1 shape: ')\n#     print(dz1.shape)\n    dw1 = (1 / m) * np.dot(X, dz1.T).T\n#     print('dw1 shape: ')\n#     print(dw1.shape)\n    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n#     print('db1 shape: ')\n#     print(db1.shape)\n    return dw1, db1, dw2, db2, loss\n\n\n# gradient descent\n\ndef optimize_nn(w1, b1, w2, b2, X, Y, n_iter, alpha):\n    \n    loss = []\n    \n    for i in range(n_iter):\n        \n        dw1, db1, dw2, db2, cur_loss = propagate_nn(w1, b1, w2, b2, X, Y)\n        w1 -= alpha * dw1\n        b1 -= alpha * db1\n        w2 -= alpha * dw2\n        b2 -= alpha * db2\n        \n        if i % 100 == 0:\n            loss.append(cur_loss)\n            if i % 1000 == 0:\n                print(\"Loss at iteration %i: %f\" % (i, cur_loss))\n        \n    return w1, b1, w2, b2, loss\n\n# predict \n\ndef predict_nn(w1, b1, w2, b2, X):\n    z1 = np.dot(w1, X) + b1\n    A1 = sigmoid(z1)\n    z2 = np.dot(w2, A1) + b2\n    A2 = sigmoid(z2)\n    return A2 > 0.5\n\n\ndef model_nn(X_train, Y_train, X_test, Y_test, num_iter=2000, alpha=0.5):\n    # init. weights\n    w1 = np.random.rand(8, n)\n    b1 = np.random.rand(8, 1)\n    w2 = np.random.rand(1, 8)\n    b2 = np.random.rand()\n    \n    w1, b1, w2, b2, loss = optimize_nn(w1, b1, w2, b2, X_train, Y_train, num_iter, alpha)\n    \n    Y_hat_test = predict_nn(w1, b1, w2, b2, X_test)\n    Y_hat_train = predict_nn(w1, b1, w2, b2, X_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_hat_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_hat_test - Y_test)) * 100))\n    \n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple logistic regression w/ gradient descent\n\n\n# learning rate & iterations of gradient descent\nlearning_rate = 0.09\nn_iter = 10000\n\nloss = model_logistic(train_x, train_y, test_x, test_y, num_iter=10000, alpha=learning_rate)\n\nplot = sns.lineplot(range(len(loss)), loss)\nplot.set(xlabel='Iteration', ylabel='Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shallow neural network w/ gradient descent\n\n\n# learning rate & iterations of gradient descent\nlearning_rate = 0.09\nn_iter = 10000\n\nloss = model_nn(train_x, train_y, test_x, test_y, num_iter=10000, alpha=learning_rate)\n\nplot = sns.lineplot(range(len(loss)), loss)\nplot.set(xlabel='Iteration', ylabel='Loss')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}