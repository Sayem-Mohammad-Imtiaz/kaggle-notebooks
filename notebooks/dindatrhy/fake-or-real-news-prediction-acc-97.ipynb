{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wellcome, Guys!","metadata":{}},{"cell_type":"markdown","source":"I think to celebrate **World Press Freedom Day (WPFD) on 03 May 2021**, it will be fun to do something about press. Since we know that hoax or fake pers is almost around the world and give wors impact. So, I think it will be nice if we learn about predicting fake or real news, especially for me as a beginner in this field.\nI choose this dataset that completely easy to use for me as well.\n*Let's do it!*","metadata":{}},{"cell_type":"code","source":"import os\nfrom IPython.display import Image\nImage(filename=\"../input/images/fake.jpg\", width= 1000, height=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import Library Package**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re, string\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_news = pd.read_csv('../input/fake-news-detection/data.csv')\ndata_news.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explanatory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(data_news.shape)\nprint(50*'-')\nprint(data_news.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*So, we're gonna merge the Headline and Body columns and remove the unnecessary feature.*","metadata":{}},{"cell_type":"code","source":"data_news['Body'] = data_news['Headline']+data_news['Body']\ndata_news = data_news.drop(['URLs', 'Headline'], axis=1)\ndata_news.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values","metadata":{}},{"cell_type":"markdown","source":"*Before we start to preprocessing the dataset, we have to make sure that there is no any missing values. It could be worst our preprocessing process.*","metadata":{}},{"cell_type":"code","source":"data_news.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Looks there is a lot missing values, 21 missing values in Body columns. Here we go to drop it.*","metadata":{}},{"cell_type":"code","source":"data_news.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_news.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data_news['Label'], palette='pastel')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"*Let's clean the dataset, especially for the feature one.*","metadata":{}},{"cell_type":"code","source":"def clean_data(news):\n    punc = set(string.punctuation)\n    news = ''.join(ch for ch in news if ch not in punc)\n    return news\n    \n    news = news.lower()\n    news = str(text).lower()\n    news = re.sub('\\[.*?\\]', '', news)\n    news = re.sub('https?://\\S+|www\\.\\S+', '', news)\n    news = re.sub('<.*?>+', '', news)\n    news = re.sub('[%s]' % re.escape(string.punctuation), '', news)\n    news = re.sub('\\n', '', news)\n    news = re.sub('\\w*\\d\\w*', '', news)\n    news = re.sub('Reuters','',news)\n    news = re.sub(r\"\\d+\", \"\", news)\n    \ndata_news['Body'] = data_news['Body'].apply(lambda x: clean_data(x))\ndata_news['Body']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove the Stopwords**","metadata":{}},{"cell_type":"code","source":"stopword = stopwords.words('english')\ndata_news['Body'] = data_news['Body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopword]))\ndata_news['Body']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatize Words**","metadata":{}},{"cell_type":"code","source":"def lemmatize_word(news):\n    lem = WordNetLemmatizer()\n    lemmatizer = ''.join([lem.lemmatize(i) for i in news.split()])\n    return news\n\ndata_news['Body'] = data_news['Body'].apply(lambda x: lemmatize_word(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's Train Our Feature**","metadata":{}},{"cell_type":"code","source":"#Define the label and feature\nX = data_news['Body']\ny = data_news['Label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer(stop_words='english')\ncount_train = vectorizer.fit_transform(X_train.values)\ncount_test = vectorizer.transform(X_test.values)\nprint(count_test)\nprint(count_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Model 1 - default parameter \n\nnb_classifier1 = MultinomialNB()\nnb_classifier1.fit(count_train, y_train)\n\npred1 = nb_classifier1.predict(count_test)\n\nprint(classification_report(y_test, pred1, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1\n\nsvc_model1 = SVC(C=1, kernel='linear', gamma= 1)\nsvc_model1.fit(count_train, y_train)\n\nprediction1 = svc_model1.predict(count_test)\n\nprint(classification_report(y_test, prediction1, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2\nsvc_model2 = SVC(C= 100, kernel='linear', gamma= 1)\nsvc_model2.fit(count_train, y_train)\n\nprediction2 = svc_model2.predict(count_test)\n\nprint(classification_report(y_test, prediction2, target_names = ['Fake','True']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, here we go with 97% Accuracy, yeay!\nThanks for your attention! ","metadata":{}},{"cell_type":"markdown","source":"**Don't forget to vote this code.**\n**Hope you have a nice day!**","metadata":{}}]}