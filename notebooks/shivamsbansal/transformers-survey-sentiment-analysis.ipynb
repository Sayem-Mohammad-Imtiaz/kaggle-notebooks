{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Sentiment Analysis using Transformers by HuggingFace Pytorch</h1></center>\n<br>\n<center>Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.</center>\n<br>\n<center><img height=200 width=200 src = https://pytorch.org/assets/images/huggingface-logo.png></center>\n\n<br>\n<center><h4>I will be using the HuggingFace Python package for predicting question tags for this StackOverflow dataset. I'm just a beginner with this so please feel free to comment if I can do something better.</h4></center>\n\n<br>\n<center><img src = https://www.codemotion.com/magazine/wp-content/uploads/2020/05/bert-google-896x504.png></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\n\nfrom torch.utils.data import TensorDataset\n\nimport transformers\nfrom transformers import BertForSequenceClassification\n\nimport numpy as np\nimport pandas as pd\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch\n#     torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin-1')\nsurvey_df = pd.read_csv('../input/survey-wos/survey_with_overall_sentiment.csv')\nsurvey_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting of mentions and hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_hash_tags(s):\n    hashes = re.findall(r\"#(\\w+)\", s)\n    return \" \".join(hashes)\ndf['hashtags'] = df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_mentions(s):\n    hashes = re.findall(r\"@(\\w+)\", s)\n    return \" \".join(hashes)\ndf['mentions'] = df['OriginalTweet'].apply(lambda x : extract_mentions(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding classes [total 5]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\n# df['encoded_sentiment'] = encoder.fit_transform(df['Sentiment'])\nsurvey_df['encoded_sentiment'] = encoder.fit_transform(survey_df['OverallSentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(survey_df.head(),df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# xtrain, xval, ytrain, yval = train_test_split(df['OriginalTweet'], df['encoded_sentiment'], test_size = 0.2)\ns_xtrain, s_xval, s_ytrain, s_yval = train_test_split(survey_df['CleanText'], survey_df['encoded_sentiment'], test_size = 0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(type(xtrain[7]),type(s_xtrain[7]))\n# print(type(s_xtrain), type(xtrain))\n# print(xtrain[7])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = {}\nfor x in s_xtrain:\n    types[type(x)]=1\n    if str(type(x))==\"<class 'float'>\":\n        print(x)\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Words to Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_str = survey_df['CleanText'].str.len().quantile(0.98)\nprint(max_str, type(max_str), int(max_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_encoded_data_train = tokenizer.batch_encode_plus(\n    s_xtrain, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=int(max_str), \n    return_tensors='pt'\n)\n\ns_encoded_data_val = tokenizer.batch_encode_plus(\n    s_xval, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=int(max_str), \n    return_tensors='pt'\n)\n\n# encoded_data_train = tokenizer.batch_encode_plus(\n#     xtrain, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=50, \n#     return_tensors='pt'\n# )\n\n# encoded_data_val = tokenizer.batch_encode_plus(\n#     xval, \n#     add_special_tokens=True, \n#     return_attention_mask=True, \n#     pad_to_max_length=True, \n#     max_length=50, \n#     return_tensors='pt'\n# )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting inputs and attention masks out of encoded data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input_ids_train = encoded_data_train['input_ids']\n# attention_masks_train = encoded_data_train['attention_mask']\n# labels_train = torch.tensor(ytrain.values)\n\ns_input_ids_train = s_encoded_data_train['input_ids']\ns_attention_masks_train = s_encoded_data_train['attention_mask']\ns_labels_train = torch.tensor(s_ytrain.values)\n\n# input_ids_val = encoded_data_val['input_ids']\n# attention_masks_val = encoded_data_val['attention_mask']\n# labels_val = torch.tensor(yval.values)\n\ns_input_ids_val = s_encoded_data_val['input_ids']\ns_attention_masks_val = s_encoded_data_val['attention_mask']\ns_labels_val = torch.tensor(s_yval.values)\n\n\n# Pytorch TensorDataset Instance\n# dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n# dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n\ns_dataset_train = TensorDataset(s_input_ids_train, s_attention_masks_train, s_labels_train)\ns_dataset_val = TensorDataset(s_input_ids_val, s_attention_masks_val, s_labels_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing the model\n\n# model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n#                                                       num_labels=5,\n#                                                       output_attentions=False,\n#                                                       output_hidden_states=False)\ns_model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=4,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# dataloader_train = DataLoader(dataset_train, \n#                               sampler=RandomSampler(dataset_train), \n#                               batch_size=128)\n\n# dataloader_validation = DataLoader(dataset_val, \n#                                    sampler=SequentialSampler(dataset_val), \n#                                    batch_size=128)\n\ns_dataloader_train = DataLoader(s_dataset_train, \n                              sampler=RandomSampler(s_dataset_train), \n                              batch_size=8)\n\ns_dataloader_validation = DataLoader(s_dataset_val, \n                                   sampler=SequentialSampler(s_dataset_val), \n                                   batch_size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\n# optimizer = AdamW(model.parameters(),\n#                   lr=1e-5, \n#                   eps=1e-8)\ns_optimizer = AdamW(s_model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)\n                  \nepochs = 5\n\n# scheduler = get_linear_schedule_with_warmup(optimizer, \n#                                             num_warmup_steps=0,\n#                                             num_training_steps=len(dataloader_train)*epochs)\n\ns_scheduler = get_linear_schedule_with_warmup(s_optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(s_dataloader_train)*epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.to(device)\n\n# for epoch in tqdm(range(1, epochs+1)):\n    \n#     model.train()\n    \n#     loss_train_total = 0\n\n#     progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n#     for batch in progress_bar:\n\n#         model.zero_grad()\n        \n#         batch = tuple(b.to(device) for b in batch)\n        \n#         inputs = {'input_ids':      batch[0].to(device),\n#                   'attention_mask': batch[1].to(device),\n#                   'labels':         batch[2].to(device),\n#                  }       \n\n#         outputs = model(**inputs)\n        \n#         loss = outputs[0]\n#         loss_train_total += loss.item()\n#         loss.backward()\n\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n#         optimizer.step()\n#         scheduler.step()\n        \n#         progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n        \n#     tqdm.write(f'\\nEpoch {epoch}')\n    \n#     loss_train_avg = loss_train_total/len(dataloader_train)            \n#     tqdm.write(f'Training loss: {loss_train_avg}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_model.to(device)\n\nfor epoch in tqdm(range(1, epochs+1)):\n    \n    s_model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(s_dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        s_model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0].to(device),\n                  'attention_mask': batch[1].to(device),\n                  'labels':         batch[2].to(device),\n                 }       \n\n        outputs = s_model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(s_model.parameters(), 1.0)\n\n        s_optimizer.step()\n        s_scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(s_dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def evaluate(dataloader_val):\n\n#     model.eval()\n    \n#     loss_val_total = 0\n#     predictions, true_vals = [], []\n    \n#     for batch in dataloader_val:\n        \n#         batch = tuple(b.to(device) for b in batch)\n        \n#         inputs = {'input_ids':      batch[0],\n#                   'attention_mask': batch[1],\n#                   'labels':         batch[2],\n#                  }\n\n#         with torch.no_grad():        \n#             outputs = model(**inputs)\n            \n#         loss = outputs[0]\n#         logits = outputs[1]\n#         loss_val_total += loss.item()\n\n#         logits = logits.detach().cpu().numpy()\n#         label_ids = inputs['labels'].cpu().numpy()\n#         predictions.append(logits)\n#         true_vals.append(label_ids)\n    \n#     loss_val_avg = loss_val_total/len(dataloader_val) \n    \n#     predictions = np.concatenate(predictions, axis=0)\n#     true_vals = np.concatenate(true_vals, axis=0)\n            \n#     return loss_val_avg, predictions, true_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def s_evaluate(dataloader_val):\n\n    s_model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = s_model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# val_loss, predictions, true_vals = evaluate(dataloader_validation)\n# val_f1 = f1_score_func(predictions, true_vals)\n\ns_val_loss, s_predictions, s_true_vals = s_evaluate(s_dataloader_validation)\ns_val_f1 = f1_score_func(s_predictions, s_true_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Val Loss = ', val_loss)\n# print('Val F1 = ', val_f1)\n\nprint('Val Loss = ', s_val_loss)\nprint('Val F1 = ', s_val_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\ns_model.to(device)\ntemp_val_loss = 1\n\nfor epoch in tqdm(range(1, epochs+1)):\n    \n    s_model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(s_dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        s_model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0].to(device),\n                  'attention_mask': batch[1].to(device),\n                  'labels':         batch[2].to(device),\n                 }       \n\n        outputs = s_model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(s_model.parameters(), 1.0)\n\n        s_optimizer.step()\n        s_scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(s_dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    s_val_loss, s_predictions, s_true_vals = s_evaluate(s_dataloader_validation)\n    s_val_f1 = f1_score_func(s_predictions, s_true_vals)\n    \n    print(temp_val_loss,s_val_loss)\n    if (temp_val_loss-s_val_loss) < 0: \n        break\n    else:\n        temp_val_loss = s_val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Val Loss = ', s_val_loss)\nprint('Val F1 = ', s_val_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoded_classes = encoder.classes_\n# predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n# true_category = [encoded_classes[x] for x in true_vals]\n\ns_encoded_classes = encoder.classes_\ns_predicted_category = [s_encoded_classes[np.argmax(x)] for x in s_predictions]\ns_true_category = [s_encoded_classes[x] for x in s_true_vals]\nprint(s_encoded_classes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = 0\n# for i in range(len(true_category)):\n#     if true_category[i] == predicted_category[i]:\n#         x += 1\n        \n# print('Accuracy Score = ', x / len(true_category))\n\nx = 0\nfor i in range(len(s_true_category)):\n    if s_true_category[i] == s_predicted_category[i]:\n        x += 1\n        \nprint('s_Accuracy Score = ', x / len(s_true_category))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))\n\ns_confusion_mat = confusion_matrix(y_true = s_true_category, y_pred = s_predicted_category, labels=list(s_encoded_classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(s_true_category)):\n    print(s_true_category[i],s_predicted_category[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# df = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\n# sns.heatmap(df)\n\ns_df = pd.DataFrame(s_confusion_mat, index = list(s_encoded_classes),columns = list(s_encoded_classes))\nsns.heatmap(s_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}