{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Task","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Build a model with an F1 measure greater than 0.59.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preparations","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/bank-customer-churn-modeling/Churn_Modelling.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we delete the columns that are unnecessary for our task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['RowNumber'], data['CustomerId'], data['Surname']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gender** and **Geography** are categorical variables. We will generate dummy columns for them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_dummies = pd.get_dummies(data['Gender'])\ncountry_dummies = pd.get_dummies(data['Geography'])\ndata = pd.concat([data, gender_dummies, country_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's clear the table of already unnecessary columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['Gender'], data['Geography']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data has been studied and prepared for research. Dummy columns are created for categorical variables, you can proceed to the next step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Task Research","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's try the decision tree first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\ny = data['Exited']\nx = data.drop(['Exited'], axis = 1)\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.iloc[perm[:train_end]]\n    validate = df.iloc[perm[train_end:validate_end]]\n    test = df.iloc[perm[validate_end:]]\n    return train, validate, test\n\ndata_train, data_valid, data_test = train_validate_test_split(data, seed=12345)\n\nx_train = data_train.drop(['Exited'], axis = 1)\ny_train = data_train['Exited']\n\nx_valid = data_valid.drop(['Exited'], axis = 1)\ny_valid = data_valid['Exited']\n\nx_test = data_test.drop(['Exited'], axis = 1)\ny_test = data_test['Exited']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier(random_state=12345)\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got an unsatisfactory F1 metric and a good AUC score. We will study columns and other metrics for this dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Exited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A big discrepancy between those who left the program and those remaining, you can try to fix this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"7963/2037","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The discrepancy is almost 4 times !!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Unbalance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, try to play around with the arguments and model type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlinear_model = LogisticRegression(random_state=12345)\nlinear_model.fit(x_train, y_train)\nresult = linear_model.predict(x_valid)\n\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = linear_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model didn’t say anything at all, even though the AUC score is pretty good","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_model = RandomForestClassifier(random_state=12345)\nforest_model.fit(x_train, y_train)\nresult = forest_model.predict(x_valid)\n\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here it’s already better than a decisive tree, while we stop the choice on this model, try to twist its arguments. Also note that the AUC score has grown significantly, we will choose this model for improvement.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estim = None\nbest_depth = None\nbest_f1_score = None\nfor estimators in range (5, 121, 10):\n    for depth in range (5, 26, 5):\n        forest_model = RandomForestClassifier(random_state=12345, max_depth=depth, n_estimators=estimators)\n        forest_model.fit(x_train, y_train)\n        result = forest_model.predict(x_valid)\n        score = f1_score(result, y_valid)\n        if best_f1_score is None:\n            best_f1_score = score\n            best_estim = estimators\n            best_depth = depth\n        elif best_f1_score < score:\n            best_f1_score = score\n            best_estim = estimators\n            best_depth = depth\n        print(\"{0} estim, {1} depth, {2} score\".format(estimators, depth, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Estimators - {0}, Depth - {1}, F1 Score - {2}\".format(best_estim, best_depth, best_f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Already reached 0.547, not bad! For the task you need 0.59, let's try to fix it with the balance of classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestClassifier(random_state=12345, class_weight='balanced', n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_train, y_train)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It became a little better, based on the last step, we will try to do upsampling of some classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def upsample(features, target, repeat):\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    \n    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n\n    return features_upsampled, target_upsampled\n\nx_upsampled, y_upsampled = upsample(x_train, y_train, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth,class_weight='balanced')\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a slightly better metric, try without balance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without balance, it turned out better. Let's try to do Downsampling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ndef downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_downsampled = pd.concat(\n        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n    target_downsampled = pd.concat(\n        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n    \n    features_downsampled, target_downsampled = shuffle(\n        features_downsampled, target_downsampled, random_state=12345)\n    \n    return features_downsampled, target_downsampled\n\nx_downsampled, y_downsampled = downsample(x_train, y_train, 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth, class_weight='balanced')\nforest_model.fit(x_downsampled, y_downsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_downsampled, y_downsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It turned out worse, which does not reach our goal, we will stop at Upsampling without balance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x_upsampled, y_upsampled = upsample(x_train, y_train, 4)\n\nforest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_test)\nf1_score(result, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nprobs = forest_model.predict_proba(x_test)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_test, probs)))\nfpr, tpr, thresholds = roc_curve(y_test, probs)\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It would be nice if there was a unit, but it will do! The result is pretty good, I think we are satisfied. The result is much higher than 0.5, which means that our classifier works well and we are on the right track.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the entire project, a model was constructed that satisfies the conditions of the assignment, other metrics were also used to validate the result. We looked at several variants of models for the problem, opted for the forest as the most accurate model and brought the F1 metric slightly above 0.59 using the Upsampling technique.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}