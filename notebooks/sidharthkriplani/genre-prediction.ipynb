{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport nltk \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies = pd.read_csv('/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv')\ncredits = pd.read_csv('/kaggle/input/tmdb-movie-metadata/tmdb_5000_credits.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FLOW:\n'''\n1. Multi label classification problem\n2. Issue of multiple genres for a movie can be resolved using binary relevance (one hot encoding the possible target values)\n3. The usual data preprocessing of text data will be required here. \n4. Binary relevance of the genres is done using MultiLabelBinarizer\n5. Now we get the features from our summaries (vectorization, word embedding like word2vec, glove, or elmo)\n6. Since the summaries are cleaned, for the above method, we can cut down the total number of words to use based on the frequency. \n7. Also, before feature extraction, we split the data\n8. The total number of distinct genres we have now as our target will determine the number of models we have. \n9. This multiclass problem will be solved using the OvR strategy. \n10. After fitting binary classifiers like Logreg, SVM, perception on the one hot encoded data, we will get the predictions. \n11. Using inverse transform, we will get the actual genres back. \n12. Then we find out the f1 score on the predicted data and actual data. \n13. The probabilities generated were calculated on threshold which we can calculate using k fold cv. This might even improve the f1 score.\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IDEAS:\n'''\n1. Co occurence matrix of all the genres (by a heatmap, atleast of the most frequent genres that can be visualized together)\n2. We can also use a slightly customized VGGNet on movie posters of the movies we have here, pedict their genres, and compare the results\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First it will be purely NLP based genre prediction in which we will not use any feature other than the ones derives using textual data. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since this is a genre prediction task, we will need all the text data so let's clean all of that. Would genre depend on the country of production as well? And such data?\n# first, homepage, id, original_language, release_date, runtime, status, vote_count won't be needed \n\n# some new features that can be derived: whether a movie has a homepage or not, date could be separated into month, year, week, the runtime could \n# be binned, the original language could be replaced with frequency counts. This only in the case if we want to maybe predict the movie revenue?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's automate all the text cleaning steps and see what we get for the provided data\ndef cleanit(x):\n    x = re.sub('[^a-zA-Z#]', ' ', x)\n    x = x.lower()\n    x = list(set([x for x in x.split() if x not in stopwords.words(\"english\")]))\n    wordnet = WordNetLemmatizer()\n    x = [wordnet.lemmatize(x) for x in x]\n    x = [w for w in x if len(w)>3]\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies = movies[~movies.overview.isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies['overview'] = pd.Series([cleanit(w) for w in movies['overview']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GENRES:\n\nmovies.genres.isnull().sum()/len(movies.index)\n# since there are missing values in this feature which is our target, we will create a new dataframe first to split the movies dataframe\ndata = movies[['id', 'title', 'overview', 'genres']]\ndata = data.dropna()\ndata['genres'] = pd.Series([re.findall('\"name\": \"(\\w+)\"}', w) for w in data.genres])\ndata = data.dropna()\n# an idea to wonder about: dictionary of genres might work somewhere?\n# would there be any point to doing visual analysis by deriving a new column 'number_of_genres'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's create a list of unique genre values \nallgenres = sum(data.genres, [])\nlen(set(allgenres))\n# just total 18 genres. Let's create a frequency distribution","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allgenres = nltk.FreqDist(allgenres)\n\nallgenresdf = pd.DataFrame({'Genre': list(allgenres.keys()), \n                              'Count': list(allgenres.values())})\n\ng = allgenresdf.nlargest(columns=\"Count\", n = 50) \nplt.figure(figsize=(12,15)) \nax = sns.barplot(data=g, x= \"Count\", y = \"Genre\") \nax.set(ylabel = 'Count') \nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we create a function which will visualize the most frequent words in the given overview \nalltext = sum(data.overview, [])\nfdist = nltk.FreqDist(alltext)\nwords_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n\nd = words_df.nlargest(columns=\"count\", n = 30) \n  \n# visualize words and frequencies\nplt.figure(figsize=(12,15)) \nax = sns.barplot(data=d, x= \"count\", y = \"word\") \nax.set(ylabel = 'Word') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's one hot encode the target variable now \nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nmlb.fit(data.genres)\ny = mlb.transform(data.genres)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.Series([\" \".join(x) for x in X_train])\nX_train[6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntfidf = TfidfVectorizer(max_df=0.8, max_features=5000)\nX_train, X_test, y_train, y_test = train_test_split(data['overview'], y, test_size=0.2, random_state=9)\nX_train_tfidf = tfidf.fit_transform(pd.Series([\" \".join(x) for x in X_train]))\nX_test_tfidf = tfidf.transform(pd.Series([\" \".join(x) for x in X_test]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score\nlogreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ovr.fit(X_train_tfidf, y_train)\ny_pred = ovr.predict(X_test_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlb.inverse_transform(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_test, y_pred, average=\"micro\")\n# f1 score is not good. So we can try to change the default threshold value of 0.5 and see if the f1 score gets improved. \n# Recall, f1_score is the harmonic mean of recall and precision. The formula does not matter as much as the meaning does, which is simply that\n# f1_score is a combined metric: it is used when neither precision nor recall is favored over the other, so we need both. \n# Why is neither precision nor recall is more important over the other here? Because here, the classes are more than 2, and there is no positve or negative \n# class; there are just many labels we have to assign to the movies. This is what I think. You?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_prob = ovr.predict_proba(X_test_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new = (y_pred_prob >= 0.25).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_test, y_pred_new, average=\"micro\")\n# 0.56 is a good jump. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see how we can use KFold CV \n# INCOMPLETE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NEXT WE COULD TRY: add other non textual features along with the overview to predict the genres, genre prediction using the movie posters,\n# revenue prediction as well ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# we create a new column stating whether a movie has a homepage or not, and \n# then compare it with its revenue being high or not \nmovies['homepage'] = [1 if len(str(w))>3 else 0 for w in movies['homepage']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TAGLINE \n\nmovies.tagline\n# already clean, but has missing values. Do we drop those rows? Depends on the total features we end up with after the data preprocssing. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PRODUCTION COUNTRIES:\n\nmovies['production_countries'] = movies['production_countries'].fillna(str(movies['production_countries'].mode()[0]))\n\nmovies['number_of_production_countries'] = pd.Series([len(w.split('name'))-1 for w in movies.production_countries])\nmovies['production_countries'] = pd.Series([re.findall(\"'name': '([\\w ]+)'}\", w) for w in movies.production_countries])\nmovies['production_countries'] = pd.Series([fillblank(w) for w in movies['production_countries']])\n\ndef ifUSA(lis):\n    for w in lis:\n        if w == 'United States of America':\n            return 1\n        else:\n            return 0\n        \nmovies['production_countries'] = pd.Series([fillblank(w) for w in movies['production_countries']])\nmovies['USA_Producing'] = pd.Series([ifUSA(w) for w in movies['production_countries']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PRODUCTION COMPANIES:\n\nmovies['production_companies'] = movies['production_companies'].fillna(str(movies['production_companies'].mode()[0]))\n\nmovies['production_companies'] = pd.Series([re.findall(\"'name': '([\\w ]+)',\", w) for w in movies.production_companies])\nmovies['#production_companies'] = pd.Series([len(w) for w in movies['production_companies']])\nmovies['production_companies'] = pd.Series([fillblank(w) for w in movies['production_companies']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GENRES:\n\nmovies['genres'] = movies['genres'].fillna(str(movies['genres'].mode()[0]))\nmovies['genres'] = pd.Series([re.findall('\"name\": \"(\\w+)\"}', w) for w in movies.genres])\n# genres are not arranged in alphbetical order either, so we can discard everything \n# but the primary genre \nmovies['genres'] = pd.Series([fillblank(w) for w in movies['genres']])\n\n# an idea to wonder about: dictionary of genres might work somewhere?\n# would there be any point to doing visual analysis by deriving a new column 'number_of_genres'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ORIGINAL TITLE\n\n# what is the different between title and original_title?\nround(len([x for x in movies['title']==movies['original_title'] if x is True])/len(movies.index)*100, 2)\n# 88% of the times the original_title matches the title. Even if we have to drop one\n# what feature do we drop? can we, say, merge? \nmovies[['title', 'original_title']]\n# we stick with the title \nmovies = movies.drop('original_title', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OVERVIEW\n\nmovies.overview\n#already clean. Can only have stopwwords remove now, be tokenized and be lemmatized \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef word_dist(w):\n    w = str(w)\n    lis = set(word_tokenize(w.lower()))-set(stopwords.words('english'))\n    li = []\n    for i in lis:\n        if len(i)>2:    \n            li.append(i)\n    return \" \".join(li)\n\nmovies['overview'] = pd.Series([word_dist(w) for w in movies.overview])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# POPULARITY\n# popularity can be rounded to 2 decimal places\nmovies['popularity'] = round(movies['popularity']*100, 2)\nmovies['popularity'] = movies['popularity'].astype(int)\n# just not sure if this feature will be used or not ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RELEASE DATE\n\nmovies['release_date'] = pd.to_datetime(movies['release_date'])\nmovies['release_date'] = movies['release_date'].fillna(movies['release_date'].mode())\nmovies['release_month'] = pd.Series([pd.to_datetime(w) for w in movies.release_date]).dt.month\nmovies = movies.drop('release_date', axis = 1)\n\nquart = []\nfor i in movies['release_month']:\n    if i <4:\n        quart.append(1)\n    elif i<7 and i>3:\n        quart.append(2)\n    elif i<10 and i>6:\n        quart.append(3)\n    else:\n        quart.append(4)\nmovies['release_month_quarter'] = pd.Series(quart)\nmovies = movies.drop('release_month', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ORIGINAL LANGUAGE:\n\n# need to check the variations in original_language\nround(movies['original_language'].value_counts()/len(movies.index)*100, 2)\n# 85% + movies are of the english origin. We could either remove this feature or \n# rather derive a simple new feature where value is 1 if original language is \n# English else 0\nrevenue_percentage_m_train = movies[['original_language', 'revenue']].groupby('original_language').sum()/sum(movies.revenue)\nround(revenue_percentage_m_train*100, 2).sort_values('revenue', ascending = False)\n# 96% of the revenue is generated from movies with their official_language as 'en'\n# we can either drop the other languages or just drop the whole feature itself\nmovies = movies.drop(['original_language'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SPOKEN LANGUAGES:\n\nmovies['spoken_languages'] = movies['spoken_languages'].fillna(str(movies['spoken_languages'].mode()))\nmovies['number_of_spoken_languages'] = pd.Series([len(w.split('{'))-1 for w in movies.spoken_languages])\nmovies['spoken_languages'] = pd.Series([re.findall(\"'name': '(\\w+)'\", w) for w in movies.spoken_languages])\ndef has_english(lis):\n    for w in lis:\n        if w=='English':\n            return 1\n        else:\n            return 0\n\nmovies['spoken_languages'] = [fillblank(w) for w in movies['spoken_languages']]\nmovies['has_english'] = pd.Series([has_english(w) for w in movies.spoken_languages])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can divide the datasets into the following specificities at a later time: \n'''\n1. Dataset with features that will help us make a predictive model for revenue prediction: mpm\n2. A complete textual dataset to apply NLP on to predict the genres: mgp\n3. A dataset where we import data from external source to get poster path so we can do a CNN genre prediction as well: mgpp\n4. A recommender: mr (will be using all kinds of filtering: demographic, content based and collaborative)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy \nmovies.columns\nmpm = copy.deepcopy(movies)\nmgp = copy.deepcopy(movies)\nmgpp = copy.deepcopy(movies)\nmr = copy.deepcopy(movies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First we will focus on the mgp for genre prediction using NLP","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}