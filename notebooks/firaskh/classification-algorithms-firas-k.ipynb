{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Water Quality - First Dataset**\n\n\nData Mining Project\n\n\n\nFiras k","metadata":{"id":"Ni0_Vrkna8HH"}},{"cell_type":"code","source":"              #Working on our fisrt dataset - Water Quality (Prediction Model - Water Potability Classification)\nimport pandas as pd\nimport numpy as np\n              #importing both numpy and pandas.\ndataset1=pd.read_csv('../input/water-potability/water_potability.csv', header=0) \n              # with header=0 is like the default value, treating firtst row as header.\n\nprint('No. of Instances = %d' % (dataset1.shape[0]))\nprint('No. of Attributes = %d' % (dataset1.shape[1])) \n              #Printing no. of objects/instances  and attributes,  shape[0]give us no. of raws, shape[1] no. of columns\ndataset1.head()\n              #Printing the first 5 records on the dataframe, We can notice that all the missing values left blanked in th original file.\n              #While uploading to dataset1 dataframe, it filled all blank with NaN. a standard missing value ( Pandas will recognize both empty cells and “NA” types as missing values)\n\n","metadata":{"id":"y0FsUpIWavgF","outputId":"b3bfec19-32cc-4ba4-ba56-5ab48c067dd6","execution":{"iopub.status.busy":"2021-08-17T00:54:23.682722Z","iopub.execute_input":"2021-08-17T00:54:23.683096Z","iopub.status.idle":"2021-08-17T00:54:23.721271Z","shell.execute_reply.started":"2021-08-17T00:54:23.683067Z","shell.execute_reply":"2021-08-17T00:54:23.720188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of missing values = ') \nfor col in dataset1.columns:\n    print('\\t%s:%d' % (col,dataset1[col].isna().sum()))\n        # for loop syntex to print how many missing values in each column. isna to count how many NaN\n        # \\t%s:%d this is a formating, \\t means adding a tab the val %s (string) then %d a decimal no.) \n\nprint('\\n Number of rows contain missing  = %d ' % (dataset1.isnull().any(axis = 1).sum()) ) \n        #Printing no. of rows/records/objects that containn at least one missing value.\n\n","metadata":{"id":"-YNfIFWfkypC","outputId":"d76067b4-c7de-4b21-80ef-971752261201","execution":{"iopub.status.busy":"2021-08-17T00:54:23.72317Z","iopub.execute_input":"2021-08-17T00:54:23.723586Z","iopub.status.idle":"2021-08-17T00:54:23.736382Z","shell.execute_reply.started":"2021-08-17T00:54:23.723542Z","shell.execute_reply":"2021-08-17T00:54:23.735297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making sure for all columns' datatype\ndataset1.dtypes","metadata":{"id":"EkhECm7kCKQq","outputId":"97e11b4c-a273-4c87-ec82-fca462ece231","execution":{"iopub.status.busy":"2021-08-17T00:54:23.738116Z","iopub.execute_input":"2021-08-17T00:54:23.738518Z","iopub.status.idle":"2021-08-17T00:54:23.749731Z","shell.execute_reply.started":"2021-08-17T00:54:23.738484Z","shell.execute_reply":"2021-08-17T00:54:23.748614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Dealing with missing numbers\n\n*ph*: 491\n\n*Sulfate*:781\n\n*Trihalomethanes*:162\n\nTotal missing cells = 1434\n\nbut in one row we may have multiple missing cells, therefore the missing rows are 1265 less than no. of missing cells.\n\n\n**The procedure will be: imputing/replacing the missing values with mean/meadian value for each attribute**\n","metadata":{"id":"Sa2ib6Nd-TCP"}},{"cell_type":"code","source":"          #Handeling Missing data\n          #replacing with Mean vlaue and Median value and check both for accuracy\n          #Mean vlaue\ndataset1_noNaN1=dataset1\ndataset1_noNaN1=dataset1_noNaN1.fillna(dataset1_noNaN1.mean()) #replacing al NaN with Mean of each column\n#dataset1_noNaN1.head()\n         \n          #Median vlaue- this was a test to check performance if we replaced by mean or by median- the difference is min, and we consider the mean.\ndataset1_noNaN2=dataset1\ndataset1_noNaN2=dataset1_noNaN2.fillna(dataset1_noNaN2.median()) #replacing al NaN with Median of each column\n#dataset1_noNaN2.head()\n","metadata":{"id":"ACBcktTQC6b0","execution":{"iopub.status.busy":"2021-08-17T00:54:23.751726Z","iopub.execute_input":"2021-08-17T00:54:23.752139Z","iopub.status.idle":"2021-08-17T00:54:23.771709Z","shell.execute_reply.started":"2021-08-17T00:54:23.752105Z","shell.execute_reply":"2021-08-17T00:54:23.770349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Checking for outliers","metadata":{"id":"ZXe_rkG0u-c9"}},{"cell_type":"code","source":"#applying boxplot to detect outliers\n%matplotlib inline \n        # library to be used for plotting boxplot\n\ndataset1_noNaN1.boxplot(figsize=(20,10))\n","metadata":{"id":"wihHGV9TvHTT","outputId":"54ad9a4a-ca13-44d7-8892-27560196c959","execution":{"iopub.status.busy":"2021-08-17T00:54:23.773311Z","iopub.execute_input":"2021-08-17T00:54:23.773724Z","iopub.status.idle":"2021-08-17T00:54:24.234464Z","shell.execute_reply.started":"2021-08-17T00:54:23.773682Z","shell.execute_reply":"2021-08-17T00:54:24.233273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying boxplot to detect outliers - Median not to be used \n#%matplotlib inline \n      # library to be used for plotting boxplot\n\n#dataset1_noNaN2.boxplot(figsize=(20,10))\n","metadata":{"id":"AWGP6t6pwmMN","execution":{"iopub.status.busy":"2021-08-17T00:54:24.235858Z","iopub.execute_input":"2021-08-17T00:54:24.236198Z","iopub.status.idle":"2021-08-17T00:54:24.240311Z","shell.execute_reply.started":"2021-08-17T00:54:24.236167Z","shell.execute_reply":"2021-08-17T00:54:24.239187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We** can see that a lot of outliers appear in solid, but we will creat new data frame with the standard Z score.\n\n\n\n","metadata":{"id":"-8PHpX3ewD0Y"}},{"cell_type":"code","source":"Z = (dataset1_noNaN1-dataset1_noNaN1.mean())/dataset1_noNaN1.std() \n          # new dataframe z for dataset1 no NaN/replaced by mean\n\nZ","metadata":{"id":"VjByge-AxG0F","outputId":"78156b92-9ad2-4fa6-fd4a-3bce00471ec6","execution":{"iopub.status.busy":"2021-08-17T00:54:24.241573Z","iopub.execute_input":"2021-08-17T00:54:24.241914Z","iopub.status.idle":"2021-08-17T00:54:24.299326Z","shell.execute_reply.started":"2021-08-17T00:54:24.241862Z","shell.execute_reply":"2021-08-17T00:54:24.298024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of rows before discarding outliers Z = %d' % (Z.shape[0]))\n\n\nZ1 = Z.loc[((Z > -3).sum(axis=1)==10) & ((Z < 3).sum(axis=1)==10),:]\nprint('Number of rows after discarding outliers values = %d' % (Z1.shape[0]))\nZ1\n","metadata":{"id":"r9wKjRvDyMf_","outputId":"22f198b6-92af-4398-e294-eb79839540c1","execution":{"iopub.status.busy":"2021-08-17T00:54:24.302516Z","iopub.execute_input":"2021-08-17T00:54:24.302883Z","iopub.status.idle":"2021-08-17T00:54:24.341144Z","shell.execute_reply.started":"2021-08-17T00:54:24.302831Z","shell.execute_reply":"2021-08-17T00:54:24.339936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#outliers with Z score\n#applying boxplot to detect outliers\n%matplotlib inline \n        # library to be used for plotting boxplot\n\nZ.boxplot(figsize=(15,10))\n","metadata":{"id":"38iVd6c62VON","outputId":"78d9de78-76df-4a85-93ae-5552dd73cb2b","execution":{"iopub.status.busy":"2021-08-17T00:54:24.343017Z","iopub.execute_input":"2021-08-17T00:54:24.343332Z","iopub.status.idle":"2021-08-17T00:54:24.723931Z","shell.execute_reply.started":"2021-08-17T00:54:24.343294Z","shell.execute_reply":"2021-08-17T00:54:24.722987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No. of rows after discarding outliers = 3129","metadata":{"id":"kvEGT495YKOo"}},{"cell_type":"code","source":"#Now another way to create a new dataframe without outliers that we will use it\n #is by using SciPy\n \n#SciPy is a free (as Sckitleaard) and open-source Python library used for scientific computing and technical computing. \n#SciPy contains modules for optimization, linear algebra, integration, interpolation, \n\n\n#using scipy libraby\nfrom scipy import stats\nz_scores = stats.zscore(dataset1_noNaN1)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\ndata1 = dataset1_noNaN1[filtered_entries]\n\nprint(data1)\n\n ","metadata":{"id":"Nar-n-32YUSD","outputId":"2f6e8568-25af-43ff-fcae-5bc262038818","execution":{"iopub.status.busy":"2021-08-17T00:54:24.725228Z","iopub.execute_input":"2021-08-17T00:54:24.725553Z","iopub.status.idle":"2021-08-17T00:54:25.560395Z","shell.execute_reply.started":"2021-08-17T00:54:24.725523Z","shell.execute_reply":"2021-08-17T00:54:25.559192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlations\n","metadata":{"id":"CTpfrhMYUHNY"}},{"cell_type":"code","source":"corr_dataset1=data1.corr()\ncorr_dataset1","metadata":{"id":"ihoVwiMNUY3w","outputId":"21da6fcf-cb9d-4c12-9c75-160dabaac9fa","execution":{"iopub.status.busy":"2021-08-17T00:54:25.561843Z","iopub.execute_input":"2021-08-17T00:54:25.562178Z","iopub.status.idle":"2021-08-17T00:54:25.586686Z","shell.execute_reply.started":"2021-08-17T00:54:25.562147Z","shell.execute_reply":"2021-08-17T00:54:25.585637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation Seaborn Heatmap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.heatmap(data1.corr());","metadata":{"id":"VyYE4HvtUwXU","outputId":"135a18bb-7b8d-4eb3-cdcf-e97fc1b730cb","execution":{"iopub.status.busy":"2021-08-17T00:54:25.588685Z","iopub.execute_input":"2021-08-17T00:54:25.589006Z","iopub.status.idle":"2021-08-17T00:54:26.119071Z","shell.execute_reply.started":"2021-08-17T00:54:25.588971Z","shell.execute_reply":"2021-08-17T00:54:26.118025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Increase the size of the heatmap.\nplt.figure(figsize=(16, 6))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(data1.corr(), vmin=-1, vmax=1, annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","metadata":{"id":"Ao-6sHXgVpsA","outputId":"6a4d8851-ad35-4eaf-bfe2-7430bcf648ab","execution":{"iopub.status.busy":"2021-08-17T00:54:26.120103Z","iopub.execute_input":"2021-08-17T00:54:26.1204Z","iopub.status.idle":"2021-08-17T00:54:27.193297Z","shell.execute_reply.started":"2021-08-17T00:54:26.120371Z","shell.execute_reply":"2021-08-17T00:54:27.192507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trying playing with coloring\nplt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(data1.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n# save heatmap as .png file\n# dpi - sets the resolution of the saved image in dots/inches\n# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\nplt.savefig('heatmap_dataset1.png', dpi=300, bbox_inches='tight')","metadata":{"id":"7r4y64QhWgzp","outputId":"46d67656-af68-4088-89be-f2fde9edc03e","execution":{"iopub.status.busy":"2021-08-17T00:54:27.194315Z","iopub.execute_input":"2021-08-17T00:54:27.194753Z","iopub.status.idle":"2021-08-17T00:54:29.257613Z","shell.execute_reply.started":"2021-08-17T00:54:27.194721Z","shell.execute_reply":"2021-08-17T00:54:29.256611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making sure for all columns' datatype\ndata1.dtypes","metadata":{"id":"b9vABbAegogq","outputId":"0dfce0fa-2450-4289-e609-3cd4fe97a95d","execution":{"iopub.status.busy":"2021-08-17T00:54:29.258947Z","iopub.execute_input":"2021-08-17T00:54:29.259254Z","iopub.status.idle":"2021-08-17T00:54:29.26578Z","shell.execute_reply.started":"2021-08-17T00:54:29.259222Z","shell.execute_reply":"2021-08-17T00:54:29.265109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All out Data Mining task will be on data1 (after removing outliers) ","metadata":{"id":"ZyENKVKla8CR"}},{"cell_type":"code","source":"#exporting data Frame to CSV\ndata1.to_csv('data1.csv', index=False)","metadata":{"id":"SqYTtaftvGMo","execution":{"iopub.status.busy":"2021-08-17T00:54:29.266839Z","iopub.execute_input":"2021-08-17T00:54:29.267309Z","iopub.status.idle":"2021-08-17T00:54:29.345062Z","shell.execute_reply.started":"2021-08-17T00:54:29.267278Z","shell.execute_reply":"2021-08-17T00:54:29.34399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##DATA MINING TASK\nClassification by Decesion Tree using **scikit liabrary**","metadata":{"id":"idITQjHBC8bq"}},{"cell_type":"code","source":"        #Load libraries-\n        #Scikit-learn is a free software machine learning library for the Python programming language. \n        #It features various classification, regression and clustering algorithms including support vector machines.\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n        \n        #split dataset in features and target variable\nfeature_cols = ['ph', 'Hardness', 'Solids', \n                'Chloramines','Sulfate','Conductivity',\n                'Organic_carbon','Trihalomethanes','Turbidity']\nX = data1[feature_cols] # Features \ny = data1.Potability # Target variable\n\n     # Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n          # 70% training and 30% test\n          # Split dataset into training set and test set\n         # Create Decision Tree classifer object\ntree = DecisionTreeClassifier()\n         # criterion: default is Gini, max_depth: defualt is non,\n                              #splitter : default is best\n\n          # Train Decision Tree Classifer\ntree = tree.fit(X_train,y_train)\n\n          #Predict the response for test dataset\ny_pred = tree.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","metadata":{"id":"0N8JqXcl63U-","outputId":"77bb3c50-c449-4f9e-d057-9e08fc386d77","execution":{"iopub.status.busy":"2021-08-17T00:54:29.346403Z","iopub.execute_input":"2021-08-17T00:54:29.346708Z","iopub.status.idle":"2021-08-17T00:54:29.738972Z","shell.execute_reply.started":"2021-08-17T00:54:29.346676Z","shell.execute_reply":"2021-08-17T00:54:29.738022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_train\n#y_test","metadata":{"id":"9F1PMheFu9fU","execution":{"iopub.status.busy":"2021-08-17T00:54:29.740141Z","iopub.execute_input":"2021-08-17T00:54:29.740422Z","iopub.status.idle":"2021-08-17T00:54:29.744371Z","shell.execute_reply.started":"2021-08-17T00:54:29.740395Z","shell.execute_reply":"2021-08-17T00:54:29.743297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nVisualizing Decision Trees\nYou can use Scikit-learn's export_graphviz function for display the tree For plotting tree, you also need to install graphviz and pydotplus.","metadata":{"id":"Lv0LcUYXKx33"}},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('water.png')\nImage(graph.create_png())","metadata":{"id":"fhrpD1GiKzoc","outputId":"bb5a23ba-c8f0-455c-cfc9-16edd0b0e6c5","execution":{"iopub.status.busy":"2021-08-17T00:54:29.745566Z","iopub.execute_input":"2021-08-17T00:54:29.745831Z","iopub.status.idle":"2021-08-17T00:54:29.865815Z","shell.execute_reply.started":"2021-08-17T00:54:29.745806Z","shell.execute_reply":"2021-08-17T00:54:29.862452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix for first DC tree module","metadata":{"id":"6qyZyb_Oy4Xe"}},{"cell_type":"code","source":"# import the metrics class \n\nfrom sklearn import metrics\ncnf_matrix_1 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix_1","metadata":{"id":"t2udPy3cxFhE","outputId":"337d2569-bc26-4404-93ff-81ebe50d3464","execution":{"iopub.status.busy":"2021-08-17T00:55:09.208393Z","iopub.execute_input":"2021-08-17T00:55:09.208753Z","iopub.status.idle":"2021-08-17T00:55:09.221487Z","shell.execute_reply.started":"2021-08-17T00:55:09.208722Z","shell.execute_reply":"2021-08-17T00:55:09.220356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred)) #classification report showing the accuracy, recall and precision","metadata":{"id":"0EqNhvRzeG-W","outputId":"432e1c6a-1dc8-42c2-ff80-1a74fbc1f9e5","execution":{"iopub.status.busy":"2021-08-17T00:55:11.835991Z","iopub.execute_input":"2021-08-17T00:55:11.836372Z","iopub.status.idle":"2021-08-17T00:55:11.85017Z","shell.execute_reply.started":"2021-08-17T00:55:11.836326Z","shell.execute_reply":"2021-08-17T00:55:11.848039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import required modules\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix_1), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n","metadata":{"id":"4Z8dhX3lywRz","outputId":"e75732e3-c9b5-4f35-d354-432efc863bc6","execution":{"iopub.status.busy":"2021-08-17T00:55:16.282172Z","iopub.execute_input":"2021-08-17T00:55:16.28267Z","iopub.status.idle":"2021-08-17T00:55:16.629186Z","shell.execute_reply.started":"2021-08-17T00:55:16.282636Z","shell.execute_reply":"2021-08-17T00:55:16.628002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Optimizing Decision Tree Performance\nScikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning.\n","metadata":{"id":"ZzLbdRhzMRdS"}},{"cell_type":"code","source":"#Create Decision Tree classifer object\ntree2 = DecisionTreeClassifier(criterion=\"entropy\",max_depth=5)\n\n# Train Decision Tree Classifer\ntree2 = tree2.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = tree2.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"id":"HgM2sJwyMWVi","outputId":"028dfa80-35df-45e1-af5b-633248b81e75","execution":{"iopub.status.busy":"2021-08-17T00:55:19.616692Z","iopub.execute_input":"2021-08-17T00:55:19.617063Z","iopub.status.idle":"2021-08-17T00:55:19.651675Z","shell.execute_reply.started":"2021-08-17T00:55:19.617031Z","shell.execute_reply":"2021-08-17T00:55:19.65062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing Tree2\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(tree2, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('water2.png')\nImage(graph.create_png())","metadata":{"id":"knlWfy6WPdtt","outputId":"c5c94f1f-c4d0-497f-9c5a-2128f17e74e5","execution":{"iopub.status.busy":"2021-08-17T00:55:22.412272Z","iopub.execute_input":"2021-08-17T00:55:22.412739Z","iopub.status.idle":"2021-08-17T00:55:22.433776Z","shell.execute_reply.started":"2021-08-17T00:55:22.412704Z","shell.execute_reply":"2021-08-17T00:55:22.432085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the metrics class \n\nfrom sklearn import metrics\ncnf_matrix_2 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix_2","metadata":{"id":"gtEb-SfqzeXa","outputId":"470fc741-e8c7-49ba-cb51-39224b2e39d4","execution":{"iopub.status.busy":"2021-08-17T00:55:25.274389Z","iopub.execute_input":"2021-08-17T00:55:25.274817Z","iopub.status.idle":"2021-08-17T00:55:25.285111Z","shell.execute_reply.started":"2021-08-17T00:55:25.274782Z","shell.execute_reply":"2021-08-17T00:55:25.283924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred)) #classification report showing the accuracy, recall and precision","metadata":{"id":"a3DaeQM6fcb4","outputId":"dd33ec16-46a7-4162-fbcf-4d63d6f482e0","execution":{"iopub.status.busy":"2021-08-17T00:55:27.850168Z","iopub.execute_input":"2021-08-17T00:55:27.850608Z","iopub.status.idle":"2021-08-17T00:55:27.863447Z","shell.execute_reply.started":"2021-08-17T00:55:27.850572Z","shell.execute_reply":"2021-08-17T00:55:27.862333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import required modules\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix_2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n","metadata":{"id":"TpwsYxiizoRk","outputId":"1c690846-47ba-428e-ca80-41f6e1d85c4e","execution":{"iopub.status.busy":"2021-08-17T00:55:30.21172Z","iopub.execute_input":"2021-08-17T00:55:30.212097Z","iopub.status.idle":"2021-08-17T00:55:30.515801Z","shell.execute_reply.started":"2021-08-17T00:55:30.212067Z","shell.execute_reply":"2021-08-17T00:55:30.514761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the model for overfitting\n\n########################################\n# Training and Test set creation\n#########################################\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n#########################################\n# Model fitting and evaluation\n#########################################\n\nmaxdepths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n\ntrainAcc = np.zeros(len(maxdepths))\ntestAcc = np.zeros(len(maxdepths))\n\nindex = 0\nfor depth in maxdepths:\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    clf = clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc[index] = accuracy_score(y_train, y_predTrain)\n    testAcc[index] = accuracy_score(y_test, y_predTest)\n    index += 1\n    \n#########################################\n# Plot of training and test accuracies\n#########################################\n    \nplt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')","metadata":{"id":"P6uXrGNRhXtf","outputId":"b1f44173-39ef-44bf-b734-0149ae5248f3","execution":{"iopub.status.busy":"2021-08-17T00:55:33.505122Z","iopub.execute_input":"2021-08-17T00:55:33.505486Z","iopub.status.idle":"2021-08-17T00:55:34.118614Z","shell.execute_reply.started":"2021-08-17T00:55:33.505457Z","shell.execute_reply":"2021-08-17T00:55:34.117793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Cross-Validation\nTo eliminate over-fitting, we can apply cross-validation. We are going to apply k-fold cross-validation by spliting the original data set into k subsets and use one of the subsets as the testing set and the remaining as the training sets.\n\nThis process iterated k times until every subset have been used as the testing set. \n\n\n##10-folds - (k-Folds cross-validation)","metadata":{"id":"uQMWHMuUlCo_"}},{"cell_type":"code","source":"#Cross-Validation\n#To eliminate over-fitting, we can apply cross-validation.\n#going to apply k-fold cross-validation.\n#split the original data set into k subsets and use one of the subsets as the testing set and the remaining as the training sets.\n#This process iterated k times until every subset have been used as the testing set.\n# 10-folds - k Folds cross validation cv=10 parameter for K=10 in cross_val_score function\n\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n\ndtc = DecisionTreeClassifier()\ncv_scores = cross_val_score(dtc, X, y, cv=10) \nsns.distplot(cv_scores)\nplt.title('Average score: {}'.format(np.mean(cv_scores)))","metadata":{"id":"yNbyQSecdDH9","outputId":"e373e2e4-a293-4c74-9ac6-ad5a445c5f9c","execution":{"iopub.status.busy":"2021-08-17T00:55:37.622144Z","iopub.execute_input":"2021-08-17T00:55:37.622639Z","iopub.status.idle":"2021-08-17T00:55:38.383711Z","shell.execute_reply.started":"2021-08-17T00:55:37.622597Z","shell.execute_reply":"2021-08-17T00:55:38.382909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Parameter Tuning\n\nIn every classification technique, \nThere are some parameters that can be tuned to optimize the classification. Some parameters that can be tuned in the decision tree is max depth (the depth of the tree), max feature (the feature used to classify), criterion, and splitter.\n\n##To tune parameter is to use Grid Search. \nBasically, it explores a range of parameters and finds the best combination of parameters. Then repeat the process several times until the best parameters are discovered.\n\nWe will also use Stratified k-fold cross-validation that will prevent a certain class only split them to the same subset","metadata":{"id":"kdbkuVEJjeEj"}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\ndtc = DecisionTreeClassifier()\n#testing for multiple depth and features\nparameter_grid = {'criterion': ['gini', 'entropy'],\n                  'splitter': ['best', 'random'],\n                  'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n                  'max_features': [1, 2, 3, 4,5,6,7,8,9]}\n\n\ncross_validation = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(dtc, param_grid=parameter_grid, cv=cross_validation)\n\ngrid_search.fit(X, y)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\n\ndtc = grid_search.best_estimator_\ndtc","metadata":{"id":"K5RdXSgXhADF","outputId":"d2310cd2-d6cb-44b5-ab25-a923924ba860","execution":{"iopub.status.busy":"2021-08-17T00:55:41.409275Z","iopub.execute_input":"2021-08-17T00:55:41.409933Z","iopub.status.idle":"2021-08-17T00:56:12.722131Z","shell.execute_reply.started":"2021-08-17T00:55:41.409898Z","shell.execute_reply":"2021-08-17T00:56:12.721015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we got the best parameters for this model. \nWe can directly assign it to the decision tree classifier and we can check all of its property. \n##checking it’s accuracy- with optimal parameter","metadata":{"id":"z_rvWn18ly8I"}},{"cell_type":"code","source":"#got the best parameters for this model. \n#We can directly assign it to the decision tree classifier and we can check all of its property. \n#checking it’s accuracy.\ncv_scores = cross_val_score(dtc, X, y)\nsns.distplot(cv_scores)\nplt.title('Average score: {}'.format(np.mean(cv_scores)))\n","metadata":{"id":"B25ssHw3kDyH","outputId":"f4748ff2-5882-4a71-ad7f-0c3e9185c046","execution":{"iopub.status.busy":"2021-08-17T00:56:12.723973Z","iopub.execute_input":"2021-08-17T00:56:12.724394Z","iopub.status.idle":"2021-08-17T00:56:13.020612Z","shell.execute_reply.started":"2021-08-17T00:56:12.724351Z","shell.execute_reply":"2021-08-17T00:56:13.019303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy getting better","metadata":{"id":"MtZRQ8vEmENO"}},{"cell_type":"markdown","source":" **KNN Classifier **\n\nKnn from sklearn","metadata":{"id":"12CUlX1aTDjx"}},{"cell_type":"code","source":"\n       \n#Load libraries-\n        #Scikit-learn is a free software machine learning library for the Python programming language. \n        #It features various classification, regression and clustering algorithms including support vector machines.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n        \n        #split dataset in features and target variable\nfeature_cols = ['ph', 'Hardness', 'Solids', \n                'Chloramines','Sulfate','Conductivity',\n                'Organic_carbon','Trihalomethanes','Turbidity']\nX = data1[feature_cols] # Features \ny = data1.Potability # Target variable\n\n     # Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n          # 70% training and 30% test\n          # Split dataset into training set and test set\n\n#KNN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=30) # no of neighbors\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\ncnf_matrix_3 = metrics.confusion_matrix(y_test, y_pred) \ncnf_matrix_3\n\n\n","metadata":{"id":"UBllmoSFXhrb","outputId":"f6801830-ee85-42e6-d7b4-a004d82fa36e","execution":{"iopub.status.busy":"2021-08-17T00:56:17.883078Z","iopub.execute_input":"2021-08-17T00:56:17.883452Z","iopub.status.idle":"2021-08-17T00:56:18.015574Z","shell.execute_reply.started":"2021-08-17T00:56:17.883419Z","shell.execute_reply":"2021-08-17T00:56:18.014333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing confusion Matrix \n# import required modules\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix_3), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","metadata":{"id":"M55P3jXpZG0O","outputId":"ff7effef-4ccf-4bd4-a6d6-86a8661410eb","execution":{"iopub.status.busy":"2021-08-17T00:56:22.314769Z","iopub.execute_input":"2021-08-17T00:56:22.315156Z","iopub.status.idle":"2021-08-17T00:56:22.65066Z","shell.execute_reply.started":"2021-08-17T00:56:22.315124Z","shell.execute_reply":"2021-08-17T00:56:22.649877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##KNN  testing different K values and check the accuracy over the change in both test and train data\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n      #split dataset in features and target variable\nfeature_cols = ['ph', 'Hardness', 'Solids', \n                'Chloramines','Sulfate','Conductivity',\n                'Organic_carbon','Trihalomethanes','Turbidity']\nX = data1[feature_cols] # Features \ny = data1.Potability # Target variable\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n# To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n#Split dataset into training set and test set\n# 70% training and 30% test\n\nnumNeighbors = [1, 5, 7, 10, 15, 20, 25, 30,35,40, 50, 60, 70, 80, 90, 100, 150]\ntrainAcc = []\ntestAcc = []\n\nfor k in numNeighbors:\n    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n    clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc.append(accuracy_score(y_train, y_predTrain))\n    testAcc.append(accuracy_score(y_test, y_predTest))\n\nplt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')","metadata":{"id":"OvBCJSFwenKd","outputId":"ab8ce002-5594-4217-ab1f-476147843c84","execution":{"iopub.status.busy":"2021-08-17T00:56:25.611198Z","iopub.execute_input":"2021-08-17T00:56:25.611692Z","iopub.status.idle":"2021-08-17T00:56:28.421799Z","shell.execute_reply.started":"2021-08-17T00:56:25.61166Z","shell.execute_reply":"2021-08-17T00:56:28.420803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Cross-Validation for KNN Classification\nIn k-fold cross-validation, the original sample is randomly partitioned into k \nequal size subsamples.\n\n","metadata":{"id":"UFWFd8nxoFr3"}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n\nknn = KNeighborsClassifier(n_neighbors = 100)\n\nscores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    # X,y will automatically devided by 10 folder, the scoring I will still use the accuracy\n\nprint(scores)\n  # print all 5 times scores \n# then I will do the \nprint(scores.mean())\n#average about these  scores to get more accuracy score.\n","metadata":{"id":"UQWewehCopBr","outputId":"f2124928-d42f-48a0-99ac-b454ea9c88b4","execution":{"iopub.status.busy":"2021-08-17T00:56:31.885928Z","iopub.execute_input":"2021-08-17T00:56:31.886468Z","iopub.status.idle":"2021-08-17T00:56:32.162239Z","shell.execute_reply.started":"2021-08-17T00:56:31.886436Z","shell.execute_reply":"2021-08-17T00:56:32.161255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we could choose differenct neighbors to see which K is the best K.\n","metadata":{"id":"fXEeOrzjqiyE"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n%matplotlib inline\n# choose k between 1 to 300\nk_range = range(1, 300)\nk_scores = []\n# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    k_scores.append(scores.mean())\n# plot to see clearly\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')\nplt.show()","metadata":{"id":"GeZLs225mctF","outputId":"ac20f6e9-2b11-4ad2-a0d4-74fb61b6c9f2","execution":{"iopub.status.busy":"2021-08-17T00:56:34.985536Z","iopub.execute_input":"2021-08-17T00:56:34.985927Z","iopub.status.idle":"2021-08-17T00:58:02.935804Z","shell.execute_reply.started":"2021-08-17T00:56:34.98588Z","shell.execute_reply":"2021-08-17T00:58:02.934981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" \n **Logistic regression** \n\nLogisticRegression from sklearn\n","metadata":{"id":"orxIaHIJ2DNm"}},{"cell_type":"code","source":"        #Load libraries-\n        #Scikit-learn is a free software machine learning library for the Python programming language. \n        #It features various classification, regression and clustering algorithms including support vector machines.\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n        \n        #split dataset in features and target variable\nfeature_cols = ['ph', 'Hardness', 'Solids', \n                'Chloramines','Sulfate','Conductivity',\n                'Organic_carbon','Trihalomethanes','Turbidity']\nX = data1[feature_cols] # Features \ny = data1.Potability # Target variable\n\n     # Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n          # 70% training and 30% test\n          # Split dataset into training set and test set\n","metadata":{"id":"8FwXAf8B2e8N","execution":{"iopub.status.busy":"2021-08-17T00:58:02.937069Z","iopub.execute_input":"2021-08-17T00:58:02.937494Z","iopub.status.idle":"2021-08-17T00:58:02.947546Z","shell.execute_reply.started":"2021-08-17T00:58:02.937465Z","shell.execute_reply":"2021-08-17T00:58:02.946245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\n\n#\ny_pred=logreg.predict(X_test)","metadata":{"id":"hXOcxZtn2Gbx","execution":{"iopub.status.busy":"2021-08-17T00:58:02.949862Z","iopub.execute_input":"2021-08-17T00:58:02.950216Z","iopub.status.idle":"2021-08-17T00:58:03.001527Z","shell.execute_reply.started":"2021-08-17T00:58:02.950182Z","shell.execute_reply":"2021-08-17T00:58:03.000142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the metrics class\nfrom sklearn import metrics\n\ncnf_matrix_4 = metrics.confusion_matrix(y_test, y_pred) \nprint(cnf_matrix_4)\nprint(classification_report(y_test, y_pred))","metadata":{"id":"3IpT1-W42iuI","outputId":"121654a6-6411-4973-b0e5-cb0eadb570a3","execution":{"iopub.status.busy":"2021-08-17T00:58:03.003606Z","iopub.execute_input":"2021-08-17T00:58:03.004109Z","iopub.status.idle":"2021-08-17T00:58:03.032917Z","shell.execute_reply.started":"2021-08-17T00:58:03.004064Z","shell.execute_reply":"2021-08-17T00:58:03.029709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import required modules\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix_4), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n","metadata":{"id":"pjx768wM55Oh","outputId":"f7f156d6-d391-4906-f173-7260f1e8be88","execution":{"iopub.status.busy":"2021-08-17T00:58:03.034488Z","iopub.execute_input":"2021-08-17T00:58:03.034981Z","iopub.status.idle":"2021-08-17T00:58:03.310238Z","shell.execute_reply.started":"2021-08-17T00:58:03.034936Z","shell.execute_reply":"2021-08-17T00:58:03.309501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n","metadata":{"id":"u3cO6Ddj6mPe","outputId":"e7eb47a2-7ad0-457d-ca67-5efb6fedf6f4","execution":{"iopub.status.busy":"2021-08-17T00:58:03.311245Z","iopub.execute_input":"2021-08-17T00:58:03.311649Z","iopub.status.idle":"2021-08-17T00:58:03.325603Z","shell.execute_reply.started":"2021-08-17T00:58:03.311619Z","shell.execute_reply":"2021-08-17T00:58:03.324421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we got a classification rate of 64.4%, considered as fine accuracy.\n\nPrecision: Precision is about being precise, i.e., how accurate our model is. \nIn other words, you can say, when a model makes a prediction, how often it is correct. \nIn our prediction case, when our Logistic Regression model predicted water are going to be potable , that sample has 0% of the time. and that a bad no.\nRecall: If there are samples with 1 (potable) in the testset and our Logistic Regression model can identify it 0% of the time.\n\nF1 Score is 0\nthe logistic regression is behaving bad to this data set.\n","metadata":{"id":"ogVNAZvc8MxP"}},{"cell_type":"markdown","source":"KNN is better the DT or logistic regression to this dataset with those parameters/attributes/features.","metadata":{}}]}