{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I was trying to divide bank's clients into groups, based on their behaviour. I used a few methods for this:\n- KMeans clustering\n- Hierarchical clustering (Agglomerative)\n- Silhouette score\n- DBSCAN\n- TSNE ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/german-credit/german_credit_data.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to have quiet a lot of categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's a relatively small dataframe, so we can apply any method or function. It's still will take not too much time to perform on my laptop.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check if we have any nulls.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's easy to count %, in this case. So, we have 18.3% of nulls in 'Saving account' feature and 39.4% of nulls in 'Checking account' feature. There aren't any solid rule for what to do with missing values. Usually, I get rid of observations, if it have less than 5% of nulls. In this case, let's take a closer look.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, let's substitute Nulls with 'None', to see how distribution of observations will look like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Saving accounts'] = df['Saving accounts'].fillna('None')\ndf['Saving accounts'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's not clear why we don't have information about Saving accounts in some observations. And we can't just drop 18.3 % of data. So I will leave it as it is, with additional option 'None'. Basically, we iterpret this nulls as one more category of the feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Later we will encode it with numbers. We could use one-hot encoders, but here we can use just numbers (as ranking feature). Because we previously suggest, that nulls is a distinct category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Checking account'] = df['Checking account'].fillna('None')\ndf['Checking account'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's stick to our hypothesis. And do the same thing with 'Checking account' as with 'Saving account'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Just checking, that's everything is ok.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Sex.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There two times more males than females. Let's encode sex. Male - 1, female - 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sex'] = df['Sex'].apply(lambda x: 1 if x=='male' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Housing.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature should be encoded with one hot encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Purpose.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can decrease the amount of categories for this feature. I take the last three categories and sum them up into category 'others'(each of them is presented in less than 5%).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Purpose'].replace(['repairs', 'domestic appliances', 'vacation/others'], 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check, if we've done everything right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Purpose.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Saving accounts'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Saving accounts'].replace(['None', 'little', 'moderate', 'rich', 'quite rich'], [0,1,2,3,4], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Checking account'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Checking account'].replace(['None', 'little', 'moderate', 'rich'], [0,1,2,3], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just checking if everything is looks right. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have only 'Housing' and 'Purpose', that cannot be encoded like ranking, so we will use one-hot-encoding instead.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Overwiew of the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(12,12));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Duration'] = np.log(df['Duration'])\ndf['Age'] = np.log(df['Age'])\ndf['Credit amount'] = np.log(df['Credit amount'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age, credit amount and duration - numerical features with long tail. So we should try log them to get more normalized distrubution.\n\nAs we don't have too much observations, making pairplot won't take much time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It doesn't give any additional information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have too correlated features. However we can see, that the most correlated features are: 'Job', 'Credit amount' and 'Duration'. Also it seems like that among clients of the banks men are older than women.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling","execution_count":null},{"metadata":{},"cell_type":"raw","source":"After performing log transformation, we don't have to use scaling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://)Everything is working. Nice. So we are ready to go in clustering and stuff.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Creating models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Means","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To use K-means method we should find the amount of clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia = []\nk = range(1, 11)\nfor k_i in k:\n    km = KMeans(n_clusters=k_i).fit(df)\n    inertia.append(km.inertia_)\n    \nplt.plot(k, inertia)\nplt.xlabel('k')\nplt.ylabel('inertia')\nplt.title('The Elbow Method showing the optimal k');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From KMeans it seems to be 2, 4 or maybe 5 clusters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Hierarchical clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_mat = pdist(df)\n\nZ = hierarchy.linkage(distance_mat, 'ward')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nplt.title('Hierarchical Clustering Dendrogram (truncated)')\nplt.xlabel('cluster size')\nplt.ylabel('distance')\nhierarchy.dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=12,  \n    leaf_font_size=12.,\n    show_contracted=True, \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, from the dendrogram 2, 3 and 4 - are the best fit of clusters. However, it's not clear.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Silhouettte score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we using sklearn impolemention of metric, to better understand how much clusters we have with hierarchical clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_scores = [] \nk = range(2,8)\n\nfor n_cluster in k:\n    silhouette_scores.append( \n        silhouette_score(df, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(df))) \n    \n    \n# Plotting a bar graph to compare the results \n\nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From silhouette score we have 3 clusters here. Probably 2 or 4, but not 5.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2 clusters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### DBSCAN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It's always feels like a game to guess, what is the best parameters for DBSCAN. However, this model can give you the percantage of noise (that still can be another cluster).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I was trying to get epsilon with minimal amount of noise for 2 clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"db = DBSCAN(eps=1.61, min_samples=4).fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\nn_noise_ = list(db.labels_).count(-1)\n\nprint('Estimated number of clusters: {}'.format(n_clusters_))\nprint('Estimated percentage of noise points: {:.2f}%'.format(100*n_noise_/df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TSNE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how good we are able to group clients in clusters. This is a function for choosing perplexity (it's not the best way to do it).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_tsne(df):\n    _, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8), sharey=True)\n\n    tsne=TSNE(perplexity=5).fit_transform(df)\n    axes[0, 0].title.set_text('Perplexity 5')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 0]);\n\n    tsne=TSNE(perplexity=10).fit_transform(df)\n    axes[0, 1].title.set_text('Perplexity 10')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 1]);\n\n    tsne=TSNE(perplexity=20).fit_transform(df)\n    axes[0, 2].title.set_text('Perplexity 20')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[0, 2]);\n\n    tsne=TSNE(perplexity=30).fit_transform(df)\n    axes[1, 0].title.set_text('Perplexity 30')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 0]);\n\n    tsne=TSNE(perplexity=40).fit_transform(df)\n    axes[1, 1].title.set_text('Perplexity 40')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 1]);\n\n    tsne=TSNE(perplexity=50).fit_transform(df)\n    axes[1, 2].title.set_text('Perplexity 50')\n    sns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], ax=axes[1, 2]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_tsne(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne=TSNE(perplexity=30).fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.title('Perplexity 30')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.title('DBSCAN, 2 clusters')\nplt.scatter(tsne[:, 0], tsne[:, 1], c=db.labels_);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, there is some pattern. But not too good.\nLet's compare Kmeans and hierarchical clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=2).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 2).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 2 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0]);\n\n\nplt.title('Hierarchical clustering, 2 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like hierachical clustering is better for two clusters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3 clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=3).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 3).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 3 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0], palette=['green','orange','brown']);\n\n\nplt.title('Hierarchical clustering, 3 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1], palette=['green','orange','brown']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And again we can see three groups.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4 clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=4).fit(df)\nagg_cluster = AgglomerativeClustering(n_clusters = 4).fit(df)\n\n_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n\naxes[0].title.set_text('K-MEANS, 4 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=km.labels_, ax=axes[0], palette=['green','orange','brown', 'yellow']);\n\n\nplt.title('Hierarchical clustering, 4 clusters')\nsns.scatterplot(x = tsne[:, 0], y = tsne[:, 1], hue=agg_cluster.labels_, ax=axes[1], palette=['green','orange','brown', 'yellow']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it's seem like we have 3 clusters. So let's try to find out who are they.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Interpretation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's try to interpret, what are these three groups.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_cluster = AgglomerativeClustering(n_clusters = 3).fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(40, 20))\n\ni_col = 0\ni_row = 0\n\nfor column in df.columns:\n    sns.boxplot(y=column, x=agg_cluster.labels_, \n                     data=df, \n                     palette=\"colorblind\", ax=ax[i_row, i_col])\n    if i_row < 4:\n        i_row += 1\n    else:\n        i_col += 1\n        i_row = 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, there are three groups:\n- Men, with a moderate jobs and now savings\n- Women, with a highly skilled jobs and some savings. Also this group take higher amount of money for longer periods, and not for TV/radio.\n- Men, with no job or not a resident and with a lot of savings","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}