{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def normalize(df):\n    result = df.copy()\n    max_value = df.max()\n    min_value = df.min()\n    result = (df - min_value) / (max_value - min_value)\n    return result\n\nfrom pandas.api.types import is_string_dtype\n\ndata = pd.read_csv('../input/german-credit-data-with-risk/german_credit_data.csv',index_col=0,sep=',')\nlabels = data.columns\n# lets go through column 2 column\nfor col in labels:\n    if is_string_dtype(data[col]):\n        if col == 'Risk':\n            # we want 'Risk' to be a binary variable\n            data[col] = pd.factorize(data[col])[0]\n            continue\n        # the other categorical columns should be one-hot encoded\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)], axis=1)\n        data.drop(col, axis=1, inplace=True)\n    else:\n        data[col] = normalize(data[col])\n\n# move 'Risk' back to the end of the df\ndata = data[[c for c in data if c not in ['Risk']] + ['Risk']]\n\ndata_train = data.iloc[:800]\ndata_valid = data.iloc[800:]\nx_train = data_train.iloc[:,:-1]\ny_train = data_train.iloc[:,-1]\nx_val = data_valid.iloc[:,:-1]\ny_val = data_valid.iloc[:,-1]","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras.layers import Dense, Dropout\n\nsgd = optimizers.SGD(lr=0.03, decay=0, momentum=0.9, nesterov=False)\n\nmodel = Sequential()\nmodel.add(Dense(units=50, activation='tanh', input_dim=24, kernel_initializer='glorot_normal', bias_initializer='zeros'))#, kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(units=1, activation='sigmoid', kernel_initializer='glorot_normal', bias_initializer='zeros'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(x_train.values, y_train.values, validation_data=(x_val.values, y_val.values), epochs=30, batch_size=128)","execution_count":19,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nTrain on 800 samples, validate on 200 samples\nEpoch 1/30\n800/800 [==============================] - 0s 350us/step - loss: 0.6617 - acc: 0.6763 - val_loss: 0.6511 - val_acc: 0.6800\nEpoch 2/30\n800/800 [==============================] - 0s 19us/step - loss: 0.6428 - acc: 0.6763 - val_loss: 0.6200 - val_acc: 0.6800\nEpoch 3/30\n800/800 [==============================] - 0s 18us/step - loss: 0.6002 - acc: 0.6938 - val_loss: 0.5965 - val_acc: 0.6750\nEpoch 4/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5827 - acc: 0.7075 - val_loss: 0.5802 - val_acc: 0.6800\nEpoch 5/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5695 - acc: 0.7087 - val_loss: 0.5654 - val_acc: 0.6900\nEpoch 6/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5587 - acc: 0.7112 - val_loss: 0.5542 - val_acc: 0.7000\nEpoch 7/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5535 - acc: 0.7275 - val_loss: 0.5454 - val_acc: 0.7050\nEpoch 8/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5447 - acc: 0.7275 - val_loss: 0.5419 - val_acc: 0.7000\nEpoch 9/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7250 - val_loss: 0.5342 - val_acc: 0.7200\nEpoch 10/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5476 - acc: 0.7250 - val_loss: 0.5271 - val_acc: 0.7450\nEpoch 11/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5418 - acc: 0.7112 - val_loss: 0.5233 - val_acc: 0.7450\nEpoch 12/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5316 - acc: 0.7312 - val_loss: 0.5236 - val_acc: 0.7300\nEpoch 13/30\n800/800 [==============================] - 0s 22us/step - loss: 0.5384 - acc: 0.7338 - val_loss: 0.5198 - val_acc: 0.7650\nEpoch 14/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5240 - acc: 0.7375 - val_loss: 0.5198 - val_acc: 0.7550\nEpoch 15/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5212 - acc: 0.7413 - val_loss: 0.5190 - val_acc: 0.7500\nEpoch 16/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5260 - acc: 0.7263 - val_loss: 0.5182 - val_acc: 0.7550\nEpoch 17/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5269 - acc: 0.7350 - val_loss: 0.5177 - val_acc: 0.7500\nEpoch 18/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5170 - acc: 0.7425 - val_loss: 0.5172 - val_acc: 0.7500\nEpoch 19/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5257 - acc: 0.7375 - val_loss: 0.5192 - val_acc: 0.7300\nEpoch 20/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5200 - acc: 0.7400 - val_loss: 0.5205 - val_acc: 0.7450\nEpoch 21/30\n800/800 [==============================] - 0s 21us/step - loss: 0.5244 - acc: 0.7325 - val_loss: 0.5219 - val_acc: 0.7400\nEpoch 22/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5247 - acc: 0.7238 - val_loss: 0.5196 - val_acc: 0.7400\nEpoch 23/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5115 - acc: 0.7500 - val_loss: 0.5185 - val_acc: 0.7350\nEpoch 24/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5233 - acc: 0.7338 - val_loss: 0.5207 - val_acc: 0.7400\nEpoch 25/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5217 - acc: 0.7388 - val_loss: 0.5165 - val_acc: 0.7550\nEpoch 26/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5172 - acc: 0.7362 - val_loss: 0.5178 - val_acc: 0.7400\nEpoch 27/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5194 - acc: 0.7375 - val_loss: 0.5188 - val_acc: 0.7400\nEpoch 28/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5185 - acc: 0.7437 - val_loss: 0.5197 - val_acc: 0.7300\nEpoch 29/30\n800/800 [==============================] - 0s 18us/step - loss: 0.5206 - acc: 0.7388 - val_loss: 0.5178 - val_acc: 0.7450\nEpoch 30/30\n800/800 [==============================] - 0s 24us/step - loss: 0.5236 - acc: 0.7375 - val_loss: 0.5189 - val_acc: 0.7300\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<keras.callbacks.History at 0x7f80583959e8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train.values, y_train.values, validation_data=(x_val.values, y_val.values), epochs=30, batch_size=128)","execution_count":26,"outputs":[{"output_type":"stream","text":"Train on 800 samples, validate on 200 samples\nEpoch 1/30\n800/800 [==============================] - 0s 69us/step - loss: 0.5109 - acc: 0.7512 - val_loss: 0.5150 - val_acc: 0.7450\nEpoch 2/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5143 - acc: 0.7450 - val_loss: 0.5142 - val_acc: 0.7400\nEpoch 3/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5031 - acc: 0.7575 - val_loss: 0.5130 - val_acc: 0.7350\nEpoch 4/30\n800/800 [==============================] - 0s 18us/step - loss: 0.5107 - acc: 0.7612 - val_loss: 0.5122 - val_acc: 0.7250\nEpoch 5/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5017 - acc: 0.7575 - val_loss: 0.5122 - val_acc: 0.7150\nEpoch 6/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5068 - acc: 0.7500 - val_loss: 0.5123 - val_acc: 0.7250\nEpoch 7/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5090 - acc: 0.7450 - val_loss: 0.5129 - val_acc: 0.7200\nEpoch 8/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5081 - acc: 0.7462 - val_loss: 0.5139 - val_acc: 0.7400\nEpoch 9/30\n800/800 [==============================] - 0s 18us/step - loss: 0.4979 - acc: 0.7625 - val_loss: 0.5150 - val_acc: 0.7200\nEpoch 10/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5189 - acc: 0.7425 - val_loss: 0.5156 - val_acc: 0.7450\nEpoch 11/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5082 - acc: 0.7563 - val_loss: 0.5161 - val_acc: 0.7350\nEpoch 12/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5083 - acc: 0.7500 - val_loss: 0.5154 - val_acc: 0.7350\nEpoch 13/30\n800/800 [==============================] - 0s 19us/step - loss: 0.5066 - acc: 0.7600 - val_loss: 0.5159 - val_acc: 0.7350\nEpoch 14/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5049 - acc: 0.7563 - val_loss: 0.5162 - val_acc: 0.7250\nEpoch 15/30\n800/800 [==============================] - 0s 20us/step - loss: 0.5078 - acc: 0.7400 - val_loss: 0.5152 - val_acc: 0.7250\nEpoch 16/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5110 - acc: 0.7362 - val_loss: 0.5158 - val_acc: 0.7250\nEpoch 17/30\n800/800 [==============================] - 0s 18us/step - loss: 0.5094 - acc: 0.7500 - val_loss: 0.5160 - val_acc: 0.7300\nEpoch 18/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5003 - acc: 0.7600 - val_loss: 0.5157 - val_acc: 0.7300\nEpoch 19/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5105 - acc: 0.7488 - val_loss: 0.5146 - val_acc: 0.7300\nEpoch 20/30\n800/800 [==============================] - 0s 17us/step - loss: 0.5063 - acc: 0.7525 - val_loss: 0.5146 - val_acc: 0.7250\nEpoch 21/30\n800/800 [==============================] - 0s 18us/step - loss: 0.4999 - acc: 0.7662 - val_loss: 0.5148 - val_acc: 0.7300\nEpoch 22/30\n800/800 [==============================] - 0s 18us/step - loss: 0.5073 - acc: 0.7512 - val_loss: 0.5152 - val_acc: 0.7250\nEpoch 23/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5079 - acc: 0.7475 - val_loss: 0.5152 - val_acc: 0.7200\nEpoch 24/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5073 - acc: 0.7587 - val_loss: 0.5143 - val_acc: 0.7300\nEpoch 25/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5104 - acc: 0.7525 - val_loss: 0.5155 - val_acc: 0.7250\nEpoch 26/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5100 - acc: 0.7475 - val_loss: 0.5136 - val_acc: 0.7400\nEpoch 27/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5094 - acc: 0.7425 - val_loss: 0.5135 - val_acc: 0.7300\nEpoch 28/30\n800/800 [==============================] - 0s 16us/step - loss: 0.5000 - acc: 0.7525 - val_loss: 0.5150 - val_acc: 0.7150\nEpoch 29/30\n800/800 [==============================] - 0s 15us/step - loss: 0.5080 - acc: 0.7538 - val_loss: 0.5130 - val_acc: 0.7350\nEpoch 30/30\n800/800 [==============================] - ETA: 0s - loss: 0.5406 - acc: 0.742 - 0s 15us/step - loss: 0.5099 - acc: 0.7437 - val_loss: 0.5126 - val_acc: 0.7500\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<keras.callbacks.History at 0x7f8058043fd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Performance validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(x_val.values)\ny_val = y_val.values\n\nfrom sklearn.metrics import confusion_matrix, precision_score\nimport seaborn as sns\n\nsns.heatmap(confusion_matrix(y_val,y_pred),annot=True,fmt='.5g') \nprint('Precision Score on validation data is {}'.format(precision_score(y_val, y_pred, average='weighted')))","execution_count":27,"outputs":[{"output_type":"stream","text":"Precision Score on validation data is 0.7390525746722529\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}