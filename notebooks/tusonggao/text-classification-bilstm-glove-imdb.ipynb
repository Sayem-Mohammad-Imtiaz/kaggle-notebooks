{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://github.com/AnthonyK97/Text-Classification-on-IMDB\n# https://github.com/AnthonyK97/Text-Classification-on-IMDB/blob/main/2%20CNN%2BGlove.ipynb\n\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass CFG:\n    batch_size = 20\n    lr = 0.01\n    eval_step_num = 300\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2032\n    loss_type = 'bce'  # 'bce' for binary cross entropy; 'mse' for mean squared error\n    \nDEBUG_RUN = True\nglobal_start_t = time.time()\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:20.531282Z","iopub.execute_input":"2021-06-02T17:56:20.531828Z","iopub.status.idle":"2021-06-02T17:56:22.358355Z","shell.execute_reply.started":"2021-06-02T17:56:20.531665Z","shell.execute_reply":"2021-06-02T17:56:22.357425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(30000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:22.360183Z","iopub.execute_input":"2021-06-02T17:56:22.360829Z","iopub.status.idle":"2021-06-02T17:56:24.078189Z","shell.execute_reply.started":"2021-06-02T17:56:22.360785Z","shell.execute_reply":"2021-06-02T17:56:24.077321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000   # 仅考虑最高频的10000个词\nMAX_LEN = 200\nword_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2]     # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:24.080143Z","iopub.execute_input":"2021-06-02T17:56:24.080835Z","iopub.status.idle":"2021-06-02T17:56:28.118957Z","shell.execute_reply.started":"2021-06-02T17:56:24.0807Z","shell.execute_reply":"2021-06-02T17:56:28.118153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2] # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:56:28.120669Z","iopub.execute_input":"2021-06-02T17:56:28.121028Z","iopub.status.idle":"2021-06-02T17:56:31.854875Z","shell.execute_reply.started":"2021-06-02T17:56:28.120991Z","shell.execute_reply":"2021-06-02T17:56:31.853921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(data_list, pad_length):\n    padded_list = data_list.copy()\n    \n    if len(data_list) > pad_length:\n        padded_list = data_list[-pad_length:]\n        \n    if len(data_list) < pad_length:\n        padded_list = [1] * (pad_length-len(data_list)) + data_list\n        \n    return padded_list\n\ndef text_to_token(text):\n    cleaned_text = clean_text(text)\n    word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n    pad_list = pad(word_token_list, MAX_LEN)\n    token = ' '.join([str(x) for x in pad_list])\n    return token\n            \nprocess_start_t = time.time()\nprint('start processing...')\nimdb_data['review_tokens'] = imdb_data['review'].map(text_to_token)\nprint('ok, cost time: ', time.time()-process_start_t)\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:31.856264Z","iopub.execute_input":"2021-06-02T17:56:31.856619Z","iopub.status.idle":"2021-06-02T17:56:35.749326Z","shell.execute_reply.started":"2021-06-02T17:56:31.856582Z","shell.execute_reply":"2021-06-02T17:56:35.74854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_NUM = 15000\nimdb_data_test = imdb_data.iloc[:5000]\nimdb_data_valid = imdb_data.iloc[5000:10000]\nimdb_data_train = imdb_data.iloc[10000:TRAIN_NUM+10000]\n\nif DEBUG_RUN:\n    SAMPLE_NUM = 300\n    imdb_data_test = imdb_data_test.sample(SAMPLE_NUM)\n    imdb_data_valid = imdb_data_valid.sample(SAMPLE_NUM)\n    imdb_data_train = imdb_data_train.sample(2*SAMPLE_NUM)\n\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')\n\nimdb_data_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:35.750597Z","iopub.execute_input":"2021-06-02T17:56:35.750942Z","iopub.status.idle":"2021-06-02T17:56:35.771915Z","shell.execute_reply.started":"2021-06-02T17:56:35.750906Z","shell.execute_reply":"2021-06-02T17:56:35.771214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_path = '/kaggle/input/glove6b/glove.6B.100d.txt'\n\ncnt = 0\nword_2_vector_map = {}\nwith open(glove_path) as fin:\n    for line in fin:\n        line = line.strip()\n        word = line.split()[0]\n        vector = np.array([float(val) for val in line.split()[1:]])\n        word_2_vector_map[word] = vector\n        cnt += 1\nprint('cnt is', cnt, 'len of word_2_vector_map: ', len(word_2_vector_map))\n\nembed_size = 100\nglove6b_100d_weight = torch.zeros(len(word_id_dict), embed_size)\n\nfor word, idx in word_id_dict.items():\n    try:\n        vector = word_2_vector_map[word]\n    except:\n        print('not found in : ', word)\n        continue\n    glove6b_100d_weight[idx, :] = torch.from_numpy(vector)\n    \nprint('glove6b_100d_weight.shape: ', glove6b_100d_weight.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:35.773036Z","iopub.execute_input":"2021-06-02T17:56:35.773546Z","iopub.status.idle":"2021-06-02T17:56:54.195012Z","shell.execute_reply.started":"2021-06-02T17:56:35.773504Z","shell.execute_reply":"2021-06-02T17:56:54.192887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:54.198157Z","iopub.execute_input":"2021-06-02T17:56:54.198422Z","iopub.status.idle":"2021-06-02T17:56:54.206462Z","shell.execute_reply.started":"2021-06-02T17:56:54.198396Z","shell.execute_reply":"2021-06-02T17:56:54.205649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class imdbDataset(Dataset):\n    def __init__(self, data_df):\n        self.data_df = data_df\n        \n    def __len__(self):\n        return len(self.data_df)\n    \n    def __getitem__(self, index):\n        label = self.data_df.iloc[index]['sentiment']\n        label = torch.tensor([float(label)], dtype=torch.float, device=device)\n        \n        tokens = self.data_df.iloc[index]['review_tokens']\n        feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long, device=device)\n            \n        return feature, label\n    \ndef generate_data_iter(cfg):\n    global imdb_data_train, imdb_data_valid, imdb_data_test\n    ds_train = imdbDataset(imdb_data_train)\n    ds_valid = imdbDataset(imdb_data_valid)\n    ds_test = imdbDataset(imdb_data_test)\n    print('len of ds_train: ', len(ds_train), 'len of ds_valid: ', len(ds_valid),\n          'len of ds_test: ', len(ds_test))\n\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n    dl_valid = DataLoader(ds_valid, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    dl_test = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    return dl_train, dl_valid, dl_test\n\ndl_train, dl_valid, dl_test = generate_data_iter(cfg)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:54.207838Z","iopub.execute_input":"2021-06-02T17:56:54.208219Z","iopub.status.idle":"2021-06-02T17:56:54.219921Z","shell.execute_reply.started":"2021-06-02T17:56:54.208184Z","shell.execute_reply":"2021-06-02T17:56:54.219035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H_SIZE = 64\nN_LAYERS = 1\nEMBEDDING_DIM = glove6b_100d_weight.shape[1]\n\nclass BiLSTM_Net(nn.Module):\n    def __init__(self, hidden_size=H_SIZE, num_layers=N_LAYERS, bidirectional=True, dropout=0.0):\n        global glove6b_100d_weight\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        \n        #self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=3, padding_idx=1)\n        self.embedding = nn.Embedding.from_pretrained(glove6b_100d_weight, freeze=False)\n        assert self.embedding.weight.requires_grad==True, 'should be True, because freeze=False'\n        \n        self.lstm = nn.LSTM(EMBEDDING_DIM, hidden_size, num_layers, batch_first=True, \n                            bidirectional=self.bidirectional, dropout=dropout)\n        if self.bidirectional:\n            self.fc = nn.Linear(hidden_size*2, 1)\n        else:\n            self.fc = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n            c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n        else:\n            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        \n        x = self.embedding(x)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        x_in = lstm_out[:, -1, :]\n        \n        y = nn.Sigmoid()(self.fc(x_in))\n        return y\n    \nmodel = BiLSTM_Net(bidirectional=True, dropout=0.1)\nprint(model)\nmodel.to(device)     \n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:54.221117Z","iopub.execute_input":"2021-06-02T17:56:54.221644Z","iopub.status.idle":"2021-06-02T17:56:59.262804Z","shell.execute_reply.started":"2021-06-02T17:56:54.221605Z","shell.execute_reply":"2021-06-02T17:56:59.261892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    if type(y_pred)==list:\n        y_pred = np.array(y_pred)\n    y_pred = (y_pred > 0.5)\n    if type(y_true)==list:\n        y_true = np.array(y_true)\n    acc = (y_pred==y_true).mean()\n    return acc\n\ndef evaluate(model, dl_test, device):\n    global cfg\n    model.eval()\n    \n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(dl_test):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    model.train() # 恢复模型为训练状态\n    acc = accuracy(y_pred_lst, y_true_lst)\n\n    return acc\n    \ndef train(model, dl_train, optimizer, loss_func, device):\n    global cfg, global_step_num\n    model.train()  # 将模型置为训练状态\n    \n    y_true_lst, y_pred_lst = [], []\n    for step, batch in enumerate(dl_train):\n        global_step_num += 1\n        feature, label = batch\n        feature, label = feature.to(device), label.to(device)\n        y_pred = model(feature)\n        train_loss = loss_func(y_pred, label)\n        y_pred_lst += list(y_pred.detach().cpu().numpy())\n        y_true_lst += list(label.detach().cpu().numpy())\n        train_loss.backward()\n        optimizer.step()\n        model.zero_grad()\n        \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:56:59.264016Z","iopub.execute_input":"2021-06-02T17:56:59.264532Z","iopub.status.idle":"2021-06-02T17:56:59.276861Z","shell.execute_reply.started":"2021-06-02T17:56:59.264493Z","shell.execute_reply":"2021-06-02T17:56:59.275644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\nepochs = 10\n# optimizer=torch.optim.Adagrad(model.parameters(), lr=cfg.lr)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)\nif cfg.loss_type=='mse':\n    loss_func = nn.MSELoss()\nelse:\n    loss_func = nn.BCELoss()\n\nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'get new best_valid_acc: {global_best_valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:56:59.278473Z","iopub.execute_input":"2021-06-02T17:56:59.278921Z","iopub.status.idle":"2021-06-02T17:57:10.189163Z","shell.execute_reply.started":"2021-06-02T17:56:59.278887Z","shell.execute_reply":"2021-06-02T17:57:10.188292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BiLSTM_Net(bidirectional=True, dropout=0.1)\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')\n\nprint('total finished, cost time: ', time.time() - global_start_t)\n\n# final test_acc: 0.62000, best_val_acc: 0.63667, train_acc: 0.79500, best_train_acc: 1.00000\n# total finished, cost time:  48.055832386016846","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:57:10.190451Z","iopub.execute_input":"2021-06-02T17:57:10.190916Z","iopub.status.idle":"2021-06-02T17:57:10.411717Z","shell.execute_reply.started":"2021-06-02T17:57:10.190879Z","shell.execute_reply":"2021-06-02T17:57:10.410859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adam  lr=0.005  seed=2032  TRAIN_NUM=20000\n# final test_acc: 0.88280, best_val_acc: 0.88340, train_acc: 0.79680, best_train_acc: 0.99510\n# total finished, cost time:  461.7042815685272\n\n# Adam  lr=0.005  seed=2032  TRAIN_NUM=15000\n# in epoch: 0, train_acc: 0.80460, valid_acc: 0.87620, test_acc: 0.87060","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:57:10.41293Z","iopub.execute_input":"2021-06-02T17:57:10.413281Z","iopub.status.idle":"2021-06-02T17:57:10.416832Z","shell.execute_reply.started":"2021-06-02T17:57:10.413245Z","shell.execute_reply":"2021-06-02T17:57:10.416015Z"},"trusted":true},"execution_count":null,"outputs":[]}]}