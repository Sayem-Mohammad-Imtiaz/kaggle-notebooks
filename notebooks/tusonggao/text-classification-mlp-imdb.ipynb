{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://github.com/AnthonyK97/Text-Classification-on-IMDB\n# https://github.com/AnthonyK97/Text-Classification-on-IMDB/blob/main/2%20CNN%2BGlove.ipynb\n\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass CFG:\n    batch_size = 20\n    lr = 0.02\n    eval_step_num = 50\n    mid_eval = False\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2032\n    use_ema = False\n    use_adversial_training = False\n    \nDEBUG_RUN = True\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:25:55.387987Z","iopub.execute_input":"2021-06-02T06:25:55.388414Z","iopub.status.idle":"2021-06-02T06:25:57.238429Z","shell.execute_reply.started":"2021-06-02T06:25:55.388318Z","shell.execute_reply":"2021-06-02T06:25:57.236817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(30000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:25:57.240796Z","iopub.execute_input":"2021-06-02T06:25:57.241281Z","iopub.status.idle":"2021-06-02T06:25:59.509722Z","shell.execute_reply.started":"2021-06-02T06:25:57.241235Z","shell.execute_reply":"2021-06-02T06:25:59.508315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000   # 仅考虑最高频的10000个词\nMAX_LEN = 200\nword_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2]     # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:25:59.512177Z","iopub.execute_input":"2021-06-02T06:25:59.512654Z","iopub.status.idle":"2021-06-02T06:26:04.580684Z","shell.execute_reply.started":"2021-06-02T06:25:59.512609Z","shell.execute_reply":"2021-06-02T06:26:04.579609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(data_list, pad_length):\n    padded_list = data_list.copy()\n    \n    if len(data_list) > pad_length:\n        padded_list = data_list[-pad_length:]\n        \n    if len(data_list) < pad_length:\n        padded_list = [1] * (pad_length-len(data_list)) + data_list\n        \n    return padded_list\n\ndef text_to_token(text):\n    cleaned_text = clean_text(text)\n    word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n    pad_list = pad(word_token_list, MAX_LEN)\n    token = ' '.join([str(x) for x in pad_list])\n    return token\n            \nprocess_start_t = time.time()\nprint('start processing...')\nimdb_data['review_tokens'] = imdb_data['review'].map(text_to_token)\nprint('ok, cost time: ', time.time()-process_start_t)\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:04.582677Z","iopub.execute_input":"2021-06-02T06:26:04.583279Z","iopub.status.idle":"2021-06-02T06:26:09.569532Z","shell.execute_reply.started":"2021-06-02T06:26:04.583235Z","shell.execute_reply":"2021-06-02T06:26:09.568171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(imdb_data['review'].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.571358Z","iopub.execute_input":"2021-06-02T06:26:09.571669Z","iopub.status.idle":"2021-06-02T06:26:09.57836Z","shell.execute_reply.started":"2021-06-02T06:26:09.571639Z","shell.execute_reply":"2021-06-02T06:26:09.576744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_NUM = 15000\nimdb_data_test = imdb_data.iloc[:5000]\nimdb_data_valid = imdb_data.iloc[5000:10000]\nimdb_data_train = imdb_data.iloc[10000:TRAIN_NUM+10000]\n\nif DEBUG_RUN:\n    SAMPLE_NUM = 3000\n    imdb_data_test = imdb_data_test.sample(SAMPLE_NUM)\n    imdb_data_valid = imdb_data_valid.sample(SAMPLE_NUM)\n    imdb_data_train = imdb_data_train.sample(2*SAMPLE_NUM)\n\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.580564Z","iopub.execute_input":"2021-06-02T06:26:09.581064Z","iopub.status.idle":"2021-06-02T06:26:09.604279Z","shell.execute_reply.started":"2021-06-02T06:26:09.581018Z","shell.execute_reply":"2021-06-02T06:26:09.602982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.606395Z","iopub.execute_input":"2021-06-02T06:26:09.606875Z","iopub.status.idle":"2021-06-02T06:26:09.616861Z","shell.execute_reply.started":"2021-06-02T06:26:09.606798Z","shell.execute_reply":"2021-06-02T06:26:09.615876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class imdbDataset(Dataset):\n    def __init__(self, data_df):\n        self.data_df = data_df\n        \n    def __len__(self):\n        return len(self.data_df)\n    \n    def __getitem__(self, index):\n        label = self.data_df.iloc[index]['sentiment']\n        label = torch.tensor([float(label)], dtype=torch.float, device=device)\n        \n        tokens = self.data_df.iloc[index]['review_tokens']\n        feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long, device=device)\n            \n        return feature, label\n    \ndef generate_data_iter(cfg):\n    global imdb_data_train, imdb_data_valid, imdb_data_test\n    ds_train = imdbDataset(imdb_data_train)\n    ds_valid = imdbDataset(imdb_data_valid)\n    ds_test = imdbDataset(imdb_data_test)\n    print('len of ds_train: ', len(ds_train), 'len of ds_valid: ', len(ds_valid),\n          'len of ds_test: ', len(ds_test))\n\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n    dl_valid = DataLoader(ds_valid, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    dl_test = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    return dl_train, dl_valid, dl_test\n\ndl_train, dl_valid, dl_test = generate_data_iter(cfg)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.619558Z","iopub.execute_input":"2021-06-02T06:26:09.619923Z","iopub.status.idle":"2021-06-02T06:26:09.634036Z","shell.execute_reply.started":"2021-06-02T06:26:09.619874Z","shell.execute_reply":"2021-06-02T06:26:09.633211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model_EMA:\n    '''\n    # https://zhuanlan.zhihu.com/p/68748778\n    Example\n    # 初始化\n    ema = EMA(model, 0.999)\n\n    # 训练阶段，更新完参数后\n\n    '''\n    def __init__(self, model, decay=0.99):\n        self.model = model\n        self.decay = decay\n        self.registered = False\n        self.shadow = {}\n        self.backup = {}\n\n    def is_registered(self):\n        return self.registered\n\n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n        self.registered = True\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nclass FGM():\n    '''\n    Example\n    # 初始化\n    fgm = FGM(model,epsilon=1,emb_name='word_embeddings.')\n    for batch_input, batch_label in data:\n        # 正常训练\n        loss = model(batch_input, batch_label)\n        loss.backward() # 反向传播，得到正常的grad\n        # 对抗训练\n        fgm.attack() # 在embedding上添加对抗扰动\n        #model.zero_grad()  # 如果需要两次回传梯度不累加, 只使用后面添加扰动之后的得到的梯度，则去掉该行的注释！\n        loss_adv = model(batch_input, batch_label)\n        loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n        fgm.restore() # 恢复embedding参数\n        # 梯度下降，更新参数\n        optimizer.step()\n        model.zero_grad()\n    '''\n    def __init__(self, model, emb_name, epsilon=1.0, adv_random=False):\n        # emb_name这个参数要换成你模型中embedding的参数名\n        self.model = model\n        self.epsilon = epsilon\n        self.emb_name = emb_name\n        self.adv_random = adv_random\n        self.backup = {}\n\n    def attack(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                #print('found an param: ', name)\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                #print('in attack() norm is ', norm)\n                #print('param.data: ', param.data, 'param.grad: ', param.grad)\n                #print('in attack() norm.shape is ', norm.shape, 'param.data.shape: ', param.data.shape, 'param.grad.shape: ', param.grad.shape)\n                if norm!=0 and not torch.isnan(norm):\n                    epsilon = self.epsilon\n                    if self.adv_random:\n                        epsilon *= random.uniform(0.5, 1.5)\n                    r_at = epsilon * param.grad / norm\n                    #r_at = 0.1 * random.uniform(0.5, 1.5) * param.grad\n                    #r_at = 0.1 * param.grad\n                    param.data.add_(r_at)\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.635271Z","iopub.execute_input":"2021-06-02T06:26:09.635544Z","iopub.status.idle":"2021-06-02T06:26:09.657317Z","shell.execute_reply.started":"2021-06-02T06:26:09.635518Z","shell.execute_reply":"2021-06-02T06:26:09.65613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 50\n\nclass MLP_Net(nn.Module):\n    def __init__(self, hidden_size=150):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=EMBEDDING_DIM, padding_idx=1)\n        \n        self.fc = nn.Sequential()\n        self.fc.add_module('fc_1', nn.Linear(EMBEDDING_DIM*MAX_LEN, hidden_size))\n        self.fc.add_module('relu_1', nn.ReLU())\n        self.fc.add_module('fc_2', nn.Linear(hidden_size, 1))\n        self.fc.add_module('sigmoid_1', nn.Sigmoid())\n        \n    def forward(self, x):\n        x = self.embedding(x).view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n    \nmodel = MLP_Net()\nprint(model)\nmodel.to(device)     \n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n\nprint('ok')\n\n# model_param_num:  2000301 model_trainable_param_num:  2000301","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.65849Z","iopub.execute_input":"2021-06-02T06:26:09.658784Z","iopub.status.idle":"2021-06-02T06:26:09.739526Z","shell.execute_reply.started":"2021-06-02T06:26:09.658756Z","shell.execute_reply":"2021-06-02T06:26:09.738526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    if type(y_pred)==list:\n        y_pred = np.array(y_pred)\n    y_pred = (y_pred > 0.5)\n    if type(y_true)==list:\n        y_true = np.array(y_true)\n    acc = (y_pred==y_true).mean()\n    return acc\n\ndef evaluate(model, dl_test, device):\n    global cfg\n    model.eval()\n    \n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(dl_test):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    model.train() # 恢复模型为训练状态\n    acc = accuracy(y_pred_lst, y_true_lst)\n\n    return acc\n    \ndef train(model, dl_train, optimizer, loss_func, device):\n    global cfg, global_step_num, global_best_valid_acc, dl_valid,  model_ema, fgm\n    model.train()  # 将模型置为训练状态\n    \n    y_true_lst, y_pred_lst = [], []\n    for step, batch in enumerate(dl_train):\n        global_step_num += 1\n        feature, label = batch\n        feature, label = feature.to(device), label.to(device)\n        y_pred = model(feature)\n        train_loss = loss_func(y_pred, label)\n        y_pred_lst += list(y_pred.detach().cpu().numpy())\n        y_true_lst += list(label.detach().cpu().numpy())\n        train_loss.backward()\n        optimizer.step()\n        model.zero_grad()\n        \n        if cfg.mid_eval and (global_step_num % cfg.eval_step_num == 0):\n            valid_acc = evaluate(model, dl_valid, device)\n            print(f'step_num: {global_step_num}, valid_acc: {valid_acc:.5f}')\n            if valid_acc > global_best_valid_acc:\n                global_best_valid_acc = valid_acc\n                print(f'step_num: {global_step_num}, get new best val_acc: {valid_acc:.5f}, save the model now!')                \n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))\n        \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:26:09.741354Z","iopub.execute_input":"2021-06-02T06:26:09.741807Z","iopub.status.idle":"2021-06-02T06:26:09.758164Z","shell.execute_reply.started":"2021-06-02T06:26:09.741763Z","shell.execute_reply":"2021-06-02T06:26:09.757356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\nepochs = 10\n# optimizer=torch.optim.Adagrad(model.parameters(), lr=0.06)\n# optimizer=torch.optim.Adadelta(model.parameters(), lr=10.0)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.007, weight_decay=1e-5)\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.007, weight_decay=0.01)\nloss_func = nn.BCELoss()\n\nmodel_ema = None\nfgm = None\nif cfg.use_ema:\n    model_ema = model_EMA(model, decay=0.999)\n    model_ema.register()\n    \nif cfg.use_adversial_training:\n    fgm = FGM(model, 'embedding', epsilon=1.0, adv_random=True)\n    \nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'at the end of epoch, global_step_num: {global_step_num} get new best_valid_acc: {valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T06:26:09.759541Z","iopub.execute_input":"2021-06-02T06:26:09.759866Z","iopub.status.idle":"2021-06-02T07:06:31.828439Z","shell.execute_reply.started":"2021-06-02T06:26:09.759823Z","shell.execute_reply":"2021-06-02T07:06:31.827191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MLP_Net()\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')\n\nprint('total finished, cost time: ', time.time() - global_start_t)\n\n# Adam lr=0.007, weight_decay=1e-5\n# final test_acc: 0.74667, best_val_acc: 0.74333, train_acc: 0.97450, best_train_acc: 0.97683\n# total finished, cost time:  111.0548083782196","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:06:31.829828Z","iopub.execute_input":"2021-06-02T07:06:31.830144Z","iopub.status.idle":"2021-06-02T07:06:33.466575Z","shell.execute_reply.started":"2021-06-02T07:06:31.830114Z","shell.execute_reply":"2021-06-02T07:06:33.464227Z"},"trusted":true},"execution_count":null,"outputs":[]}]}