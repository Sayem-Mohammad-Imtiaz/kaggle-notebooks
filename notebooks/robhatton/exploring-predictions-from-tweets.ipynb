{"cells":[{"metadata":{"_cell_guid":"73befa52-d715-4c38-9b43-65676b51ffe5","_uuid":"4d69bfc10bb4a706a36aa53fcd77920d380de0bd"},"cell_type":"markdown","source":"The goal for this notebook is to work through the steps needed to predict if a tweet is postitive, negative or netural based on the values in the Tweets.csv file. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics \nfrom sklearn.metrics import classification_report\n\nprint(os.listdir(\"../input\"))\n","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"78cca72b-140a-4761-80dd-5aca76d815f2","_uuid":"44dd347a19f60a49895eb140d9de1923258cea86","collapsed":true,"trusted":true},"cell_type":"code","source":"tweets = pd.read_csv(\"../input/Tweets.csv\")\ntweets = tweets.reindex(np.random.permutation(tweets.index))","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"4736270d-72e5-4915-a72a-d68749c57c70","_uuid":"f25127232f91d0debbc4dd32c5ab5c46eb083598"},"cell_type":"markdown","source":"We've loaded the Tweets csv file (and randomized it), now let take a look at the first few records. This shows us that we have columns that we can use to predict the sentiment as well as columns that indicate what the actual sentiment  of each tweet was. While our objective here is to simply predict sentiment, there are additional columns that we won't use that could provide value insight in sentiment based on location or time of day. "},{"metadata":{"_cell_guid":"0c74a928-a484-42e0-8119-ee0b9e06a0ec","_uuid":"88d802ae985d85757f6cc81af0fb85963f270398","scrolled":true,"trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"830f9731-88c7-4dbd-97fc-2042e397bf05","_uuid":"c88e1ec27d385ee66dfeb8dabfdf1bd830d95ff7"},"cell_type":"markdown","source":"Let's start exploring the data by seeing which columns have data. We can see that airline_sentiment_gold and negativereason_gold don't have much data, so we'll be able to delete those columns. We can also see that we have 14,640 total records. "},{"metadata":{"_cell_guid":"3a7b183b-9599-4b8a-82c4-5d45b5918eda","_uuid":"8c0246ff949755c934b3162800a42585f422d7a1","trusted":true},"cell_type":"code","source":"tweets.count()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"361e36bc-f2bb-4078-b244-658b1519f145","_uuid":"f25768a1769a9bee94326c5282e8161c07d3c26c","collapsed":true},"cell_type":"markdown","source":"Let's delete the columns we won't need just to clean things up. Let's leave the airline name in the data to see if it adversely biases the prediction."},{"metadata":{"_cell_guid":"a0ba792a-b052-4da0-87bf-236995c8b593","_uuid":"f3bc6aaacda40dca3894fe4d1378e542f79386e7","collapsed":true,"trusted":true},"cell_type":"code","source":"del tweets['airline_sentiment_confidence']\ndel tweets['negativereason_confidence']\ndel tweets['airline_sentiment_gold']\ndel tweets['name']\ndel tweets['negativereason']\ndel tweets['negativereason_gold']\ndel tweets['retweet_count']\ndel tweets['tweet_coord']\ndel tweets['tweet_created']\ndel tweets['tweet_location']\ndel tweets['user_timezone']","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"a3006f89-4a01-4c0e-813e-67b90757e6b9","_uuid":"f6d5f8b0ab73b89b98ff9d1733e76159a6d578a9","scrolled":true,"trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"fc6fe663-63d2-448b-beef-c0439d5199f9","_uuid":"4a860130012351d7b48667aeda8c5751d65f997b"},"cell_type":"markdown","source":"Now that the irrelevant data is gone, let's check to see if the data is roughly balanced (similar counts of airline_sentiment and airline).\n\nWe can see that while most of the airline_sentiment values are negative, they're all the same order of magnitude. The airline Virgin America is poorly represented in the data set, this will bear watching to see if it influences results. We won't use Airline in the training, but we'll want to check if sentiment accuracy for Virgin America is reduced when we group the predictions by airline."},{"metadata":{"_cell_guid":"12d31cd1-f682-4c8e-9d09-62b8405a2e52","_uuid":"d9748eecdb59e7f0cc169b6f82218dc3cc9c23ad","scrolled":true,"trusted":true},"cell_type":"code","source":"pd.value_counts(tweets['airline_sentiment'].values, sort = False)","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"6ee6be80-b8d2-4c44-b056-8e08a372515c","_uuid":"49a588b513487beb396f45c977d89f32eeb936ac","trusted":true},"cell_type":"code","source":"pd.value_counts(tweets['airline'].values, sort = False)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"96886fa9-013a-487f-ba41-6e013fc677b9","_uuid":"95df057bf92f087a6ae3dddd7899030447005fff"},"cell_type":"markdown","source":"As we'll be using the Tweet text to predect sentiment, let's look at the text characteristics (average text length, variablity of text length, word counts, lemmatized word count, capitalized words, ect.). An earlier line of investigation involved building a function that strips out all but letters, changes everything to lowercase and strips out 'stop_words', then return an array of single word tokens. This approach ended up removing words that seem to be relevent ('not' and 'delayed'). Lemmatization also simplifyed some words with a negative context to their root for when had a positive sentiment. \n\nRather than pursuing the line of investigation above, let's just into a word count vector structure. \n\nStart off by creating a CountVectorizer object, then use the 'fit' method to learn the vocabulary. "},{"metadata":{"_cell_guid":"438628f4-c3d0-4ffa-a125-ec17af4d50a8","_uuid":"3bcc59bc0b4347ff3b8142e4fbe8270f13679e2a","scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# create the transform\ncv = CountVectorizer()\n# tokenize and build vocab\ncv.fit(tweets.text)\n","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"74b3a4ca-b663-484b-8d5a-3dd8dd6101e1","_uuid":"90246d56759133c06f4082ff1da99e9df8e97665"},"cell_type":"markdown","source":"This shows that we have 15051 distinct words in the vocabulary."},{"metadata":{"_cell_guid":"d2886897-62e4-40cd-b317-3e39497b73ab","_uuid":"d12f05381a04f9403b556782e4316591fa801438","trusted":true},"cell_type":"code","source":"len(cv.vocabulary_)\n","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"7447438a-a849-4aa5-a7ed-6077765eb52b","_uuid":"dfdb43e65926c1310c5923e374f42977d55984ac"},"cell_type":"markdown","source":"This show that the sparce matrix housing our tweets has 14640 rows (one for each tweet) and 15051 columns (one for each word in the vocabulary)"},{"metadata":{"_cell_guid":"ef1ad645-6909-48c7-9065-d286f8f0ab41","_uuid":"0c3c5e9996cb80f49c0c10ad8124d6e001fff0d0","trusted":true},"cell_type":"code","source":"docTerms = cv.fit_transform(tweets.text)\nprint(type(docTerms))\nprint(docTerms.shape)","execution_count":44,"outputs":[]},{"metadata":{"_cell_guid":"66669639-92e5-462f-83e2-0543666818be","_uuid":"311a9cc1f7cd8ead4e07dec82849b0199f99ae77"},"cell_type":"markdown","source":"If we visualize the sparce array, we can see that there's a lot of commonality in vocabulary use. "},{"metadata":{"_cell_guid":"ea085556-006b-4828-abbc-423403101a01","_uuid":"23d840d1b2fa7ef50b0e7368aeb5dbbc79679b5a","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.spy(docTerms,markersize=.25)\n","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"100348b9-589d-46a1-80d8-370b3d41cb24","_uuid":"f17ecded5ac18858995f9465f0f970a3e68656bc","scrolled":true,"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(docTerms, tweets['airline_sentiment'].values, test_size = .7, random_state=25)\nLogReg = LogisticRegression()\nLogReg.fit(X_train, y_train)","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"006025aa-8435-4630-833c-f7a4c86ceae1","_uuid":"ffef8ddf68c916418e6d855e27b466c2401e3681","collapsed":true,"trusted":true},"cell_type":"code","source":"y_pred = LogReg.predict(X_test)","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"731ff76b-3fcf-4335-af7c-2f33b7b17096","_uuid":"d68b810d63f27d4e2a8c25953c568d5cef6f63a4","trusted":true},"cell_type":"code","source":"confusion_matrix = confusion_matrix(y_test, y_pred)\nconfusion_matrix","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"187909b3-e990-4c69-adcb-d02ef98adfc2","_uuid":"586e0dff61f7fe309e7eae187dac357c4d9ba373","scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"497de585-303c-40e8-af07-57b3ee013a2e","_uuid":"ad079d930fa81f7ec438b1a55cc01fdf724d9ab6","trusted":true},"cell_type":"code","source":"tweet = [\"late flight rude dirty refund wait\"]\nt = cv.transform(tweet)\nprint(tweet, \" is \", LogReg.predict(t))\ntweet = [\"great fun nice good\"]\nt = cv.transform(tweet)\nprint(tweet, \" is \", LogReg.predict(t))\ntweet = [\"bad\"]\nt = cv.transform(tweet)\nprint(tweet, \" is \", LogReg.predict(t))\ntweet = [\"bad bad bad bad bad \"]\nt = cv.transform(tweet)\nprint(tweet, \" is \", LogReg.predict(t))","execution_count":71,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c0f9b63bf5eec00c7d23deb6b82fb836d8e09b"},"cell_type":"code","source":"tweet = [\"should have taken the train\"]\nt = cv.transform(tweet)\nprint(tweet, \" is \", LogReg.predict(t))","execution_count":76,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}