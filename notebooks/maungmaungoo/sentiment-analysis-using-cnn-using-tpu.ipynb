{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndef ingest_train():\n#     data = pd.read_csv('/kaggle/input/sentiment-analysis-dataset-100000/Sentiment Analysis Dataset 100000.csv', encoding='latin-1')\n    data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n#     data = data[data.Sentiment.isnull() == False]\n    data = data[data.sentiment.isnull() == False]\n    data['Sentiment'] = data['sentiment']\n    data['SentimentText'] = data['review']\n#     data = data[data['SentimentText'].isnull() == False]\n    data = data[data['review'].isnull() == False]\n    data.reset_index(inplace=True)\n    data.drop('index', axis=1, inplace=True)\n    return data\ntrain = ingest_train()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import re\npat_1 = r\"(?:\\@|https?\\://)\\S+\"\npat_2 = r'#\\w+ ?'\nascii = r'[^\\x00-\\x7F]+'\ncombined_pat = r'|'.join((pat_1, pat_2))\nwww_pat = r'www.[^ ]+'\nhtml_tag = r'<[^>]+>'\nnegations_ = {\"isn't\":\"is not\", \"can't\":\"can not\",\"couldn't\":\"could not\", \"hasn't\":\"has not\",\n                \"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"aren't\":\"are not\",\n                \"haven't\":\"have not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\"n't\":\"not\",\n                \"don't\":\"do not\",\"shouldn't\":\"should not\",\"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"mightn't\":\"might not\",\"mustn't\":\"must not\",\"im\":\"i am\",\"mi\":\"my\"}\nnegation_pattern = re.compile(r'\\b(' + '|'.join(negations_.keys()) + r')\\b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_cleaner(text):\n    try:\n        stripped = re.sub(combined_pat, '', text)\n        stripped = re.sub(www_pat, '', stripped)\n        cleantags = re.sub(html_tag, '', stripped)\n        rm_non_ascii = re.sub(ascii, '', cleantags)\n        lower_case = rm_non_ascii.lower()\n        neg_handled = negation_pattern.sub(lambda x: negations_[x.group()], lower_case)\n        letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n        return letters_only\n    except:\n        return 'NC'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntrain['Text_Clean'] = np.vectorize(data_cleaner)(train['SentimentText'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\ntokens = [word_tokenize(sen) for sen in train.Text_Clean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstoplist = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stop_words(tokens): \n    return [word for word in tokens if word not in stoplist]\n\nfiltered_words = [remove_stop_words(sen) for sen in tokens]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nltk.stem import PorterStemmer\n\n# porter = PorterStemmer()\n\n# stem = []\n# for sentence in filtered_words:\n#     stem.append([\"\".join(porter.stem(word)) for word in sentence])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = [' '.join(sen) for sen in filtered_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Text_Final'] = result\ntrain['Tokens'] = filtered_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = []\nneg = []\nfor l in train.Sentiment:\n    if l == 'negative':\n        pos.append(0)\n        neg.append(1)\n    elif l == 'positive':\n        pos.append(1)\n        neg.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Pos']= pos\ntrain['Neg']= neg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[['Text_Final', 'Tokens', 'Sentiment', 'Pos', 'Neg']]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nneg_tweets = train[train.Neg == 1]\nneg_string = []\nfor t in neg_tweets.Text_Final:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200, background_color='white').generate(neg_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tweets = train[train.Pos == 1]\npos_string = []\nfor t in pos_tweets.Text_Final:\n    pos_string.append(t)\npos_string = pd.Series(pos_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma', background_color='white').generate(pos_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata_train, data_test = train_test_split(train, test_size=0.10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_training_words = [word for tokens in data_train[\"Tokens\"] for word in tokens]\ntraining_sentence_lengths = [len(tokens) for tokens in data_train[\"Tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_test_words = [word for tokens in data_test[\"Tokens\"] for word in tokens]\ntest_sentence_lengths = [len(tokens) for tokens in data_test[\"Tokens\"]]\nTEST_VOCAB = sorted(list(set(all_test_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\nprint(\"Max sentence length is %s\" % max(test_sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.models.keyedvectors as word2vec\nword2vec_path = \"/kaggle/input/google-news-vectors/GoogleNews-vectors-negative300-SLIM.bin\"\nword2vec = word2vec.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_average_word2vec(tokens_list, vector, generate_missing=True, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n    embeddings = clean_comments['Tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 544\nEMBEDDING_DIM = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\ntokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n\ntrain_word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(train_word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nx_train = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\nx_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_names = ['Pos', 'Neg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = data_train[label_names].values\ny_test = data_test[label_names].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Dropout, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\nfrom keras.models import Model\ndef ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=False)\n    \n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    convs = []\n    filter_sizes = [2,3,4,5,6]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n\n\n    l_merge = concatenate(convs, axis=1)\n\n    x = Dropout(0.1)(l_merge)  \n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    preds = Dense(labels_index, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=False, show_layer_names=False,\n    rankdir='TB', expand_nested=False, dpi=80\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 50\nbatch_size = 32 * tpu_strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple early stopping\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size, callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score = model.evaluate(x_test, y_test, verbose=1)\n# print(\"Test Score:\", score[0])\n# print(\"Test Accuracy:\", score[1])\n# evaluate the model\n_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n_, test_acc = model.evaluate(x_test, y_test, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(cnn_model.history['acc'])\nplt.plot(cnn_model.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc = 'upper left')\nplt.show()\n\nplt.plot(cnn_model.history['loss'])\nplt.plot(cnn_model.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc = 'upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#F1 Score, Recall and Precision\ntarget_names=['Positive', 'Negative']\ny_pred = model.predict(x_test)\nprint(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC AUC curve\nrocAuc = roc_auc_score(y_test, y_pred)\n\nfalsePositiveRate, truePositiveRate, _ = roc_curve(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n\nplt.figure()\n\nplt.plot(falsePositiveRate, truePositiveRate, color='green',\n         lw=3, label='ROC curve (area = %0.2f)' % rocAuc)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic of Sentiiment Analysis Model')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Other accuracy metrices\ny_pred = (y_pred > 0.5)\n\n#confusion metrix\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nsn.set(font_scale=1.4) # for label size\nx_axis_labels = y_axis_labels = target_names\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g', xticklabels=x_axis_labels, yticklabels=y_axis_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# movie reviews from IMDB website, Jocker Movie (2019)\nt_1 = \"This is a movie that only those who have felt alone and isolated can truly relate to it. You understand the motive and you feel sorry for the character. A lot of people will see this movie and think that it encourages violence. But truly, this movie should encourage each and every one of us to become a better person, treat everyone with respect and make each other feel like they belong in this world, instead of making them feel isolated.\"\nt_2 = \"Truly a masterpiece, The Best Hollywood film of 2019, one of the Best films of the decade... And truly the Best film to bring a comic book so chillingly and realistically to real ife. Remarkable Direction, Cinematography, Music and the Acting. Some people are surprised to find it DISTURBING and VIOLENT, but it's a necessity and message. It's about society and reflects those underappreciated/unrecognized/bullied people, proving they can do something too. The way it shows class difference, corruption and how rich and talented rule others around them is not exaggerated and that's what makes it different. It's BELIEVABLE. There could be multiple JOKERs living in our society that could shake those around them in much bitter way than the film shows making people uncomforting people. Consider this a wake up call, a message, but first a film. A PERFECT film.\"\nt_3 = \"Joaquin Phoenix gives a tour de force performance, fearless and stunning in its emotional depth and physicality. It's impossible to talk about this without referencing Heath Ledger's Oscar-winning performance from The Dark Knight, widely considered the definitive live-action portrayal of the Joker, so let's talk about it. The fact is, everyone is going to be stunned by what Phoenix accomplishes, because it's what many thought impossible - a portrayal that matches and potentially exceeds that of The Dark Knight's Clown Prince of Crime\"\ntests = [t_1, t_2, t_3]\ntests_tokens = tokenizer.texts_to_sequences(tests)\ntest_tokens_pad = pad_sequences(tests_tokens, maxlen=MAX_SEQUENCE_LENGTH)\n\n# predict\nmodel.predict(x=test_tokens_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('sentiment_analysis_using_cnn.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}