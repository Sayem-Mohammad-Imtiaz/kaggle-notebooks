{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\nimport pickle\nfrom gensim import corpora, models\nimport re\nfrom gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\nfrom nltk.corpus import stopwords\nimport pickle\nimport en_core_web_sm\nimport csv\nimport json\n\nfrom sklearn.preprocessing import Binarizer\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\n\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom scipy.spatial.distance import cosine, cdist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing the functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**To get similar docs to the target:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making a function for getting the top similar to add to the target table\n\ndef get_similar_docs(target_df, meta_df, d2v_model, d2v_target):\n    \"\"\"\n      This function takes:\n      [1] a target table dataframe\n      [2] the metadata table dataframe\n      [3] doc2vec model based on the metadata abstracts\n      [4] doc2vec model of the target table obtained with the metadata doc2vec model\n\n      Both the target and the metadata tables should contain columns: title, abstract and pdf_json_files.\n\n      For this function to run successfully, \n      the following packages need to be installed:\n       from gensim.models.doc2vec import Doc2Vec\n       import pandas as pd\n\n      At the end it prints the value count of the final dataframes that contains the following columns:\n      ('index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files');\n\n      It mades 3 dataframes:\n      * not_target: it contains all the new docs found\n      * similar_to_target_df: it contains the original results from the similarity function (target articles + 1st, 2nd and 3rd most similar)\n      * new_docs_target_df: it contains the target articles + similar docs that are not in the target table\n      \n      At the end it returns the new_docs_target_df and the not_target.\n      \n    \"\"\"\n    # Run the similarity test assuming all titles are in the filtered dataset:\n    similar_to_target = []\n    for i in range(len(target_df.title)):\n        sim_test = d2v_model.docvecs.most_similar(positive=[d2v_target[i]], topn=3)\n        #this way the list could be used to create a dataframe\n        similar_to_target.append([i, 'target', 1, \n                                  target_df.title[i], \n                                  target_df.abstract[i], \n                                  target_df.pdf_json_files[i]])\n        \n        similar_to_target.append([meta_df.index[sim_test[0][0]], 'most similar', sim_test[0][1], \n                                  meta_df.title[sim_test[0][0]], \n                                  meta_df.abstract[sim_test[0][0]], \n                                  meta_df.pdf_json_files[sim_test[0][0]]])\n\n        #checking if the second and third most similar docs are in target table, if not then append them:\n        if meta_df.title[sim_test[1][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df.index[sim_test[1][0]], 'second most similar', sim_test[1][1], \n                                      meta_df.title[sim_test[1][0]], meta_df.abstract[sim_test[1][0]], meta_df.pdf_json_files[sim_test[1][0]]])\n        \n        elif meta_df.title[sim_test[2][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df.index[sim_test[2][0]], 'third most similar', sim_test[2][1], \n                                      meta_df.title[sim_test[2][0]], meta_df.abstract[sim_test[2][0]], meta_df.pdf_json_files[sim_test[2][0]]])\n\n# creating a dataframe with the top 3 most similar docs of the target ones!\n    df_colum = ['original_index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files']\n    similar_to_target_df = pd.DataFrame(similar_to_target, index=range(len(similar_to_target)), columns=df_colum)\n    # removing the duplicates\n    new_docs_target_df = similar_to_target_df.drop_duplicates(subset='title', keep=\"first\", inplace=False)\n    new_docs_target_df.reset_index(drop=True, inplace=True)\n    # filtering the target docs, and staying only with the new docs\n    not_target = new_docs_target_df[new_docs_target_df['original_db'] !='target']\n    not_target.reset_index(drop=True, inplace=True)\n    \n    print('From the orginal similarity test, we get a total of ' + str(len(similar_to_target_df)) +' articles, counting the target ones and their most similars from metadata.')\n    print('After filtering the duplicates from that dataframe, we get a total of ' + str(len(new_docs_target_df)) +' articles.')\n    print('Finally, after filtering the target ones, we end with a total of ' + str(len(not_target)) +' new possible articles for the target table.')\n    \n    #return not_target, similar_to_target_df, new_docs_target_df\n    return new_docs_target_df, not_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To get relevant documents checking json files:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relevant_docs(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = original index,\n        [1] = original body text \n        [2] = if target or not   \n    \n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        ori_ind = dataframe.original_index[i]\n        ori_tab = dataframe.original_db[i]\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list that has: target_index, clean_json_body, original_json_body\n            if len(relevant_parts) != 0:\n                # convert json body_text into a text to have the original text \n                original_text=''\n                for d in range(len(body)):\n                    original_text = original_text+body[d]['text']\n                related_docs.append([ori_ind, original_text, ori_tab])\n                #related_docs.append([ori_ind, original_text, relevant_parts, clean_body])\n            \n        except:\n            TypeError\n        \n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To build a final dataframe of the new docs for the target table:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_relevant_docs_df(meta_df, relevant_docs_list):\n    \"\"\"\n    This function needs the metadata dataframe and a list of relevant documents.\n    \n    The list of relevant documents must contain one list for each relevant doc, that \n    has 3 values: [0] = original index,\n                  [1] = original body text \n                  [2] = if target or not  \n                  \n    Finally this function returns a dataframe with all the relevant documents body text obtained from the json file\n    and its corresponding columns from the metadata table.\n    \n    \"\"\"\n    relevant_for_target = []\n    for i in range(len(relevant_docs_list)): \n    #this way the list could be used to create a dataframe\n        index_rev = relevant_docs_list[i][0]\n        relevant_for_target.append([index_rev, \n                                    meta_df.publish_time[index_rev], \n                                    meta_df.title[index_rev], \n                                    meta_df.abstract[index_rev], \n                                    meta_df.cord_uid[index_rev], \n                                    meta_df.doi[index_rev],\n                                    meta_df.journal[index_rev],\n                                    meta_df.url[index_rev],\n                                    meta_df.pdf_json_files[index_rev],\n                                    meta_df.date[index_rev],\n                                    relevant_docs_list[i][1],\n                                    relevant_docs_list[i][2]\n                                   ])\n    # creating a dataframe with the relevant docs including all needed columns\n    df_colum = ['original_index', 'publish_time', 'title', 'abstract', 'cord_uid', 'doi',\n                    'journal', 'url', 'pdf_json_files', 'date', 'body_text', 'target_or_not']\n    relevant_df = pd.DataFrame(relevant_for_target, index=range(len(relevant_for_target)), columns=df_colum)\n    return relevant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the metadata table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/CORD-19-research-challenge/'\nmeta_df = pd.read_csv(path + 'metadata.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are ' + str(len(meta_df)) + ' articles in total')\nprint(\"Cols names: {}\".format(meta_df.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading filtered metadata file\nIt has the stemmed abstracts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the filtered original dataset into a dataframe\npath2 = '../input/stemmed-meta-and-risk-targets-doc2vec-model/'\nmeta = pickle.load(open(path2 + 'meta_stemmed', 'rb'))\nmeta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are ' + str(len(meta)) + ' articles in total')\nprint(\"Cols names: {}\".format(meta.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making the corpus for the filtered metadata","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_meta= [doc.split() for doc in list(meta.abstract)]\nprint(corpus_meta[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the doc2vec model created with the filtered metadata\nThis model is implemented with epochs=200","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pickle.load(open(path2+'d2v_model_saved.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing the diabetes target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading the stemmed dataframe of the target risk","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_df= pickle.load(open(path2 + 'diabetes_stemmed', 'rb'))\ndiabetes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(diabetes_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying the model to the target dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"making the corpus for smoking target:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_diabetes= [doc.split() for doc in list(diabetes_df.abstract)]\nprint(corpus_diabetes[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the doc2vec model of the target risk\nd2v_diabetes = []\nfor i in range(len(corpus_diabetes)):\n    model.random.seed(0)\n    d2v_diabetes.append(model.infer_vector(corpus_diabetes[i], epochs=200))\n\nd2v_diabetes[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting similar docs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"running the function and obtaining **similar_to_target_df, new_docs_target_df**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function filtering the second and third most similar\ndiabetes_new_df, dia_not_target = get_similar_docs(diabetes_df, meta, model, d2v_diabetes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### change to a dataframe that includes original target articles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_new_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking how many articles are missing their json file in the new dataframe\ndiabetes_new_df.notna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(diabetes_df))\nprint(len(diabetes_new_df))\nprint(len(diabetes_new_df)-len(diabetes_df))\nprint('we only run the topic test with this many articles because of no json file:')\nprint(len(diabetes_new_df)-72)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking how many articles are missing their json file in the new dataframe\ndiabetes_df.notna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For the diabetes topic:**\n\nAfter the similarity test:\n* We got 64 new possible target articles, one for each target article.\n* Of the total 128 articles, only 56 have a json file available.  \n\n* From the original target table, only 31 articles have a json file available, \n* So we can apply the relevance function to only 25 new articles.\n\n**After running the relevance function:**\n* There are 27 original target articles and 9 new articles identified as relevant\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Getting the relevant documents based on their body text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('diabetes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia_relevant_docs = get_relevant_docs(diabetes_new_df, 'diabet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia_new_relevant = get_relevant_docs(dia_not_target, 'diabet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Modifying function so that it works with target table dataframe\ndef get_relevant_docs_mod(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = original index,\n        [1] = original body text \n        [2] = if target or not   \n    \n    MODIFIED SO IT WORKS WITH TARGET TABLE!!\n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list that has: target_index, clean_json_body, original_json_body\n            if len(relevant_parts) != 0:\n                original_text=''\n                for d in range(len(body)):\n                    original_text = original_text+body[d]['text']\n                #related_docs.append([ori_ind, original_text, ori_tab])\n                related_docs.append([relevant_parts, clean_body])\n            \n        except:\n            TypeError\n        # convert json body_text into a text list\n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia_relevant_target = get_relevant_docs_mod(diabetes_df, 'diabet')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('New articles added: ' + str(len(dia_new_relevant)))\nprint('Total relevant target articles: ' + str(len(dia_relevant_docs)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a table of the relevant docs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtaining the info of the relevant doc in the similarity dataframe\ndiabetes_new_df[diabetes_new_df.original_index == dia_relevant_docs[0][0]]\n# we can see the similarity of this article","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia_relevant_df = build_relevant_docs_df(meta, dia_relevant_docs)\ndia_relevant_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia_relevant_df.target_or_not.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(dia_relevant_df, open(\"diabetes_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing the hypertension target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_df= pickle.load(open(path2 + 'hypertension_stemmed', 'rb'))\nhyper_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(hyper_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a corpus to run the doc2vec model\ncorpus_hyper= [doc.split() for doc in list(hyper_df.abstract)]\nprint(corpus_hyper[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the doc2vec model of the target risk\nd2v_hyper = []\nfor i in range(len(corpus_hyper)):\n    model.random.seed(0)\n    d2v_hyper.append(model.infer_vector(corpus_hyper[i], epochs=200))\n\nd2v_hyper[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function filtering the second and third most similar\n#hyper_new_docs, hyper_similar_raw, hyper_uniq_sim = get_similar_docs(hyper_df, meta, model, d2v_hyper)\nhyper_new_docs, hyper_not_target = get_similar_docs(hyper_df, meta, model, d2v_hyper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_new_docs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('hypertension')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_relevant_docs = get_relevant_docs(hyper_new_docs, 'hypertens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_new_relevant = get_relevant_docs(hyper_not_target, 'hypertens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_relevant_target = get_relevant_docs_mod(hyper_df, 'hypertens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtaining the info of the relevant doc in the similarity dataframe\nhyper_new_docs[hyper_new_docs.original_index == hyper_relevant_docs[0][0]]\n# we can see the similarity of this article","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_relevant_df = build_relevant_docs_df(meta, hyper_relevant_docs)\nhyper_relevant_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(hyper_relevant_df, open(\"hypertension_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing the smoking target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_df= pickle.load(open(path2 + 'smoking_stemmed', 'rb'))\nsmoke_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(smoke_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a corpus to run the doc2vec model\ncorpus_smoke= [doc.split() for doc in list(smoke_df.abstract)]\nprint(corpus_smoke[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the doc2vec model of the target risk\nd2v_smoke = []\nfor i in range(len(corpus_smoke)):\n    model.random.seed(0)\n    d2v_smoke.append(model.infer_vector(corpus_smoke[i], epochs=200))\n\nd2v_smoke[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function filtering the second and third most similar\n#hyper_new_docs, hyper_similar_raw, hyper_uniq_sim = get_similar_docs(hyper_df, meta, model, d2v_hyper)\nsmoke_new_docs, smoke_not_target = get_similar_docs(smoke_df, meta, model, d2v_smoke)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_new_docs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_new_docs.notna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('smoke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_relevant_docs = get_relevant_docs(smoke_new_docs, 'smoke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_new_relevant = get_relevant_docs(smoke_not_target, 'smoke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smok_target_relevant = get_relevant_docs_mod(smoke_df, 'smoke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_relevant_df = build_relevant_docs_df(meta, smoke_relevant_docs)\nsmoke_relevant_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_relevant_df.target_or_not.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(smoke_relevant_df, open(\"smoking_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}