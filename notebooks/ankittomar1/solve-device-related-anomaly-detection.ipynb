{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Why should you work with this type of dataset\nIf you work on this type of dataset, you can implmenet similar solutions in any IoT related project in your organization or personal project. \n\nThis dataset can help you to learn:\n    - how to approach sensors data\n    - how to find anomaly which can help you to know when device is going to break\n    - It do have timeseries angle, so could look for that too. ","metadata":{}},{"cell_type":"markdown","source":"# Step 1. \n - Have a quick look on the dataset, which is very much needed to build the thought process around the data\n - Reread problem statement multiple times and try to understand how to correlate the dataset and problem. \n - Plot the visualization \n \n Our problem statment: Look for the correlation of sensors which leads to device breakdown\n","metadata":{}},{"cell_type":"markdown","source":"\n1. Quick Checklist for this dataset\n*     timeseries forecasting problem \n*     machine status - 3 convert that into label encoding \n*     all numerical values\n*     anomlay detection \n*     labelled data - supervised learning , classification \n*     look for correlation matrix \n*     look for skewness in the data \n*     check imbalance data if any ","metadata":{}},{"cell_type":"code","source":"# load the dataset \nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('../input/sensor.csv')\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()\n\n# 01-Apr-2018 to 31-Aug-2018 \n# apr, may, jun, jul, aug - 5 months every min data is collected ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df['Unnamed: 0']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert time into index \ndf['index'] = pd.to_datetime(df['timestamp'])\ndf.index = df['index']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete the colunmns \ndel df['index']\ndel df['timestamp']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sensor_15'].nunique() # no unique - complete zero\n# drop the column \ndf.drop(['sensor_15'], axis=1, inplace = True)\ndf.shape","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# machine status - no null \n# we will drop na in whole dataframe \ndf['sensor_00'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# machine status\ndf['machine_status'].unique()#'NORMAL', 'BROKEN', 'RECOVERING' \ndf['machine_status'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw a countplot for machine status \nimport seaborn as sns\nsns.countplot(y = df['machine_status'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** We are going to figureout what makes device to breakdown, \n* it is highly imbalanced data  and undersampling can't help here \n* we will experiement with SMOTE or oversampling ","metadata":{}},{"cell_type":"code","source":"# apply label encoder to encode the machine status\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['machine_status'] = le.fit_transform(df['machine_status'])\ndf['machine_status'].value_counts()\n\n# 1 - normal \n# 2 - recovering \n# 0 - broken","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  look on complete data frame when device is broken\ndf_broken = df[df.machine_status ==0]\ndf_broken\n\n# there is no nan value corellation for broken device \n#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.plot(df['sensor_02'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputation for null values \ndf['sensor_04'].hist()\n# data is skewwed so we need to use median value to fill the data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us figureout NaN values \ndf['sensor_00'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sensor_50'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# used ffill method to fill the missing values\ndf = df.fillna(method='ffill')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['machine_status'], axis=1)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = df['machine_status']\nY.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the logitic regression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.30, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply \nlogit = LogisticRegression()\nmodel = logit.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\ny_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ncm = pd.crosstab(y_test,y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy is not a good metrics for Anomaly detection and imblaanced dataset\naccuracy = accuracy_score(y_test, y_pred)\naccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\ncr = classification_report(y_pred, y_test)\nprint(cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# these reports are not good \nwe will use Isolation forest and oneSVM for modelling \nxgboosting feature_importance and PCA for dimension reduction \nbefore that we will divide this dataset into 2 probelms\n machine status - normal + broken, normal + recovery, recovery+ broken ","metadata":{}},{"cell_type":"code","source":"df.shape # look on the shape of the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" machine status - normal + broken","metadata":{}},{"cell_type":"code","source":"df1 = df.copy()\ndf1 = df[(df1.machine_status ==1) | (df1.machine_status ==0)]\ndf1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}