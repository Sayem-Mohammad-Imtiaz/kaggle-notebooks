{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Trabalho Data Mining e Machine Learning II\n\nCampos do arquivo CSV\n\n**BAD** \n1 = client defaulted on loan 0 = loan repaid\n\n**LOAN** \nAmount of the loan request\n\n**MORTDUE** \nAmount due on existing mortgage\n\n**VALUE** \nValue of current property\n\n**REASON** \nDebtCon = debt consolidation HomeImp = home improvement\n\n**JOB** \nSix occupational categories\n\n**YOJ** \nYears at present job\n\n**DEROG** \nNumber of major derogatory reports\n\n**DELINQ** \nNumber of delinquent credit lines\n\n**CLAGE** \nAge of oldest trade line in months\n\n**NINQ** \nNumber of recent credit lines\n\n**CLNO** \nNumber of credit lines\n\n**DEBTINC** \nDebt-to-income ratio\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n# Trabalho final Data Mining e Machine Learning II\n\nNosso objetivo é criar uma modelo para que consiga se prever quando uma pessoa vai ou não pagar seu emprestimo\n\nTemos uma base de dados com 5960 rows e 13 colunas. A coluna com o resultado é a 'BAD' que com valor 0 significa que pagou e com valor 1 significa que não pagou.\n\nTemos uma proporção inicial de quase 20% para BAD. Para resolver isso vamos tentar utilizar resample nos dados.\n\nTemos 5271 missing values, vamos utilizar diversos métodos abaixo, preencher com valores default, como Other e DebtCon para os campos JOB e REASON que são os valores com maior quantidade e depois transformar essas 2 colunas em númericas.\nDEROG e DELINQ tem como maior quantidade dos valores 0.0, foi feito default para os 2 campos como 0.\n\nDepois destes tratamento temos ainda a maioria de missing values no campo DEBTINC que acredito que seja ter um valor bem importante para a previsão.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Análise exploratória dos dados\n# Estatística do negócio\n# correlação\n# distribuição\n# interpretação desses dados\n# tratamento converter, dividir, criar colunas, etc\n# grid search\n# hiperparametros","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/hmeq-data/hmeq.csv')\n\ndf.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['REASON'].astype('category').cat.categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['JOB'].astype('category').cat.categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['REASON'].astype('category').cat.codes\n\n# -1 é nulo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['JOB'].astype('category').cat.codes\n\n# -1 é nulo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manter apenas rows com pelo menos 8 campos não nulos \ndf.dropna(thresh=8, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preencher no campo REASON DebtCon como default para nulo\ndf.REASON.fillna('DebtCon', inplace=True)\n\n# preencher no campo JOB Other como default para nulo\ndf.JOB.fillna('Other', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.JOB.fillna('Other', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convertendo as colunas categórias em colunas numéricas\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        df[col] = df[col].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nTotal Missing: \\n\", df.isna().sum())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"DEROG\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"DEROG\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"DELINQ\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"MORTDUE\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"VALUE\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"YOJ\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"CLAGE\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"NINQ\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"CLNO\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"DEROG\"].fillna(value=0,inplace=True)\ndf[\"DELINQ\"].fillna(value=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"INFO     : \" ,df.info())\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())\nprint (\"\\nTotal Missing: \\n\", df.isna().sum())\nprint (\"\\nPorcentagem Missing: \\n\", df.isna().mean().round(4) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tratamento final dos campos\n\nforam criadas 2 bases df2 e df3.\n\nna df2 foram dropadas qualquer row com dados nulos\nna df3 foram preenchidos utilizando ffill e bfill que é forward propagation e backward propagation, pois acredito que a média não seria um valor muito bom pela diferença dos valores"},{"metadata":{"trusted":true},"cell_type":"code","source":"# criar uma base com dados limpos (removendo todos os missing)\ndf2 = df.dropna()\n\n# criar uma base com dados utilizando ffill e bfill\ndf3 = df.fillna(method='ffill')\ndf3 = df3.fillna(method='bfill')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nExiste uma correlação entre montdue e value que é percebida nos 2 gráficos abaixo\n\npode-se perceber que a base df3 tem uma quantidade bem maior de valores BAD = 1, espera-se que isso não cause problemas nos modelos, caso aconteça vai ser percebido nos resultados finais"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(18,18))\n\n\ncorrMatrix = df[df['DEBTINC'].notnull()].corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import norm\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 30, 15\n\ndic = {\"LOAN\":df[\"LOAN\"],\"BAD\":df[\"BAD\"],\"MORTDUE\":df[\"MORTDUE\"],\"VALUE\":df[\"VALUE\"],\"YOJ\":df[\"YOJ\"]}\nrcParams['figure.figsize'] = 5, 5\n\ndf_pair = pd.DataFrame(dic)\nsns.pairplot(df_pair,vars=['LOAN', 'MORTDUE',\"VALUE\",\"YOJ\"],hue=\"BAD\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df2['BAD'], data=df2).set_title(\"Distribuição para df2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=df3['BAD'], data=df3).set_title(\"Distribuição para df3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['BAD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['BAD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=df[col], data=df)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nfeats  = [c for c in df.columns if c not in ['BAD']]\n\n\n# df2\ndf2_x_treino, df2_x_valid, df2_y_treino, df2_y_valid = train_test_split(df2[feats],\n                                                                              df2['BAD'], \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\n# df3\ndf3_x_treino, df3_x_valid, df3_y_treino, df3_y_valid = train_test_split(df3[feats],\n                                                                              df3['BAD'], \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\nsmote = SMOTE()\n\n# resample df2\ndf2_rx, df2_ry = smote.fit_sample(df2[feats], df2['BAD'])\n\ndf2_rx_treino, df2_rx_valid, df2_ry_treino, df2_ry_valid = train_test_split(df2_rx,\n                                                                              df2_ry, \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n\n\n# resample df3\ndf3_rx, df3_ry = smote.fit_sample(df3[feats], df3['BAD'])\n\ndf3_rx_treino, df3_rx_valid, df3_ry_treino, df3_ry_valid = train_test_split(df3_rx,\n                                                                              df3_ry, \n                                                                              train_size=0.7, test_size=0.3,\n                                                                random_state=123)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_rx.shape, df2_ry.shape, df2_rx_treino.shape, df2_rx_valid.shape, df2_ry_treino.shape, df2_ry_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3_rx.shape, df3_ry.shape, df3_rx_treino.shape, df3_rx_valid.shape, df3_ry_treino.shape, df3_ry_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_x.shape, df2_y.shape, df2_x_treino.shape, df2_x_valid.shape, df2_y_treino.shape, df2_y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3_x.shape, df3_y.shape, df3_x_treino.shape, df3_x_valid.shape, df3_y_treino.shape, df3_y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom xgboost import XGBClassifier\n\n\nmodels = []\nmodels.append(('RandomForest', RandomForestClassifier(random_state=123)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=123)))\nmodels.append(('XGB', XGBClassifier(random_state=123)))\n\n\nX_train = df2_x_treino\nX_valid =df2_x_valid\ny_train = df2_y_treino\ny_valid = df2_y_valid\n\nscore = []\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base dropada - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    score.append(f_dict)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = df3_x_treino\nX_valid =df3_x_valid\ny_train = df3_y_treino\ny_valid = df3_y_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base fill  - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    \n    score.append(f_dict)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = df2_rx_treino\nX_valid =df2_rx_valid\ny_train = df2_ry_treino\ny_valid = df2_ry_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base dropada resample - ' + name,\n        'accuracy': accuracy_valid,\n    }\n    score.append(f_dict)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = df3_rx_treino\nX_valid =df3_rx_valid\ny_train = df3_ry_treino\ny_valid = df3_ry_valid\n\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v)\n    f_dict = {\n        'model': 'base fill resample - ' + name ,\n        'accuracy': accuracy_valid,\n    }\n    \n    score.append(f_dict)\n    \nscore = pd.DataFrame(score, columns = ['model', 'accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analise dos resultados\n\nTodos nossos modelos foram submetidos ao GridSearchCV para encontrar os melhores parametros (hiper parameters)\n\nO melhor modelo para a base com dados dropados (df2) os dados foi a mesma random forest e XGB com resample SMOTE tendo uma acuracia de 98.1%\n\nJá para a base preenchida com ffill e bfill aconteceu o mesmo comportamento aciuma com o algoritmo random forest e XGB utilizando resample SMOTE tendo acuracia de 95.6%\n\nO melhor modelo para sem resample foi a com base com dados nulos dropados utilizando XGB com acuracia de 96%\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/ajay1735/my-credit-scoring-model\n# função alterada e utilizada de maneira simplificada para exemplo abaixo\nimport itertools\ndef plot_confusion_matrix(cm,title='Matrix de confusão',classes=[0,1], cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Valor real')\n    plt.xlabel('Valor predito')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df2_rx_treino\nX_valid =df2_rx_valid\ny_train = df2_ry_treino\ny_valid = df2_ry_valid\n\nparam_grid = {}\nmy_model = GridSearchCV(RandomForestClassifier(random_state=123),param_grid)\nmy_model.fit(X_train, y_train)\npredictions_t = my_model.predict(X_train) \npredictions_v = my_model.predict(X_valid)\naccuracy_train = accuracy_score(y_train, predictions_t) \naccuracy_valid = accuracy_score(y_valid, predictions_v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# informações do melhor modelo, random forest com resample na base com nulos dropados\nprint(my_model.best_estimator_)\npd.Series(my_model.best_estimator_.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_valid, predictions_v)\nnp.set_printoptions(precision=2)\nprint('Matrix de confusão')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm, 'base dropada resample - RandomForest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df2_x_treino\nX_valid =df2_x_valid\ny_train = df2_y_treino\ny_valid = df2_y_valid\n\nparam_grid = {}\nmy_model = GridSearchCV(RandomForestClassifier(random_state=123),param_grid)\nmy_model.fit(X_train, y_train)\npredictions_t = my_model.predict(X_train) \npredictions_v = my_model.predict(X_valid)\naccuracy_train = accuracy_score(y_train, predictions_t) \naccuracy_valid = accuracy_score(y_valid, predictions_v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_valid, predictions_v)\nnp.set_printoptions(precision=2)\nprint('Matrix de confusão')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm, 'base dropada - RandomForest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# informações do melhor modelo, random forest com resample na base com nulos dropados\nprint(my_model.best_estimator_)\npd.Series(my_model.best_estimator_.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Informações dos melhores modelos\n\nPode-se ver a matrix de confusão para base resample e base sem resample (melhor caso) com valores muito bons, tendo em vista sua acuracia, era de se esperar isso\n\nA base sem resample é possível perceber a importancia do DEBTINC bem grande se comparada a importancia no outro modelo."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}