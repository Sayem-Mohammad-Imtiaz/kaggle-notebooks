{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Linear models and Least Squares using equation\nGiven a vector of inputs $X^T = (X_1,X_2,...,X_p),$ we predict the output $Y$ via the model <br/>\n\n$\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^pX_j\\hat{\\beta_j}$<br/>\n\nThe term $\\hat{\\beta_0}$ is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable 1 in $X$, include $\\hat{\\beta_0}$ in the vector of coefficients $\\hat{\\beta}$, and then write the linear model in vector form as an inner product <br/>\n\n$\\hat{Y} = X^T\\hat{\\beta}$ <br/>\n\nwhere $X^T$ denotes vector or matrix transpose ($X$ being a column vector). Here we are modeling a single output, so $\\hat{Y}$ is a scalar; in general $\\hat{Y}$ can be a K–vector, in which case $\\beta$ would be a $p \\times K$ matrix of coefficients. In the (p + 1)-dimensional input–output space, $(X, \\hat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the $Y$-axis at the point $(0,\\hat{\\beta_0})$. From now on we assume that the intercept is included in $\\hat{\\beta}$.<br/>\n\nViewed as a function over the p-dimensional input space, $f(X) = X^T\\beta$ is linear, and the gradient $f'(X) = \\beta$ is a vector in input space that points\nin the steepest uphill direction. <br/>\n\nHow do we fit the linear model to a set of training data? There are many different methods, but by far the most popular is the method of **least squares**. In this approach, we pick the coefficients β to minimize the residual sum of squares <br/>\n\n$RSS(\\beta) = \\sum_{i=1}^N(y_i − x^T_i\\beta)^2$ <br/>\n\n$RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize\nin matrix notation. We can write <br/>\n\n$RSS(\\beta) = (y − X\\beta)^T(y − X\\beta)$ <br/>\n\nwhere $X$ is an $N \\times p$ matrix with each row an input vector, and y is an N-vector of the outputs in the training set. Differentiating w.r.t. $\\beta$ we get the normal equations <br/>\n\n$X^T(y − X\\beta) = 0$ <br/>\n\nIf $X^TX$ is nonsingular, then the unique solution is given by <br/>\n\n$\\hat{\\beta} = (X^TX)^{−1}X^Ty $ <br/>\n\nand the fitted value at the ith input $x_i$ is $\\hat{y_i} = \\hat{y}(x_i) = x^T_i\\hat{\\beta}$. At an arbitrary input $x_0$ the prediction is $\\hat{y}(x_0) = x^T_0\\hat{\\beta}$ The entire fitted surface is characterized by the p parameters $\\hat{\\beta}$. Intuitively, it seems that we do not need a very large data set to fit such a model."},{"metadata":{},"cell_type":"markdown","source":"# Linear models and Least Squares using iteration\nFinding $\\hat{\\beta}$ using $\\hat{\\beta} = (X^TX)^{−1}X^Ty $ equation may take much longer time due to finding od inverse of matrix ($O(n^3)$). If data is larger than 10000 inputs so we need alternate method. <br/>\n\nThe below method we can use for larger inputs. It takes $O(kn^2)$ time. <br/>\n\nrepeat until convergence:<br/>\n{<br/>\n&nbsp;&nbsp;&nbsp; $ \\beta_j = \\beta_j − \\alpha \\frac{1}{m} \\sum_{i=1}^p(\\hat{y}(x(i))−y(i))⋅x_j(i) $  &nbsp;&nbsp; for j = 0...n <br/>\n}<br/>\nUse temporary $\\beta$ in between loop and after 1 iteration is over then simultaneously update all values of $\\beta$ vector. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Training\n\ndf = pd.read_csv('/kaggle/input/random-linear-regression/train.csv')\ndf = df.dropna()\ndata = df.values\n\nX = data[:,0]\nY = data[:,1]\n\ntemp = np.dot(X,X)\nB = np.dot(X,Y)\nBeta = B/temp #Model\n\nprint(Beta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$\\hat{\\beta} = (X^TX)^{−1}X^Ty $ <br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing\n\ndf_test=pd.read_csv('/kaggle/input/random-linear-regression/test.csv')\ndf_test = df_test.dropna()\ndata_test = df_test.values\n\nX_test = data_test[:,0]\nY_test = data_test[:,1]\n\nY_cap = X_test * Beta #Predection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison\n\nrmse = (((Y_cap - Y_test)**2).mean())**0.5\nprint(rmse)\n\nplt.figure(num=None, figsize=(20, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(X_test, Y_test,'go',X_test, Y_cap,'r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as rm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.0001\nbeta = rm.random()\nm = len(Y_cap)\n'''\nfor i in range(50):\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp/m\n    print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    print(\"RMSE =\",rmse)\n'''\n\nrmse1 = 1000\nrmse2 = 1000\nrmse3 = 1000\n\nwhile True:\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp/m\n    #print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    #print(\"RMSE =\",rmse)\n    rmse1 = rmse2\n    rmse2 = rmse3\n    rmse3 = rmse\n    temp_mean = (rmse1 +rmse2 +rmse3 )/3\n    if(abs(temp_mean - rmse) < 0.001):\n        break\nprint(\"Beta\",beta)\nprint(\"RMSE =\",rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_rmse = 0 \nwhile True:\n    Y_cap = beta * X\n    temp = np.dot((Y_cap - Y),X)\n    beta = beta - alpha*temp/m\n    #print(\"Beta\",beta)\n    rmse = (((Y_cap - Y)**2).mean())**0.5\n    #print(\"RMSE =\",rmse)\n    if(temp_mean > rmse):\n        break\n    temp_mean = rmse\nprint(\"Beta\",beta)\nprint(\"RMSE =\",rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$ \\beta_j = \\beta_j − \\alpha \\frac{1}{m} \\sum_{i=1}^p(\\hat{y}(x(i))−y(i))⋅x_j(i) $  &nbsp;&nbsp; for j = 0...n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing\n\nY_cap = X_test * beta #Predection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison\n\nrmse = (((Y_cap - Y_test)**2).mean())**0.5\nprint(rmse)\n\nplt.figure(num=None, figsize=(20, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(X_test, Y_test,'go',X_test, Y_cap,'r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}