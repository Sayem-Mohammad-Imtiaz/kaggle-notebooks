{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- https://www.kaggle.com/team-ai/spam-text-message-classification\n\n- https://www.kaggle.com/mdmahmudferdous/nlp-tutorial-spam-msg-classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# importとdf確認","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# やりたいこと\n- spamとhamの分類(モデルはなんでも良い)\n- nlp使って特徴生成\n- 参考notebook\n - https://www.kaggle.com/nihalbey/spam-detection-and-deep-nlp","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df[\"Message\"][:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- messageは前処理必要\n    - 大文字小文字\n    - 数字\n    - 記号\n    - \n- 前処理系関数作っておくと便利なので、いろいろ集めておく\n    - https://github.com/upura/probspace-youtube/blob/master/ayniy/preprocessing/text.py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_text(text):\n    text = re.sub(\"[^a-zA-Z]\",\" \",text) # かなり大胆なsub\n    text = text.lower()\n    \n    text = nltk.word_tokenize(text)\n    \n    lemma = nltk.WordNetLemmatizer()\n    text= [lemma.lemmatize(word) for word in text]\n    \n    text = \" \".join(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Message\"] = df[\"Message\"].apply(preprocessing_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Message\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(analyzer='word', stop_words = \"english\", ngram_range=(1,2))\ntfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = \"english\", ngram_range=(1,2))\n\ncount_mat = count_vectorizer.fit_transform(df[\"Message\"])\ntfidf_mat = tfidf_vectorizer.fit_transform(df[\"Message\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_mat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_comp_SVD = 20\nn_comp_TSNE = 2\n\n# SVD\ntext_svd = TruncatedSVD(n_components=n_comp_SVD,random_state=0)\ndf_count_svd = pd.DataFrame(text_svd.fit_transform(count_mat))\ndf_count_svd.columns = ['count_vec_svd_'+str(j+1) for j in range(n_comp_SVD)]\n\ntext_svd = TruncatedSVD(n_components=n_comp_SVD,random_state=0)\ndf_tfidf_svd = pd.DataFrame(text_svd.fit_transform(tfidf_mat))\ndf_tfidf_svd.columns = ['tfidf_vec_svd_'+str(j+1) for j in range(n_comp_SVD)]\n\n# TSNE(ちょっと重いので、sklearnではない方のmodule使うとか(こっちの方が速いらしい))\ntsne = TSNE(n_components=n_comp_TSNE, random_state = 0)\ndf_count_tsne = pd.DataFrame(tsne.fit_transform(count_mat))\ndf_count_tsne.columns = ['count_vec_tsne_'+str(j+1) for j in range(n_comp_TSNE)]\n\ntsne = TSNE(n_components=n_comp_TSNE, random_state = 0)\ndf_tfidf_tsne = pd.DataFrame(tsne.fit_transform(tfidf_mat))\ndf_tfidf_tsne.columns = ['tfidf_vec_tsne_'+str(j+1) for j in range(n_comp_TSNE)]\n\ndf = pd.concat([df, df_count_svd,df_tfidf_svd,df_count_tsne,df_tfidf_tsne],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 80)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"count_vec_tsne_1\", y = \"count_vec_tsne_2\", hue = \"Category\", data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"tfidf_vec_tsne_1\", y = \"tfidf_vec_tsne_2\", hue = \"Category\", data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- tfidfの方が明らかに分離しやすそう。\n- tfidf_vec_tsne1だけでspamは結構抽出できてる","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 簡単な学習","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.iloc[:,0].values\nx = df.drop([\"Category\", \"Message\"], axis = 1)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We make model for predict\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"the accuracy of our model: {}\".format(nb.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 200)\nlr.fit(x_train,y_train)\nprint(\"our accuracy is: {}\".format(lr.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 生のtfidfを相互情報量で特徴選択してから投入","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, mutual_info_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_tfidf, x_test_tfidf, y_train, y_test = train_test_split(tfidf_mat,y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 重い\nselector = SelectKBest(k = 10000, score_func = mutual_info_classif)\nselector.fit(x_train_tfidf, y_train)\nx_train_new = selector.transform(x_train_tfidf)\nx_test_new = selector.transform(x_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 200)\nlr.fit(x_train_new,y_train)\nprint(\"our accuracy is: {}\".format(lr.score(x_test_new,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lgbでcv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf[\"Category\"] = le.fit_transform(df[\"Category\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Category\"]\nx = df.drop([\"Category\", \"Message\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習\ny_preds = []\nmodels = []\noof_train = np.zeros(len(x))\n\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n\nparams = {\n    \"boosting_type\" : \"gbdt\",\n    \"objective\" : \"binary\",\n    \"metric\" : \"binary_logloss\",\n    \"max_depth\" : -1, # デフォルト値は-1で0以下の値は制限なしを意味する．\n    #\"num_leaves\" : 31,  #木にある分岐の個数．sklearnのmax_depthsklearnのmax_depthとの関係はnum_leaves=2^max_depth\n                                    #デフォルトは31．大きくすると精度は上がるが過学習が進む．\n    \"learning_rate\": 0.03, \n    \"feature_fraction\" : 0.7, #学習の高速化と過学習の抑制に使用される．データの特徴量のfeature_fraction * 100 % だけ使用する．\n    'bagging_fraction': 0.7, #like feature_fraction, but this will randomly select part of data without resampling\n    \"bagging_freq\" : 5, #frequency for bagging. 0 means disable bagging; k means perform bagging at every k iteration\n                                    # Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n    \"early_stopping_rounds\" : 100,\n    'n_estimators':2000, # aliases: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators. \n                                        # number of boosting iterations\n     \"seed\" : 42,\n    \"reg_alpha\":0.1,\n    \"reg_lambda\":0.1 #0.1\n}\n\n\nfor fold, (train_index, val_index) in enumerate(kf.split(x, y)):\n    tr_x = x.iloc[train_index, :]\n    tr_y = y[train_index]\n    val_x = x.iloc[val_index, :]\n    val_y = y[val_index]\n    \n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(val_x, val_y, reference = lgb_train)\n    \n    model = lgb.train(params, \n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval], \n                      verbose_eval= 10\n                     )\n    oof_train[val_index] = model.predict(val_x, num_iteration = model.best_iteration)\n    #y_pred = model.predict(test, num_iteration = model.best_iteration)\n    \n    #y_preds.append(y_pred)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(np.round(oof_train), y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = pd.DataFrame(index = x.columns)\nfor i, m in enumerate(models):\n    fi[\"model_\"+str(i+1)] = m.feature_importance(importance_type = \"gain\")\nfi[\"fi_ave\"] = fi.mean(axis = 1)\nfi.sort_values(by = \"fi_ave\", inplace = True, ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,12))\nsns.barplot(x = fi[\"fi_ave\"][:30], y = fi.index[:30])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"tfidf_vec_tsne_1\", y = \"tfidf_vec_svd_5\", data = df, hue = \"Category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LatentDirichletAllocation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=10,random_state=0)\ncount_lda = lda.fit_transform(count_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_features = count_vectorizer.get_feature_names()\nfor tn in range(10):\n    print(\"topic #\"+str(tn))\n    row = lda.components_[tn]\n    words = ', '.join([count_features[i] for i in row.argsort()[:-20-1:-1]])\n    print(words, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_lda = lda.fit_transform(tfidf_mat)\ntfidf_features = tfidf_vectorizer.get_feature_names()\nfor tn in range(10):\n    print(\"topic #\"+str(tn))\n    row = lda.components_[tn]\n    words = ', '.join([tfidf_features[i] for i in row.argsort()[:-20-1:-1]])\n    print(words, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_count_df = pd.DataFrame(count_lda)\nlda_count_df = lda_count_df.add_prefix(\"lda_count_\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_tfidf_df = pd.DataFrame(tfidf_lda)\nlda_tfidf_df = lda_tfidf_df.add_prefix(\"lda_tfidf_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## くっつけて学習","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lda = pd.concat([df,lda_count_df, lda_tfidf_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_lda[\"Category\"]\nx = df_lda.drop([\"Category\", \"Message\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習\ny_preds = []\nmodels = []\noof_train = np.zeros(len(x))\n\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n\nparams = {\n    \"boosting_type\" : \"gbdt\",\n    \"objective\" : \"binary\",\n    \"metric\" : \"binary_logloss\",\n    \"max_depth\" : -1, # デフォルト値は-1で0以下の値は制限なしを意味する．\n    #\"num_leaves\" : 31,  #木にある分岐の個数．sklearnのmax_depthsklearnのmax_depthとの関係はnum_leaves=2^max_depth\n                                    #デフォルトは31．大きくすると精度は上がるが過学習が進む．\n    \"learning_rate\": 0.03, \n    \"feature_fraction\" : 0.7, #学習の高速化と過学習の抑制に使用される．データの特徴量のfeature_fraction * 100 % だけ使用する．\n    'bagging_fraction': 0.7, #like feature_fraction, but this will randomly select part of data without resampling\n    \"bagging_freq\" : 5, #frequency for bagging. 0 means disable bagging; k means perform bagging at every k iteration\n                                    # Note: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as well\n    \"early_stopping_rounds\" : 100,\n    'n_estimators':2000, # aliases: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators. \n                                        # number of boosting iterations\n     \"seed\" : 42,\n    \"reg_alpha\":0.1,\n    \"reg_lambda\":0.1 #0.1\n}\n\n\nfor fold, (train_index, val_index) in enumerate(kf.split(x, y)):\n    tr_x = x.iloc[train_index, :]\n    tr_y = y[train_index]\n    val_x = x.iloc[val_index, :]\n    val_y = y[val_index]\n    \n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(val_x, val_y, reference = lgb_train)\n    \n    model = lgb.train(params, \n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval], \n                      verbose_eval= 10\n                     )\n    oof_train[val_index] = model.predict(val_x, num_iteration = model.best_iteration)\n    #y_pred = model.predict(test, num_iteration = model.best_iteration)\n    \n    #y_preds.append(y_pred)\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(np.round(oof_train), y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = pd.DataFrame(index = x.columns)\nfor i, m in enumerate(models):\n    fi[\"model_\"+str(i+1)] = m.feature_importance(importance_type = \"gain\")\nfi[\"fi_ave\"] = fi.mean(axis = 1)\nfi.sort_values(by = \"fi_ave\", inplace = True, ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,12))\nsns.barplot(x = fi[\"fi_ave\"][:30], y = fi.index[:30])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}