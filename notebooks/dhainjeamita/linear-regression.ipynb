{"cells":[{"metadata":{"_cell_guid":"9f569ed0-c6a5-47ac-a2dc-135ada8db2df","_uuid":"7baa7e89284ef4af88fbeefdcd616baf184ef3ee"},"cell_type":"markdown","source":"# Linear Regression for Boston House Price Prediction"},{"metadata":{"_uuid":"e5d4e824fb126545b4b06656c48ceab8eb0eb21a"},"cell_type":"markdown","source":"## Lets load the dataset and apply columns"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Linear Regression for Boston house prices\nimport numpy\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nfilename = '../input/boston-house-prices/housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndataset = pd.read_csv(filename, delim_whitespace=True, names=names)","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"4090e811-d1dd-452d-855e-1012e0038a5d","_uuid":"2700820d364288b1eb33148ad04101d60815c332","trusted":true},"cell_type":"code","source":"# SHAPE OF THE DATASET\nprint (dataset.shape)\n# Describe the dataset\nprint (dataset.describe())","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"fb66ad91-4226-4035-9c2b-c9976f110553","_uuid":"74771a497f2100a727aa0b0384a0c3c837259b13"},"cell_type":"markdown","source":"# Lets apply the linear regression model and check the score"},{"metadata":{"_cell_guid":"926671fd-fdde-4a2f-adec-0d650627a40d","_uuid":"afc8981520173201edf1123953632739ca8c0f41","trusted":true},"cell_type":"code","source":"array = dataset.values\nX = array[:,0:13]\nY = array[:,13]\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nfinalResult = result*100\nprint(\"The Accuracy Score  - {}\". format(finalResult))","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"be996936-887d-40db-9373-3c45c3bc991b","_uuid":"897f418024a03b4ad3f89bac5c39cb066db54aa0"},"cell_type":"markdown","source":"# Lets apply K-Fold Cross Validation for Linear Regression"},{"metadata":{"_cell_guid":"0b82ca24-8a85-4ba3-9c22-4c5f916159d1","_uuid":"e4bac8804c9b71f97e4b9e472c8350c935060f03","trusted":true},"cell_type":"code","source":"kfold = KFold(n_splits=5, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"MAE: %.3f (%.3f) \" % (results.mean(), results.std()))\nprint(\"The Actual result  - {}\". format(results))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"458bf0bb-3bbd-4e4b-90c5-cfb11668fbf6","_uuid":"ef5804df3215b08e424177daf7b1e11db05823d7","collapsed":true},"cell_type":"markdown","source":"# Lets generate a sample dataset for one more linear regression dataset"},{"metadata":{"_cell_guid":"9e3906a4-e2ac-4899-ad06-5fd1c7bdccf1","_uuid":"f0c7d5acf28c1ec5845aa94fd1c823d9fa8a2cb6","trusted":true},"cell_type":"code","source":"# synthetic dataset for classification (binary) \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification, make_blobs, make_regression\nplt.style.use('ggplot')\nplt.style.use('seaborn-colorblind')\nplt.figure(figsize=(7,7))\nplt.title('Sample regression problem with one input variable')\nX_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n                            n_informative=1, bias = 150.0,\n                            noise = 30, random_state=0)\nplt.scatter(X_R1, y_R1, marker= 'o', s=50)\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"ee888a39-5fd0-4aea-ab4c-987f8eb87240","_uuid":"fcc5fc62ddc606145bc3c58657efd38c44d5a9de"},"cell_type":"markdown","source":"# Lets implement the linear regression model"},{"metadata":{"_cell_guid":"b5a8925f-e6d6-4747-a255-6d0d1948da32","_uuid":"d37ac961461c8a057fd6f0b6b1ec32c2ef1fac94","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,random_state = 0)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('linear model coeff (w): {}'.format(linreg.coef_))\nprint('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"89d44112-a60a-4329-9471-814951dfb97f","_uuid":"1e70eafcd78260caa9b158adea4b9977c51230ff"},"cell_type":"markdown","source":"# Lets plot down the linear regression output"},{"metadata":{"_cell_guid":"8e6614fd-3e39-49aa-9c7c-6f5b1c483bc0","_uuid":"81ad0c9e78ceaf5d12b15d2a9d4fdfff2cee67d0","scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nplt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\nplt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\nplt.title('Least-squares linear regression')\nplt.xlabel('Feature value (x)')\nplt.ylabel('Target value (y)')\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"2182feb17693ff669da537ec50b882a6f71f29a9"},"cell_type":"markdown","source":"# Lets consider one more dataset,  the crime dataset describing the violent crimes commited in a specific areas."},{"metadata":{"_cell_guid":"bc55767a-a56a-40ef-8b05-99cd08bafd8a","_uuid":"a1928323a50912648b1e4bbd8c52f82caf0bdcdd","collapsed":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/additionalcrimedataset/crimedata.csv', sep=',', na_values='?', header=0)\nrequiredColumns = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \ndf = df.iloc[:,requiredColumns].dropna()\n\nX = df.iloc[:,range(0,88)]\ny = df['ViolentCrimesPerPop']","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"48580a42-7631-4043-8250-c169b2a4cb9c","_uuid":"ec1a2bceaa04fda1232730ecb3516a025be3589e","collapsed":true},"cell_type":"markdown","source":"# lets apply the linear regression model"},{"metadata":{"_cell_guid":"c63daf65-b610-4416-a7b7-64d5d85c21cc","_uuid":"b1f6d2eccd4b65af8015ad997044a64b92ba3542","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\nlinearReg = LinearRegression().fit(X_train,y_train)\n\n#print (\"Linear Model Intercept - {}\".format(linearReg.intercept_))\n#print (\"Linear Model Coefficient - \\n {}\".format(linearReg.coef_))\nprint (\"Training Score - {:.3f}\".format(linearReg.score(X_train,y_train)))\nprint (\"Testing Score - {:.3f}\".format(linearReg.score(X_test,y_test)))","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"4286937d-c672-4137-b471-70e303b384e1","_uuid":"a3339daa383f57741bb104e96aac96533f8eee31"},"cell_type":"markdown","source":"# Lets try to apply rigde regression on the same dataset and check whether we can get some better accuracy"},{"metadata":{"_cell_guid":"0436946d-24de-4504-be2d-25ed7a285a3e","_uuid":"c4b6cb558e315b3d3019695321d4af5c6755aec3","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nimport numpy as np\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\nridgeReg = Ridge(alpha=20.0).fit(X_train,y_train)\n\nprint (\"Training Score - {:.3f}\".format(ridgeReg.score(X_train,y_train)))\nprint (\"Testing Score - {:.3f}\".format(ridgeReg.score(X_test,y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(ridgeReg.coef_ != 0)))","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"44e941cf-ff7a-4bd5-97c7-598d4a29d5d6","_uuid":"a07e8e6e4e24f1e1961ff5d03cc83d044dc11298"},"cell_type":"markdown","source":"# Now lets apply Normalization to Ridge regression"},{"metadata":{"_cell_guid":"c324e7b5-c161-4162-8005-733348094db7","_uuid":"a9297847122ed45cdc805cf4487b44a8e3d82f35","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\n\nridgeReg = Ridge(alpha=20.0).fit(X_train_scale,y_train)\n\nprint (\"Training Score - {:.3f}\".format(ridgeReg.score(X_train_scale,y_train)))\nprint (\"Testing Score - {:.3f}\".format(ridgeReg.score(X_test_scale,y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(ridgeReg.coef_ != 0)))\n","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"4d0beb88-b71c-4c94-8993-5b11f02d9b3c","_uuid":"3dcac236ed20c66f936c3253d73a9d4947036161"},"cell_type":"markdown","source":"# Lets try ridge regression with normalization for different values of alpha. But we see that as the alpha increases the accuracy decreases "},{"metadata":{"_cell_guid":"884cbe51-db47-41f0-b653-073b0020e59e","_uuid":"f2e79ca942dcace483d253818db82ba6386187c1","trusted":true},"cell_type":"code","source":"for newAlpha in [0, 1, 10, 20, 50, 100, 1000]:\n    linridge = Ridge(alpha = newAlpha).fit(X_train_scale, y_train)\n    r2_train = linridge.score(X_train_scale, y_train)\n    r2_test = linridge.score(X_test_scale, y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\nr-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n         .format(newAlpha, num_coeff_bigger, r2_train, r2_test))","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"53ea5914-9a09-4539-a6ed-34e41252f726","_uuid":"4164e99062b101b97e63c92b28c5943eb43af93a","collapsed":true},"cell_type":"markdown","source":"# Now lets apply Lasso regression and check how many features get reduced"},{"metadata":{"_uuid":"26bc0fa98c80fd98ca3512ccbe592c593a4bc2c2","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso\nscaler = MinMaxScaler()\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\n\nLassoReg = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scale,y_train)\n\nprint (\"Training Score - {:.3f}\".format(LassoReg.score(X_train_scale,y_train)))\nprint (\"Testing Score - {:.3f}\".format(LassoReg.score(X_test_scale,y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(LassoReg.coef_ != 0)))","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"c5774bb4c42f032cc9d47578ec9e94b93c8381fb"},"cell_type":"markdown","source":"# By changing the  alpha parameter , we can check that as the alpha increases the accuracy decreases"},{"metadata":{"_uuid":"4a7b8d0e86984f6b4a44757e21cb1295abb4a461","trusted":true},"cell_type":"code","source":"print('Lasso regression: effect of alpha regularization\\n\\\nparameter on number of features kept in final model\\n')\n\nfor alpha in [0.5, 1, 2, 3, 4, 5, 10, 20, 50]:\n    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scale, y_train)\n    r2_train = linlasso.score(X_train_scale, y_train)\n    r2_test = linlasso.score(X_test_scale, y_test)\n    \n    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\nr-squared test: {:.2f}\\n'.format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9342b74d323e6089bc6615fa909cb4b99eedb743","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}