{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n\n\n-  The former employeesâ€™ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n- A sizeable department has to be maintained, for the purposes of recruiting new talent\n- More often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n\nHence, the management has contracted an HR analytics firm to understand what factors they should focus on, in order to curb attrition. In other words, they want to know what changes they should make to their workplace, in order to get most of their employees to stay. Also, they want to know which of these variables is most important and needs to be addressed right away.\n\n\nSince you are one of the star analysts at the firm, this project has been given to you.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Goal of the case study","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"You are required to model the probability of attrition using a logistic regression. The results thus obtained will be used by the management to understand what changes they should make to their workplace, in order to get most of their employees to stay.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Importing and Merging Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np, seaborn as sns,matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time = pd.read_csv('/kaggle/input/hr-analytics-case-study/in_time.csv')\nmanager_survey_data = pd.read_csv('/kaggle/input/hr-analytics-case-study/manager_survey_data.csv')\nemployee_survey_data = pd.read_csv('/kaggle/input/hr-analytics-case-study/employee_survey_data.csv')\ndata_dictionary= pd.read_excel('/kaggle/input/hr-analytics-case-study/data_dictionary.xlsx')\nout_time = pd.read_csv('/kaggle/input/hr-analytics-case-study/out_time.csv')\ngeneral_data = pd.read_csv('/kaggle/input/hr-analytics-case-study/general_data.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time=in_time.replace(np.nan,0)\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nin_time.iloc[:, 1:] = in_time.iloc[:, 1:].apply(pd.to_datetime, errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_time=out_time.replace(np.nan,0)\nout_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_time.iloc[:, 1:] = out_time.iloc[:, 1:].apply(pd.to_datetime, errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time=in_time.append(out_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time=in_time.diff(periods=4410)\nin_time=in_time.iloc[4410:]\nin_time.reset_index(inplace=True)\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.drop(columns=['index','Unnamed: 0'],axis=1,inplace=True)\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.drop(['2015-01-01', '2015-01-14','2015-01-26','2015-03-05',\n             '2015-05-01','2015-07-17','2015-09-17','2015-10-02',\n              '2015-11-09','2015-11-10','2015-11-11','2015-12-25'\n             ], axis = 1,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time['Actual Time']=in_time.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time['Actual Time'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time['hrs']=in_time['Actual Time']/np.timedelta64(1, 'h')\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.reset_index(inplace=True)\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.drop(in_time.columns.difference(['index','hrs']), 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_time.rename(columns={'index': 'EmployeeID'},inplace=True)\nin_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"general_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_survey_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining all data files into one consolidated dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = pd.merge(employee_survey_data, general_data, how='inner', on='EmployeeID')\nhr = pd.merge(manager_survey_data, df_1, how='inner', on='EmployeeID')\nhr = pd.merge(in_time, hr, how='inner', on='EmployeeID')\nhr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correcting Datatype for the variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['JobLevel']=hr['JobLevel'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoding Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['Education'] = hr['Education'].replace({ 1 : 'Below College', 2: 'College',3: 'Bachelor',4: 'Master',5 : 'Doctor'})\nhr['EnvironmentSatisfaction'] = hr['EnvironmentSatisfaction'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\nhr['JobInvolvement'] = hr['JobInvolvement'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\nhr['JobSatisfaction'] = hr['JobSatisfaction'].replace({ 1 : 'Low', 2: 'Medium',3: 'High',4: 'Very High'})\n#hr['RelationshipSatisfaction'] = hr['RelationshipSatisfaction'].replace({ 1 : 'Low', 2: 'Medium',\n#                   3: 'High',4: 'Very High'})\nhr['PerformanceRating'] = hr['PerformanceRating'].replace({ 1 : 'Low', 2: 'Good',3: 'Excellent',4: 'Outstanding'})\nhr['WorkLifeBalance'] = hr['WorkLifeBalance'].replace({ 1 : 'Bad', 2: 'Good',3: 'Better',4: 'Best'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['EmployeeCount'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['Over18'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['StandardHours'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop Non Required Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.drop(['EmployeeID', 'EmployeeCount','StandardHours','Over18'], axis = 1,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Inspecting the Dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of our master dataset\nhr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the dimensions of the dataframe\nhr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the statistical aspects of the dataframe\nhr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the type of each column\nhr.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['EnvironmentSatisfaction'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='EnvironmentSatisfaction',data=hr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- With analysis, we found mean & median for EnvironmentSatisfaction field is 2.72 & 3, it needs to be whole number hence, accepted is  3.\n- From data_dictionary, we know 3 means High.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Compute missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['EnvironmentSatisfaction'] = hr['EnvironmentSatisfaction'].fillna('High')\nhr['EnvironmentSatisfaction'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['JobSatisfaction'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='JobSatisfaction',data=hr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- With analysis, we found mean & median for JobSatisfaction field is 2.72 & 3, it needs to be whole number hence, accepted is  3.\n- From data_dictionary, we know 3 means High.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Compute missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['JobSatisfaction'] = hr['JobSatisfaction'].fillna('High')\nhr['JobSatisfaction'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['WorkLifeBalance'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='WorkLifeBalance',data=hr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- With analysis, we found mean & median for WorkLifeBalance field is 2.76 & 3, it needs to be whole number hence, accepted is  3.\n- From data_dictionary, we know 3 means Better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Compute missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['WorkLifeBalance'] = hr['WorkLifeBalance'].fillna('Better')\nhr['WorkLifeBalance'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['NumCompaniesWorked'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='NumCompaniesWorked',data=hr);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='NumCompaniesWorked',data=hr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- With analysis, we found mean & median for NumCompaniesWorked field is 2.69 & 2, it needs to be whole number along with handling outliers hence, accepted is  2.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Compute missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['NumCompaniesWorked'] = hr['NumCompaniesWorked'].fillna(2)\nhr['NumCompaniesWorked'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['TotalWorkingYears'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.distplot(hr['TotalWorkingYears'], hist=True, kde=False, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Employees')\nax.set_xlabel('TotalWorkingYears');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='TotalWorkingYears',data=hr);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- With analysis, we found mean & median for TotalWorkingYears field is 11.27 & 10, it needs to be whole number along with handling outliers hence, accepted is  10.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['TotalWorkingYears'] = hr['TotalWorkingYears'].fillna(2)\nhr['TotalWorkingYears'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='WorkLifeBalance',data=hr,hue=\"Attrition\")\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insights\n- Attrition : Whether the employee left in the previous year or not\n1. Employee who left in the previous year are 17% of population (1019) i.e. 174 who believe WorkLifeBalance is Good in org.\n2. Employee who left in the previous year are 18% of population(454) i.e. 81 who believe WorkLifeBalance is Best in org.\n3. Employee who left in the previous year are 34% of population (239) i.e. 81 who believe WorkLifeBalance is Bad in org.\n4. Employee who left in the previous year are 14% of population (2698) i.e. 378 who believe WorkLifeBalance is Better in org.\n- People who left in the previous year & believe WorkLifeBalance is Better in org were 52% of population who left in the previous year, second by 24% people who left in the previous year & believe WorkLifeBalance is Good in org","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='PerformanceRating', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insights\n- Attrition : Whether the employee left in the previous year or not\n1. Employee who left in the previous year are 16% of population (3732) i.e. 597 whose PerformanceRating was Excellent in org.\n2.  Employee who left in the previous year are 18% of population (678) i.e. 122 whose PerformanceRating was Outstanding in org.\n- People who left in the previous year & PerformanceRating was Excellent in org were 83% of population who left in the previous year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,10))\nax = sns.countplot(x='EnvironmentSatisfaction', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 30, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insights\n- Attrition : Whether the employee left in the previous year or not\n1. Employee who left in the previous year are 14% of population (1375) i.e. 192 who believe EnvironmentSatisfaction is High in org. in org.\n1. Employee who left in the previous year are 15% of population (856) i.e. 129 who believe EnvironmentSatisfaction is Medium in org. \n1. Employee who left in the previous year are 13% of population (1334) i.e. 173 who believe EnvironmentSatisfaction is Very High in org.\n1. Employee who left in the previous year are 25% of population (845) i.e. 211 who believe EnvironmentSatisfaction is Low in org. \n- People who left in the previous year & believe EnvironmentSatisfaction is Low in org were 30% of population who left in the previous year. Second by People who left in the previous year & believe EnvironmentSatisfaction is High in org","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='JobSatisfaction', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='WorkLifeBalance', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='Age',x='Attrition',data=hr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='BusinessTravel', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='Department', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='DistanceFromHome',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='Education', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax = sns.countplot(x='EducationField', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='Gender', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='JobLevel', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nax = sns.countplot(x='JobRole', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='MaritalStatus', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='MonthlyIncome',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='PercentSalaryHike',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.countplot(x='StockOptionLevel', data=hr, hue=\"Attrition\")\nax.set_ylabel('# of Employee')\nbars = ax.patches\nhalf = int(len(bars)/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\n\nfor left, right in zip(left_bars, right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    total = height_l + height_r\n\n    ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.0%}'.format(height_l/total), ha=\"center\")\n    ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.0%}'.format(height_r/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(8,8))\nsns.violinplot(y='TotalWorkingYears',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='TrainingTimesLastYear',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='YearsAtCompany',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='YearsSinceLastPromotion',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='YearsWithCurrManager',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.violinplot(y='hrs',x='Attrition',data=hr)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,18))\nsns.heatmap(hr.corr(), annot = True, cmap=\"Accent\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr_num=hr[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n           'DistanceFromHome','Age','hrs']]\n\nsns.pairplot(hr_num, diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For categorical variables with multiple levels, create dummy features (one-hot encoded)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(hr[['JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction',\n                                 'JobSatisfaction', 'WorkLifeBalance','BusinessTravel', 'Department',\n                                 'Education','EducationField', 'Gender', 'JobLevel', 'JobRole',\n                                 'MaritalStatus']], drop_first=True)\n\n# Adding the results to the master dataframe\nhr = pd.concat([hr, dummy1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping the repeated variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have created dummies for the below variables, so we can drop them\nhr = hr.drop(['JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction',\n                                 'JobSatisfaction', 'WorkLifeBalance','BusinessTravel', 'Department',\n                                 'Education','EducationField', 'Gender', 'JobLevel', 'JobRole',\n                                 'MaritalStatus'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mapping Attrition to 1/0 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hr['Attrition'] = hr['Attrition'].replace({'Yes': 1, \"No\": 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hr.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Step 3: Test-Train Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Putting feature variable to X\nX = hr.drop(['Attrition'], axis=1)\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting response variable to y\ny = hr['Attrition']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n           'DistanceFromHome','Age','hrs']] = scaler.fit_transform(X_train[[ 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n           'DistanceFromHome','Age','hrs']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Checking the Attrition Rate\nAttrition = (sum(hr['Attrition'])/len(hr['Attrition'].index))*100\nAttrition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have almost 16.13% Attrition rate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Looking at Correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the correlation matrix \nplt.figure(figsize = (50,40))   \nsns.heatmap(hr.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nhr.corr()['Attrition'].sort_values(ascending = False).plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping highly correlated dummy variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = X_train.corr() \ncorrdf = corrmat.where(np.triu(np.ones(corrmat.shape), k=1).astype(np.bool))\ncorrdf = corrdf.unstack().reset_index()\ncorrdf.columns = ['Var1', 'Var2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\ncorrdf['Correlation'] = abs(corrdf['Correlation'])\nmatrix= corrdf.sort_values(by = 'Correlation', ascending = False).head(50)\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique=list(set(matrix.Var2))\nlen(unique)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping highly correlated dummy variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test.drop(unique,1)\nX_train = X_train.drop(unique,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the Correlation Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After dropping highly correlated variables now let's check the correlation matrix again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (50,25))\nsns.heatmap(X_train.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Step 7: Model Building\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start by splitting our data into a training set and a test set.\n- Running Your First Training Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n# Logistic regression model\nlogm1 = sm.Logit(y_train,(sm.add_constant(X_train)))\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 8: Feature Selection Using RFE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 13)             \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assessing the model with StatsModels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.Logit(y_train,X_train_sm)\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Attrition':y_train.values, 'Attrition_Prob':y_train_pred})\ny_train_pred_final['EmployeeID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating new column 'predicted' with 1 if Attrition_Prob > 0.5 else 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Attrition, y_train_pred_final.predicted )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking VIFs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few variables with high p value . It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex. The variable 'JobRole_Human Resources' has the highest p value. So let's start by dropping that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('JobRole_Human Resources', 1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.Logit(y_train,X_train_sm)\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Attrition_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check the VIFs again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop Education_Doctor since it has a high VIF\ncol = col.drop('Education_Doctor')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.Logit(y_train,X_train_sm)\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check the VIFs again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop EducationField_Other since it has a high p value\ncol = col.drop('EducationField_Other')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.Logit(y_train,X_train_sm)\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is still practically the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now check the VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop EducationField_Technical Degree since it has a high p value\ncol = col.drop('EducationField_Technical Degree')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.Logit(y_train,X_train_sm)\nres = logm6.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now check the VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop JobInvolvement_Very High since it has a high p value\ncol = col.drop('JobInvolvement_Very High')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm7 = sm.Logit(y_train,X_train_sm)\nres = logm7.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is still practically the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now check the VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop JobInvolvement_Low since it has a high p value\ncol = col.drop('JobInvolvement_Low')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm8 = sm.Logit(y_train,X_train_sm)\nres = logm8.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now check the VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop JobLevel_5 since it has a high p value\ncol = col.drop('JobLevel_5')\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm9 = sm.Logit(y_train,X_train_sm)\nres = logm9.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Attrition_Prob'] = y_train_pred\n# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's now check the VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All variables have a good value of VIF. So we need not drop any more variables and we can proceed with making predictions using this model only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Metrics beyond simply accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - \nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 9: Plotting the ROC Curve","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"An ROC curve demonstrates several things:\n\n* It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n* The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n* The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Attrition, y_train_pred_final.Attrition_Prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Attrition, y_train_pred_final.Attrition_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 10: Finding Optimal Cutoff Point","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Attrition_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Attrition, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nax= cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nax.set_axisbelow(True)\nax.minorticks_on()\n# Customize the major grid\nax.grid(which='major', linestyle='-', linewidth='0.5', color='red')\n# Customize the minor grid\nax.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the curve above, 0.18 is the optimum point to take it as a cutoff probability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Attrition_Prob.map( lambda x: 1 if x > 0.18 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Attrition, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Attrition, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate \nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and Recall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_pred_final.Attrition, y_train_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_pred_final.Attrition, y_train_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and recall tradeoff","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\ny_train_pred_final.Attrition.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" y_train_pred_final.predicted.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Attrition, y_train_pred_final.Attrition_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting EmployeeID to index\ny_test_df['EmployeeID'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Attrition_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['EmployeeID','Attrition','Attrition_Prob'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Attrition_Prob.map(lambda x: 1 if x > 0.18 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Attrition, y_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Attrition, y_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}