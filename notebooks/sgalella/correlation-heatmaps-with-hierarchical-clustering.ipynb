{"cells":[{"metadata":{},"cell_type":"markdown","source":"The purpose of this notebook is to present the benefits of hierarchical clustering for data analysis and for improving the representation of correlation heatmaps. For this notebook, we use the *breast-cancer-wisconsin* dataset. \n\n# Correlation coefficient"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data \ndata = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\n\n# Show dataframe\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will get rid of some of the columns, including `id`, `diagnosis` and `Unnamed: 32`. `id` won't apport any important information to the model, `diagnosis` is the target and `Unnamed: 32` just contains *NaN* values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"id\"], axis=1)\ndata = data.drop([\"diagnosis\"], axis=1)\ndata = data.drop([\"Unnamed: 32\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To measure the interplay between the different features, we will calculate the correlation coefficient between pairs. The correlation between two features gives us a degree of their relation. This relation can be linear or nonlinear. For this notebook, we will use linear correlation calculating the [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) ($\\rho$). \n\nCorrelation ranges from -1 to 1, where -1 indicates anticorrelation (or negative correlation), 0 no correlation and 1 correlation (or positive correlation). The following plot shows a pair of signals with their respective correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nplt.figure(figsize=(14,3))\n\nline1 = np.array([2, 1, 1, 2])\nline2 = np.array([1, 2, 2, 1])\n\nplt.subplot(131)\nplt.plot(line1, color='royalblue')\nplt.plot(line2, color='lightcoral')\nplt.title(np.corrcoef(line1, line2)[1,0], fontsize=18)\n\nline1 = np.array([2, 1, 1, 2])\nline2 = np.array([1, 2, 2, 3])\n\nplt.subplot(132)\nplt.plot(line1, color='royalblue')\nplt.plot(line2, color='lightcoral')\nplt.title(round(np.corrcoef(line1, line2)[1,0],2), fontsize=18)\n\nline1 = np.array([2, 1, 1, 2])\nline2 = 2*line1\n\nplt.subplot(133)\nplt.plot(line1, color='royalblue')\nplt.plot(line2, color='lightcoral')\nplt.title(np.corrcoef(line2, 2*line2)[1,0], fontsize=18);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The three plots show a negative correlation, no correlation, and positive correlation, respectively. \n\nLinear correlation is helpful in gaining quick intuition about the relation between two signals. However, it has some drawbacks. For instance, it won't account for sequential displacements (we would need to use lags) or non-linearities. Also, keep in mind that [correlation doesn't necessary mean causation](https://www.tylervigen.com/spurious-correlations). \n\nIn the next figure, we plot a heatmap with all the correlations between features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ncorrelations = data.corr()\nsns.heatmap(round(correlations,2), cmap='RdBu', annot=True, \n            annot_kws={\"size\": 7}, vmin=-1, vmax=1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first sight, we see many positive correlations (blue). However, this heatmap is messy. For visualization purposes, it would be better to group features that are highly correlated together. To do so, we will do [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). \n\n# Hierarchical Clustering\n\nHierarchical clustering is a method to find hierarchy within our data. This hierarchy allows ordering the data in clusters. It arranges the data using a dissimilarity matrix (also called distance matrix), which gives information on how far are two features. The distance can be computed in many different ways. Since we're using the Pearson Correlation Coefficient, the distance matrix will be calculated as follows:\n\n$$\nd(X, Y) = 1 - \\big | \\ \\rho_{X, Y} \\ \\big |\n$$ \n\nFor negative and positive correlations, the distance will be close to zero. If there is no correlation whatsoever, the distance will be $\\approx 0$.\n\nAfter computing the distance matrix, we have to group features hierarchically according to their distances. Then we can visualize the relationship between features in a tree diagram called dendrogram.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom scipy.spatial.distance import squareform\n\nplt.figure(figsize=(12,5))\ndissimilarity = 1 - abs(correlations)\nZ = linkage(squareform(dissimilarity), 'complete')\n\ndendrogram(Z, labels=data.columns, orientation='top', \n           leaf_rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially, before starting the algorithm, each feature is a cluster. The algorithms take close features and combine them into a brand new cluster. Iteratively, the algorithm keeps grouping clusters until there is only one. Each leaf in the dendrogram represents a feature and each node a cluster. The *y-axis* shows the distance between points (ranging from 0 to 1). The number of clusters in our data will depend on which distance we take as a threshold. If we select a small distance, more clusters will be formed. Conversely, if we choose a large distance as a threshold, we would less clusters. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clusterize the data\nthreshold = 0.8\nlabels = fcluster(Z, threshold, criterion='distance')\n\n# Show the cluster\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`label` shows which cluster each of the features belongs to. Finally, to observe the clusters in the correlation plot, we have to rearrange the features in the dataframe according to the cluster output."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Keep the indices to sort labels\nlabels_order = np.argsort(labels)\n\n# Build a new dataframe with the sorted columns\nfor idx, i in enumerate(data.columns[labels_order]):\n    if idx == 0:\n        clustered = pd.DataFrame(data[i])\n    else:\n        df_to_append = pd.DataFrame(data[i])\n        clustered = pd.concat([clustered, df_to_append], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we plot the clustered correlation plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ncorrelations = clustered.corr()\nsns.heatmap(round(correlations,2), cmap='RdBu', annot=True, \n            annot_kws={\"size\": 7}, vmin=-1, vmax=1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since our threshold was set at 0.7, we will be able to see five different clusters (grouped in the main diagonal). The biggest one corresponds to the red tree in the dendrogram. Since these features are related to the geometry of cancer, they form a robust cluster. Another distinct cluster is formed just by `texture_mean` and `texture_worst`, in green in the dendrogram. Note that in order for this cluster to disappear, we'd have to decrease the threshold considerably. The remaining clusters are less recognizable, given that their distances are higher.\n\nThe following plot shows the different clusters using a different threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nfor idx, t in enumerate(np.arange(0.2,1.1,0.1)):\n    \n    # Subplot idx + 1\n    plt.subplot(3, 3, idx+1)\n    \n    # Calculate the cluster\n    labels = fcluster(Z, t, criterion='distance')\n\n    # Keep the indices to sort labels\n    labels_order = np.argsort(labels)\n\n    # Build a new dataframe with the sorted columns\n    for idx, i in enumerate(data.columns[labels_order]):\n        if idx == 0:\n            clustered = pd.DataFrame(data[i])\n        else:\n            df_to_append = pd.DataFrame(data[i])\n            clustered = pd.concat([clustered, df_to_append], axis=1)\n            \n    # Plot the correlation heatmap\n    correlations = clustered.corr()\n    sns.heatmap(round(correlations,2), cmap='RdBu', vmin=-1, vmax=1, \n                xticklabels=False, yticklabels=False)\n    plt.title(\"Threshold = {}\".format(round(t,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seaborn also includes a function *clustermap* to plot the correlation heatmats with dendrograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(correlations, method=\"complete\", cmap='RdBu', annot=True, \n               annot_kws={\"size\": 7}, vmin=-1, vmax=1, figsize=(15,12));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Conclusions\n\nHierarchical clustering can be helpful in understanding our data better. It also improves the visual representation of correlation heatmaps, making it easier to find groups of correlated features."},{"metadata":{},"cell_type":"markdown","source":"# References\n1. [Hierarchical Clustering with Python and Scikit Learn](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/)\n2. [Scipy Hierarchical Clustering](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}