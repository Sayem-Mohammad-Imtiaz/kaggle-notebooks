{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\nThe objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing packages\nimport re\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading train test\ntrain  = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#putting train and test together\ncombi = train.append(test, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to remove unwanted text patterns from the tweets\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove twitter handles (@user)\ncombi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove special characters, numbers, punctuations\ncombi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing small words\ncombi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenization\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\ncombi['tidy_tweet'] = tokenized_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Understanding tweets\n#word cloud\nall_words = ' '.join([text for text in combi['tidy_tweet']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#words in non- racist tweets\nnormal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words in racist tweets\nnegative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\nwordcloud = WordCloud(width=800, height=500,\nrandom_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#understanding hashtags present in tweets\n# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting hashtags from non racist/sexist tweets\n\nHT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n\n# extracting hashtags from racist/sexist tweets\nHT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n\n# unnesting list\nHT_regular = sum(HT_regular,[])\nHT_negative = sum(HT_negative,[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non racist\na = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# racist\nb = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n# selecting top 10 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 10)   \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"most of the terms are negative with a few neutral terms as well. So, it’s not a bad idea to keep these hashtags in our data as they contain useful information."},{"metadata":{"trusted":true},"cell_type":"code","source":"#bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(combi['tidy_tweet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bow)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n\nLet’s have a look at the important terms related to TF-IDF:\n\nTF = (Number of times term t appears in a document)/(Number of terms in the document)\nIDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\nTF-IDF = TF*IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOW model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\ntrain_bow = bow[:31962,:]\ntest_bow = bow[31962:,:]\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n\nlreg = LogisticRegression()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int) # calculating f1 score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = lreg.predict_proba(test_bow)\ntest_pred_int = test_pred[:,1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['label'] = test_pred_int\nsubmission = test[['id','label']]\nsubmission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TAF-IDF model\ntrain_tfidf = tfidf[:31962,:]\ntest_tfidf = tfidf[31962:,:]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\n\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ref: https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}