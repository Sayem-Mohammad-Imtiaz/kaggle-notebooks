{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-16T19:27:41.069539Z","iopub.execute_input":"2021-09-16T19:27:41.069817Z","iopub.status.idle":"2021-09-16T19:27:42.240517Z","shell.execute_reply.started":"2021-09-16T19:27:41.069782Z","shell.execute_reply":"2021-09-16T19:27:42.239852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os \nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nfrom tqdm import tqdm # to see process\n# Audio Signal Processing Libarary\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:34:35.964645Z","iopub.execute_input":"2021-09-16T19:34:35.964938Z","iopub.status.idle":"2021-09-16T19:34:35.972124Z","shell.execute_reply.started":"2021-09-16T19:34:35.964898Z","shell.execute_reply":"2021-09-16T19:34:35.971257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Information of data (Data Structure)\n# using os to check the audio file in each folder\npath = [\"/kaggle/input/urbansound8k/fold1\",\"/kaggle/input/urbansound8k/fold2\",\"/kaggle/input/urbansound8k/fold3\",\"/kaggle/input/urbansound8k/fold4\",\"/kaggle/input/urbansound8k/fold5\",\n        \"/kaggle/input/urbansound8k/fold6\",\"/kaggle/input/urbansound8k/fold7\",\"/kaggle/input/urbansound8k/fold8\",\"/kaggle/input/urbansound8k/fold9\",\"/kaggle/input/urbansound8k/fold10\"]\nfor i in range(10):\n  for dirpath, dirname,filename in os.walk(path[i]):\n    print(f\"this is {i+1}st folder having {len(filename)} sound file in '{dirpath}'.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:30:46.399771Z","iopub.execute_input":"2021-09-16T19:30:46.400567Z","iopub.status.idle":"2021-09-16T19:30:46.427414Z","shell.execute_reply.started":"2021-09-16T19:30:46.400528Z","shell.execute_reply":"2021-09-16T19:30:46.426711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis One Particular Audio File","metadata":{}},{"cell_type":"code","source":"filename = \"/kaggle/input/urbansound8k/fold1/102842-3-0-1.wav\"\n\nplt.figure(figsize = (14,5))\n\n## Librosa normalize the sound give it in in one single sample_rate by deafult this is 22050 or 22KHz\n#---> and this normalize signal data in 0 to 1 and this change signal into one mono channel.\n#---> Librosa converts the signal to mono, meaning the channel will alays be 1\n\nsound_data, sample_rate = librosa.load(filename) # Load file to find data and sr(how many times per sec sound sample)\nprint(\"sample_rate : \",sample_rate)\nprint(\"data : \",sound_data)\n# data come in 1-dimensional beacuse librosa change 2 channel into 1 mono channel\nlibrosa.display.waveplot(sound_data, sr = sample_rate) # Plotting audio file\nplt.title(\"SINGLE Channel audio signal using LIBROSA\")\nipd.Audio(filename) # play the audio\n\n#This is Dog noisy Sound","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:31:29.950313Z","iopub.execute_input":"2021-09-16T19:31:29.950608Z","iopub.status.idle":"2021-09-16T19:31:31.008484Z","shell.execute_reply.started":"2021-09-16T19:31:29.950579Z","shell.execute_reply":"2021-09-16T19:31:31.007761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The DataSet","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/urbansound8k/UrbanSound8K.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:32:13.709636Z","iopub.execute_input":"2021-09-16T19:32:13.709949Z","iopub.status.idle":"2021-09-16T19:32:13.762774Z","shell.execute_reply.started":"2021-09-16T19:32:13.709919Z","shell.execute_reply":"2021-09-16T19:32:13.761965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:32:21.220202Z","iopub.execute_input":"2021-09-16T19:32:21.220771Z","iopub.status.idle":"2021-09-16T19:32:21.227371Z","shell.execute_reply.started":"2021-09-16T19:32:21.220734Z","shell.execute_reply":"2021-09-16T19:32:21.226516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check whether the dataset is imbalanced by seeing target value_count(give unique value)\ndata['class'].value_counts()\n#By sseing we saw that mostly class have same data so there is little chnace of imbalanced so just ignore this","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:32:31.259746Z","iopub.execute_input":"2021-09-16T19:32:31.260312Z","iopub.status.idle":"2021-09-16T19:32:31.274535Z","shell.execute_reply.started":"2021-09-16T19:32:31.260277Z","shell.execute_reply":"2021-09-16T19:32:31.273773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data PreProcessing\n\n**Feature Extraction**\n\ntaking audio signal and create independent vector which will represent audio data into vector form.\n\n**Feature Extraction Method : MFCC**\n\nWe use Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.","metadata":{}},{"cell_type":"code","source":"def features_extract(file_name):\n    audio, sample_rate = librosa.load(file_name, res_type = 'kaiser_fast') \n    mfccs_features = librosa.feature.mfcc(y = audio, sr = sample_rate, n_mfcc = 40)\n    mfccs_scaled_features = np.mean(mfccs_features.T,axis = 0)\n    \n    return mfccs_scaled_features","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:33:30.63033Z","iopub.execute_input":"2021-09-16T19:33:30.630909Z","iopub.status.idle":"2021-09-16T19:33:30.638207Z","shell.execute_reply.started":"2021-09-16T19:33:30.630862Z","shell.execute_reply":"2021-09-16T19:33:30.637121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we iterate through every audio file and extract features \n# using Mel-Frequency Cepstral Coefficients (MFCC)\naudio_dataset_path = '/kaggle/input/urbansound8k/'\nextracted_features = []\nfor index_num,row in tqdm(data.iterrows()):\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    final_class_labels = row[\"class\"]\n    data = features_extract(file_name)\n    extracted_features.append([data,final_class_labels])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:34:39.985074Z","iopub.execute_input":"2021-09-16T19:34:39.985654Z","iopub.status.idle":"2021-09-16T19:50:26.259776Z","shell.execute_reply.started":"2021-09-16T19:34:39.985615Z","shell.execute_reply":"2021-09-16T19:50:26.259072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting extracted_features to Pandas dataframe\nfeatures_df=pd.DataFrame(extracted_features,columns=['feature','class'])\nfeatures_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:02.358738Z","iopub.execute_input":"2021-09-16T19:51:02.359031Z","iopub.status.idle":"2021-09-16T19:51:02.376114Z","shell.execute_reply.started":"2021-09-16T19:51:02.358995Z","shell.execute_reply":"2021-09-16T19:51:02.375386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the DataSet","metadata":{}},{"cell_type":"code","source":"# Split the dataset into independent and dependent dataset\nX = np.array(features_df['feature'].tolist())\ny = np.array(features_df['class'].tolist())","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:05.09378Z","iopub.execute_input":"2021-09-16T19:51:05.094079Z","iopub.status.idle":"2021-09-16T19:51:05.108199Z","shell.execute_reply.started":"2021-09-16T19:51:05.094049Z","shell.execute_reply":"2021-09-16T19:51:05.10737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:08.098653Z","iopub.execute_input":"2021-09-16T19:51:08.09922Z","iopub.status.idle":"2021-09-16T19:51:08.105053Z","shell.execute_reply.started":"2021-09-16T19:51:08.099182Z","shell.execute_reply":"2021-09-16T19:51:08.104222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class label\ny","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:10.659161Z","iopub.execute_input":"2021-09-16T19:51:10.659725Z","iopub.status.idle":"2021-09-16T19:51:10.665074Z","shell.execute_reply.started":"2021-09-16T19:51:10.659686Z","shell.execute_reply":"2021-09-16T19:51:10.66422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding \n# using label encoder to get back the class name using inverse label encoder \nlabelencoder = LabelEncoder()\ny = to_categorical(labelencoder.fit_transform(y)) # tranform class label \ny","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:13.309101Z","iopub.execute_input":"2021-09-16T19:51:13.30937Z","iopub.status.idle":"2021-09-16T19:51:13.319628Z","shell.execute_reply.started":"2021-09-16T19:51:13.309343Z","shell.execute_reply":"2021-09-16T19:51:13.318621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nprint(f'X_train shape is {X_train.shape}')\nprint(f'X_test shape is {X_test.shape}')\nprint(f'y_train shape is {y_train.shape}')\nprint(f'y_test shape is {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:15.823443Z","iopub.execute_input":"2021-09-16T19:51:15.823714Z","iopub.status.idle":"2021-09-16T19:51:15.835183Z","shell.execute_reply.started":"2021-09-16T19:51:15.823687Z","shell.execute_reply":"2021-09-16T19:51:15.834227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"labels = y.shape[1] # total target variable or class variable\ninput_size = X.shape[1] # total feature value like here n_mfcc value \nprint(f\"number of total class label '{labels}'\")\nprint(f\"number of features used '{input_size}' \")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:18.469409Z","iopub.execute_input":"2021-09-16T19:51:18.470077Z","iopub.status.idle":"2021-09-16T19:51:18.475806Z","shell.execute_reply.started":"2021-09-16T19:51:18.470037Z","shell.execute_reply":"2021-09-16T19:51:18.474952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# network in sequential nature i.e. output of previous layer is input of the next layer\nmodel = Sequential()\n\n#first layer\nmodel.add(Dense(units = 1024, input_shape = (input_size,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n#second layer\nmodel.add(Dense(units = 512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n#third layer\nmodel.add(Dense(units = 256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n#final layer\n# add neural network so flatten the output comming from last layer of cnn model \nmodel.add(Flatten()) \nmodel.add(Dense(units = labels, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:21.083969Z","iopub.execute_input":"2021-09-16T19:51:21.084541Z","iopub.status.idle":"2021-09-16T19:51:23.451951Z","shell.execute_reply.started":"2021-09-16T19:51:21.084504Z","shell.execute_reply":"2021-09-16T19:51:23.451027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:28.70962Z","iopub.execute_input":"2021-09-16T19:51:28.710402Z","iopub.status.idle":"2021-09-16T19:51:28.721977Z","shell.execute_reply.started":"2021-09-16T19:51:28.71036Z","shell.execute_reply":"2021-09-16T19:51:28.720031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss = 'categorical_crossentropy', \n              metrics = ['accuracy'],\n              optimizer = 'adam')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:31.788416Z","iopub.execute_input":"2021-09-16T19:51:31.788666Z","iopub.status.idle":"2021-09-16T19:51:31.804748Z","shell.execute_reply.started":"2021-09-16T19:51:31.78864Z","shell.execute_reply":"2021-09-16T19:51:31.803989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model\n","metadata":{}},{"cell_type":"code","source":"num_epochs = 100\nnum_batch_size = 32\nmodel.fit(X_train, y_train,\n          batch_size = num_batch_size, \n          epochs = num_epochs,\n          validation_data = (X_test, y_test),\n          verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:51:36.789293Z","iopub.execute_input":"2021-09-16T19:51:36.78956Z","iopub.status.idle":"2021-09-16T19:52:45.05758Z","shell.execute_reply.started":"2021-09-16T19:51:36.789534Z","shell.execute_reply":"2021-09-16T19:52:45.056883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result Visualization","metadata":{}},{"cell_type":"code","source":"#saving the model history\nloss = pd.DataFrame(model.history.history)\n\n#plotting the loss and accuracy \nplt.figure(figsize=(10,10))\n\nplt.subplot(2,2,1)\nplt.plot(loss[\"loss\"], label =\"Loss\")\nplt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\nplt.legend()\nplt.title(\"Training and Validation Loss\")\n\nplt.subplot(2,2,2)\nplt.plot(loss['accuracy'],label = \"Training Accuracy\")\nplt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\nplt.legend()\nplt.title(\"Training-Validation Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:52:55.735036Z","iopub.execute_input":"2021-09-16T19:52:55.735624Z","iopub.status.idle":"2021-09-16T19:52:56.145999Z","shell.execute_reply.started":"2021-09-16T19:52:55.735587Z","shell.execute_reply":"2021-09-16T19:52:56.145137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(X_test)\n\n# finding class with larget predicted probability using argmax of numpy \ny_pred = np.argmax(prediction, axis = 1)  # prediction using model \ny_test_orig = np.argmax(y_test, axis = 1) # original y_test\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:05.715175Z","iopub.execute_input":"2021-09-16T19:53:05.715469Z","iopub.status.idle":"2021-09-16T19:53:05.863642Z","shell.execute_reply.started":"2021-09-16T19:53:05.715441Z","shell.execute_reply":"2021-09-16T19:53:05.862937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting Class Label Name\nclass_label_lst = np.array(features_df['class'].unique().tolist())\nprint(class_label_lst)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:08.769729Z","iopub.execute_input":"2021-09-16T19:53:08.7703Z","iopub.status.idle":"2021-09-16T19:53:08.777593Z","shell.execute_reply.started":"2021-09-16T19:53:08.770267Z","shell.execute_reply":"2021-09-16T19:53:08.776495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_name = ['dog_bark','children_playing','car_horn','air_conditioner','street_music',\n              'gun_shot','siren','engine_idling','jackhammer','drilling']\nprint(classification_report(y_test_orig, y_pred, target_names = class_name))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:11.279228Z","iopub.execute_input":"2021-09-16T19:53:11.27949Z","iopub.status.idle":"2021-09-16T19:53:11.293974Z","shell.execute_reply.started":"2021-09-16T19:53:11.279464Z","shell.execute_reply":"2021-09-16T19:53:11.293075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_df = pd.DataFrame(confusion_matrix(y_test_orig, y_pred), columns = class_name, index = class_name)\nprint(\"\\n\")\nprint(\"**************************** CONFUSION METRIX *********************************\")\nprint(\"\\n\")\nconfusion_df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T19:53:15.023686Z","iopub.execute_input":"2021-09-16T19:53:15.02428Z","iopub.status.idle":"2021-09-16T19:53:15.047526Z","shell.execute_reply.started":"2021-09-16T19:53:15.024242Z","shell.execute_reply":"2021-09-16T19:53:15.046905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}