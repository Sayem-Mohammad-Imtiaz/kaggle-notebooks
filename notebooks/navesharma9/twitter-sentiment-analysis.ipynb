{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option(\"display.max_colwidth\",200)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest_tweets = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=train_tweets, x='label', hue='label')\nplt.title('Types of comments : 0 - > Non Rasict/Sexist , 1 - > Rasict/Sexist')\nplt.xlabel('Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rasict & Sexist Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets[train_tweets['label']==1].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not Rasict & Sexist Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets[train_tweets['label']==0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of length of the tweets, in terms of words, in both train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = train_tweets['tweet'].str.len()\ntest_len = test_tweets['tweet'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train data length :\" , train_len)\nprint(\"test data length :\" , test_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train_len, bins=20,label='train_tweets')\nplt.hist(test_len , bins=20, label='test_tweets')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = train_tweets.append(test_tweets,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_pattern(input_text,pattern):\n    r = re.findall(pattern, input_text)\n    for i in r:\n        input_text = re.sub(i,\"\",input_text)\n    return input_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['tidy_tweet'] = np.vectorize(remove_pattern)(dataset['tweet'],\"@[\\w]*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['tidy_tweet'] = dataset['tidy_tweet'].str.replace('[^a-zA-Z#]',\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopword(input_text):\n    txt_clean = \" \".join([word for word in input_text.split() if len(word)>3])\n    return txt_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['tidy_tweet'] = dataset['tidy_tweet'].apply(lambda x:remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_tweet = dataset['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pstem = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_tweet = tokenized_tweet.apply(lambda x:[pstem.stem(i) for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = \" \".join(tokenized_tweet[i])\ndataset['tidy_tweet'] = tokenized_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nall_words = ' '.join([text for text in dataset['tidy_tweet']])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Words in non racist/sexist tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in dataset['tidy_tweet'][dataset['label']==0]])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Words in racist/sexist tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in dataset['tidy_tweet'][dataset['label']==1]])  \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_vector = CountVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')\nbow = bow_vector.fit_transform(dataset['tidy_tweet'])\nbow.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow.data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bow[:31962,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = bow[31962:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,train_tweets['label'],test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lg.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_int = pred[:,1]>=0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_int = pred_int.astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test,pred_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}