{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns\nsns.set(style='dark')\nimport sklearn\nimport imblearn\nimport matplotlib.pyplot as plt\nimport time\nimport sklearn.metrics as m\nimport xgboost as xgb\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndf1=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")#,nrows = 50000\ndf2=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\ndf3=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n\ndf5=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\ndf6=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (df1[' Label'].unique())\nprint (df2[' Label'].unique())\nprint (df3[' Label'].unique())\n\nprint (df5[' Label'].unique())\nprint (df6[' Label'].unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\n\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\" Label\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Droping Null values","metadata":{}},{"cell_type":"code","source":"# Check for missing data\nprint(f\"Missing values: {data.isnull().sum().sum()}\")\n\n# Check for infinite values, replace with NAN so it is easy to remove them\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)\nprint(f\"Missing values: {data.isnull().sum().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deleteCol = []\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop([column],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deleteCol = []\nfor column in data.columns:\n    if column == ' Label':\n        continue\n    elif data[column].dtype==np.object:\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop(column,axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[' Flow Duration'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in data.columns:\n    if data[column].dtype == np.int64:\n        maxVal = data[column].max()\n        if maxVal < 120:\n            data[column] = data[column].astype(np.int8)\n        elif maxVal < 32767:\n            data[column] = data[column].astype(np.int16)\n        else:\n            data[column] = data[column].astype(np.int32)\n            \n    if data[column].dtype == np.float64:\n        maxVal = data[column].max()\n        minVal = data[data[column]>0][column]\n        if maxVal < 120 and minVal>0.01 :\n            data[column] = data[column].astype(np.float16)\n        else:\n            data[column] = data[column].astype(np.float32)\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benign = data[data[' Label'] == 'BENIGN'].sample(frac=0.1).reset_index(drop=True)\nattack = data[data[' Label'] != 'BENIGN']\ndata = pd.concat([attack, benign])\ndata[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ddos = data[data[' Label'] == 'DDoS'].sample(frac=0.32).reset_index(drop=True)\nattack = data[data[' Label'] != 'DDoS']\ndata = pd.concat([attack, ddos])\ndata[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PortScan = data[data[' Label'] == 'PortScan'].sample(frac=0.32).reset_index(drop=True)\nattack = data[data[' Label'] != 'PortScan']\ndata = pd.concat([attack, PortScan])\ndata[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data[' Label']\nX = data.drop([' Label'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\nFeature selection is a technique where we choose those features in our data that contribute most to the target variable.\n\n\n    1. Reduces Overfitting: Less redundant data means less possibility of making decisions based on redundant data/noise.\n    2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n    3. Reduces Training Time: Less data means that algorithms train faster.","metadata":{}},{"cell_type":"markdown","source":"SelectKBest\n\nThe SelectKBest class just scores the features using a function (in this case f_classif for classification) and then \"removes all but the k highest scoring features\"","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,y)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(30,'Score'))  #print 10 best features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = pd.DataFrame()\nn = len(featureScores['Specs'])\nfor i in featureScores.nlargest(n//2,'Score')['Specs']:\n        feature[i] = data[i]\nfeature[' Label'] = data[' Label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt \nimport seaborn as sns\nfig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Correlation**, statistical technique which determines how one variables moves/changes in relation with the other variable.\nWhen we have highly correlated features in the dataset it increses the variance and unreliable.\n\nClearly there is an exellent correlation in -\n\n* Bwd Packet Length Std\n* Bwd Packet Length Mean\n* Avg Bwd Segment Size\n* Bwd Packet Length Max\n* Packet Length Std\n* Average Packet Size\n* Packet Length Mean\n* Max Packet Length\n* Packet Length Variance\n\nAlso,correlattion lies in -\n\n* Flow IAT Max\n* Idle Max\n* Fwd IAT Max\n* Flow IAT Std\n* Idle Std\n* Idle Mean\n                  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature.drop([' Bwd Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Avg Bwd Segment Size'],axis=1,inplace=True)\nfeature.drop(['Bwd Packet Length Max'],axis=1,inplace=True)\nfeature.drop([' Packet Length Std'],axis=1,inplace=True)\nfeature.drop([' Average Packet Size'],axis=1,inplace=True)\nfeature.drop([' Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Max Packet Length'],axis=1,inplace=True)\nfeature.drop([' Packet Length Variance'],axis=1,inplace=True)\n\n\nfeature.drop([' Idle Max'],axis=1,inplace=True)\nfeature.drop([' Fwd IAT Max'],axis=1,inplace=True)\nfeature.drop([' Flow IAT Std'],axis=1,inplace=True)\nfeature.drop([' Idle Std'],axis=1,inplace=True)\nfeature.drop(['Idle Mean'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attackType = feature[' Label'].unique()\nfeature[' Label'] = feature[' Label'].astype('category')\nfeature[' Label'] = feature[' Label'].astype(\"category\").cat.codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (attackType)\nprint (feature[' Label'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature0 = feature[feature[' Label'] == 0]\nfeature0.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = feature[' Label']\nX = feature.drop([' Label'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler('majority')\nX_rus, y_rus = rus.fit_resample(X, y)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_rus.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_rus.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek('not majority' ,n_jobs=-1)\nX_smt, y_smt = smt.fit_resample(X_rus, y_rus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_smt.to_csv('/kaggle/working/X_smt.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_smt.to_csv('/kaggle/working/y_smt.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_smt.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split dataset on train and test\nfrom sklearn.model_selection import train_test_split\ntrain_X, test_X,train_y,test_y=train_test_split(X_smt,y_smt,test_size=0.3, random_state=10)\n\n#Exploratory Analysis\n# Descriptive statistics\n# print (train_X.describe())\n\n\n# Packet Attack Distribution\n# train[' Label'].value_counts()\n# test[' Label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scalling numerical attributes\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train_X.select_dtypes(include=['float32','float16','int32','int16','int8']).columns\nsc_train = scaler.fit_transform(train_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\nsc_test = scaler.fit_transform(test_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\n\n# turn the result back to a dataframe\ntrain_X = pd.DataFrame(sc_train, columns = cols)\ntest_X = pd.DataFrame(sc_test, columns = cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Feature Selection\n# from sklearn.ensemble import ExtraTreesClassifier\n# etc = ExtraTreesClassifier();\n\n# # fit random forest classifier on the training set\n# etc.fit(train_X, train_y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # extract important features\n# score = np.round(etc.feature_importances_,3)\n# importances = pd.DataFrame({'feature':train_X.columns,'importance':score})\n# importances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n# # plot importances\n# plt.rcParams['figure.figsize'] = (11, 4)\n# importances.plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset Partition\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,train_y,train_size=0.70, random_state=2)\n\n#Fitting Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# Train KNeighborsClassifier Model\n# KNN_Classifier = KNeighborsClassifier(n_jobs=-1)\n# KNN_Classifier.fit(X_train, Y_train); \n\n# Random Forest Model \n# RTC_Classifier = RandomForestClassifier(criterion='entropy')\n# RTC_Classifier.fit(X_train, Y_train)\n\n# Train LogisticRegression Model\n# LGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\n# LGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\n# BNB_Classifier = BernoulliNB()\n# BNB_Classifier.fit(X_train, Y_train)\n\n# Train Decision Tree Model\n# DTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\n# DTC_Classifier.fit(X_train, Y_train)\n# print ('DTC Classifier Trained')\n\n# XGB_Classifier = xgb.XGBClassifier(criterion='entropy', random_state=0)\n# XGB_Classifier.fit(X_train, Y_train)\n# print ('XGB Classifier Trained')\n\n\n\n\n# ADA_Classifier = AdaBoostClassifier(\n#     DTC_Classifier,\n#     n_estimators=100,\n#     learning_rate=1.5)\n\n# ADA_Classifier.fit(X_train, Y_train)\n# print ('ADA Classifier Trained')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Model \nRTC_Classifier = RandomForestClassifier(criterion='entropy')\nRTC_Classifier.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB_Classifier = xgb.XGBClassifier(criterion='entropy', random_state=0)\nXGB_Classifier.fit(X_train, Y_train)\nprint ('XGB Classifier Trained')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ADA_Classifier = AdaBoostClassifier(\n    DTC_Classifier,\n    n_estimators=100,\n    learning_rate=1.5)\n\nADA_Classifier.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate Models\nfrom sklearn import metrics\n\nmodels = []\n\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('Random Forest Classifier', RTC_Classifier))\nmodels.append(('Gaussian Naive Baye Model', BNB_Classifier))\nmodels.append(('ADA_Classifier', ADA_Classifier))\nmodels.append(('XGB_Classifier', XGB_Classifier))\nmodels.append(('KNN_Classifier', KNN_Classifier))\nmodels.append(('Lgr_classifier', LGR_Classifier))\n\nfor i, v in models:\n    vpred = v.predict(X_train)\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, vpred)\n    confusion_matrix = metrics.confusion_matrix(Y_train, vpred)\n    classification = metrics.classification_report(Y_train, vpred)\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)  \n    print()\n\n    \n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validate Models\nfor i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}