{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\nimport folium\nfrom folium import plugins\nsns.set()\n\nimport os\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndata_folder = '../input/crimes-in-boston'\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRIME_DATA_FILE = os.path.join(data_folder, \"crime.csv\")\nprint(\"Crime data:\", CRIME_DATA_FILE)\ncrime_df = pd.read_csv(CRIME_DATA_FILE, encoding = 'ISO-8859-1')  # note the odd encoding...\n\nprint(\"Number of crimes:\", len(crime_df))\n\n# Peek a bit\ncrime_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Do a bit of data processing\ncrime_df.UCR_PART = crime_df.UCR_PART.astype('category')\ncrime_df.OFFENSE_CODE_GROUP = crime_df.OFFENSE_CODE_GROUP.astype('category')\ncrime_df.SHOOTING.fillna(\"N\", inplace=True)  # replace nan for shootings with 'N' for \"no\"\ncrime_df.OCCURRED_ON_DATE = pd.to_datetime(crime_df.OCCURRED_ON_DATE)  # use Pandas datetime\ncrime_df.Location = crime_df.Location.apply(eval)  # set location as tuple\ncrime_df.Lat.replace(-1.0, None, inplace=True)\ncrime_df.Long.replace(-1.0, None, inplace=True)\ncrime_df.dropna(subset=[\"Lat\", \"Long\"], inplace=True)\n\nrename_map = {\n    \"OFFENSE_CODE\": \"Code\",\n    \"OFFENSE_CODE_GROUP\": \"Code_group\",\n    \"OFFENSE_DESCRIPTION\": \"Description\",\n    \"OCCURRED_ON_DATE\": \"Date\",\n    \"DISTRICT\": \"District\",\n    \"STREET\": \"Street\",\n    \"YEAR\": \"Year\",\n    \"MONTH\": \"Month\",\n    \"HOUR\": \"Hour\",\n    \"DAY_OF_WEEK\": \"Day_of_week\",\n    \"SHOOTING\": \"Shooting\"\n}\n\ncrime_df.rename(columns=rename_map, inplace=True)\ncrime_df.sort_values(by=\"Date\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will focus on the major **Part One** crimes: burglaries, larceny, homicides..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's keep the major crimes\ncrime_df = crime_df[crime_df.UCR_PART == \"Part One\"]\n# Remove the unused categories\ncrime_df.Code_group.cat.remove_unused_categories(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crime_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistics\n\nLet's get a few statistics on the number and nature of crimes."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(crime_df.Code_group.value_counts())\nprint()\nprint(crime_df.groupby(\"Year\").Code_group.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NB: removing unused categories was helpful here because\n# setting the code groups to 'categorical' before filtering Part One crimes\n# meant Seaborn would pickup the empty Part (One, Two) categories\ng = sns.catplot(y=\"Code_group\", kind=\"count\",\n                data=crime_df,\n                order=crime_df.Code_group.value_counts().index,\n                aspect=1.6,\n                palette=\"muted\")\ng.set_axis_labels(\"Number of occurrences\", \"Offense group\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NB: removing unused categories was helpful here because\n# setting the code groups to 'categorical' before filtering Part One crimes\n# meant Seaborn would pickup the empty Part (One, Two) categories\ng = sns.catplot(y=\"Code_group\", col=\"Year\", col_wrap=2, kind=\"count\",\n                data=crime_df,\n                order=crime_df.Code_group.value_counts().index,\n                aspect=1.5,\n                height=3,\n                palette=\"muted\")\ng.set_axis_labels(x_var=\"Number of occurrences\", y_var=\"Offense group\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Putting data on the map\n\nWe'll use Folium for data visualisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_location = crime_df.Location.iloc[0]  # grab location of one offense\nbase_location","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston_map = folium.Map(location=base_location,\n                        prefer_canvas=True,\n                        zoom_start=12,\n                        min_zoom=12)\nplugins.ScrollZoomToggler().add_to(boston_map)\n\nboston_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we add the homicides up to 2016\nyear = 2016\nfor row in crime_df[(crime_df.Code_group == \"Homicide\") & (crime_df.Year <= year)].itertuples(index=False):\n    icon = folium.Icon(color='red', icon='times', prefix='fa')\n    popup_txt = str(row.Date)\n    if row.Shooting == 'Y':\n        popup_txt = \"Shooting \" + popup_txt\n    folium.Marker(row.Location, icon=icon, popup=popup_txt).add_to(boston_map)\n\nboston_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"Let's try modelling the occurrence of crimes as a random process of time."},{"metadata":{},"cell_type":"markdown","source":"We'll consider the number of homicides as a counting process $N = \\{ N_t \\}$.\n\nLet's plot it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"times_ = crime_df[crime_df.Code_group == \"Homicide\"].groupby(by=\"Year\").Date\n\n# Create counters for number of events\ncounts_ = []\nfor i, t in times_:\n    counts_.append(np.arange(len(t)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(14, 9))\naxes = axes.flatten()\nfor i, (year, t) in enumerate(times_):\n    axes[i].step(t, counts_[i], where='post')\n    axes[i].set_title(f\"Number of homicides. Year = {year}\")\n    axes[i].set_xlabel(\"Date\")\n    axes[i].set_ylabel(\"Homicide count\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple model: Poisson process\n\nWe start by modelling the homicides as a Poisson process: the number of homicides occurring between $t_1$ and $t_2$ is taken to follow a Poisson distribution:\n$$\n    N(t_1,t_2] \\sim \\mathcal{P}\\left(\\lambda(t_2-t_1)\\right).\n$$\nwhere $\\lambda > 0$ is the _intensity_ parameter."},{"metadata":{},"cell_type":"markdown","source":"There are several questions with such a model: over which timeframe can we consider $\\lambda$ to be constant?"},{"metadata":{},"cell_type":"markdown","source":"The likelihood function of a sequence of events $\\{t_1,\\ldots,t_n\\}$ under the model $\\mathrm{PP}(\\lambda)$ is\n$$\n    p(\\{t_1, \\ldots, t_n\\}\\mid \\lambda) = \\exp(-\\lambda \\Delta t)\\lambda^n\n$$\nThe MLE of the rate $\\lambda$ is given by\n$$\n    \\hat\\lambda = \\frac{n}{\\Delta t},\n$$\ncorresponding to the intuitive notion of what a \"rate\" is: the _crime rate_ or the _homicide rate_.\n\nLet's compute the rates for years 2015-2018:"},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_homic_rates = {}\nfor i, (y, ti) in enumerate(times_):\n    dt_window = ti.iloc[-1] - ti.iloc[0]\n\n    rate_mle = counts_[i][-1] / dt_window.days  # Poisson event rate in days\n    yearly_homic_rates[y] = rate_mle\n    print(f\"Rate for year {y} (days^{{-1}}): {rate_mle:.3f}\", f\" i.e. every {1./rate_mle:.2f} days\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, supposing the daily homicide rate over a year is homogeneous, the rates computed for each year are pretty much the same."},{"metadata":{},"cell_type":"markdown","source":"### Testing homogeneity - Ripley's $K$-function\n\nAccording to the literature, we can test for homogeneity in a dataset by introducing Ripley's $K$-function\n\\\\[\n    K(t) = \\mathbb{E} \\left[ \\#\\{t_i \\mid |t_j-t_i| \\leq t\\} \\mid t_j \\sim \\mathcal{U}(\\mathcal{T}) \\right]\n\\\\]\nwhich measures how the process fills up an area around any event, and where $\\mathcal{T} = \\{t_i\\}$ is a realization of the process and $t_j$ is uniformly chosen in $\\mathcal{T}$ (for a stationary process the random choice of $t_j$ can be dropped and replaced by a fixed point in the observation window). It can be computed empirically as\n$$\n    \\widehat{K}(t) = \\lambda^{-1}n ^{-1}\\sum_{i\\neq j} \\mathbf 1(|t_i-t_j|\\leq t)\n$$\n\nFor an actual Poisson process, the theoretical Ripley $K$ function in one dimension is given by $K(t) = 2t$."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial import distance as scdist\ndef ripley_K(data, t):\n    \"\"\"\n    Args\n        data (ndarray): point pattern data array\n        t (float): interval radius\n    \"\"\"\n    data = data.astype('datetime64[s]')  # ensure time data is in ms\n    dist = scdist.pdist(data, 'euclidean')  # compute the array of pair-wise distances\n    lbda = data.size / (data[-1] - data[0]).astype(float)\n    ksum = 1./lbda * np.sum((dist <= t), axis=-1) / data.shape[0]\n    return ksum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evt_time_data = crime_df[crime_df.Code_group == \"Homicide\"].Date[:, None].astype('datetime64[s]')\n\ndist = scdist.pdist(evt_time_data.astype(\"datetime64[s]\"), 'euclidean')\ndist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbda = evt_time_data.size / (evt_time_data[-1] - evt_time_data[0]).astype(float)\nprint(f\"Event rate {lbda.item():.3e} events/s\")\nprint(f\"Event rate {86400*lbda.item():.3f} events/day\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make our testing time values run over 1 1/2 years\nripley_test_times = np.linspace(0, 86400 * 365 * 1.5, 150)  # time expressed in seconds\n\nkstat = ripley_K(evt_time_data, ripley_test_times[:, None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 7))\nax.plot(ripley_test_times, kstat, label=\"Empirical Ripley $\\widehat{K}(t)$\")\nax.plot(ripley_test_times, 2*ripley_test_times, label=\"Theoretical Ripley $K(t) = 2t$\")\nax.legend()\nax.set_title(\"Ripley $K$-function\")\nax.set_xlabel(\"Time $t$ (s)\")\nax.set_ylabel(\"Statistic $\\widehat{K}(t)$\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we have that the empirical Ripley function is $\\hat{K}(t) < 2t$, which means the temporal point pattern is repulsive (with increased repulsiveness at long distances) and we can **reject the homogeneous Poisson hypothesis**."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Nonhomogeneous Poisson process\n\nNow we suppose that\n$$\n    N(t_1, t_2] \\sim \\mathcal{P}\\left(\\int_{t_1}^{t_2} \\lambda(s)\\,ds \\right)\n$$"},{"metadata":{},"cell_type":"markdown","source":"### Kernel estimation\n\nGiven a set of observed events $\\mathcal T = \\{ t_i\\}$, we can estimate the intensity function as\n$$\n    \\widehat{\\lambda}(t) = \\sum_{t_i\\in\\mathcal T} K\\left(\\frac{t-t_i}{h}\\right)\n$$\nwhere $h > 0$ is the bandwidth and $K$ is a smoothing kernel, with $\\int K(x)\\,dx = 1$."},{"metadata":{},"cell_type":"markdown","source":"We introduce the folliwing kernels:\n* local-average kernel $K_h(x) = \\frac{1}{2h}\\mathbf{1}_{|x|\\leq h}$.\n* Gaussian kernel:\n$$\n    K_h(x) = \\frac{1}{\\sqrt{2\\pi h^2}} \\exp\\left( - \\frac{x^2}{2h^2} \\right)\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LocalAverageKernel:\n    def __init__(self, bandwidth):\n        self.bandwidth = bandwidth\n    \n    def __call__(self, x):\n        normalize_cst = float(2. * self.bandwidth)\n        ks = (np.abs(x) <= self.bandwidth).astype(dtype='float')\n        return ks / normalize_cst\n\nclass GaussianKernel:\n    def __init__(self, bandwidth):\n        self.bandwidth = bandwidth\n    \n    def __call__(self, x):\n        x = x.astype(float)\n        bw = float(self.bandwidth)  # in nanoseconds\n        argument = - 0.5 * (x / bw) ** 2\n        return np.exp(argument) / np.sqrt(2 * np.pi * bw ** 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfmt = '%Y%m%d'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's do a kernel estimation for 2017\nevt_times_ = times_.get_group(2017)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dates = pd.date_range('20170101', '20180101', freq='D')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(plot_dates.shape)\n\nprint(evt_times_[None, :].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(plot_dates[:, None] - evt_times_[None, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bandwidths = []\nfor bw in [7, 10, 15, 20]:\n    bandwidth = pd.Timedelta('1D') * bw\n    print(\"Bandwidth:\", bandwidth)\n    bandwidths.append(bandwidth)\n\n# Let's get a few different kernels for our bandwidths\nkernels_ = [GaussianKernel(bw.asm8) for bw in bandwidths]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dts = plot_dates[:, None] - evt_times_[None, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_ns_day = 86400 * 1e9  # number of nanoseconds in a day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intensity_estims_ = [num_ns_day * kern(dts).sum(axis=-1) for kern in kernels_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 9))\nfor j, ins_estim in enumerate(intensity_estims_):\n    plt.plot(plot_dates, ins_estim, linestyle='-', label=f\"Bandwidth = {bandwidths[j]}\")\nplt.title(\"Intensity estimate for homicides, year 2017\")\nylims = plt.ylim()\nplt.vlines(evt_times_, *ylims, linestyles='--', lw=1.0, alpha=0.8)\nplt.ylim(*ylims)\nplt.legend()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Intensity (1/days)\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}