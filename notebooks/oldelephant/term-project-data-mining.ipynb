{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n<h3><font color='black'>Data Mining Project  </font></h3>\nAmnah Abdelrahman","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#                          <center style='color:blue'><u> BoardGameGeek Reviews</u></center>  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Table of Content: \n* [Introduction](#Introduction)\n* [Data Description](#Data_Description)\n* [Data Processing](#Data_Processing)\n* [Data Exploration](#Data_Exploration)\n* [Train Test Split](#train_test_split)\n\n* [Text preprocessing](#Text_preprocessing)\n    * [Count Vectorizer](#CountVectorizer) \n    * [TfidfVectorizer (Term Frequency times Inverse Document Frequency)](#TfidfVectorizer)\n        \n* [Model Selectoin](#Model_Selectoin)\n    * [Linear and Logistic Regression](#Linear_and_Logistic_Regression)\n    * [Linear Classifiers](#Linear_Classifiers)\n    * [Support Vector Machine](#Support_Vector_Machine)\n    * [Linear Support Vector Machine](#Linear_Support_Vector_Machine)\n    * [Naive Bayes Classifier](#Naive_Bayes_Classifier)\n    * [Decision Tree Classifier](#Decision_Tree_Classifier)\n    * [Ensemble Model](#Ensemble_Model)\n        * [Boosting Models](#Boosting_Models)\n        * [Random Forest Classifier](#Random_Forest_Classifier) \n    * [Recurrent Neural Network (Futuer work)](#Recurrent_Neural_Network) \n        \n* [hyperparameter Tuning ](#hyperparameter_Tuning)\n     * [Grid Search Cross Validation](#GridSearchCV)\n     * [Randomized Search Cross Validation](#RandomizedSearchCV)\n        \n* [Results](#Results)\n* [Challenges](#Challenges)\n* [References](#References)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Introduction\" style='color:blue'>Introduction</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\nThe goal of this project is to predict the rating of a board-game given a review, using text analysis tools and machine learning methods. BoardGameGeek is the world's largest board game site. It provides huge data of all board-games with much information on each game like the game difficulty, the number of players, the time required for the game, the designer, and some recommendations. it is also giving the number of reviews, user comments, games ranking, and much more. \n\nThe general sequence to build a prediction/classification model starts by understanding the problem and describing the dataset by using visualization tools as an example. Then, selects the features and prepares the dataset to fit the proposed predictive models. After that, it remains to apply the model evaluation and select the most promising model. Finally, optimizes and measure the final model performance, before presenting the last result. This is going to be our methodology in this project.\n\nThis work is mainly our work, we read many blogs and websites to understand and grasp the general way to solve the problem. In terms of contribution, we set various model parameters and trials during different stages in this work, and the credits and citations are given for the implementation we learned from others.\n\nSo let's start mining \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load Libraries\nThe first thing we need to do is loading the required libraries,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic libraries for dataframe, graphs \nimport sys, sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# for model selection\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import decomposition, ensemble\nfrom sklearn import tree\n\n# Model optimization \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# for data preparation and model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn import linear_model, preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import model_selection, preprocessing, metrics\nfrom sklearn.model_selection import cross_val_score , train_test_split\n\n\n# Text analysis and transformation\nimport nltk\nimport string\nfrom nltk.stem import PorterStemmer\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n\n\n# ANN model \nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n#from keras.models import Sequential  low_memory=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"Data_Description\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Data_Description\" style='color:blue'>Data Description</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n The source of the data is Kaggle https://www.kaggle.com/jvanelteren/boardgamegeek-reviews#2019-05-02.csv.\n The data spread over 3 CSV files:\n \n- <i>games_detailed_info.csv</i>  (with 56 columns and 17063) : \nThis file has a many detials about all type of board games, exmaples: are  the game rank, number of plyers,    number of user rated, time requried for the game, desiner and some instructions.    \n*** \n-  <i>bgg-13m-reviews.csv</i> (with 6 columns and 13170073):\nprovids the user names with there reviews and rating for each game.\n*** \n- <i>2019-05-02.csv</i> (9 columns and 17065) : \nThis file contain more precisis information for each game the game name ,year, rank and number of users rated. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"detailed_df = pd.read_csv('/kaggle/input/boardgamegeek-reviews/games_detailed_info.csv',low_memory=False)  # for the interpretuer to deterimne datatype it take hight memorey\nreviews_df = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv')\nranking_df = pd.read_csv('/kaggle/input/boardgamegeek-reviews/2019-05-02.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" detailed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ranking_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the size of each file \nprint(\"detailed_df (rows ,columns )\",detailed_df.shape)\nprint(\"ranking_df (rows ,columns )\", ranking_df.shape)\nprint(\"reviews_df (rows ,columns )\",reviews_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the method  'info()' to have a gereral idea about the columns names, data type, and missing data in each column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the columns names, data type, and missing data\ndetailed_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ranking_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"Data_Processing\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Data_Processing\" style='color:blue'> Data Processing </h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\nData cleaning and data processing are important steps for a good model, usually, these steps required much of time but it totally worth it at the end. \nIn this section, we are going to drop all the missing columns and the missing reviews. Then we are going to merge all the data frames together to produce one data frame with all the required features.\n\n","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# count the missing data\ndetailed_df.isnull().sum()\nranking_df.isnull().sum()\nreviews_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nIn our way to prepare the dataset, we will remove all the unrelated features to the problem. In the game's details file, there are 25 columns are missing more than 80% of their values, so they will be removed, as well as the 'URL','Thumbnail' from the ranking file, and 'Unnamed: 0' form the review file. Finally, we will delete all the 10532317 missing comments rows. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop all the coulmns with missing values and the redundent/uneeded once for our classfiaction problem\n\ndetailed_df = detailed_df.drop(axis=1, index=None, columns=[\n'Unnamed: 0',   'Abstract Game Rank','Accessory Rank', 'Amiga Rank','Arcade Rank', 'Atari ST Rank',                   \n\"Children's Game Rank\", 'Commodore 64 Rank','Customizable Rank', 'Family Game Rank','Party Game Rank',                  \n'RPG Item Rank', 'Strategy Game Rank', 'Thematic Rank', 'Video Game Rank',                  \n'War Game Rank', 'alternate', 'boardgameartist', 'boardgamecategory', 'boardgamecompilation',             \n'boardgamedesigner', 'boardgameexpansion',  'boardgamefamily',  'boardgameimplementation',          \n'boardgameintegration', 'boardgamemechanic','boardgamepublisher', 'description','image',                               \n'suggested_language_dependence', 'suggested_playerage', 'thumbnail','type' ])\n\nranking_df = ranking_df.drop(axis=1, index=None, columns=['URL','Thumbnail'])\n\nreviews_df = reviews_df.drop(axis=1 ,columns=['Unnamed: 0']) \n\nreviews_df = reviews_df.dropna(axis=0 ,subset=['comment'])    # Drop all missing reviews ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\nThe following step is an Inner join between the detailed_df & ranking_df data-farms, based on ID's, where both of them almost has the same numbers of rows. Using an inner join requires each row in the two joined data-frames to have the same number of rows, this could lead to loss of some rows, unlike the left join. Then the following step is a left join between the outcome of the inner join with the review data-frame where this last one has 13170073 instances. \n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ranking_detailed_df = pd.merge(left=ranking_df, right=detailed_df, left_on='ID', right_on='id') # inner join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joint_df = pd.merge(left=reviews_df, right=ranking_detailed_df, how='left', left_on='ID', right_on='ID') # left join\njoint_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joint_df = joint_df.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nNow after we did some cleaning and merged the data-frames, we now have one data-frame, but still, there some work that needs to be done, making sure that we have only the useful data without redundant or missing values. \nSo now we are going to dig a little bit deeper into the columns, understanding the correlation between them regards to the target vector which is the 'rating'.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nThe method 'corr()' is a statistical tool to calculate the correlation between the feature and the target. The correlation between the numerical features is a good way to reduce model complexity and avoid what's known as overfitting situation. The closer the value to one the strongest correlation, the closer to zero the weaker correlation.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ths corolation of numeric values to the the target 'Rating' the main ranking\ncorrelations = joint_df.corr()\ncorrelations['rating'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot # 'rating', 'Average','averageweight', 'Bayes average', 'stddev','numweights', 'Rank','Board Game Rank'\nplt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"rating\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"Average\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"Rank\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"averageweight\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"numweights\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"Bayes average\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_df[\"stddev\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nFrom the above, we can see that some features have a negative correlation with the target so we element them, as well as the very strong correlated once.  Rank, Average, Bayes average, ID, and stddev, all of them will be removed to reduce the model complexity. \nThe above graphs give us more sight on these data, obviously Rating and Average have the same purpose, then the Bayes average and averge-weight are related to the game complexity. Finally Rank and num-weights are looks the same. All of these data can be ignored for our model.  A feature engineering we can do is to find the difference between the following:\n\n                                        * playes_num = 'maxplayers'-'minplayers'\n                                        * play_time = 'maxplaytime' - 'minplaytime'\n                                        * set the max value of 'playingtime','play_time'\n\nThe last one is taking the maximum value between the newly play_time column we created and the existing playing-time column.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"joint_df['playes_num'] = joint_df['maxplayers'] - joint_df['minplayers']\njoint_df['play_time'] = joint_df['maxplaytime'] - joint_df['minplaytime']\njoint_df['Time'] = joint_df[['playingtime','play_time']].max(axis=1)\n\n#joint_df['Board Game Rank'] = joint_df['Board Game Rank'].apply(pd.to_numeric, errors='coerce')  # convert from object to numeric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just want to order the features for a better visualisation. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\njoint_reviews_renking_detailed = joint_df[['Name','Year','rating','numweights','usersrated',\n                                           'numcomments','user','comment', 'wanting', 'wishing', \n                                           'trading', 'owned','Time','playes_num','minage']]\njoint_reviews_renking_detailed.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for last time lets drop all the missing and dupliactes rows\njoint_reviews_renking_detailed = joint_reviews_renking_detailed.dropna(axis=0)\njoint_reviews_renking_detailed = joint_reviews_renking_detailed.drop_duplicates(subset=None, keep=\"first\", inplace=False)\njoint_reviews_renking_detailed.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFree our memory from unused data-frames using del keyword, this will help to reduce the allocated memory and speed the computation time. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del detailed_df\ndel reviews_df\ndel ranking_df\ndel ranking_detailed_df\ndel joint_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"Data_Exploration\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Data_Exploration\" style='color:blue'>Data Exploration</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nPresently, let's focus narrowly on the data we end up with. Using a heatmap can present the correlation between all the numerical features. The extremely dark spots in the heat map indicate a significant correlation between the features. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure1 = joint_reviews_renking_detailed.corr()\nfig =plt.figure (figsize=(9,7))\nsns.heatmap(figure1, vmax= .8 ,square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 7))\nsns.set(color_codes=\"True\")\nsns.distplot(joint_reviews_renking_detailed[\"rating\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joint_reviews_renking_detailed.rating.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nAn important notice from the above graph is the rating is a continuous variable later will convert it to integer and treat the rates as classes. \n\nMoreover, most of the games are rated between 6 to 8 where rate 7 represents the peak. This means we may challenge a skewed data representation or an imbalanced class representation.\n\nThis challenge would affect the machine learning model, where the less rating representation might consider as an outlier. We will see later that some of the models can deal with this issue, and others are not. However, our main focus is the model accuracy, so we will ignore this problem for now.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nNow let's group the data by the game name, user, and year. So we can have an idea of the number of games, the number of users who wrote comments.  The most rated game is Carcassonne. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rate_group = joint_reviews_renking_detailed.groupby(['rating'])\nname_group = joint_reviews_renking_detailed.groupby(['Name'])\nyear_group = joint_reviews_renking_detailed.groupby(['Year'])\nuser_group = joint_reviews_renking_detailed.groupby(['user'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(joint_reviews_renking_detailed['rating'].nunique())   \nprint(joint_reviews_renking_detailed['Name'].nunique()) \nprint(joint_reviews_renking_detailed['Year'].nunique()) \nprint(joint_reviews_renking_detailed['user'].nunique()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(joint_reviews_renking_detailed.rating.mode().value_counts())\nprint(joint_reviews_renking_detailed.Name.mode().value_counts())\nprint(joint_reviews_renking_detailed.Year.mode().value_counts())\nprint(joint_reviews_renking_detailed.user.mode().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the top 25 reviewed board games","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 7))\njoint_reviews_renking_detailed.loc[joint_reviews_renking_detailed['comment'].notna()]['Name'].value_counts()[:25].plot(kind='bar')\nplt.xlabel('Board Game Name')\nplt.ylabel('Review Count')\nplt.title('Top 25 Most Reviewed Board Games')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"Text_preprocessing\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Text_preprocessing\" style='color:blue'>Text preprocessing</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\nInstantly we are in our way to process the user reviews 'comment', dealing with a text is part of NLP Natural Language processing.\n\nNatural language processing NLP concerned by the interactions between computers and human languages. Where texts are required to be transferred from raw text to flat features, so machine learning methods can process and analyze. There are different techniques and many recommendations when working with text, it typically depends on the objective. \n\nRemember our goal is predicting the rate based on a given review. Therefore, to begin with, we have to transfer the text to tokens (Means split the text into separate words.) , second, we can do text cleaning as follows:\n\n* Remove the stop words(the highly frequent words like the, a, I, am, are, and so on) the reseason is the machine learning method will not learn significant information form these words, and doing so will appreciably reduce the computation time. \n* Remove the punctuations and all the tags, alter the text to lower case and so on \n* We can also drop all the words/vocabulary that appear rarely than five times in the whole reviews.\n* We also can see the most frequent words and indicate our new features, this what know as a Bag-of-Words(BoW).\n* An important point now the features will be in a shape of a matrix, this is mean that each word will represent a column and the value one will be assigned with corresponding to the review if the words are presents and zero otherwise, this is a basic technique to present the text as a feature and be able to train the model.\n\nFor all the above we perform the following three functions:\n\n* tokenize_remove_punctuations(): to split the text into tokens (words), also lower them to avoid double counting, remove the stop_words=[\"all\",\"in\",\"the\",\"is\",\"and\",\"it\",\"we\",\"to\",\"an\",\"of\"], and count terms occurrence.\n***\n* sort_words_freq(): To sort the tokens based on their occurrence \n***\n* delete_threshold() : to remove the least common words if the occurrence is less than 5 times \n***\n* vector_matrix_review(): this function will give us the matrix representation of each token across the reviews, which we can use for the required machine learning method.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport re \n\nstop_words = set(stopwords.words('english')) \nreplace_signs = re.compile('[/(){}\\[\\]\\|@,;]')    # Remove characteris from dict \nreplace_digit = re.compile('[^0-9a-z #+_]')       # Remove digits from dict\nwords = set(nltk.corpus.words.words())\n\n\n# credits goes to this blog \n# https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the text into tokens and count their frequencies\ndef tokenize_remove_punctuations(X):               \n    wordfreq = {}\n    for text in X:\n        text = ' '.join(word for word in text.split() if word not in stop_words \n                                         if not word.isdigit() if len(word)>1 )   #if not word.isalpha())\n        tokens = nltk.RegexpTokenizer(r\"\\w+\").tokenize(text.lower())                                                 # len(word)>2   \n        for token in tokens:                                    \n            if token not in wordfreq.keys():\n                wordfreq[token] = 1\n            else:\n                wordfreq[token] += 1\n                \n    return wordfreq \n\n\n\n# Sort words frequency in reverse order\ndef sort_words_freq(wordfreq):                        \n    wordfreq_sorted = dict(sorted(wordfreq.items(), key=lambda x: x[1], reverse=True))\n    return wordfreq_sorted\n \n    \n # ignore rare words if the occurrence is less than five times (delete keys with value less than 5)   \ndef delete_threshold(wordfreq_sorted):              \n    delete = []                                      \n    for key, val in wordfreq_sorted.items(): \n        if val < 5 : \n            delete.append(key) \n\n    for i in delete: \n        del wordfreq_sorted[i] \n    return   wordfreq_sorted\n    \n    \n#  BoW    \ndef vector_matrix_review(X):\n    sentence_vectors = []\n    for txt,i  in zip(X,y):   #txt,i in train_set.iterrows():\n        sentence_tokens = nltk.RegexpTokenizer(r\"\\w+\").tokenize(txt)\n        sent_vec = []\n        for token in wordfreq_sorted_X:   \n            if token in sentence_tokens:\n                sent_vec.append(1)\n            else:\n                sent_vec.append(0)\n        sentence_vectors.append([sent_vec,i])\n    return sentence_vectors    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tokenize_remove_punctuations(joint_reviews_renking_detailed['comment'])\nx = sort_words_freq(x)\nx = delete_threshold(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe can see we end up with 104873 words to use as features, These features are terms form the reviews.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nLet's print the top 50 words in the dictionary,\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntop50 = {k: x[k] for k in list(x)[:50]}\nprint(top50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Generate a word cloud. Looks we need to stemm the words to thier roots, will this later in this notebook. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\ntext = \" \".join(review for review in joint_reviews_renking_detailed['comment'].sample(10000))\n\n\n# Generate a word cloud\nwordcloud = WordCloud(stopwords=stop_words, background_color=\"white\").generate(text)\nplt.figure(figsize=(10, 8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"train_test_split\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Split out Training Set and Test Set ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Splitting the dataset into training and testing sets is one of the main steps for building a machine learning model. The training data with the labels will use to train the model. Then the test data without the labels will be used to predict and so we can evaluate the model accuracy and identify how the model will generalize on unseen data. These steps also will assist us to check if the model is overfitting. \n\nOverfitting (model with high variance) means that the model is unable to generalize well for new samples (unseen data). This situation happens will the model has a high variance and it trained very well on the training set but it would perform poorly on the test set (the unseen data).\n\nMathematically, it means the testing error is much higher than the training error. On the contrary, an underfitting (model with high bias) problem means that the model does not fit the new data well. This can be due to the low variance in the training set. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next we defined the input variable X (reviews) and the output variable y (Rating).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = joint_reviews_renking_detailed['comment'].sample(6000) #frac = 0.10 # this is less than 10% of the dataset \ny = joint_reviews_renking_detailed['rating'].astype(int).sample(6000) # convert all float to a intger for classification ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nTrain_test_split is a sklearn method to split the dataset into training and testing sets, there are diverse ways to split the amount of data between both. Usually, it is not a good practice to use all the data for training purpose; we have to hide some data to be able to evaluate the model generalization. We can try a different value for example 70 training/30 testing or 80 training/20 testing and that will actually help to measure how the model behaves regards to data size. But generally more data means better performance. \n\nFor this project, the amount of data is about 2637734 reviews x 104873 features. We might conduct a portion of the number of reviews, operate the model, and evaluate. In addition for more simplification we would take a sample from the dataset to train the model, and evalute the perfomavce of the baseline model based on different size of the data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset to train, test set \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoder = preprocessing.LabelEncoder()\n#y_train = encoder.fit_transform(y_train)\n#y_test = encoder.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"CountVectorizer\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CountVectorizer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Part of what we did on section [Text preprocessing](#Text_preprocessing)<a id = \"Text_preprocessing\"></a> simply can be done using built in SKlearn methods two of theme are: CountVectorizer and TfidfVectorizer. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\nCountVectorizer convert our text documents to a matrix of token counts, every column represent a term, every row obtain a review and the cell is the term frequency count. As a parameter for this function, we sat the minimum occurrence of words to five and convert all terms to lowercase before tokenizing so for example 'TEXT' and 'text' counts as the same.\nAdd more the parameter ngram_range=(1, 2) , this is means consider both unigrams and bigrams, count the occurrences of a single term as a minimum and count the maximum occurrences of two pairs of consecutive words for example 'good game', removed the stop words and punctuation. Finally, fitting the text will build a dictionary of features and then transforms review to feature vectors. One last thing we can add is Lemmatize the words to their root to get out of the redundancy.  [https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html]\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Deduct inflected words to their word root,\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nclass LemmaTokenizer:\n    def __init__(self):\n         self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n\n# credits to [https://scikit-learn.org/stable/modules/feature_extraction.html] \n# CountVectorizer(tokenizer=LemmaTokenizer(),\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# convert our text documents to a matrix of token counts\nvect = CountVectorizer(min_df=5,lowercase=True,ngram_range=(1, 2),\n                       stop_words=stopwords.words('english') + list(string.punctuation))                     \nX_train_0 = vect.fit_transform(X_train)   # fit and transfom train data\nX_test_0 = vect.transform(X_test)         # transfrom test data\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"TfidfVectorizer\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TfidfVectorizer (Term Frequency times Inverse Document Frequency)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nTfidf is able to overcome the problem of balancing between the short and the lengthy reviews in terms of the average count values, where they may have the same opinion but different average. Tfidf is a two-part function, Tf (Term frequency) is the normalization of the occurrence of each word in all reviews by the number of total words. Then idf (Inverse Document Frequency) means downscale the weights of such common words in the reviews, the reductoin will reflect the importance of the less common words across all the reviews as well. \n\n    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n    \n    IDF(t) = log_e(Total number of documents / Number of documents with term t in it) [1](#1)\n\n\nThe same parameter were passed in CountVectorizer is passed in the TfidfVectorizer, except for the max_df which means ignore terms that appear in more than 75% of the documents. These cut will also help us to speed the compuation time.[https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html]\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.75,\n                        smooth_idf=True, stop_words=stopwords.words('english') + list(string.punctuation))    \n             # smoothing by adding 1 (Laplac) min_df occure less than 5 , max_df=0.5 occure more than half \n\nX_train_1 = tfidf.fit_transform(X_train)\n\nX_test_1 = tfidf.transform(X_test)      \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_1.shape[0]  # Number of featuers to train with","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"Model_Selectoin\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Model_Selectoin\" style='color:blue'>Model Selectoin</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nTo Begin with, we are consider this problem as a multinomial classification problem. Machine learning methods can divide as supervised, unsupervised, and reinforcement learning. The models we are implementing are supervised machine learning. Supervised machine learning can work for classification or regression problems. Classification is when the output is a categorical variable, regression for continuous output. \n\nIn this section, we will try using different methods to be able to select which machine learning method is more promising for our problem. The selection is based on model accuracy.\n\nThere are many machine learning methods some of them are recommendable for text classification applications so we have a variety of choices we will introduce and apply some of them in our approach to find the most promising model based on the model accuracy.\n\nAfter we processed the reviews (features vector) our data are ready to be fitted in the Machine learning algorithms to predict the rating. \n\n\nFirst of all, we started by building a classifiers/Regressor for:\n* [Logistic Regression](#Linear_and_Logistic_Regression) \n* [Support Vector Machine (LinearSVC)](#Support_Vector_Machine) \n* [Linear Support Vector Machine (SGDClassifier)](#Linear_Support_Vector_Machine)\n* [Naive Bayes Classifier (MultinomialNB)](#Naive_Bayes_Classifier)\n* [Decision Tree Classifier](#Decision_Tree_Classifier)\n* [Ensemble Model](#Ensemble_Model)\n    * [Bagging Models](#Bagging_Models)\n    * [Random Forest Classifier](#Random_Forest_Classifier) \n* [Recurrent Neural Network  (Futuer Work)](#Recurrent_Neural_Network)  \n\n\n\nThe main terms here are: \n* Training: fit the model\n* Evaluation: calculat the model accuracy on the training dataset\n* Prediction: predict the outpu and evaluate the model on the testset\n\nThe steps for this section is: fit the model, calculat the accuracy of the model on the train dataset, then evaluate the model on the testset. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"Linear_and_Logistic_Regression\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear and Logistic Regression:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nLogistic regression is a simple and powerful method for modelling and estimating the relationship between the input (independent) variable X (the predictor) and an output (dependent) variable Y (target/lable). It is commnly used as baseline model for any binary/multinomial/ordinal classification  problem. Linear regression output is continuous, while logistic regression output is discrete.\n\nThe value of the paramters are defult , ‘lbfgs’ solvers support only L2 regularization and it is the defult kernel for the new version 0.22.[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html] \n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id =\"Linear_Classifiers\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear Classifiers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nLinear classifiers (SVM, logistic regression, a.o.) with SGD training.\n\nThis method combines the regularized linear models with stochastic gradient descent (SGD) learning. This method usually works well with the sparse array. The sparse array is when the vector has many zeros where this case is shown in the bag of words model. This estimator fits a linear support vector machine (SVM).\nTo understand what is regularized model we need to understand what is regularizer. \"Regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net)\" [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier].\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Support_Vector_Machine\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nSupport Vector Machine is considered as a simple and powerful algorithm. It produces significant accuracy with less computation power. It can be used for regression and classification problems. The algorithm defines a hyperplane in an N-dimensional space between the support vectors data point to classify the data point. The objective is to find a hyperplane that maximizes the margin between the data points from each class. \n\nIt might not perform well for text classification because it is unable to handle missing data, where we are going to implement this method LinearSVC() it can handle sparse and multiclass data point by train each class against the rest. \n\nRegularization parameter. C=1.0 [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Naive_Bayes_Classifier\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes classifier is commonly used for the analysis of categorical data, specifically text data.\nWorked based on the Bayes' theorem probability for each class, it assumes the conditional independence between features. It calculates the conditional probability for each class to be able to predict. An example of the computation look is the following. \nP(y |x1,x2,...,xn) = P (x1|y) * P(x2|y) * ... * P(xn|y) * P(y) \n\nWe are using MultinomialNB(), this method from SKlearn for multiclass. [https://scikit-learn.org/stable/modules/naive_bayes.html]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"Decision_Tree_Classifier\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nThe decision tree is one of the basic knowledge representations used in different machine learning applications. It is a divide-and-conquer algorithm that maps the input features of an item to predict the item’s target value. The tree can grow deeply to learn highly irregular patterns[https://www-users.cs.umn.edu/~kumar001/dmbook/ch4.pdf].\nGenerally, there are three types of nodes for the whole tree: root node, inner root node and leaf node. The root node and inner node are both called decision nodes. They test selected samples (pairs of features and values) to split the tree. However, only the samples with a lower error are selected for the testing process and belong to the decision nodes. The leaf nodes contain an averaged numeric value or class as the final decision. This method stops when the tree reaches the maximum depth forced by the user. The disadvantage of this method are: its high variance may lead to an overfitting when applied to a full data set, also it does not perform will the missing data.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"Ensemble_Model\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Learning ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nThe ensemble learning principle combines the strengths of several learning models such as a decision tree, support vector machine, polynomial regression and so on. The goal is to improve the prediction accuracy and overcome the problem of overfitting.\nThe combination of different types of base models have two approaches. The combination can be based on heterogeneous models with different algorithms or homogeneous models with the same algorithm. At the end, the ensemble produces an averaging result from all trained base models. This method is powerful for a scenario when we do not know the suitable learning method to use[https://scikit-learn.org/stable/modules/ensemble.html].\n\nThere are many ensemble methods, some the most common are the Bagging trees, the Boosting trees and random forest approaches. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"Boosting_Models\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Boosting Trees","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n\nBoosting Trees.\nThe boosting tree is a meta-estimator that combines all the prediction models to produce a strong predictor. It was originally designed for classification problems, but it can be extended to regression problems. It works by first creating weak learners. All successive learners are weighted according to their success and the outputs are aggregated by average (for regression) and vote (for classification) the final prediction.\nTwo of popular boosting methods are the AdaBoost algorithm, introduced in 1995 by Freund and Schapire and Gradient Boosted Regression Trees (GBRT)[https://scikit-learn.org/stable/modules/ensemble.html].\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"Random_Forest_Classifier\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nIs an ensemble of decision trees with controlled variables for better prediction. It is a combination of bagging and a random selection of features. The random subspace method creates a random subset of the feature space to train the algorithm and then combine the outputs for the final result. \nThe random forest uses multiple random trees for the given set of inputs then averages the prediction result of all these trees to provide a good estimation. It gives an algorithm the ability to train each tree using a selected subset from the training data called in-bag-samples. The remaining training data set (out-of-bag) is used to estimate error and variable importance for an individual’s tree, as well as for the whole forest. This is known as the validation process. The two key parameters that control the forest are the number of trees or estimators and the depth of each tree. Random forest did not suffer from the overfitting problem. \n\n\n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# X_train_0 apply CountVectorizer\n\nclassifiers = [\n    linear_model.LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=1000), # to handle the FutureWarning\n    svm.LinearSVC(),   \n    SGDClassifier(), #(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=1000, tol=None),\n    MultinomialNB(),\n    DecisionTreeClassifier(),\n    ensemble.AdaBoostClassifier(),\n    ensemble.RandomForestClassifier(n_estimators = 100)\n]\n\n\n\nfor item in classifiers:\n    print(item)\n    model = item\n    model.fit(X_train_0, y_train)\n    \n    y_train_pred = model.predict(X_train_0)\n    print(\"training Score = \",model.score(X_train_0,y_train))   #training score\n    print(\"Train Mean Squer Error =  \", mean_squared_error(y_train_pred.astype(int), y_train))\n    \n    y_pred = model.predict(X_test_0)\n    print(\"test Score = \", accuracy_score(y_pred.astype(int), y_test))    #test score\n    print(\"Test Mean Squer Error =  \", mean_squared_error(y_pred.astype(int), y_test),\"\\n\")\n       \n    \n    plt.figure(figsize=(10, 8))\n    sns.set(color_codes=\"True\")\n    sns.distplot(y_test.values[:15], color='b', label= 'Actual', kde_kws={'bw':1.1})   # its shows only the first 15 data point\n    sns.distplot(y_pred[:15], color='orange', label='Predicted', kde_kws={'bw':1.1})\n    plt.legend()\n    plt.show() \n    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the above methods during out trail we used different amount od data between way less than 10% to 25%. The compuation takes a very long time by incerasing the size of the data set. But what we examind that the perfomance is getting slightly better for some methods I mean by that: \n(superizly the  LogisticRegression, and Linear support vector classifier, DecisionTreeClassifier and finally RandomForestClassifier)\n\n|    | Logistic Regression | Linear support vector classifier | Decision Tree Classifier | RandomForestClassifier |\n|:--:|:---------------------:|:---------------------:       |:---------------------:   |:---------------------: |\n| Training Score |   0.786   |         0.832                |        0.948              |        0.948            | \n| Test Score     |   0.205   |         0.190                |        0.186              |        0.23            | \n\n\nOn the other hand, Multinomial Niave Bayes and AdaBoostClassifier had a very poor perfomamce for the traning set,\nbut slightly better than others in the test set. \n\n|    | Multinomial Niave Bayes |  Stochastic Gradient Descent | AdaBoostClassifier       | \n|:--:|:---------------------:  |:---------------------:       |:---------------------:   |\n| Training Score |   0.547     |         0.547                |        0.265             | \n| Test Score     |   0.220     |         0.199                |        0.267             | \n\n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# X_train_1  apply TfidfVectorizer\n\nclassifiers = [\n    linear_model.LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=1000), # to handle the FutureWarning\n    svm.LinearSVC(),   \n    SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=1000, tol=None),\n    MultinomialNB(),\n    DecisionTreeClassifier(),\n    ensemble.AdaBoostClassifier(),\n    ensemble.RandomForestClassifier(n_estimators = 100)\n]\n\n\n\nfor item in classifiers:\n    print(item)\n    model = item\n    model.fit(X_train_1, y_train)\n    \n    y_train_pred = model.predict(X_train_1)\n    print(\"training Score = \",model.score(X_train_1,y_train))   #training score\n    print(\"Train Mean Squer Error =  \", mean_squared_error(y_train_pred.astype(int), y_train))\n    \n    y_pred = model.predict(X_test_1)\n    print(\"test Score = \", accuracy_score(y_pred.astype(int), y_test))    #test score\n    print(\"Test Mean Squer Error =  \", mean_squared_error(y_pred.astype(int), y_test), \"\\n\")\n    \n    \n    plt.figure(figsize=(10, 8))\n    sns.set(color_codes=\"True\")\n    sns.distplot(y_test.values[:15], color='b', label= 'Actual', kde_kws={'bw':1.1})\n    sns.distplot(y_pred[:15], color='orange', label='Predicted', kde_kws={'bw':1.1})\n    plt.legend()\n    plt.show() \n    \n    \n    \n    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|    | Logistic Regression | Linear support vector classifier | Decision Tree Classifier | RandomForestClassifier |\n|:--:|:---------------------:|:---------------------:       |:---------------------:   |:---------------------: |\n| Training Score |   0.735   |         0.807                |        0.948             |        0.948            | \n| Test Score     |   0.212   |         0.200                |        0.192              |        0.338            | \n\n***\n\n|    | Multinomial Niave Bayes |  Stochastic Gradient Descent | AdaBoostClassifier       | \n|:--:|:---------------------:  |:---------------------:       |:---------------------:   |\n| Training Score |   0.509     |         0.793                |        0.265             | \n| Test Score     |   0.228     |         0.186                |        0.268             | \n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's Have a closer look to the numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"y_test=\" ,y_test.values[:15] ,\"y_pred=\", y_pred[:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict the first review as an example ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(joint_reviews_renking_detailed['comment'][0],\"Actual rating is\", joint_reviews_renking_detailed['rating'][0])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\n# you can try diffrent text to perdict \n\nmodel = ensemble.RandomForestClassifier(n_estimators = 100)\nmodel.fit(X_train_1, y_train)\ny_pred = model.predict(X_test_1)    \n    \ny_pred1 = model.predict(tfidf.transform([\"Currently, this sits on my list as my favorite game\"]))\nprint(\"test Score = \",y_pred1.astype(int))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSGDClassifier, DecisionTreeClassifier and RandomForestClassifier are the methods had a good training score but a very poor test score. Therefore, we realize that our models is suffering from overfitting problem. \n\nTo overcome the overfitting problem there are techniques we can try:\n* Cross-validation\n* Train model with more data\n* Remove features to reduce model complexity\n* Regularization, minimzes this loss function to balance between the error and model complexity.\n* Ensembles are machine learning methods for combining predictions from multiple separate models, this one we already included in our classifiers set.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id = \"hyperparameter_Tuning\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"hyperparameter_Tuning\" style='color:blue'>hyperparameter Tuning and Model Optimization</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nSince <i>Random Forest Classifier</i> was the best model performance in approach 1 with the training score equals to 90% and test score 36%; which is not good engouh, we will try to improve the model accuracy. In the following, we will introduce model evaluation measurements.\n\n\nHyperparameter tuning is a model optimization technique that aims at setting the hyperparameter values for a learning method for the best performance. Examples of hyperparameters are the number of iterations or the number of decision trees or forest, as well as, the tree depth. One hyperparameter can on top of that be the learning rate which determines the contribution of each tree to a prediction.\nThe process of finding the best value is called search space. Basically, this is a method to test all possible values and parameter combinations to set the best parameter values. The latter parameters are evaluated using the validation set after the model has been trained and tested. A <i>grid search</i> is example of this technique. Furthermore, it can work with the <i>K-fold cross validation</i>.\nUsually, when the model accuracy is too close to be random, it means something went wrong like the presence of useless features or of a hyperparameter that is not correctly tuned.\n\nMore iomprantalty, the best hyperparameters are usually impossible to determine with one trial, at this stage the work turn to be human role.[https://www.kaggle.com/emanueleamcappella/random-forest-hyperparameters-tuning]   \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Model validation procedure is to select from the candidate models based on their scores values. To chose the most successful model for further enhancement. On of the validation method is shuffle split cross validation, where it gave us a better score than the 10-fold. We set three as a number of the iteration to split and train the model. In addition, we used shuffle split cross validation with a search grid to tune the hyperprameter parameters. Within that we present the model feature importance. Finally, we plot the learning curve to check the model generalization.\nNext section we will disccuse the Parameter Tuning and  the Model accracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Training Visualizations and Model Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nContinue with our best model <i>RandomForestClassifier</i>, we are going to look at the confusion matrix, and show the discrepancies between predicted and actual labels. The confusion matrix is a great way to see which categories model is mixing.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion matrix \\n\", confusion_matrix( y_test, np.array(y_pred))) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are some labels almost are not mentioned 1,2,3,4,5 and 10. Non of these appaer in the prediaction result. Becuase, the model is not good enough, the data is imbalanced and the sample size we took is small. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"GridSearchCV\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Grid Search Cross Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nIn this grid search I will try different combinations of RF hyperparameters. Most important hyperparameters of Random Forest:\n\nn_estimators = n of trees\nmax_features = max number of features considered for splitting a node\nmax_depth = max number of levels in each decision tree\nmin_samples_split = min number of data points placed in a node before the node is split\nmin_samples_leaf = min number of data points allowed in a leaf node\nbootstrap = method for sampling data points (with or without replacement)\n\nAs for how I decided the numbers to try I simply followed the advice of Aurelion Geron (2017): 'When you have no idea what value a hyperparameter should have, a simple approach is to try out consecutive powers of 10 (or a smaller number if you want a more fine-grained search)' [].\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Parameter estimation using grid search with cross-validation\n\nX = joint_reviews_renking_detailed['comment'].sample(6000) # this is 10% of the dataset \ny = joint_reviews_renking_detailed['rating'].astype(int).sample(6000) # convert all float to a intger for classification \n#split for 30% test set, 70% train set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.75, smooth_idf=True, \n                    stop_words=stopwords.words('english') + list(string.punctuation))    \n                    \nX_trainT = tfidf.fit_transform(X_train)\nX_testT = tfidf.transform(X_test)  \n\n\nfrom sklearn.model_selection import GridSearchCV\n\ntuned_parameters = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\n    \nrf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10, scoring='neg_mean_squared_error')\nrf.fit(X_trainT, y_train)\n\n#best_estimator_ returns the best estimator chosen by the search\n#print(rf.best_estimator_)\nprint()\nprint(\"best_parameters\", rf.best_params_)\n\nmeans = rf.cv_results_['mean_test_score'][rf.best_index_]\nstds = rf.cv_results_['std_test_score'][rf.best_index_]\n#params = rf.cv_results_['params']\n#print(\"mean , std )\" % (mean, std))\nprint()\n\ny_true, y_pred = y_test, rf.predict(X_testT)\nprint(classification_report(y_true, y_pred.astype(int)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is still low, ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Mean squared error (MSE )","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Mean squared error measures the average of the squared error between the actual and predicted value.\nThe closer value to zero the more efficient estimator we have. The value of 2.95 indicates the distance between the actual rating and the predicted rating which still not good enough. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngrid_best= rf.best_estimator_.predict(X_testT)\nMSE = np.square(np.subtract(y_test,y_pred)).mean()   # calculate Mean squared error\nprint('The model error is', round(MSE, 2),'%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Absolute Percentage Error (MAPE)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"MAPE is another performance metric similar to MSE, but this one represents percentages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngrid_best= rf.best_estimator_.predict(X_testT)\nerrors = abs(grid_best - y_test)\nmape = np.mean(100 * (errors / y_test))\naccuracy = 100 - mape    \nprint('The final accuracy of', round(accuracy, 2),'%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id =\"RandomizedSearchCV\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Randomized Search Cross Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nThe grid search approach is often too costly, as many combinations are tested. In these cases it is easier to use a randomized search, that evaluates a only an user defined number of random combinations for each hyperparameter at every iteration. This way we could also test more hyperparameters. See this tutorial and this resource, with its notebook.\n\n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Parameter estimation using Randomized Search with cross-validation\n\n\nX = joint_reviews_renking_detailed['comment'].sample(6000) # this is 10% of the dataset \ny = joint_reviews_renking_detailed['rating'].astype(int).sample(6000) #frac = 0.7\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.75, smooth_idf=True, \n                    stop_words=stopwords.words('english') + list(string.punctuation))    \n                    \nX_trainTT = tfidf.fit_transform(X_train)\nX_testTT = tfidf.transform(X_test)  \n\n\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\n\n\nrf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), \n                               param_distributions = random_grid, n_iter = 5, cv = 10, \n                               verbose=2, random_state=42, scoring='neg_mean_squared_error')\n# Fit the random search model\nrf_random.fit(X_trainTT, y_train)\nrandom_best= rf_random.predict(X_trainTT)\n\n\n# best random model \nprint(rf_random.best_estimator_)\n\n# best combination of parameters of random search\nprint(rf_random.best_params_)\n\n\n#this is the RMSE\nfinal_mse = mean_squared_error(y_train, random_best)\nfinal_rmse = np.sqrt(final_mse)\nprint('The best model on training set has a RMSE of', round(final_rmse, 2))\n\n    \n# Credits goes to     \n#https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n#https://www.kaggle.com/emanueleamcappella/random-forest-hyperparameters-tuning  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#import scipy\n#scipy.test() clean memory ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = rf_random.best_estimator_\n# Predicting test set results\nfinal_pred = final_model.predict(X_testTT)\nfinal_mse = mean_squared_error(y_test, final_pred)\nfinal_rmse = np.sqrt(final_mse)\nprint('The final RMSE on the test set is', round(final_rmse, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate accuracy\nerrors = abs(final_pred - y_test)\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors / y_test))\n# Calculate and display accuracy\naccuracy = 100 - mape    \n#print result\nprint('The final model accuracy on the test set is', round(accuracy, 2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Results\" style='color:blue'>Results and Futuer Work</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As a resut we we can say that a good text classifier is depends in many factors, the size of data we are using, the featuer enginnering and text cleaning and selecting an apporobpate machine learning method. \n\nWe found that using TfidfVectorizer is more efficatve than using only CounterVectorizer. The most promsing machine learing methods in this project are : \n* Linear SVC \n* SGD Classifier\n* Decision Tree Classifier\n* Random Forest Classifier\n\nBut as we can see from the number our model suffred from overfitting, where it had a poor testing score. \nStill there is a room for imporvment. \n\nThen we applied the Randomized Search Cross Validation optimxzation and the finall accuracy is ~70% to 72.58% on the testset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the future work we would like to consider the following: \n* Implement Neural Network model \n* Use regression methods \n* Use the other features as a supplement with the text\n* Use sampleing based approaches ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"Challenges\" style='color:blue'>Challenges</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nWe experienced many challenges during this work, usually preparing the data time the most time, then working to improve the model accuracy and tuning the model parameters. The challenged we faced are:\n\n* The balance between the computational speed limitation we have and the amount of data to process\n* The Changed in version 0.20 from version 0.22\n* Understanding all the data columns and processing them in our way to determine the target variable and understand the problem better.\n* Determine if the problem is a classification or regression. \n* Selecting the model to fit the data and discover a suitable version for text classification. \n* Reduce the number of features to fit the model \n* Trying to improving the final accuracy cost too much time\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"References\" style='color:blue'>References</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* https://www.kaggle.com/ngrq94/boardgamegeek-reviews-data-preparation\n \n* https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n \n* https://medium.com/@galen.ballew/board-games-meet-machine-learning-34026870f8d5\n \n* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n\n* https://towardsdatascience.com/columntransformer-meets-natural-language-processing-da1f116dd69f   \n\n* https://www.mooc-list.com/course/build-board-game-predictor-using-machine-learning-eduonix\n \n* https://monkeylearn.com/text-classification/\n\n* https://www.kaggle.com/emanueleamcappella/random-forest-hyperparameters-tuning\n\n* https://guneetkohli.github.io/machine-learning/board-game-reviews/#.XqK_ky-z1QI\n\n* https://docs.google.com/viewer?a=v&pid=sites&srcid=YWltcy5hYy50enxhaW1zLXRhbnphbmlhLWFyY2hpdmV8Z3g6YmI1ODkxYjEwOGQ4NGVi\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}