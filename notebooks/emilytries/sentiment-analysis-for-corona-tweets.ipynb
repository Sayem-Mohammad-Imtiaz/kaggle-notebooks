{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import Word, TextBlob\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\npd.set_option('display.width', 200)\npd.set_option(\"display.max_colwidth\", -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding='latin1')\ndf = test.append(train).reset_index(drop=True)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ = df[[\"OriginalTweet\", \"Sentiment\"]]\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1 Text Preprocesing","metadata":{}},{"cell_type":"markdown","source":"Lower - upper case","metadata":{}},{"cell_type":"code","source":"df_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping punctuation marks","metadata":{}},{"cell_type":"code","source":"df_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].str.replace('[^\\w\\s]', '')\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping numbers","metadata":{}},{"cell_type":"code","source":"df_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].str.replace('\\d', '')\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stopwords","metadata":{}},{"cell_type":"code","source":"sw=stopwords.words('english')\ndf_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rare Words","metadata":{}},{"cell_type":"code","source":"dropping=pd.Series(' '.join(df_['OriginalTweet']).split()).value_counts()[-1000:]\ndf_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in dropping))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lemmatization","metadata":{}},{"cell_type":"code","source":"df_[\"OriginalTweet\"]=df_[\"OriginalTweet\"].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\ndf_.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. DATA VISUALIZATION","metadata":{}},{"cell_type":"markdown","source":"Word Cloud","metadata":{}},{"cell_type":"code","source":"text = \" \".join(i for i in df_.OriginalTweet)\nwordcloud = WordCloud(max_font_size=50,\n                      max_words=300,\n                      background_color=\"white\",\n                     width=1600, height=800).generate(text)\nplt.figure(figsize=(20,10),facecolor='k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. SENTIMENT ANALYSIS","metadata":{}},{"cell_type":"code","source":"sia = SentimentIntensityAnalyzer()\ndf_[\"sentiment_label\"] = df_[\"OriginalTweet\"].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\ndf_.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"* Count Vectors\n* TF - IDF Vectors (words, characters, n-grams)\n* Word Embeddings","metadata":{}},{"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(df_[\"OriginalTweet\"],\n                                                    df_[\"sentiment_label\"],\n                                                    random_state=17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Count Vectors","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nvectorizer.fit(train_x)\nx_train_count = vectorizer.transform(train_x)\nx_test_count = vectorizer.transform(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Word Level","metadata":{}},{"cell_type":"code","source":"tf_idf_word_vectorizer = TfidfVectorizer().fit(train_x)\nx_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x)\nx_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF N-Gram Level","metadata":{}},{"cell_type":"code","source":"tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3)).fit(train_x)\nx_train_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(train_x)\nx_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Characters Level","metadata":{}},{"cell_type":"code","source":"tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3)).fit(train_x)\nx_train_tf_idf_chars = tf_idf_chars_vectorizer.transform(train_x)\nx_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"# TF-IDF Word-Level\nrf_model = RandomForestClassifier().fit(x_train_tf_idf_word, train_y)\ncross_val_score(rf_model, x_test_tf_idf_word, test_y, cv=5, n_jobs=-1).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF N-GRAM\nrf_model = RandomForestClassifier().fit(x_train_tf_idf_ngram, train_y)\ncross_val_score(rf_model, x_test_tf_idf_ngram, test_y, cv=5, n_jobs=-1).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF CHARLEVEL\nrf_model = RandomForestClassifier().fit(x_train_tf_idf_chars, train_y)\ncross_val_score(rf_model, x_test_tf_idf_chars, test_y, cv=5, n_jobs=-1).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count Vectors\nrf_model = RandomForestClassifier().fit(x_train_count, train_y)\ncross_val_score(rf_model, x_test_count, test_y, cv=5).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example","metadata":{}},{"cell_type":"code","source":"new_comment = pd.Series(\"I really need toilet paper\")\nnew_comment = CountVectorizer().fit(train_x).transform(new_comment)\n\nrf_model.predict(new_comment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_comment = pd.Series(\"Good\")\nnew_comment = CountVectorizer().fit(train_x).transform(new_comment)\n\nrf_model.predict(new_comment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}