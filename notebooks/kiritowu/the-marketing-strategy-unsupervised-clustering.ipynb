{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mall Customer Segmentation Clustering\r\n### Discovering the Hidden Chest\r\n\r\nAuthor : Wong Zhao Wu, Bryan\r\n\r\n# Modelling Objective\r\n- Achieve customer segmentation using unsupervised machine learning algorithms.\r\n- Identify the target customers and devise a marketing strategy to boost the growth of the mall supermarket.\r\n\r\n## Keywords\r\n- Unsupervised Learning\r\n- K-Means Clustering\r\n- Silhouette Analysis\r\n- Hierarchical Clustering\r\n- Spectral Clustering\r\n- Clusters Interpretation","metadata":{}},{"cell_type":"code","source":"from typing import Sequence, Tuple\r\nfrom itertools import combinations, chain\r\nimport numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.cm as cm\r\nimport sklearn\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\r\nfrom scipy.cluster.hierarchy import dendrogram\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.metrics import silhouette_samples, silhouette_score\r\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:41.613792Z","iopub.execute_input":"2021-08-19T07:55:41.614192Z","iopub.status.idle":"2021-08-19T07:55:41.620851Z","shell.execute_reply.started":"2021-08-19T07:55:41.614155Z","shell.execute_reply":"2021-08-19T07:55:41.619933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Dataset\nThe [Mall Customer](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python) dataset is a simple & clean example with 4 attributes and 200 records. The ultimate task of this dataset is to perform clustering and devise a feasible strategy to increase the mall's profit.\n## Data Dictionary\n| Columns | Descriptions |\n| :--     | :--          |\n| CustomerId | Unique ID assigned to the customer |\n| Gender | Gender of the customer |\n| Age | Age of the customer |\n| Annual Income (k$) | Annual Income of the customer |\n| Spending Score (1-100) | Score assigned by the mall based on customer behavior and spending nature |\n","metadata":{}},{"cell_type":"code","source":"cust_df = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\").rename(columns={'Genre':'Gender'}) #Rename to Gender for Genre Column\r\ncust_df.Gender = cust_df.Gender.astype('category') # Turns Gender to category dtype\r\ncust_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:41.62732Z","iopub.execute_input":"2021-08-19T07:55:41.627693Z","iopub.status.idle":"2021-08-19T07:55:41.657796Z","shell.execute_reply.started":"2021-08-19T07:55:41.627646Z","shell.execute_reply":"2021-08-19T07:55:41.656759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cust_df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:41.659284Z","iopub.execute_input":"2021-08-19T07:55:41.659596Z","iopub.status.idle":"2021-08-19T07:55:41.687886Z","shell.execute_reply.started":"2021-08-19T07:55:41.659561Z","shell.execute_reply":"2021-08-19T07:55:41.687174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\nA good data science project starts from understanding your data. With that, I will perform some basic visualisation to spot some trends and errors in my dataset.\n\n## Pairplot\nSince our feature space is not extremely large, we can afford to plot out a **Pairplot** to visualise the distribution of datapoints, categorised by the gender with the hope of answering the following questions:\n\n1. Is there an **identifiable cluster** by glancing through the datapoints?\n2. Is there a **significant difference** of the Spending Behaviour **across different gender**?\n3. Is there a need to **remove/create certain features** to reduce ambiguity for the clustering?\n ","metadata":{}},{"cell_type":"code","source":"sns.pairplot(cust_df.drop(columns='CustomerID'), hue = 'Gender', palette=['b', 'r'], plot_kws=dict(alpha=0.4))\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:41.689251Z","iopub.execute_input":"2021-08-19T07:55:41.689676Z","iopub.status.idle":"2021-08-19T07:55:44.530657Z","shell.execute_reply.started":"2021-08-19T07:55:41.689635Z","shell.execute_reply":"2021-08-19T07:55:44.529478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n1. By just comparing `Annual Income (k$)` and `Spending Score (1-100)`, there **seems to be 5 identifiable clusters**.\n2. Female seems to have higher mean `Spending Score (1-100)` as compared to male. However, by observing the scatterplots, the distribution of male and female datapoints seems to be **randomly scattered across different measures**. Hence, it is arguable that there *might not be a significant different of spending habits across Male and Female*.\n3. There are **no identifiable clusters wrt to `Age` attribute**, which might lead to ambiguity for the clustering algorithm. However, further analysis is required to unveil the relationship of `Age` with other attributes.","metadata":{}},{"cell_type":"markdown","source":"## Correlation Plot\nFrom the correlation plot, there seems to be weak linear relationship observed between `Age` and `Spending Score (1-100)`, implying that there might be something going on for `Age` that we have yet to understand. As for the rest of features, the sample correlation value is relatively small and close to zero.\n\nSince our objective is to perform clustering, which is not badly affected by collinearity/correlations, we can just leave all attributes as it is and move on to the next step.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(cust_df.drop(columns='CustomerID').corr(), annot= True, cmap='Spectral')\r\nplt.title(\"Correlation Matrix\")\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.532526Z","iopub.execute_input":"2021-08-19T07:55:44.533124Z","iopub.status.idle":"2021-08-19T07:55:44.82922Z","shell.execute_reply.started":"2021-08-19T07:55:44.533082Z","shell.execute_reply":"2021-08-19T07:55:44.828366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\nThere are a few preprocessing steps that we shall perform, before proceeding to the actual clutering, which include:\n\n1. **Dropping `CustomerID`** column *since it does not review any useful information*.\n2. **Standard Scaling** of the datapoints *to ensure all attributes are in the same scale*.\n3. Create **Dichotomous Variable** for `Gender` column *since our model cannot interpret categorical values as it is*.","metadata":{}},{"cell_type":"markdown","source":"## 1. Dropping CustomerID","metadata":{}},{"cell_type":"code","source":"cust_df.drop(columns='CustomerID', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.830239Z","iopub.execute_input":"2021-08-19T07:55:44.830647Z","iopub.status.idle":"2021-08-19T07:55:44.835476Z","shell.execute_reply.started":"2021-08-19T07:55:44.83061Z","shell.execute_reply":"2021-08-19T07:55:44.834754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Standard Scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\r\ncust_df_scale = cust_df.copy()\r\ncust_df_scale[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']] = scaler.fit_transform(cust_df_scale[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])\r\ncust_df_scale.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.836577Z","iopub.execute_input":"2021-08-19T07:55:44.837047Z","iopub.status.idle":"2021-08-19T07:55:44.867352Z","shell.execute_reply.started":"2021-08-19T07:55:44.837015Z","shell.execute_reply":"2021-08-19T07:55:44.866463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Dichotomous Variable","metadata":{}},{"cell_type":"code","source":"cust_df_onehot = pd.get_dummies(cust_df_scale, drop_first=True)\r\ncust_df_onehot.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.868514Z","iopub.execute_input":"2021-08-19T07:55:44.869006Z","iopub.status.idle":"2021-08-19T07:55:44.89413Z","shell.execute_reply.started":"2021-08-19T07:55:44.868961Z","shell.execute_reply":"2021-08-19T07:55:44.893054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering\nAfter we are done with our data, we will then proceed with the main dish of the day, performing some clustering with some commonly used clustering algorithms.\n\nI will tryout algorithms from different family and different approach to compare and contrast and find out the algorithm that can give us the most distinguishable clustering with an interpretable result.","metadata":{}},{"cell_type":"markdown","source":"## K-Means Clustering (Centroid-based)\nK-Means Clustering is one of the most famous clustering algorithms that assign clusters based on the distance towards the cluster centroids. I will start by defining a utility function that will return fitted kmean model and some other metrics to save us a few line of code during parameter tuning.","metadata":{}},{"cell_type":"code","source":"def calculate_k_mean(n_cluster: int, X: Sequence)-> Tuple[float, Sequence, Sequence, sklearn.cluster._kmeans.KMeans]:\r\n    '''\r\n    General Function to returns commonly used metrics for K-Means Clustering and the fitted instance\r\n    '''\r\n    kmean = KMeans(n_clusters = n_cluster, random_state=24)\r\n    cluster_labels = kmean.fit_predict(X)\r\n    return kmean.inertia_, cluster_labels, kmean.cluster_centers_, kmean\r\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.897308Z","iopub.execute_input":"2021-08-19T07:55:44.897962Z","iopub.status.idle":"2021-08-19T07:55:44.908776Z","shell.execute_reply.started":"2021-08-19T07:55:44.897913Z","shell.execute_reply":"2021-08-19T07:55:44.907638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choosing Number of $k$\nI will be using two main approach to evaluate the quality of the clusters formed with K-Means of different $k$ values.","metadata":{}},{"cell_type":"code","source":"log = [] \r\nsilhoettes = True\r\nk_range = range(2,9) # Range of k values\r\n\r\nfor k in k_range:\r\n    inertia, cluster_labels, _, _ = calculate_k_mean(k, cust_df_onehot) # Fitting the model\r\n    if silhoettes: # Generate Silhoettes Score\r\n        silhoettes_avg = silhouette_score(cust_df_onehot, cluster_labels)\r\n        log.append([k, inertia, silhoettes_avg])\r\n        continue\r\n    log.append([k, inertia])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:44.91112Z","iopub.execute_input":"2021-08-19T07:55:44.911674Z","iopub.status.idle":"2021-08-19T07:55:45.445677Z","shell.execute_reply.started":"2021-08-19T07:55:44.911623Z","shell.execute_reply":"2021-08-19T07:55:45.444491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Elbow Method (Inertia)\nElbow Method is used to identify $k$ when the **Inertia Dropped Significantly** as compared to inertia before.\n\n\n$$ \\text{Inertia} = \\sum^m_{i=1}||x^{(i)}-\\mu_{c^{(i)}}||^2_2 $$\n\n- $m$ : Number of Datapoints\n- $x^{(i)}$ : i-th Datapoints\n- $\\mu_{c^{(i)}}$ : Cluster Centroids for i-th Datapoints\n\n> Inertia Measure the Sum of Squared Distance of each datapoints to its assigned cluster centroids.\n\n**Remarks:**\n- With $k$ increase, the inertia will decrease. This is because with more clusters being formed, each datapoints will be closer to its centroids. However, this come at the cost of interpretability of the cluster formed with high number of $k$\n- The \"Elbow\" might not be visible if the clustering is ambiguous.\n\n### Silhouette Analysis\nSilhouette Coefficient $\\in [-1,1]$ is a convenient metric to **Quantify the Distinguishability of the Cluster Formed** with 1 denote a highly packed cluster and 0 denote an overlapping cluster.\n\n$$ \\text{Silhouette Coefficient}, s(o) = \\frac{b(o) - a(o)}{max(a(o), b(o))} $$\n<p align='center'>\n    <img src=\"https://2.bp.blogspot.com/-dQi4lSpbbnw/Wpr7iFzrjFI/AAAAAAAACps/qliCgVQ1gW4Jrri_UT_7d9JKI_LXd0n8wCLcBGAs/s1600/ssi-formula.png\" style=\"background: #fff;\">\n</p>\n\n- $s(o)$ : Silhouette Coefficient of datapoints $o$\n- $a^{(i)}$ : Average Distance to other datapoints of the Assigned Cluster Centroids\n- $b^{(i)}$ : Average Distance to Nearest Cluster Centroids\n","metadata":{}},{"cell_type":"code","source":"plot_df = pd.DataFrame(\r\n    log, columns = [\r\n        'k', 'Inertias (Sum of squared distances to Nearest Cluster Centroids)', 'Silhouette Coefficient'\r\n        ]\r\n    )\r\nfig, axes = plt.subplots(1,2, figsize=(18,7))\r\nsns.lineplot(\r\n    x='k', y='Inertias (Sum of squared distances to Nearest Cluster Centroids)', \r\n    data = plot_df, marker= 'o', ax = axes[0])\r\naxes[0].set_title(\"Elbow Method (Inertia)\")\r\nsns.lineplot(x='k', y='Silhouette Coefficient', data=plot_df, marker='o', ax = axes[1])\r\naxes[1].set_title(\"Silhoettes Score\")\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:45.447328Z","iopub.execute_input":"2021-08-19T07:55:45.448047Z","iopub.status.idle":"2021-08-19T07:55:45.857361Z","shell.execute_reply.started":"2021-08-19T07:55:45.447992Z","shell.execute_reply":"2021-08-19T07:55:45.856072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Through the **Elbow Method**, choosing the number of **$k$ is quite ambiguous** as the Inertia seems to decrease uniformly when $k$ increase.\n\n- However, through analysis of the average **Silhouettes Scores**, we noticed the **Silhouettes score max out at $k \\in [4,6]$ with peak at $k=6$**.\n\nHence, **$k = 4,5,6$ seems to be a good candidates** for the number of clusters generated, let us perform a more comprehensive **Silhouettes Analysis** and visualisation of the clusters through **PCA** (*as we cannot straight away plot a chart with 4-Dimension*).","metadata":{}},{"cell_type":"markdown","source":"### Silhouette Analysis","metadata":{}},{"cell_type":"code","source":"def silhouette_analysis_with_pca(n_clusters : int,  X: Sequence):\r\n    '''\r\n    Perform Silhoette Analysis and Visualising the clusters generated using PCA\r\n\r\n    Reference: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\r\n    '''\r\n    \r\n    # Perform K-Mean Clustering\r\n    _, cluster_labels, cluster_centroids, kmean = calculate_k_mean(n_clusters, X)\r\n\r\n    # Compute Individual Silhoette Score\r\n    silhoettes_avg = silhouette_score(X, cluster_labels)\r\n\r\n    # Compute Average Silhoette Score\r\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\r\n\r\n    # Create a subplot with 1 row and 2 columns\r\n    fig, (ax1, ax2) = plt.subplots(1, 2)\r\n    fig.set_size_inches(18, 7)\r\n\r\n    ################################### Silhouette Plot #############################################\r\n \r\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\r\n    # plots of individual clusters, to demarcate them clearly.\r\n    ax1.set_xlim([-0.1, 1])\r\n    ax1.set_ylim([0, len(cust_df) + (n_clusters + 1) * 10])\r\n    y_lower = 10\r\n\r\n    # Assign colours for different cluster\r\n    for i in range(n_clusters):\r\n         # Aggregate the silhouette scores for samples belonging to\r\n         # cluster i, and sort them\r\n         ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\r\n         ith_cluster_silhouette_values.sort()\r\n         size_cluster_i = ith_cluster_silhouette_values.shape[0]\r\n         y_upper = y_lower + size_cluster_i\r\n         color = cm.nipy_spectral(float(i) / n_clusters)\r\n         ax1.fill_betweenx(np.arange(y_lower, y_upper),\r\n                           0, ith_cluster_silhouette_values,\r\n                           facecolor=color, edgecolor=color, alpha=0.7)\r\n         # Label the silhouette plots with their cluster numbers at the middle\r\n         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\r\n         # Compute the new y_lower for next plot\r\n         y_lower = y_upper + 10  # 10 for the 0 samples\r\n\r\n    ax1.set_title(\"The Silhouette Plot for the various clusters.\")\r\n    ax1.set_xlabel(\"The silhouette coefficient values\")\r\n    ax1.set_ylabel(\"Cluster label\")\r\n    # The vertical line for average silhouette score of all the values\r\n    ax1.axvline(x=silhoettes_avg, color=\"red\", linestyle=\"--\")\r\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\r\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\r\n\r\n    ################################### PCA Plot #############################################\r\n \r\n    # Compute PCA with only First 2 Number of Component\r\n    pca2 = PCA(n_components=2)\r\n    sample_pca2 = pca2.fit_transform(X)\r\n\r\n    # Plotting the PCA graph\r\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\r\n    ax2.scatter(sample_pca2[:,0], sample_pca2[:,1], marker='.', s=75, lw=0, alpha=1,\r\n            c=colors, edgecolor='k')\r\n\r\n    # Labeling the clusters\r\n    centers = cluster_centroids.dot(pca2.components_.T) # Calculate New Cluster Positions after PCA\r\n\r\n    # Labelling each cluster centroids\r\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\r\n                 c=\"white\", alpha=1, s=200, edgecolor='k')\r\n    for i, c in enumerate(centers):\r\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\r\n                    s=50, edgecolor='k')\r\n    ax2.set_title(\"The visualization of the PCA with total variance explained of {:.2f}%.\".format(pca2.explained_variance_ratio_.sum()*100))\r\n    ax2.set_xlabel(\"Feature space for the 1st principle component\")\r\n    ax2.set_ylabel(\"Feature space for the 2nd principle component\")\r\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\r\n                  \"with n_clusters = %d\" % n_clusters),\r\n                 fontsize=14, fontweight='bold')\r\n\r\nfor i, k in enumerate(range(4,7)): # Analyse k = (4,5,6)\r\n    silhouette_analysis_with_pca(n_clusters = k, X = cust_df_onehot)\r\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:45.859284Z","iopub.execute_input":"2021-08-19T07:55:45.859748Z","iopub.status.idle":"2021-08-19T07:55:47.686771Z","shell.execute_reply.started":"2021-08-19T07:55:45.859703Z","shell.execute_reply":"2021-08-19T07:55:47.685732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By visualising the cluster formed as well as the silhouette analysis, the following are the observations for each $k \\in \\{4,5,6\\}$:\n- $k=4$\n\n    The clusters formed are quite balanced, by seperating the datapoints into 4 different region without much overlapping.\n    \n- $k=5$\n\n    Cluster formed for 2nd class is quite inconsistent and overlapping as shown in the steep in silhouette analysis as well as the PCA scatterplot.\n\n- $k=6$\n\n    1st Class seems to be slightly overlapping with 0th Class and 5th Classs. However, the clustering seems to be able to capture the general trend quite well.\n\nHence, from the observations gathered, the **plausible values of $k$ is between $4 \\text{ or } 6$** when we perform clustering with all 4 features. Before we proceed to the next algorithm let us explore further on how Feature Selection can help to improve the quality of clustering.","metadata":{}},{"cell_type":"markdown","source":"## Unsupervised Feature Selections\nTo examine which combinations of features can generate more quality clustering, one method is to perform clustering with different subset of features. The aim of this is to maximise the Silhoette Score and generate high quality and interpretable clusters. I will be using the same Elbow Method and Silhoette Score to evaluate the quality of the clusters formed.","metadata":{}},{"cell_type":"code","source":"features_sets = chain(combinations(cust_df_onehot.columns,3), combinations(cust_df_onehot.columns,4)) # Different Subset of attributes\r\nlogs = []\r\nk_range = range(2,10) # Range of k values\r\nfor features in features_sets:\r\n    for k in k_range:\r\n        inertia, cluster_labels, _, _ = calculate_k_mean(k, cust_df_onehot[list(features)])\r\n        silhoettes_avg = silhouette_score(cust_df_onehot, cluster_labels)\r\n        logs.append([features, k, inertia, silhoettes_avg])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:47.689654Z","iopub.execute_input":"2021-08-19T07:55:47.690316Z","iopub.status.idle":"2021-08-19T07:55:51.002445Z","shell.execute_reply.started":"2021-08-19T07:55:47.690258Z","shell.execute_reply":"2021-08-19T07:55:51.001259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"records_df = pd.DataFrame(logs, columns=['Features', 'k', 'Inertia', 'Silhoettes_Avg']).set_index(\"Features\") # Saving history to dataframe\r\nsilhoettes_pivot = records_df.pivot_table(values = 'Silhoettes_Avg', index='k', columns='Features') \r\ninertia_pivot = records_df.pivot_table(values = 'Inertia', index='k', columns='Features')\r\n\r\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 7))\r\n\r\ninertia_pivot.plot(ax = ax1)\r\nax1.set_title(\"Elbow Method (Inertia)\")\r\n\r\nsilhoettes_pivot.plot(ax=ax2)\r\nax2.set_title(\"Silhoettes Score\")\r\nax2.set_ylim(0.15, 0.45) # Set y lim to make legend more visible\r\n\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:51.008928Z","iopub.execute_input":"2021-08-19T07:55:51.012833Z","iopub.status.idle":"2021-08-19T07:55:51.777067Z","shell.execute_reply.started":"2021-08-19T07:55:51.00964Z","shell.execute_reply":"2021-08-19T07:55:51.775802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\nFrom the Inertia Plot and Silhoettes Plot generated, we can argue that **Removing Gender_Male as a clustering feature has close to no effect to the clusters formed** as shown by the Silhoettes_avg of having almost similar shape. This is corroborated with the **ambiguous datapoints** as shown in the Pairplot earlier in EDA.\n\nHence, for the sake of visualisation, I've decided to **Remove Gender_Male from my feature set**.\n\nBefore we move straight away into Clusters Interpretation, let us try out multiple clustering algorithms and compare the clusters formed than over-relying on the public's favourite K-Means.","metadata":{}},{"cell_type":"markdown","source":"### Dropping Gender Column","metadata":{}},{"cell_type":"code","source":"# Dropping Gender from my features\r\ncust_df_scale.drop(columns=\"Gender\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:51.778431Z","iopub.execute_input":"2021-08-19T07:55:51.778753Z","iopub.status.idle":"2021-08-19T07:55:51.784392Z","shell.execute_reply.started":"2021-08-19T07:55:51.778723Z","shell.execute_reply":"2021-08-19T07:55:51.783107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggrolomative Clustering (Hierarchical)\n\nAs many sources of data are generated from a process defined by an underlying hierarchy or taxonomy, we can argue that there is a potential hierarchical relationship for our data. Let us explore this notion further by plotting the Dendogram and Evaluate the Clusters Formed.","metadata":{}},{"cell_type":"code","source":"def plot_dendrogram(model, **kwargs):\r\n    # Create linkage matrix and then plot the dendrogram\r\n\r\n    # create the counts of samples under each node\r\n    counts = np.zeros(model.children_.shape[0])\r\n    n_samples = len(model.labels_)\r\n    for i, merge in enumerate(model.children_):\r\n        current_count = 0\r\n        for child_idx in merge:\r\n            if child_idx < n_samples:\r\n                current_count += 1  # leaf node\r\n            else:\r\n                current_count += counts[child_idx - n_samples]\r\n        counts[i] = current_count\r\n\r\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\r\n                                      counts]).astype(float)\r\n\r\n    # Plot the corresponding dendrogram\r\n    dendrogram(linkage_matrix, **kwargs)\r\n\r\n# setting distance_threshold=0 ensures we compute the full tree.\r\nagg_cluster = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(cust_df_scale)\r\n\r\nplt.figure(figsize = (9,7))\r\nplt.title('Hierarchical Clustering Dendrogram')\r\nplot_dendrogram(agg_cluster, truncate_mode='level', p=3)\r\nplt.hlines(5.5, 0, 300, colors = 'r') # Plotting the clutering line with optimal seperation\r\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:51.785857Z","iopub.execute_input":"2021-08-19T07:55:51.786258Z","iopub.status.idle":"2021-08-19T07:55:52.078803Z","shell.execute_reply.started":"2021-08-19T07:55:51.786127Z","shell.execute_reply":"2021-08-19T07:55:52.077596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\nFrom the dendrogram generated, the reasonable number of cluster would be $k=6$ since moving forward, the taller the edge makes notion of merging the clusters more dodgy.\n\nHence, let us visualise the cluster formed through the scatterplots for all 3 features.","metadata":{}},{"cell_type":"code","source":"# Setting Number of Cluster to 6\r\nagg_cluster_6 = AgglomerativeClustering(n_clusters=6)\r\n\r\nagg_labels = agg_cluster_6.fit_predict(cust_df_scale)\r\n\r\n\r\n# Plotting the PCA\r\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,7))\r\n# Plotting Age against Spending Score (1-100)\r\nsns.scatterplot(x = 'Spending Score (1-100)', y='Age', hue = agg_labels, palette = 'tab10', data = cust_df_scale, ax = ax1)\r\nax1.set_title(\"Age against Spending Score (1-100)\")\r\n\r\n# Plotting Annual Income (k$) against Spending Score (1-100)\r\nsns.scatterplot(x = 'Spending Score (1-100)', y='Annual Income (k$)', hue = agg_labels, palette = 'tab10', data = cust_df_scale, ax = ax2)\r\nax2.set_title(\"Annual Income (k$) against Spending Score (1-100)\")\r\n\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:52.080195Z","iopub.execute_input":"2021-08-19T07:55:52.080805Z","iopub.status.idle":"2021-08-19T07:55:52.771846Z","shell.execute_reply.started":"2021-08-19T07:55:52.080762Z","shell.execute_reply":"2021-08-19T07:55:52.770472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\nBy comparing the cluster formed using Agglomerative Clustering I noticed that the 0th cluster seems to be overlapping with 3rd and 5th cluster. This suggests that the cluster formed are slightly more ambiguous.","metadata":{}},{"cell_type":"markdown","source":"## Spectral Clustering\nAs the final attempt, let us perform clustering with Spectral Clustering that make use of eigenvalues of the similarity matrix of data (with each element measuring the similarity between data points) to perform dimensionality reduction before clustering in fewer dimensions. One advantage of this approach is to find clusters of arbitrary shapes without over relying on a centroid based approach.","metadata":{}},{"cell_type":"code","source":"spec_cluster_6 = SpectralClustering(n_clusters=6, assign_labels='discretize',random_state=0).fit(cust_df_scale)\r\n\r\nspec_labels = spec_cluster_6.labels_\r\n\r\n# Plotting the PCA\r\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,7))\r\n# Plotting Age against Spending Score (1-100)\r\nsns.scatterplot(x = 'Spending Score (1-100)', y='Age', hue = spec_labels, palette = 'tab10', data = cust_df_scale, ax = ax1)\r\nax1.set_title(\"Age against Spending Score (1-100)\")\r\n\r\n# Plotting Annual Income (k$) against Spending Score (1-100)\r\nsns.scatterplot(x = 'Spending Score (1-100)', y='Annual Income (k$)', hue = spec_labels, palette = 'tab10', data = cust_df_scale, ax = ax2)\r\nax2.set_title(\"Annual Income (k$) against Spending Score (1-100)\")\r\n\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:52.773778Z","iopub.execute_input":"2021-08-19T07:55:52.774202Z","iopub.status.idle":"2021-08-19T07:55:53.564159Z","shell.execute_reply.started":"2021-08-19T07:55:52.774156Z","shell.execute_reply":"2021-08-19T07:55:53.562904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The clustering of AgglomerativeClustering and SpectralClustering reviews similar trend to the K-Means clustering but it seems that K-Means is able to seperate the classes better with lesser overlapping points than the rest of algorithm. Hence, we will be using $K-Means, k=6$ for the Final Interpretation.","metadata":{}},{"cell_type":"markdown","source":"# Clusters Interpretations\nTo Interpret the cluster formed, I will be utilising two method of Visualisation and Decision Tree in attempt to explain the 6 distinct cluster formed.\n\n## Clustering Visualisation","metadata":{}},{"cell_type":"code","source":"# Use Scaled Dataframe without Gender_Male to Generate Final Clustering\r\ncluster_df = cust_df_scale.copy()\r\n\r\n# Generate Final Clustering Labels\r\n_, final_cluster_labels, _, _ = calculate_k_mean(6, cluster_df)\r\n\r\n# Inverse Scaling to Review Original Scale for Interpretation\r\ncluster_df[cluster_df.columns] = scaler.inverse_transform(cluster_df) \r\n\r\ncluster_df['Cluster'] = final_cluster_labels\r\n\r\nsns.pairplot(cluster_df, hue = 'Cluster', palette = 'tab10', plot_kws=dict(alpha=0.4))\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:53.566047Z","iopub.execute_input":"2021-08-19T07:55:53.566521Z","iopub.status.idle":"2021-08-19T07:55:57.077072Z","shell.execute_reply.started":"2021-08-19T07:55:53.566472Z","shell.execute_reply":"2021-08-19T07:55:57.075814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Surrogate Model : Decision Tree\nTo better quantify the clustering rule, a Decision Tree can be used as a surrogate model for us to better understand the decision rule by which each cluster is associated.","metadata":{}},{"cell_type":"code","source":"clf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10)\r\n\r\nclf.fit(cluster_df.drop(columns='Cluster'), cluster_df['Cluster'])\r\n\r\nfig, ax = plt.subplots(figsize = (17,8))\r\nplot_tree(\r\n            clf,\r\n            feature_names = cluster_df.drop(columns='Cluster').columns,\r\n            class_names= np.unique(cluster_df['Cluster'].values.astype(str)),\r\n            impurity=False,\r\n            filled = True, \r\n            ax = ax, \r\n            fontsize=11\r\n            )\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:55:57.078868Z","iopub.execute_input":"2021-08-19T07:55:57.079328Z","iopub.status.idle":"2021-08-19T07:55:57.989465Z","shell.execute_reply.started":"2021-08-19T07:55:57.079281Z","shell.execute_reply":"2021-08-19T07:55:57.988697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following are the interpretation observation for each classes:\n\n0. **Overspending Youngsters** a.k.a. ***\"Trendy Ah Beng\"***\n\n    Class 0 has a higher tendency to overspends (spend more than they earn) with most of the class members being youngsters that are at most 40 years old.\n\n    > Youngsters that can spend more and earn less are most likely getting their incentives from their parents, and the marketing strategy to devise is probably to sell **items that can resonate with youths** better like sport equipments or idols endorsed products.\n\n1. **Senior Moderate Buyers** a.k.a. ***\"Housewife Aunty\"***\n\n    Class 1 has a moderate income and spending score which reflected their strong financial awareness which is Highly Similar to Class 2 with Age being the only seperating factors. Besides, Class 1 is also the largest spending group with 45 members provided the sampling is done randomly and unbiased.\n\n    > Being the most loyal customer to the supermarket, strategy like campaigns would not actually make much effects to their spending behaviours. The subtle strategy that could be adopted is through **occasional offerings** like Bundle Sale or Buy 1 Get 1 Free to boost their spending behaviour for staple goods.\n\n2. **Junior Moderate Buyers** a.k.a. ***\"Employed Alan\"***\n\n    Class 2 also has a moderate income and spending score like Class 1 but with most of its member below age of 41.\n\n    > They are youth who are just starting to work with relative moderate money to spend on groceries. Perhaps some **campaigns or games that allows them to exchange vouchers** or winning lucky draws can inspire them to spend more and earn more points for the campaign.\n\n3. **The Stingy Buyers** a.k.a. ***\"Kiamsiap Uncle John\"***\n\n    Class 3 spend less on marketing despite earning more. Class 3 also spans across a wider range of age group of between around 20 to 80. This group of customer also make up of a good deal of members of 34 out of the 200 total customer in the dataset.\n\n    > They should be the **main focus of marketing strategy** as they has the **capabilities to spend more than what they are spending now**. Hence, further analysis should be done to study closely their spending behaviours in order to devise more specific strategy like maybe expanding the luxury item section etc.\n\n4. **Low Income Buyers** a.k.a. ***\"Needy Jenny\"***\n\n    Class 4 belongs to customers from lower income and lower spending group across ages.\n\n    > They are the one that **needs the most help** from the supermarket although it might be a blunt to just directly provide offers and incentives for them, one strategy is to organise campaigns that **allow them exchange staple goods through the green act** of maybe collecting reusable materials or returning plastics bags to boost the companies' image of being environmental aware and to promote supermarket to their peers. \n\n5. **Young Generous Buyers** a.k.a. ***\"Entrepreneur Ben\"***\n\n    Class 5 belongs to very special group of youngsters that has higher than average spending and earning capabilities\n\n    > Since this customer group is highly populated by youth, stategy mention in Class 1 like importing items that can **resonate with youths** and expanding the luxurious items sections like wines could catch their attentions.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion : The Marketing Strategy\nTo answer the second objective and based on the clustering interpretation, the mall supermarket should devise the following two strategy to boost the growth and reputation of the supermarket.\n\n## 1. Appealing to the Millennials and Gen-Z\n\nAs we have two major customer cluster(Class 0 & Class 5) that is made up of **Millennials(Age 24-40) and Gen-Z(Age 6-24)**, the mall should make sure relevant action is taken to **instill some element resonance** to appeal these two groups of young customer. As millennials are ***experience seeker, eager for self-expression and identity and tech-savvy***,\nthe following are the approach proposed:\n- Online Purchasing Platform with Delivery Service\n- Diversify Product Sold to Include Youth-Appealing Items *(e.g. Fashionables, Entertainment Systems and Mobile Devices, Sports Peripherals)*\n\n## 2. Engage the Mature Customer\n\nFrom Class 1 and Class 3 we noticed a huge proportional of our customer are made up of **mature buyer** be it those with higher spending score or those with moderate spending score. Their **spending behaviour are relatively consistent** and hence the approach taken should not be more subtle and not too drastic to improve their spending behaviour.\nThe following are the approach proposed:\n- Bundle Sales and Rotational Discount\n- Coupon Based Campaign/Lucky Draw to encourage Spending and Bulk Purchase by the customer\n\nReference :\n- [Understanding Gen Z, millennial shopping behaviour is key to the future of retail](https://www.straitstimes.com/business/companies-markets/understanding-gen-z-millennial-shopping-behaviour-is-key-to-the-future-of)\n- [Segment and Sell to Gen Y: 10 Ways Younger and Older Millennials Shop Differently](https://www.npd.com/news/thought-leadership/2018/10-ways-younger-and-older-millennials-shop-differently/)","metadata":{}},{"cell_type":"markdown","source":"# Personal Learning Journey\n\nI do enjoy the entire process of Unsupervised Learning : Clustering as I found it thrilling to **Discover the Hidden Chest** in the dataset. Although I am aware that this example is in an extremely ideal state, it serves as a good example to demonstrate the clustering process. I also appreciate the multidisciplinary nature of data science that allows me to gain more exposure on devising inclusive and viable business strategy for the Mall Supermarket.\n\nWritten By : Wong Zhao Wu\n\nLast Modified : 26 July 2020\n\n![Unsplash Fred Meyer Superstore](https://images.unsplash.com/photo-1515706886582-54c73c5eaf41?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=750&q=80)\n\nImage retrieved from [Unsplash](https://unsplash.com/photos/KfvknMhkmw0).","metadata":{}}]}