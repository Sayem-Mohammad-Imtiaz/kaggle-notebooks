{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Navie Bayes Classification"},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes (NB**) is ‘naive’ because it makes the assumption that features of a measurement are independent of each other. This is naive because it is (almost) never true. Here is why NB works anyway.\n\n* A naive Bayes classifier is an algorithm that uses Bayes' theorem to classify objects. Naive Bayes classifiers assume strong, or naive, independence between attributes of data points. Popular uses of naive Bayes classifiers include spam filters, text analysis and medical diagnosis. These classifiers are widely used for machine learning because they are simple to implement.\n\n* Naive Bayes is also known as simple Bayes or independence Bayes.\n\n* A naive Bayes classifier uses probability theory to classify data. Naive Bayes classifier algorithms make use of Bayes' theorem. The key insight of Bayes' theorem is that the probability of an event can be adjusted as new data is introduced.\n\n* A naive Bayes classifier is not a single algorithm, but a family of machine learning algorithms that make uses of statistical independence. These algorithms are relatively easy to write and run more efficiently than more complex Bayes algorithms.\n\n* The most popular application is spam filters. A spam filter looks at email messages for certain key words and puts them in a spam folder if they match.\n\n* Despite the name, the more data it gets, the more accurate a naive Bayes classifier becomes, such as from a user flagging email messages in an inbox for spam.\n\n* What makes a naive Bayes classifier naive is its assumption that all attributes of a data point under consideration are independent of each other. A classifier sorting fruits into apples and oranges would know that apples are red, round and are a certain size, but would not assume all these things at once. Oranges are round too, after all.\n\n* One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them.\n\n* The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known.\n\nIt rarely ever overfits the data.\n\nAnother important advantage is that its model training and prediction times are very fast for the amount of data it can handle.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# To read the csv files in arrays and dataframes.\nimport numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/spam.csv\", encoding = \"latin-1\")\n# # encoding='latin-1' is used to download all special characters and everything in python. If there is no encoding on the data, it gives an error. Let's check the first five values.\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for the null values if any and count the total number of null values."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are so many null values in the 3rd, 4th and 5th columns and it is better to remove them. Also rename the column names as they doesn't sound familiar."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],axis=1)\ndata.rename(columns= { 'v1' : 'class' , 'v2' : 'message'}, inplace= True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ncount =pd.value_counts(data[\"class\"], sort= True)\ncount.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Bar chart')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see that the count of spam email is less."},{"metadata":{"trusted":true},"cell_type":"code","source":"count.plot(kind = 'pie',autopct='%1.2f%%') # 1.2 is the decimal points for 2 places\nplt.title('Pie chart')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('class').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add a new column called **Length** and check the size of each message."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['length'] = data['message'].apply(len)\n# swapping the columns\ndata = data[['message', 'length', 'class']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words.\n\n**What are Stop words?**\n\nStop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\n\nWe would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words.\n\n* Remove all **Non-words** in the message( ex : if there are any special characters or numbers, they are replaced with spaces.)\n\n\n* Changing all the characters to **lower case letters**. We can do it in with the upper case as well, but lower case looks better in     approach. ( ex : the syste must treat the characters 'A' and 'a' the same.\n\n\n*  Splitting each word in the sentence and separated by **comma**\n\n\n* Checking the **stop words ( if any )** and removing them accordingly. \n\n\nThe idea of **stemming** is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is    involved.\n\n The reason why we stem is to shorten the lookup, and normalize sentences.\n\n **Consider:**\n\n \"I was taking a ride in the car.\"\n \n \"I was riding in the car.\"\n\n This sentence means the same thing. in the car is the same\n\n5. **Joining** all the words into a single sentence after splitting and checking each word in a sentence. it joins all the words.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\ndef clean_message(message):\n    message = re.sub(\"[^A-Za-z]\", \" \", message) #1\n    message = message.lower() #2\n    message = message.split() #3\n    stemmer = PorterStemmer()   #4. to find the  root meaning word of each word         \n    message = [stemmer.stem(word) for word in message if word not in set(stopwords.words(\"english\"))] #5\n    message = \" \".join(message) #6 #Keeping cleaned words together\n    return message","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test how our function works. We shall take the original data and the 1st value."},{"metadata":{"trusted":true},"cell_type":"code","source":"message = data.message[0]\nprint(message)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the data after text mining from the actual data given and performing all the operiations on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"message = clean_message(message)\nprint(message)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us apply the function to all the rows in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"messages = []\nfor i in range(0, len(data)):\n    message = clean_message(data.message[i])\n    messages.append(message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"message\"],axis=1)\ndata['messages'] = messages\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's seperate the output and documents\ny = data[\"class\"].values\nx = data[\"messages\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#splitting the data in training and test set\nxtrain , xtest , ytrain , ytest = train_test_split(x,y, test_size = 0.3, random_state = 1)\n# test size is 0.3 which is 70 : 30\nprint(xtrain.shape, ytrain.shape, xtest.shape, ytest.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A **bag-of-words model**, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n\nBag of Words (BoW) is an algorithm that counts how many times a word appears in a document. It’s a tally. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification and topic modeling. BoW is a also method for preparing text for input in a deep-learning net.\n\nBoW lists words paired with their word counts per document. In the table where the words and documents that effectively become vectors are stored, each row is a word, each column is a document, and each cell is a word count. Each of the documents in the corpus is represented by columns of equal length. Those are wordcount vectors, an output stripped of context.\n\nWhenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm. Hence, Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n\n**Example :** Hello, how are you ?\n\nAfter making the sentence into tokens  : \"Hello\", \"how\", \"are\", \"you\"\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **TF-IDF ( Term Frequency - Inverse Document Frequency )**\n"},{"metadata":{},"cell_type":"markdown","source":"This method is also called as Normalization. TF - How many times a particular word appears in a single doc. IDF - This downscales words that appear a lot across documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english',max_df=0.5)\n\n#fitting train data and then transforming it to count matrix#fitting \nx_train = vect.fit_transform(xtrain)\n#print(x_train)\n\n#transforming the test data into the count matrix initiated for train data\nx_test = vect.transform(xtest)\n\n# importing naive bayes algorithm\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n#fitting the model into train data \nnb.fit(x_train,ytrain)\n\n#predicting the model on train and test data\ny_pred_test = nb.predict(x_test)\ny_pred_train = nb.predict(x_train)\n\n#checking accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(ytest,y_pred_test)*100)\n\n#Making Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest,y_pred_test)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Count Vectorizer **  \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"The most straightforward one, it counts the number of times a token shows up in the document and uses this value as its weight. In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. \n\nFor more information, one can go through the link below.\n\nhttps://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n\nhttps://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvect1 = CountVectorizer(stop_words='english',max_df=0.5)\n\n#fitting train data and then transforming it to count matrix#fitting \nx_train = vect1.fit_transform(xtrain)\n\n#transforming the test data into the count matrix initiated for train data\nx_test = vect1.transform(xtest)\n\n# importing naive bayes algorithm\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n#fitting the model into train data \nnb.fit(x_train,ytrain)\n\n#predicting the model on train and test data\ny_pred_test = nb.predict(x_test)\ny_pred_train = nb.predict(x_train)\n\n#checking accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(ytest,y_pred_test)*100)\n\n#Making Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest,y_pred_test)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks like the count vectorizer is giving the most accutate result of 98 where as TfIdf is at 97**"},{"metadata":{},"cell_type":"markdown","source":"Let us take some random sample data and try to apply the model and see how that actually works."},{"metadata":{},"cell_type":"markdown","source":"# Testing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_text = pd.Series('WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. valid 12 hours')\nnew_text_transform = vect.transform(new_text)\nprint(\" The email is a\" ,nb.predict(new_text_transform))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_text = pd.Series(\" Hello, how are you?\")\nnew_text_transform = vect.transform(new_text)\nprint(\" The email is a\" ,nb.predict(new_text_transform))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}