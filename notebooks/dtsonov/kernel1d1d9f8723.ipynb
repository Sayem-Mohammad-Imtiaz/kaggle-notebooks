{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sqlite3\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport squarify\nimport random\n\nrandom.seed(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data and clean up text"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/twitter-airline-sentiment/Tweets.csv\")\n\nstop_words = set(stopwords.words('english'))\ndef clean_text(text):\n    # Remove http links\n    res = re.sub(\"(?<!\\S)https?://\\S+\", \"\", text)\n    \n    # Remove numbers\n    res = re.sub(\"(?<!\\S)[-+]?(?=.*?\\d)\\d*[.,]?\\d*(?!\\S)\", \"\", res)\n    \n    # Remove special characters and make everything lowercase. This can be improved because this way we lose emojis like :(, :) etc\n    res = re.sub('[^A-Za-z0-9]+', ' ', res).lower().strip()\n    \n    # We do not expand contracted spelling forms i.e. forms like \"he's\" = \"he is\" because we accept the user spelling style as is and keep them intact.\n    # Otherwise the trained model should handle this and choose between expanded/contrÐ°cted forms which is an additional issue.\n    \n    # Remove stopwords\n    words = [word for word in nltk.word_tokenize(res) if not word in stop_words]\n    res = ' '.join(words)\n    \n    return res\n\ndf['text_cleaned'] = df['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word clouds for different sentiments"},{"metadata":{"trusted":false},"cell_type":"code","source":"# The following code prepares data for the word cloud (that shows the most frequent words by sentiment).\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Labels of each column\nprint(\"Columns in the data base:\\n\")\nprint(df.keys())\nprint(\"\\n\")\nprint(\"We have \" + str(df.shape[0]) + \" data points spread between \" + str(df.shape[1]) + \" features.\")\n\n# Popular words\npos = df[df['airline_sentiment'] == 'positive']['text_cleaned'].head(25000).str.cat(sep=' ')\nneg = df[df['airline_sentiment'] == 'negative']['text_cleaned'].head(25000).str.cat(sep=' ')\nneut = df[df['airline_sentiment'] == 'neutral']['text_cleaned'].head(25000).str.cat(sep=' ')\n\n# Generate a word cloud image\nposwc = WordCloud(width=1280, height=800).generate(pos)\nnegwc = WordCloud(width=1280, height=800).generate(neg)\nneutwc = WordCloud(width=1280, height=800).generate(neut)\n\ndef plwordcl(fig_num, word_cloud, title):\n    plt.figure(fig_num, figsize=(20,10), facecolor='w')\n    plt.imshow(word_cloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.title(title)\n\nplwordcl(1, poswc, \"Word cloud for POSITIVE sentiment\")\nplwordcl(2, negwc, \"Word cloud for NEGATIVE sentiment\")\nplwordcl(3, neutwc, \"Word cloud for NEUTRAL sentiment\")\nplt.show()\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Charts of the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Airlines chart\ndf_ac = df.groupby('airline').size()\ndf_ac.plot(kind='pie', subplots=True, figsize=(8, 8))\nplt.title(\"Airlines data chart\")\nplt.ylabel(\"\")\nplt.show()\n\n\n#Sentiment treemap chart\ndf_sent = df.groupby('airline_sentiment').size().reset_index(name='counts')\nlabels = df_sent.apply(lambda x: str(x[0]) + \"\\n (\" + str(x[1]) + \")\", axis=1)\nsizes = df_sent['counts'].values.tolist()\ncolors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\nplt.figure(figsize=(6,4), dpi= 80)\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)\n\n\n#Negative reasons\ndf_negative_airlines = df.groupby('negativereason').size().reset_index(name='counts')\nn = df_negative_airlines['negativereason'].unique().__len__()+1\nall_colors = list(plt.cm.colors.cnames.keys())\nrandom.seed(100)\nc = random.choices(all_colors, k=n)\n\n# Plot Bars\nplt.figure(figsize=(12,8))\nplt.bar(df_negative_airlines['negativereason'], df_negative_airlines['counts'], color=c, width=.5)\nfor i, val in enumerate(df_negative_airlines['counts'].values):\n    plt.text(i, val, float(val), horizontalalignment='center', verticalalignment='bottom', fontdict={'fontweight':500, 'size':12})\n\n# Decoration\nplt.gca().set_xticklabels(df_negative_airlines['negativereason'], rotation=60, horizontalalignment= 'right')\nplt.title(\"Negative reasons\", fontsize=22)\nplt.ylabel('# Tweets')\nplt.ylim(0, 3500)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment model using RNN"},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\n\nfrom scipy.spatial.distance import cdist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndata_text = df['text_cleaned'].tolist()\ny_orig = df['airline_sentiment'].tolist()\n\nle = LabelEncoder()\nneg_label_ind, pos_label_ind, neut_label_ind = le.fit_transform(['negative', 'positive', 'neutral'])\nprint(f'negative encoded as {neg_label_ind}, positive encoded as {pos_label_ind}, neutral encoded as {neut_label_ind}')\n\ny_le = le.transform(y_orig)\ny = to_categorical(y_le)\n\nx_train_text, x_test_text, y_train, y_test = train_test_split(data_text, y, test_size=0.20, random_state=12)\n\nnum_words = 10000\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(x_train_text)\n\nx_train_tokens = tokenizer.texts_to_sequences(x_train_text)\nx_test_tokens = tokenizer.texts_to_sequences(x_test_text)\n\nnum_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\nnum_tokens = np.array(num_tokens)\nnp.mean(num_tokens)\n\nnp.max(num_tokens)\n\nmax_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\nmax_tokens = int(max_tokens)\nmax_tokens\n\nnp.sum(num_tokens < max_tokens) / len(num_tokens)\n\npad = 'pre'\n\nx_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n                            padding=pad, truncating=pad)\n\nx_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n                           padding=pad, truncating=pad)\n\nprint(x_train_pad.shape)\nprint(x_test_pad.shape)\n\nmodel_rnn = Sequential()\nembedding_size = 50\n\nmodel_rnn.add(Embedding(input_dim=num_words,\n                    output_dim=embedding_size,\n                    input_length=max_tokens\n                   ))\n\nmodel_rnn.add(GRU(units=20, return_sequences=True))\n\nmodel_rnn.add(GRU(units=10, return_sequences=True))\n\nmodel_rnn.add(GRU(units=5))\n\nmodel_rnn.add(Dense(3, activation='softmax'))\n\noptimizer = Adam(0.01)\n\nmodel_rnn.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\nmodel_rnn.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"modelh = model_rnn.fit(x_train_pad, y_train,\n          validation_split=0.07, epochs=5, batch_size=200)\n\nresult = model_rnn.evaluate(x_test_pad, y_test, verbose=2)\nprint(\"\\n\\n\")\nprint(\"Accuracy on validation set: {0:.2}\".format(result[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = model_rnn.predict(x=x_test_pad)\n\ny_pred_unenc = [np.argmax(p) for p in y_pred]\ny_test_unenc = [np.argmax(p) for p in y_test]\n\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\n\nconf_mat_dl = confusion_matrix(y_test_unenc, y_pred_unenc, labels=[0,1,2])\ncm_normalized_dl = conf_mat_dl.astype('float') / conf_mat_dl.sum(axis=1)[:, np.newaxis]\n\nprint(\"Confusion matrix:\")\nprint(conf_mat_dl)\nprint(\"Normalized confusion matrix:\")\nprint(cm_normalized_dl)\n\ndef plot_confusion_matrix(array, columns, title):\n    import seaborn as sn\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n      \n    df_cm = pd.DataFrame(array, index=columns, columns=columns)\n    df_cm.round(2)\n    #plt.figure(figsize = (10,7))\n    sn.set(font_scale=1.4)#for label size\n    sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\n\n    plt.title(title)\n    \n    plt.show()\n    \nplot_confusion_matrix(cm_normalized_dl, le.inverse_transform([0,1,2]), \"Normalized Conf. Matrix, RNN\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RNN results."},{"metadata":{},"cell_type":"markdown","source":"#### Plot RNN loss and accuracy on the training set."},{"metadata":{"trusted":false},"cell_type":"code","source":"def draw_loss(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string], '')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n    \ndraw_loss(modelh, 'accuracy')\ndraw_loss(modelh, 'loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To improve accuracy we need more training examples and more layers to the RNN could be added too."},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix in details:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Out of \" + str(np.sum(conf_mat_dl[neg_label_ind])) + \"  negative the RNN predicts \" + str(conf_mat_dl[neg_label_ind][neg_label_ind]) + \" as negative.\")\n\nprint(\"Out of \" + str(np.sum(conf_mat_dl[neut_label_ind])) + \" neutral the RNN predicts \" + str(conf_mat_dl[neut_label_ind][neut_label_ind]) + \" as neutral.\")\n\nprint(\"Out of \" + str(np.sum(conf_mat_dl[pos_label_ind])) + \" positive the RNN predicts \" + str(conf_mat_dl[pos_label_ind][pos_label_ind]) + \" as positive.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Negative sentiment precision: \" + str(conf_mat_dl[neg_label_ind][neg_label_ind]/np.sum(conf_mat_dl.T[neg_label_ind])))\n\nprint(\"Neutral sentiment precision: \" + str(conf_mat_dl[neut_label_ind][neut_label_ind]/np.sum(conf_mat_dl.T[neut_label_ind])))\n\nprint(\"Positive sentiment precision: \" + str(conf_mat_dl[pos_label_ind][pos_label_ind]/np.sum(conf_mat_dl.T[pos_label_ind])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Negative sentiment recall: \" + str(conf_mat_dl[neg_label_ind][neg_label_ind]/np.sum(conf_mat_dl[neg_label_ind])))\n\nprint(\"Neutral sentiment recall: \" + str(conf_mat_dl[neut_label_ind][neut_label_ind]/np.sum(conf_mat_dl[neut_label_ind])))\n\nprint(\"Positive sentiment recall: \" + str(conf_mat_dl[pos_label_ind][pos_label_ind]/np.sum(conf_mat_dl[pos_label_ind])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy from the confusion matrix:\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"accuracy = (np.trace(conf_mat_dl) )/(sum(sum(conf_mat_dl)))\nprint(f\"We conclude that {accuracy:.2%} of the predicted outputs should be correctly classified using RNN.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment model using boosting and ensemble methods"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom collections import Counter, OrderedDict\nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\n\ncount_vect = CountVectorizer( max_df=0.5,stop_words=stopwords.words('english'), max_features=10000)\n\nX = count_vect.fit_transform(df['text_cleaned'])\n\nmodel_rf = RandomForestClassifier(n_estimators=5, class_weight='balanced', random_state=0)\nmodel_ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=56)\nmodel_xgb = GradientBoostingClassifier(loss='deviance', \n                                       learning_rate=0.1, n_estimators=50, \n                                       subsample=1.0, \n                                       criterion='friedman_mse', min_samples_split=5, \n                                       min_samples_leaf=1, \n                                       min_weight_fraction_leaf=0.0, \n                                       max_depth=3, \n                                       min_impurity_decrease=0.0, \n                                       min_impurity_split=None, \n                                       random_state=56, \n                                       max_features=None, \n                                       verbose=0, \n                                       max_leaf_nodes=None, \n                                       warm_start=False, \n                                       validation_fraction=0.1, tol=0.0001)\n\nmodels = { 'random forest': model_rf,\n          'ada boost classifier': model_ada,\n          'gradient boosting classifier': model_xgb\n         }\n\nmodels = { k: OneVsRestClassifier(m) for k, m in models.items()}\n\nfor key, model in models.items():\n    print('-----------')\n    print(f\"Results for {key}:\")\n    y_pred = cross_val_predict(model, X, y, cv=4)\n    \n    y_pred_unenc = [np.argmax(p) for p in y_pred]\n    y_unenc = [np.argmax(p) for p in y]\n    \n    conf_mat = confusion_matrix(y_unenc, y_pred_unenc, labels=[0,1,2])\n    cm_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n    acc = accuracy_score(y_pred_unenc, y_unenc, normalize=True)\n    print(\"Confusion matrix:\")\n    print(conf_mat)\n    print(\"Normalized confusion matrix:\")\n    print(cm_normalized)\n    print(f\"Accuracy: {acc}\")\n    plot_confusion_matrix(cm_normalized, le.inverse_transform([0,1,2]), \"Normalized Conf. Matrix, %s\" % key)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion \nWhen we have massive datasets neural networks converge to lower generalization error.\n\nThe overall accuracy metrics with RNN are much better (especially for the NEUTRAL and POSITIVE cases) compared to the case with boosting.\n\nOtherwise with smaller datasets boosting converges faster and with smaller error."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}