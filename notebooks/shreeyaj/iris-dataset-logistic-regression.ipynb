{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"iris_df=pd.read_csv(\"/kaggle/input/iris-flower-dataset/IRIS.csv\")\niris_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Inspection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the data frame: \",iris_df.shape)\nprint(\"Total null values: \",iris_df.isna().sum().sum())\nprint(\"Duplicate values: \",iris_df.duplicated().sum() )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Removing the duplicated entries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.drop_duplicates(inplace=True)\nprint(\"Shape of the data frame: \",iris_df.shape)\nprint(\"\\n\")\nprint(\"Species categories with its count \\n\",iris_df[\"species\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> On comparng the mean and median values for these four paramater we can observe skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#iris_df.plot(kind='box')\n#plt.show()\nsns.boxplot(data=iris_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> from the above box plot it's clear \n1. sepal_width has outliers and it's right skewed\n2. petal_length and petal_with are left skewed\n3. sepal_length is symmetrical","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Measure of skewness\n#https://pythontic.com/pandas/dataframe-computations/skew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness_value=iris_df.skew(axis=0)\nprint(\"Measure of skewness column wise:\\n\",skewness_value)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. > If skewness value lies above +1 or below -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric\n1. > For Symmetric distribution, mean=median\n1. > Positively skewed data:\nIf tail is on the right as that of the second image in the figure, it is right skewed data. It is also called positive skewed data.\nCommon transformations of this data include square root, cube root, and log.\n1. > Negatively skewed data:\nIf the tail is to the left of data, then it is called left skewed data. It is also called negatively skewed data.\nCommon transformations include square , cube root and logarithmic.\n* https://medium.com/@TheDataGyan/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55* ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Measure of kurtosis\n#https://pythontic.com/pandas/dataframe-computations/kurtosis\n#https://pythontic.com/pandas/dataframe-computations/kurtosis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nkurtosis_values=iris_df.kurt(axis=0)\nprint(kurtosis_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Categorical values\n#https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\niris_df[\"species\"]=label_encoder.fit_transform(iris_df[\"species\"])\nprint(iris_df.head(10))\nprint(\"\\n\")\nprint(iris_df[\"species\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=iris_df.drop([\"species\"],axis=1)\nY=iris_df[\"species\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1,X_test1,Y_train1,Y_test1=train_test_split(X,Y,test_size=0.2,random_state=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selected model - Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogistic_reg=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy prior scaling/normalizing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nlogistic_reg.fit(X_train1,Y_train1)\npredicted_result1=logistic_reg.predict(X_test1)\nprint(\"Accuracy Score: \",accuracy_score(Y_test1, predicted_result1))\n\nprint(\"Confusion Matrix:\\n \",confusion_matrix(Y_test1, predicted_result1))\n\nprint(\"Classification Report:\\n \",classification_report(Y_test1, predicted_result1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">  From the above result it is clear that the model is 96% accurate and f1score as 1 indicating very low false positive and false negative prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# Implementing Scaling,LDA followed by generating a model:-","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1.Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train2,X_test2,Y_train2,Y_test2=train_test_split(X,Y,test_size=0.2,random_state=5,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Standard Scaler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX_train2=ss.fit_transform(X_train2)\nX_test2=ss.transform(X_test2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.LDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nLDA=LinearDiscriminantAnalysis()\nX_train2=LDA.fit_transform(X_train2,Y_train2)\nX_test2=LDA.transform(X_test2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.LG Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression()\nLR.fit(X_train2,Y_train2)\ny_prediction=LR.predict(X_test2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint(\"\\n Accuracy Score:\",accuracy_score(Y_test2,y_prediction))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(Y_test2,y_prediction))\nprint(\"Classification Report:\")\nprint(classification_report(Y_test2,y_prediction))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}