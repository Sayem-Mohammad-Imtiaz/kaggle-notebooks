{"cells":[{"metadata":{},"cell_type":"markdown","source":"## NLP Using spaCy\n\nWhenever you need to do any kind of NLP work, there is a bit more ease when using spaCy and I have found that out during my time working on NLP applications.<br>\n\nThis notebook is done to get you started on anything that is related to spaCy.\n\n\nSo, here are some of the things that you can do when you use spaCy.<br>\n\n<h1>Table of Contents</h1>\n\n1. [Tokenisation in spaCy](https://www.kaggle.com/charlessamuel/nlp-using-spacy-a-simple-explanation#Tokenisation)\n2. [Removing stop words using spaCy](https://www.kaggle.com/charlessamuel/nlp-using-spacy-a-simple-explanation#Removing-Stopwords-in-Sentences)\n3. [Normalisation in spaCy](https://www.kaggle.com/charlessamuel/nlp-using-spacy-a-simple-explanation#Removing-Stopwords-in-Sentences#Normalisation)\n4. [NER in spaCy](https://www.kaggle.com/charlessamuel/nlp-using-spacy-a-simple-explanation#Removing-Stopwords-in-Sentences#NER-in-spaCy)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import spacy\nimport pandas as pd\nimport numpy as np\n\nnlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this cell just loads up the basic English model. Other codes include:\n\n* French (fr)\n* Chinese (zh)\n* German (de)\n\nAnd many more. Head over [here](https://spacy.io/models) to check them all out.<br>\n\nIf you're interested in a more comprehensive model you just need to execute this cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download en_core_web_md","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, 'md' stands for Medium. Other sizes are 'sm'(Small- Already present in Kaggle) and 'lg'(Large)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/newyork-room-rentalads/room-rental-ads.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now load in our DataFrame from which we will perform our first "},{"metadata":{},"cell_type":"markdown","source":"# Tokenisation\n\nOne of the basic spaCy operations is Tokenisation. This is where the words of a sentence are split up into words called Tokens. For instance if you have a sentence:<br>\n\nI like planes and tanks <br>\n\nThe sentence is split into:<br>\n\nI<br>\nlike<br>\nplanes<br>\nand<br>\ntanks<br>\n\nIts one of the most easiest code to implement in spaCy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenise(msg):\n    doc = nlp(msg)\n    \n    for token in doc:\n        print(token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenise(\"I like planes and tanks\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the base of all spaCy operations. "},{"metadata":{},"cell_type":"markdown","source":"# Removing Stopwords in Sentences\n\nStopwords are words that never contribute to the overrall meaning of a sentence. Removing them is a very big part in cleaning the text in the data. It is once again really simple to remove them using our tokenisation function earlier. <br>\n\nWhen we use token.is_stop, it returns True if the Token is a stop word else it is False. <br>\n\nThe following cell shows the stop words that are available in the model. More stopwords are available in bigger models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopwords Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_remove(msg):\n    doc = nlp(msg)\n    \n    for token in doc:\n        if token.is_stop:\n            pass\n        else:\n            print(token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_remove(\"I have liked reading books for a good few years now\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stop words successfully filtered out."},{"metadata":{},"cell_type":"markdown","source":"# Normalisation\n\nThis is something I personally use to clean up the text in the data. It is a really simple function that also includes our stop_word condition along with a few more conditions:\n\n* token.is_digit - Used to remove any digits\n* token.is_punct - Used to remove any punctuations\n* token.is_oov - Used to remove any words that are not present in the vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(msg):\n    \n    doc = nlp(msg)\n    res=[]\n    \n    for token in doc:\n        if(token.is_stop or token.is_digit or token.is_punct or not(token.is_oov)):\n            pass\n        else:\n            res.append(token.lemma_.lower()) #Lower case of the token lemma is added\n    \n    return \" \".join(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can apply our normalisation function to the Description column in the dataset. This dataset in particular has special characters which can be removed using RegEx. The function that I used in this notebook can be found [here](https://www.kaggle.com/charlessamuel/rentals-in-the-big-apple-xgboost#NLP-Work)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef normalize_1(msg):\n    \n    msg = re.sub('[^A-Za-z]+', ' ', str(msg)) #remove special character and intergers\n    doc = nlp(msg)\n    res=[]\n    for token in doc:\n        if(token.is_stop or token.is_punct or token.is_currency or token.is_space or len(token.text) <= 2): #Remove Stopwords, Punctuations, Currency and Spaces\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Description'] = df['Description'].apply(normalize_1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is in a list to make sure I can take counts of number of words much easily."},{"metadata":{},"cell_type":"markdown","source":"# NER in spaCy"},{"metadata":{},"cell_type":"markdown","source":"This is Named Entity Recognition(NER) using the basic English model. Displacy is a function used to render the entities that are found by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy import displacy\n\ndef displacify(msg):\n    \n    doc = nlp(msg)\n    \n    return displacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displacify(\"spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is some of the basic functions I have done so far in spaCy. <br>\n\n\n## Things I am planning to add:\n\n1. Sentiment detection(Have to look into this)\n\nAnything else I can add? Let me know in the comments. Upvote if you liked this notebook :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}