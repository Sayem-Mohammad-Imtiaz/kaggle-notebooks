{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. Fatty deposits build up over time, forming plaques in your heart's arteries. If a plaque ruptures, a blood clot can form and block your arteries, causing a heart attack.**","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.subplots import make_subplots\nimport matplotlib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\nprint('Libraries imported successfully..!!')","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About dataset","metadata":{}},{"cell_type":"markdown","source":"   - **Age** : Age of the patient\n\n  - **Sex** : Sex of the patient\n\n  - **exang**: exercise induced angina (1 = yes; 0 = no)\n\n  - **ca**: number of major vessels (0-3)\n\n  - **cp** : Chest Pain type chest pain type\n\n    - **Value 1**: typical angina\n\n    - **Value 2**: atypical angina\n\n    - **Value 3**: non-anginal pain\n\n    - **Value 4**: asymptomatic\n\n  - **trtbps** : resting blood pressure (in mm Hg)\n\n  - **chol** : cholestoral in mg/dl fetched via BMI sensor\n\n  - **fbs** : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\n  - **restecg** : resting electrocardiographic results\n\n      - **Value 0**: normal\n\n      - **Value 1**: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n\n      - **Value 2**: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n  - **thalachh** : maximum heart rate achieved\n\n  - **output** : 0 = less chance of heart attack 1 = more chance of heart attack","metadata":{}},{"cell_type":"markdown","source":"## Reading the Dataset","metadata":{}},{"cell_type":"code","source":"#Reading the csv file heart.csv in variable \ndf = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the first 5 rows of our data\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the shape of DataFrame","metadata":{}},{"cell_type":"code","source":"print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pandas Profiling Report","metadata":{}},{"cell_type":"code","source":"# !pip install pandas-profiling==2.7.1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(df, title = \"Pandas Profiling Report\",html = {'style' : {'full_width' : True}})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile.to_notebook_iframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the number of unique values in each column","metadata":{}},{"cell_type":"code","source":"dict = {}\nfor i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Info of Dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for duplicate rows","metadata":{}},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing the duplicates","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking new shape","metadata":{}},{"cell_type":"code","source":"print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## List of continuous and categorical features and output feature","metadata":{}},{"cell_type":"code","source":"cont_features = [i for i in df.columns if df[i].nunique()>5]\ncat_features = [i for i in df.columns if df[i].nunique()<=5]\ntarget_feature = [\"output\"]\nprint(\"The categorial cols are : \", cat_features)\nprint(\"The continuous cols are : \", cont_features)\nprint(\"The target variable is :  \", target_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Describing the Dataset","metadata":{}},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking null values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking how many classes in target variable ","metadata":{}},{"cell_type":"code","source":"df['output'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing the correlation matrix","metadata":{}},{"cell_type":"code","source":"df.corr().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df1 = df.copy()\not = {0: \"Less chance of HA\",1:'More chance of HA'}\ndf1.output = [ot[item] for item in df1.output]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(data=df, x='output',palette=['#85bfdc','#f64c72'])\nax.set(xticklabels=['less chance of heart attack', 'more chance of heart attack'],title=\"Target Distribution\")\nax.tick_params(bottom=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df1, x=\"age\",color=\"output\",\n                   marginal=\"box\",\n                   hover_data=df.columns,\n                  color_discrete_sequence=['#f64c72','#85bfdc'])\nfig.update_layout(\n    title=\"Heart attack chance corresponding to age\"\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> This comes as a surprise that in this data the mean age is lesser for higher chance of heart attack\n> ","metadata":{}},{"cell_type":"code","source":"more = df[df['output']==1]['trtbps']\nless = df[df['output']==0]['trtbps']\nfig = ff.create_distplot([less, more],['less chance of heart attack', 'more chance of heart attack']\n                         , show_hist=False, \n                        colors=['#85bfdc','#f64c72'])\nfig.update_layout(\n    title=\"Heart Attack chance corresponding to resting heart rate\",\n    xaxis_title=\"Resting heart rate\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Some, features like resting heart rate are indifferent to chances of heart attack\n","metadata":{}},{"cell_type":"code","source":"more = df[df['output']==1]['thalachh']\nless = df[df['output']==0]['thalachh']\nfig = ff.create_distplot([less, more],['less chance of heart attack', 'more chance of heart attack']\n                         , bin_size=5,\n                        colors=['#85bfdc','#f64c72'])\nfig.update_layout(\n    title=\"Heart Attack chance corresponding to maximum heart rate achieved\",\n    xaxis_title=\"Maximum heart rate achieved\",\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Here, we can clearly see that maximum heart rate is directly proportional to the chances of heart attack\n","metadata":{}},{"cell_type":"code","source":"fig = px.box(df1, x=\"cp\", y=\"chol\",color='output',color_discrete_map={'Less chance of HA':'#85bfdc','More chance of HA':'#f64c72'})\nfig.update_layout(title=\"Effects of cholestrol corresponding to chest pain type on chances of heart attack\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = df.drop(['sex','cp','fbs','exng','restecg','exng','thall','caa','slp'], axis=1)\nfig, ax = plt.subplots(1, 1, figsize=(6,6))\ndf_cor = temp.corr()\nhalf = np.triu(np.ones_like(df_cor, dtype=np.bool))\n\nmy_colors = ['#85bfdc','#f64c72']\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom', my_colors)\n\nheatmap = sns.heatmap(df_cor, \n            square=True, \n            mask=half,\n            linewidth=2.5, \n            vmax=0.4, vmin=0, \n            cmap=cmap, \n            cbar=False, \n            ax=ax,annot=True)\n\nheatmap.set(title=\"Heatmap of continous variables\")\nheatmap.set_yticklabels(heatmap.get_xticklabels(), rotation = 0)\nheatmap.spines['top'].set_visible(True)\nfig.text(1.2, 0.85, '''* thalachh(Maximum heart rate achieved) is positively correlated while,\n* oldpeak is negatively correlated with the output ''', \n         fontweight='light', fontfamily='serif', fontsize=11, va='top', ha='right') \n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create dimensions","metadata":{}},{"cell_type":"code","source":"# Create dimensions\nexng = go.parcats.Dimension(\n    values=df.exng,label=\"exng\"\n)\n\ncp = go.parcats.Dimension(\n    values=df.cp,label=\"cp\"\n)\n\nfbs = go.parcats.Dimension(\n    values=df.fbs,label=\"fbs\"\n)\n\ngender_dim = go.parcats.Dimension(values=df.sex, label=\"sex\")\n\nrestecg = go.parcats.Dimension(values=df.sex, label=\"restecg\")\nthall = go.parcats.Dimension(values=df.sex, label=\"thall\")\ncaa = go.parcats.Dimension(values=df.sex, label=\"caa\")\nslp = go.parcats.Dimension(values=df.sex, label=\"slp\")\n\nsurvival_dim = go.parcats.Dimension(\n    values=df.output, label=\"Outcome\", categoryarray=[0, 1],\n    ticktext=['Less chance', 'More chance']\n)\n\n# Create parcats trace\ncolor = df.output;\ncolorscale = [[0, '#85bfdc'], [1, '#f64c72']];\n\nfig = go.Figure(data = [go.Parcats(dimensions=[exng,slp,restecg,fbs,thall,caa,cp,\n                                              gender_dim,survival_dim],\n        line={'color': color, 'colorscale': colorscale},\n        hoveron='color', hoverinfo='count+probability',\n        labelfont={'size': 18, 'family': 'Times'},\n        tickfont={'size': 16, 'family': 'Times'},\n        arrangement='freeform')])\nfig.update_layout(title=\"Plotly parallel categorical plot for all the categorical labels\", )\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lbs = ['sex','cp','fbs','exng','restecg','thall','caa','slp']\n\nrows = 3\ncols = 3\n\nsubplot_titles = [l for l in lbs]\n\nspecs=[[{\"type\": \"bar\"},{\"type\": \"bar\"},{\"type\": \"bar\"}],\n       [{\"type\": \"bar\"},{\"type\": \"bar\"},{\"type\": \"bar\"}],\n       [{\"type\": \"bar\"},{\"type\": \"bar\"},None]]\n\n\nfig = make_subplots(\n        rows=rows,\n        cols=cols,\n        subplot_titles=subplot_titles,\n        specs=specs,  \n        print_grid=False\n)\n\nfor i, b in enumerate(lbs):\n    row = i // cols + 1\n    col = (i % rows) + 1\n    name = lbs[i]\n    l = [(100)*df[df[name]==x]['output'].sum()/len(df[df[name]==x]['output']) \n         for x in range(len(df[name].value_counts().tolist()))]\n    fig.add_trace(go.Bar(\n    x = [x for x in range(len(df[name].value_counts().tolist()))],\n    y = l,\n    marker_color=['#85bfdc','#9999c9','#aa77aa','#cc6397','#f64c72'],\n    ),row=row,col=col)\n\nfig.update_layout(autosize = True,\n                  title=\"Percertage of people having 'more chance of heart attack' for each type\", \n                  title_x=0.5,\n                 showlegend=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">For certain categories the chances of heart attack was found high:-\n> - Age = 0\n> - cp = 2,3\n> - thall = 2\n> - caa = 0,4\n> - slp = 2","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(df1, x='oldpeak', y='thalachh', z='age',\n              color='output',size='trtbps',color_discrete_sequence=['#f64c72','#85bfdc'])\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(df1,\n                x='thalachh',\n                y= 'chol',\n                color='output',\n                facet_col='cp', \n                facet_row='sex',\n                color_discrete_sequence=['#f64c72','#85bfdc'], \n                )\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Packages ","metadata":{}},{"cell_type":"code","source":"# Scaling\nfrom sklearn.preprocessing import RobustScaler\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Models\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, confusion_matrix\n\n# Cross Validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nprint('Packages imported...')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making features model ready","metadata":{}},{"cell_type":"markdown","source":"## Scaling and Encoding features","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### Using MaxAbsScaler","metadata":{}},{"cell_type":"code","source":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = MaxAbsScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using MinMaxScaler","metadata":{}},{"cell_type":"code","source":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = MinMaxScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using RobustScaler","metadata":{}},{"cell_type":"code","source":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = RobustScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using StandardScaler","metadata":{}},{"cell_type":"code","source":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = StandardScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and test split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## Linear Classifiers","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"code","source":"# instantiating the object and fitting\nclf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n\n# predicting the values\ny_pred = clf.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of SVM is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Machines:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning of SVC","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nsvm = SVC()\n\n# setting a grid - not so extensive\nparameters = {\"C\":np.arange(1,10,1),'gamma':[0.00001,0.00005, 0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5]}\n\n# instantiating the GridSearchCV object\nsearcher = GridSearchCV(svm, parameters)\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# the scores\nprint(\"The best params are :\", searcher.best_params_)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of SVM after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of SVM after hyper-parameter tuning:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nlogreg = LogisticRegression()\n\n# fitting the object\nlogreg.fit(X_train, y_train)\n\n# calculating the probabilities\ny_pred_proba = logreg.predict_proba(X_test)\n\n# finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Logistric Regression is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tree Models","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"# instantiating the object\ndt = DecisionTreeClassifier(random_state = 42)\n\n# fitting the model\ndt.fit(X_train, y_train)\n\n# calculating the predictions\ny_pred = dt.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Decision Tree is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Decision Tree:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nrf = RandomForestClassifier()\n\n# fitting the model\nrf.fit(X_train, y_train)\n\n# calculating the predictions\ny_pred = dt.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Random Forest is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Classifier - without tuning","metadata":{}},{"cell_type":"code","source":"# instantiate the classifier\ngbt = GradientBoostingClassifier(n_estimators = 300,max_depth=1,subsample=0.8,max_features=0.2,random_state=42)\n\n# fitting the model\ngbt.fit(X_train,y_train)\n\n# predicting values\ny_pred = gbt.predict(X_test)\n# print(\"The test accuracy score of Gradient Boosting Classifier is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Gradient Boosting Classifie:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores=[]\nbest_estimators = {}\n\n\nmodel_params = {  \n    \n    \n    'KNeighborsClassifier': {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors': [2,3,4,5,6,7,18,19,20],\n            'algorithm' : ['auto','ball_tree'],\n            'weights' : ['uniform','distance'],\n            'leaf_size' : [27,28,29,30,31]\n        }\n    },\n    \n    'DecisionTreeClassifier': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            'max_depth' : [None,1,2,6,5]\n        }\n    },\n    \n    \n    'AdaBoostClassifier': {\n        'model': AdaBoostClassifier(),\n        'params': {\n            'n_estimators': [30,35,40,45,50,55],\n            'learning_rate' : [1,1.1,1.2,1.3,1.4,1.5],\n            'algorithm' : ['SAMME', 'SAMME.R']\n        }\n    },\n    \n    'GaussianNB': {\n        'model': GaussianNB(),\n        'params': {\n            \n        }\n    },\n    \n     'LOGISTIC_REGRESSION': {\n        'model': LogisticRegression(),\n        'params': {\n            'C': [1,2,3,4,5,6,7],\n            'solver' : [ 'liblinear', 'lbfgs'],\n            'multi_class' : ['auto', 'ovr' ]\n        }\n    },\n    \n        \n    'SVM': {\n        'model': SVC(),\n        'params': {\n             'C': [1,2,3,5,6,7],\n             'kernel': ['rbf','linear'],\n             'gamma': ['auto', 'scale']\n        }\n    },\n       \n    'RANDOM_FOREST':{\n        'model' : RandomForestClassifier(),\n        'params': {\n            'n_estimators':[1,2,3,4,5,10,15],\n            'criterion': ['entropy'],\n            'random_state' : [12,13],\n            'max_depth' : [5,6]\n\n        }\n    }\n}\n\nimport time\n\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    # print(mp['model'], mp['params'])\n    start_time = time.time()\n    \n    clf.fit(X_train, y_train)    \n    \n\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': str(clf.best_params_)\n    })\n    best_estimators[model_name] = clf.best_estimator_\n    # print(f'{(time.time() - start_time)/60} minutes')\n\nimport pandas as pd    \ndf3 = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf3 = df3.sort_values(by='best_score',ascending=False)\n# print(df3)\n\n# for i in df3['model'].values.tolist():\n#     print(i ,':', cross_val_score(best_estimators[i],X_test,y_test,cv=5).mean())\n#     print(i)\n\nplt.figure(figsize = (10,5))\nsns.barplot(x = df3['best_score'], y = df3['model'], palette='pastel')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}