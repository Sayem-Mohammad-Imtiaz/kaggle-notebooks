{"cells":[{"metadata":{"id":"jNKaJz5j_ylj"},"cell_type":"markdown","source":"# IMDB Reviews with Bert (1 epoch)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"Ok002ceNB8E7","outputId":"06ef90d2-7518-4209-da66-1dd45c357c78","trusted":true},"cell_type":"code","source":"import os\nfrom pytorch_transformers import BertTokenizer, BertConfig\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_transformers import BertTokenizer, BertConfig\nfrom pytorch_transformers import AdamW, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstop_words_en = set(stopwords.words('english'))\nstemmer_en = SnowballStemmer('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"oYsV4H8fCpZ-","outputId":"b8812c8e-3149-475f-b4c0-262160485c39","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cpu':\n    print('cpu')\nelse:\n    n_gpu = torch.cuda.device_count()\n    print(torch.cuda.get_device_name(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_data = '../input/imdb-dataset-of-50k-movie-reviews'\n# dir_models = '../models'\nname_file = 'IMDB Dataset.csv'\n# os.makedirs(dir_data, exist_ok=True)\n# os.makedirs(dir_models, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(dir_data, name_file))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    'TextPreprocessor': {\n        'del_orig_col': False,\n        'mode_stemming': True,\n        'mode_norm': True,\n        'mode_remove_stops': True,\n        'mode_drop_long_words': True,\n        'mode_drop_short_words': True,\n        'min_len_word': 3,\n        'max_len_word': 15,\n        'columns_names': 'review'        \n    },\n}\n\nclass TextPreprocessor(object):\n    def __init__(self, config):\n        \"\"\"Preparing text features.\"\"\"\n        self._del_orig_col = config.get('del_orig_col', True)\n        self._mode_stemming = config.get('mode_stemming', True)\n        self._mode_norm = config.get('mode_norm', True)\n        self._mode_remove_stops = config.get('mode_remove_stops', True)\n        self._mode_drop_long_words = config.get('mode_drop_long_words', True)\n        self._mode_drop_short_words = config.get('mode_drop_short_words', True)\n        self._min_len_word = config.get('min_len_word', 3)\n        self._max_len_word = config.get('max_len_word', 17)\n        self._max_size_vocab = config.get('max_size_vocab', 100000)\n        self._max_doc_freq = config.get('max_doc_freq', 0.8) \n        self._min_count = config.get('min_count', 5)\n        self._pad_word = config.get('pad_word', None)\n        self._columns_names = config.get('columns_names', None)\n\n    def _clean_text(self, input_text):\n        \"\"\"Delete special symbols.\"\"\"\n        input_text = input_text.str.lower()\n        input_text = input_text.str.replace(r'[^a-z ]+', ' ')\n        input_text = input_text.str.replace(r' +', ' ')\n        input_text = input_text.str.replace(r'^ ', '')\n        input_text = input_text.str.replace(r' $', '')\n        return input_text\n\n    def _text_normalization_en(self, input_text):\n        '''Normalization of english text'''\n        return ' '.join([lemmatizer.lemmatize(item) for item in input_text.split(' ')])\n\n    def _remove_stops_en(self, input_text):\n        '''Delete english stop-words'''\n        return ' '.join([w for w in input_text.split() if not w in stop_words_en])\n\n    def _stemming_en(self, input_text):\n        '''Stemming of english text'''\n        return ' '.join([stemmer_en.stem(item) for item in input_text.split(' ')])\n\n    def _drop_long_words(self, input_text):\n        \"\"\"Delete long words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) < self._max_len_word])\n\n    def _drop_short_words(self, input_text):\n        \"\"\"Delete short words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) > self._min_len_word])\n    \n    def transform(self, df):        \n        \n        df[self._columns_names] = df[self._columns_names].astype('str')\n        df['union_text'] = df[self._columns_names]\n            \n        if self._del_orig_col:\n            df = df.drop(self._columns_names, 1)\n    \n        df['union_text'] = self._clean_text(df['union_text'])\n        \n        if self._mode_norm:\n            df['union_text'] = df['union_text'].apply(self._text_normalization_en, 1)\n            \n        if self._mode_remove_stops:\n            df['union_text'] = df['union_text'].apply(self._remove_stops_en, 1)\n            \n        if self._mode_stemming:\n            df['union_text'] = df['union_text'].apply(self._stemming_en)\n            \n        if self._mode_drop_long_words:\n            df['union_text'] = df['union_text'].apply(self._drop_long_words, 1)\n            \n        if self._mode_drop_short_words:\n            df['union_text'] = df['union_text'].apply(self._drop_short_words, 1)\n            \n        df.loc[(df.union_text == ''), ('union_text')] = 'empt'\n\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = TextPreprocessor(config['TextPreprocessor']).transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [\"[CLS] \" + sentence[0:500] + \" [SEP]\" for sentence in df['union_text'].values]\nlabels = [[1] if i == 'positive' else [0] for i in df['sentiment'].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sentences[1000], labels[1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences, test_sentences, train_gt, test_gt = train_test_split(\n    sentences, \n    labels, \n    test_size=0.3, \n    random_state=123,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_gt), len(test_gt))","execution_count":null,"outputs":[]},{"metadata":{"id":"ex5O1eV-Pfct"},"cell_type":"markdown","source":"## Inputs","execution_count":null},{"metadata":{"id":"Z474sSC6oe7A","outputId":"fbaa8fd8-bccd-4feb-ce52-beba5d293cfa","trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\nprint (tokenized_texts[10])","execution_count":null,"outputs":[]},{"metadata":{"id":"Cp9BPRd1tMIo","trusted":true},"cell_type":"code","source":"input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]","execution_count":null,"outputs":[]},{"metadata":{"id":"Cp9BPRd1tMIo","trusted":true},"cell_type":"code","source":"input_ids = pad_sequences(\n    input_ids,\n    maxlen=250,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"Cp9BPRd1tMIo","trusted":true},"cell_type":"code","source":"attention_masks = [[float(i>0) for i in seq] for seq in input_ids]","execution_count":null,"outputs":[]},{"metadata":{"id":"aFbE-UHvsb7-","trusted":true},"cell_type":"code","source":"train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, train_gt, \n    random_state=123,\n    test_size=0.1\n)\n\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks,\n    input_ids,\n    random_state=123,\n    test_size=0.1\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"jw5K2A5Ko1RF","trusted":true},"cell_type":"code","source":"train_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)","execution_count":null,"outputs":[]},{"metadata":{"id":"GEgLpFVlo1Z-","trusted":true},"cell_type":"code","source":"train_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(\n    train_data,\n    sampler=RandomSampler(train_data),\n    batch_size=32\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(\n    validation_data,\n    sampler=SequentialSampler(validation_data),\n    batch_size=32\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"pNl8khAhPYju"},"cell_type":"markdown","source":"## Train the model","execution_count":null},{"metadata":{"id":"gFsCTp_mporB","outputId":"dd067229-1925-4b37-f517-0c14e25420d1","trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"id":"QxSMw0FrptiL","trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)","execution_count":null,"outputs":[]},{"metadata":{"id":"6J-FYdx6nFE_","outputId":"8e388ad1-f9db-4c7b-d080-6c0a0e964610","trusted":true},"cell_type":"code","source":"from IPython.display import clear_output\n\ntrain_loss_set = []\ntrain_loss = 0\n\n# Switch on training mode\nmodel.train()\n\nfor step, batch in enumerate(train_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    optimizer.zero_grad()\n    \n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n    train_loss_set.append(loss[0].item())  \n\n    loss[0].backward()\n    \n    optimizer.step()\n    train_loss += loss[0].item()\n    \n    clear_output(True)\n    plt.plot(train_loss_set)\n    plt.title(\"Training loss\")\n    plt.xlabel(\"Batch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    \nprint(\"Train Loss: {0:.5f}\".format(train_loss / len(train_dataloader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.save(model, os.path.join(dir_models, 'Bert_epoch_1'))","execution_count":null,"outputs":[]},{"metadata":{"id":"6J-FYdx6nFE_","outputId":"8e388ad1-f9db-4c7b-d080-6c0a0e964610","trusted":true},"cell_type":"code","source":"# Validate\n# Switch on evaluation mode\nmodel.eval()\n\nvalid_preds, valid_labels = [], []\n\nfor batch in validation_dataloader:   \n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)     \n    valid_preds.extend(batch_preds)\n    valid_labels.extend(batch_labels)\n\nprint(\"Valid ACC: {0:.2f}%\".format(\n    accuracy_score(valid_labels, valid_preds) * 100\n))","execution_count":null,"outputs":[]},{"metadata":{"id":"mkyubuJSOzg3"},"cell_type":"markdown","source":"# Test the model","execution_count":null},{"metadata":{"id":"mAN0LZBOOPVh","trusted":true},"cell_type":"code","source":"tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\ninput_ids = pad_sequences(\n    input_ids,\n    maxlen=250,\n    dtype=\"long\",\n    truncating=\"post\",\n    padding=\"post\"\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(test_gt)\n\nprediction_data = TensorDataset(\n    prediction_inputs,\n    prediction_masks,\n    prediction_labels\n)\n\nprediction_dataloader = DataLoader(\n    prediction_data, \n    sampler=SequentialSampler(prediction_data),\n    batch_size=32\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"Hba10sXR7Xi6","trusted":true},"cell_type":"code","source":"model.eval()\ntest_preds, test_labels = [], []\n\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    with torch.no_grad():\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n    logits = logits[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    batch_preds = np.argmax(logits, axis=1)\n    batch_labels = np.concatenate(label_ids)  \n    test_preds.extend(batch_preds)\n    test_labels.extend(batch_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_score = accuracy_score(test_labels, test_preds)\nprint('Test ACC: {0:.2f}%'.format(\n    acc_score*100\n))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}