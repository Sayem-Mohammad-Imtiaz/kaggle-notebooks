{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking data"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `id` column is not needed\n- `bmi` contains missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\ncont_features = ['age', 'avg_glucose_level', 'bmi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in cat_features + ['stroke']:\n    print(f + ':')\n    print(raw_data[f].value_counts(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['stroke'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- target variable `stroke` is highly imbalanced, stroke events represents only 5% of the data\n- `stroke` events represent individuals that had experienced stroke in the past, not those who are likely to experience it in the future\n- features are also imbalanced, `Other` (gender) and `Never_worked` (work type) and may be worth dropping\n- `Unknown` value of smoking status is basically a missing value\n- smoking status could be treated as an ordinal category - never smoked -> formerly smoked -> smokes"},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_data.copy()\n\n# drop not needed id column\n\ndata.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace Unknown with NA\n\ndata['smoking_status'].replace('Unknown', np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert categories for easier manipulation\n\nfor f in cat_features:\n    data[f] = data[f].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set smoking_status to be an ordinal category\n\ndata['smoking_status'] = data['smoking_status'].cat.reorder_categories(new_categories = ['never smoked', 'formerly smoked', 'smokes']).cat.as_ordered()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_summary = data.isna().sum()\ntotal_count = data.shape[0]\nprint('missing data ratio')\nprint('bmi:            {:.2f}'.format(na_summary['bmi'] / total_count))\nprint('smoking_status: {:.2f}'.format(na_summary['smoking_status'] / total_count))\n\nprint()\nprint('missing bmi vs. stroke')\nprint(data[data['bmi'].isna()]['stroke'].value_counts())\n\nprint()\nprint('missing smoking_status vs. stroke')\nprint(data[data['smoking_status'].isna()]['stroke'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `bmi` contains missing values - 4% of DS\n- `smoking_status` contains missing values - 30% of DS\n\nBoth groups of \"missing values\" contain rare positive stroke events. Therefore simple removal of these rows will also remove valuable stroke data. Filling NAs with fake values might be OK for bmi. Missing smoking statuses however represent a big chunk of data and fakes might have bad impact on this predictor."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in cat_features:\n    sns.countplot(x = data[f])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots show how imbalanced some features are. There are also visible minority classes like `Other` or `Never_worked`"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[['stroke'] + cont_features].sort_values('stroke'), hue='stroke', height=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- age seems to have the biggiest impact on stroke event\n- bmi looks less important\n- 2 clusters of glucose level visible (low, high)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use simple label encoding for further analysis\n\ndata_label_enc = data.copy()\nfor f in cat_features:\n    data_label_enc[f] = data_label_enc[f].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\nsns.heatmap(data_label_enc.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- correlation heatmap shows higher correlation between stroke and age, glucose level, hypertension, heart disease, ever married\n- lower correlation between stroke and gender, work type, residence type, bmi, smoking status\n- ever married is highly correlated to age which is natural since older people had been more likely married once"},{"metadata":{},"cell_type":"markdown","source":"# Data preparation"},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\ndata_fs = data.copy()\n\ndata_fs.dropna(inplace = True)\n\nfor f in cat_features:\n    data_fs[f] = data_fs[f].cat.codes\n\ndata_fs = MinMaxScaler().fit_transform(data_fs)\n\nX_fs = data_fs[:,:-1]\nY_fs = data_fs[:,-1:]\n\nfit = SelectKBest(score_func = chi2, k = 4).fit(X_fs, Y_fs)\n\ndf_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(data.columns)\nfeature_scores = pd.concat([df_columns, df_scores],axis = 1)\nfeature_scores.columns = ['feature','score']\nfs_results = feature_scores.nlargest(20,'score')\nfs_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most important features are hypertension, heart disease, age and glucose level. These results prove feature importance assumptions made in EDA. Despite ever married seems somehow important, I will not use it since it is highly correlated to age (and age looks like a better predictor)."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = fs_results['feature'][0:4].values\ndata_final_features = data.copy()[final_features]\ndata_final_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values\n\nLuckily I don't need to bother with missing values since `bmi` and `smoking_status` features were not selected"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final_features.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding categorical data\n\nSelected categorical features are binary so I will use simple label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_enc = data_final_features.copy()\n\nfor f in data_final_features.select_dtypes('category').columns:\n    data_enc[f] = data_enc[f].cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train/test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = data['stroke']\nX = data_enc.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data resampling\n\nStoke event is quite rare (5% of the data) and therefore dataset needs to be resampled. To do that I have decided to use method that combines both over and under-sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"dataset size: \" + str(y_train.size))\nprint(\"stroke ratio: \" + str(y_train.sum() / y_train.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.combine import SMOTEENN\n\nsmt = SMOTEENN(random_state=42, sampling_strategy = 0.7)\nX_train, y_train = smt.fit_resample(X_train, y_train)\n\nprint(\"dataset size: \" + str(y_train.size))\nprint(\"stroke ratio: \" + str(y_train.sum() / y_train.size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset now contains 40% of stroke events"},{"metadata":{},"cell_type":"markdown","source":"## Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, plot_confusion_matrix\n\nplot_confusion_matrix(model, X_test, y_test)  \n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# use grid search to tune hyper-parameters\n\nparam_grid = {\n    'bootstrap': [True, False],\n    'n_estimators': [100, 200, 400]\n}\n\nmodel = GridSearchCV(RandomForestClassifier(random_state = 42), param_grid, scoring = 'f1')\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmodel.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model, X_test, y_test)  \n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nI have trained two classification models on stroke-prediction dataset.\n\nAs a result of EDA and feature selection I have picked four features to train models:\n\n- categorical: `hypertension`, `heart_disease`\n- continuous: `age`, `avg_glucose_level`\n\n## Logistic regression classifier\n\nModel performed with **f1-score: 0.22**. Recall of this model is 0.73 and precision 0.13 which means there is a higher chance to predict stroke but with more false positive predictions.\n\n\n## Random forset classifier\n\nModel performed with **f1-score 0.24**. Recall was however much lower comparing to logistic regression - 0.59. Precision 0.15 is slightly higher. This results in less false positive predictions but more missed stroke events (true positive).\n\n# Conclusion\n\nI would say that in case of stroke prediction is higher recall much more important than higher precision. Stroke observations represent individuals that **had already experienced stroke** in the past not those who will experience it in the future. Therefore it is likely that part of false positive predictions is actually subject to experience stroke in the future."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}