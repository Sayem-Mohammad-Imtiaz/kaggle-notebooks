{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Load Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import SVC, LinearSVC\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nimport featuretools as ft\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom boruta import BorutaPy\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.externals import joblib \nfrom  xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Load Funtions"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spot-Check Algorithms "},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetBasedModel():\n    models = []\n    models.append(('LN', linear_model.LinearRegression())) #(Regression - Supervised)\n    models.append(('RID', Ridge())) #(Regression - Supervised)\n    models.append(('LSO', Lasso(max_iter=4000))) #(Regression - Supervised)\n    models.append(('EN',  ElasticNet())) #(Regression - Supervised)\n    models.append(('KNNR', KNeighborsRegressor(n_jobs=-1))) #(Regression - Supervised)\n    models.append(('CARTR', DecisionTreeRegressor())) #(Regression - Supervised)\n    models.append(('AB', AdaBoostRegressor())) #(Regression - Supervised)\n    models.append(('GBM',GradientBoostingRegressor())) #(Regression - Supervised)\n    models.append(('RFR', RandomForestRegressor(n_jobs=-1,n_estimators=100))) #(Regression - Supervised)\n    models.append(('ETR', ExtraTreesRegressor(n_jobs=-1,n_estimators= 100))) #(Regression - Supervised) \n    #models.append(('SVR', SVR(gamma='scale'))) #(Regression - Supervised)\n    models.append(('XGB', XGBRegressor(n_jobs=-1))) #(Regression - Supervised)\n\n    \n    return models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kfold "},{"metadata":{},"cell_type":"markdown","source":"If it is a Classification problem, scoring can be 'accuracy', 'neg_log_loss' , 'roc_auc' also we can use  confusion_matrix(Y_test, predicted) and classification_report(Y_test, predicted)\n\nIf it is a Regression problem, scoring can be 'neg_mean_absolute_error' , 'neg_mean_squared_error', 'r2'."},{"metadata":{"trusted":true},"cell_type":"code","source":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 2\n    scoring = 'neg_mean_absolute_error'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds, random_state=42)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PlotBox"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score Data Frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n    #elif nameOfScaler == 'Robust':\n        #scaler= RobustScaler()\n    #elif nameOfScaler == 'normalizer':\n        #scaler= Normalizer()\n        \n    pipelines = []\n    pipelines.append((nameOfScaler+'LN'  , Pipeline([('Scaler', scaler),('LN',  linear_model.LinearRegression())])))\n    pipelines.append((nameOfScaler+'RID'  , Pipeline([('Scaler', scaler),('RID',  Ridge())])))\n    pipelines.append((nameOfScaler+'LSO'  , Pipeline([('Scaler', scaler),('LSO',  Lasso(max_iter=4000))])))\n    pipelines.append((nameOfScaler+'EN'  , Pipeline([('Scaler', scaler),('EN',  ElasticNet())])))\n    pipelines.append((nameOfScaler+'kNNR'  , Pipeline([('Scaler', scaler),('KNNR',  KNeighborsRegressor())])))\n    pipelines.append((nameOfScaler+'CARTR'  , Pipeline([('Scaler', scaler),('CARTR',  DecisionTreeRegressor())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB', AdaBoostRegressor())])))\n    pipelines.append((nameOfScaler+'GBM'  , Pipeline([('Scaler', scaler),('GBM',GradientBoostingRegressor())])))\n    pipelines.append((nameOfScaler+'RFR'  , Pipeline([('Scaler', scaler),('RFR', RandomForestRegressor(n_jobs=-1,n_estimators= 100))])))\n    pipelines.append((nameOfScaler+'ETR'  , Pipeline([('Scaler', scaler),('ETR', ExtraTreesRegressor(n_jobs=-1,n_estimators= 100))])))\n    #pipelines.append((nameOfScaler+'SVR'  , Pipeline([('Scaler', scaler),('SVR',  SVR(gamma='scale'))])))\n    pipelines.append((nameOfScaler+'XGB'  , Pipeline([('Scaler', scaler),('XGB',  XGBRegressor(n_jobs=-1))])))\n    return pipelines","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Algorithm Tunning"},{"metadata":{},"cell_type":"markdown","source":"### Random Searh"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 2\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 2\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Start Playing "},{"metadata":{},"cell_type":"markdown","source":"#  Understand Your Data With Descriptive Statistics and Visualizations and cleaning it."},{"metadata":{},"cell_type":"markdown","source":"## Load Data Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/weatherHistory.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[['Summary', 'Precip Type', 'Temperature (C)',\n       'Apparent Temperature (C)', 'Humidity', 'Wind Speed (km/h)',\n       'Wind Bearing (degrees)', 'Visibility (km)', 'Loud Cover',\n       'Pressure (millibars)', 'Daily Summary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data=df.isnull()\nmissing_data.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for Categorical Values"},{"metadata":{},"cell_type":"markdown","source":"In this case we dont have categorical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with missing data"},{"metadata":{},"cell_type":"markdown","source":"In this case we dont have missing data"},{"metadata":{},"cell_type":"markdown","source":" This part needs to be analisy carefully because we need to know why we are droping and replacing the null values"},{"metadata":{},"cell_type":"markdown","source":"What we can do to deal with missing data?"},{"metadata":{},"cell_type":"markdown","source":" 1-drop data\n \n    a. drop the whole row\n    b. drop the whole column\n    \n 2-replace data\n \n    a. replace it by mean\n    b. replace it by frequency\n    c. replace it based on other functions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Histograms"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(bins=10, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ploting Box Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All together with a scaler plot matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter_matrix(df,figsize=(20,20))\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysing Correlation "},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.get_dummies(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.to_csv('df.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix= df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix['Humidity'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.heatmap(df.corr(), vmin=-1, vmax=1.0, annot=True)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boruta Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"#### Define X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X =  df.drop(['Humidity'],axis=1)\ny = df['Humidity']  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load X and y\n# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\nX = X.values\ny = y.values\ny = y.ravel()\n\n# define random forest classifier, with utilising all cores and\n# sampling in proportion to y labels\nrf = RandomForestRegressor(n_jobs=-1)\n\n# define Boruta feature selection method\nfeat_selector = BorutaPy(rf, n_estimators=5, verbose=2, random_state=42)\n\n# find all relevant features - 5 features should be selected\nfeat_selector.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('\\n Initial features: ',  df.drop(['Humidity'],axis=1).columns.tolist() )\nprint ('\\n Number of selected features:')\nprint (feat_selector.n_features_)\nfeature_df = pd.DataFrame( df.drop(['Humidity'],axis=1).columns.tolist(), columns=['features'])\nfeature_df['rank']=feat_selector.ranking_\nfeature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('\\n Top %d features:' % feat_selector.n_features_)\nprint (feature_df.head(feat_selector.n_features_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected =  df.drop(['Humidity'],axis=1).columns[feat_selector.support_]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_selected=df[selected]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_selected.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time to play with Algorithms"},{"metadata":{},"cell_type":"markdown","source":"## Define X_train, y_train,X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_selected.values\ny = df['Humidity'].values  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape , y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Algorithm Comparison "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)\nfig = pyplot.figure()\nfig.suptitle( ' Algorithm Comparison ' )\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scores Comparison "},{"metadata":{"trusted":true},"cell_type":"code","source":"basedLineScore = ScoreDataFrame(names,results)\nbasedLineScore.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scores Comparison After Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"#### Standard Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, y_train,models)\n\nscaledScoreStandard = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard], axis=1)\ncompareModels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithm Tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_selected.values\ny = df['Humidity'].values  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tunning Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Pipeline\nestimators=[]\nestimators.append(('Standardize',StandardScaler()))\nestimators.append(('rfr',RandomForestRegressor(n_jobs=-1)))\npipeline=Pipeline(estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare Hyperparameters\nhyperparameters={\n'rfr__criterion':['mse','mae'], \n'rfr__n_estimators':np.array([100]), \n'rfr__max_features':[ 'auto','sqrt','log2' ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune model using cross-validation pipeline\nrf=GridSearchCV(pipeline,param_grid=hyperparameters,cv=2,scoring = 'neg_mean_absolute_error',n_jobs=-1)\nresults=rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check best score and best params for the model \nprint(\"Best: %f using %s\" % (results.best_score_, results.best_params_))\n#means = results.cv_results_['mean_test_score']\n#stds = results.cv_results_['std_test_score']\n#params = results.cv_results_['params\n#for mean, stdev, param in zip(means, stds, params):\n    #print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Finalize Random Forest Regresor"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model=results.best_estimator_\nfinal_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate model on test data\npred=final_model.predict(X_test)\n\nprint(mean_absolute_error(y_test, pred))\n#print (r2_score(y_test, pred))\n#print (mean_squared_error(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":"1","nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"307.2px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}