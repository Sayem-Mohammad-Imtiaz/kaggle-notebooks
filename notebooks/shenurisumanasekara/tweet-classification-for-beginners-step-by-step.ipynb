{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-03T19:56:05.467245Z","iopub.execute_input":"2021-09-03T19:56:05.467649Z","iopub.status.idle":"2021-09-03T19:56:05.482725Z","shell.execute_reply.started":"2021-09-03T19:56:05.467561Z","shell.execute_reply":"2021-09-03T19:56:05.481785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport unidecode\nimport nltk\n\nfrom tensorflow import keras\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\npd.options.display.max_rows = 300\npd.options.display.max_columns = 300","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:05.484236Z","iopub.execute_input":"2021-09-03T19:56:05.484524Z","iopub.status.idle":"2021-09-03T19:56:08.05276Z","shell.execute_reply.started":"2021-09-03T19:56:05.484455Z","shell.execute_reply":"2021-09-03T19:56:08.051949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Dataset using pandas read_csv ","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv')\ntestset = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.05467Z","iopub.execute_input":"2021-09-03T19:56:08.055057Z","iopub.status.idle":"2021-09-03T19:56:08.234357Z","shell.execute_reply.started":"2021-09-03T19:56:08.055019Z","shell.execute_reply":"2021-09-03T19:56:08.233397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About Dataset","metadata":{}},{"cell_type":"markdown","source":"Lets now see how our dataset looks like and identify the important features","metadata":{}},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.235932Z","iopub.execute_input":"2021-09-03T19:56:08.236275Z","iopub.status.idle":"2021-09-03T19:56:08.253687Z","shell.execute_reply.started":"2021-09-03T19:56:08.236241Z","shell.execute_reply":"2021-09-03T19:56:08.252677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.255084Z","iopub.execute_input":"2021-09-03T19:56:08.255596Z","iopub.status.idle":"2021-09-03T19:56:08.261932Z","shell.execute_reply.started":"2021-09-03T19:56:08.255553Z","shell.execute_reply":"2021-09-03T19:56:08.260969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which dataset.shape describe how our dataset is. (Number of rows, Number of columns). WHich is there are 41157 data rows and 6 columns of features","metadata":{}},{"cell_type":"code","source":"dataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.263303Z","iopub.execute_input":"2021-09-03T19:56:08.263706Z","iopub.status.idle":"2021-09-03T19:56:08.294548Z","shell.execute_reply.started":"2021-09-03T19:56:08.263671Z","shell.execute_reply":"2021-09-03T19:56:08.293633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unique Values Of Sentiment Feature which shows our labels to classify**","metadata":{}},{"cell_type":"code","source":"dataset['Sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.295947Z","iopub.execute_input":"2021-09-03T19:56:08.296342Z","iopub.status.idle":"2021-09-03T19:56:08.307044Z","shell.execute_reply.started":"2021-09-03T19:56:08.296303Z","shell.execute_reply":"2021-09-03T19:56:08.306145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.Location.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.311292Z","iopub.execute_input":"2021-09-03T19:56:08.311573Z","iopub.status.idle":"2021-09-03T19:56:08.327039Z","shell.execute_reply.started":"2021-09-03T19:56:08.311548Z","shell.execute_reply":"2021-09-03T19:56:08.326239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like there are 12220 different locations.","metadata":{}},{"cell_type":"markdown","source":"**Now lets see about null values**","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.328477Z","iopub.execute_input":"2021-09-03T19:56:08.328746Z","iopub.status.idle":"2021-09-03T19:56:08.352406Z","shell.execute_reply.started":"2021-09-03T19:56:08.328721Z","shell.execute_reply":"2021-09-03T19:56:08.351259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 8590 null values in Location. Since location is categorical we can refill the null values with the most used feature value in that column. But since 8590 is a huge amount and if we fill those values our dataset will be highly biased towards that. So I will drop Loacation feature from the data base  ","metadata":{}},{"cell_type":"code","source":"dataset = dataset.drop(columns='Location')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.354077Z","iopub.execute_input":"2021-09-03T19:56:08.354533Z","iopub.status.idle":"2021-09-03T19:56:08.361726Z","shell.execute_reply.started":"2021-09-03T19:56:08.354469Z","shell.execute_reply":"2021-09-03T19:56:08.361034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.363199Z","iopub.execute_input":"2021-09-03T19:56:08.363796Z","iopub.status.idle":"2021-09-03T19:56:08.377062Z","shell.execute_reply.started":"2021-09-03T19:56:08.363758Z","shell.execute_reply":"2021-09-03T19:56:08.376136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check what we can identify from ScreenName and UserName feature","metadata":{}},{"cell_type":"code","source":"dataset.ScreenName.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.378475Z","iopub.execute_input":"2021-09-03T19:56:08.378851Z","iopub.status.idle":"2021-09-03T19:56:08.38784Z","shell.execute_reply.started":"2021-09-03T19:56:08.378812Z","shell.execute_reply":"2021-09-03T19:56:08.386925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.UserName.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.389273Z","iopub.execute_input":"2021-09-03T19:56:08.389668Z","iopub.status.idle":"2021-09-03T19:56:08.398472Z","shell.execute_reply.started":"2021-09-03T19:56:08.389629Z","shell.execute_reply":"2021-09-03T19:56:08.397521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both ScreenName and UserName have 41157 unique values. ","metadata":{}},{"cell_type":"markdown","source":"Lets take a look at Original Tweet Feature","metadata":{}},{"cell_type":"markdown","source":"# Lets Convert Sentiments in to factorial values","metadata":{}},{"cell_type":"code","source":"dataset['label'] = dataset.Sentiment.factorize()[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.399793Z","iopub.execute_input":"2021-09-03T19:56:08.400206Z","iopub.status.idle":"2021-09-03T19:56:08.410434Z","shell.execute_reply.started":"2021-09-03T19:56:08.400167Z","shell.execute_reply":"2021-09-03T19:56:08.409558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.411649Z","iopub.execute_input":"2021-09-03T19:56:08.412003Z","iopub.status.idle":"2021-09-03T19:56:08.426221Z","shell.execute_reply.started":"2021-09-03T19:56:08.411975Z","shell.execute_reply":"2021-09-03T19:56:08.425187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will rename the Sentiments into numbers\n* 0 - Neutral\n* 1 - Positive\n* 2 - Extremely Negative\n* 3 - Negative \n* 4 - Extremely Positive\n\n","metadata":{}},{"cell_type":"markdown","source":"# Data Visualaization","metadata":{}},{"cell_type":"code","source":"target_category = dataset['Sentiment'].unique()\ntarget_category","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.427109Z","iopub.execute_input":"2021-09-03T19:56:08.427409Z","iopub.status.idle":"2021-09-03T19:56:08.43876Z","shell.execute_reply.started":"2021-09-03T19:56:08.427382Z","shell.execute_reply":"2021-09-03T19:56:08.437986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby('Sentiment').label.count().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.439757Z","iopub.execute_input":"2021-09-03T19:56:08.440184Z","iopub.status.idle":"2021-09-03T19:56:08.453927Z","shell.execute_reply.started":"2021-09-03T19:56:08.440158Z","shell.execute_reply":"2021-09-03T19:56:08.452994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby(\"Sentiment\").Sentiment.count().plot.bar(ylim=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.454917Z","iopub.execute_input":"2021-09-03T19:56:08.455156Z","iopub.status.idle":"2021-09-03T19:56:08.627586Z","shell.execute_reply.started":"2021-09-03T19:56:08.455133Z","shell.execute_reply":"2021-09-03T19:56:08.626596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.Sentiment.value_counts().plot(kind='pie', y='label',figsize=(10,8),autopct='%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.62898Z","iopub.execute_input":"2021-09-03T19:56:08.629338Z","iopub.status.idle":"2021-09-03T19:56:08.761113Z","shell.execute_reply.started":"2021-09-03T19:56:08.629295Z","shell.execute_reply":"2021-09-03T19:56:08.760084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = dataset.OriginalTweet\ntweets.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.762627Z","iopub.execute_input":"2021-09-03T19:56:08.763063Z","iopub.status.idle":"2021-09-03T19:56:08.770547Z","shell.execute_reply.started":"2021-09-03T19:56:08.763022Z","shell.execute_reply":"2021-09-03T19:56:08.769739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Tweet set has lot of special charaters and unwanted stuff for data training. So lets do some data preprocessing","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def processing(text): \n    \n    \n#tokenization using keras text to word sequence tokenizer\n    tokenized_text = text_to_word_sequence(text)\n   \n        \n#stop word removal using remove_stopwords from gensim\n    text = ' '.join(tokenized_text)\n    text = text.replace(\"'\", \"\")\n    stop_word_removed_text = remove_stopwords(text)\n    \n        \n#remove numbers\n    number_removed_text = new_string = ''.join(filter(lambda x: not x.isdigit(), stop_word_removed_text))\n   \n        \n#remove extra white spaces\n    extra_whitespace_removed = word_tokenize(number_removed_text)\n    extra_whitespace_removed = number_removed_text.split()\n    \n        \n    extra_whitespace_removed = ' '.join(extra_whitespace_removed)\n    \n        \n#Convert Accented Characters(û -> u)\n    accented_removed_text = unidecode.unidecode(extra_whitespace_removed)\n  \n        \n#lemmatization\n    lemmatizer = WordNetLemmatizer()\n\n    def get_wordnet_pos(word):\n        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n        tag = nltk.pos_tag([word])[0][1][0].upper()\n        tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n\n        return tag_dict.get(tag, wordnet.NOUN)\n\n    lem_input = nltk.word_tokenize(accented_removed_text)\n    lem_text= ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lem_input])\n    \n       \n#stemming \n    stemmer= PorterStemmer()\n\n    stem_input= nltk.word_tokenize(lem_text)\n    stem_text=' '.join([stemmer.stem(word) for word in stem_input])\n   \n        \n#remove single letters\n    preprocessed_text = ' '.join( [w for w in stem_text.split() if len(w)>1] )\n    \n        \n    return preprocessed_text\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.771935Z","iopub.execute_input":"2021-09-03T19:56:08.772222Z","iopub.status.idle":"2021-09-03T19:56:08.783564Z","shell.execute_reply.started":"2021-09-03T19:56:08.772195Z","shell.execute_reply":"2021-09-03T19:56:08.782706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['OriginalTweet']=dataset['OriginalTweet'].apply(processing)  ","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:56:08.784904Z","iopub.execute_input":"2021-09-03T19:56:08.785276Z","iopub.status.idle":"2021-09-03T19:59:23.603405Z","shell.execute_reply.started":"2021-09-03T19:56:08.785238Z","shell.execute_reply":"2021-09-03T19:59:23.602362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = dataset['OriginalTweet']\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:23.606481Z","iopub.execute_input":"2021-09-03T19:59:23.606779Z","iopub.status.idle":"2021-09-03T19:59:23.613913Z","shell.execute_reply.started":"2021-09-03T19:59:23.606753Z","shell.execute_reply":"2021-09-03T19:59:23.613002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment = dataset.Sentiment","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:23.615248Z","iopub.execute_input":"2021-09-03T19:59:23.615516Z","iopub.status.idle":"2021-09-03T19:59:23.623584Z","shell.execute_reply.started":"2021-09-03T19:59:23.615489Z","shell.execute_reply":"2021-09-03T19:59:23.622873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split test/train sets","metadata":{}},{"cell_type":"markdown","source":"Lets now split the dataset into train and test sets baesd on 0.3 ratio which is 70% of data for training purpose and 30% of data for testing purpose. You can use 80%,20% ratio as well.  ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(tweets,sentiment, test_size = 0.3, random_state = 60,shuffle=True)\n\nprint(len(X_train))\nprint(len(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:23.624868Z","iopub.execute_input":"2021-09-03T19:59:23.625131Z","iopub.status.idle":"2021-09-03T19:59:23.646471Z","shell.execute_reply.started":"2021-09-03T19:59:23.625107Z","shell.execute_reply":"2021-09-03T19:59:23.645863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SGD Classifier","metadata":{}},{"cell_type":"markdown","source":"Let's train our model with SGD Classifier. For this I have used a pipeline with Tf-Idf Vectorizor which basically do the vectorization part. It will convert word in our preprocessed datset into a matrix of TF-IDF features. Based on those features the model will be trained.","metadata":{}},{"cell_type":"code","source":"sgd = Pipeline([('tfidf', TfidfVectorizer()),\n                ('sgd', SGDClassifier()),\n               ])\n\nsgd.fit(X_train, Y_train)\n\ntest_predict = sgd.predict(X_test)\n\ntrain_accuracy = round(sgd.score(X_train,Y_train)*100)\ntest_accuracy =round(accuracy_score(test_predict, Y_test)*100)\n\nprint(\"SGD Train Accuracy Score : {}% \".format(train_accuracy ))\nprint(\"SGD Test Accuracy Score  : {}% \".format(test_accuracy ))\nprint()\nprint(classification_report(test_predict, Y_test, target_names=target_category))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:23.64743Z","iopub.execute_input":"2021-09-03T19:59:23.64781Z","iopub.status.idle":"2021-09-03T19:59:26.330721Z","shell.execute_reply.started":"2021-09-03T19:59:23.647784Z","shell.execute_reply":"2021-09-03T19:59:26.330049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Set","metadata":{}},{"cell_type":"code","source":"testset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:26.331679Z","iopub.execute_input":"2021-09-03T19:59:26.331942Z","iopub.status.idle":"2021-09-03T19:59:26.343763Z","shell.execute_reply.started":"2021-09-03T19:59:26.331918Z","shell.execute_reply":"2021-09-03T19:59:26.342707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the data set with the sgd model","metadata":{}},{"cell_type":"code","source":"testset['OriginalTweet'] = testset['OriginalTweet'].apply(processing)\n\n\ntweet = testset['OriginalTweet']\ny_predict = sgd.predict(tweet)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:26.344963Z","iopub.execute_input":"2021-09-03T19:59:26.345275Z","iopub.status.idle":"2021-09-03T19:59:44.829694Z","shell.execute_reply.started":"2021-09-03T19:59:26.34525Z","shell.execute_reply":"2021-09-03T19:59:44.828829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentiments = testset['Sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:44.831085Z","iopub.execute_input":"2021-09-03T19:59:44.831445Z","iopub.status.idle":"2021-09-03T19:59:44.836665Z","shell.execute_reply.started":"2021-09-03T19:59:44.831408Z","shell.execute_reply":"2021-09-03T19:59:44.835684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Accuracy","metadata":{}},{"cell_type":"code","source":"test_accuracy =round(accuracy_score(test_sentiments, y_predict)*100)\nprint(\"SGD Classifier Test Accuracy Score  : {}% \".format(test_accuracy ))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:59:44.837918Z","iopub.execute_input":"2021-09-03T19:59:44.838225Z","iopub.status.idle":"2021-09-03T19:59:44.853416Z","shell.execute_reply.started":"2021-09-03T19:59:44.838199Z","shell.execute_reply":"2021-09-03T19:59:44.852282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since this is for the begginers I have only used SGD Classifier. I'll soon come with a LSTM model for this one. Hope you learned something. Please do upvote if you learned anything and leave a feedback. Good Luck!**","metadata":{}}]}