{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None) #display all the columns\ndf=pd.read_csv(\"/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now to understand what each column means**\n1. sl_no -> Serial number for every row \n2. gender - > gender of the person , M for male , F for female\n3. ssc_p ->  10th Grade percentage \n4. ssc_b ->  10th Grade Board of Education - Central or Others \n5. hsc_p -> 12th Grade percentage\n6. hsc_b -> 12th Grade Board of Education- Central or Others\n7. hsc_s -> Specialization in Higher Secondary Education \n8. degree_p -> Degree Percentage\n9. degree_t -> Degree type \n10. workex -> Work Experience\n11. etest_p -> Employability test percentage\n12. specialisation -> Post Graduation(MBA)- Specialization\n13. mba_p -> MBA percentage\n14. status -> Status of placement\n15. salary -> Salary offered if placed ","metadata":{}},{"cell_type":"markdown","source":"Now that we have a clear picture about the meaning of each column, we can now unerstand what should be our end goal \n\nFor the above given data we can have a total of 3 tasks \n1. Given the data , predict wether a candidate will get placed \n2. Predict what kind of placement package will be offered to the candidate - > can be used for negotiation \n3. Predict wether the candidate will get placed if yes then go predict his salary \n","metadata":{}},{"cell_type":"markdown","source":"> Performing EDA to better understand the data","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that salary has null values , this is because the candidate didn't get placed , so naturally their salary value will be empty / 0\n","metadata":{}},{"cell_type":"code","source":"#fill the null values in the dataset \ndf[\"salary\"]=df[\"salary\"].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ***Visulaizing the given data***","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = \"gender\", hue = \"gender\" ,data = df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = [\"gender\",\"ssc_b\",\"hsc_b\",\"hsc_s\",\"degree_t\",\"workex\",\"specialisation\",\"status\"]\nplt.figure(figsize = (25, 20))\nplotnumber = 1\n\nfor col in categorical:\n    if plotnumber <= 9: \n        ax = plt.subplot(3, 3, plotnumber)\n        sns.countplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label Encoding the categorical columns ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in categorical:\n    df[i]=le.fit_transform(df[i]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will check for any outliers in the dataset ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor col in df.columns:\n    if plotnumber <= 8:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.boxplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n    \n    plotnumber += 1\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\ncorr = df.corr()\ngraph = go.Figure()\ngraph.add_trace(go.Heatmap(z=corr.values, x=corr.index.values, y=corr.columns.values))\ngraph.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we cam see that there are a couple of outliers in the dataset ,we can use the following ways to deal with them \n1. Choose a model which is not Affected by outliers \n2. Remove or transform the data -> we can't remove the data since we only have 215 rows","metadata":{}},{"cell_type":"markdown","source":"However we have another issue with the given dataset , that is imbalenced data distribution\n1. To deal with this we will be using the smote and smogn packages \n2. SMOTE is used for classification\n3. SMOGN is used for regression ","metadata":{}},{"cell_type":"markdown","source":"But first we will be dropping the unnessary columns present in the dataset and create two seperate dataset , one for regression and other for classification","metadata":{}},{"cell_type":"code","source":"import copy\ndf=df.drop(\"sl_no\",axis=1)\ndf_reg = copy.deepcopy(df)         \ndf_class = copy.deepcopy(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_class=df_class.drop(\"salary\",axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In notebooks it is not necessary to use the head functions to view the data\n1. The reason i split the dataframes is so that we can deal with the imbalence problem easily","metadata":{}},{"cell_type":"code","source":"!pip install smogn","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smogn\ndf_reg_smogn = smogn.smoter(\n    data = df_reg,       #dataset \n    y = 'salary'         #label for the prediction ,i.e in our case salary\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df_reg['salary'], label = \"Original\")           #inblue\nsns.kdeplot(df_reg_smogn['salary'], label = \"Modified\")     #in orange","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Now that we have dealt with the imbalance issue we can move onto splitting the data and scaling it\n2. Since our dataset dosen't have many rows i have avoided dropping the rows with outliers in it\n3. the above is also the reason i have reduced the test_size","metadata":{}},{"cell_type":"code","source":"X = df_reg_smogn.iloc[:,:-1].values\ny = df_reg_smogn.iloc[:,-1].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have dealt with the imbalance problem for the regression dataset and aslo split it into training and testing, we will now move on to select a model ","metadata":{}},{"cell_type":"markdown","source":"There are a lot of models to select from , but remember that our data has outliers present in it \n1. This means we can't select models that are sensitive to outliers -> this means treebased models or XGB  model and a few others are viable\n2. Some models don,t require scaling so, it's better to avoid it whenever it's not necessary\n3. When confused to select a model simply call the lazypredict\n4. Lazypredict trains models which require scaling , so we will scale the data before calling it ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lazypredict","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Lazypredict trains the data on all the default sklearn models and returns the metrics report as shown bellow ,it is useful when you have no idea on what model to use.\n2. However it only trains the base models , we can improve the accuracy of the models listed with hyper parameter optimization\n","metadata":{}},{"cell_type":"code","source":"from lazypredict.Supervised import LazyRegressor\nreg = LazyRegressor(ignore_warnings=True, custom_metric=None)     \nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see that the XGBRegressor and the RandomForest model are the ones with the highest accuracy , this is only to be ecpected since both the models aren't affected by outliers ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(verbosity=0) \nxgbr.fit(X_train,y_train)\n#bellow is the Adjusted R-Squared for the model\n1 - (1-xgbr.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. I will not be optimising any hyperparameters simply because Adjusted R-Squared value of .94 is pretty good\n2. However the data available is extremely limited so , the actual performance of the model is questionable","metadata":{}},{"cell_type":"markdown","source":"Now to move onto classification","metadata":{}},{"cell_type":"code","source":"X = df_class.iloc[:,:-1].values\ny = df_class.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\nprint(X_train.shape,X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calling the lazypredict again","metadata":{}},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nclf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train,y_test)\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets try doing that again but this time using smote","metadata":{}},{"cell_type":"code","source":"X = df_class.iloc[:,:-1].values\ny = df_class.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\nprint(X_train.shape,X_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nX_train, y_train = sm.fit_resample(X_train, y_train.ravel())\nprint(X_train.shape,X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nclf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train,y_test)\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a small improvement in the balanced accuracy ","metadata":{}},{"cell_type":"markdown","source":"Now to train the model present and optimize it","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report\nbc = BernoulliNB()\nbc.fit(X_train,y_train)\nprint(classification_report(y_test,bc.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to improve the performance of the model with hyperparameter optimisation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score\nli = np.arange(0.0, 10.0, 0.1)    \nli_ = [i for i in range(10)]\n\n\nparams = {'alpha': li ,\n         'binarize' : li_,\n         'fit_prior' : [True,False]\n         }\n\n\nbernoulli_nb_grid = GridSearchCV(BernoulliNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5 ,scoring = 'balanced_accuracy')\nbernoulli_nb_grid.fit(X_train,y_train)\n\n\n\nprint('Test Accuracy : %.3f'%bernoulli_nb_grid.best_estimator_.score(X_test, y_test))\n\n\nprint('Best Accuracy Through Grid Search : %.3f'%bernoulli_nb_grid.best_score_)\nprint('Best Parameters : ',bernoulli_nb_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will try o keep updating the notebook as time goes on , Thanks for your time ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}