{"cells":[{"metadata":{"_uuid":"b1302704c74b006f02d10704d0fcf8aec706788a"},"cell_type":"markdown","source":"# Classification of Stars, Galaxies and Quasars with a simple random forest\n## Dataset from the Sloan Digital Sky Survey RD14\n---"},{"metadata":{"_uuid":"c4da3056e70c791cbabc9f6f5cc3c241f7d311d8"},"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, a random forest will perform a task of classification (galaxy, quasar or star) based on space observations. The data come from  the **Sloan Digital Sky Survey** (release 14). For more information on this dataset, [here is the link to its \"Overview\" section on kaggle](https://www.kaggle.com/lucidlenn/sloan-digital-sky-survey/home).  \nYou can also visit the SDSS website for even more information: https://www.sdss.org/.\n\nSo, let's get started by importing libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = FutureWarning)\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3aea8982301dabcf73b33023a15416a495b01a40"},"cell_type":"markdown","source":"## Data exploration\n\nLet's import the data in a Dataframe in order to have a rough idea of what the dataset looks like."},{"metadata":{"trusted":true,"_uuid":"42d418bf2e6ae281c5899f9d550fbf48d7366ba9"},"cell_type":"code","source":"sdss_pd = pd.read_csv(\"../input/Skyserver_SQL2_27_2018 6_51_39 PM.csv\")\nsdss_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97034b47abfb1a693363eabb529c5c3f6a475530"},"cell_type":"code","source":"sdss_pd.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a75c26461b42ee307fbd8b1c5490e9c3a4beb163"},"cell_type":"markdown","source":"Ok, so 17 feature columns with various types (**int64**, **float64**) and 1 target column. Fortunately, no missing values. Let's see how many examples we have for each category:"},{"metadata":{"trusted":true,"_uuid":"39b122a710ea4d0ef94602e51a23e15d35c5f575"},"cell_type":"code","source":"sdss_pd[\"class\"].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4ea4b668ab7e3d4af000b8fb05ea408bc2b7f01"},"cell_type":"markdown","source":"Almost **50%** of the examples are **GALAXY**, whereas **QUASAR** is less than **10%** of the examples. This inequality in the repartition of the labels may be a problem later."},{"metadata":{"trusted":true,"_uuid":"8148e07e76112dee1111136fa22e74d2310c9e2e"},"cell_type":"code","source":"sdss_pd.columns.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5741b594d6f0b9ae823ccff6db05bd3058d9c67c"},"cell_type":"markdown","source":"There are already a few features that we can guess are not useful for the classification task. Both **objid** and **specobjid** are just identifers in the original dataset. Moreover, features related to the camera (**run**, **rerun**, **camcol**, **field**) can also be dropped:"},{"metadata":{"trusted":true,"_uuid":"6d36e1103c1a33dabd790fd9ebcab5b5f644b1b0"},"cell_type":"code","source":"sdss_pd.drop([\"objid\",\"specobjid\",\"run\",\"rerun\",\"camcol\",\"field\"], axis = 1, inplace = True)\nsdss_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce8730ce26aa3be05e123175fc012722829a78c7"},"cell_type":"markdown","source":"We need to transform the class column in a more convenient way than a list of string. Here, we will transform the 3 strings into 3 integers:"},{"metadata":{"trusted":true,"_uuid":"4cd5f932bb33904b3eb1d102a541d79c27c7b8c6"},"cell_type":"code","source":"print(\"Mapping: \", dict(enumerate([\"GALAXY\",\"QSO\",\"STAR\"])))\nsdss_pd[\"class\"] = sdss_pd[\"class\"].astype(\"category\")\nsdss_pd[\"class\"] = sdss_pd[\"class\"].cat.codes\nprint(sdss_pd[\"class\"].value_counts().sort_index())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40c494463d8a7e7df3a1a78e44a176cf01380516"},"cell_type":"markdown","source":"Now, we can make some computations, for example with the correlation matrix."},{"metadata":{"trusted":true,"_uuid":"4e1dac4ba9774bb9125cd146bf52cb39cc36bcc1"},"cell_type":"code","source":"corr_matrix = sdss_pd.corr()\ncorr_matrix[\"class\"].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3a6d97fc6d42ca7f0208e6b1f0ef24873f9d65d"},"cell_type":"markdown","source":"The **correlation coefficient** is a rough representation of how much a variable is **related** to another: it ranges from -1 to 1, extremum mean complete correlation and 0 means no relation.  \nFrom the output, 2 features seem very correlated: **mjd** and **plate**. With the random forest, we will see if these relations are meaningful."},{"metadata":{"_uuid":"94527b7873960406315019674d1795c941072a8e"},"cell_type":"markdown","source":"We are going to separate the target column from the others and split the dataset into a training set and a test set with a stratified method where the repartition of each label in both sets is the same as the original one to limit bias."},{"metadata":{"trusted":true,"_uuid":"95a781a24a207dacee161c4d4fa3e0d13495419f"},"cell_type":"code","source":"sdss_feat = sdss_pd.drop(\"class\", axis = 1)\nsdss_labels = sdss_pd[\"class\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db016daedc4dc60cf1a26c7f574a8272d498b010"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(sdss_feat, sdss_labels, test_size=0.2, random_state=42, stratify=sdss_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b8ca81cfcb6155bbfe6f2e30384a4aaf8bfd4f0"},"cell_type":"markdown","source":"For now,  the values will not be scaled, as scaling is not mandatory for the random forest. But it can still give room for possible improvements."},{"metadata":{"_uuid":"03a7e20b4a98d75ef4f63b03ba320db860f1ad0d"},"cell_type":"markdown","source":"## Training the model and evaluate it\n\nLet's see what a random forest with the default values of scikit can achieve. It is trained on the whole training set and evaluated on the test set."},{"metadata":{"trusted":true,"_uuid":"bec03e6357e9e41859869f64cae33fcc3cc025ed"},"cell_type":"code","source":"default_forest = RandomForestClassifier(random_state = 42)\ndefault_forest.fit(X_train, y_train)\ndefault_forest.get_params()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c26430e959ed41829c3330ea7a535e52b31e35ca"},"cell_type":"markdown","source":"We can evaluate some metrics with the test set:"},{"metadata":{"trusted":true,"_uuid":"efa70a78d870db11aa5dbe90027bbe620d350129"},"cell_type":"code","source":"print(\"Test accuracy for default forest:\", default_forest.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a726c2489d83c536a829a85d8b6bc6fc14b901b"},"cell_type":"code","source":"y_pred = default_forest.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix_pd = pd.DataFrame(data = conf_matrix, \n                              index = [\"GALAXY\",\"QSO\",\"STAR\"],\n                              columns = [\"GALAXY\",\"QSO\",\"STAR\"])\nconf_matrix_pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b6aaa37a2c92c58cb9d2f6876907306071e0190"},"cell_type":"markdown","source":"Finally, we can plot the importance of each feature:"},{"metadata":{"trusted":true,"_uuid":"7fea8b81a2397f4cac65df4c25c313c8265686f1"},"cell_type":"code","source":"feat_imp_pd = pd.DataFrame(data = default_forest.feature_importances_,\n                          index = sdss_feat.columns,\n                          columns = [\"Importance\"])\nfeat_imp_pd = feat_imp_pd.sort_values(by = 'Importance', ascending = False)\nfeat_imp_pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0845410b8397014867b37913963d3dff8d92e6a"},"cell_type":"code","source":"feat_imp_pd.plot(kind = \"bar\", figsize = (10,5), grid = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53b525940ae0fe5a50b3559d7018de10397857f0"},"cell_type":"markdown","source":"As predicted before, **mjd** and **plate** are two important features but the most important one is the **redshift**. The correlation matrix did not point out this relation.\nMoreover, 3 features have less than 1%: **dec**, **fiberid** and **ra**. We can consider the possibility to drop these features.\nFinally, with the relative importance of **i**, **g**, **z**, **u** and **r**, dimension reduction can also be an option."},{"metadata":{"_uuid":"c8192d78afee1501258aa6c18c29e274d68cab40"},"cell_type":"markdown","source":"## Deeper training\n\nTo improve the model, we perform at the same time a random search for hyperparemeters optimization and a 5-fold cross validation. The hyperparameters that we try to optimize here are:\n* **n_estimators**: Number of trees in the forest\n* **max_features**: Number of features the tree can use at every split\n* **max_depth**: Maximum level of a tree\n* **min_samples_split**: Minimum number of samples required to split a node\n* **min_samples_leaf**: Minimum number of samples in a leaf node\n* **bootstrap**: Method to choose the samples for training each tree  \n\nWe are going to to train 25 models with different hyperparemeters. With the 4-fold CV, the total is 100 models to train."},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6862b71f042600e670f48f82ec6a36b13e6b6db2"},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\nmax_features = ['auto', 'log2']\nmax_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               scoring = 'accuracy', \n                               n_iter = 25, \n                               cv = 4, \n                               verbose = 2, \n                               random_state = 42,\n                               n_jobs = -1)\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fd19b1156fd20e0ecac858beae1f58cdd207c7"},"cell_type":"code","source":"rf_random.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0698cc686d400484c1609be54a90f43085a51632"},"cell_type":"code","source":"best_forest = rf_random.best_estimator_\nbest_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e509e8396cad18586d9f966a406ed80ac24123fc"},"cell_type":"code","source":"print(\"Test accuracy for best forest:\", best_forest.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"973bc4d67c6a993c3cc928c762fa88b76b969f33"},"cell_type":"code","source":"y_pred = best_forest.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix_pd = pd.DataFrame(data = conf_matrix,\n                              index = [\"GALAXY\",\"QSO\",\"STAR\"],\n                              columns = [\"GALAXY\",\"QSO\",\"STAR\"])\nconf_matrix_pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39847c74599c7a835fab27a706fffd2c0dcf4477"},"cell_type":"code","source":"feat_imp_pd = pd.DataFrame(data = best_forest.feature_importances_,\n                           index = sdss_feat.columns,\n                           columns = [\"Importance\"])\nfeat_imp_pd = feat_imp_pd.sort_values(by = 'Importance', ascending = False)\nfeat_imp_pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0bc891058d4f1b7704d8c100f5b83cdd6c1dec0"},"cell_type":"code","source":"feat_imp_pd.plot(kind = \"bar\", figsize = (10,5), grid = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de683b755299feb423f0bab8f917b97b8c239853"},"cell_type":"markdown","source":"The model obtained with a random search performs better than the default one from scikit-learn: **98.875%** against **99.1%**  which is more than **0.2%** of improvement. The confusion matrix has the same structure as the previous one (e.g. a recall of 100% for the \"GALAXY\" class). The bar chart for the feature importance confirms what has been stated for the previous one and suggests more processing on the features."},{"metadata":{"_uuid":"819b9cea87eff616b507a9921503483b0f974e55"},"cell_type":"markdown","source":"## Further developments\n\nSome ways of improvement have been stated before, let's summarize the ones which are available here.\n* **Scaling** the values of each feature. Generally, scaling helps improving the model performances. *Normalization* or *standardization* can be performed.\n*  **Dropping** more features like *dec*, *fiberid* and *ra*.\n* **Feature engineering** and **Feature extraction** e.g. *PCA* (Principal Component Analysis).\n* **Using different metrics**. Here, we evaluated the performances of our model mostly on the accuray and a bit on the confusion matrix. However, different metrics exist and can be revelant according to what you want to achieve: *recall*, *precision*, *F1 score*, *ROC AUC*."},{"metadata":{"_uuid":"afeeab134afc4bc8cadcd91602f2ace96a1af459"},"cell_type":"markdown","source":"## Conclusion\n\nDuring this notebook, I tried to present the workflow of a machine learning problem, from data analysis to testing the model in a simple way. We explored the SDSS RD14 dataset, selected the revelant features at first sight by analyzing them. Then we created a simple random forest classifier which we tried to improve and took some insights from. Finally, we gave some paths to explore in order to enhance the model.  \nAs I am not an expert and as I am still learning new concepts constantly on machine learning and trying to apply them to real world problems, I found this dataset interesting to apply some of the techniques I have learned."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}