{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Detection from X Ray Images of Lungs"},{"metadata":{},"cell_type":"markdown","source":"# Accuracy Graph of DL Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/graph-of-model/plot.png', width=800) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can find the dataset used in training of this Deep Learning Model from [Open Source Kaggle Dataset of 5000 Lungs X Ray Images +ve COVID and 5000 Healthy Persons Lungs Images](https://www.kaggle.com/nabeelsajid917/covid-19-x-ray-10000-images)"},{"metadata":{},"cell_type":"markdown","source":"# Accessing dataset from the Kaggle\nDownload the whole directory available [here](https://www.kaggle.com/nabeelsajid917/covid-19-x-ray-10000-images).\n* Create a New Python Local Environment and paste all th files in main directory of local environment.\n* Check the file named \"requirements.txt\". This file contains the information about all the libraries we gonna use in this process.\n* run the command ***pip install -r requirements.txt*** this will install all the required libraries we needed."},{"metadata":{},"cell_type":"markdown","source":"* Now Open \"generate_images.py\" file of directory"},{"metadata":{},"cell_type":"markdown","source":"# Lets dive into generate_images.py"},{"metadata":{},"cell_type":"markdown","source":"* from line 2-9 we are importing necessary packages needed."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# import the necessary packages\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nimport numpy as np\nimport argparse\nimport cv2\nimport os\nfrom imutils import paths\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from line 12-19 we are setting the command line arguments."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-d\", \"--dataset\", required=True,\n\thelp=\"path to input dataset\")\nap.add_argument(\"-o\", \"--output\", required=True,\n\thelp=\"path to output directory to store augmentation examples\")\nap.add_argument(\"-t\", \"--total\", type=int, default=100,\n\thelp=\"# of training samples to generate\")\nargs = vars(ap.parse_args())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In line 22 we are taking all the image paths in a list\n* In line 23 we are initializing an array to store all the images "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"imagePaths = list(paths.list_images(args[\"dataset\"]))\ndata = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 26-34 we are opening all the paths of images one by one and than appending images into data array."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# loop over the image paths\nfor imagePath in imagePaths:\n\t# extract the class label from the filename\n\tlabel = imagePath.split(os.path.sep)[-2]\n\n\t# load the image, swap color channels, and resize it to be a fixed\n\t# 224x224 pixels while ignoring aspect ratio\n\timage = cv2.imread(imagePath)\n\t# update the data and labels lists, respectively\n\tdata.append(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 35-40 we are converting image to array so that we can apply data augmentation on it."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for image in data:\n\t# load the input image, convert it to a NumPy array, and then\n\t# reshape it to have an extra dimension\n\tprint(\"[INFO] loading example image...\")\n\timage = img_to_array(image)\n\timage = np.expand_dims(image, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 44-52 we are doing data augmentation"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(\n\t\trotation_range=30,\n\t\tzoom_range=0.15,\n\t\twidth_shift_range=0.2,\n\t\theight_shift_range=0.2,\n\t\tshear_range=0.15,\n\t\thorizontal_flip=True,\n\t\tfill_mode=\"nearest\")\n\ttotal = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 54-57 we are passing each image on by one to our data augmentation object and saving it in \"jpg\"format. and rest code loop over all images and check if we reached a specifies number of examples which is 100 by default."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# construct the actual Python generator\n\tprint(\"[INFO] generating images...\")\n\timageGen = aug.flow(image, batch_size=1, save_to_dir=args[\"output\"],\n\t\tsave_prefix=\"image\", save_format=\"jpg\")\n\t# loop over examples from our image data augmentation generator\n\tfor image in imageGen:\n\t\t# increment our counter\n\t\ttotal += 1\n\n\t\t# if we have reached the specified number of examples, break\n\t\t# from the loop\n\t\tif total == args[\"total\"]:\n\t\t\tbreak","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* and these commands one by one."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"$ python generate_images.py --dataset dataset/covid --output generated_dataset/covid\n$ python generate_images.py --dataset dataset/normal --output generated_dataset/normal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE**: *Wait for some time because these commands will create data augmentation and will create around 10000 images. On a good GPU it will take a few minutes*.   "},{"metadata":{},"cell_type":"markdown","source":"Now you will be able to see the images dataset in the \"generated-data\" folder(containing 2 folders covid & normal) which we are gonna use in this whole process."},{"metadata":{},"cell_type":"markdown","source":"# Lets Train the Deep Learning Model "},{"metadata":{},"cell_type":"markdown","source":"* Now open the file in \"train_covid19.py\" in editor"},{"metadata":{},"cell_type":"markdown","source":"# Lets dive into train_covid19.py"},{"metadata":{},"cell_type":"markdown","source":"* From line 4-24 we are importing necessary packages need for this file"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport cv2\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 26-33 we are setting command line arguments"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"ap = argparse.ArgumentParser()\nap.add_argument(\"-d\", \"--dataset\", required=True,\n\thelp=\"path to input dataset\")\nap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n\thelp=\"path to output loss/accuracy plot\")\nap.add_argument(\"-m\", \"--model\", type=str, default=\"covid19.model\",\n\thelp=\"path to output loss/accuracy plot\")\nargs = vars(ap.parse_args())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from line 37-39 we are setting innitial learning rate, no or epochs and batch size."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"INIT_LR = 1e-3\nEPOCHS = 100\nBS = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 39-61 we are taking each image from the path, changing it to RGB, resizing it, adding its lable to label and adding image to data."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for imagePath in imagePaths:\n\t# extract the class label from the filename\n\tlabel = imagePath.split(os.path.sep)[-2]\n\n\t# load the image, swap color channels, and resize it to be a fixed\n\t# 224x224 pixels while ignoring aspect ratio\n\timage = cv2.imread(imagePath)\n\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\timage = cv2.resize(image, (128, 128))\n\n\t# update the data and labels lists, respectively\n\tdata.append(image)\n\tlabels.append(label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from line 65-71 we are creating np array and encoding images lables "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data = np.array(data) / 255.0\nlabels = np.array(labels)\n\n# perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 75-76 we setting training data, test data and 79-81 we doing data augmentation and finaly from line 85-86 we are loading VGG16 network whome we will trai. "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"(trainX, testX, trainY, testY) = train_test_split(data, labels,\n\ttest_size=0.20, stratify=labels, random_state=42)\n\n# initialize the training data augmentation object\ntrainAug = ImageDataGenerator(\n\trotation_range=15,\n\tfill_mode=\"nearest\")\n\n# load the VGG16 network, ensuring the head FC layer sets are left\n# off\nbaseModel = VGG16(weights=\"imagenet\", include_top=False,\n\tinput_tensor=Input(shape=(128, 128, 3)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From line 90-99 we are setting some training parameters and than from 103-104 we are freezing base layers so that they might not update during first training process."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"headModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(4, 4))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(64, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the first training process\nfor layer in baseModel.layers:\n\tlayer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from line 107-10 we are compiling our model and from 113-119 we are traing the head of our network "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(\"[INFO] compiling model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,\n\tmetrics=[\"accuracy\"])\n\n# train the head of the network\nprint(\"[INFO] training head...\")\nH = model.fit_generator(\n\ttrainAug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,\n\tvalidation_data=(testX, testY),\n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from line 121-145 we are evaluating our model. Priniting reports and confusion matrix."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# make predictions on the testing set\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\n\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs,\n\ttarget_names=lb.classes_))\n\n# compute the confusion matrix and and use it to derive the raw\n# accuracy, sensitivity, and specificity\ncm = confusion_matrix(testY.argmax(axis=1), predIdxs)\ntotal = sum(sum(cm))\nacc = (cm[0, 0] + cm[1, 1]) / total\nsensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nspecificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n\n# show the confusion matrix, accuracy, sensitivity, and specificity\nprint(cm)\nprint(\"acc: {:.4f}\".format(acc))\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\nprint(\"specificity: {:.4f}\".format(specificity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* and finally we are poling the graph of accuracy from 148-163 "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(args[\"plot\"])\n\n# serialize the model to disk\nprint(\"[INFO] saving COVID-19 detector model...\")\nmodel.save(args[\"model\"], save_format=\"h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* and run the following command"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"$ python train_covid19.py --dataset generated_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This will create a Deep Learning model \"covid19.model\" in the same directory"},{"metadata":{},"cell_type":"markdown","source":"Lets discuss the accuracy of our created model from the graph ploted"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"Train for 79 steps, validate on 102 samples\nEpoch 1/100\n11/11 [==============================] - 219s 20s/step - loss: 0.8125 - accuracy: 0.4706 - val_loss: 0.4332 - val_accuracy: 0.7500\nEpoch 2/100\n11/11 [==============================] - 255s 23s/step - loss: 0.6254 - accuracy: 0.6765 - val_loss: 0.3841 - val_accuracy: 0.7500\nEpoch 3/100\n11/11 [==============================] - 399s 36s/step - loss: 0.5812 - accuracy: 0.6569 - val_loss: 0.3528 - val_accuracy: 0.9000\nEpoch 4/100\n11/11 [==============================] - 398s 36s/step - loss: 0.5092 - accuracy: 0.7353 - val_loss: 0.3150 - val_accuracy: 0.8500\nEpoch 5/100\n11/11 [==============================] - 395s 36s/step - loss: 0.4465 - accuracy: 0.8137 - val_loss: 0.2848 - val_accuracy: 0.9000\nEpoch 6/100\n11/11 [==============================] - 397s 36s/step - loss: 0.3994 - accuracy: 0.8824 - val_loss: 0.2566 - val_accuracy: 0.9000\nEpoch 7/100\n11/11 [==============================] - 403s 37s/step - loss: 0.3739 - accuracy: 0.8529 - val_loss: 0.2346 - val_accuracy: 0.9000\nEpoch 8/100\n.\n.\n.\n11/11 [==============================] - 221s 20s/step - loss: 0.0959 - accuracy: 0.9804 - val_loss: 0.0429 - val_accuracy: 1.0000\nEpoch 99/100\n11/11 [==============================] - 204s 19s/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.0228 - val_accuracy: 1.0000\nEpoch 100/100\n11/11 [==============================] - 202s 18s/step - loss: 0.0723 - accuracy: 0.9706 - val_loss: 0.0278 - val_accuracy: 1.0000\n[INFO] evaluating network...\n              precision    recall  f1-score   support\n\n       covid       1.00      0.93      0.96        14\n      normal       0.93      1.00      0.97        14\n\n    accuracy                           0.96        28\n   macro avg       0.97      0.96      0.96        28\nweighted avg       0.97      0.96      0.96        28\n\n[[13  1]\n [ 0 14]]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/graph-of-model/plot.png', width=800) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Accuracy: 100.00%\n* Sensitivity: 92.86%\n* Specificity: 100.00%"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy**\nHere we have 100% accuracy means we can use this model for detection of COVID from X Rays."},{"metadata":{},"cell_type":"markdown","source":"**Sensitivity**\n92.86% Sensitivity means we could accurately identify them as “COVID-19 positive” 92.86% of the time using our model."},{"metadata":{},"cell_type":"markdown","source":"**Specificity**\n100% Specificity means we could accurately identify them as “COVID-19 negative” 100.00% of the time using our model."},{"metadata":{},"cell_type":"markdown","source":"**NOTE**: Through this model is almost 100% accurate but still there is a lot of work need to be done as I didn't used other parameters like geo-location, travel history etc to detect COVID-19. Using only X Rays images is not enough. If you want to use this for further research kindly contact me for further studies and research. "},{"metadata":{},"cell_type":"markdown","source":"Download the source code and dataset [here](https://www.kaggle.com/nabeelsajid917/covid-19-x-ray-10000-images)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}