{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport nltk\nfrom collections import Counter\nimport itertools\nimport torch\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['imdb_master.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id\n#Класс словаря. Метод word2id возвращает номер слова, id2word - наоборот, восстанавливает слово.\n\nclass Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]\nfrom tqdm import tqdm_notebook\n#Интерфейс объекта, преобразующего тексты в последовательности номеров. transform выполняет преобразование при помощи словаря. fit_transform выучивает словарь из текста и возвращает такое же преобразование при помощи свежеполученного словаря.\n\nclass TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\n#Простая реализация данного интерфейса. Разбиение на слова производится с помощью библиотеки NLTK. В словаре содержатся несколько спец. слов. После токенизации, к полученной последовательности слов добавляются слева и справа спец. слова для начала и конца текста.\n\nclass SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n#Строим экземпляр входных данных. Обеспечиваем длину последовательности номеров равной max_seq_len.\n\ndef build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])\n        \n#Собираем экземпляры в тензоры\n\ndef features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor\nfrom sklearn import model_selection\nimdb_df = pd.read_csv('../input/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)\nmax_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}\ntext2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])\nprint(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])","execution_count":2,"outputs":[{"output_type":"stream","text":"If you are a Crispin Glover fan, you must see this. If you are a Sean Penn fan, you must see this. If you are a movie fan in general, you must see this. If you \n[2, 61, 31, 37, 8, 9993, 3146, 349, 5, 31, 229, 84, 19, 6, 61, 31, 37, 8, 1963, 5149, 349, 5, 31, 229, 84, 19, 6, 61, 31, 37]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]\n\ntrain_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\n\ntrain_ds = TensorDataset(train_tensor,train_labels)\nval_ds = TensorDataset(val_tensor,val_labels)\ntest_ds = TensorDataset(test_tensor,test_labels)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_ds,batch_size=128)\nval_loader = DataLoader(val_ds, batch_size=128)\ntest_loader = DataLoader(test_ds, batch_size=128)\nvocab_len = len(text2id.vocab)\nprint(vocab_len)","execution_count":10,"outputs":[{"output_type":"stream","text":"10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.emb = nn.Embedding(vocab_len, 100)\n        self.properties = nn.Sequential(\n            nn.Conv1d(in_channels=100, out_channels=120, kernel_size=3, padding=2),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=120, out_channels=130, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool1d(5)\n        )\n        self.estimator = nn.Sequential(\n            nn.Linear(5200,1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = self.emb(x)\n        x = x.transpose(1,2)\n        return  self.estimator(self.properties(x).view(x.size(0), -1))\n        \n    def train(self,train_loader,val_loader,epoch,waiting,optimizer):\n        self.cuda()\n        best_val_loss=1000\n        crit = nn.BCELoss()\n        for i in range(epoch):\n            train_loss = 0\n            val_loss = 0\n            for xx,yy in train_loader:\n                xx = xx.cuda()\n                yy=yy.cuda()\n                optimizer.zero_grad()\n                y_pred = self.forward(xx)\n                loss = crit(y_pred,yy.float())\n                train_loss += loss\n                loss.backward()\n                optimizer.step()\n            train_loss = train_loss/len(train_loader)\n            with torch.no_grad():\n                for xx,yy in val_loader:\n                    xx, yy = xx.cuda(), yy.cuda()\n                    y_pred = self.forward(xx)\n                    loss = crit(y_pred,yy.float())\n                    val_loss += loss\n                val_loss = val_loss/len(val_loader)\n                \n                if best_val_loss>val_loss:\n                    torch.save(self.state_dict(), \"../best_model.py\")\n                    best_val_loss = val_loss\n                    wait=waiting\n                else:\n                    wait -=1\n                    if wait==0:\n                        break\n            print(\"train loss:\", float(train_loss), \"___best val loss:\",float(best_val_loss), \"___remaining:\", wait)","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\nclf = Model()\n\noptimizer = torch.optim.Adam(clf.parameters(), lr=0.001)\nclf.train(train_loader,val_loader,20,10,optimizer)","execution_count":108,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([70])) that is different to the input size (torch.Size([70, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([98])) that is different to the input size (torch.Size([98, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n","name":"stderr"},{"output_type":"stream","text":"train loss: 0.6330012083053589 ___best val loss: 0.47826939821243286 ___remaining: 10\ntrain loss: 0.4182429611682892 ___best val loss: 0.3677915930747986 ___remaining: 10\ntrain loss: 0.3137264549732208 ___best val loss: 0.34338900446891785 ___remaining: 10\ntrain loss: 0.2379106879234314 ___best val loss: 0.34338900446891785 ___remaining: 9\ntrain loss: 0.18835684657096863 ___best val loss: 0.34338900446891785 ___remaining: 8\ntrain loss: 0.16327981650829315 ___best val loss: 0.34338900446891785 ___remaining: 7\ntrain loss: 0.09789006412029266 ___best val loss: 0.34338900446891785 ___remaining: 6\ntrain loss: 0.08037378638982773 ___best val loss: 0.34338900446891785 ___remaining: 5\ntrain loss: 0.05502751097083092 ___best val loss: 0.34338900446891785 ___remaining: 4\ntrain loss: 0.06737899780273438 ___best val loss: 0.34338900446891785 ___remaining: 3\ntrain loss: 0.10068079084157944 ___best val loss: 0.34338900446891785 ___remaining: 2\ntrain loss: 0.06328121572732925 ___best val loss: 0.34338900446891785 ___remaining: 1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nclf.load_state_dict(torch.load(\"../best_model.py\"))\ny_true = []\ny_pred = []\nfor xx,yy in test_loader:\n    out = clf.forward(xx.cuda())\n    for i in out:\n        if i<=0.4:\n            y_pred.append(0)\n        else:\n            y_pred.append(1)\n    for i in yy:\n        y_true.append(int(i))\nprint(classification_report(y_true,y_pred))","execution_count":109,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.84      0.82      0.83     12500\n           1       0.83      0.85      0.84     12500\n\n   micro avg       0.84      0.84      0.84     25000\n   macro avg       0.84      0.84      0.84     25000\nweighted avg       0.84      0.84      0.84     25000\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}