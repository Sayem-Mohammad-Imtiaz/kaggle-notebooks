{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy import percentile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nimport re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200) \nimport string\nimport nltk # for text manipulation\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom tqdm import tqdm\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import stats \nfrom sklearn import metrics \nfrom sklearn.metrics import mean_squared_error,mean_absolute_error, make_scorer,classification_report,confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.TweetAt.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Location'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = pd.DataFrame()\nmissing_values['column'] = df.columns\n\nmissing_values['percent'] = [round(100* df[col].isnull().sum() / len(df), 2) for col in df.columns]\nmissing_values = missing_values.sort_values('percent')\nmissing_values = missing_values[missing_values['percent']>0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nsns.set(style='whitegrid', color_codes=True)\nsplot=sns.barplot(x='column', y='percent', data=missing_values)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.ylabel(\"Percentage\", size=14, weight=\"bold\")\nplt.title(\"Percentage of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(17, 5))\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False)\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.title(\"Places of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_df = pd.DataFrame()\nunique_df['Features'] = df.columns\nunique=[]\nfor i in df.columns:\n    unique.append(df[i].nunique())\nunique_df['Uniques'] = unique\n\nf, ax = plt.subplots(1,1, figsize=(15,7))\n\nsplot = sns.barplot(x=unique_df['Features'], y=unique_df['Uniques'], alpha=0.8)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title('Bar plot for number of unique values in each column',weight='bold', size=15)\nplt.ylabel('#Unique values', size=12, weight='bold')\nplt.xlabel('Features', size=12, weight='bold')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loc_analysis = pd.DataFrame(df['Location'].value_counts().sort_values(ascending=False))\nloc_analysis = loc_analysis.rename(columns={'Location':'count'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\n   \"values\": loc_analysis['count'][:15],\n   \"labels\": loc_analysis.index[:15],\n   \"domain\": {\"column\": 0},\n   \"name\": \"Location Name\",\n   \"hoverinfo\":\"label+percent+name\",\n   \"hole\": .4,\n   \"type\": \"pie\"\n}\nlayout = go.Layout(title=\"<b>Ratio on Location</b>\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\n\ndata = [data]\nfig = go.Figure(data = data, layout = layout)\nfig.update_layout(title_x=0.5)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g=sns.catplot(\"TweetAt\", data=df, kind=\"count\", height=8)\ng.set_xticklabels(rotation=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Top_Location_Of_tweet= df['Location'].value_counts().head(10)\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.set_style('white')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Top_Location_Of_tweet.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nTop_Location_Of_tweet_df=pd.DataFrame(Top_Location_Of_tweet)\nTop_Location_Of_tweet_df.reset_index(inplace=True)\nTop_Location_Of_tweet_df.rename(columns={'index':'Location', 'Location':'Location_Count'}, inplace=True)\nTop_Location_Of_tweet_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_1=sns.barplot(x=\"Location\", y=\"Location_Count\", data=Top_Location_Of_tweet_df,\n                 palette='Blues_d')\nviz_1.set_title('Locations with most of the tweets')\nviz_1.set_ylabel('Count of listings')\nviz_1.set_xlabel('Location Names')\nviz_1.set_xticklabels(viz_1.get_xticklabels(), rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.1)\nsns.catplot(\"Sentiment\", data=df, kind=\"count\", height=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n### A) Removing @user","metadata":{}},{"cell_type":"code","source":"# write function for removing @user\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new column with removed @user\ndf['Tweet'] = np.vectorize(remove_pattern)(df['OriginalTweet'], '@[\\w]*')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B) REMOVED HTTP AND URLS FROM TWEET","metadata":{}},{"cell_type":"code","source":"import re\ndf['Tweet'] = df['Tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C) Removing Punctuations, Numbers, and Special Characters","metadata":{}},{"cell_type":"code","source":"# remove special characters, numbers, punctuations\ndf['Tweet'] = df['Tweet'].str.replace('[^a-zA-Z#]+',' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D) Removing Short Words","metadata":{}},{"cell_type":"code","source":"# remove short words\ndf['Tweet'] = df['Tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### E) Tokenization\n","metadata":{}},{"cell_type":"code","source":"# create new variable tokenized tweet \ntokenized_tweet = df['Tweet'].apply(lambda x: x.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### F) Stemming","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\n\n# apply stemmer for tokenized_tweet\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### G) tokenized_tweet","metadata":{}},{"cell_type":"code","source":"# join tokens into one sentence\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n# change df['Tweet'] to tokenized_tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Tweet']  = tokenized_tweet\ndf.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the common words used in the tweets: WordCloud","metadata":{}},{"cell_type":"code","source":"# create text from all tweets\nall_words = ' '.join([text for text in df['Tweet']])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Extremely Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Positive']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Extremely Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Negative']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text from just normal tweets\nnormal_words = ' '.join([text for text in df['Tweet'][df['Sentiment'] == 'Neutral']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the impact of Hashtags on tweets sentiment","metadata":{}},{"cell_type":"code","source":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    for i in x:\n        ht = re.findall(r'#(\\w+)', i)\n        hashtags.append(ht)\n    return hashtags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting hashtags from non racist/sexist tweets\nHT_Extremely_Positive = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Extremely Positive'])\nHT_Positive = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Positive'])\nHT_Neutral = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Neutral'])\nHT_Negative = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Negative'])\nHT_Extremely_Negative = hashtag_extract(df['OriginalTweet'][df['Sentiment'] == 'Extremely Negative'])\n\n\n\n# unnesting list\nHT_Extremely_Positive = sum(HT_Extremely_Positive, [])\nHT_Positive = sum(HT_Positive, [])\nHT_Neutral = sum(HT_Neutral, [])\nHT_Negative = sum(HT_Negative,[])\nHT_Extremely_Negative = sum(HT_Extremely_Negative,[])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making frequency distribution top 10 Extremely Positive hashtags\na = nltk.FreqDist(HT_Extremely_Positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making frequency distribution top 10 Positive hashtags\na = nltk.FreqDist(HT_Positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making frequency distribution top 10 Neutral hashtags\na = nltk.FreqDist(HT_Neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making frequency distribution top 10 Extremely Negative hashtags\na = nltk.FreqDist(HT_Extremely_Negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making frequency distribution top 10 Negative hashtags\na = nltk.FreqDist(HT_Negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count' : list(a.values())})\n\nd = d.nlargest(columns = 'Count', n = 10)\n\nplt.figure(figsize = (16,5))\nax = sns.barplot(data =d, x = 'Hashtag', y = 'Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Extracting Features from Cleaned Tweets","metadata":{}},{"cell_type":"code","source":"new_df = df[['Tweet','Sentiment']]\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting into lower case","metadata":{}},{"cell_type":"code","source":"new_df[\"Tweet\"] = new_df[\"Tweet\"].str.lower()#.str.split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing StopWords","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['Tweet'].apply(lambda x: [item for item in x if item not in stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spitting Our Dataset into Training And Testing Dataset ( For Multiclass Classification)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(new_df,test_size = 0.2,random_state=0,stratify = new_df.Sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Of Counter Vectorizer For Multi Class Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.Tweet.values)\nX_valid = vectorizer.transform(valid.Tweet.values)\n\ny_train = train.Sentiment.values\ny_valid = valid.Sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes Classifier for MULTICLASS Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent-SGD Classifier( MULTICLASS CLASSIFICATION)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOM FOREST CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extreme Gradient Boosting","metadata":{}},{"cell_type":"code","source":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support vector machine","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# All the multiclass models test accuracy in descending order","metadata":{}},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost',],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONVRTING OUR MULTICLASS CLASSIFICATION INTO BINARY CLASSIFICATION","metadata":{}},{"cell_type":"code","source":"CATBOOST = new_df[['Tweet','Sentiment']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Positive',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Extremely Positive',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Neutral',1) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Negative',0) \nCATBOOST[\"Sentiment\"]= CATBOOST[\"Sentiment\"].replace('Extremely Negative',0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CATBOOST.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CATBOOST['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DIVIDING OUR DATASET INTO TRAINING AND TESTING","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(CATBOOST,test_size = 0.2,random_state=0,stratify = CATBOOST.Sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# USING COUNT VECTORIZER","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.Tweet.values)\nX_valid = vectorizer.transform(valid.Tweet.values)\n\ny_train = train.Sentiment.values\ny_valid = valid.Sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NAIVE BAYES CLASSIFIER FOR BINARY CLASSIFICATION.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOM FOREST CLASSIFIER FOR BINARY CLASSIFICATION","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOGISTIC REGRESSION FOR BINARY CLASSIFICATION","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XG BOOST( BINARY CLASSIFICATION)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUPPORT VECTOR MACHINE(BINARY CLASSIFICATION)","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent-SGD Classifier( BINARY CLASSIFICATION)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION OF ALL BINARY CLASSIFICATION MODELS","metadata":{}},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost'],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Winner Model: Stochastic Gradient Descent-SGD Classifier","metadata":{}},{"cell_type":"code","source":"# Get the predicted classes\ntrain_class_preds = sgd_clf.predict(X_train)\ntest_class_preds = sgd_clf.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the confusion matrix for both train and test. We are getting very low type and type 2 errors.\n\nlabels = ['Negative', 'Positive']\ncm = confusion_matrix(y_train, train_class_preds)\nprint(cm)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the overall accuracy. Overall accuracy is very good.\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\ny_pred = sgd_clf.predict(X_valid)\n\nscore =accuracy_score(y_valid,y_pred)\nprint('accuracy is', score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# F1 score for our classifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n\ny_pred =  sgd_clf.predict(X_valid)\nprint(f1_score(y_valid,y_pred, average=\"macro\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score is mean accuracy\nscikit_score = sgd_clf.score(X_valid,y_valid)\nprint('scikit score:', scikit_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recall score for our winner model\nrecall_score(y_valid, y_pred, average='macro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report for our stochastic gradient descent algorithm\nclassification_report(y_valid,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Very low type 1 and type 2 error\nconfusion_matrix(y_valid,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}