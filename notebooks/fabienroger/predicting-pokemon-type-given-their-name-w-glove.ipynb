{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Predicting Pokemon Type Given Their Name with GLOVE"},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pokemon/Pokemon.csv')\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Name','Type 1']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(len(df['Type 1'].unique()))\nplt.bar(x, df['Type 1'].value_counts())\nplt.show()\n\ndf['Type 1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Naïve Predictions"},{"metadata":{},"cell_type":"markdown","source":"So if we only predicted \"water\", our accuracy would be : "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Type 1'].value_counts()[0]/df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I calculate TF.IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabet = 'abcdefghijklmnopqrstuvwxyz'\n\ndf['Name'] = df['Name'].apply(lambda s:s.lower())\n\ndef apparition(s,c):\n    r = 0\n    for x in s:\n        if x==c: r+=1\n    return r\n\napparitions = []\nfor i,c in enumerate(alphabet):\n    df[str(i)] = df['Name'].apply(apparition, args=c)\n    apparitions.append(sum(df[str(i)]))\n\nletters = sum(apparitions)\n\nfor i,c in enumerate(alphabet):\n    tf = df[str(i)]/df['Name'].apply(len)\n    idf = letters/apparitions[i]\n    df['tfidf'+str(i)] = tf*idf\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction using logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['tfidf'+str(i) for i,c in enumerate(alphabet)]]\ny = df['Type 1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n                                                  random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LogisticRegression(max_iter = 10000, C=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg.predict(X_val)\ny_pred_train = reg.predict(X_train)\nprint(accuracy_score(y_pred_train,y_train))\nprint(accuracy_score(y_pred,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's not much better than only gessing \"water\". Let's try to do something better by using a corpus of text, and it seems to be hard to reduce overfitting."},{"metadata":{},"cell_type":"markdown","source":"# Using GLOVE to make predictions using a LSTM trained to recognize synonyms of the types (Unsuccessful)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.downloader as api\nglove = api.load(\"glove-wiki-gigaword-100\")\nbests = glove.most_similar(\"fire\", topn= 5)\nprint(bests)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbests = glove.most_similar(\"fire\", topn= 5)\nprint(' '.join([str(x[0])+' '+str(x[1])+'\\n' for x in bests]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bests = glove.most_similar(\"flying\", topn= 500)\nprint(' '.join([str(x[0]) for x in bests]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, LSTM, Input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 27\npk_types = len(df['Type 1'].unique())\nmax_name_length = df['Name'].map(len).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():'''\ninputs = Input(shape=(max_name_length,vocab_size))\nX = LSTM(32, return_sequences=True, recurrent_dropout = 0.3 , dropout = 0.3)(inputs)\nX = LSTM(32, recurrent_dropout = 0.2 , dropout = 0.2)(X)\nX = Dense(pk_types, activation='softmax')(X)\nmodel = Model(inputs=inputs, outputs=X)\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = list(df['Type 1'].unique())\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabet = '#abcdefghijklmnopqrstuvwxyz'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(s):\n    return re.sub(r'[^a-z]','', s.lower())\n\ndef left_pad_and_clean(s):\n    s = clean(s)\n    if len(s)>=max_name_length:\n        return s[:max_name_length]\n    else:\n        while len(s)<max_name_length:\n            s = '#'+s\n        return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(left_pad_and_clean('  heyé--'))\nprint(clean('dej JF,'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def str_to_vec(s):\n    s = left_pad_and_clean(s)\n    vec = np.zeros(shape=(max_name_length,vocab_size))\n    for i,c in enumerate(s):\n        if c not in alphabet:print(c)\n        vec[i,alphabet.index(c)] = 1\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str_to_vec('azerty').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 500\n#subtask_df = pd.DataFrame(columns=['X','y'])\nN = n*len(types)\nstX = np.zeros(shape=(N,max_name_length,vocab_size))\nstXweight = np.zeros(shape=(N,1))\nsty = np.zeros(shape=(N,len(types)))\nact_i = 0\nfor j,t in enumerate(types):\n    ms = glove.most_similar(clean(t), topn=n-1)\n    close_words = [clean(str(x[0])) for x in ms]\n    close_words_w = [x[1] for x in ms]\n    close_words.append(clean(t))\n    close_words_w.append(1)\n    for i,w in enumerate(close_words):\n        vec = str_to_vec(w)\n        stX[act_i,:,:] = vec\n        stXweight[act_i,0] = close_words_w[i]\n        sty[act_i,j] = 1\n        act_i += 1\n        \n        #subtask_df = subtask_df.append({'X': str_to_vec(w),'y':str_to_vec(cleant)}, ignore_index=True)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stX.shape, sty.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nstX_train, stX_val,stXweight_train,stXweight_val, sty_train, sty_val =\\\n    train_test_split(stX, stXweight,sty,shuffle=True,\\\n                     test_size=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(stX_train, sty_train,sample_weight=stXweight_train,batch_size=128,\\\n          epochs=20, validation_data = (stX_val, sty_val,stXweight_val),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = model.evaluate(stX_val, sty_val,sample_weight=stXweight_val)\nprint(r[1])\nr = model.evaluate(stX_train, sty_train,sample_weight=stXweight_train)\nprint(r[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of overfitting, but this might not be a problem. I want the model to \"remember\" the words associated with water. But I still want the model to generalize to imaginary words close to the original concept. Let's see if that's the case."},{"metadata":{},"cell_type":"markdown","source":"## Direct prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"N = df.shape[0]\ndiX = np.zeros(shape=(N,max_name_length,vocab_size))\ndiy = np.zeros(shape=(N,len(types)))\nfor index, row in df.iterrows():\n    diX[index,:,:] = str_to_vec(row['Name'])\n    diy[index,types.index(row['Type 1'])] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(diX.shape)\nr = model.evaluate(diX, diy)\nprint(r[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wut :'("},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = model.predict(diX)\nypred_words = model.predict(stX_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def amax(array):\n    return array.argmax()\n\nb = np.apply_along_axis(amax, 1, ypred_words)\nplt.hist(b, alpha = 0.5)\na = np.apply_along_axis(amax, 1, ypred)\nplt.hist(a, alpha = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pokemons have only psychic names..."},{"metadata":{},"cell_type":"markdown","source":"## Indirect Prediction"},{"metadata":{},"cell_type":"markdown","source":"I fit pokemon names to the predicted types"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_of_predictor = model.predict(diX)\ny_of_predictor = np.apply_along_axis(lambda a:a.argmax(), 1, diy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_of_predictor, bins = len(types))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idiX_train, idiX_val, idiy_train, idiy_val =\\\n    train_test_split(X_of_predictor,y_of_predictor,test_size = 0.33,\\\n                     shuffle=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor.fit(idiX_train, idiy_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = predictor.predict(idiX_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idiy_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ypred_classes = np.apply_along_axis(lambda a:a.argmax(), 1, ypred)\n#plt.hist(ypred,alpha=0.5)\nplt.hist([ypred,idiy_val], bins=len(types))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(ypred, idiy_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"worse than random :'((((("},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(idiy_val, ypred)\nplt.imshow(np.log(cm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using GLOVE to make predictions by counting the number of subwords of synonyms of the types inside Pokemon names"},{"metadata":{},"cell_type":"markdown","source":"I try using the well-know fact that fire-related word are used in the name of a pokemon (for example \"Typhlosion\" contains a part of \"explosion\", which is in the set of words close too \"fire\"). So I look for matches."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For which values of k a k-uplet will be looked for in the pokemon name\nchar_range = [4,5,6,7] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = np.zeros(shape=(df.shape[0], len(types)*len(char_range)))\n\nname_s = {}\n\nfor index, row in df.iterrows():\n    name = clean(row['Name'])\n    \n    for k in char_range:\n        for i in range(len(name)-k):\n            sub_name = name[i:i+k]\n            if sub_name not in name_s:\n                name_s[sub_name] = []\n            name_s[sub_name].append(index)\n    \nfor j,t in enumerate(types):\n    ms = glove.most_similar(clean(t), topn=n-1)\n    close_words = [(clean(str(x[0])),x[1]) for x in ms]\n    close_words.append((clean(t),1))\n\n    for ind,k in enumerate(char_range):\n        for w,s in close_words:\n            for i in range(len(w)-k):\n                sub_word = w[i:i+k]\n                if sub_word in name_s:\n                    pokemons_containing_sw = name_s[sub_word]\n                    for index in pokemons_containing_sw:\n                        scores[index,j*len(char_range)+ind] += s\n            \n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.imshow(scores[171,:].reshape((len(types),len(char_range))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inv_avg(x):\n    return 0 if x.mean() == 0 else 1/x.mean()\ninverse_averages = np.apply_along_axis(inv_avg, 0, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(inverse_averages.reshape((len(types),len(char_range))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting a simple model"},{"metadata":{"trusted":true},"cell_type":"code","source":"freqX = inverse_averages*scores\nfreqy = y_of_predictor\nnames = list(df['Name'])\n\nfreqX_train, freqX_val, freqy_train, freqy_val, names_train, names_val =\\\n    train_test_split(freqX,freqy,names,test_size = 0.33,\\\n                     shuffle=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_reg = LogisticRegression(max_iter = 10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_reg.fit(freqX_train, freqy_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(freqy_val, freq_reg.predict(freqX_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the best model and hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_models = [LogisticRegression(C = 1, max_iter = 10000), \n               LogisticRegression(C = 100, max_iter = 10000), \n               SVC(), \n               SVC(kernel='linear'), \n               DecisionTreeClassifier(max_depth=6)]\n\nfor m in freq_models:\n    m.fit(freqX_train, freqy_train)\n    print(accuracy_score(freqy_val, m.predict(freqX_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [0.5,1,2,3,4]:\n    print('using p =',p)\n    for n in [200,250,300,350,400]:\n        print('using n =',n)\n\n        scores = np.zeros(shape=(df.shape[0], len(types)*len(char_range)))\n\n        name_s = {}\n\n        for index, row in df.iterrows():\n            name = clean(row['Name'])\n\n            for k in char_range:\n                for i in range(len(name)-k):\n                    sub_name = name[i:i+k]\n                    if sub_name not in name_s:\n                        name_s[sub_name] = []\n                    name_s[sub_name].append(index)\n\n        for j,t in enumerate(types):\n            ms = glove.most_similar(clean(t), topn=n-1)\n            close_words = [(clean(str(x[0])),x[1]) for x in ms]\n            close_words.append((clean(t),1))\n\n            for ind,k in enumerate(char_range):\n                for w,s in close_words:\n                    for i in range(len(w)-k):\n                        sub_word = w[i:i+k]\n                        if sub_word in name_s:\n                            pokemons_containing_sw = name_s[sub_word]\n                            for index in pokemons_containing_sw:\n                                scores[index,j*len(char_range)+ind] += s**p\n\n        inverse_averages = np.apply_along_axis(inv_avg, 0, scores)\n\n        freqX = inverse_averages*scores\n        freqy = y_of_predictor\n        names = list(df['Name'])\n\n        freqX_train, freqX_val, freqy_train, freqy_val, names_train, names_val =\\\n            train_test_split(freqX,freqy,names,test_size = 0.33,\\\n                             shuffle=True, random_state=1)\n\n        freq_model = LogisticRegression(penalty='none', max_iter = 10000)\n\n        freq_model.fit(freqX_train, freqy_train)\n        print(accuracy_score(freqy_val, freq_model.predict(freqX_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 300\n\nscores = np.zeros(shape=(df.shape[0], len(types)*len(char_range)))\n\nname_s = {}\n\nfor index, row in df.iterrows():\n    name = clean(row['Name'])\n\n    for k in char_range:\n        for i in range(len(name)-k):\n            sub_name = name[i:i+k]\n            if sub_name not in name_s:\n                name_s[sub_name] = []\n            name_s[sub_name].append(index)\n\nfor j,t in enumerate(types):\n    ms = glove.most_similar(clean(t), topn=n-1)\n    close_words = [(clean(str(x[0])),x[1]) for x in ms]\n    close_words.append((clean(t),1))\n\n    for ind,k in enumerate(char_range):\n        for w,s in close_words:\n            for i in range(len(w)-k):\n                sub_word = w[i:i+k]\n                if sub_word in name_s:\n                    pokemons_containing_sw = name_s[sub_word]\n                    for index in pokemons_containing_sw:\n                        scores[index,j*len(char_range)+ind] += s**2\n\ninverse_averages = np.apply_along_axis(inv_avg, 0, scores)\n\nfreqX = inverse_averages*scores\nfreqy = y_of_predictor\nnames = list(df['Name'])\n\nfreqX_train, freqX_val, freqy_train, freqy_val, names_train, names_val =\\\n    train_test_split(freqX,freqy,names,test_size = 0.33,\\\n                     shuffle=True, random_state=1)\n\nfreq_model = LogisticRegression(penalty='none', max_iter = 10000)\n\nfreq_model.fit(freqX_train, freqy_train)\nprint('Accuracy : ',accuracy_score(freqy_val, freq_model.predict(freqX_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our best guess accuracy can be achieved using this method, and is arround 22%."},{"metadata":{},"cell_type":"markdown","source":"## Error Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy over the training set :', accuracy_score(freqy_train, freq_model.predict(freqX_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is quite a lot of overfitting, but all my attempts at reducing it only resulted in less accuracy on the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = freq_model.predict(freqX_val)\ncm = confusion_matrix(freqy_val, preds)\nplt.imshow(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist([preds, freqy_val], bins=len(types))\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to love the type \"Normal\"."},{"metadata":{},"cell_type":"markdown","source":"# Final prediction on the validation set"},{"metadata":{},"cell_type":"markdown","source":"The prediction made by the algorithm :"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame(columns=['Name', 'Predicted type', 'Actual type'])\n\nfor i,x in enumerate(preds):\n    name = names_val[i]\n    pred_df = pred_df.append({'Name':name,\n                              'Predicted type':types[x],\n                              'Actual type': types[freqy_val[i]]}, ignore_index = True)\n\npd.set_option('display.max_rows', None)\nprint(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}