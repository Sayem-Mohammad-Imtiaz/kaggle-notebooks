{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is this Notebook about?"},{"metadata":{},"cell_type":"markdown","source":"In this notebook I will try to share different aspects of a NLP problem and well see what difference one makes. This is not a traditional approach to a pproblem but a compairative one."},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n1. [Prepairing the data](#1)\n2. [Tokenization](#2)\n3. [Problem based Cleaning](#3)\n4. [Where embedding might fail](#4)\n5. [Traditional data prep and where to use it](#5)\n6. [Augmentation](#6)\n7. [Resolving StopWords](#7)\n8. [A look at Collection extraction](#8)\n9. [Similarity Analysis Among Sentences and feature extraction](#9)"},{"metadata":{},"cell_type":"markdown","source":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)</font>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nfrom IPython.display import YouTubeVideo\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"1\" > </a>\n## 1.Prepairing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin8')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before doing anything else let's first create folds for our dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_folds(X,y):\n    \n    df['kfold'] = -1\n    \n    splitter = StratifiedKFold(n_splits=5)\n    \n    for f, (t_, v_) in enumerate(splitter.split(X, y)):\n        \n        X.loc[v_, 'kfold'] = f\n        \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"df = create_folds(df, df['Sentiment'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['OriginalTweet', 'Sentiment', 'kfold']]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"2\" > </a>\n## 2.Tokenization\nWhile tokenizing the tweets we have many tokenizers to choose from. Here we have to wise and to see what gives what?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import TweetTokenizer, word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = df['OriginalTweet'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in sentences[2:3]:\n    print(\"Original:\\n\")\n    print(i)\n    print('\\nTensorflow Tokenizer\\n:')\n    a = Tokenizer()\n    a.fit_on_texts([i])\n    print(a.word_index)\n    print(\"\\nTweet Tokenizer:\\n\")\n    print(TweetTokenizer().tokenize(i))\n    print('\\nNLTK word_tokenizer:\\n')\n    print(word_tokenize(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see these all yield different results and you have to see which works best for your use case. For now we will use NLTK Tweet-Tokenizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = []\n\nfor i in tqdm(df['OriginalTweet']):\n    \n    tweet = TweetTokenizer().tokenize(i)\n    tweet = ' '.join(tweet)\n    tweets.append(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tweets[:3]:\n    print(i, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"3\" > </a>\n## 3.Cleaning (Like thoughtful cleaning)"},{"metadata":{},"cell_type":"markdown","source":"Ok. Now what ?? <br>\nWell, Now we can do data cleaning but before that we have to see how we should do that. And for that I have a very good kernel which I will take insights from. - https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings. I will still write the code but for better explanation and deeper understanding I would highly recommend that notebook and also a couple more from the same author."},{"metadata":{},"cell_type":"markdown","source":"In short, we will load the embeddings and see how much vocablary is covered by the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nfrom gensim import downloader\n\nembedding_file = '../input/embeddings/GoogleNews-vectors-negative-300d.bin'\n\nembedding_model =  KeyedVectors.load_word2vec_format(embedding_file, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab([tweet.split() for tweet in tweets])\nprint({k: vocab[k] for k in list(vocab)[:5]})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov = check_coverage(vocab,embedding_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the punctuation which is not in the embeddings"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, tweet in enumerate(tweets):\n    \n    tweets[index] = clean_text(tweet)\n\n\nvocab = build_vocab([tweet.split() for tweet in tweets])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov = check_coverage(vocab,embedding_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we did improve a lot but as we can see we still have a significat portion of vocab which still has no embeddings. <br>\nThe main reason is our use case that is \"Covid-19\" which itself is a new term and hence the previously trained word embeddings will be useless. So, what can we possibly do in this case. Well, In my opinion we do have one option and that to replace every \"COVID\" occurance with \"crisis\" ( Just a word for which we have embedding) also we can replace the \"SocialDistancing\" with distancing."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"crisis\" in embedding_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"distancing\" in embedding_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we make these changes let's look at which oov words have a significant length."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(oov)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count = 0\nindex = 0\n\nwhile((count != 30) and count < len(oov)):\n    \n    if len(oov[index][0]) > 3:\n        print(oov[index])\n        count += 1\n        \n    index += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh man I hate cleaning. But well what can I say it's damn important."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_replace = [('COVID', 'health crisis'),\n            ('COVID19', 'health crisis'),\n            ('Covid19', 'health crisis'),\n            ('Covid', 'health crisis'),\n            ('COVID2019', 'health crisis'),\n            ('covid19', 'health crisis'),\n            ('toiletpaper', 'toilet paper'),\n            ('covid', 'health crisis'),\n            ('CoronaCrisis', 'health crisis'),\n            ('CoronaVirus', 'health crisis'),\n            ('SocialDistancing', 'Social distancing'),\n            ('2020', 'this year'),\n            ('CoronavirusPandemic', 'health crisis'),\n            ('CoronavirusOutbreak', 'health crisis'),\n            ('StayHomeSaveLives', 'Stay Home Save Lives'),\n            ('StayAtHome', 'Stay At Home'),\n            ('StayHome', 'Stay Home'),\n            ('panicbuying', 'Panic Buying'),\n            ('socialdistancing', 'Social Distancing'),\n            ('CoronaVirusUpdate', 'health crisis update'),\n            ('StopHoarding', 'Stop Hoarding'),\n            ('realDonaldTrump', 'real Donald Trump'),\n            ('StopPanicBuying', 'Stop Panic Buying'),\n            ('covid19UK', 'health crisis'),\n            ('QuarantineLife', 'Quarantine life'),\n            ('behaviour', 'behave')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_replace_dict = {}\n\nfor i in to_replace:\n    \n    to_replace_dict[i[0]] = i[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, tweet in tqdm(enumerate(tweets)):\n    \n    cleaned_tweet = []\n    \n    for word in tweet.split():\n        \n        if len(word) > 2:\n            \n            if word in to_replace_dict:              \n                cleaned_tweet.append(to_replace_dict[word])\n            else:\n                cleaned_tweet.append(word)\n                \n    tweets[index] = ' '.join(cleaned_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab([tweet.split() for tweet in tweets])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov = check_coverage(vocab,embedding_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"???? Why are we only at 88%. Let's  look again at our oov words."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"count = 0\nindex = 0\n\nwhile((count != 30) and count < len(oov)):\n    \n    if len(oov[index][0]) > 3:\n        print(oov[index])\n        count += 1\n        \n    index += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OH MAN I HATE THIS. Why can't they just use normal english and use Space between characters. Also as you can see \"can't\" is written as \"canÂ\" and thanks to that I can't use someone else cleaning code. Well, I just want to show you that there are things that you need to take care of."},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"4\" > </a>\n## 4.Some other examples where embeddings might fail\n\nWhat we have seen is only one use case. Some time ago there was this competition \"Toxic comment classification\" which used many emoticons and some vulgur words and so let's have a look at our embedding and see if we have anything related to that."},{"metadata":{},"cell_type":"markdown","source":"I apolozise in advance for the mention of these words. I am using them simply to show the importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_check = ['fuck', 'motherfucker', ':)', \":{\", 'bastard', ':(']\n\nfor i in to_check:\n    if i in embedding_model:\n        print('yes')\n    else:\n        print('no')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, we may or may not have embeddings for all the words and especially emoticons and nowdays emoticons are really popular. I would recommend to make your own embeddings as it will have better coverage. One more thing is that you could use the pretrained embeddings and then just finetune them. <br> Also let me show you the good thing about tweet_tokenizer of nltk."},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetTokenizer().tokenize('This word has a :) face')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see. it recognizes the emoticons and that's really helpful while making embeddings."},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"5\" > </a>\n## 5.Traditional Data prep and where to use it\n\nBy now you could be wondering where is all the Stemming, Lemmatization and etc etc. <br>\nWell the thing is we generally don't need those when using pretrained embeddings. Why so? Well let's have a look. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer, WordNetLemmatizer\n\nword = 'elegant'\nstem_word = SnowballStemmer('english').stem(word)\nlemma = WordNetLemmatizer().lemmatize(word)\n\nprint(\"Stem word: \", stem_word)\nprint(\"\\nLemma: \", lemma)\n\nprint(\"\\nIs stemmed word present in embedding :\", stem_word in embedding_model)\nprint(\"\\nIs lemma present in embedding :\", lemma in embedding_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see eleg is not a word and isn't present in the embedding and hence it will only reduce our coverage. let's have a few more examples for understanding it better."},{"metadata":{"trusted":true},"cell_type":"code","source":"word1 = 'feet'\nword2 = 'foot'\n\nprint(WordNetLemmatizer().lemmatize(word1))\n\nprint(word1 in embedding_model)\nprint(word2 in embedding_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well as you can see both feet and foot are present in our embedding and so if do lemmatize the word we will lose the availabe variance and hence we should not do that. <br>\nTo be truthful you could try lemmatization and see what results it yields and then choose wheather to use it or not."},{"metadata":{},"cell_type":"markdown","source":"#### Now the question is where to use this Stemming then ?\n\nWell, the answer to that question is not so simple. I will give you 2 places wher you could use Stemming and Lemmmatization. More likely stemming.\n    * 1) We have small data and the pretrained embeddings doesn't have good vocab coverage ( Also you can search for domain wise\n    pretrained embeddings and you will likey find one.) or you just don't want to use pretrained embeddings,in such a case stemming will be \n    very useful as it will provide better vectors for words as after stemming the occurance will be incresed significantly.\n    \n    * 2) Instead of using simple embeddings you could use Tf-idf weighted embeddings and can use stemming in the creation of Tf-idf vectors."},{"metadata":{},"cell_type":"markdown","source":"#### Using tf-idf weighted embeddings you you an extra edge in most of the cases. At least I find so."},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"6\" > </a>\n## 6.Augmentation\n\nWell, this part is really less discussed and I am also not too sure how to deal with it. For all the video and articles/blogs I have read regarding this I found 2 very useful."},{"metadata":{},"cell_type":"markdown","source":"* 1) Use Synonyms of words and make new sentences by replacing the word by its synonyms.\n* 2) Convert text into another language and then convert it back again to the original language."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install nlpaug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.word as naw","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sent = 'All month there hasn been crowding the supermarkets restaurants however reducing all the hours and closing the malls means everyone now using the same entrance and dependent single supermarket manila lockdown covid2019 Philippines https tco HxWs9LAnF9'\nprint('original: ', sent)\nprint('\\nAugmented: ', naw.SynonymAug(aug_src='wordnet').augment(sent))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty good write!!! <br>\nIt would be interseting to explore this library further. I will list a few more such libraries : <br>\n    * textattack\n    * textaugment"},{"metadata":{},"cell_type":"markdown","source":"### I would recommend these 2 videos:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"YouTubeVideo('BBR3J2HI5xI')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"YouTubeVideo('VpLAjOQHaLU')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for converting into different language you should see this - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038"},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"7\" > </a>\n## 7.Resolving StopWords\nWell, there's nothing much to say here but a small remainder \"Be careful as 'not' is also a stopword\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'not' in stop_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"8\" > </a>\n## 8.A look at Collection Extraction\n\nA good read - https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = ['break the rules', 'free time', 'draw a conclusion', 'keep in mind', 'get ready']\n\nfor i in words:\n    \n    print(i in embedding_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see these are not individual words but have a meaning due to continuty. You will find the solution in the above mentioned read. <br>\nAlso, the n-gram approach of BOW and Tf-idf could be very helpful in this context."},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------\n<a id =\"9\" > </a>\n## 9.Similarity Analysis among sentences"},{"metadata":{},"cell_type":"markdown","source":"USE CASE - A couple years back there was a Competition \"Quora Question Pair Similarity..\" in which we had to predict given 2 questions whether they are simmilar or not and the following features could be very useful."},{"metadata":{},"cell_type":"markdown","source":"A good look - https://github.com/seatgeek/fuzzywuzzy#usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Ratio - How much macthing."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.ratio(\"this is a test\", \"this is a test!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Partial Ration - Does it have a partial match"},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_ratio(\"this is a test\", \"this is a test!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Token Sort Ration - After sorting tokens how much match"},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Token Set Ratio - Matching Ratio after making a set of tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}