{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Welcome to my kernel ! \n## <br>What you will find here . \n\n*      [EDA (Explotary Data Analysis)](#1)\n*      [Hand-made Forward-Backward Functions](#2)\n*     [Sklearn-Logistic Regression](#3)"},{"metadata":{"_uuid":"f636220b08099118f51ad38742274b0c24e855db"},"cell_type":"markdown","source":"<div id=\"1\"/>\n## EDA (Explotary Data Analysis)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # Linear Algebra Library\nimport pandas as pd # Data Processing Library\nimport matplotlib.pyplot as plt # Visualize Library","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90038dbac0c0bad13a99c071fb59ef98f9402405"},"cell_type":"code","source":"df = pd.read_csv(\"../input/voice.csv\")\ndf.label = [1 if each == \"male\" else 0 for each in df.label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"857b9555762385fe2bbbd7a878fbd6d1cb931047"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59de54d04123442135a964a80c2d32b505862d4f"},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"964b033372462cf49f8a75f93eab96ae887ce604"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c247bcb2e488b89f929b35665d9fa8107ff18a54"},"cell_type":"code","source":"y = df.label.values\nx_data = df.drop(\"label\",axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"791903834bb70b99d41c43c30b99c8df82f89cd4"},"cell_type":"markdown","source":"<div id=\"2\"/>\n## Hand-made Forward-Backward Functions\n\n> "},{"metadata":{"_uuid":"a306f55f65dcf28812f822c3db58ddcd47ebe09f"},"cell_type":"markdown","source":"### Normalization"},{"metadata":{"trusted":true,"_uuid":"1fa05fa45d7b891504bf2f956cf827f435f9e402"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d010f8ac91d2136e1707274084927cf3a19769a5"},"cell_type":"markdown","source":"### Train Test Split with Sklearn "},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"_uuid":"4b89e0cd73e65aee64f217019a97194728c2c061"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train : \",x_train)\nprint(\"x_test : \",x_test)\nprint(\"y_train : \",y_train)\nprint(\"y_test : \",x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d76ae3a046c2310a39e856b9bb78dd72765a4d4"},"cell_type":"markdown","source":"### Initializing weights and bias to our model."},{"metadata":{"trusted":true,"_uuid":"500db904b63c91c1949f6bbbbe671ab1f33c45e4"},"cell_type":"code","source":"def initialize_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.00\n    return w,b\ndef sigmoid(z):\n    y_head = 1/ (1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dd13ace4b3e07c7000668b1c5bf8c8d275e1b96"},"cell_type":"markdown","source":"### We need forward & backward propagation to decrease loss function"},{"metadata":{"trusted":true,"_uuid":"be10c55d980f922644ae21cdd432bda7d8ac164c"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    #for forward\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    \n    #for backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a3bf229348b096a94f4ca26446f88ba385f9c4e"},"cell_type":"markdown","source":"### Depend on our function we need to update our data."},{"metadata":{"trusted":true,"_uuid":"38693ecc01f96752b887cfaa9d228645367f6a93"},"cell_type":"code","source":"def update(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []    \n    \n    for i in range(number_of_iterations):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train,learning_rate,number_of_iterations)\n        cost_list.append(cost)\n        w =  w - learning_rate*gradients[\"derivative_weight\"]\n        b =  b - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i : %f\" %(i,cost))\n        \n        \n        \n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation = \"vertical\")\n    plt.xlabel(\"Number of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"646369ff3061641ad40c341968a1510b67a21c9a"},"cell_type":"code","source":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+ b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99f3a8df2f848458d7f05ab9e61c372d15cff972"},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iterations):\n    dimension = x_train.shape[0]\n    w,b = initialize_weight_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,number_of_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"test accuracy : {} %\".format(100-np.mean(np.abs(y_prediction_test - y_test))*100))\n\n\nlogistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,number_of_iterations = 500) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cca6e582625c8eaaa9712f5142b9240ce7a1968"},"cell_type":"markdown","source":"<div id=\"2\"/>\n## Sklearn Logistic Regression\n>"},{"metadata":{"_uuid":"c0ad8ad9f684ced28ec0ec79949bd0fd3d1fb8d4"},"cell_type":"markdown","source":"These all functions all we have defined we can find all of it in sklearn library. Why we have done it? Actually we need to learn in fundamentally. Now you get its functionality."},{"metadata":{"trusted":true,"_uuid":"17e5fc2f20acbf05b99a92051035d121c75f53fd"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}