{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"id":"lDtDBFSbqrh_"}},{"cell_type":"markdown","source":"### Dataset Information: \n\nIBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. Clustering analysis can be performed on this dataset to group the employee based on the similar characteristics among them. ","metadata":{"id":"SaPvGmCqqriD"}},{"cell_type":"markdown","source":"### ATTRIBUTES:","metadata":{"id":"cL2vrepqqriE"}},{"cell_type":"markdown","source":"1 Age\n2 Attrition\n3 BusinessTravel\n4 DailyRate\n5 Department\n6 DistanceFromHome\n7 Education\n8 EducationField\n9 EmployeeCount\n10 EmployeeNumber\n11 EnvironmentSatisfaction\n12 Gender\n13 HourlyRate\n14 JobInvolvement\n15 JobLevel\n16 JobRole\n17 JobSatisfaction\n18 MaritalStatus\n19 MonthlyIncome\n20 MonthlyRate\n21 NumCompaniesWorked\n22 Over18\n23 OverTime\n24 PercentSalaryHike\n25 PerformanceRating\n26 RelationshipSatisfaction\n27 StandardHours\n28 StockOptionLevel\n29 TotalWorkingYears\n30 TrainingTimesLastYear\n31 WorkLifeBalance\n32 YearsAtCompany\n33 YearsInCurrentRole\n34 YearsSinceLastPromotion\n35 YearsWithCurrManager","metadata":{"id":"gaK_DB4bqriE"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans,AgglomerativeClustering\nfrom scipy.stats import zscore\nfrom sklearn.metrics import silhouette_score,classification_report\nimport pandas as pd\n\npd.options.display.max_columns=1000","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:17.245735Z","iopub.execute_input":"2021-05-27T17:32:17.24617Z","iopub.status.idle":"2021-05-27T17:32:18.686631Z","shell.execute_reply.started":"2021-05-27T17:32:17.24613Z","shell.execute_reply":"2021-05-27T17:32:18.685569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\n    \ndf=pd.read_csv(\"../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\ndf.head()\n","metadata":{"id":"LKQC6w15qriK","execution":{"iopub.status.busy":"2021-05-27T17:32:20.433274Z","iopub.execute_input":"2021-05-27T17:32:20.433749Z","iopub.status.idle":"2021-05-27T17:32:20.504667Z","shell.execute_reply.started":"2021-05-27T17:32:20.433718Z","shell.execute_reply":"2021-05-27T17:32:20.503791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Understanding","metadata":{"id":"UfW4YEq4qriK"}},{"cell_type":"code","source":"#data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:21.61854Z","iopub.execute_input":"2021-05-27T17:32:21.618926Z","iopub.status.idle":"2021-05-27T17:32:21.646767Z","shell.execute_reply.started":"2021-05-27T17:32:21.618896Z","shell.execute_reply":"2021-05-27T17:32:21.645975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the dataset\nprint('number of rows:',df.shape[0])\nprint('number of columns:',df.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:21.799628Z","iopub.execute_input":"2021-05-27T17:32:21.800137Z","iopub.status.idle":"2021-05-27T17:32:21.805638Z","shell.execute_reply.started":"2021-05-27T17:32:21.800107Z","shell.execute_reply":"2021-05-27T17:32:21.804558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:22.015685Z","iopub.execute_input":"2021-05-27T17:32:22.01618Z","iopub.status.idle":"2021-05-27T17:32:22.045731Z","shell.execute_reply.started":"2021-05-27T17:32:22.016144Z","shell.execute_reply":"2021-05-27T17:32:22.044491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:22.21925Z","iopub.execute_input":"2021-05-27T17:32:22.219629Z","iopub.status.idle":"2021-05-27T17:32:22.321625Z","shell.execute_reply.started":"2021-05-27T17:32:22.219598Z","shell.execute_reply":"2021-05-27T17:32:22.32052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical variables\ndf.select_dtypes(include='object').columns","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:23.043595Z","iopub.execute_input":"2021-05-27T17:32:23.044076Z","iopub.status.idle":"2021-05-27T17:32:23.054062Z","shell.execute_reply.started":"2021-05-27T17:32:23.04404Z","shell.execute_reply":"2021-05-27T17:32:23.05298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage of department category\na=df['Attrition'].value_counts()\nper=(a.values/df.shape[0])*100\np1=pd.DataFrame()\np1['Attrition']=df['Attrition'].unique()\np1['Percentage']=per\np1","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:23.240541Z","iopub.execute_input":"2021-05-27T17:32:23.240943Z","iopub.status.idle":"2021-05-27T17:32:23.256281Z","shell.execute_reply.started":"2021-05-27T17:32:23.240901Z","shell.execute_reply":"2021-05-27T17:32:23.255201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage of department category\na=df['Department'].value_counts()\nper=(a.values/df.shape[0])*100\np1=pd.DataFrame()\np1['Department_name']=df['Department'].unique()\np1['Percentage']=per\np1","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:23.421564Z","iopub.execute_input":"2021-05-27T17:32:23.422Z","iopub.status.idle":"2021-05-27T17:32:23.436633Z","shell.execute_reply.started":"2021-05-27T17:32:23.421962Z","shell.execute_reply":"2021-05-27T17:32:23.435857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage \na=df['MaritalStatus'].value_counts()\nper=(a.values/df.shape[0])*100\np1=pd.DataFrame()\np1['MaritalStatus']=df['MaritalStatus'].unique()\np1['Percentage']=per\np1","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:23.608526Z","iopub.execute_input":"2021-05-27T17:32:23.608902Z","iopub.status.idle":"2021-05-27T17:32:23.624044Z","shell.execute_reply.started":"2021-05-27T17:32:23.608863Z","shell.execute_reply":"2021-05-27T17:32:23.622789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage\na=df['Gender'].value_counts()\nper=(a.values/df.shape[0])*100\np1=pd.DataFrame()\np1['Over18']=df['Gender'].unique()\np1['Percentage']=per\np1","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:23.793472Z","iopub.execute_input":"2021-05-27T17:32:23.793878Z","iopub.status.idle":"2021-05-27T17:32:23.811426Z","shell.execute_reply.started":"2021-05-27T17:32:23.793846Z","shell.execute_reply":"2021-05-27T17:32:23.810156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percentage \na=df['OverTime'].value_counts()\nper=(a.values/df.shape[0])*100\np1=pd.DataFrame()\np1['OverTime']=df['OverTime'].unique()\np1['Percentage']=per\np1","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:24.009477Z","iopub.execute_input":"2021-05-27T17:32:24.009891Z","iopub.status.idle":"2021-05-27T17:32:24.026649Z","shell.execute_reply.started":"2021-05-27T17:32:24.009853Z","shell.execute_reply":"2021-05-27T17:32:24.025628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation\ndf.corr()","metadata":{"id":"gMnEm2KHqriL","execution":{"iopub.status.busy":"2021-05-27T17:32:24.226674Z","iopub.execute_input":"2021-05-27T17:32:24.227063Z","iopub.status.idle":"2021-05-27T17:32:24.296078Z","shell.execute_reply.started":"2021-05-27T17:32:24.227028Z","shell.execute_reply":"2021-05-27T17:32:24.294827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# covariance\ndf.cov()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:24.418469Z","iopub.execute_input":"2021-05-27T17:32:24.418877Z","iopub.status.idle":"2021-05-27T17:32:24.495071Z","shell.execute_reply.started":"2021-05-27T17:32:24.41884Z","shell.execute_reply":"2021-05-27T17:32:24.493564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# Pairplot\nsns.pairplot(df)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr() , annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:24.762128Z","iopub.execute_input":"2021-05-27T17:32:24.762513Z","iopub.status.idle":"2021-05-27T17:32:28.1544Z","shell.execute_reply.started":"2021-05-27T17:32:24.76248Z","shell.execute_reply":"2021-05-27T17:32:28.153424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### INFERENCES:\n1. From the above correlation matrix, we can observe there is strong positive correlation between some variables\n","metadata":{}},{"cell_type":"code","source":"# boxplot\ndf_num=df.select_dtypes(exclude='object')\ndf_num.drop(columns='PerformanceRating' , inplace=True)\nfor i in range(len(df_num.columns)):\n    sns.boxplot(df_num.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:28.155967Z","iopub.execute_input":"2021-05-27T17:32:28.156547Z","iopub.status.idle":"2021-05-27T17:32:31.378451Z","shell.execute_reply.started":"2021-05-27T17:32:28.156504Z","shell.execute_reply":"2021-05-27T17:32:31.377437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation \n\na.\tScale / Transform/ clean the data so that it is suitable for model building.\n","metadata":{"id":"4O0K48PXqriO"}},{"cell_type":"code","source":"# Check unique values\ndf.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:31.380785Z","iopub.execute_input":"2021-05-27T17:32:31.381222Z","iopub.status.idle":"2021-05-27T17:32:31.403917Z","shell.execute_reply.started":"2021-05-27T17:32:31.381177Z","shell.execute_reply":"2021-05-27T17:32:31.403194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is 1 unique value in column, we will remove this column\ndf.drop(columns=['EmployeeCount','StandardHours'] , inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:31.405058Z","iopub.execute_input":"2021-05-27T17:32:31.405444Z","iopub.status.idle":"2021-05-27T17:32:31.410908Z","shell.execute_reply.started":"2021-05-27T17:32:31.405414Z","shell.execute_reply":"2021-05-27T17:32:31.409722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# null values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:31.412136Z","iopub.execute_input":"2021-05-27T17:32:31.412435Z","iopub.status.idle":"2021-05-27T17:32:31.431182Z","shell.execute_reply.started":"2021-05-27T17:32:31.412408Z","shell.execute_reply":"2021-05-27T17:32:31.429984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are no null values present in the data","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:31.432528Z","iopub.execute_input":"2021-05-27T17:32:31.432974Z","iopub.status.idle":"2021-05-27T17:32:31.444079Z","shell.execute_reply.started":"2021-05-27T17:32:31.432931Z","shell.execute_reply":"2021-05-27T17:32:31.443259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check Outliers\n# boxplot\ndf_num=df.select_dtypes(exclude='object')\nfor i in range(len(df_num.columns)):\n    sns.boxplot(df_num.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:31.738968Z","iopub.execute_input":"2021-05-27T17:32:31.739462Z","iopub.status.idle":"2021-05-27T17:32:34.852083Z","shell.execute_reply.started":"2021-05-27T17:32:31.739432Z","shell.execute_reply":"2021-05-27T17:32:34.851089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num.skew()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:34.853845Z","iopub.execute_input":"2021-05-27T17:32:34.854255Z","iopub.status.idle":"2021-05-27T17:32:34.865196Z","shell.execute_reply.started":"2021-05-27T17:32:34.854213Z","shell.execute_reply":"2021-05-27T17:32:34.86404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. We can observe from the above boxplot & skewness values that there are no extreme values present in any of the column\n2. Skewness values are also not large so this much skewness is fine to proceed for furthur analysis\n3. we will transform two columns having high skewness yearsatcompany & yearssince last promotion","metadata":{}},{"cell_type":"code","source":"# transformation of yearsatcompany\n\nprint('\\nSkewness before transformation:',df['YearsAtCompany'].skew())\ndf['YearsAtCompany']=np.sqrt(df['YearsAtCompany'])\nprint('\\nSkewness after transformation:',df['YearsAtCompany'].skew())","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:34.867251Z","iopub.execute_input":"2021-05-27T17:32:34.867733Z","iopub.status.idle":"2021-05-27T17:32:34.877884Z","shell.execute_reply.started":"2021-05-27T17:32:34.867664Z","shell.execute_reply":"2021-05-27T17:32:34.876617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformation of YearsSinceLastPromotion\n\nprint('\\nSkewness before transformation:',df['YearsSinceLastPromotion'].skew())\ndf['YearsSinceLastPromotion']=np.sqrt(df['YearsSinceLastPromotion'])\nprint('\\nSkewness after transformation:',df['YearsSinceLastPromotion'].skew())","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:34.879462Z","iopub.execute_input":"2021-05-27T17:32:34.879948Z","iopub.status.idle":"2021-05-27T17:32:34.89096Z","shell.execute_reply.started":"2021-05-27T17:32:34.8799Z","shell.execute_reply":"2021-05-27T17:32:34.889444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will scale the numerical data\ndf_num=df.select_dtypes(exclude='object')\ndf_num_scaled=df_num.apply(zscore)\n\n# we will encode categorical data\ndf_cat=df.select_dtypes(include='object')\ndf_cat_dummy=pd.get_dummies(df_cat, drop_first=True)\n\n\n# concat numerical & categorical data\nxscaled=pd.concat([df_num_scaled,df_cat_dummy] , axis=1).reset_index(drop=True)\nxscaled.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:34.932543Z","iopub.execute_input":"2021-05-27T17:32:34.932933Z","iopub.status.idle":"2021-05-27T17:32:34.998552Z","shell.execute_reply.started":"2021-05-27T17:32:34.932901Z","shell.execute_reply":"2021-05-27T17:32:34.997256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xscaled.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:36.191214Z","iopub.execute_input":"2021-05-27T17:32:36.19159Z","iopub.status.idle":"2021-05-27T17:32:36.199776Z","shell.execute_reply.started":"2021-05-27T17:32:36.191561Z","shell.execute_reply":"2021-05-27T17:32:36.198772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimensionality Reduction : Principal component analysis\n","metadata":{"id":"n5B6F4tZqriR"}},{"cell_type":"markdown","source":"1. When we observe the correlation between two independent features in the dataset , we can say that there is multicollinearity exists in the given data.\nSo we can apply Principal component analysis to reduce the independent/insignificant feature dimentions so that to remove multicollinearity effect in the dataset\n\n2. Features which are strongly correlated with each other needs PCA.\n\n3. From the correlation matrix in the que1 , we can observe that there are some variables which are having strong correlation with another variable.\n\n4. The total working years is strongly correlated with the age,job level , monthly income and years at company\nalso years at company are strongly correlated with the  Years in current role, years with current manager.\n\n5. We need to remove on of the features or 2 or more features which are correlated with each other . so that redundant data will not be present while building model.\n\n5. We will apply PCA to remove the multicollinearity effect\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:39.729651Z","iopub.execute_input":"2021-05-27T17:32:39.730087Z","iopub.status.idle":"2021-05-27T17:32:39.73339Z","shell.execute_reply.started":"2021-05-27T17:32:39.730055Z","shell.execute_reply":"2021-05-27T17:32:39.732699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will use the scaled data - xscaled\n# COLUMNS \nxscaled.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:40.54842Z","iopub.execute_input":"2021-05-27T17:32:40.548932Z","iopub.status.idle":"2021-05-27T17:32:40.554204Z","shell.execute_reply.started":"2021-05-27T17:32:40.5489Z","shell.execute_reply":"2021-05-27T17:32:40.553542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will take all the components to find cumulative variance\npca=PCA(n_components=29)\ncompo=pca.fit_transform(xscaled)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:41.201021Z","iopub.execute_input":"2021-05-27T17:32:41.201546Z","iopub.status.idle":"2021-05-27T17:32:41.248611Z","shell.execute_reply.started":"2021-05-27T17:32:41.201515Z","shell.execute_reply":"2021-05-27T17:32:41.247378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explained variance ratio\nevr=pca.explained_variance_ratio_*100\nevr","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:41.822203Z","iopub.execute_input":"2021-05-27T17:32:41.822542Z","iopub.status.idle":"2021-05-27T17:32:41.829768Z","shell.execute_reply.started":"2021-05-27T17:32:41.822513Z","shell.execute_reply":"2021-05-27T17:32:41.828422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cumulative explained variance \ncevr=np.cumsum(evr)\ncevr","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:42.693311Z","iopub.execute_input":"2021-05-27T17:32:42.693649Z","iopub.status.idle":"2021-05-27T17:32:42.701919Z","shell.execute_reply.started":"2021-05-27T17:32:42.69362Z","shell.execute_reply":"2021-05-27T17:32:42.700658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot explained variance ratio & cumulative evr\nplt.figure(figsize=(20,12))\nplt.bar(np.arange(29),evr)\nplt.step(np.arange(29),cevr)\nplt.xticks(np.arange(29))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:43.402995Z","iopub.execute_input":"2021-05-27T17:32:43.403391Z","iopub.status.idle":"2021-05-27T17:32:43.901015Z","shell.execute_reply.started":"2021-05-27T17:32:43.403357Z","shell.execute_reply":"2021-05-27T17:32:43.899849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### INFERENCES:\n1. From the above cummulative graph & the cummulative variance ratio graph , we can clearly see that after dimention 18 there is no much difference in the variance values \n\n2. For the 90%  variance in the data , we will choose the 18 components which are explaining 90.53% of the variance in the data.\n\n3. Number of components choosen using PCA are 18.","metadata":{}},{"cell_type":"code","source":"# xpca - for furthur analysis\n\nxpca=PCA(n_components=22).fit_transform(xscaled)\n\n#shape of xpca\nxpca.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:45.010876Z","iopub.execute_input":"2021-05-27T17:32:45.011221Z","iopub.status.idle":"2021-05-27T17:32:45.179982Z","shell.execute_reply.started":"2021-05-27T17:32:45.011194Z","shell.execute_reply":"2021-05-27T17:32:45.178636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 18 components\nxpca","metadata":{"id":"KOVUJiMcqriS","execution":{"iopub.status.busy":"2021-05-27T17:32:45.637163Z","iopub.execute_input":"2021-05-27T17:32:45.637503Z","iopub.status.idle":"2021-05-27T17:32:45.644199Z","shell.execute_reply.started":"2021-05-27T17:32:45.637474Z","shell.execute_reply":"2021-05-27T17:32:45.643138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframe using pca\ndfpca=pd.DataFrame(data=xpca, columns=np.arange(1,23))\ndfpca.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:47.378643Z","iopub.execute_input":"2021-05-27T17:32:47.37903Z","iopub.status.idle":"2021-05-27T17:32:47.409802Z","shell.execute_reply.started":"2021-05-27T17:32:47.379001Z","shell.execute_reply":"2021-05-27T17:32:47.408591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check multicollinearity\nplt.figure(figsize=(20,10))\nsns.heatmap(dfpca.corr() , annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:47.949015Z","iopub.execute_input":"2021-05-27T17:32:47.949367Z","iopub.status.idle":"2021-05-27T17:32:50.631801Z","shell.execute_reply.started":"2021-05-27T17:32:47.949338Z","shell.execute_reply":"2021-05-27T17:32:50.630762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENECES:\n\n1. From the above correlation matrix , we can say that there is no multicollinearity present in the features obtained using PCA.\n2. Our main objective was to remove the multicollinearity using PCA.\n2. We can procced with the above dimentions for furthur analysis .","metadata":{}},{"cell_type":"code","source":"# spread of the data using distribution plot\nfor i in range(len(dfpca.columns)):\n    sns.distplot(dfpca.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:50.633515Z","iopub.execute_input":"2021-05-27T17:32:50.633978Z","iopub.status.idle":"2021-05-27T17:32:56.053191Z","shell.execute_reply.started":"2021-05-27T17:32:50.633934Z","shell.execute_reply":"2021-05-27T17:32:56.052223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#skewness in the pca dimentions\ndfpca.skew()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:56.056837Z","iopub.execute_input":"2021-05-27T17:32:56.057164Z","iopub.status.idle":"2021-05-27T17:32:56.06667Z","shell.execute_reply.started":"2021-05-27T17:32:56.057132Z","shell.execute_reply":"2021-05-27T17:32:56.06565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check Outliers\n# boxplot\nfor i in range(len(dfpca.columns)):\n    sns.boxplot(dfpca.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:56.068416Z","iopub.execute_input":"2021-05-27T17:32:56.068758Z","iopub.status.idle":"2021-05-27T17:32:58.799666Z","shell.execute_reply.started":"2021-05-27T17:32:56.068714Z","shell.execute_reply":"2021-05-27T17:32:58.798916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#skewness in the pca dimentions\ndfpca.skew()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:58.800667Z","iopub.execute_input":"2021-05-27T17:32:58.801018Z","iopub.status.idle":"2021-05-27T17:32:58.811266Z","shell.execute_reply.started":"2021-05-27T17:32:58.800986Z","shell.execute_reply":"2021-05-27T17:32:58.810171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENCES\n\n1. From the above distribution plot, we can say that the data is normally distributed in each feature.\n2. Also from the boxplot & skewness values , we can observe that there are no extreme values exists in the features.\n3. The skewness in each feature is also not large , so we can procced with the pca dimentions without treating outliers.\n4. There is no need to treat outliers , but still we will use Inter quartile range method to remove them if any.","metadata":{}},{"cell_type":"code","source":"q1=dfpca.quantile(0.25)\nq3=dfpca.quantile(0.75)\niqr=q3-q1\n\nll=q1-iqr\nul=q3+iqr\n\ndfpca=dfpca[~((dfpca<ll) | (dfpca>ul)).any(axis=1)]\ndfpca=dfpca.reset_index(drop=True)\ndfpca.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:58.812683Z","iopub.execute_input":"2021-05-27T17:32:58.81314Z","iopub.status.idle":"2021-05-27T17:32:58.854861Z","shell.execute_reply.started":"2021-05-27T17:32:58.813107Z","shell.execute_reply":"2021-05-27T17:32:58.853815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check Outliers after removing them\n# boxplot\nfor i in range(len(dfpca.columns)):\n    sns.boxplot(dfpca.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:32:58.856182Z","iopub.execute_input":"2021-05-27T17:32:58.856464Z","iopub.status.idle":"2021-05-27T17:33:01.64214Z","shell.execute_reply.started":"2021-05-27T17:32:58.856436Z","shell.execute_reply":"2021-05-27T17:33:01.641047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check skewness\ndfpca.skew()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:01.645734Z","iopub.execute_input":"2021-05-27T17:33:01.646043Z","iopub.status.idle":"2021-05-27T17:33:01.656455Z","shell.execute_reply.started":"2021-05-27T17:33:01.646013Z","shell.execute_reply":"2021-05-27T17:33:01.655216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can observe that outliers are removed from pca dataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:01.658334Z","iopub.execute_input":"2021-05-27T17:33:01.658893Z","iopub.status.idle":"2021-05-27T17:33:01.663732Z","shell.execute_reply.started":"2021-05-27T17:33:01.658842Z","shell.execute_reply":"2021-05-27T17:33:01.662625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KMeans clustering","metadata":{}},{"cell_type":"code","source":"# kmeans clustering \n\n#INERTIA VALUES\n\ninert=[]\n\nfor k in range(1,12):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(dfpca)\n    inert.append(kmeans.inertia_)\n    \n    \n    \n# inertia values for each cluster number\ninertia=pd.DataFrame()\ninertia['Clusters']=np.arange(1,12)\ninertia['inertia']=inert\ninertia \n ","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:01.665489Z","iopub.execute_input":"2021-05-27T17:33:01.665993Z","iopub.status.idle":"2021-05-27T17:33:03.600223Z","shell.execute_reply.started":"2021-05-27T17:33:01.665915Z","shell.execute_reply":"2021-05-27T17:33:03.59924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ELBOW PLOT - to find best value for cluster number\nplt.figure(figsize=(15,8))\nplt.plot(range(1,12),inertia['inertia'],color='red',marker='*')\nplt.xticks(np.arange(1,12))\nplt.xlabel('Number of clusters',fontsize=15)\nplt.ylabel('Inertia',fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:03.60201Z","iopub.execute_input":"2021-05-27T17:33:03.602652Z","iopub.status.idle":"2021-05-27T17:33:03.835036Z","shell.execute_reply.started":"2021-05-27T17:33:03.60261Z","shell.execute_reply":"2021-05-27T17:33:03.834274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. From the above elbow plot & inertia score , we will choose the best cluster value as 3.\nbecause after that the change in inertia value is very less as compared to others\n\n2. We can observe sharp bend at 2 clusters, so we will build  Kmeans model using 3 clusters","metadata":{}},{"cell_type":"code","source":"# kmeans model is build\nkmeans=KMeans(n_clusters=2 , n_init=15, random_state=10)\nkmeans.fit(dfpca)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:11.685228Z","iopub.execute_input":"2021-05-27T17:33:11.685763Z","iopub.status.idle":"2021-05-27T17:33:11.810118Z","shell.execute_reply.started":"2021-05-27T17:33:11.685729Z","shell.execute_reply":"2021-05-27T17:33:11.809218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inertia score\nprint('\\nInertia in the kmeans clustering',kmeans.inertia_)\n\n#silhoutte score\nprint('\\nSilhoutte score for kmeans clustering',silhouette_score(dfpca,kmeans.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:11.934019Z","iopub.execute_input":"2021-05-27T17:33:11.93437Z","iopub.status.idle":"2021-05-27T17:33:11.962903Z","shell.execute_reply.started":"2021-05-27T17:33:11.934342Z","shell.execute_reply":"2021-05-27T17:33:11.961544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframe with label attached for kmeans clustering \n\ndfkmean=dfpca.copy()\ndfkmean['label']=kmeans.labels_\ndfkmean.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:12.565521Z","iopub.execute_input":"2021-05-27T17:33:12.565895Z","iopub.status.idle":"2021-05-27T17:33:12.599544Z","shell.execute_reply.started":"2021-05-27T17:33:12.565862Z","shell.execute_reply":"2021-05-27T17:33:12.598388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of clusters \n# 2D\n# pca component 1 & 2\nplt.figure(figsize=(10,6))\nplt.scatter(dfkmean[dfkmean.columns[0]],dfkmean[dfkmean.columns[1]],c=kmeans.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:13.083087Z","iopub.execute_input":"2021-05-27T17:33:13.083449Z","iopub.status.idle":"2021-05-27T17:33:13.262431Z","shell.execute_reply.started":"2021-05-27T17:33:13.08342Z","shell.execute_reply":"2021-05-27T17:33:13.261422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca component 2 & 3\nplt.figure(figsize=(10,8))\nplt.scatter(dfkmean[dfkmean.columns[1]],dfkmean[dfkmean.columns[2]],c=kmeans.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:33:13.990352Z","iopub.execute_input":"2021-05-27T17:33:13.990758Z","iopub.status.idle":"2021-05-27T17:33:14.176384Z","shell.execute_reply.started":"2021-05-27T17:33:13.990721Z","shell.execute_reply":"2021-05-27T17:33:14.175099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Agglomerative clustering","metadata":{}},{"cell_type":"code","source":"# dendrogram to find best clusters number\n\nfrom scipy.cluster.hierarchy import linkage,dendrogram\n\nplt.figure(figsize=(15,10))\n\nz=linkage(dfpca, method='ward')\n\n\n# note that, color threshold is adjusted after observing dendrogram\n\ndendrogram(z , leaf_rotation=90,color_threshold=21)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:06.661831Z","iopub.execute_input":"2021-05-27T17:34:06.662389Z","iopub.status.idle":"2021-05-27T17:34:18.929454Z","shell.execute_reply.started":"2021-05-27T17:34:06.662342Z","shell.execute_reply":"2021-05-27T17:34:18.92876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. We can clearly observe the 3 clusters in the above dendrogram plot\n2. So for agglomerative clustering , we will choose 3 clusters as best cluster number.\n3. Optimum clusters got using dendrogram are also 3\n3. We will build agglomerative clustering model using 3 clusters","metadata":{}},{"cell_type":"code","source":"# Agglomerative clustering\n\naglo=AgglomerativeClustering(n_clusters=3 , affinity='euclidean', linkage='ward')\naglo.fit(dfpca)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:29.274729Z","iopub.execute_input":"2021-05-27T17:34:29.275358Z","iopub.status.idle":"2021-05-27T17:34:29.293511Z","shell.execute_reply.started":"2021-05-27T17:34:29.275312Z","shell.execute_reply":"2021-05-27T17:34:29.29265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframe is saved with agglomerative clustering labels\ndfaglo=dfpca.copy()\ndfaglo['label']=aglo.labels_\ndfaglo.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:30.527319Z","iopub.execute_input":"2021-05-27T17:34:30.527677Z","iopub.status.idle":"2021-05-27T17:34:30.557872Z","shell.execute_reply.started":"2021-05-27T17:34:30.527648Z","shell.execute_reply":"2021-05-27T17:34:30.55697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inertia for agglomerative clusters - calculated manually","metadata":{}},{"cell_type":"code","source":"# group by labels\nagc=dfaglo.groupby(['label'])\ndf0=agc.get_group(0)\ndf1=agc.get_group(1)\ndf2=agc.get_group(2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:39.072363Z","iopub.execute_input":"2021-05-27T17:34:39.072919Z","iopub.status.idle":"2021-05-27T17:34:39.080201Z","shell.execute_reply.started":"2021-05-27T17:34:39.072886Z","shell.execute_reply":"2021-05-27T17:34:39.079198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find centroids for each cluster\nc0=np.array(df0.mean())\nc1=np.array(df1.mean())\nc2=np.array(df2.mean())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:42.387584Z","iopub.execute_input":"2021-05-27T17:34:42.388163Z","iopub.status.idle":"2021-05-27T17:34:42.395955Z","shell.execute_reply.started":"2021-05-27T17:34:42.38813Z","shell.execute_reply":"2021-05-27T17:34:42.395104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# exclude last column of labels\nc0=c0[:-1]\nc1=c1[:-1]\nc2=c2[:-1]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:34:46.354512Z","iopub.execute_input":"2021-05-27T17:34:46.356685Z","iopub.status.idle":"2021-05-27T17:34:46.361437Z","shell.execute_reply.started":"2021-05-27T17:34:46.35664Z","shell.execute_reply":"2021-05-27T17:34:46.360516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find inertia for each cluster \nagi0=0\nagi1=0\nagi2=0\n\nfor i in np.arange(df0.shape[0]):\n    agi0=agi0+np.sum((df0.iloc[i,:-1]-c0)**2)\n    \n\nfor i in np.arange(df1.shape[0]):\n    agi1=agi1+np.sum((df1.iloc[i,:-1]-c1)**2)    \n    \n\nfor i in np.arange(df2.shape[0]):\n    agi2=agi2+np.sum((df2.iloc[i,:-1]-c2)**2)    ","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:03.089809Z","iopub.execute_input":"2021-05-27T17:35:03.09015Z","iopub.status.idle":"2021-05-27T17:35:03.491574Z","shell.execute_reply.started":"2021-05-27T17:35:03.090121Z","shell.execute_reply":"2021-05-27T17:35:03.490757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add all the inertia scores\n\ntotal_aglo_inertia=agi0+agi1 #+agi2\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:05.21742Z","iopub.execute_input":"2021-05-27T17:35:05.217942Z","iopub.status.idle":"2021-05-27T17:35:05.221564Z","shell.execute_reply.started":"2021-05-27T17:35:05.21791Z","shell.execute_reply":"2021-05-27T17:35:05.220652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inertia score\nprint('\\nInertia in the agglomerative clustering',total_aglo_inertia)\n\n#silhoutte score\nprint('\\nSilhoutte score for agglomerative clustering',silhouette_score(dfpca,aglo.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:06.353347Z","iopub.execute_input":"2021-05-27T17:35:06.353916Z","iopub.status.idle":"2021-05-27T17:35:06.375134Z","shell.execute_reply.started":"2021-05-27T17:35:06.353844Z","shell.execute_reply":"2021-05-27T17:35:06.373589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clusters plot for agglomerative clustering","metadata":{}},{"cell_type":"code","source":"# Visualization of clusters in agglomerative clustering \n# 2D\n# pca component 1 & 2\nplt.figure(figsize=(10,6))\nplt.scatter(dfaglo[dfaglo.columns[0]],dfaglo[dfaglo.columns[1]],c=aglo.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:10.114913Z","iopub.execute_input":"2021-05-27T17:35:10.115235Z","iopub.status.idle":"2021-05-27T17:35:10.297836Z","shell.execute_reply.started":"2021-05-27T17:35:10.115209Z","shell.execute_reply":"2021-05-27T17:35:10.296758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca component 1 & 2\nplt.figure(figsize=(10,6))\nplt.scatter(dfaglo[dfaglo.columns[1]],dfaglo[dfaglo.columns[2]],c=aglo.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:12.114398Z","iopub.execute_input":"2021-05-27T17:35:12.115028Z","iopub.status.idle":"2021-05-27T17:35:12.288887Z","shell.execute_reply.started":"2021-05-27T17:35:12.114975Z","shell.execute_reply":"2021-05-27T17:35:12.288163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## COMAPARISON: silhouette_score and Inertia of KMeans & Agglomerative clustering","metadata":{}},{"cell_type":"code","source":"# Kmeans clustering\n\n# inertia score\nprint('\\nInertia in the kmeans clustering',kmeans.inertia_)\n#silhoutte score\nprint('\\nSilhoutte score for kmeans clustering',silhouette_score(dfpca,kmeans.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:19.099082Z","iopub.execute_input":"2021-05-27T17:35:19.099683Z","iopub.status.idle":"2021-05-27T17:35:19.128357Z","shell.execute_reply.started":"2021-05-27T17:35:19.099636Z","shell.execute_reply":"2021-05-27T17:35:19.1252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# agglomerative clustering\n\n# inertia score\nprint('\\nInertia in the agglomerative clustering',total_aglo_inertia)\n#silhoutte score\nprint('\\nSilhoutte score for agglomerative clustering',silhouette_score(dfpca,aglo.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:22.887915Z","iopub.execute_input":"2021-05-27T17:35:22.888272Z","iopub.status.idle":"2021-05-27T17:35:22.914211Z","shell.execute_reply.started":"2021-05-27T17:35:22.888244Z","shell.execute_reply":"2021-05-27T17:35:22.912856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENCES\n\n1. From the inertia , we can observe that the inertia value for clusters obtained using kmeans clustering is less than the inertia value for the clusters obtained using agglomerative clusters.\nIt means that the clusters obtained from kmeans clustering are more tighter than the clusters obtained from the agglomerative clustering\n\n2. Silhoutte score is another matrix to check the quality of the clusters , silhoutte score is high for kmeans clusteing than the agglomerative clustering\n\n3. So by comaparing inertia & silhoutte score , we will choose the clusters obtained from the kmeans clustering model for furthur classification model building","metadata":{}},{"cell_type":"markdown","source":"## COMAPARISON: Cluster visualization of KMeans & Agglomerative clustering","metadata":{}},{"cell_type":"code","source":"# Visualization of clusters with kmeans clustering\n# 2D\n# pca component 1 & 2\nplt.figure(figsize=(10,6))\nplt.scatter(dfkmean[dfkmean.columns[0]],dfkmean[dfkmean.columns[1]],c=kmeans.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:34.24011Z","iopub.execute_input":"2021-05-27T17:35:34.24048Z","iopub.status.idle":"2021-05-27T17:35:34.420991Z","shell.execute_reply.started":"2021-05-27T17:35:34.240451Z","shell.execute_reply":"2021-05-27T17:35:34.419854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of clusters in agglomerative clustering \n# 2D\n# pca component 1 & 2\nplt.figure(figsize=(10,6))\nplt.scatter(dfaglo[dfaglo.columns[0]],dfaglo[dfaglo.columns[1]],c=aglo.labels_ , cmap=plt.cm.Set1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:35:35.990969Z","iopub.execute_input":"2021-05-27T17:35:35.991331Z","iopub.status.idle":"2021-05-27T17:35:36.176591Z","shell.execute_reply.started":"2021-05-27T17:35:35.991302Z","shell.execute_reply":"2021-05-27T17:35:36.175536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENCES:\n\n1. Cluster plot for 2 components obtained from kmeans clustering are clearly visble as 2 clusters.\nbut we cannot observe seperate clusters in agglomerative cluster plot\n\n2. Using agglomerative clustering we got 3 clusters.\n\n2. We will choose aglomerative clustering labeled dataframe for classification model build because it has less inertia score.","metadata":{}},{"cell_type":"code","source":"# spread of the data using distribution plot\nfor i in range(len(dfpca.columns)):\n    sns.distplot(dfpca.iloc[:,i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:08.655295Z","iopub.execute_input":"2021-05-27T17:37:08.655666Z","iopub.status.idle":"2021-05-27T17:37:13.302306Z","shell.execute_reply.started":"2021-05-27T17:37:08.655634Z","shell.execute_reply":"2021-05-27T17:37:13.301195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"sBPpyKmFqriV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##\tUse the cluster labels from the best method above and interpret the clusters formed.","metadata":{"id":"67brwj1QqriZ"}},{"cell_type":"markdown","source":"### KMeans clustering labelled dataframe is used & Logistic regression classification model is build to check the accuracy of the model","metadata":{}},{"cell_type":"code","source":"# As decide above , we go with the labelled dataframe obatained from kmeans clusterig\n# For interpreting the clusters formed using above clustering models \n# we will build classification model to check the accuracy of the model , so that we will get to know how correctly\n# kmeans clusters has beed done","metadata":{"id":"cboxHWJLqria","execution":{"iopub.status.busy":"2021-05-27T17:37:13.304137Z","iopub.execute_input":"2021-05-27T17:37:13.304491Z","iopub.status.idle":"2021-05-27T17:37:13.30844Z","shell.execute_reply.started":"2021-05-27T17:37:13.304458Z","shell.execute_reply":"2021-05-27T17:37:13.30742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use this dataframe\ndfkmean.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:13.311157Z","iopub.execute_input":"2021-05-27T17:37:13.311665Z","iopub.status.idle":"2021-05-27T17:37:13.349372Z","shell.execute_reply.started":"2021-05-27T17:37:13.311545Z","shell.execute_reply":"2021-05-27T17:37:13.348091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split x & y\nfrom sklearn.model_selection import train_test_split\nx=dfkmean.drop(columns=['label'])\ny=dfkmean['label']\n\nxtrain,xtest,ytrain,ytest=train_test_split(x , y , test_size=0.3, random_state=20)\n\n#check shape of the dataframe\n(xtrain.shape, xtest.shape , ytrain.shape , ytest.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:13.35167Z","iopub.execute_input":"2021-05-27T17:37:13.352165Z","iopub.status.idle":"2021-05-27T17:37:13.365014Z","shell.execute_reply.started":"2021-05-27T17:37:13.352118Z","shell.execute_reply":"2021-05-27T17:37:13.363678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(xtrain,ytrain)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:13.366836Z","iopub.execute_input":"2021-05-27T17:37:13.367292Z","iopub.status.idle":"2021-05-27T17:37:13.389991Z","shell.execute_reply.started":"2021-05-27T17:37:13.367244Z","shell.execute_reply":"2021-05-27T17:37:13.388642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# train data accuracy\nytrain_pred=lr.predict(xtrain)\nacc_train=accuracy_score(ytrain , ytrain_pred)\nprint('\\nAccuracy for train data : ',acc_train)\n\n# test data accuracy\nytest_pred=lr.predict(xtest)\nacc_test=accuracy_score(ytest , ytest_pred)\nprint('\\nAccuracy for test data : ',acc_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:13.391385Z","iopub.execute_input":"2021-05-27T17:37:13.391683Z","iopub.status.idle":"2021-05-27T17:37:13.406621Z","shell.execute_reply.started":"2021-05-27T17:37:13.391652Z","shell.execute_reply":"2021-05-27T17:37:13.405484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report\nprint('\\nClassification Report : \\n')\nprint(classification_report(ytest,ytest_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:13.408083Z","iopub.execute_input":"2021-05-27T17:37:13.408922Z","iopub.status.idle":"2021-05-27T17:37:13.419637Z","shell.execute_reply.started":"2021-05-27T17:37:13.408863Z","shell.execute_reply":"2021-05-27T17:37:13.418896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENCES\n\n1. From the above measures,\nThe accuracy score for the train model is 99.3% & for the test model it is 98.38%.\nSince both the accuracy scores are almost same and having better accuracy scores we can say that the clusters which we got in KMeans clustering are extracting perfect hidden patterns from the data and then giving accurate clusters.\n\n2. Classification report is showing precision , recall & f1-score for each label.\nAll the scores are good.\n\n3. By observing above accuracy scores , we can say that the clusters got from kmeans clustering models are accurate.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##\tSummary\n\n\n","metadata":{"id":"vIJKTA_9qric"}},{"cell_type":"markdown","source":"1. Initially data is observed, size of the rows & column, five point summary \n2. Data is cleaned - Null values , outliers treatement\n3. Checked multicollinearity - To remove multicollinearity we used PCA to reduce the dimention & redundancy in the data\n4. Then we used two clustering models - 1. KMeans 2.Agglomerative\n5. Before building clustering model , we found best value for cluster\nIn kmeans- we choosen optimal value for cluster using Iertia & ELBOW plot\nIn Agglomerative - we choosen optimal value for cluster using Dendrogram\n6. In both Elbow plot & dendrogram , we observed that the best value for clusters is 3\n7. We build two models and found out inertia & silhoutte scores for each.We can observe it below.\nWe got less inertia & high silhoutte score for kmeans clustering model, so we can say that for this dataset kmeans clustering is showing better clusters than agglomerative clusters.\n8. To interpret whether the clustering is accurate & how much accurate it is , we build classification model to check the accuracy scores of the train & test data.\n9. The accuracy score for the train model is 99.3% & for the test model it is 98.38%.\nSince both the accuracy scores are almost same and having better accuracy scores we can say that the clusters which we got in KMeans clustering are extracting perfect hidden patterns from the data and then giving accurate clusters.\n\n10. To check the model is good or not:We used below measures\n\n   1. To choose best clustering model  ->  INERTIA, silhoette score\n   2. To check the clusters accuracy   ->  accuracy score , precision, recall, f1-score","metadata":{}},{"cell_type":"code","source":"# Kmeans clustering\nsilhouette_score\n# inertia score\nprint('\\nInertia in the kmeans clustering',kmeans.inertia_)\n#silhoutte score\nprint('\\nSilhoutte score for kmeans clustering',silhouette_score(dfpca,kmeans.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:14.306716Z","iopub.execute_input":"2021-05-27T17:37:14.307062Z","iopub.status.idle":"2021-05-27T17:37:14.329776Z","shell.execute_reply.started":"2021-05-27T17:37:14.307033Z","shell.execute_reply":"2021-05-27T17:37:14.328364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# agglomerative clustering\n\n# inertia score\nprint('\\nInertia in the agglomerative clustering',total_aglo_inertia)\n#silhoutte score\nprint('\\nSilhoutte score for agglomerative clustering',silhouette_score(dfpca,aglo.labels_))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:15.236608Z","iopub.execute_input":"2021-05-27T17:37:15.236996Z","iopub.status.idle":"2021-05-27T17:37:15.259832Z","shell.execute_reply.started":"2021-05-27T17:37:15.236966Z","shell.execute_reply":"2021-05-27T17:37:15.258573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For business interpretetion , i have used cluster plots to check on which basis the clusters are made","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:15.984112Z","iopub.execute_input":"2021-05-27T17:37:15.984476Z","iopub.status.idle":"2021-05-27T17:37:15.988592Z","shell.execute_reply.started":"2021-05-27T17:37:15.984448Z","shell.execute_reply":"2021-05-27T17:37:15.98732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kmeans model is build\nkmeans1=KMeans(n_clusters=2 , n_init=15, random_state=10)\nkmeans1.fit(xscaled)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:17.671539Z","iopub.execute_input":"2021-05-27T17:37:17.671917Z","iopub.status.idle":"2021-05-27T17:37:19.823357Z","shell.execute_reply.started":"2021-05-27T17:37:17.671887Z","shell.execute_reply":"2021-05-27T17:37:19.822217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xscaled.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:19.824853Z","iopub.execute_input":"2021-05-27T17:37:19.825187Z","iopub.status.idle":"2021-05-27T17:37:19.834475Z","shell.execute_reply.started":"2021-05-27T17:37:19.825155Z","shell.execute_reply":"2021-05-27T17:37:19.832674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of clusters with kmeans clustering\n# 2D\nplt.figure(figsize=(10,6))\nplt.scatter(xscaled['YearsAtCompany'],xscaled['MonthlyIncome'],c=kmeans1.labels_ , cmap=plt.cm.Set1)\nplt.xlabel('Years at company', fontsize=15)\nplt.ylabel('Monthly income', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:19.836225Z","iopub.execute_input":"2021-05-27T17:37:19.83657Z","iopub.status.idle":"2021-05-27T17:37:20.052818Z","shell.execute_reply.started":"2021-05-27T17:37:19.836538Z","shell.execute_reply":"2021-05-27T17:37:20.051738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(xscaled['YearsAtCompany'],xscaled['PercentSalaryHike'],c=kmeans1.labels_ , cmap=plt.cm.Set1)\nplt.xlabel('Years at company', fontsize=15)\nplt.ylabel('Percent salary hike', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T17:37:20.054109Z","iopub.execute_input":"2021-05-27T17:37:20.054411Z","iopub.status.idle":"2021-05-27T17:37:20.259169Z","shell.execute_reply.started":"2021-05-27T17:37:20.054381Z","shell.execute_reply":"2021-05-27T17:37:20.258414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERENCES:\n\n1. We have build model using 2 clusters & got good accuracy scores, so we can say that there are 3 types of employes categorized based on the features given in the dataset.\n2. This analysis says that the employees are grouped into 3 categories based on the similar caracteristics.\n3. In the above plot, i have plotted the clusters based on percentage salary hike & monthly salary Vs Yeas at company\n4. We can clearly observe 2 categories of the employees for the 2 features in each plot.\n5. from the 2nd plot , we can say that 2 categories are:\n  1. Years at company less , percent salary hike less\n  2. Years at company more , percent salary hike less\n  3. Years at company more , percent salary hike moderate\n6. Like this, we can explore each and evry feature  & extract 2 clusters of employee categories  \n  ","metadata":{}},{"cell_type":"markdown","source":"1. Accuracy of the classification model build using the clusters obtained from the kmeans clustering  is good but still we cannot be 100 % sure that the results are correct.\n2. There can be some misleading interpretations got from the data .\n3. It is possible that some unnecessary columns are present in the dataset which should be removed at the start of the datacleaning.\nfor that we need some logical thinking & domain knowledge\n4. There are chances that if we could have taken some another variance % explained by features rather than 90% variance explained . It may give good results or accurate clusters\n5. No one can sure about the model build is 100% accurate  & accuracy scores are that much obtained using model testing.\n6. While choosing clusters there can be more than 3 employee groups if we go for hierarchial clustering.\nWe can find more insights by going in depth using agglomerative clustering.\nBut here we said that kmeans clustering is the perfect clustering model for the given data.So this result can be misleading us about employee groups.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"VO78i8TJqrid"},"execution_count":null,"outputs":[]}]}