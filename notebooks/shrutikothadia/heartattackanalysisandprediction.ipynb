{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"About this dataset\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack\n\nDataset URL: https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n","metadata":{}},{"cell_type":"markdown","source":"Import all the required Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(color_codes=True)\nfrom scipy import stats\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the data from local machine","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndata1 = pd.DataFrame(data1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.info()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#number of records and features in the dataset\ndata1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check duplicate rows in data\nduplicate_rows = data1[data1.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have one duplicate row.\n#Removing the duplicate row\n\ndata1 = data1.drop_duplicates()\nduplicate_rows = data1[data1.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)\n\n#Number of duplicate rows after dropping one duplicate row","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if the other data is consistent\ndata1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking for null values\nprint(\"Null values :: \")\nprint(data1.isnull() .sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As there are no null values in data, we can proceed with the next steps.\n#Detecting Outliers\n\n# 1. Detecting Outliers using IQR (InterQuartile Range)\nsns.boxplot(x=data1['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#No Outliers observed in 'age'\nsns.boxplot(x=data1['sex'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#No outliers observed in sex data\nsns.boxplot(x=data1['cp'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#No outliers in 'cp'\nsns.boxplot(x=data1['trtbps'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some outliers are observed in 'trtbps'. They will be removed later\nsns.boxplot(x=data1['chol'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Some outliers are observed in 'chol'. They will be removed later\nsns.boxplot(x=data1['fbs'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=data1['restecg'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=data1['thalachh'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Outliers present in thalachh\nsns.boxplot(x=data1['exng'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=data1['oldpeak'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Outliers are present in 'OldPeak'\nsns.boxplot(x=data1['slp'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=data1['caa'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Outliers are present in 'caa'\nsns.boxplot(x=data1['thall'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find the InterQuartile Range\nQ1 = data1.quantile(0.25)\nQ3 = data1.quantile(0.75)\n\nIQR = Q3-Q1\nprint('*********** InterQuartile Range ***********')\nprint(IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the outliers using IQR\ndata2 = data1[~((data1<(Q1-1.5*IQR))|(data1>(Q3+1.5*IQR))).any(axis=1)]\ndata2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing outliers using Z-score\nz = np.abs(stats.zscore(data1))\ndata3 = data1[(z<3).all(axis=1)]\ndata3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the correlation between variables\npearsonCorr = data3.corr(method='pearson')\nspearmanCorr = data3.corr(method='spearman')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the correlation between variables\npearsonCorr = data3.corr(method='pearson')\nspearmanCorr = data3.corr(method='spearman')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(pearsonCorr, vmin=-1,vmax=1, cmap = \"Greens\", annot=True, linewidth=0.1)\nplt.title(\"Pearson Correlation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(spearmanCorr, vmin=-1,vmax=1, cmap = \"Blues\", annot=True, linewidth=0.1)\nplt.title(\"Spearman Correlation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create mask for both correlation matrices\n\n#Pearson corr masking\n#Generating mask for upper triangle\nmaskP = np.triu(np.ones_like(pearsonCorr,dtype=bool))\n\n#Adjust mask and correlation\nmaskP = maskP[1:,:-1]\npCorr = pearsonCorr.iloc[1:,:-1].copy()\n\n#Setting up a diverging palette\ncmap = sns.diverging_palette(0, 200, 150, 50, as_cmap=True)\n\nfig = plt.subplots(figsize=(14,8))\nsns.heatmap(pCorr, vmin=-1,vmax=1, cmap = cmap, annot=True, linewidth=0.3, mask=maskP)\nplt.title(\"Pearson Correlation\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create mask for both correlation matrices\n\n#Spearson corr masking\n#Generating mask for upper triangle\nmaskS = np.triu(np.ones_like(spearmanCorr,dtype=bool))\n\n#Adjust mask and correlation\nmaskS = maskS[1:,:-1]\nsCorr = spearmanCorr.iloc[1:,:-1].copy()\n\n#Setting up a diverging palette\ncmap = sns.diverging_palette(0, 250, 150, 50, as_cmap=True)\n\nfig = plt.subplots(figsize=(14,8))\nsns.heatmap(sCorr, vmin=-1,vmax=1, cmap = cmap, annot=True, linewidth=0.3, mask=maskS)\nplt.title(\"Spearman Correlation\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From this we observe that the minimum correlation between output and other features in\n#fbs,trtbps and chol\n\n\nx = data3.drop(\"output\", axis=1)\ny = data3[\"output\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building classification models\nnames = ['Age', 'Sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\n\n#   ****************Logistic Regression*****************\nlogReg = LogisticRegression(random_state=0, solver='liblinear')\nlogReg.fit(x_train, y_train)\n\n#Check accuracy of Logistic Regression\ny_pred_logReg = logReg.predict(x_test)\n#Model Accuracy\nprint(\"Accuracy of logistic regression classifier :: \" ,metrics.accuracy_score(y_test,y_pred_logReg))\n\n#Removing the features with low correlation and checking effect on accuracy of model\nx_train1 = x_train.drop(\"fbs\",axis=1)\nx_train1 = x_train1.drop(\"trtbps\", axis=1)\nx_train1 = x_train1.drop(\"chol\", axis=1)\nx_train1 = x_train1.drop(\"restecg\", axis=1)\n\nx_test1 = x_test.drop(\"fbs\", axis=1)\nx_test1 = x_test1.drop(\"trtbps\", axis=1)\nx_test1 = x_test1.drop(\"chol\", axis=1)\nx_test1 = x_test1.drop(\"restecg\", axis=1)\n\nlogReg1 = LogisticRegression(random_state=0, solver='liblinear').fit(x_train1,y_train)\ny_pred_logReg1 = logReg1.predict(x_test1)\nprint(\"\\nAccuracy of logistic regression classifier after removing features:: \" ,metrics.accuracy_score(y_test,y_pred_logReg1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ***********************Decision Tree Classification***********************\ndecTree = DecisionTreeClassifier(max_depth=6, random_state=0)\ndecTree.fit(x_train,y_train)\n\ny_pred_decTree = decTree.predict(x_test)\n\nprint(\"Accuracy of Decision Trees :: \" , metrics.accuracy_score(y_test,y_pred_decTree))\n\n#Remove features which have low correlation with output (fbs, trtbps, chol)\nx_train_dt = x_train.drop(\"fbs\",axis=1)\nx_train_dt = x_train_dt.drop(\"trtbps\", axis=1)\nx_train_dt = x_train_dt.drop(\"chol\", axis=1)\nx_train_dt = x_train_dt.drop(\"age\", axis=1)\nx_train_dt = x_train_dt.drop(\"sex\", axis=1)\n\nx_test_dt = x_test.drop(\"fbs\", axis=1)\nx_test_dt = x_test_dt.drop(\"trtbps\", axis=1)\nx_test_dt = x_test_dt.drop(\"chol\", axis=1)\nx_test_dt = x_test_dt.drop(\"age\", axis=1)\nx_test_dt = x_test_dt.drop(\"sex\", axis=1)\n\ndecTree1 = DecisionTreeClassifier(max_depth=6, random_state=0)\ndecTree1.fit(x_train_dt, y_train)\ny_pred_dt1 = decTree1.predict(x_test_dt)\n\nprint(\"Accuracy of decision Tree after removing features:: \", metrics.accuracy_score(y_test,y_pred_dt1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Random forest classifier\n\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(x_train,y_train)\n\ny_pred_rf = rf.predict(x_test)\n\nprint(\"Accuracy of Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf))\n\n#Find the score of each feature in model and drop the features with low scores\nf_imp = rf.feature_importances_\nfor i,v in enumerate(f_imp):\n    print('Feature: %s, Score: %.5f' % (names[i],v))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing the following features : fbs(score=0.006), sex(score=0.02), trtbps(score=0.072), chol(score=0.078), \n#restecg(score=0.02), exng(score=0.06), slp(score=0.06)\n\n#names = ['Age', 'Sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\nnames1 = ['Age', 'Sex', 'cp', 'trtbps', 'chol','restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\n\nx_train_rf2 = x_train.drop(\"fbs\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"sex\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"restecg\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"slp\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"exng\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"trtbps\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"chol\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"age\",axis=1)\n\nx_test_rf2 = x_test.drop(\"fbs\", axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"sex\", axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"restecg\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"slp\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"exng\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"trtbps\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"chol\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"age\",axis=1)\n\nrf2 = RandomForestClassifier(n_estimators=500)\nrf2.fit(x_train_rf2,y_train)\n\ny_pred_rf2 = rf2.predict(x_test_rf2)\nprint(\"Accuracy of Random Forest Classifier after removing features with low score :\")\nprint(\"New Accuracy :: \" , metrics.accuracy_score(y_test,y_pred_rf2))\nprint(\"\\n\")\nprint(\"---------------------------------------------------------------------------------------------\")\n\nf_imp = rf2.feature_importances_\nfor i,v in enumerate(f_imp):\n    print('Feature: %s, Score: %.5f' % (names1[i],v))\nprint(\"---------------------------------------------------------------------------------------------\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K Neighbours Classifier\n\nknc =  KNeighborsClassifier()\nknc.fit(x_train,y_train)\n\ny_pred_knc = knc.predict(x_test)\n\nprint(\"Accuracy of K-Neighbours classifier :: \", metrics.accuracy_score(y_test,y_pred_knc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Models and their accuracy\n\nprint(\"*****************Models and their accuracy*****************\")\nprint(\"Logistic Regression Classifier :: \", metrics.accuracy_score(y_test,y_pred_logReg1))\nprint(\"Decision Tree :: \", metrics.accuracy_score(y_test,y_pred_dt1))\nprint(\"Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf))\nprint(\"K Neighbours Classifier :: \", metrics.accuracy_score(y_test,y_pred_knc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, the logistic regression gives us a highest accuracy amongst other algorithms used.","metadata":{}}]}