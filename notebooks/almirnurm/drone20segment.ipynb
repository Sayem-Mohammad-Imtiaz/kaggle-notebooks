{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport torchvision.transforms.functional as TF\nfrom torchvision import transforms, models\nimport numpy as np # linear algebra\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport os\n\nimg_h = 384\nimg_w = 512\nseed = 879\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nn_classes = 23\nclass_colors = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], \n [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0], [64, 128, 0], \n [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128], \n [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128], [52, 247, 12], [243, 22, 243]]\n#tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, \n#person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle and some unknown objects\n\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_root = '/kaggle/working'\nprint(os.listdir(data_root))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['data', 'answer']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name, 'unknown'), exist_ok=True)\n\nsource_dir0  = os.path.join('../input/semantic-drone-dataset/semantic_drone_dataset/original_images')\nsource_dir  = os.path.join('../input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic')\nfor i, file_name in enumerate(tqdm(os.listdir(source_dir0))):\n    if i % 26 != 0:\n        dest_dir0  = os.path.join(train_dir, 'data')\n        dest_dir = os.path.join(train_dir, 'answer')\n    else:\n        dest_dir0  = os.path.join(val_dir, 'data')\n        dest_dir = os.path.join(val_dir, 'answer')\n    shutil.copy(os.path.join(source_dir0,  file_name), os.path.join(dest_dir0, 'unknown', file_name))\n    shutil.copy(os.path.join(source_dir, file_name[:-3] + 'png'), \n                os.path.join(dest_dir, 'unknown', file_name[:-3] + 'png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ValDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, target_paths, train=True):\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n\n    def transform(self, image, mask):\n        # Resize\n        resize = transforms.Resize(size=(img_h, img_w))\n        image = resize(image)\n        mask = resize(mask)\n\n        # Random horizontal flipping\n        if random.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n\n        # Random vertical flipping\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n\n        # Transform to tensor\n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        image = TF.normalize(image, mean, std)\n        return image, mask\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        x, y = self.transform(image, mask)\n        return x, y\n\n    def __len__(self):\n        return len(self.image_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, target_paths, train=True):\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n\n    def transform(self, image, mask):\n        # Resize\n        resize = transforms.Resize(size=(600, 900))\n        image = resize(image)\n        mask = resize(mask)\n\n        # Random crop\n        i, j, h, w = transforms.RandomCrop.get_params(\n            image, output_size=(img_h, img_w))\n        image = TF.crop(image, i, j, h, w)\n        mask = TF.crop(mask, i, j, h, w)\n\n        # Random horizontal flipping\n        if random.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n\n        # Random vertical flipping\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        \n        #Random Rotation\n        if random.random() > 0.6:\n            angle = random.randint(1, 45)\n            image = TF.rotate(image, angle)\n            mask = TF.rotate(mask, angle)\n        \n        jitter = transforms.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.5, hue=0.)\n        image = jitter(image)\n        # Transform to tensor\n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        image = TF.normalize(image, mean, std)\n        return image, mask\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        x, y = self.transform(image, mask)\n        return x, y\n\n    def __len__(self):\n        return len(self.image_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = 'train/data'\nval_dir   = 'val/data'\ntrain_a   = 'train/answer'\nval_a     = 'val/answer'\n\ntrain_data = os.listdir('train/data/unknown')\ntrain_labels = os.listdir('train/answer/unknown')\nfor i in range(len(train_data)):\n    train_data[i] = os.path.join('train/data/unknown', train_data[i])\n    train_labels[i] = os.path.join('train/answer/unknown', train_labels[i])\nval_data = os.listdir('val/data/unknown')\nval_labels = os.listdir('val/answer/unknown')\nfor i in range(len(val_data)):\n    val_data[i] = os.path.join('val/data/unknown', val_data[i])\n    val_labels[i] = os.path.join('val/answer/unknown', val_labels[i])\n    \ntrain_dataset = TrainDataset(sorted(train_data), sorted(train_labels))\nval_dataset = ValDataset(sorted(val_data), sorted(val_labels))\n\nbatch_size = 12\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size, drop_last=True\n)\n\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=8, shuffle=True, num_workers=batch_size, drop_last=True\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataloader), len(train_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(val_dataloader), len(val_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_mask_above(img_a, img_a_mask):\n    img_a = img_a.permute(1, 2, 0).numpy()\n    img_a = std * img_a + mean\n    img_a_mask = img_a_mask.reshape(img_h, img_w)\n    plt.figure(1,figsize=(20,8))\n    plt.subplot(121)\n    plt.imshow(img_a);plt.title('Raw Drone footage ');plt.axis('off')\n    plt.subplot(122)\n    plt.imshow(img_a,alpha=0.9);\n    plt.imshow(img_a_mask,alpha=0.6);plt.title('Drone with  mask');plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_three(original, pred, true):\n    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n    original = original.permute(1, 2, 0).numpy()\n    original = std * original + mean\n    pred = pred.reshape(img_h, img_w)\n    true = true.reshape(img_h, img_w)\n    axs[0].imshow(original)\n    axs[0].set_title('original image-001.jpg')\n    axs[0].grid(False)\n\n    axs[1].imshow(pred)\n    axs[1].set_title('prediction image-out.png')\n    axs[1].grid(False)\n\n    axs[2].imshow(true)\n    axs[2].set_title('true label image-001.png')\n    axs[2].grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndata, labels = next(iter(val_dataloader))\nsecondary = 255*labels\nprint(torch.unique(secondary.reshape(secondary.shape[0]*img_h*img_w)))\nfor real, test, true in zip(data, secondary, labels):\n    show_mask_above(real, test)\n''' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndata, labels = next(iter(train_dataloader))\nsecondary = 255*labels\nprint(torch.unique(secondary.reshape(secondary.shape[0]*img_h*img_w)))\nfor real, test, true in zip(data, secondary, labels):\n    show_three(real, test, true)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UpSample(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpSample, self).__init__()\n        self.ConvTrans = torch.nn.ConvTranspose2d(in_channels, in_channels//2, 2, 2)\n        self.seq0 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            torch.nn.BatchNorm2d(out_channels),\n            torch.nn.LeakyReLU(),\n            torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            torch.nn.BatchNorm2d(out_channels),\n            torch.nn.LeakyReLU()\n        )\n    def forward(self, x, y):\n        x = self.ConvTrans(x)\n        x = torch.cat((x[:,:], y[:,:]), dim=1)\n        x = self.seq0(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.vgg13_bn(pretrained=True)\nfor param in model.parameters():\n    param.requires_grad = False\n    \nclass VGGU(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(VGGU, self).__init__()\n        lst = list(list(model.children())[0])\n        self.seq0 = torch.nn.Sequential(\n            *lst[:6]\n        )\n        self.seq1 = torch.nn.Sequential(\n            *lst[6:13]\n        )\n        self.seq2 = torch.nn.Sequential(\n            *lst[13:20]\n        )\n        self.seq3 = torch.nn.Sequential(\n            *lst[20:27]\n        )\n        self.seq4 = torch.nn.Sequential(\n            *lst[27:34]\n        )\n        self.seq5 = torch.nn.Sequential(\n            torch.nn.MaxPool2d(2, 2),\n            torch.nn.Conv2d(512, 1024, 3, padding=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(1024, 1024, 3, padding=1),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU()\n        )\n        self.seq6   = UpSample(1024, 1024)\n        self.seq7   = UpSample(1024, 512)\n        self.seq8   = UpSample(512, 256)\n        self.seq9   = UpSample(256, 128)\n        self.seq10  = UpSample(128, 64)\n        \n        self.conv = torch.nn.Conv2d(64, num_classes, 1)\n    def forward(self, x):\n        x = self.seq0(x)\n        help0 = x\n        x  = self.seq1(x)\n        help1 = x\n        x  = self.seq2(x)\n        help2 = x\n        x  = self.seq3(x)\n        help3 = x\n        x = self.seq4(x)\n        help4 = x\n        x = self.seq5(x)\n        x = self.seq6(x, help4)\n        x = self.seq7(x, help3)\n        x = self.seq8(x, help2)\n        x = self.seq9(x, help1)\n        x = self.seq10(x, help0)\n        x = self.conv(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_func(res, target):\n    target = target.type(torch.LongTensor).to(device)\n    res = (torch.stack((res[:, 0], res[:, 1], res[:, 2], res[:, 3], res[:, 4],\n                        res[:, 5], res[:, 6], res[:, 7], res[:, 8], res[:, 9],\n                        res[:, 10], res[:, 11], res[:, 12], res[:, 13], res[:, 14],\n                        res[:, 15], res[:, 16], res[:, 17], res[:, 18], res[:, 19], \n                        res[:, 20], res[:, 21], res[:, 22]), dim=3)). \\\n    reshape(res.shape[0]*res.shape[2]*res.shape[3], n_classes)\n    f = torch.nn.CrossEntropyLoss()\n    loss = f(res, target)\n    \n    res = (torch.nn.functional.softmax(res,dim=1))\n    res = res.argmax(dim=1)\n    acc = ((target == res).float()).mean()\n    return acc, loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, accnlossfunc, optimizer, scheduler, num_epochs):\n    try:\n        for epoch in range(num_epochs):\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    dataloader = train_dataloader\n                    model.train()  # Set model to training mode\n                else:\n                    dataloader = val_dataloader\n                    model.eval()   # Set model to evaluate mode\n                running_loss = 0.\n                running_acc = 0\n                # Iterate over data.\n                for inputs, labels in (dataloader):\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    labels = (labels*255).reshape(labels.shape[0]*img_h*img_w)\n                    optimizer.zero_grad()\n                    # forward and backward\n                    with torch.set_grad_enabled(phase == 'train'):\n                        preds = model(inputs)\n                        acc, loss_value = accnlossfunc(preds, labels)\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            loss_value.backward()\n                            optimizer.step()\n                            scheduler.step()\n\n                    # statistics\n                    running_loss += loss_value.item()\n                    running_acc += acc.item()\n\n                epoch_loss = running_loss / len(dataloader)\n                epoch_acc = running_acc / len(dataloader)\n\n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n    except KeyboardInterrupt:\n        return model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = VGGU(n_classes)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nloss = loss_func\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n# Decay LR by a factor of 0.1 every 88 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)\nparams = list(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n# Decay LR by a factor of 0.1 every 88 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.65)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train_model(model, loss, optimizer, scheduler, num_epochs=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20, 24):\n    params[i].requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_three0(original, pred, true):\n    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n    original = original.permute(1, 2, 0).numpy()\n    original = std * original + mean\n    #pred = pred.reshape(img_h, img_w)\n    pred = pred.permute(1, 2, 0).numpy()\n    true = true.reshape(img_h, img_w)\n    axs[0].imshow(original)\n    axs[0].set_title('original image-001.jpg')\n    axs[0].grid(False)\n\n    axs[1].imshow(pred)\n    axs[1].set_title('prediction image-out.png')\n    axs[1].grid(False)\n\n    axs[2].imshow(true)\n    axs[2].set_title('true label image-001.png')\n    axs[2].grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, labels = next(iter(val_dataloader))\ndata = data.to(device)\nres = model(data)\nres = (torch.stack((res[:, 0], res[:, 1], res[:, 2], res[:, 3], res[:, 4],\n                        res[:, 5], res[:, 6], res[:, 7], res[:, 8], res[:, 9],\n                        res[:, 10], res[:, 11], res[:, 12], res[:, 13], res[:, 14],\n                        res[:, 15], res[:, 16], res[:, 17], res[:, 18], res[:, 19], \n                        res[:, 20], res[:, 21], res[:, 22]), dim=3)). \\\n    reshape(res.shape[0]*res.shape[2]*res.shape[3], n_classes)\nres = res.argmax(dim=1)\nres = res.reshape(data.shape[0], img_h, img_w)\nfor real, test in zip(data, res):\n    show_mask_above(real.cpu().detach(), test.cpu().detach())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nstate = torch.get_rng_state()\nX_batch0, X_batch1 = next(iter(val_dataloader))\nX_batch0 = X_batch0.to(device)\ng = X_batch0.clone()\nres = model(X_batch0)\nres = (torch.stack((res[:, 0], res[:, 1], res[:, 2], res[:, 3], res[:, 4],\n                        res[:, 5], res[:, 6], res[:, 7], res[:, 8], res[:, 9],\n                        res[:, 10], res[:, 11], res[:, 12], res[:, 13], res[:, 14],\n                        res[:, 15], res[:, 16], res[:, 17], res[:, 18], res[:, 19], \n                        res[:, 20], res[:, 21], res[:, 22]), dim=3)). \\\n    reshape(res.shape[0]*res.shape[2]*res.shape[3], n_classes)\nres = res.argmax(dim=1)\nres = res.reshape(X_batch0.shape[0], img_h, img_w)\n\nfor imgr, imgg, imgb in zip(res, g, X_batch1):\n    show_three(imgg.cpu().detach(), imgr.cpu().detach(), imgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, labels = next(iter(train_dataloader))\ndata = data.to(device)\nres = model(data)\nres = (torch.stack((res[:, 0], res[:, 1], res[:, 2], res[:, 3], res[:, 4],\n                        res[:, 5], res[:, 6], res[:, 7], res[:, 8], res[:, 9],\n                        res[:, 10], res[:, 11], res[:, 12], res[:, 13], res[:, 14],\n                        res[:, 15], res[:, 16], res[:, 17], res[:, 18], res[:, 19], \n                        res[:, 20], res[:, 21], res[:, 22]), dim=3)). \\\n    reshape(res.shape[0]*res.shape[2]*res.shape[3], n_classes)\nres = res.argmax(dim=1)\nres = res.reshape(data.shape[0], img_h, img_w)\nfor real, test in zip(data, res):\n    show_mask_above(real.cpu().detach(), test.cpu().detach())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}