{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"true_news = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\nfake_news = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) 변수탐색"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(true_news.isnull().sum())\nprint(fake_news.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('true: ',true_news.shape,true_news.columns)\nprint('fake: ',fake_news.shape,fake_news.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_index = random.sample(range(true_news.shape[0]),5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('true\\n',true_news['title'][random_index])\nprint('fake\\n',fake_news['title'][random_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('true\\n',true_news['text'][random_index])\nprint('fake\\n',fake_news['text'][random_index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- true_news의 경우 Reuters(출처)가 존재"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('true\\n',true_news['subject'].value_counts())\nprint('fake\\n',fake_news['subject'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 서로 다른 범주를 갖고 있음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_MON = true_news['date'].apply(lambda x: x[0:3].upper())\nfake_MON = fake_news['date'].apply(lambda x: x[0:3].upper())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(true_MON.value_counts())\nprint(fake_MON.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 내용(title, text)만으로 true, fake 뉴스를 가려내는 것이 목표이기 때문에 그 외의 요소 제거 필요\n- title의 경우 대략적인 확인으로는 눈에 띄는 차이 존재하지 않음\n- text의 경우 ture_news의 경우에만 기사 출처가 명시되어 있음 (Reuters) \n    => 내용만으로 분류를 하기 위해 제거해줘야 함\n- subject는 true와 fake의 범주가 각각 다르기 때문에 사용 불가\n- date의 경우 앞의 세글자만 추출해 월별 건수를 확인해 보았으나 뉴스 분류를 위한 피쳐로는 부적절하다고 판단 "},{"metadata":{},"cell_type":"markdown","source":"# 2) 데이터 처리"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 기사 출처 제거를 위해 '-' 를 기준으로\nReuters = true_news['text'].apply(lambda x : x.split('-')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reuters를 포함하지 않는 데이터 확인\nnotReuters = Reuters[Reuters.apply(lambda x: 'Reuters' not in x)]\nnotReuters[1:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reuters가 아니더라도 출처를 나타내고 있는 경우가 확인 되기 때문에\n# 텍스트의 길이(30)를 기준으로 분류 시도\nnotReuters[notReuters.apply(lambda x:len(x)>30)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 출처가 없이 본문이 시작하는 기사 선별, 해당 기사들은 출처 제거 작업에서 제외 되어야함\nno_remove = notReuters[notReuters.apply(lambda x:len(x)>30)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# case1) 출처가 있으며, '-'이 존재하는 경우 '-'를 기준으로 텍스트를 분리해 뒷부분 선택\n# case2) 출처가 없으며, '-'이 존재하지 않는 경우 텍스트가 분리되지 않아 데이터 그대로 선택\n# case3) 출처가 없으며, '-'이 존재하는 경우\n\nfor i in range(true_news.shape[0]):\n    if i not in no_remove:\n        try:\n            true_news['text'][i] = '-'.join(true_news['text'][i].split(' - ')[1:])\n        except:\n            true_news['text'][i] = true_news['text'][i]\n    else:\n        true_news['text'][i] = true_news['text'][i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 내용에 텍스트가 없는 경우 존재 (공백만 있음)\ntrue_news['text'][true_news['text'].apply(lambda x: len(x)<3)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 특히 fake_news의 경우 그 수가 매우 많은 편\nprint(len(fake_news['text'][fake_news['text'].apply(lambda x: len(x)<3)]))\nprint(len(fake_news['title'][fake_news['title'].apply(lambda x: len(x)<3)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title이 공백인 경우는 없음, 따라서 제목 + 내용을 하나의 변수로 만들어 분석 예정\ntrue_news['content'] = true_news['title'] + true_news['text']\nfake_news['content'] = fake_news['title'] + fake_news['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최소한 한문장은 갖게 됨\nprint(fake_news['content'][fake_news['content'].apply(lambda x: len(x)<3)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 라벨 인코딩\ntrue_news['target'] = 'true'\nfake_news['target'] = 'fake'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) fake, real new 주제별 키워드 추출"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport re\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 영어가 아닌 것 삭제\ntrue_news['content'] = true_news['content'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))\nfake_news['content'] = true_news['content'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3-1) True News Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_vect = CountVectorizer(max_df=0.95, max_features=3000, stop_words='english')\nfeat_vect = cnt_vect.fit_transform(true_news['content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_vect.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=2)\nlda.fit(feat_vect)\nprint(lda.components_.shape)\nlda.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    for topic_index, topic in enumerate(model.components_):\n        print('Topic #', topic_index)\n        topic_word_indexes = topic.argsort()[::-1]\n        top_indexes = topic_word_indexes[:no_top_words]\n        \n        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n        print(feature_concat)\n        \nfeature_names = cnt_vect.get_feature_names()\ndisplay_topics(lda, feature_names, 15)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is not Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(true_news[true_news.subject == 'politicsNews'].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is not Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(true_news[true_news.subject == 'worldnews'].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is not Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(true_news.text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3-2) fake_news"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_vect = CountVectorizer(max_df=0.95, max_features=3000, stop_words='english')\nfeat_vect = cnt_vect.fit_transform(fake_news['content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=6)\nlda.fit(feat_vect)\nprint(lda.components_.shape)\nlda.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = cnt_vect.get_feature_names()\ndisplay_topics(lda, feature_names, 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news['subject'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(fake_news.text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4)모델 적용"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 텍스트만으로 뉴스를 분류할 예정이기 때문에 불필요한 변수 전부 제외\ntrue_news = true_news[['content','target']]\nfake_news = fake_news[['content','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결합\nfinal_data = pd.concat([true_news,fake_news])\nfinal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(true_news.shape, fake_news.shape)\nfinal_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 학습 50%, 검정 30%, 테스트20%로 데이터 분할\nX_train, X_test, y_train, y_test = train_test_split(final_data['content'],final_data['target'],train_size=0.5)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test, train_size = 0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target 값이 몰린 부분이 없는지 확인\nprint(y_train.value_counts())\nprint('='*30)\nprint(y_valid.value_counts())\nprint('='*30)\nprint(y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,X_valid.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터화\ntf_vect = TfidfVectorizer(max_features = 10000, stop_words='english')\ntf_vect.fit(X_train)\nX_train_tf_vect = tf_vect.transform(X_train)\nX_valid_tf_vect = tf_vect.transform(X_valid)\nX_test_tf_vect = tf_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBM 모델 학습\nlgbm_clf = LGBMClassifier(n_estimators = 1000)\nevals = [(X_valid_tf_vect, y_valid)]\nlgbm_clf.fit(X_train_tf_vect, y_train, early_stopping_rounds=100, eval_metric='accuracy',eval_set = evals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최종 예측값\npred = lgbm_clf.predict(X_test_tf_vect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 정확도\naccuracy_score(pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n    ('lr_clf', LogisticRegression())\n])\n\nparams = {'tfidf_vect__max_features':[7000,10000,12000],\n          'tfidf_vect__max_df':[0.7,0.9],\n         'lr_clf__C':[0.1,1,5,10]}\ngrid_cv_pipe = GridSearchCV(pipeline, param_grid = params, cv=3, scoring='accuracy',verbose=1)\ngrid_cv_pipe.fit(X_train,y_train)\nprint(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npred = grid_cv_pipe.predict(X_test)\nprint('Logistic Reg 정확도 {0:.3f}'.format(accuracy_score(y_test,pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}