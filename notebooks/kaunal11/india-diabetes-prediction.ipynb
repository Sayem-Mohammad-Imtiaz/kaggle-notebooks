{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport missingno as msno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Let's ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing data and verfiying\n\ndf=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Descriptions:**\n\n* Pregnancies = No. of times a pregnancy has occurred\n* Glucose = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure = Diastolic blood pressure (mm Hg)\n* SkinThickness = Triceps skin fold thickness (mm)\n* Insuling = 2-Hour serum insulin (mu U/ml)\n* BMI = Body mass index (weight in kg/(height in m)^2)\n* DiabetesPedigreeFunction = Diabetes pedigree function\n* Age = Age (in years)\n* Outcome = Class variable (0 or 1) 268 of 768 are 1, the others are 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Analysis of the data\n\ndf.describe(),df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data types\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing values\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# While we can see above that the dataset does not have null values, we can see that it still has ZERO values which may not make a \n# lot of sense for fields such as Glucose, Blood Pressure, Skin Thickness, Insulin, BMI. We want to replace ZERO with Nan so that it\n# reflects as missing values\n\ndf[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]] = df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].replace({0:np.nan})\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing null values with mean\n\ndf['Glucose'] = df['Glucose'].fillna(value=df['Glucose'].mean())\ndf['BloodPressure'] = df['BloodPressure'].fillna(value=df['BloodPressure'].mean())\ndf['SkinThickness'] = df['SkinThickness'].fillna(value=df['SkinThickness'].mean())\ndf['Insulin'] = df['Insulin'].fillna(value=df['Insulin'].mean())\ndf['BMI'] = df['BMI'].fillna(value=df['BMI'].mean())\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeing the spread of people with diabetes (Our Target feature -> Outcome)\n\nfig = plt.figure(figsize=(20,1))\nsns.countplot(data=df,y='Outcome')\ndf['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking correlation between various parameters\n\nrcParams[\"figure.figsize\"] = 20,10\nplt.title(\"Corellation between different features\")\nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize = (20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"Outcome\"],axis = 1),),columns=['Pregnancies', 'Glucose', 'BloodPressure', \n                                                                              'SkinThickness', 'Insulin','BMI',\n                                                                              'DiabetesPedigreeFunction', 'Age'])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target feature\ny=df['Outcome']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test and train data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing ML Libraries\n\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\n\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# k-Nearest Neighbours\n\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC\n\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent\n\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM\n\ntrain_pred_svm, acc_linear_svm, acc_cv_linear_svm = fit_ml_algo(SVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svm)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Classifier\n\ntrain_pred_decision, acc_linear_decision, acc_cv_linear_decision = fit_ml_algo(DecisionTreeClassifier(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_decision)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_decision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking the importance of features\n\nfrom sklearn.ensemble import RandomForestClassifier \nmodel= RandomForestClassifier(n_estimators=100,random_state=0)\nX=df[df.columns[:8]]\nY=df['Outcome']\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **We can see that the most important features are Glucose, BMI, Age, Diabates Pedigree Function. So we will repeat the steps above with the above features only**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[['Glucose','BMI','DiabetesPedigreeFunction','Age','Outcome']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\n\n\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"Outcome\"],axis = 1),),columns=['Glucose','BMI','DiabetesPedigreeFunction','Age'])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test and train data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\n\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# k-Nearest Neighbours\n\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear SVC\n\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent\n\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM\n\ntrain_pred_svm, acc_linear_svm, acc_cv_linear_svm = fit_ml_algo(SVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svm)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Classifier\n\ntrain_pred_decision, acc_linear_decision, acc_cv_linear_decision = fit_ml_algo(DecisionTreeClassifier(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_decision)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_decision)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **We now see that the models which are consistently having the greatest accurace are SVM (Linear and Radial) and Logistic Regression. So we will restrict to compare these. **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [1,5,10]\n        }\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nmodel_df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\nmodel_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Based on above, we can conclude that the best model is Logistic Regression with C as 1**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}