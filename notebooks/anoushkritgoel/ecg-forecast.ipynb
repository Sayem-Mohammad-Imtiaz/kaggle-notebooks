{"cells":[{"metadata":{"id":"CLUZdPrtG-14","trusted":true},"cell_type":"code","source":"pip install wfdb","execution_count":null,"outputs":[]},{"metadata":{"id":"FES4Ixa0HDrw","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport wfdb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"PtLhbwN4HFk6","outputId":"a894babf-6625-4b24-f7da-911ce5c16f20","trusted":true},"cell_type":"code","source":"ecg_record = wfdb.rdrecord('../input/mitbiharrythmia/100', channels= [0])\n# ecg_record.fs","execution_count":null,"outputs":[]},{"metadata":{"id":"LfUt0JU_HI-p","trusted":true},"cell_type":"code","source":"ecg_record = wfdb.rdrecord('../input/mitbiharrythmia/100', sampfrom= 0, sampto= 108300, channels= [0])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sAN8YAyoHS1z","trusted":true},"cell_type":"code","source":"# Splitting into training and validation set\n\necg = ecg_record.p_signal[:,0]\ntrain_len = 108000\n\necg_train = ecg[0: train_len]\necg_test = ecg[train_len:]","execution_count":null,"outputs":[]},{"metadata":{"id":"kQ0KxLVTHVgT","trusted":true},"cell_type":"code","source":"ecg_train = torch.FloatTensor(ecg_train).view(-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"c4FawVydHbuz","trusted":true},"cell_type":"code","source":"ecg_test = torch.FloatTensor(ecg_test).view(-1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-_KznKaZHday","trusted":true},"cell_type":"code","source":"def create_inout_sequences(input_data, tw):\n    inout_seq = []\n    L = len(input_data)\n    for i in range(L-tw):\n        train_seq = input_data[i:i+tw]\n        train_label = input_data[i+tw:i+tw+1]\n        inout_seq.append((train_seq ,train_label))\n    return inout_seq","execution_count":null,"outputs":[]},{"metadata":{"id":"rM5e-vyMHgri","trusted":true},"cell_type":"code","source":"def create_dataloader(data_entered, train_window, batch_size):\n  data = create_inout_sequences(data_entered, train_window)\n  data_loader = torch.utils.data.DataLoader(data,\n                                          batch_size=batch_size,\n                                          shuffle=True, drop_last= True)\n  return data_loader","execution_count":null,"outputs":[]},{"metadata":{"id":"ZITmLkUfHidQ","trusted":true},"cell_type":"code","source":"train_window = 850  # 3 cardiac cycles in each sequence\n# train_data = create_inout_sequences(ecg_train, train_window)\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"id":"6akfWbEqHkzt","trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_dim=100, n_layers=5, output_size=1, drop_prob=0.5, lr=0.001):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.drop_prob = drop_prob\n        self.lr = lr\n\n        ## define the LSTM\n        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers= n_layers, dropout=drop_prob, batch_first=True)\n        \n        ## define a dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n\n        ## define the final, fully-connected output layer\n        self.linear = nn.Linear(hidden_dim, output_size)\n\n\n    def forward(self, input_seq, hidden):\n        lstm_out, hidden = self.lstm(input_seq, hidden)\n        out = self.dropout(lstm_out)\n        out = out.contiguous().view(-1, self.hidden_dim)\n\n        #fully-connected layer\n        out = self.linear(out)\n\n        batch_size = input_seq.size(0)\n        \n        # return the final output and the hidden state\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"id":"j8twHmegHnUN","outputId":"9e4753d1-032a-4ac2-af7b-2c9fef6450df","trusted":true},"cell_type":"code","source":"ecg_val = wfdb.rdrecord('../input/mitbiharrythmia/100', sampfrom= 108300, sampto= 129900, channels= [0])\necg_val = ecg_val.p_signal[:,0]\n\nwfdb.plot_items(ecg_val)\necg_val = torch.FloatTensor(ecg_val).view(-1)\n\nseq_length = train_window\ntrain_data_loader = create_dataloader(ecg_train, seq_length, batch_size)\nval_data_loader = create_dataloader(ecg_val, seq_length, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"id":"NeZTv1XQHszj","trusted":true},"cell_type":"code","source":"def train(net, train_data_loader, val_data_loader, epochs=20, batch_size=batch_size, lr=0.1, clip=5, print_every=10):\n    ''' Training a network \n    \n        Arguments\n        ---------\n        \n        net: LSTM network\n        data: ecg data to train the network\n        epochs: Number of epochs to train\n        batch_size: Number of mini-sequences per mini-batch, aka batch size\n        seq_length: Number of character steps per mini-batch\n        lr: learning rate\n        clip: gradient clipping\n        val_frac: Fraction of data to hold out for validation\n        print_every: Number of steps for printing training and validation loss\n    \n    '''\n    net.train()\n    \n    opt = torch.optim.Adam(net.parameters(), lr=lr)\n    criterion = nn.L1Loss()\n    \n    if(train_on_gpu):\n        net.cuda()\n    \n    counter = 0\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n        \n        for x, y in train_data_loader:\n            counter += 1\n            x = torch.unsqueeze(x, 2)\n\n            inputs, targets = x, y\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n            \n            # get the output from the model\n            output, h = net(inputs, h)\n            \n            # calculate the loss and perform backprop\n            loss = criterion(output, targets.view(batch_size).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in val_data_loader:\n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n\n                    x = torch.unsqueeze(x, 2)\n\n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"id":"1gSGmvJFHvwW","outputId":"6062052c-0a7e-46a8-b92c-d4ec60ef6ae4","trusted":true},"cell_type":"code","source":"hidden_dim=32\nn_layers=3\n\nnet = LSTM(hidden_dim= hidden_dim, n_layers= n_layers)\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"id":"YPt2_7q9Hycx","trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"id":"rwYBzrMMH00U","outputId":"8453f3d9-faee-48db-c7ee-e4cae8977c8e","trusted":true},"cell_type":"code","source":"batch_size = 64\nseq_length = train_window\nn_epochs = 10\n\n# model training\n# train(net, ecg_train, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n\ntrain(net, train_data_loader, val_data_loader, epochs=n_epochs, batch_size=batch_size, lr=0.1, print_every=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"x1yoRU-2Igz5","trusted":true},"cell_type":"code","source":"model_name = 'rnn_20_epoch.net'\n\ncheckpoint_20E = {'n_hidden': net.hidden_dim,\n              'n_layers': net.n_layers,\n              'state_dict': net.state_dict()}\n\nwith open(model_name, 'wb') as f:\n    torch.save(checkpoint_20E, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}