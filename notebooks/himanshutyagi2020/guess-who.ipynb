{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Guess Who!\n\nThis Notebook is a quick bit of fun to try and create an AI playing Guess Who. This notebook shows the method used to train an image classifier on each attribute of the dataset. The final game can be found at https://github.com/JonnyEvans321/Guess_Who_celebrities\n \nTo save time I started with Marcos Alvarado's gender recognition notebook (http://www.kaggle.com/bmarcos/image-recognition-gender-detection-inceptionv3/notebook), thanks Marcos! Note that due to Kaggle's memory limitation, I am using a reduced amount of images to train and validate.\n\n## Instructions:\n1. Pick an attribute from this list to ask our AI: 5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair, Blond_Hair, Blurry, Brown_Hair, Bushy_Eyebrows, Chubby, , Double_Chin, Eyeglasses, Goatee, Gray_Hair, Heavy_Makeup, High_Cheekbones, Male, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face, Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns, Smiling, Straight_Hair, Wavy_Hair, Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young\n\n2. Click the 'fast-forward' button and scroll to the bottom of this notebook. Wait for a Console message to say 'TRAINING COMPLETE', then press the 'play' button on the penultimate code cell (section).\n\n## Dataset\n\nFor this project we will use the CelebA dataset (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), which is available on Kaggle.\n\nDescription of the CelebA dataset from kaggle (https://www.kaggle.com/jessicali9530/celeba-dataset): \n\n### Overall\n\n202,599 number of face images of various celebrities\n10,177 unique identities, but names of identities are not given\n40 binary attribute annotations per image\n5 landmark locations\n\n### Data Files\n\n- <b>img_align_celeba.zip</b>: All the face images, cropped and aligned\n- <b>list_eval_partition.csv</b>: Recommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing\n- <b>list_bbox_celeba.csv</b>: Bounding box information for each image. \"x_1\" and \"y_1\" represent the upper left point coordinate of bounding box. \"width\" and \"height\" represent the width and height of bounding box\n- <b>list_landmarks_align_celeba.csv</b>: Image landmarks and their respective coordinates. There are 5 landmarks: left eye, right eye, nose, left mouth, right mouth\n- <b>list_attr_celeba.csv</b>: Attribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative\n\n---\n","metadata":{"_uuid":"0bbc0f3fd3cac3ba420b59ffab6248f589b2080c"}},{"cell_type":"markdown","source":"### Import libraries","metadata":{"_uuid":"cba9cc1ea22168a0da81d39181ef5da1951cb1f4"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\n\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\n\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\nimport gc\n\nplt.style.use('ggplot')\n\n%matplotlib inline\n\nimport tensorflow as tf\nprint(tf.__version__)","metadata":{"_uuid":"49d886506abe4d6d3ec2729036966a0729ee56d8","execution":{"iopub.status.busy":"2021-08-11T01:42:10.987912Z","iopub.execute_input":"2021-08-11T01:42:10.988192Z","iopub.status.idle":"2021-08-11T01:42:15.298796Z","shell.execute_reply.started":"2021-08-11T01:42:10.988143Z","shell.execute_reply":"2021-08-11T01:42:15.29722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set the variables","metadata":{"_uuid":"2f50e8a95e091038da54ec33bb27d6f3ef9f909b"}},{"cell_type":"code","source":"# set variables \nmain_folder = '../input/celeba-dataset/'\nimages_folder = main_folder + 'img_align_celeba/img_align_celeba/'\n\nTRAINING_SAMPLES = 10000\nVALIDATION_SAMPLES = 2000\nTEST_SAMPLES = 2000\nIMG_WIDTH = 178\nIMG_HEIGHT = 218\nBATCH_SIZE = 16\n#just one epoch to save computing time, for more accurate results increase this number\nNUM_EPOCHS = 1\n\n#what characteristic are we going to train and test for? (note if set to 'all' it'll use all of them)\nATTR='Blond_Hair'","metadata":{"_uuid":"95224815fa8f0f1572c4c805e5d11faec7ef9eee","execution":{"iopub.status.busy":"2021-08-11T01:42:15.300141Z","iopub.execute_input":"2021-08-11T01:42:15.300417Z","iopub.status.idle":"2021-08-11T01:42:15.307586Z","shell.execute_reply.started":"2021-08-11T01:42:15.300372Z","shell.execute_reply":"2021-08-11T01:42:15.306103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the model ready\n\nWe're going to use Inception v3, TensorfFlow's image recognition model, which was trained for the ImageNet Large Visual Recognition Challenge using the data from 2012.\n\n'I can use the model too?? Aw shucks!'","metadata":{"_uuid":"4e8e994670f15f04927c8d7de90c0cd1dc9c1829"}},{"cell_type":"code","source":"# Import InceptionV3 Model\ninc_model = InceptionV3(weights='../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                        include_top=False,\n                        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n#inc_model.summary()","metadata":{"_uuid":"0f17c72d4e473f95f7d0838d09dadf856733990b","execution":{"iopub.status.busy":"2021-08-11T01:42:15.309002Z","iopub.execute_input":"2021-08-11T01:42:15.309421Z","iopub.status.idle":"2021-08-11T01:42:32.098381Z","shell.execute_reply.started":"2021-08-11T01:42:15.309231Z","shell.execute_reply":"2021-08-11T01:42:32.097598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Inception-V3 model structure</h2>\nThis is the structure of Inception-V3.\n\n\n<img src=\"https://i.imgur.com/kdXUzu1.png\" width=\"1000px\"/>\nsource: https://hackathonprojects.files.wordpress.com/2016/09/74911-image03.png\n\nWe're not going to include the final 5 layers, because we're transfer learning to our task. These layers will be replaced for the following layers:","metadata":{"_uuid":"0862ef2a6448f852fbd400faeaf5730361f112f6"}},{"cell_type":"code","source":"#Adding custom Layers\nx = inc_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation=\"relu\")(x)\npredictions = Dense(2, activation=\"softmax\")(x)","metadata":{"_uuid":"53b7ce5be34d28acd2f6c8db22497d29b56d68c9","execution":{"iopub.status.busy":"2021-08-11T01:42:32.099674Z","iopub.execute_input":"2021-08-11T01:42:32.099952Z","iopub.status.idle":"2021-08-11T01:42:32.157224Z","shell.execute_reply.started":"2021-08-11T01:42:32.099908Z","shell.execute_reply":"2021-08-11T01:42:32.156469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>New classification layers</h2>\nClassification layers to be trained with the new model.\n<img src=\"https://i.imgur.com/rWF7bRY.png\" width=\"800px\"/>","metadata":{"_uuid":"5b277284385ce7f6a8989a0df7dbb2d211837c4a"}},{"cell_type":"code","source":"# create the model \nmodel_ = Model(inputs=inc_model.input, outputs=predictions)\n\n# Lock initial layers to not be trained\nfor layer in model_.layers[:52]:\n    layer.trainable = False\n\n# compile the model\nmodel_.compile(optimizer=SGD(lr=0.0001, momentum=0.9)\n                    , loss='categorical_crossentropy'\n                    , metrics=['accuracy'])","metadata":{"_uuid":"88b6a2c8592a9bad341bdd9025c77556b106e7b4","execution":{"iopub.status.busy":"2021-08-11T01:42:32.158381Z","iopub.execute_input":"2021-08-11T01:42:32.158778Z","iopub.status.idle":"2021-08-11T01:42:32.218999Z","shell.execute_reply.started":"2021-08-11T01:42:32.158604Z","shell.execute_reply":"2021-08-11T01:42:32.218444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting data\n\nWe will be using the CelebA Dataset, which has images of 178 x 218 px. ","metadata":{"_uuid":"352244740550c3cfb275c496add9c71bc9865b41"}},{"cell_type":"markdown","source":"### Load the attributes (characteristics) of every picture\nFile: list_attr_celeba.csv","metadata":{"_uuid":"d4effe59d9137ff22c55ae1f331772e4e728fb5e"}},{"cell_type":"code","source":"# import the data set that include the attribute for each picture\ndf_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\ndf_attr.set_index('image_id', inplace=True)\ndf_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\ndf_attr.shape\n\n#if you choose to train on all attributes, set ATTR='all', else write out the desired attributes in a list\nif(ATTR=='all'):\n    ATTR=list(df_attr.columns)\nelif(isinstance(ATTR, str)):\n    ATTR=[ATTR]\n\n# List of available attributes\nfor i, j in enumerate(df_attr.columns):\n    print(i, j)","metadata":{"_uuid":"1a6e65c380d3050be07f88488f495e34c76fc860","execution":{"iopub.status.busy":"2021-08-11T01:42:32.220461Z","iopub.execute_input":"2021-08-11T01:42:32.220858Z","iopub.status.idle":"2021-08-11T01:42:33.141737Z","shell.execute_reply.started":"2021-08-11T01:42:32.220681Z","shell.execute_reply":"2021-08-11T01:42:33.141095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recommended splits for training, validation and testing\n\nThe recommended partitioning of images into training, validation, testing of the data set is: \n* 1-162770 are training\n* 162771-182637 are validation\n* 182638-202599 are testing\n\nThe recommended partition is in file <b>list_eval_partition.csv</b>\n\nHowever, due to Kaggle's restrictions, we will be using a reduced number of images:\n\n* Training 20000 images\n* Validation 5000 images\n* Test 5000 Images\n","metadata":{"_uuid":"ba04abfd3a6f51409bdfec909afcfd44c67421aa"}},{"cell_type":"code","source":"# Recomended partition\ndf_partition = pd.read_csv(main_folder + 'list_eval_partition.csv')\ndf_partition.head()\n\n# display counter by partition\n# 0 -> TRAINING\n# 1 -> VALIDATION\n# 2 -> TEST\ndf_partition['partition'].value_counts().sort_index()\n\ndf_partition.set_index('image_id', inplace=True)\n","metadata":{"_uuid":"60fd112d3dfb79ae7ecf6963d74919f6d44d7c76","execution":{"iopub.status.busy":"2021-08-11T01:42:33.145731Z","iopub.execute_input":"2021-08-11T01:42:33.145958Z","iopub.status.idle":"2021-08-11T01:42:33.29734Z","shell.execute_reply.started":"2021-08-11T01:42:33.145916Z","shell.execute_reply":"2021-08-11T01:42:33.296469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This degree project explains how imbalanced training data can impact on CNNs models:\n\nhttps://www.kth.se/social/files/588617ebf2765401cfcc478c/PHensmanDMasko_dkand15.pdf\n\nSo we will create functions that will help us to create balanced partitions.","metadata":{"_uuid":"6becbeba1d5fefc4416e041cef5f5d701dacc4aa"}},{"cell_type":"code","source":"def load_reshape_img(fname):\n    img = load_img(fname)\n    x = img_to_array(img)/255.\n    x = x.reshape((1,) + x.shape)\n\n    return x\n\ndef generate_df(df,partition, attr, num_samples):\n    '''\n    partition\n        0 -> train\n        1 -> validation\n        2 -> test\n    '''\n    \n    #The sample size is at most the number stated above, but at least the size of the smallest class of the dataframe. This results in some uncommon attributes (e.g. sideburns) having to train on very few samples.\n    min_class_size=min(len(df[(df['partition'] == partition) & (df[attr] == 0)]),len(df[(df['partition'] == partition) & (df[attr] == 1)]) )\n    sample_size=int(num_samples/2)\n    if(min_class_size<int(num_samples/2)):\n        sample_size=min_class_size\n    \n    df_ = df[(df['partition'] == partition) & (df[attr] == 0)].sample(sample_size)\n    df_ = pd.concat([df_,\n                      df[(df['partition'] == partition) \n                                  & (df[attr] == 1)].sample(sample_size)])\n\n    # for Train and Validation\n    if partition != 2:\n        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n        x_ = x_.reshape(x_.shape[0], 218, 178, 3)\n        y_ = np_utils.to_categorical(df_[attr],2)\n    # for Test\n    else:\n        x_ = []\n        y_ = []\n\n        for index, target in df_.iterrows():\n            im = cv2.imread(images_folder + index)\n            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n            im = np.expand_dims(im, axis =0)\n            x_.append(im)\n            y_.append(target[attr])\n\n    return x_, y_","metadata":{"_uuid":"56b4af1f2f980957659c0272b92c1f77c13162c4","execution":{"iopub.status.busy":"2021-08-11T01:42:33.300622Z","iopub.execute_input":"2021-08-11T01:42:33.300866Z","iopub.status.idle":"2021-08-11T01:42:33.311942Z","shell.execute_reply.started":"2021-08-11T01:42:33.300823Z","shell.execute_reply":"2021-08-11T01:42:33.31107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training \nFunction to train the model","metadata":{"_uuid":"659ad807ced4e62a900c31fb5d53ae81be550880"}},{"cell_type":"code","source":"def training(ATTR, train_generator,x_valid,y_valid):\n    #https://keras.io/models/sequential/ fit generator\n    checkpointer = ModelCheckpoint(filepath='weights.best.inc.'+ATTR+'.hdf5', \n                                   verbose=1, save_best_only=True)\n\n    hist = model_.fit_generator(train_generator\n                         , validation_data = (x_valid, y_valid)\n                          , steps_per_epoch= TRAINING_SAMPLES/BATCH_SIZE\n                          , epochs= NUM_EPOCHS\n                          , callbacks=[checkpointer]\n                          , verbose=1\n                        )\n    return hist","metadata":{"_uuid":"4a9565a59572a1f3034f66c58e7571f30190fe59","execution":{"iopub.status.busy":"2021-08-11T01:42:33.313345Z","iopub.execute_input":"2021-08-11T01:42:33.313739Z","iopub.status.idle":"2021-08-11T01:42:33.324374Z","shell.execute_reply.started":"2021-08-11T01:42:33.313689Z","shell.execute_reply":"2021-08-11T01:42:33.323668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Pre-processing Images: Data Augmentation\n\nData Augmentation allows us generate images with modifications from our dataset. The model will learn from these variations (changing angle, size and position), which will help it to predict new images which could have the same variations in position, size and angle.\n\nHeres the function to do that:","metadata":{"_uuid":"0e6b8f795b5c6a6f841cada7ee85e6fce644ebd0"}},{"cell_type":"code","source":"def generator(ATTR,df_partition):    \n    # join the partition with the chosen attribute in the same data frame\n    df_par_attr = df_partition.join(df_attr[ATTR], how='inner')\n\n    # Create Train dataframes\n    x_train, y_train = generate_df(df_par_attr,0, ATTR, TRAINING_SAMPLES)\n\n    # Create Validation dataframes\n    x_valid, y_valid = generate_df(df_par_attr,1, ATTR, VALIDATION_SAMPLES)\n\n    # Train - Data Preparation - Data Augmentation with generators\n    train_datagen =  ImageDataGenerator(\n      preprocessing_function=preprocess_input,\n      rotation_range=30,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n    )\n\n    train_datagen.fit(x_train)\n\n    train_generator = train_datagen.flow(x_train, y_train,batch_size=BATCH_SIZE)\n    \n    del x_train, y_train\n    \n    return train_generator, x_valid, y_valid","metadata":{"_uuid":"06f4c3fd15e33673946a14ca7ee0b00c7849bd81","execution":{"iopub.status.busy":"2021-08-11T01:42:33.326681Z","iopub.execute_input":"2021-08-11T01:42:33.327286Z","iopub.status.idle":"2021-08-11T01:42:33.337266Z","shell.execute_reply.started":"2021-08-11T01:42:33.327235Z","shell.execute_reply":"2021-08-11T01:42:33.336414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get training\n\nNow that we have all the functions we need, lets train our model(s). This cell takes each attribute, and trains an Inception model to recognise the feature.","metadata":{"_uuid":"ba61a0daf2db5a6fad9061984d6c6aff6d695a46"}},{"cell_type":"code","source":"#for each attribute, run the necessary functions in order to train, and then save an Inception model for the task\nfor attr in ATTR:\n    print('Learning to recognise: ',attr,', which is attribute',ATTR.index(attr)+1,' of ',len(ATTR))\n    train_generator, x_valid, y_valid=generator(attr,df_partition)\n    #gotta save memory\n    gc.collect()\n    training(attr, train_generator,x_valid,y_valid)\n    print(' ')\n    print(' ')\n    #gotta save memory\n    gc.collect()","metadata":{"_uuid":"a5e9ca535e820c62a923d66cf4edbf4e1484f092","execution":{"iopub.status.busy":"2021-08-11T01:42:33.339767Z","iopub.execute_input":"2021-08-11T01:42:33.340381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's see some results\n\nNow that we have a trained model for each of the attributes we're interested in, it's time to see how the model(s) did. The cell below contains some functions that we're going to use to do that nicely.","metadata":{"_uuid":"611ac23970abdadaae912cc3e9e3779265bd9fc1"}},{"cell_type":"code","source":"#function to load the image of the character we want to predict for\ndef img_to_display(filename):\n    # inspired on this kernel:\n    # https://www.kaggle.com/stassl/displaying-inline-images-in-pandas-dataframe\n    # credits to stassl :)\n    \n    i = Image.open(filename)\n    i.thumbnail((200, 200), Image.LANCZOS)\n    \n    with BytesIO() as buffer:\n        i.save(buffer, 'jpeg')\n        return base64.b64encode(buffer.getvalue()).decode()\n    \n#use some cool html to print out the prediction in a nice way\ndef display_result(filename, test_attr, prediction, target):\n    '''\n    Display the results in HTML\n    \n    '''\n    #convert ai speak to what we want to say is our true value of an attribute\n    attribute_target = {0: 'False'\n                    , 1: 'True'}\n    \n    #finds out if the model is going to predict true or false\n    attribute = 'True'\n    if prediction[1] <= 0.5:\n        attribute = 'False'\n            \n    display_html = '''\n    <div style=\"overflow: auto;  border: 2px solid #D8D8D8;\n        padding: 5px; width: 420px;\" >\n        <img src=\"data:image/jpeg;base64,{}\" style=\"float: left;\" width=\"200\" height=\"200\">\n        <div style=\"padding: 10px 0px 0px 20px; overflow: auto;\">\n            <h3 style=\"margin-left: 15px; margin-top: 2px;\">Does this person have {}?</h3>\n            <p style=\"margin-left: 15px; margin-top: -0px; font-size: 12px\">Prediction: {}</p>\n            <p style=\"margin-left: 15px; margin-top: -16px; font-size: 12px\">{} probability.</p>\n            <p style=\"margin-left: 15px; margin-top: -0px; font-size: 12px\">Truth: {}</p>\n            <p style=\"margin-left: 15px; margin-top: -16px; font-size: 12px\">Filename: {}</p>\n\n        </div>\n    </div>\n    '''.format(img_to_display(filename)\n               , test_attr\n               , attribute\n               , \"{0:.2f}%\".format(round(max(prediction)*100,2))\n               , attribute_target[target]\n               , filename.split('/')[-1]\n               \n               )\n\n    display(HTML(display_html))\n\n#predict an image\ndef attribute_prediction(filename):\n    '''\n    predict the attribute\n    \n    input:\n        filename: str of the file name\n        \n    return:\n        array of the prob of the targets.\n    \n    '''\n    \n    im = cv2.imread(filename)\n    im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (178, 218)).astype(np.float32) / 255.0\n    im = np.expand_dims(im, axis =0)\n    \n    # prediction\n    result = model_.predict(im)\n    prediction = np.argmax(result)\n    \n    return result\n    ","metadata":{"_uuid":"5ad0cdabb6e82d73de3815a10cebf9f8df4b378b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reminder or the available attributes:","metadata":{"_uuid":"cd0ca3b2da9729a0f7302bc25ac363db436b35a1"}},{"cell_type":"code","source":"#what are the attributes?\nfor attr in ATTR:\n    print(attr)","metadata":{"_uuid":"5a4ed737e7b9dc21b090c6d9fbf3e15b6d25158c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's see the AI in action!\n\nThe user selects which attribute to look at by changing the 'test_attr' variable. Then by running the cell, a prediction of one of the test set images is made. This is compared with the truth to see how it did.","metadata":{"_uuid":"7fd3e68c118bbfddc934d8c311944c164e54a2a7"}},{"cell_type":"code","source":"#choose which attribute to use\ntest_attr='Blond_Hair'\n\n#Run this cell to see the AI in action!\n\n# make the dataframe with the chosen attribute\ndf_par_attr = df_partition.join(df_attr[ATTR], how='inner')\n\n#first, load the relevant model weights\nmodel_.load_weights('weights.best.inc.'+test_attr+'.hdf5')\n\n#select random image from the test set\ndf_to_test = df_par_attr[(df_par_attr['partition'] == 2)].sample(10)\n\nfor index, target in df_to_test.iterrows():\n    result = attribute_prediction(images_folder + index)\n    \n    #display result\n    display_result(images_folder + index, test_attr, result[0], target[test_attr])","metadata":{"_uuid":"0cad6f9ef4e1ecba957a3086580ebfa392776ade","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look above for the image classifier outputs.\n\nConclusions:\n* Wow, transfer learning really works! I'm sure I'd never get such good results training a model from scratch. The fact it could re-train to work on each attribute in one epoch with ~600 images blows my mind. By just saving the weights of the re-trained layers, I can see how transfer learning could be used in a lot of real-world AI systems, becoming really useful really quickly, while not needing to store large models or datasets.\n* I started thinking about how transfer learning can be used for traninig a futue artificial general intelligence (AGI) system. First thought it, this isn't how our human brains work is it? Whenever we start a different task, we don't alter the weights (biological structure) of our neurons, right? We are just able to do a different task with exactly the same weights in the brain. So for an AGI, wouldn't it be best to train on a great many tasks with the same NN structure? Perhaps with some sort of gating network for the AGI to decide which task it was doing, then use a particular NN (from a system of many) for that task? I'd like to become more informed on transfer learning research for AGI, so any thoughts on this would be welcomed.","metadata":{"_uuid":"5421e8392384c684d416c17225b374a65517796c"}}]}