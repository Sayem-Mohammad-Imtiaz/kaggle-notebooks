{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Card Transaction Fraud Detection Dataset EDA\n\nIn this notebook, we explore the dataset to determine the important attributes that influence our dependent variable - is_fraud\n\nis_fraud = 1 indicates fraudulent transaction\n         = 0 indicates non-fraudulent transaction\n\nWe need to generate new features. From intuition, we can generate the following new features:\n* We can say that the hour of the day could be an influential factor. There is a higher chance that fraud transactions might be occuring during odd hours. So, we'll extract the hour.\n* Now, having extracted the hour, we can further encode it. Say, if the transaction is between 21:00-05:00, there is a higher chance it might be fraudulent.\n* We can also say that the frequency of the transaction can be an influential factor. For example, if the number of transactions in last 1/7/30 days suddenly increases, it might indicate a fraudulent transaction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv',parse_dates=['trans_date_trans_time',])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Features"},{"metadata":{},"cell_type":"markdown","source":"## Add hour feature\n\nHere, first, we simply extract the hour. Then, we encode transactions done in normal hours 0500-2100 as normal (0) and transactions done in abnormal hours 2100-0500 as abnormal (1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hour'] = df.trans_date_trans_time.dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hourEnc'] = 0\ndf.loc[df.hour < 5,'hourEnc'] = 1\ndf.loc[df.hour > 21,'hourEnc'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add frequencies of transactions\n\nNow, we need to generate frequencies of transactions done in last 1/7/30 days. For this, we use pandas rolling function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract frequencies of transactions in last 1/7/30 days\ndef last1DayTransactionCount(x):\n    temp = pd.Series(x.index, index = x.trans_date_trans_time, name='count_1_day').sort_index()\n    count_1_day = temp.rolling('1d').count() - 1\n    count_1_day.index = temp.values\n    x['count_1_day'] = count_1_day.reindex(x.index)\n    return x\ndef last7DaysTransactionCount(x):\n    temp = pd.Series(x.index, index = x.trans_date_trans_time, name='count_7_days').sort_index()\n    count_7_days = temp.rolling('7d').count() - 1\n    count_7_days.index = temp.values\n    x['count_7_days'] = count_7_days.reindex(x.index)\n    return x\ndef last30DaysTransactionCount(x):\n    temp = pd.Series(x.index, index = x.trans_date_trans_time, name='count_30_days').sort_index()\n    count_30_days = temp.rolling('30d').count() - 1\n    count_30_days.index = temp.values\n    x['count_30_days'] = count_30_days.reindex(x.index)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.groupby('cc_num').apply(last1DayTransactionCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.groupby('cc_num').apply(last7DaysTransactionCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.groupby('cc_num').apply(last30DaysTransactionCount)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add times since last transaction - time_diff"},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeDifference(x):\n    x['time_diff'] = x.trans_date_trans_time - x.trans_date_trans_time.shift()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.groupby('cc_num').apply(timeDifference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['time_diff'] = df1['time_diff'].dt.seconds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Display correlation heatmaps"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True).set_title('Correlation heatmap without generated features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(df1.corr(),annot=True).set_title('Correlation heatmap with generated features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# is_fraud correlation\n\nAs you can see from the following correlation series, amount, normal/abnormal hour, count_30_days, count_7_days, time_diff, hour are important contributors"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.corr()['is_fraud'].abs().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fin."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}