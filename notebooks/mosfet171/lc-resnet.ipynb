{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install wandb --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport glob\nimport datetime\n\nfrom wandb.keras import WandbCallback\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import Sequence\nfrom keras.layers import AveragePooling2D, Input, Conv2D\nfrom sklearn.model_selection import StratifiedKFold\nfrom skimage.io import imread,imshow\nfrom skimage.transform import resize\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\n%reload_ext tensorboard\n\nwandb.login()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MergedGenerators(Sequence):\n\n    def __init__(self, batch_size, generators=[], sub_batch_size=[]):\n        self.generators = generators\n        self.sub_batch_size = sub_batch_size\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(\n            sum([(len(self.generators[idx]) * self.sub_batch_size[idx])\n                 for idx in range(len(self.sub_batch_size))]) /\n            self.batch_size)\n\n    def __getitem__(self, index):\n        X_batch = []\n        Y_batch = []\n        for generator in self.generators:\n            if generator.class_mode is None:\n                x1 = generator[index % len(generator)]\n                X_batch = [*X_batch, *x1]\n\n            else:\n                x1, y1 = generator[index % len(generator)]\n                X_batch = [*X_batch, *x1]\n                Y_batch = [*Y_batch, *y1]\n\n        if self.generators[0].class_mode is None:\n            return np.array(X_batch)\n        return np.array(X_batch), np.array(Y_batch)\n\n\ndef build_datagenerator(dir1=None, dir2=None, dir3=None, batch_size=32):\n    n_images_in_dir1 = sum([len(files) for r, d, files in os.walk(dir1)])\n    n_images_in_dir2 = sum([len(files) for r, d, files in os.walk(dir2)])\n    n_images_in_dir3 = sum([len(files) for r, d, files in os.walk(dir3)])\n\n    generator1_batch_size = int((n_images_in_dir1 * batch_size) /\n                                (n_images_in_dir1 + n_images_in_dir2 + n_images_in_dir3))\n    \n    generator2_batch_size = int((n_images_in_dir2 * batch_size) /\n                                (n_images_in_dir1 + n_images_in_dir2 + n_images_in_dir3))\n\n    generator3_batch_size = batch_size - generator1_batch_size - generator2_batch_size\n\n    generator = ImageDataGenerator(rescale=1. / 255)\n\n    generator1 = generator.flow_from_directory(\n        dir1,\n        target_size=(600, 600),\n        color_mode='rgb',\n        class_mode='binary',\n        batch_size=generator1_batch_size,\n        shuffle=True,\n        seed=171)\n\n    generator2 = generator.flow_from_directory(\n        dir2,\n        target_size=(600, 600),\n        color_mode='rgb',\n        class_mode='binary',\n        batch_size=generator2_batch_size,\n        shuffle=True,\n        seed=171)\n    \n    generator3 = generator.flow_from_directory(\n        dir3,\n        target_size=(600, 600),\n        color_mode='rgb',\n        class_mode='binary',\n        batch_size=generator3_batch_size,\n        shuffle=True,\n        seed=171)\n\n    return MergedGenerators(\n        batch_size,\n        generators=[generator1, generator2, generator3],\n        sub_batch_size=[generator1_batch_size, generator2_batch_size, generator2_batch_size])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_module(layer_in, n_filters):\n    merge_input = layer_in\n    if layer_in.shape[-1] != n_filters:\n        merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    conv1 = Conv2D(n_filters, wandb.config.kernel_size, padding='same', activation='relu')(layer_in)\n    conv2 = Conv2D(n_filters, wandb.config.kernel_size, padding='same', activation='relu')(conv1)\n    layer_out = add([conv2, merge_input])\n    layer_out = Activation('relu')(layer_out)\n    return layer_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import *\nfrom keras.utils import plot_model\nfrom keras.activations import relu, sigmoid\n\ndef MyModel():\n    num_fil = wandb.config.conv_filters\n    ks = wandb.config.kernel_size\n    inputs = Input((600, 600, 3))\n    layer = AveragePooling2D(pool_size=(2,2))(inputs)\n    layer = Conv2D(num_fil, (ks, ks), activation='relu')(layer)\n    layer = AveragePooling2D(pool_size=(2,2))(layer)\n    layer = residual_module(layer, num_fil)\n    layer = residual_module(layer, num_fil)\n    layer = AveragePooling2D(pool_size=(2,2))(layer)\n    layer = residual_module(layer, num_fil*2)\n    layer = residual_module(layer, num_fil*2)\n    layer = AveragePooling2D(pool_size=(2,2))(layer)\n    layer = residual_module(layer, num_fil*3)\n    layer = residual_module(layer, num_fil*3)\n    layer = GlobalAveragePooling2D()(layer)\n    layer = Dropout(wandb.config.dropout)(layer)\n    outputs = Dense(units=1,activation='sigmoid')(layer)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    # Specify the hyperparameter to be tuned along with\n    # an initial value\n    config_defaults = {\n        'batch_size': 8,\n        'learning_rate': 1,\n        'conv_filters': 16,\n        'kernel_size': 3\n    }\n\n    # Initialize wandb with a sample project name\n    wandb.init(config=config_defaults)\n\n    # Specify the other hyperparameters to the configuration, if any\n    wandb.config.epochs = 20\n\n    # Prepare train dataset\n    train_ds = build_datagenerator(dir1='/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_0/',\n                                   dir2='/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_1/',\n                                   dir3='/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_2/',\n                                   batch_size=wandb.config.batch_size)\n    \n    \n    # prepare validation dataset \n    generator = ImageDataGenerator(rescale=1. / 255)\n    val_dir = '../input/leukemia-classification/C-NMC_Leukemia/validation_data/'\n    val_df = pd.read_csv(val_dir + 'C-NMC_test_prelim_phase_data_labels.csv',dtype=str)\n    val_df.loc[(val_df['labels'] == '0'), ['labels'] ] =  '2'\n    val_df.loc[(val_df['labels'] == '1'), ['labels'] ] =  '0'\n    val_df.loc[(val_df['labels'] == '2'), ['labels'] ] =  '1'\n    val_ds = generator.flow_from_dataframe(val_df,\n                                           directory=val_dir+'C-NMC_test_prelim_phase_data',\n                                           x_col=\"new_names\",\n                                           y_col=\"labels\",\n                                           class_mode=\"binary\",\n                                           target_size=(600,600),\n                                           validate_filenames=False,\n                                           batch_size=wandb.config.batch_size,\n                                           prefetch=tf.data.experimental.AUTOTUNE)\n    \n    # Iniialize model with hyperparameters\n    keras.backend.clear_session()\n    model = MyModel()\n    \n    # Compile the model\n    opt = tf.keras.optimizers.SGD(lr=wandb.config.learning_rate)\n    model.compile(opt, loss='binary_crossentropy', metrics=['acc'])\n    model.summary()\n    \n    # Train the model\n    _ = model.fit(train_ds,\n                  epochs=wandb.config.epochs, \n                  validation_data=val_ds,\n                  callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n                            \n    # Evaluate    \n    loss, accuracy = model.evaluate(val_ds, callbacks=[WandbCallback()])\n    print('Test Error Rate: ', round((1-accuracy)*100, 2))\n    wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)}) # wandb.log to track custom metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sweep_config = {\n  'method': 'bayes', \n  'metric': {\n      'name': 'val_loss',\n      'goal': 'minimize'\n  },\n  'early_terminate':{\n      'type': 'hyperband',\n      'min_iter': 5\n  },\n  'parameters': {\n      'batch_size': {\n          'values': [16,32]\n      },\n      'learning_rate':{\n          'values': [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]\n      },\n      'conv_filters':{\n          'values': [16,32,48,64]\n      },\n      'kernel_size':{\n          'values': [5,7,9,11]\n      },\n      'dropout':{\n          'values': [0, 0.2, 0.5, 0.8]\n      }\n  }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"LC-ResNet\")\n(sweep_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sweep_id = '33i5anlw'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}