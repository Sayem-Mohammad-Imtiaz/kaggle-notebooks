{"cells":[{"metadata":{},"cell_type":"markdown","source":"** Model aggregation **\n\n* Bagging algorithm\n* Boosting algorithm\n* Voting algorithm\n\n\n\n1. This approach allows us to improve the model accuracy.\n2. Lower error.\n3. Higher consistency that means avoids over fitting.\n4. Reduce bias and variance error.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading Iris Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/iris-flower-dataset/IRIS.csv', header=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns=['species'], axis=1)\ny = data['species']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It's an overfitting"},{"metadata":{},"cell_type":"markdown","source":"# Random forest classifier \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelRF = RandomForestClassifier(n_estimators=10)\nmodelRF.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelRF.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelRF.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Still Overfitting"},{"metadata":{},"cell_type":"markdown","source":"# Now let's start with bagging classifier from sklearn\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging = BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5, max_features=1, n_estimators=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try ada-boosting\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=10, learning_rate=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bagging.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's check out voting ensemble classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nsvm = SVC(kernel='poly', degree=2)\ndt = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = VotingClassifier(estimators=[('lr', lr),('dt', dt), ('svm', svm)], voting='hard')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The lesson in this article is that we can improve model accuracy using model aggregation. However, in this case model accuracy is not improved. So, we can conclude that this dataset or approach wasn't appropriate to improve the model accuracy for this particular dataset. You can try MNIST default iris or digit dataset to do the model aggregation. You will observe the changes in model accuracy. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}