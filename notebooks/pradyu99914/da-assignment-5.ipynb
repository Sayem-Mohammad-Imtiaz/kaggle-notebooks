{"cells":[{"metadata":{},"cell_type":"markdown","source":"DA Assignment 5<br>\nPradyumna YM           PES12301700986<br>\nAnush V Kini           PES1201701646<br>\nPunit Pranesh Koujalgi PES1201701502"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Question-1 <br><br>\nFashion catalog images has about 44000 images of different categories. ‘style.csv’ contains metadata about the images. <br>\nConsider ‘masterCategory’ from this file as your target variable. Using the images, do the following:<br><br>\n1.Classify the given set of images using a vanilla CNN( Don’t apply PCA for this!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reqad the csv file that contains the details\nds = pd.read_csv(\"/kaggle/input/fashion-product-images-small/myntradataset/styles.csv\",error_bad_lines=False)\nds.head()\n#combine the id with .jpg to get the filenames...\nds['image'] = ds.apply(lambda row: str(row['id']) + \".jpg\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n#image generator object from keras. reference : Keras Docs\nimage_generator = ImageDataGenerator(\n    validation_split=0.2\n)\n\n#create a flow of images for training the model.\ntraining_generator = image_generator.flow_from_dataframe(\n    dataframe=ds,\n    directory= \"/kaggle/input/fashion-product-images-small/myntradataset/images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(80,60),\n    batch_size=256,\n    subset=\"training\"\n\n)\n\n#create a flow of images for validating(testing) the trained model.\nvalidation_generator = image_generator.flow_from_dataframe(\n    dataframe=ds,\n    directory=\"/kaggle/input/fashion-product-images-small/myntradataset/images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(80,60),\n    batch_size=256,\n    subset=\"validation\"\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras import layers,models\n#create a sequential model\nmodel = models.Sequential()\n\n#add the necessary layers.\nmodel.add(layers.Conv2D(32, (5,5), strides = (2,2), activation = 'relu' , input_shape = (80,60,3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(7, activation='softmax'))\n\n#compile\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n#fit the model\nhistory = model.fit_generator(training_generator, epochs=5, steps_per_epoch = 139 , verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the accuracy is pretty high, at around 96%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras provides an evaluate function that returns [metric, accuracy]\n#this model takes the validation generator and number of steps/batches to validate on. (test_set_size/batch_size = 8960/256)\nmodel.evaluate_generator(validation_generator,35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"2.PCA is one of the most common dimensionality reduction techniques used. Using PCA with number of components ranging from 2 to 5, classify the given set of images using<br>a.K-Nearest Neighbours ( consider k=7)<br>b.Artificial Neural Network<br>\n(Hint: “from sklearn.decomposition import PCA”. Use this module for doing PCA. Use sklearn for KNN and ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt # plotting\nimport matplotlib.image as mpimg\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.decomposition import PCA #PCA\nimport cv2\n\nimport os # accessing directory structure","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#path to the dataset..\nDATASET_PATH = \"/kaggle/input/fashion-product-images-small/myntradataset/\"\nprint(os.listdir(DATASET_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the csv file with the details. find the file name by appending .jpg to the image id\ndf = pd.read_csv(DATASET_PATH + \"styles.csv\", error_bad_lines=False)\ndf['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n#shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = []\nrowstoberemoved = []\n\n#code to read the images based on the dataframe\nfor img_id in range(len(df['id'])):\n    img_path = DATASET_PATH + 'images/' + str(df.loc[img_id,\"image\"])\n    #read the image\n    img = cv2.imread(img_path)\n    try:\n        #resize to the required size\n        img = cv2.resize(img, (28,28)) \n        #flatten the image\n        img = img.flatten()\n    except:\n        #remove the row corresponding to the image with error\n        rowstoberemoved.append(img_id)\n        continue\n    img = pd.Series(img,name=img_path)\n    images.append(img)\n#drop rows with errors\ndf = df.drop(df.index[rowstoberemoved])\nprint(\"number of proper images:\",len(images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = list(set(df[\"masterCategory\"]))\n\n#convert categorical to class numbers(sklearn handles categories given as numbers)\nylabels = np.asarray([indices.index(i) for i in df[\"masterCategory\"]])\nylabels_onehot = []\nfor i in ylabels:\n    ylabels_onehot.append([0 for i in range(len(indices))])\n    ylabels_onehot[-1][i] = 1\nylabels_onehot = np.asarray(ylabels_onehot)\nprint(ylabels_onehot[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nKNN_Scores = []\nANN_Scores = []\nKNN_train_scores = []\nANN_train_scores = []\n#for each value of number of components \nfor i in tqdm(range(2, 6)):\n    #perform pca\n    pca = PCA(n_components  = i)\n    pca.fit(images)\n    \n    #take only the required rows\n    new_images = pca.transform(images)\n    \n    #split into train and test set\n    X_train, X_test, y_train, y_test = train_test_split(new_images, ylabels, test_size=0.2)\n    \n    #scale values for better fit\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    #fit knn\n    model = KNeighborsClassifier(n_neighbors=7)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n    KNN_Scores.append(accuracy_score(y_test, y_pred))\n    KNN_train_scores.append(accuracy_score(y_train, y_train_pred))\n    \n    #fit ann\n    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10,8,8,4), random_state = 4)\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    y_train_pred = clf.predict(X_train)\n    ANN_train_scores.append(accuracy_score(y_train, y_train_pred))\n    ANN_Scores.append(accuracy_score(y_test, y_pred))\nprint(\"The accuracies for c = 2 to 5 for kNN on test set are\",KNN_Scores)\nprint(\"The accuracies for c = 2 to 5 for ANN on test set are\",ANN_Scores)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The accuracies for c = 2 to 5 for kNN on train set are\",KNN_train_scores)\nprint(\"The accuracies for c = 2 to 5 for ANN on train set are\",ANN_train_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#reference link: PCA : https://www.kaggle.com/anushkini/daa-a5?scriptVersionId=23372502 <br>\nvanilla CNN: https://www.kaggle.com/pradyu99914/da-assignment-5?scriptVersionId=23098547 <br>\n3. Compare the three models with respect to the accuracy for  both train and test. Do you think the result obtained will be the same given a more complex data set?<br>\nAs we can see, the vanilla CNN model performs marginally better than the kNN model and the MLFFN on the test set. <br> This can be attributed to the higher number of features the CNN has to work with. CNN's are much better at recognising patterns as they use various filters, which help extract important features such as edges, curves, and other higher level details in an image.<br>. since we have scaled/normalized the Principal Components, the two models perform quite well, while still being computationally very efficient compared to a CNN while training.<br> On the train set, the CNN model has a higher accuracy. This is because the model is big/complex enough to \"fit\" the whole of the training data perfectly, given the number of epochs. <br> On the other hand, kNN and ANN are fairly simple, thereby, generalise well enough without performing as well as a CNN on the training set.<br>\nGiven a more complex dataset, the CNN would perform better, than the kNN and ANN. This is because kNN is affected by the curse of dimesionality, and ANN can not generalise well on complex patterns. Whereas, a CNN can perform well in such "},{"metadata":{},"cell_type":"markdown","source":"Question 2:\nAmy has come up with a series of exercises to help with Sheldon’s need for closure. The dataset Big Bang Theoryhas an audio clip which contains the best scenes from one of the episodes. Use this audio clip to extract the following features and display their dimension:<br>\n1.MFCC<br>\n2.Zero Crossing rate<br>\n3.Spectral Centroids<br>\n4.Pitch<br>\n5.Root Mean Square for the signal<br>\nFind out the use of each of the above feature. Using these features, given a problem of content classification(eg. laughter track vs dialog), which algorithm would you use to classify and why?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport librosa.display\naudio_path = '/kaggle/input/audio-data/The Big Bang Theory Season 6 Ep 21 - Best Scenes.wav'\nx , sr = librosa.load(audio_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x , sr = librosa.load(audio_path)\nprint(type(x), type(sr))\nlibrosa.load(audio_path, sr=None)\n#display waveform\n\n\nplt.figure(figsize=(30, 10))\nplt.xlabel(\"Time\")\nplt.title(\"Waveform of the given audio file\")\nlibrosa.display.waveplot(x, sr=sr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MFCC features represent phonemes (distinct units of sound) as the shape of the vocal tract (which is responsible for sound generation). <br>\nThe main application of MFCC is as a feature to many speech recognition models/systems, which can automatically recognise words spoken in an audio file.<br>\nMFCC feature is not very robust in presence of additive noise. So, pople usualli normalise their values in speech reconition systems."},{"metadata":{"trusted":true},"cell_type":"code","source":"mfccs = librosa.feature.mfcc(x, sr=sr)\nprint(mfccs.shape)\n#Displaying  the MFCCs:\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to zero to negative or from negative to zero to positive.<br>\nThis feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.<br> It is particularly useful in voice ctivity detection, ie, telling whether there is any human speech in an audio file."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Zero Crossing rate\nzc = librosa.feature.zero_crossing_rate(x)\nprint(\"The dimension of ZCR is\", sum(zc).shape)\nplt.figure(figsize = (20,10))\nplt.plot(sum(zc))\nplt.xlabel(\"Time\")\nplt.ylabel(\"Zero crossing Rate\")\nplt.title(\"Zero crossing rate of the given audio clip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The spectral centroid indicates where the center of mass of the spectrum is located. It is mainly used to characterise the spectrum in an audio segment. <br>It is the weighted mean of the frequencies present in the sound, determined using a Fourier transform, with their magnitudes as the weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"cent = librosa.feature.spectral_centroid(x, sr=sr)\nplt.figure(figsize = (20,10))\nplt.plot(sum(cent))\nplt.xlabel(\"time\")\nplt.ylabel(\"Spectral Centroid\")\nplt.title(\"Spectral centroids of the audio segment\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pitch is the fundamental period of the speech signal. It the perceptual correlate of fundamental frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"pitches, magnitudes = librosa.core.piptrack(x, sr=sr)\npitches, magnitudes = librosa.piptrack(y=x, sr=sr)\nprint(\"shape of pitch:\",pitches.shape)\na=plt.plot(sum(pitches))\nplt.xlabel(\"Time\")\nplt.ylabel(\"Pitch\")\nplt.title(\"Pitch of the given audio segment\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMS represents the average power of a signal. This feature basically indicates the \"loudness of an audio track\", and this feature might<br> be very useful\nin applications such as voice/no voice classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"rootMeanSquare=librosa.feature.rms(y=x)\nprint(\"Dimensions of root mean square feature are :\",rootMeanSquare.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a classification problem, the best model to use will be an RNN. This is because RNNs are very good at handling sequence data, and as we know, all these features <br> are a time sequence of values. So, any variant of RNNs such as LSTM, GRU can be used. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}