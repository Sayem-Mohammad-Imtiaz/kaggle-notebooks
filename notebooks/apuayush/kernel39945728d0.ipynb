{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#load necessary modules\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport h5py\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom keras.initializers import glorot_uniform\nimport scipy.misc\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\n\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nK.set_learning_phase(1)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c88df74db058932f72356c896df6c715f83a948e"},"cell_type":"code","source":"print(os.listdir(\"../lib/kagglegym\"))","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"255f3ab5-df7b-4a3f-ad19-69e517fd653e","_uuid":"a39cb8c621da54641878aa40cd12132d005cf1a6","collapsed":true,"trusted":true},"cell_type":"code","source":"\ndef load_dataset():\n    train_dataset = h5py.File('../input/hand-sign/train_signs.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('../input/hand-sign1/test_signs.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[0]               \n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    return mini_batches\n\n\n\n","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('../input/sign_mnist_train.csv')\n# test = pd.read_csv('../input/sign_mnist_train.csv')\n\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"ca13f0c7-342e-41db-b976-50441695d43e","_uuid":"29f13b381eb17bdb89e47986945ca23f56d09639","trusted":true},"cell_type":"code","source":"print(X_test_orig.shape, X_train_orig.shape)\nprint(Y_train_orig.shape)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"2afc99b5-c0c7-4a97-9ef1-41bf54e5fb6a","_uuid":"2968869172232ecdc40d2a378214193922121e94"},"cell_type":"markdown","source":"# OneHotEncoding\nIt puts 1 on the index of the current label we are accessing , say if the image is for a fist it will create a array of 24 elements and all values would be 0 except 5th position whose value would be 1 ."},{"metadata":{"_cell_guid":"5c1b0eea-5c91-41fd-b69b-f2573f089175","_uuid":"13588bacfd22fc4e9a45b6ed9bb80068b28640ca","collapsed":true,"trusted":true},"cell_type":"code","source":"def convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"1e13fc33-aa6f-4b05-9816-dd4211e3aa50","_uuid":"1abf3e058733ccefff455b9f92eed017121e2876","collapsed":true,"trusted":true},"cell_type":"code","source":"Y_train = convert_to_one_hot(Y_train_orig, 6).T\nY_test = convert_to_one_hot(Y_test_orig, 6).T","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"a0853605-3064-48c5-978b-f09d6cb521e5","_uuid":"af1c558fc990cef42c41be42104257bad3309153"},"cell_type":"markdown","source":"This simple histogram shows the count of images of gestures in the training data for each number. This graphic is used to visualize if there is an unequal sample size among the digits. The sample size for each digit appear to be comparable. There is no issue of unequal sampling."},{"metadata":{"_cell_guid":"0dd5f34a-f67c-4310-89ba-427c9337b73b","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"3349bd689dbe4b2dc2309a3a5a77d7a5f3845e98","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(Y_train)\nplt.title(\"Frequency Histogram of Numbers in Training Data\")\nplt.xlabel(\"Gesture type\")\nplt.ylabel(\"Frequency\")","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"146ea2d8-4be3-4dce-a08b-db56bdcf0254","_uuid":"8e0dbba4b0c7f9f1669df02eb806656910397398","trusted":true},"cell_type":"code","source":"import math\n# plot the first 25 images in the training set. \nf, ax = plt.subplots(5, 5)\n# plot some 4s as an example\nfor i in range(1,26):\n    data = X_train_orig[i] #this is the first number\n    nrows, ncols = 64,64\n    grid = data.reshape((nrows, ncols, 3))\n    n=math.ceil(i/5)-1\n    m=[0,1,2,3,4]*5\n    ax[m[i-1], n].imshow(grid)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"88336fc6-0abc-4dd2-8073-9c611018bb8b","_uuid":"57b3a8831602e0730d09592aebd6f9c3fccd7045"},"cell_type":"markdown","source":"# Normalization\nThe dataset we already have is in grayscale i.e only 2-d vector(but flattened) . Now we are gonna normalize data for comparison. "},{"metadata":{"_cell_guid":"9ddffd67-3cfb-4e9b-a63a-b3b59576cb18","_uuid":"099caa3e6a83244c8c46add7d62a4ca8d075ea2e","collapsed":true,"trusted":true},"cell_type":"code","source":"def normalization(x, mu, sigma):\n    return (x-mu)/sigma","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"c2dd0658-4d38-473d-ac3f-717114701db2","_uuid":"71d4ed30521cabb3c14912894c4c94d8f6731e91","collapsed":true,"trusted":true},"cell_type":"code","source":"mu = np.mean(X_train_orig, axis=0)\nsigma = np.max(X_train_orig, axis=0)-np.min(X_train_orig, axis=0)+1e-20","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"5e55cb74-c9cf-4752-9d12-7bb3c1f4b2a6","_uuid":"e29987a99ac6974c56c33da7309131a6d322579b","collapsed":true,"trusted":true},"cell_type":"code","source":"test = normalization(X_test_orig, mu, sigma)\nX_train = normalization(X_train_orig, mu, sigma)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"3b3ae072-2673-4cfa-97e4-ca575d9dbef6","_uuid":"a2fc6bfe130ba91066f0e485b51a17563053fdbc","trusted":true},"cell_type":"code","source":"print(\"the train pixels lie between %.2f to %.2f\" % (np.min(np.min(X_train)), np.max(np.max(X_train))))","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"378f8f7b-ced5-4119-a2b4-46390202d050","_uuid":"81976d4a7f9b2e43a7c0baf3e003b66f07f6506e","trusted":true},"cell_type":"code","source":"print(Y_train.shape)","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"016e9362-1895-4535-b5ad-64c63f50a7b8","_uuid":"a4233916e352fffd6c5de1bef6a81567ef316d40","trusted":true},"cell_type":"code","source":"print(X_train.shape)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"b5f186b7-8d1f-4973-97a2-7cfa24671757","_uuid":"9bb027ed0faeedf80aef9ecbb2fcc2789a229e8c"},"cell_type":"markdown","source":"# Creating Model\nusing resnet-50 which has 50 layers and is used to predict much complex features in an image\n"},{"metadata":{"_cell_guid":"44a7c344-bc6d-49c3-b2a3-e23aa7aeea34","_uuid":"8c9f565f69dbfa82f0839b36f4d60092e1ba05d6","collapsed":true,"trusted":true},"cell_type":"code","source":"def identity_block(X, f, filters, stage, block):\n    \"\"\"\n    Implementation of the identity block as defined in Figure 3\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV's window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    \n    Returns:\n    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n    \"\"\"\n    \n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"af4150a7-de74-40e4-b76f-8476c6fad64a","_uuid":"3a3c977791d18c2d4ba673668f1171bb6109fc6b","collapsed":true,"trusted":true},"cell_type":"code","source":"\ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    \"\"\"\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV's window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    s -- Integer, specifying the stride to be used\n    \n    Returns:\n    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n    \"\"\"\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    X_shortcut = X\n\n    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(F2, (f,f), strides=(1,1), name=conv_name_base+'2b',padding='same', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c',padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base+'2c')(X)\n\n    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s),padding='valid', name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3, name=bn_name_base+'1')(X_shortcut)\n\n    \n    X =  X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    \n    return X","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"747840f6-c68d-4ce7-9d64-4ad29564df84","_uuid":"589054f9655f6b51ac19b89ba9d788c9af95f7d2","collapsed":true,"trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: ResNet50\n\ndef ResNet50(input_shape = (64, 64, 3), classes = 6):\n    \"\"\"\n    Implementation of the popular ResNet50 the following architecture:\n    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n\n    Arguments:\n    input_shape -- shape of the images of the dataset\n    classes -- integer, number of classes\n\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"\n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n\n    # Stage 3 \n    X = convolutional_block(X, f = 3, filters = [128,128,512], stage = 3, block='a', s = 2)\n    X = identity_block(X, 3, [128,128,512], stage=3, block='b')\n    X = identity_block(X, 3, [128,128,512], stage=3, block='c')\n    X = identity_block(X, 3, [128,128,512], stage=3, block='d')\n\n    # Stage 4 \n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n\n    # Stage 5\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n\n    # AVGPOOL. Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D()(X)\n\n    # output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"c4db924e-4924-470c-88f6-4dfc22fc3a9a","_uuid":"a7eabebbd285458878a2fb7e6730a6242f73c081","collapsed":true,"trusted":true},"cell_type":"code","source":"model = ResNet50(input_shape = (64, 64, 3), classes = 6)","execution_count":18,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ebc3736582f136747f1b7154a4c4d5dd843b5d4d"},"cell_type":"code","source":"try:\n    model.load_weights('hand_detection_weights.h5')\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8d43de2-4f65-4989-91f5-d81bf52df044","_uuid":"d302986ad13af3dbfe19ce837b6ca219f88dbc34","collapsed":true,"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"09653967-2ce7-4819-bd0b-45f7b8b4cc5d","_uuid":"9cdbb06596e5f81066850116fdeec9399f24b811","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2078f6f-e94a-432f-9290-c0ee092961f0","_uuid":"5b418178263cd07838395cc001dcd13628ab1cc3","collapsed":true,"trusted":true},"cell_type":"code","source":"\nmodel.fit(X_train, Y_train, epochs = 10, batch_size = 32,validation_split=0.2)\nmodel.save_weights('hand_detection_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2380464b-acd9-4e9c-830a-5e4113eba9f3","_uuid":"e7967cba242a75eff05c0f422bbc02b80a1cde99","collapsed":true,"trusted":true},"cell_type":"code","source":"model.save_weights('hand_detection_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd887797-0d2c-4f15-aa85-66308f218d19","_uuid":"a8a6db496f6ebee1b7cdfa7744a3dfdca976dc21","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49c39a6c-2df2-4662-808b-ee3ea3bd22b9","_uuid":"9ecd734e0ac7b879271584156a2ac3493306ec39","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e801a5e9-cab5-470d-b328-7faafc1d6fc8","_uuid":"9d7224b87f2046bcaf14a1cdbba4f90cad462899","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}