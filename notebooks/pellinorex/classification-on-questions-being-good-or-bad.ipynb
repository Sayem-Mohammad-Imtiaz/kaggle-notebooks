{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0e236175-52eb-01c9-4a35-8dcfab0bbb13"},"source":"I only complete the model building on downsampling part, if you are interested in my final result on the whole dataset, below is the Github link:\n\n[My complete research on Github](https://github.com/PaulX-CN/StackOverflowPython/blob/master/project3.ipynb)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4d58cd1-8a62-554f-c974-5439b68ad427"},"outputs":[],"source":"# all of the imports\nimport pandas as pd\nimport numpy as np\nimport pickle \nimport patsy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.cross_validation import train_test_split\n% matplotlib inline\nfrom sklearn import preprocessing as pp\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#from sqlalchemy import create_engine\n#cnx = create_engine('postgresql://username:password@IP:PORT/user')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62454713-687a-b596-5750-68367f29efbe"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, auc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bf86dc0-5b0d-4109-5fe5-89752c51d3cf"},"outputs":[],"source":"import nltk\nimport itertools\nfrom nltk.probability import FreqDist\n\nstopset = set(nltk.corpus.stopwords.words('english'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1202465-f9af-5a07-f734-d11426ecee7a"},"outputs":[],"source":"def rotate(ax, degree):\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(degree)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ecd46595-92b9-9f7f-ce91-921e4537e110"},"outputs":[],"source":"questions = pd.read_csv('../input/Questions.csv',encoding='latin1')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ff40523-65a5-aa19-968e-4f1b5e1a9e85"},"outputs":[],"source":"questions.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73b7b49d-9456-34ad-8357-7ac1095691eb"},"outputs":[],"source":"questions.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a99001db-8e28-3b62-4491-3d890005c96d"},"outputs":[],"source":"# extract all the code part \na = questions['Body'].str.extractall(r'(<code>[^<]+</code>)')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52268999-9251-fec7-20c2-89eede74cceb"},"outputs":[],"source":"# unstack and convert into a single column for cleaning\ntest = a.unstack('match')\n\ntest.columns = test.columns.droplevel()\n# put all columns together\ncode = pd.DataFrame(test.apply(lambda x: x.str.cat(), axis=1,reduce=True))\n# rename \ncode.columns = ['CodeBody']\n# remove the html tags finally\ncode['CodeBody'] = code['CodeBody'].str.replace(r'<[^>]+>|\\n|\\r',' ')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6098b12a-0780-d2db-baaf-0f553c96fe3c"},"outputs":[],"source":"# remove the code part from questions\nbody = questions['Body'].str.replace(r'<code>[^<]+</code>',' ')\n# build up the question part from questions\nquestions['QuestionBody'] = body.str.replace(r\"<[^>]+>|\\n|\\r\", \" \")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0011fea9-74a9-4fd6-cb68-fa3a703dbb85"},"outputs":[],"source":"# Join the codebody by index\nquestions = questions.join(code)\n# final cleaned dataset\nquestions_final = questions.drop('Body',axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78a0f7c1-2722-13e6-6b98-af2017cd7fc4"},"outputs":[],"source":"# assume all answers without userID are from the same guy ID 0\nquestions_final['OwnerUserId'].fillna(0,inplace=True)\nquestions_final.OwnerUserId = questions_final.OwnerUserId.astype(int)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8773c7f8-28d5-a443-e3b1-0cbadfb886c9"},"outputs":[],"source":"tags = pd.read_csv('../input/Tags.csv',encoding='latin1')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"676d1c5e-c584-b611-a5c3-24bafdb5113c"},"outputs":[],"source":"tags.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fbe19860-2ef4-e50a-fb97-9a6088d763e9"},"outputs":[],"source":"#when I was writing data into sql I found few errors\ntagID = set(tags.Id)\nquestionID = set(questions_final.Id)\nerrors=tagID-questionID\nprint(errors)\ntags = tags[tags.Tag != 40115300]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05eaff0c-a2a7-eac8-8cb1-e90cfee15e37"},"outputs":[],"source":"tags = tags[tags.Tag.notnull()]\n#tags.to_csv('tags_final.csv',index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"657dc489-5079-d51c-bc94-4919552505d5"},"outputs":[],"source":"#tags.groupby('Id').count()\nfig, ax = plt.subplots()\nsns.distplot(tags.groupby('Id').count())\nax.set_xlabel('number of tags')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"799acbd2-ca05-61c7-aa3b-50c60c93444c"},"outputs":[],"source":"tagsByquestion = tags.groupby('Id',as_index=False).agg(lambda x: ' '.join(x))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"989f8d3d-78b8-60e4-6d7c-7ba7aafc4938"},"outputs":[],"source":"fig, ax = plt.subplots()\nsns.distplot(questions[questions.Score <=10].Score,kde=False)\nax.set_xlabel('distribution of scores')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"769c4600-9db8-01d8-4dd4-83189e5bf656"},"outputs":[],"source":"dfFinal = questions_final.loc[(questions_final.Score>=5) | (questions_final.Score<0)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"effbb0b6-3b6b-5ed6-8dbf-1ba85e9f6b48"},"outputs":[],"source":"texts = list(dfFinal.Title)\n# Tokenize the titles\ntexts = [nltk.word_tokenize(text) for text in texts]\n# pos tag the tokens\ntxtpos = [nltk.pos_tag(texts) for texts in texts]\n# for titles we only care about verbs and nouns\ntxtpos = [[w for w in s if (w[1][0] == 'N' or w[1][0] == 'V') and w[0].lower() not in stopset] \n                  for s in txtpos]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a0f94ac-d790-3f15-36cc-ce583c04c8ed"},"outputs":[],"source":"qbodys = list(dfFinal.QuestionBody)\n#break into sentences\nqsents = [nltk.sent_tokenize(text) for text in qbodys]\n# Tokenize the question body\nqbodys = [nltk.word_tokenize(text) for text in qbodys]\n# attach tags to the body\nqpos = [nltk.pos_tag(texts) for texts in qbodys]"},{"cell_type":"markdown","metadata":{"_cell_guid":"604323fb-33c7-43e7-d24c-9acd37ebe8db"},"source":"#### Building Final df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f01c716-a0b4-3478-7c17-c1528630435d"},"outputs":[],"source":"from collections import defaultdict\n\nstats = defaultdict(dict)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d02c295f-cf39-a9e0-440d-f7b0b4a3f73a"},"outputs":[],"source":"import re\n\nRE_URL = re.compile(r'https?://')\n\nfor index, body in enumerate(qsents):\n    \n    stats[index]['question'] = 0\n    stats[index]['exclam'] = 0\n    stats[index]['url'] = 0\n    for sent in body:\n        ss = sent.strip()\n        if ss:\n            if ss.endswith('?'):\n                stats[index]['question'] += 1\n            if ss.endswith('!'):\n                stats[index]['exclam'] += 1\n            stats[index]['url'] += len(RE_URL.findall(sent))\n    stats[index]['finalthanks'] = 1 if body and 'thank' in body[-1].lower() else 0\n    stats[index]['textLen'] = len(body)\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c87302ca-6667-e7ae-deb3-f381f979d71a"},"outputs":[],"source":"df = pd.DataFrame.from_dict(stats,orient='index')\n\n# this part should be done in the first place, I realize it only till the last phase\ndf['codeLen'] = [len(list) if list else 0 \n                   for list in questions.loc[(questions_final.Score>=5) | (questions_final.Score<0),\\\n                                             \"Body\"].str.findall(r'(<code>[^<]+</code>)')]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77de879d-2411-f68f-9d48-5055f3c4815c"},"outputs":[],"source":"tagNum = tags.groupby('Id')['Tag'].count()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97c37e40-05a7-869e-5268-c7bf68b51414"},"outputs":[],"source":"def getSumTF(wordlist, qfile,cfile):\n    if not wordlist or not qfile:\n        return 0\n    if cfile is np.nan:\n        cfile = []\n    if type(wordlist) is str:\n        wordlist = wordlist.split(' ')\n        wordset = set(wordlist)\n    else:\n        wordset = set([word for word,_ in wordlist])\n    freq = 0\n    freqdict = nltk.FreqDist(qfile)\n    for word in wordset:\n        freq+=freqdict[word]\n        if cfile:\n            if word in cfile:\n                freq+=5\n    return freq/len(wordlist)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9563c5e4-9647-05ca-afdb-fcc809ca1562"},"outputs":[],"source":"tagNum.columns = ['Id', 'tagNum']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14fbaaf1-8779-4699-d0fd-d47b91498d8d"},"outputs":[],"source":"clist = list(dfFinal.CodeBody)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6b30769-62b5-af94-acd8-9d1b5567b374"},"outputs":[],"source":"tagsByquestion.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"317158cf-6897-995b-6ae5-fcb11e426e5d"},"outputs":[],"source":"dfFinal = dfFinal.merge(tagsByquestion,on='Id',how='left')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccf87e3a-cde4-2a5d-5411-65348744f11e"},"outputs":[],"source":"titleTFSum = []\n\nfor index, words in enumerate(txtpos):\n    titleTFSum.append(getSumTF(words, qbodys[index], clist[index]))\n    \ntagTFSum = []\n\nfor index, words in enumerate(list(dfFinal.Tag)):\n    tagTFSum.append(getSumTF(words, qbodys[index], clist[index]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a626bdb2-8092-630c-8109-d29d7365dfb5"},"outputs":[],"source":"df['titleTFSum'] , df['tagTFSum'] = titleTFSum, tagTFSum\n\ndf['Id'] = list(dfFinal.Id)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d55b20f-e5c6-044c-3a71-a5377583c9dc"},"outputs":[],"source":"tagNum = pd.DataFrame(tagNum).reset_index(level=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7eaf4904-423f-e46a-ede2-9b8668764b70"},"outputs":[],"source":"dfFinal = dfFinal.merge(df, on ='Id',how='left').merge(tagNum, on='Id',how='left')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a31f10e-d2df-c532-1103-fddbd8440b6d"},"outputs":[],"source":"dfFinal.loc[dfFinal.Score<0,'label'] = 'Bad'\n\ndfFinal.loc[dfFinal.Score>=5,'label'] = 'Good'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73a49747-7a41-52d2-c3a1-1c0ea92979db"},"outputs":[],"source":"dfFinal.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d195e972-57d2-af52-c84e-48c7b8988ae1"},"outputs":[],"source":"# this function was required for GaussianNB but not required for SGD\nclass DenseTransformer(BaseEstimator,TransformerMixin):\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)\n\n    def fit(self, X, y=None, **fit_params):\n        return self"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87db4c0b-18b3-2cb0-a3b1-52cc6326a28c"},"outputs":[],"source":"columns = ['question', 'exclam', 'finalthanks', 'textLen', 'url', 'codeLen',\n       'titleTFSum', 'tagTFSum', 'tagNum']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f944e3dd-7584-a0ec-9625-809e06b80364"},"outputs":[],"source":"class GetItemTransformer(BaseEstimator,TransformerMixin):\n    def __init__(self, field):\n        self.field = field\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X):\n        if len(self.field)==1:\n            if self.field[0] =='QuestionBody':\n                return list(X.QuestionBody)\n            else:\n                return list(X.Title)\n        return X.loc[:,self.field]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80bd827f-5c7f-613d-d67c-ea7524356403"},"outputs":[],"source":"dftest = dfFinal.drop(['OwnerUserId','CreationDate','Score','CodeBody','Tag'],axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28362b79-6fee-c87f-d644-ff44357508d3"},"outputs":[],"source":"Y = dftest.label\nX = dftest.drop(['label','Id'], axis=1)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.30)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6705288f-3ebe-b963-52ba-b31fa85adf49"},"outputs":[],"source":"'''This is our baseline of model accuracy. We need to beat this accuracy \nwhile trying to maximize our recall on bad labels.'''\n\ndef dummyGuess(x):\n    return pd.Series(['Good'] * len(x))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64102a85-c745-c0ea-c9da-211a0ff0afb8"},"outputs":[],"source":"accuracy_score(Y_test, dummyGuess(Y_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f42cc56-6005-7bb5-6c61-0586c1964d07"},"outputs":[],"source":"'''Using nltk tokenizer will significantly slow down the fitting time but will slightly increase the accuracy.\nI trained the model with default tokenizer and then switched to nltk tokenizer later on'''\n\npipeline = Pipeline([\n    ('features', FeatureUnion(\n        transformer_list = [\n        ('stats', Pipeline([\n                ('extract', GetItemTransformer(columns)),\n                ('substractK', SelectKBest(k=5))]))\n        ,\n        ('title',Pipeline([\n            ('extract', GetItemTransformer(['Title'])),\n            ('count', TfidfVectorizer(stop_words=stopset,min_df=0.03,max_df=0.7,tokenizer=nltk.word_tokenize)),\n            #('Sum', SumTransformer())\n        ])),\n        ('question', Pipeline([\n            ('extract', GetItemTransformer(['QuestionBody'])),\n            ('tfidf', CountVectorizer(stop_words=stopset,min_df=0.01,max_df=0.8,tokenizer=nltk.word_tokenize)),\n        ])),\n    ],\n    # the weight was trained seperately, \n    # I also controlled the weight to be fairly equal assignned.\n    transformer_weights={\n            'stats': 0.4,\n            'title':0.2,\n            'Question': 0.4\n        }\n            )),\n    ('scaler',Normalizer()),    \n    ('estimators', SGDClassifier(alpha=0.001,loss='modified_huber',penalty='l2')),\n])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b12938c5-8b72-1927-30da-0f6924b74a25"},"outputs":[],"source":"pipeline.fit(X_train, Y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67e9b811-d871-f1a8-e43b-aefe2fe9a9ea"},"outputs":[],"source":"y = pipeline.predict(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7abeb2ac-ff51-085d-8dc1-51183c97037d"},"outputs":[],"source":"accuracy_score(Y_test, y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"318fd314-e475-4660-601f-f40dfbbaf600"},"outputs":[],"source":"print(classification_report(Y_test, y))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7bd6899-3ee9-dc5c-8771-40900679f3bc"},"outputs":[],"source":"test = pipeline.predict_proba(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7ff449c-4fe8-0f58-f45e-d185b267e70d"},"outputs":[],"source":"predict = ['Bad' if pair[0]>=0.35 else 'Good' for pair in test]\n\nprint(classification_report(Y_test,predict))\n\nprint(accuracy_score(Y_test,predict))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7416358d-d432-cc4d-4717-bf9991c22650"},"outputs":[],"source":"from sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n        #print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, \"{0:.0f}%\".format(cm[i, j]*100),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5a28f17-3029-ba56-2c0e-851b7b85bf20"},"outputs":[],"source":"cnf_matrix = confusion_matrix(Y_test,predict)\n                              \nnp.set_printoptions(precision=2)\n\nclass_names =['Bad','Good']\n\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e9e5ab8-cd7f-3662-de49-92027f32d78c"},"outputs":[],"source":"plt.figure()\n\nfor model in models:\n    pipeline.steps[2]= ('estimator',models[model])\n    pipeline.fit(X_train, Y_train)\n    test = pipeline.predict_proba(X_test)\n    predict = [pair[0] for pair in test]\n    # Get Receiver Operating Characteristic (ROC) and Area Under Curve (AUC)\n    fpr, tpr, _ = roc_curve(Y_test, predict,pos_label='Bad')\n    auc_ = auc(fpr, tpr)\n    # Plot it\n    plt.plot(fpr, tpr, label=model)\nplt.legend()\nplt.plot([[0,0],[1,1]])\nplt.title('AUC ')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8e7f693-1967-da82-eba9-7a61fd1d72d9"},"source":"#### I tried on other models but they are not performing well as SGD. Below is a comparative ROC curve plot. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6baf6623-7221-022e-8b8a-008e1d490a0c"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\n\nmodels = {'RF': RandomForestClassifier(max_depth=20, n_estimators=20),\n            'SGD': SGDClassifier(alpha=0.001,loss='modified_huber',penalty='l2'),\n            'NB':MultinomialNB(),\n            'LR':LogisticRegression(C=0.01)\n                   }"},{"cell_type":"markdown","metadata":{"_cell_guid":"893cf9f5-c1c7-cf53-00e9-eb875f7a5da8"},"source":"##Below was my work trying to put the model back to the whole complete dataset. I didn't run through on Kaggle since it takes much longer time and kernel constantly dies.   I pasted the classification report below:\n\n             precision    recall  f1-score   support\n\n        Bad       0.29      0.79      0.42     10935\n       Good       0.94      0.61      0.74     54712\n\navg / total       0.83      0.64      0.68     65647\n\nAccuracy:  0.636617057901\n\nIf you are interested in the final outcome, please refer to the GitHub link I post at the beginning. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27676a3b-1b3f-4259-0608-253ca27561dd"},"outputs":[],"source":"bodys = list(questions_final.QuestionBody)\n#break into sentences\nsents = [nltk.sent_tokenize(text) for text in bodys]\n# Tokenize the question body\nbodys = [nltk.word_tokenize(text) for text in bodys]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"684ed48b-7023-9c08-6710-0808051dd9bc"},"outputs":[],"source":"titles = list(questions_final.Title)\n# Tokenize the titles\ntitles = [nltk.word_tokenize(text) for text in titles]\n# pos tag the tokens\ntitlepos = [nltk.pos_tag(texts) for texts in titles]\n# for titles we only care about verbs and nouns\ntitlepos = [[w for w in s if (w[1][0] == 'N' or w[1][0] == 'V') and w[0].lower() not in stopset] \n                  for s in titlepos]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"738e8fbf-e0d2-54ab-3902-594774d6b0f0"},"outputs":[],"source":"completeDf = pd.DataFrame.from_dict(dfstats,orient='index')\n\n# this part should be done in the first place, I realize it only till the last phase\ncompleteDf['codeLen'] = [len(list) if list else 0 \n                   for list in questions.loc[:,\"Body\"].str.findall(r'(<code>[^<]+</code>)')]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52e95f1b-62db-d9a4-f533-f67dca131b47"},"outputs":[],"source":"qFinal.loc[qFinal.Score<0,'label'] = 'Bad'\n\nqFinal.loc[qFinal.Score>=0,'label'] = 'Good'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a081974-ac43-ea6c-e629-fdaaac3bcfd8"},"outputs":[],"source":"Y = qTest.label\nX = qTest.drop(['label','Id'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47e8e68e-e7d9-5b01-870e-59114e1619c7"},"outputs":[],"source":"class SumTransformer(BaseEstimator,TransformerMixin):\n\n    def transform(self, X, y=None, **fit_params):\n        X = X.todense()\n        return X.sum(axis=1)\n\n    def fit_transform(self, X, y=None, **fit_params):\n        #self.fit(X, y, **fit_params)\n        return self.transform(X)\n\n    def fit(self, X, y=None, **fit_params):\n        return self"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6562fc01-512d-efdf-aeb9-f9bc6d2342d0"},"outputs":[],"source":"from sklearn.metrics import precision_score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe3ad7dc-2262-dcf7-b6b4-3434bfefce3c"},"outputs":[],"source":"pipeline2 = Pipeline([\n    ('features', FeatureUnion(\n        transformer_list = [\n        ('stats', Pipeline([\n                ('extract', GetItemTransformer(columns)),\n                ('substractK', SelectKBest(k=5))]))\n        ,\n        ('title',Pipeline([\n            ('extract', GetItemTransformer(['Title'])),\n            ('count', TfidfVectorizer(stop_words=stopset,min_df=0.03,max_df=0.8,tokenizer=nltk.word_tokenize)),\n            #('Sum', SumTransformer())\n        ])),\n        ('question', Pipeline([\n            ('extract', GetItemTransformer(['QuestionBody'])),\n            ('tfidf', CountVectorizer(stop_words=stopset,min_df=0.01,max_df=0.8)),\n            # I didn't do this with previous dataset\n            ('substractK', SelectKBest(k=500))\n        ])),\n    ],\n    # the weight was trained seperately, \n    # I also controlled the weight to be fairly equal assignned.\n    transformer_weights={\n            'stats': 0.4,\n            'title':0.2,\n            'Question': 0.4\n        }\n            )),\n    ('scaler',Normalizer()),\n    ('estimators', SGDClassifier(alpha=0.001,loss='modified_huber',penalty='l2',class_weight={'Bad':15, 'Good':1})),\n])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00ecba6a-c637-e5b4-78e9-36a9279d8cc2"},"outputs":[],"source":"# the model will easily include all the Bad questions but the precision becomes very low\nrecall_scorer = make_scorer(precision_score, pos_label=\"Bad\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5959d1d4-ffbb-5875-3b1e-b7f38021c946"},"outputs":[],"source":"#grid_search2 = RandomizedSearchCV(pipeline2, param_grid,scoring=recall_scorer, verbose=5,n_jobs=5,n_iter=4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ff82f90-50fb-116e-ce08-dab749ee4f2a"},"outputs":[],"source":"#grid_search2.fit(X_train,Y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1be98a8-a5f6-6e64-e427-be1e7a92e9bd"},"outputs":[],"source":"#grid_search2.best_score_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"080bfe68-9979-4ea6-1f63-137cb76b04ba"},"outputs":[],"source":"#Y_pred = pipeline2.predict_proba(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b133a97-d27b-7e1f-fe3e-c163cb3651e0"},"outputs":[],"source":"#grid_search2.best_params_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e87b1b0-4a1d-ec6d-acb7-a7586d5070c3"},"outputs":[],"source":"#predict = [pair[0] for pair in Y_pred]"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}