{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nBoardGameGeek is a popular site where different types of board games like Settlers of Catan, Through the Ages: A Story of Civilization etc are discussed and reviewed. There are seven major data types. Each of these types has subtypes which differ by the various domains of this collection of sites. But the ID numbers are unique for any given type. For example [boardgames] and [rpgitems] are subtypes of [thing]; the ID numbers for [things] are unique and shared between its subtypes, such that any one [thing] could have the [boardgame] or [rpgitem] subtype, but different [boardgames] and [rpg items] will never have the same ID number.\n"},{"metadata":{},"cell_type":"markdown","source":"# **Objective**\nThe objective of this notebook to analyze the given review data and predict the rating. Let's use our analysis skills to predict the ratings. "},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n The dataset contains data on 80000 board games. The dataset contains several data points about each board game. Here’s a list of the interesting ones:\n\n* name – name of the board game.\n* playingtime – the playing time (given by the manufacturer).\n* minplaytime – the minimum playing time (given by the manufacturer).\n* maxplaytime – the maximum playing time (given by the manufacturer).\n* minage – the minimum recommended age to play.\n* users_rated – the number of users who rated the game.\n* average_rating – the average rating given to the game by users. (0-10)\n* total_weights – Number of weights given by users. Weight is a subjective measure that is made up by BoardGameGeek. It’s how “deep” or involved a game is. Here’s a full explanation.\n* average_weight – the average of all the subjective weights (0-5)."},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Algorithms Used For Predicting The Rating\nWe have used the following 3 Machine Learning Algorithms in this project:\n1.  k-means clustering\n2.  Linear Regression\n3. RandomForest Regression"},{"metadata":{},"cell_type":"markdown","source":"# k-means clustering\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\nTypically, unsupervised algorithms make inferences from datasets using only input vectors without referring to known, or labelled, outcomes.\nThe objective of K-means is simple: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.”\nA cluster refers to a collection of data points aggregated together because of certain similarities.\nYou’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\nEvery data point is allocated to each of the clusters through reducing the in-cluster sum of squares.\nIn other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\nThe ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.\nHow the K-means algorithm works\nTo process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids\nIt halts creating and optimizing clusters when either:\n* The centroids have stabilized — there is no change in their values because the clustering has been successful.\n* The defined number of iterations has been achieved."},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\nImage(\"../input/diagram/pic.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.\nIn the figure above, X (input) is the work experience and Y (output) is the salary of a person. The regression line is the best fit line for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\nImage(\"../input/downloadd/pic1.png\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While training the model we are given :\nx: input training data (univariate – one input variable(parameter))\ny: labels to data (supervised learning)\n\nWhen training the model – it fits the best line to predict the value of y for a given value of x. The model gets the best regression fit line by finding the best θ1 and θ2 values.\nθ1: intercept\nθ2: coefficient of x\n\nOnce we find the best θ1 and θ2 values, we get the best fit line. So when we are finally using our model for prediction, it will predict the value of y for the input value of x.\n\nHow to update θ1 and θ2 values to get the best fit line ?"},{"metadata":{},"cell_type":"markdown","source":"**Cost Function (J):**\nBy achieving the best-fit regression line, the model aims to predict y value such that the error difference between predicted value and true value is minimum. So, it is very important to update the θ1 and θ2 values, to reach the best value that minimize the error between predicted y value (pred) and true y value (y).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\nImage(\"../input/costfunction/Annotation 2020-05-11 091201.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cost function(J) of Linear Regression is the Root Mean Squared Error (RMSE) between predicted y value (pred) and true y value (y)."},{"metadata":{},"cell_type":"markdown","source":"**Gradient Descent:** \nTo update θ1 and θ2 values in order to reduce Cost function (minimizing RMSE value) and achieving the best fit line the model uses Gradient Descent. The idea is to start with random θ1 and θ2 values and then iteratively updating the values, reaching minimum cost."},{"metadata":{},"cell_type":"markdown","source":"# RandomForest Regression"},{"metadata":{},"cell_type":"markdown","source":"Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression.\nRandom forest is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\nIt operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\nA random forest is a meta-estimator (i.e. it combines the result of multiple predictions) which aggregates many decision trees, with some helpful modifications:\n1. The number of features that can be split on at each node is limited to some percentage of the total (which is known as the hyperparameter). This ensures that the ensemble model does not rely too heavily on any individual feature, and makes fair use of all potentially predictive features.\n2. Each tree draws a random sample from the original data set when generating its splits, adding a further element of randomness that prevents overfitting.\nThe above modifications help prevent the trees from being too highly correlated.\n\nFor Example, See these nine decision tree classifiers below :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\nImage(\"../input/chekkkkkkkkk/Annotation 2020-05-11 091744.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These decision tree classifiers can be aggregated into a random forest ensemble which combines their input. Think of the horizontal and vertical axes of the above decision tree outputs as features x1 and x2. At certain values of each feature, the decision tree outputs a classification of “blue”, “green”, “red”, etc.\n\nThese above results are aggregated, through model votes or averaging, into a single ensemble model that ends up outperforming any individual decision tree’s output.\n\nThe aggregated result for the nine decision tree classifiers is shown below :\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/\nImage(\"../input/cheeckkkkkkkk/forestttt.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature and Advantages of Random Forest :\n\nIt is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.\n\n1. It runs efficiently on large databases.\n2. It can handle thousands of input variables without variable deletion.\n3. It gives estimates of what variables that are important in the classification.\n4. It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n5. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n\nDisadvantages of Random Forest :\n\n1. Random forests have been observed to overfit for some datasets with noisy classification/regression tasks.\n2. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Therefore, the variable importance scores from random forest are not reliable for this type of data."},{"metadata":{},"cell_type":"markdown","source":"# Importing Board Game Geek Review Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing datasets and displaying the rows "},{"metadata":{"trusted":true},"cell_type":"code","source":"rank = pd.read_csv('../input/boardgamegeek-reviews/2019-05-02.csv')\nrank.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review= pd.read_csv('../input/boardgamegeek-reviews/bgg-13m-reviews.csv', index_col=0)\nreview.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rank.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rank['Average'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rank['Bayes average'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing the rows that contain NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"review.replace('-', np.nan, inplace = True)\nreview = review.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review['rating'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail= pd.read_csv('/kaggle/input/boardgamegeek-reviews/games_detailed_info.csv', index_col=0)\ndetail.iloc[:, 10:20].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail.iloc[:, :16].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail['Board Game Rank'] = detail['Board Game Rank'].replace('Not Ranked', np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail['average'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail['bayesaverage'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joining Repeated Columns in Rank & Detail datasets\nWe will drop ranking data's repeated columns and join the rest (BGG URL and Name) with game detail dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"rank_sub= rank[['ID', 'Year', 'Rank', 'Average', 'Bayes average', 'Users rated', 'Thumbnail']]\nrank_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detail_sub= detail[['id', 'yearpublished', 'Board Game Rank', 'average', 'bayesaverage', 'usersrated', 'thumbnail']]\ndetail_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joined_df = rank_sub.merge(detail_sub, left_on='ID', right_on='id', how='left')\njoined_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot to show Number of Board Games Published "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\ndetail['yearpublished'].value_counts().sort_index().plot()\nplt.xlabel('Year')\nplt.ylabel('Board Games Published')\nplt.title('Number of Board Games Published over Time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 50 Most Rated Board Games"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 7))\nreview['name'].value_counts()[:50].plot(kind='bar')\nplt.xlabel('Board Game Name')\nplt.ylabel('Rating Count')\nplt.title('Top 50 Most Rated Board Games')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = detail.iloc[:, 20:].corr()\ncorr = corr.dropna(how='all', axis=1).dropna(how='all', axis=0).round(2)\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nplt.subplots(figsize=(20,20))\nsns.heatmap(corr, cmap='PiYG', annot=True, linewidths=.5)\nplt.title('Correlation plot without genre variables')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = detail[detail.columns.difference(['Accessory Rank', \"Amiga Rank\", \"Arcade Rank\", \"Atari ST Rank\",\"Commodore 64 Rank\",\n                                               \"RPG Item Rank\", \"Video Game Rank\", \"median\", \"thumbnail\", \"id\", \n                                               \"image\"])] \\\n       .corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot=True, cmap='PiYG')\nplt.title('Correlation plot with genre variables that has >1 board game (excluding median, thumbnail, id and image)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"games = pd.read_csv('../input/boardgames/games.csv')\ngames.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(games.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Our Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import matplotlib\nimport matplotlib.pyplot as plt\n# Make a histogram of all the ratings in the average_rating column.\nplt.hist(games[\"average_rating\"])\n# Show the plot.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Zero Ratings Review"},{"metadata":{"trusted":true},"cell_type":"code","source":"games[games[\"average_rating\"] == 0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first row of all the games with zero scores.\n# The .iloc method on dataframes allows us to index by position.\nprint(games[games[\"average_rating\"] == 0].iloc[0])\n# Print the first row of all the games with scores greater than 0.\nprint(games[games[\"average_rating\"] > 0].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Games without Review"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove any rows without user reviews.\ngames = games[games[\"users_rated\"] > 0]\n# Remove any rows with missing values.\ngames = games.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sd = games[\"average_rating\"].std()\nmean = games[\"average_rating\"].mean()\n\nprint(sd,mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering Games"},{"metadata":{},"cell_type":"markdown","source":"We’ll use a particular type of clustering called k-means clustering. Scikit-learn has an excellent implementation of k-means clustering that we can use. Scikit-learn is the primary machine learning library in Python, and contains implementations of most common algorithms, including random forests, support vector machines, and logistic regression. Scikit-learn has a consistent API for accessing these algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the kmeans clustering model.\nfrom sklearn.cluster import KMeans\nfrom pandas import DataFrame\n%matplotlib inline\n\n# Initialize the model with 2 parameters -- number of clusters and random state.\nkmeans_model = KMeans(n_clusters=5, random_state=1)\n# Get only the numeric columns from games.\ngood_columns = games._get_numeric_data()\ngames_numeric = games.drop(['name','type','id'],axis=1)\ngames_mean = games_numeric.apply(np.mean, axis=1)\ngames_std =games_numeric.apply(np.std, axis=1)\n# Fit the model using the good columns.\nkmeans_model.fit(good_columns)\n# Get the cluster assignments.\nlabels = kmeans_model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations=games_numeric.corr()\n\ncorrelations[\"average_rating\"] #Shows us how each column in board game dataset is correlated with average_rating","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Clusters"},{"metadata":{},"cell_type":"markdown","source":"Now that we have cluster labels, let’s plot the clusters. One sticking point is that our data has many columns – it’s outside of the realm of human understanding and physics to be able to visualize things in more than 3 dimensions. So we’ll have to reduce the dimensionality of our data, without losing too much information. One way to do this is a technique called principal component analysis, or PCA. PCA takes multiple columns, and turns them into fewer columns while trying to preserve the unique information in each column. To simplify, say we have two columns, total_owners, and total_traders. There is some correlation between these two columns, and some overlapping information. PCA will compress this information into one column with new numbers while trying not to lose any information.\n\nWe’ll try to turn our board game data into two dimensions, or columns, so we can easily plot it out.\n\nWe first initialize a PCA model from Scikit-learn. PCA isn’t a machine learning technique, but Scikit-learn also contains other models that are useful for performing machine learning. Dimensionality reduction techniques like PCA are widely used when preprocessing data for machine learning algorithms.\n\nWe then turn our data into 2 columns, and plot the columns. When we plot the columns, we shade them according to their cluster assignment.\n\nThe plot shows us that there are 5 distinct clusters. We could dive more into which games are in each cluster to learn more about what factors cause games to be clustered."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the PCA model.\nfrom sklearn.decomposition import PCA\n# Create a PCA model.\npca_2 = PCA(2)\n# Fit the PCA model on the numeric columns from earlier.\nplot_columns = pca_2.fit_transform(good_columns)\n# Make a scatter plot of each game, shaded according to cluster assignment.\nplt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=labels)\n# Show the plot.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split into test and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import a convenience function to split the sets.\nfrom sklearn.model_selection import train_test_split\n# Generate the training set.  Set random_state to be able to replicate results.\ntrain = games.sample(frac=0.8, random_state=1)\n# Select anything not in the training set and put it in the testing set.\ntest = games.loc[~games.index.isin(train.index)]\n# Print the shapes of both sets.\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Picking Predicting Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the columns from the dataframe.\ncolumns = games.columns.tolist()\n# Filter the columns to remove ones we don't want.\ncolumns = [c for c in columns if c not in [\"bayes_average_rating\", \"average_rating\", \"type\", \"name\"]]\n# Store the variable we'll be predicting on.\ntarget = \"average_rating\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LinearRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the linearregression model.\nfrom sklearn.linear_model import LinearRegression\n# Initialize the model class.\nmodel = LinearRegression()\n# Fit the model to the training data.\nmodel.fit(train[columns], train[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting error"},{"metadata":{},"cell_type":"markdown","source":"After we train the model, we can make predictions on new data with it. This new data has to be in the exact same format as the training data, or the model won’t make accurate predictions. Our testing set is identical to the training set (except the rows contain different board games). We select the same subset of columns from the test set, and then make predictions on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error \n\ngames_numeric2= games_numeric.drop(['average_rating','bayes_average_rating'],axis=1)\n\n\nlr = LinearRegression()\nlr.fit(games_numeric2, games[\"average_rating\"])\npredictions = lr.predict(games_numeric2)\n\n\nmse = mean_squared_error(games[\"average_rating\"],predictions)\nprint(mse)\nrmse = mse ** (1/2)\nprint(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(games[\"average_rating\"])\nsns.distplot(predictions)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the random forest model.\nfrom sklearn.ensemble import RandomForestRegressor\n# Initialize the model with some parameters.\nmodel = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=1)\n# Fit the model to the data.\nmodel.fit(train[columns], train[target])\n# Make predictions.\npredictions = model.predict(test[columns])\n# Compute the error.\nmean_squared_error(predictions, test[target])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Challenges and Conclusion\n\nFrom the above analysis we can see that the error rate of random forest is less when compared to linear regression.The random forest algorithm can find nonlinearities in data that a linear regression wouldn’t be able to pick up on. A linear regression algorithm wouldn’t be able to pick up on this because there isn’t a linear relationship between the predictor and the target. Predictions made with a random forest usually have less error than predictions made by a linear regression.\n\nThe challenges I faced for this project were selecting the Machine algorithms for handling the data, choosing the columun to find the correlation and also handling the huge sample datasets. I have choosen \"average_rating\" for correlation because our final project was based on predicting rating. However, we can try predicting a different column, such as average_weight. Regarding handling of datasets, I was able to divide the dataset as test and train, which could be easily handled by Kaggle Server.\n\nFinally, we were able to find which machine algorithms were best suited for this dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"# References\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\nhttps://github.com/GuneetKohli/Predicting-Board-Game-Reviews-Using-KMeans-Clustering-Linear-Regression/blob/master/BoardGameReviews.ipynb\nhttps://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nhttps://www.kaggle.com/mrpantherson/board-game-data\n"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}