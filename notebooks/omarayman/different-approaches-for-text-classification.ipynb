{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What we have done so far?\n* We did some data exploration and semantic analysis with a hypothesis and tested it using two different approaches\n* We tested different vectorization techniques on different classifiers\n* We Justified our models decisions using LIME\n* We used Glove embeddings with fully connected nn, LSTM, Bidirectional LSTM and GRU\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  ## Data Exploration \n  \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nans percentage \n(len(df)-df.count())/len(df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like Nans in columns ['airline_sentiment_gold','negativereason_gold','tweet_coord'] are having very high percentage so let's remove them "},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['airline_sentiment_gold']\ndel df['negativereason_gold']\ndel df['tweet_coord']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's see what mood that dominates the passengers the most and what next and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"mood_count=df['airline_sentiment'].value_counts()\nplt.figure(figsize=(10,5))\nsns.barplot(mood_count.index, mood_count.values, alpha=0.8)\nplt.title('Count of Moods')\nplt.ylabel('Mood Count', fontsize=12)\nplt.xlabel('Mood', fontsize=12)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in (mood_count.values):\n    print(j/len(df)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"percentage of negativity is 62%\npercentage of neutral is 21%\npercentage of positive is 16%\n"},{"metadata":{},"cell_type":"markdown","source":"It seems that negative mood dominates the passengers now let's dive deeper and see what causes the negativity the most"},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_reasons = df['negativereason'][df['airline_sentiment']=='negative'].value_counts()\nneg_reasons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nsns.barplot(neg_reasons.index, neg_reasons.values, alpha=0.8)\nplt.title('Negative responses reasons')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Reason', fontsize=12)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['user_timezone'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_count=df['user_timezone'][df['airline_sentiment']=='negative'].value_counts()[:10]\nneg_timezone_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can assume that the 1st three time zones are in The US to be able to plot them on a map to have a better prespective about where on the map the actual negative feedback is coming"},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df = pd.DataFrame({'timezone':neg_timezone_count.index,'negative_count':neg_timezone_count.values})\nneg_timezone_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can assume that some close areas are belong to the same countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"for zone in neg_timezone_df['timezone']:\n    if zone in ['Eastern Time (US & Canada)','Central Time (US & Canada)','Pacific Time (US & Canada)']:\n        neg_timezone_df['timezone'][neg_timezone_df['timezone']==zone] = 'USA'\n    if zone in ['Atlantic Time (Canada)','Mountain Time (US & Canada)']:\n        neg_timezone_df['timezone'][neg_timezone_df['timezone']==zone] = 'Canada'\nneg_timezone_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df = neg_timezone_df.groupby('timezone',as_index=True,sort=False).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df = neg_timezone_df.head(10)\nlatitude = ['37.0902','0.1807','56.1304','34.0489','51.5074','64.2008','19.8968']\nlongtuide = ['-95.7129','-78.4678','-106.3468','-111.0937','0.1278','-149.4937','-155.5828']\nneg_timezone_df['latitude'] = latitude\nneg_timezone_df['longtuide'] = longtuide","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df['color']=neg_timezone_df['negative_count'].apply(lambda negative_count:\"Black\" if negative_count>=400 else\n                                         \"green\" if negative_count>=300 and negative_count<400 else\n                                         \"Orange\" if negative_count>=200 and negative_count<300 else\n                                         \"darkblue\" if negative_count>=150 and negative_count<200 else\n                                         \"red\" if negative_count>=100 and negative_count<150 else\n                                         \"lightblue\" if negative_count>=75 and negative_count<100 else\n                                         \"brown\" if negative_count>=50 and negative_count<75 else\n                                         \"grey\")\nneg_timezone_df['size']=neg_timezone_df['negative_count'].apply(lambda negative_count:20 if negative_count>=400 else\n                                         15 if negative_count>=300 and negative_count<400 else\n                                         12 if negative_count>=200 and negative_count<300 else\n                                         11 if negative_count>=150 and negative_count<200 else\n                                         10 if negative_count>=100 and negative_count<150 else\n                                         7 if negative_count>=75 and negative_count<100 else\n                                         5 if negative_count>=50 and negative_count<75 else\n                                         3)\n\nneg_timezone_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_timezone_df['timezone'] = neg_timezone_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nm=folium.Map([56.1304,106.3468],zoom_start=1)\n#location=location[0:2000]\nfor lat,lon,area,color,count,size in zip(neg_timezone_df['latitude'],neg_timezone_df['longtuide'],neg_timezone_df['timezone'],neg_timezone_df['color'],neg_timezone_df['negative_count'],neg_timezone_df['size']):\n     folium.CircleMarker([lat, lon],\n                            popup=area,\n                            radius=size,\n                            color='b',\n                            fill=True,\n                            fill_opacity=0.7,\n                            fill_color=color,\n                           ).add_to(m)\nm\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's clear that most of the negativity responses are coming from USA,Canada and Quito, although there's an error in latitude for USA and Canada but you got the idea of what I am after, now let's explore the moods for different airlines"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,20)})\nsns.catplot(x='airline_sentiment',kind='count',data=df,orient=\"h\",hue='airline')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see most negative responses come from the United Airline, and most positive responses comes from Southwest airline"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_length'] =  list(map(lambda x:len(x),df['text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_0 = df.loc[df['airline_sentiment'] == 'neutral']\ntarget_1 = df.loc[df['airline_sentiment'] == 'positive']\ntarget_2 = df.loc[df['airline_sentiment'] == 'negative']\n\nsns.distplot(target_0[['text_length']], hist=False, rug=False,color='red',label='Neutral')\nsns.distplot(target_1[['text_length']], hist=False, rug=True,color = 'yellow',label='positive')\nsns.distplot(target_2[['text_length']], hist=False, rug=True,color='black',label='negative')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it's likely when the tweets is too long to be negative"},{"metadata":{},"cell_type":"markdown","source":"## Let's now demonstrates some hypothesis tests on the data we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Semantic Analysis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some initial features in text\nqmarks = np.mean(df['text'].apply(lambda x: '?' in x))\nexclamation = np.mean(df['text'].apply(lambda x: '!' in x))\nat = np.mean(df['text'].apply(lambda x: '@' in x))\nfullstop = np.mean(df['text'].apply(lambda x: '.' in x))\ncapital_first = np.mean(df['text'].apply(lambda x: x[0].isupper()))\ncapitals = np.mean(df['text'].apply(lambda x: max([y.isupper() for y in x])))\nnumbers = np.mean(df['text'].apply(lambda x: max([y.isdigit() for y in x])))\nhashtags = np.mean(df['text'].apply(lambda x: '#' in x))\n\nprint('Tweets with question marks: {:.2f}%'.format(qmarks * 100))\nprint('Tweets with question hashtags: {:.2f}%'.format(hashtags * 100))\nprint('Tweets with exclamation marks: {:.2f}%'.format(exclamation * 100))\nprint('Tweets with full stops: {:.2f}%'.format(fullstop * 100))\nprint('Tweets with capitalised first letters: {:.2f}%'.format(capital_first * 100))\nprint('Tweets with capital letters: {:.2f}%'.format(capitals * 100))\nprint('Tweets with @: {:.2f}%'.format(at * 100))\nprint('Tweets with numbers: {:.2f}%'.format(numbers * 100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First hypothesis is tweets with question marks should be angrier "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['has_question'] = df['text'].apply(lambda x: '?' in x)\ndf_has_question = df[df['has_question']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_has_question.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_has_question.airline_sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in (df_has_question.airline_sentiment.value_counts().values):\n    print(j/len(df_has_question)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='airline_sentiment',kind='count',data=df_has_question,orient=\"h\",hue='airline_sentiment')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our benchmark on the unfiltered dataset was.\n\n> Negative = 62.69125683060109 %\n\n> Neutral = 21.168032786885245 %\n\n> Postive = 16.140710382513664 %\n\n\n\n\n\nAs you can see the percentage of positivity for the tweet decreased tremendously and the neutral percentage increased, So we can conclude that adding '!' to the tweets increased the probability of it being neutral or negative"},{"metadata":{},"cell_type":"markdown","source":"### Let's validate this assumption"},{"metadata":{},"cell_type":"markdown","source":"### First Approach"},{"metadata":{},"cell_type":"markdown","source":"Fit logestic regression model having X as has_question feature and Y(label) as airline_sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasquestion = df[['has_question','airline_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasquestion.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasquestion['has_question'] = [1 if df_hasquestion['has_question'][x] == True else 0 for x in range(len(df_hasquestion['has_question']))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasquestion['airline_sentiment'] = [1 if df_hasquestion['airline_sentiment'][x] == 'positive' else 0 for x in range(len(df_hasquestion['airline_sentiment']))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df_hasquestion['has_question'])\ny = np.array(df_hasquestion['airline_sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=x.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver='liblinear',random_state=0).fit(x,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nparams = np.append(clf.intercept_,clf.coef_)\npredictions = clf.predict(x)\nnewX = pd.DataFrame({\"Constant\":np.ones(len(x))}).join(pd.DataFrame(x))\nMSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n\nvar_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\nsd_b = np.sqrt(var_b)\nts_b = params/ sd_b\n\np_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\nsd_b = np.round(sd_b,3)\nts_b = np.round(ts_b,3)\np_values = np.round(p_values,)\nparams = np.round(params,4)\n\nmyDF3 = pd.DataFrame()\nmyDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"Probabilites\"] = [params,sd_b,ts_b,p_values]\nprint(myDF3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as you can see the probability for a given beta is 0.000 so their is a very small room for error so the we can now accept our hypothesis \n"},{"metadata":{},"cell_type":"markdown","source":"### Second approach"},{"metadata":{},"cell_type":"markdown","source":"The second approach we will be taking here is that we compute an unpaired T-test between samples from data that do have question mark and samples that do not have a question mark in the text field and see the p value for the predictor 'has_question'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasquestion = df[df['has_question']==True]\ndf_hasnotques = df[df['has_question']==False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_hasquestion))\nprint(len(df_hasnotques))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample1 = df_hasquestion.sample(n=1000,random_state= 1)\nsample2 = df_hasnotques.sample(n=1000, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(sample1))\nprint(len(sample2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_a = sample1.var(ddof=1)\nvar_b = sample2.var(ddof=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#std deviation\ns = np.sqrt((var_a + var_b)/2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Calculate the t-statistics\nt = (sample1.mean() - sample2.mean())/(s*np.sqrt(2/1000))\n## Compare with the critical t-value\n#Degrees of freedom\ndeg_f = 2*1000 - 2\n\n#p-value after comparison with the t \np = 1 - stats.t.cdf(t,df=deg_f)\n\n\nprint(\"t = \" + str(t))\nprint(\"p = \" + str(2*p))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the p value for the predictor 'has question' is equal to zero so we can now be certain about our assumption"},{"metadata":{},"cell_type":"markdown","source":"### Second hypothesis is tweets with full stops should be neutral"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['has_fullstops'] = df['text'].apply(lambda x: '.' in x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_has_fullstops = df[df['has_fullstops']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in (df_has_fullstops.airline_sentiment.value_counts().values):\n    print(j/len(df_has_fullstops)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our benchmark on the unfiltered dataset was.\n\n> Negative = 62.69125683060109 %\n\n> Neutral = 21.168032786885245 %\n\n> Postive = 16.140710382513664 %\n\n\n\n\n\nAs you can see the percentage didn't change much so having a full stop in the text didn't actually affect the nature of the tweets"},{"metadata":{},"cell_type":"markdown","source":"### Let's do a test to see whether having digits in the tweet really affect it's sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['has_digits'] = df['text'].apply(lambda x: max([y.isdigit() for y in x]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hasdigit=df[df['has_digits']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in (df_hasdigit.airline_sentiment.value_counts().values):\n    print(j/len(df_hasdigit)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much changes in the percenatges so we concluded that digits doesn't affect the sentiment of the tweets"},{"metadata":{},"cell_type":"markdown","source":"## WordClouds \nOne very handy visualization tool for a data scientist when it comes to any sort of natural language processing is plotting \"Word Cloud\". A word cloud (as the name suggests) is an image that is made up of a mixture of distinct words which may make up a text or book and where the size of each word is proportional to its word frequency in that text (number of times the word appears)"},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud for negative sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\ndf_x=df[df['airline_sentiment']=='negative']\nwords = ' '.join(df_x['text'].values)\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\n\nplt.figure(1,figsize=(12, 20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we may presume the most frequent words would be people complaining about flight canncellation, customer service and bags issues as appeared in the word cloud for negative responses, no clean text for positive and neutral responses to generate wordcloud for them. We can go deeper and computer the actual TFIDF weight for each word, let's see how can we do that.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_text']=df['text'].apply(lambda x: tweet_to_words(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.clean_text[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nwords = (\" \".join(df.clean_text)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Most common words and weights: \\n')\nprint(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nLeast common words and weights: ')\n(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's convert our labels , negative/neutral/positive to numbers to be able to see the correlation betwwen different features and those labels, "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment']=df['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's begin preparaing our data for ML pipeline so first step is that we should vecotrize the text we have in the tweets data. We have seen a type of vectorization the TFIDF when we gave weights to each word remeber? SKLearn got us covered in this issue we lots of vectorizing techniques and we will explore them one by one!\n\n***CountVectorizer*\n**\nCreates a matrix with frequency counts of each word in the text corpus\n\n\n***TF-IDF Vectorizer*\n****TF - Term Frequency -- Count of the words(Terms) in the text corpus (same of Count Vect)\nIDF - Inverse Document Frequency -- Penalizes words that are too frequent. We can think of this as regularization\n\n\n***HashingVectorizer***\nCreates a hashmap(word to number mapping based on hashing technique) instead of a dictionary for vocabulary\nThis enables it to be more scalable and faster for larger text coprus\nCan be parallelized across multiple threads"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(df,test_size=0.2,random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check data balance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unbalaced datasets could cause problems to our model and bias it to a wrong directions "},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = train[train['sentiment']==1]\nneg_train = train[train['sentiment']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(pos_train))\nprint(len(neg_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([pos_train,pos_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train['clean_text']\ny_train = train['sentiment']\nx_test = test['clean_text']\ny_test = test['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean_tweet=[]\nfor tweet in x_train:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in x_test:\n    test_clean_tweet.append(tweet)\ny = y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CountVector"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the words contained in our data\nprint(v.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport numpy\nnumpy.set_printoptions(threshold=sys.maxsize)\n# see the vector of the second word for example\nprint(train_features.toarray()[1:3\n                              ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nClassifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we got Random forest classifier is slightly above the bare random guessing so let's try antoher approach to mitigate the problem of unbalanced dataset, let's keep the data as is without scalling and measure precision and recall \nrecall = true positives/ true positives + false negatives\n\n> You might notice something about this equation: if we label all data pts as postives, then our recall goes to 1.0! We have a perfect classifier right?\nWell, not exactly. As with most concepts in data science, there is a trade-off in the metrics we choose to maximize. In the case of recall, when we increase the recall, we decrease the precision, so in order be confident in our model dicisions we need to account on both metrics\nprecision = true positives / true positives + false positives\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean_tweet=[]\nfor tweet in train['clean_text']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_text']:\n    test_clean_tweet.append(tweet)\ny = train['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(x_train)\ntest_features=v.transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score\nfrom sklearn.metrics import classification_report\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,y_train)\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred,y_test)\n    class_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report: '+str(class_rep))\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, there is no built-in lemmatizer in the vectorizer so we are left with a couple of options. Either implementing it separately everytime before feeding the data for vectorizing or somehow extend the sklearn implementation to include this functionality. Luckily for us, we have the latter option where we can extend the CountVectorizer class by overwriting the \"build_analyzer\" method as follows:\n\nlet's try to extending the CountVectorizer class with a lemmatizer and try again fitting our models\n"},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization + Countvector"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntrain_features= tf_vectorizer.fit_transform(x_train)\ntest_features=tf_vectorizer.transform(x_test)\ndense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,y_train)\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred, y_test)\n    class_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report: '+str(class_rep))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This got us a slight improvement on the precision-recall scale"},{"metadata":{},"cell_type":"markdown","source":"## TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntrain_features= tfv.fit_transform(x_train)\ntest_features=tfv.transform(x_test)\ndense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,y_train)\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred, y_test)\n    class_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report: '+str(class_rep))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n             use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntrain_features= tfv.fit_transform(x_train)\ntest_features=tfv.transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,y_train)\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred, y_test)\n    class_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report: '+str(class_rep))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF + SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=120)\nsvd.fit(train_features)\nxtrain_svd = svd.transform(train_features)\nxvalid_svd = svd.transform(test_features)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndense_features=xtrain_svd_scl\ndense_test= xvalid_svd_scl\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(xtrain_svd_scl,train['sentiment'])\n        pred = fit.predict(xvalid_svd_scl)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    average_precision = average_precision_score(pred, test['sentiment'])\n    classification_rep = classification_report(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report',classification_rep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost justification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifiers[7]\nmodel.fit(xtrain_svd_scl,train['sentiment'])\npred= model.predict(xvalid_svd_scl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'pred':pred,'True':y_test})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\n\nc = make_pipeline(tfv, svd,scl,Classifiers[7])\nclass_names=list(['0','1'])\nexplainer = LimeTextExplainer(class_names=class_names)\nidx = 4794\nexp = explainer.explain_instance(x_test[idx], c.predict_proba, num_features=6)\nexp.show_in_notebook(text=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see this is a correctly as positive, and model gives highest weight to the word best"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 14156\nexp = explainer.explain_instance(x_test[idx], c.predict_proba, num_features=6, labels=(1,0))\nexp.show_in_notebook(text=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This was missclassified as positive, neutral "},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=180)\nsvd.fit(train_features)\nxtrain_svd = svd.transform(train_features)\nxvalid_svd = svd.transform(test_features)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\n\ndense_features=xtrain_svd_scl\ndense_test= xvalid_svd_scl\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(xtrain_svd_scl,train['sentiment'])\n        pred = fit.predict(xvalid_svd_scl)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    average_precision = average_precision_score(pred, test['sentiment'])\n    classification_rep = classification_report(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report',classification_rep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove Embeddings\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nembeddings_index = {}\nf = open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(100)\n    return v / np.sqrt((v ** 2).sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_glove = [sent2vec(x) for x in tqdm(train_clean_tweet)]\nxvalid_glove = [sent2vec(x) for x in tqdm(test_clean_tweet)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(xtrain_glove,y_train)\n        pred = fit.predict(xvalid_glove)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred, y_test)\n    classification_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report',classification_rep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove with scalling"},{"metadata":{"trusted":true},"cell_type":"code","source":"scl = StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(xtrain_glove_scl,y_train)\n        pred = fit.predict(xvalid_glove_scl)\n    except Exception:\n        fit = classifier.fit(dense_features,y_train)\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,y_test)\n    average_precision = average_precision_score(pred, y_test)\n    classification_rep = classification_report(pred,y_test)\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n    print('classification report',classification_rep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep learning"},{"metadata":{},"cell_type":"markdown","source":"## Fully connected"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.layers import SpatialDropout1D\nfrom keras.preprocessing import text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale the data before any neural net:\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=100, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain_enc = np_utils.to_categorical(y_train)\nyvalid_enc = np_utils.to_categorical(y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\nmodel.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, epochs=50, \n          verbose=1, validation_data=(xvalid_glove_scl, yvalid_enc), callbacks=[earlystop])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(xvalid_glove_scl, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"token = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(x_train) + list(x_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_seq = token.texts_to_sequences(x_train)\nxvalid_seq = token.texts_to_sequences(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_seq[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 70\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = token.word_index\n\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     100,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=50, verbose=1, validation_data=(xvalid_pad, yvalid_enc),callbacks=[earlystop])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(xvalid_pad, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names=list(['0','1'])\nexplainer = LimeTextExplainer(class_names=class_names)\nidx = 4794\nexp = explainer.explain_instance(lsx_test[idx], model.predict, num_features=6, labels=(1,0))\nexp.show_in_notebook(text=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean_tweet[2957]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bidirectional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     100,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=50, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(xvalid_pad, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     100,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(xvalid_pad, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Take aways\n* Best approach to take from classical ML models is XGB classifier on top of tfidf vectorizer after decomposition and scalling due to its percision and recall results on the undersampling label\n* Best Deep learning model was the lstm architecture on Glove embeddings\n"},{"metadata":{},"cell_type":"markdown","source":"# Future thoughts\n* Need to justify all models to know exactly where we should be heading\n* Use different embeddings\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}