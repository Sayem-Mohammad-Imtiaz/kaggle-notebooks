{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the dataset\n\ndf_pre = pd.read_csv(\"/kaggle/input/wine-quality/winequalityN.csv\")\n\nShape=df_pre.shape\n\nprint(\"Rows:\",Shape[0])\nprint(\"Columns:\",Shape[1])\n\ndf_pre.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Info about the dataset\ndf_pre.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff=df_pre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking or the null values\n\ndff.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null value count in the dataset\n\ndff.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing the null values with the median\n\ndff = dff.fillna(dff.median())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling the categorical values\ndff = pd.get_dummies(data = dff, columns = ['type'] , prefix = ['type'] , drop_first = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check for highly correlated variables\n\ncor= dff.corr()\ncor.loc[:,:] = np.tril(cor,k=-1)  # reference:https://www.geeksforgeeks.org/numpy-tril-python/\ncor=cor.stack()\ncor[(cor > 0.55) | (cor< -0.55)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=dff.corr()\ncor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation plot\nplt.figure(figsize=(10,8))\nsns.heatmap(cor,annot=True,linewidths=.5,center=0,cbar=False,cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Target_Imb=dff[\"quality\"].value_counts(normalize=True)\nTarget_Imb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Combine 7&8 together; combine 3 and 4 with 5 so that we have only 3 levels and a more balanced Y variable\ndff['quality'] = dff['quality'].replace(8,7)\ndff['quality'] = dff['quality'].replace(3,5)\ndff['quality'] = dff['quality'].replace(4,5)\ndff['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data into training and test set for independent attributes\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test =train_test_split(dff.drop('quality',axis=1), dff['quality'], test_size=0.30,\n                                                   random_state=22)\nX_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\nmodel_entropy=DecisionTreeClassifier(criterion='entropy')\nmodel_entropy.fit(X_train, y_train)\nmodel_entropy.score(X_train, y_train)\nmodel_entropy.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pruned = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=5, min_samples_leaf=5)\nclf_pruned.fit(X_train, y_train)\npreds_pruned = clf_pruned.predict(X_test)\npreds_pruned_train = clf_pruned.predict(X_train)\nprint(accuracy_score(y_test,preds_pruned))\nprint(accuracy_score(y_train,preds_pruned_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_DT = accuracy_score(y_test, preds_pruned)\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT})\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, y_train)\npred_RF = rfcl.predict(X_test)\nacc_RF = accuracy_score(y_test, pred_RF)\ntempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [acc_RF]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf\n\nacc_RF=acc_RF*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ADA Boosting\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 100, learning_rate=0.1, random_state=22)\nabcl = abcl.fit(X_train, y_train)\npred_AB =abcl.predict(X_test)\nacc_AB = accuracy_score(y_test, pred_AB)\ntempResultsDf = pd.DataFrame({'Method':['Adaboost'], 'accuracy': [acc_AB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22)\ngbcl = gbcl.fit(X_train, y_train)\npred_GB =gbcl.predict(X_test)\nacc_GB = accuracy_score(y_test, pred_GB)\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost'], 'accuracy': [acc_GB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=50, max_samples= .7, bootstrap=True, oob_score=True, random_state=22)\nbgcl = bgcl.fit(X_train, y_train)\npred_BG =bgcl.predict(X_test)\nacc_BG = accuracy_score(y_test, pred_BG)\ntempResultsDf = pd.DataFrame({'Method':['Bagging'], 'accuracy': [acc_BG]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnum_folds = 20\nseed = 100\nkfold = KFold(n_splits=num_folds, random_state=seed)\n\ny = dff['quality']\nX = dff.loc[:, dff.columns != 'quality']\n\nresults = cross_val_score(rfcl,X, y, cv=kfold)\n\nKfold_CV=np.around(np.mean(abs(results*100)))\n\nfor i in range(num_folds):\n    print(\"Kfold\",i,\":\",results[i]*100,\"%\\n\")\n\nprint(\"Mean:\",Kfold_CV,\"%\")\n\nprint(\"\\nStandard Deviation:\",results.std())\n\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Kfold_CV-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Leave One Out Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(rfcl, X_train, y_train, cv=LeaveOneOut())\nscores\n\nprint(\"Mean accuracy:\",scores.mean()*100,\"%\")\n\nprint(\"\\nRandom Forest Accuracy\",acc_RF,\"%\")\n\nprint(\"\\nStandard Deviation:\",scores.std())\n\nLeave_One_Out=np.mean(abs(scores.mean()*100))\nimprovement=Leave_One_Out-acc_RF\nprint(\"\\nAccuracy improvement:\",np.around(improvement),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stratified Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection  import StratifiedKFold, cross_val_score\n\nk = 20\n\nstratified_kfold = StratifiedKFold(n_splits = k, random_state = 55)\nresults = cross_val_score(rfcl, X, y, cv = stratified_kfold)\n\nstrat_CV=np.around(np.mean(abs(results*100)))\n\nfor i in range(k):\n    print(\"Kfold\",i,\":\",results[i]*100,\"%\\n\")\n\nprint(\"\\n\\nMean:\",strat_CV,\"%\")\n\nprint(\"\\nStandard Deviation:\",results.std())\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=strat_CV-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bootstrapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of iterations for bootstrapping\nbootstrap_iteration = 75\naccuracy = []\n\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(bootstrap_iteration):\n    X_, y_ = resample(X_train, y_train)\n    rfcl.fit(X_, y_)\n    y_pred = rfcl.predict(X_test)\n    \n    acc = accuracy_score(y_pred, y_test)\n    accuracy.append(acc)\n    \naccuracy = np.array(accuracy)\n\nprint('Standard deviation: ', accuracy.std())\n\nBoot=np.around(accuracy.mean()*100)\n\nprint(\"\\n\\nMean:\",Boot,\"%\")\n\nprint(\"\\nStandard Deviation:\",accuracy.std())\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Boot-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Tuning using hyper parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pretty print\n\nfrom pprint import pprint\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state = 1)\n\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nprint(np.linspace(start = 5, stop = 10, num = 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10 , stop = 15, num = 2)]   # returns evenly spaced 10 numbers\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 10, num = 2)]  # returns evenly spaced numbers can be changed to any\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)\n\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 5, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train);\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [5,6],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [5,10],\n    'n_estimators': [5,6,7]\n}    \n\nrf = RandomForestRegressor(random_state = 1)\n\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)\n\ngrid_search.fit(X_train, y_train);\n\ngrid_search.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid = grid_search.best_estimator_\n\nGrid_search_cv=np.around(best_grid.score(X_test, y_test)*100)\nprint(\"Grid sarch CV Score:\",Grid_search_cv,\"%\")\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Grid_search_cv-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Randon Forest Accuracy:\", acc_RF,\"%\")\n\nprint(\"\\nK Fold:\", Kfold_CV,\"%\")\n\nprint(\"\\nLeave one Out:\", Leave_One_Out,\"%\")\n\nprint(\"\\nStratified CV:\", strat_CV,\"%\")\n\nprint(\"\\nBootstrapping:\", Boot,\"%\")\n\nprint(\"\\nRandom sarch CV:\",rf_random,\"%\")\n\nprint(\"\\Grid sarch CV Score:\",grid_search,\"%\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}