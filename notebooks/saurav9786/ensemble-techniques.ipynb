{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem statement\n\nWe have data from a Portuguese bank on details of customers related to selling a term deposit\nThe objective of the project is to help the marketing team identify potential customers who are relatively more likely to subscribe to the term deposit and this increase the hit ratio","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data dictionary\n\n**Bank client data**\n* 1 - age \n* 2 - job : type of job \n* 3 - marital : marital status\n* 4 - education \n* 5 - default: has credit in default? \n* 6 - housing: has housing loan? \n* 7 - loan: has personal loan?\n* 8 - balance in account\n\n**Related to previous contact**\n* 8 - contact: contact communication type\n* 9 - month: last contact month of year\n* 10 - day_of_week: last contact day of the week\n* 11 - duration: last contact duration, in seconds*\n\n**Other attributes**\n* 12 - campaign: number of contacts performed during this campaign and for this client\n* 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign\n* 14 - previous: number of contacts performed before this campaign and for this client\n* 15 - poutcome: outcome of the previous marketing campaign\n\n**Output variable (desired target):has the client subscribed a term deposit?**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read the dataset\n\nbank_df = pd.read_csv(\"/kaggle/input/bank-marketing/bank-additional-full.csv\",sep = ';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of the data\n\nbank_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the dataset\n\nbank_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Info about the dataset\nbank_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model\n\nbank_df.drop(['duration'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Certain variables are more relevant if they are categorical variable than numerical variables. We will convert such categorical variables to numeric variabes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['pdays']=bank_df['pdays'].astype('category')\nbank_df['y']=bank_df['y'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Univariate analysis - boxplot / histogram for numerical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=bank_df['age'], data=bank_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Age column has some outliers. The median age is about 40 years. There are some customers above 90 years of age. This data might have to be checked**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#histograms from the pair plots\nsns.pairplot(bank_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The distribution of all numerical variables other than age is highly skewed - hence we might want to transform or bin some of these variables**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**On similar lines, please perform univariate analysis of other numerical variables**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Univariate analysis - countplot / value count for categorical variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['job'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['marital'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.countplot(bank_df['education'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.countplot(bank_df['default'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**default - yes is a very very small % - we can consider deleting this column**[](http://)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['housing'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['loan'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['contact'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['poutcome'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(bank_df['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['y'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The response rate is only 11.6%. Hence the Y variable has a high class imbalance. Hence accuracy will not be a reliable model performance measure. \n\n### FN is very critical for this business case because a false negative is a customer who will potentially subscribe for a loan but who has been classified as 'will not subscribe'. Hence the most relevant model performance measure is recall","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Bivariate analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rename the dependant column from 'y ' to 'Target'\nbank_df.rename(columns={'y':'Target'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Group numerical variables by mean for the classes of Y variable\nnp.round(bank_df.groupby([\"Target\"]).mean() ,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The mean balance is higher for customers who subscribe to the term deposit compared to those who dont\n\n\n#### number of days that passed by after the client was last contacted from a previous campaign is higher for people who have subscribed\n\n#### number of contacts performed before this campaign is also higher for customers who subscribe","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### All of the above facts indicate that customers with a higher balance and those who have been contacted frequently before the campaign tend to subscribe for the term deposit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Bivariate analysis using crosstab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate analysis using crosstab","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['job'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The highest conversion is for students (31%) and lowest is for blue-collar(7%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['marital'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['education'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.crosstab(bank_df['default'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False ))\nprint(bank_df['default'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since default - yes is only 0.073% of the data and the conversion is also comparitively lower for default - yes, we can remove this column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.drop(['default'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['housing'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['loan'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['contact'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['day_of_week'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(bank_df['month'], bank_df['Target'], normalize='index').sort_values(by='yes',ascending=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### List out the high level findings from bivariate analysis that could provide pointers to feature selection\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binning:\ndef binning(col, cut_points, labels=None):\n  #Define min and max values:\n  minval = col.min()\n  maxval = col.max()\n\n  #create list by adding min and max to cut_points\n  break_points = [minval] + cut_points + [maxval]\n\n  #if no labels provided, use default labels 0 ... (n-1)\n  if not labels:\n    labels = range(len(cut_points)+1)\n\n  #Binning using cut function of pandas\n  colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n  return colBin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binning campaign\ncut_points = [2,3,4]\nlabels = [\"<=2\",\"3\",\"4\",\">4\"]\nbank_df['campaign_range'] = binning(bank_df['campaign'], cut_points, labels)\nbank_df['campaign_range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.drop(['campaign'], axis=1, inplace=True)\nbank_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bank_df.drop(\"Target\" , axis=1)\ny = bank_df[\"Target\"]   # select all rows and the 17 th column which is the classification \"Yes\", \"No\"\nX = pd.get_dummies(X, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#instantiating decision tree as the default model\ndt_model = DecisionTreeClassifier()\ndt_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is the model an overfit model? \ny_pred = dt_model.predict(X_test)\nprint(dt_model.score(X_train, y_train))\nprint(dt_model.score(X_test , y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: - Decision Tree is a non-parametric algorithm and hence prone to overfitting easily. This is evident from the difference\n# in scores in training and testing\n\n# In ensemble techniques, we want multiple instances (each different from the other) and each instance to be overfit!!!  \n# hopefully, the different instances will do different mistakes in classification and when we club them, their\n# errors will get cancelled out giving us the benefit of lower bias and lower overall variance errors.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(accuracy_score(y_test, y_pred))\n\n\nprint(recall_score(y_test, y_pred,average=\"binary\", pos_label=\"yes\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The recall score is relatively low and this has to be improves in the model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_pruned = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\nclf_pruned.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nfrom sklearn.tree import export_graphviz\n\ndata = export_graphviz(clf_pruned,out_file=None,feature_names=feature_cols,class_names=['0','1'],   \n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Calculating feature importance\n#feature_names=feature_cols\nfeat_importance = clf_pruned.tree_.compute_feature_importances(normalize=False)\n\n\nfeat_imp_dict = dict(zip(feature_cols, clf_pruned.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\nfeat_imp.sort_values(by=0, ascending=False)[0:10] #Top 10 features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_pruned = clf_pruned.predict(X_test)\npreds_pruned_train = clf_pruned.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_DT = accuracy_score(y_test, preds_pruned)\nrecall_DT = recall_score(y_test, preds_pruned, average=\"binary\", pos_label=\"yes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT, 'recall': recall_DT})\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overfitting is reduced after pruning, but recall has drastically reduced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Apply the Random forest model and print the accuracy of Random forest Model\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, y_train)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_RF = rfcl.predict(X_test)\nacc_RF = accuracy_score(y_test, pred_RF)\nrecall_RF = recall_score(y_test, pred_RF, average=\"binary\", pos_label=\"yes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [acc_RF], 'recall': [recall_RF]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Apply Adaboost Ensemble Algorithm for the same data and print the accuracy.\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 200, learning_rate=0.1, random_state=22)\nabcl = abcl.fit(X_train, y_train)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_AB =abcl.predict(X_test)\nacc_AB = accuracy_score(y_test, pred_AB)\nrecall_AB = recall_score(y_test, pred_AB, pos_label='yes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Adaboost'], 'accuracy': [acc_AB], 'recall':[recall_AB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Apply Bagging Classifier Algorithm and print the accuracy\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=100, max_samples= .7, bootstrap=True, oob_score=True, random_state=22)\nbgcl = bgcl.fit(X_train, y_train)\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_BG =bgcl.predict(X_test)\nacc_BG = accuracy_score(y_test, pred_BG)\nrecall_BG = recall_score(y_test, pred_BG, pos_label='yes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Bagging'], 'accuracy': [acc_BG], 'recall':[recall_BG]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 200, learning_rate = 0.1, random_state=22)\ngbcl = gbcl.fit(X_train, y_train)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_GB =gbcl.predict(X_test)\nacc_GB = accuracy_score(y_test, pred_GB)\nrecall_GB = recall_score(y_test, pred_GB, pos_label='yes')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Gradient Boost'], 'accuracy': [acc_GB], 'recall':[recall_GB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy', 'recall']]\nresultsDf\nresultsDf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging gives overall best model performance. However, please note that the recall is still very low and will have to be improved","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}