{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"73b5230468713507a2b87d00d23a97c9fc9cb91d","_cell_guid":"e939c118-ff0f-436f-acb1-6936bcbdc361"}},{"cell_type":"markdown","source":"A tutoial for beginners in Machine Learning","metadata":{"_uuid":"f85387a610ec266eee3a01d851ea482b7dce2e22","_cell_guid":"300701bb-5bc4-43bb-8c75-5824c62f0046"}},{"source":"data = pd.read_csv('../input/data.csv')","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"438eafa7b66c15c1e2eef781025d3bd164bf4413","collapsed":true,"_cell_guid":"dc872ada-05fa-496b-a8ea-695c77ed9289"}},{"source":"data.head()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"68bb8984f41ed1c744f7b6c3d473848190708388","_cell_guid":"c56a723c-c00d-4a5b-baa1-7f10de81e5fa"}},{"source":"# feature names as a list\ncol = data.columns       # .columns gives columns names in data \nprint(col)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"ec1c50b5fb27f8f54db1269bafb9f4801bd9f1be","_cell_guid":"96d71f0f-9ec4-43a7-abe7-dfa1863c9adf"}},{"source":"# y includes our labels and x includes our features\ny = data.diagnosis                          # M or B \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"1c27882ec2815df32a4f4d94a78d7e7f282f0390","scrolled":true,"_cell_guid":"cdbd4153-bde9-4438-9223-dce9cdd41ab3"}},{"source":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"9a4efadb7a6d66de6adff18235edb076c32ec727","_cell_guid":"fc98e3a5-b052-4eab-94bc-12e97503d144"}},{"source":"x.describe()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"9f0224d7cf01fd46b1d699d707b67e3bfc68ae22","_cell_guid":"ddd68341-5b05-4daa-9f4b-6834fff50136"}},{"cell_type":"markdown","source":"** Visualization: **\n\nIn order to visualizate data we are going to use seaborn plot that is not used in other kernels to inform you and for diversity of plots. What I use in real life is mostly violin plot . Do not forget we are not selecting feature, we are trying to know data before doing analysis. We need to normalize or standirdize because values of features are very large to observe on plot.\nI plot features in 3 group and each group includes 10 features \nto observe better.","metadata":{"_uuid":"0997a5ce4dbdb517a00e3ed07c90992fa15cf2b8","collapsed":true,"_cell_guid":"c633bbff-0a69-4f65-875d-5ddfe7683ca1"}},{"source":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"09319959dc86d1be6dd6500a565a2fa359fc499d","_cell_guid":"464cb3aa-a20f-4cbb-8cda-d2274eb2c77f"}},{"cell_type":"markdown","source":"Let us interpret the above plot.Lets interpret the plot above together. For example, in texture_mean feature, median of the Malignant and Benign looks like separated so it can be good for classification. However, in fractal_dimension_mean feature, median of the Malignant and Benign does not looks like separated so it does not gives good information for classification.","metadata":{"_uuid":"a9064fc27b4bcb74ab9515dbfa9a96b2034d0053","_cell_guid":"3a80152c-cde4-4a14-bf63-f21f227bcd7c"}},{"source":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"f4f8af0b69bd8ddce2b15e9c4265df253a80bb73","_cell_guid":"58cffd89-e386-4750-9c56-300565667c1a"}},{"source":"# third ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"e40fca8d6d6421ba6a9fe229591608178d417c69","_cell_guid":"f8bed241-b14d-41c1-b75c-de676f174c67"}},{"cell_type":"markdown","source":"They looks cool right. And you can see variance more clear. Let me ask you a question, in these three plots which feature looks like more clear in terms of classification. In my opinion area_worst in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, smoothness_se in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature.\n\nWhat if we want to observe all correlation between features? Yes, you are right. The answer is heatmap that is old but powerful plot method.","metadata":{"_uuid":"260a02d249f707dc01b8912637333e86abcab0b7","_cell_guid":"82de4dc0-0c41-4193-b3e3-cbf0f9020793"}},{"source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"0d06661472e74e8540fade6426adcefbdeb91778","_cell_guid":"75f64cb3-2e79-406c-b78d-666ed1df9ac9"}},{"cell_type":"markdown","source":"We are finally at the feature selection, let us use this heat map and figure out the features which are highly correlated.","metadata":{"_uuid":"3a4d697f2fb6b74a8fd5a1b07e66ea8195e6c126","_cell_guid":"d76416b5-1fc7-4577-a5d1-f2a6413242ab"}},{"cell_type":"markdown","source":"****Feature Selection and Random Forest Classification****\n\n\nToday our purpuse is to try new cocktails. For example, we are finaly in the pub and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to eliminate other drinks which includes lemon so as to experience very different tastes.\n\nIn this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict.","metadata":{"_uuid":"c9a7aa4c2d5ba7869c4eb4f5930f2d339d14a960","_cell_guid":"f5f72d1a-07a2-4537-b93f-e176bfc67ae4"}},{"cell_type":"markdown","source":"****1. Feature selection with correlation and random forest classification****\n\nAs it can be seen in map heat figure **radius_mean, perimeter_mean and area_mean** are correlated with each other so we will use only **area_mean**. If you ask how i choose** area_mean** as a feature to use, well actually there is no correct answer, I just look at swarm plots and **area_mean** looks like clear for me but we cannot make exact separation among other correlated features without trying. So lets find other correlated features and look accuracy with random forest classifier.\n\n**Compactness_mean, concavity_mean and concave points_mean** are correlated with each other.\n\nTherefore I only choose **concavity_mean**.\n\nApart from these,** radius_se, perimeter_se and area_se** are correlated and I only use **area_se**. \n\n**radius_worst, perimeter_worst and area_worst** are correlated so I use **area_worst**. \n\n**Compactness_worst, concavity_worst and concave points_worst** are correlated  so I use **concavity_worst**.\n\n** Compactness_se, concavity_se and concave points_se** are correlated  so I use **concavity_se**. \n\n**texture_mean and texture_worst **are correlated and I use **texture_mean**.\n\n**area_worst and area_mean** are correlated, I use **area_mean**.","metadata":{"_uuid":"c752cf0177daf6259038ca3cf5a9da625c7c852f","_cell_guid":"4153204f-d5a1-4bb0-8d90-67303894c983"}},{"source":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"6b8835d41e0bbbd64ea19babbb49026463e6269a","_cell_guid":"2a438c96-6c63-40f3-99cc-590bda898ebb"}},{"cell_type":"markdown","source":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it.","metadata":{"_uuid":"d6aafb99dd4d10ac3c1e9ce1bd599717173634fe","_cell_guid":"b0efccdd-3da4-4097-9a57-d2f11360988a"}},{"source":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"2b3e24b50e517b2293ebd4cbf18d09794edd17af","_cell_guid":"626a3fbb-2e22-452a-b395-3130a8d4bb79"}},{"cell_type":"markdown","source":"Well, we choose our features but did we choose correctly ? Lets use random forest and find accuracy according to chosen features.","metadata":{"_uuid":"59418d2c24ae5240df02f984f719deeab617be2a","_cell_guid":"61120af1-cbe3-44ab-9bc8-4a3e179c9d9b"}},{"source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier()      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"7ab6ec08b86a4b2b4057458bde992992ad51bf55","collapsed":true,"_cell_guid":"29394a4e-acd5-4466-a345-f787956139e4"}},{"cell_type":"markdown","source":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","metadata":{"_uuid":"daf1592f1db29023b038fee6bc96f8329d0ed70b","collapsed":true,"_cell_guid":"8b1202ea-06b2-49ef-bc48-0649dd5cb7c1"}},{"cell_type":"markdown","source":"****2) Univariate feature selection and random forest classification****\n\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features. http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n\nIn this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features.","metadata":{"_uuid":"e5f7d441028943df476eb5dbd322d9710efef459","_cell_guid":"9f699774-4403-468b-8f23-640ab9598767"}},{"source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"adc20cb020e0cecfd240dc09a6365ed7019e672c","collapsed":true,"_cell_guid":"5aacd560-7dd5-4fb1-b59c-4479188b138d"}},{"source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"bc52e1ffd95301920821c04a5b6fa34a0f1a6f1c","collapsed":true,"_cell_guid":"060298d1-8d62-47cb-932b-bd5efe403176"}},{"cell_type":"markdown","source":"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar. Now lets see other feature selection methods to find better results.","metadata":{"_uuid":"ebfd2d0852ec5842040c92097ef0899a555d98ef","_cell_guid":"571db74b-47b5-414b-a902-723834afa647"}},{"cell_type":"markdown","source":"****3) Recursive feature elimination (RFE) with random forest****\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html \n\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features\n\nLike previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.","metadata":{"_uuid":"1bfad2d8644d6bce7e7239f4b7ed8923d876b963","_cell_guid":"7739931a-2a25-40b9-9a33-7ddd8264465d"}},{"source":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\nprint('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"b2dc74a79475087667a1c6ef044c376158fc4acf","collapsed":true,"_cell_guid":"f5cb04e9-593f-41f6-b261-4b41520e3e7a"}},{"cell_type":"markdown","source":"Chosen 5 best features by rfe is texture_mean, area_mean, concavity_mean, area_se, concavity_worst. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is 5. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method.","metadata":{"_uuid":"3e7c4cbf15dda3603696b665e6a494ebad30075d","_cell_guid":"eedc13eb-14b2-476d-96d7-0600f5226825"}},{"cell_type":"markdown","source":"****4) Recursive feature elimination with cross validation and random forest classification****\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html \n\nNow we will not only find best features but we also find how many features do we need for best accuracy.","metadata":{"_uuid":"fa31df16f2d54315d4901507070317737a0f4f92","_cell_guid":"a9ec0d1f-ad29-49aa-8124-e6f8c994e3c4"}},{"source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"b5ee3145de12f31656144cb11af04defcb56ec45","collapsed":true,"_cell_guid":"80f553a1-22a1-499d-8034-098c9e463616"}},{"cell_type":"markdown","source":"Finally, we find best 11 features that are texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst for best classification. Lets look at best accuracy with plot.","metadata":{"_uuid":"e58b74fc5adbbb814e20d4ae839f29e69bd358aa","_cell_guid":"08432b65-af1b-426c-bc99-43fc72922bcb"}},{"source":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"fe7255e5724a7c076d7f6beca0b35d2381372924","collapsed":true,"_cell_guid":"ab51e405-5b74-418c-9861-6c48900c5f0c"}},{"cell_type":"markdown","source":"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make feature selection and understanding data. Then last make our last feature selection method.","metadata":{"_uuid":"fea28fd55fd17c2d054f7236c11baebb8d1b8e39","_cell_guid":"dbf80f82-8079-4d66-a29c-ec173b0be53c"}},{"cell_type":"markdown","source":"****5) Tree based feature selection and random forest classification****\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nIn random forest classification method there is a featureimportances attributes that is the feature importances (the higher, the more important the feature). !!! To use feature_importance method, in training data there should not be correlated features.\n\nRandom forest choose randomly at each iteration, therefore sequence of feature importance list can change.","metadata":{"_uuid":"999a31c5b9710b01020c782ede59a00de9dc765c","_cell_guid":"d9dbf939-a29a-4ebd-966b-589d612f2c81"}},{"source":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"52d7071e05ac75b738315a00c135475356a956d3","collapsed":true,"_cell_guid":"dde4668f-6cec-46f0-92c6-1c00399480ad"}},{"cell_type":"markdown","source":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them.","metadata":{"_uuid":"0877c3f994c62103ed965de0a634d82b550a1277","_cell_guid":"24666015-4d46-4d28-8c5c-2855b321c66c"}},{"cell_type":"markdown","source":"****Feature Extraction****\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html \n\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.","metadata":{"_uuid":"af21fc9baac7c0987031e9aa686ab59dc8143201","_cell_guid":"3b452175-cce2-4cc4-8d13-a00f73232c92"}},{"source":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_uuid":"9936e6a199a9f241582d4a342d564740c62d130b","collapsed":true,"_cell_guid":"aba55559-c1b3-4e8c-8e5c-e50b52402da8"}},{"cell_type":"markdown","source":"According to the above variace plot, 3 component can be chosen","metadata":{"_uuid":"a39a9b47e88118063eb764d5bd610057baea5b23","_cell_guid":"6ccb9867-77a3-45d8-90e9-679aac67177d"}},{"cell_type":"markdown","source":"Conclusion\nShortly, I tried to show importance of feature selection and data visualization. Default data includes 33 feature but after feature selection we drop this number from 33 to 5 with accuracy 95%. In this kernel we just tried basic things, I am sure with these data visualization and feature selection methods, you can easily ecxeed the % 95 accuracy. Maybe you can use other classification methods.\n\nI hope you enjoy in this kernel","metadata":{"_uuid":"9744964376262dd1b9cbb78c0e5cfe29479388af","_cell_guid":"be03cf20-76b6-4057-9600-ff324a5223c4"}}],"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}}}