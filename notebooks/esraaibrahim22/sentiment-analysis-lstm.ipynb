{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#import os\n#os.remove(\"/kaggle/working/sntimentModel.sav\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom string import punctuation\nimport re\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom sklearn.model_selection import train_test_split\n#from sklearn.metrics import accuracy_score\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\n\n\n\n\n\ndata = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\nreview=data['text']\nlabel=data['airline_sentiment']\n\nprint(\"after reading :\",len(review) , len(label))\n\ndf_class_0 = data[data['airline_sentiment'] == 'negative']\ndf_class_1 = data[data['airline_sentiment'] == 'positive']\ndf_class_2 = data[data['airline_sentiment'] == 'neutral']\ntarget_count = data.airline_sentiment.value_counts()\n    \nprint(target_count[0] , target_count[1] , target_count[2])\n\"\"\"\ndf_class_1_over = df_class_1.sample(target_count[0], replace=True)\ndf_class_2_over = df_class_2.sample(target_count[0], replace=True)\n\n\ndf_test_over_total = pd.concat([df_class_0,df_class_1_over, df_class_2_over ], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over_total.airline_sentiment.value_counts())\n\nreview=df_test_over_total.text\nlabel=df_test_over_total.airline_sentiment\n\nprint(\"after overSampling:\",len(review) , len(label))\n\"\"\"\ntrain = pd.concat([df_class_0.sample(frac=0.8,random_state=200),\n         df_class_1.sample(frac=0.8,random_state=200), df_class_2.sample(frac=0.8,random_state=200)])\ntest = pd.concat([df_class_0.drop(df_class_0.sample(frac=0.8,random_state=200).index),\n        df_class_1.drop(df_class_1.sample(frac=0.8,random_state=200).index),\n                  df_class_2.drop(df_class_2.sample(frac=0.8,random_state=200).index)])\n\ntrain = shuffle(train)\ntest = shuffle(test)\n\nprint('positive data in training:',(train.airline_sentiment == 'positive').sum())\nprint('negative data in training:',(train.airline_sentiment == 'negative').sum())\nprint('neutral data in training:',(train.airline_sentiment == 'neutral').sum())\nprint('positive data in test:',(test.airline_sentiment == 'positive').sum())\nprint('negative data in test:',(test.airline_sentiment == 'negative').sum())\nprint('neutral data in test:',(test.airline_sentiment == 'neutral').sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg = train[train['airline_sentiment'] == 'negative']\npos = train[train['airline_sentiment'] == 'positive']\nneu = train[train['airline_sentiment'] == 'neutral']\n\npos_upsampled = resample(pos, \n                                 replace=True,     # sample with replacement\n                                 n_samples= neg.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\nneu_upsampled = resample(neu, \n                                 replace=True,     # sample with replacement\n                                 n_samples= neg.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\ndata_upsampled = pd.concat([neg, pos_upsampled ,neu_upsampled ])\nprint(\"After upsampling\\n\",data_upsampled.airline_sentiment.value_counts(),sep = \"\")\nreview=data_upsampled.text\nlabel=data_upsampled.airline_sentiment\n\ntestReview=test.text\ntestLabel=test.airline_sentiment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nmodel = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True )\n\nword_vectors = model.wv\n\ndef sentenceEmbedding(tokens , typee=\"avg\"):\n    vectors=[]\n    for word in tokens:\n        if word not in word_vectors.vocab:\n            vectors.append([0]*300) #size of feature vector\n        else:\n            vectors.append(model[word])\n    \n    result=[0] * len(vectors[0])\n    res=0\n    for i in range(len(vectors[0])):\n        for vec in vectors:\n            res+=vec[i]\n        if typee== \"sum\":\n            result[i]=res\n        else:\n            result[i]=(res/(len(tokens)))\n        res=0\n         \n    return result\n\"\"\"\ndef mapSentiment(airlineSentiment):\n    if airlineSentiment == 'positive':\n        return 1\n    elif airlineSentiment == 'negative' :\n        return 0\n    else:\n        return 2\n        \"\"\"\nwordnet_lemmatizer = WordNetLemmatizer()\ndef cleanString(sentences):\n    result=[]\n    for sen in sentences:\n        s=\"\"\n        r=\"\"\n        s+=(sen.lower()+' ')\n        s = re.sub(\"(@\\w* )\", ' ', s)\n        s = re.sub(\"\\\\bhttps://(.*) \\\\b\",' ',s) \n        s = re.sub(\"[^a-z0-9\\ ]+\", ' ', s)\n        s = re.sub(' \\d+', ' ', s)\n        s = re.sub(\" +\",' ',s)\n        tokens=s.split()\n        for w in tokens:\n             r+=wordnet_lemmatizer.lemmatize(w ,pos=\"v\")+\" \"\n        result.append(r)\n    return result\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = pd.get_dummies(data_upsampled.airline_sentiment).values\ntestLabel = pd.get_dummies(test.airline_sentiment).values\nprint (len(label) , label)\n\n\nreview=cleanString(review)\ntestReview=cleanString(testReview)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = input(\"choose sum ||  avg for sentence embedding:\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntestVectors=[]\nfeatureVectors=[]\nfor r in review:\n    sentence=r.split()\n    featureVectors.append(sentenceEmbedding(sentence,t))\nfor r in testReview:\n    sentence=r.split()\n    testVectors.append(sentenceEmbedding(sentence,t))\n\n\nprint(\"last:\",len(featureVectors) , len(label))\n\nfeatureVectors, xVal, label, yVal = train_test_split(featureVectors, label, test_size=0.15, random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM , Dense , Dropout, Activation,SpatialDropout1D\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD\n\nxTrain=np.array(featureVectors)\nyTrain=np.array(label)\nxTest=np.array(testVectors)\nyTest=np.array(testLabel)\nxVal=np.array(xVal)\nyVal=np.array(yVal)\n\"\"\"\"\nxTrain=np.array(X_train)\nyTrain=np.array(y_train)\nxTest=np.array(X_test)\nyTest=np.array(y_test)\"\"\"\nprint(\"XTRAIN_SHAPE:\" , xTrain.shape , \"YTRAIN_SHAPE:\", yTrain.shape)\n\nyTrain=np.reshape(yTrain , ( yTrain.shape[0],1,3 ))\nyTest=np.reshape(yTest , (yTest.shape[0],1,3 ))\nyVal=np.reshape(yVal , (yVal.shape[0] ,1, 3))\n\nxTrain=np.reshape(xTrain , (xTrain.shape[0] ,1, 300))\nxTest=np.reshape(xTest , (xTest.shape[0] ,1, 300))\nxVal=np.reshape(xVal , (xVal.shape[0] ,1, 300))\n\nprint(\"XTRAIN_RESHAPE:\" , xTrain.shape , \"YTRAIN_RESHAPE:\", yTrain.shape)\n\n\ninput_length = None\ninput_dim = 300\nc_model=Sequential()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_model.add(LSTM(265, dropout=0.4, recurrent_dropout=0.4,input_dim = input_dim\n                 , input_length = input_length,return_sequences=True ) )\n#c_model.add(LSTM(50))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_model.add(Dense(3,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(c_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory=c_model.fit(xTrain, yTrain,nb_epoch=10 ,  batch_size=128, verbose = 1 , validation_data=(xVal, yVal) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot train and validation loss\n\"\"\"\nfrom matplotlib import pyplot\n\npyplot.plot(history.history['loss'])\npyplot.plot(history.history['val_loss'])\npyplot.title('model train vs validation loss')\npyplot.ylabel('loss')\npyplot.xlabel('epoch')\npyplot.legend(['train', 'validation'], loc='upper right')\npyplot.show()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=c_model.predict_classes(xTest,batch_size = 128)\nprint (y)\nloss , accur=c_model.evaluate(xTest , yTest)\nprint(\"loss:\", loss , \"\\nacc:\", accur)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lstmPrediction(review ,choice ):\n    smbls=dict()\n    smbls[0]=\"Negative\"\n    smbls[1]=\"Neutral\"\n    smbls[2]=\"Positive\"\n    r=cleanString([review])\n    review=sentenceEmbedding(r[0].split() , choice)\n    data=np.array([review])\n    data=np.reshape(data,(1,data.shape[0],300))\n    ps=c_model.predict(data)\n    print(ps)\n    return smbls[np.argmax(ps)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review=input(\"Enter your review: \")\nchoice=input(\"Enter sum || avg: \")\nprint(lstmPrediction(review ,choice ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}