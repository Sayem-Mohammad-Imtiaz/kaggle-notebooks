{"cells":[{"metadata":{"_uuid":"6616ed4e12b2975e3603ea2019fa92719310806b"},"cell_type":"markdown","source":"The aim of this kernel is to compare various types of regressions vs K-nearest neighbor alogrithm on the Biomechanical features of orthopedic patients data. \n\nThis kernel will also contain basic EDA to understand the data. \n\nTypes of regression covered in this kernel:\n1. Linear Regression\n2. Random Forest\n3. Decision Trees\n4. Logisitic Regression\n\nSo, lets get started with the data set:  Biomechanical features of orthopedic patients\n\nThis data set contains features for orthopedic patients. Copying over the description from data set page:\n\nEach patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):\n\n* pelvic incidence\n* pelvic tilt\n* lumbar lordosis angle\n* sacral slope\n* pelvic radius\n* grade of spondylolisthesis\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/column_2C_weka.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"657e293668c3ff7569baf8cac23629a99aa3073b"},"cell_type":"markdown","source":"Let's look at the correlation between different variables."},{"metadata":{"trusted":true,"_uuid":"8c712ec8488f3264df4096466eb6055e5692458b"},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ae32b0a2cca63bce6e7b7a550d528151706ec95"},"cell_type":"markdown","source":"Looking at the correlated data above, here are some observations:\n\nHighly correlated:\n- pelvic_incidence and sacral_slope are highly correlated\n- pelvic_tilt numeric and pelvic_incidence are highly correlated\n- lumbar_lordosis_angle and pelvic_incidence are highly correlated\n- pelvic_radius and pelvic_tilt numeric are highly correlated\n\nWeakly correlated:\n- pelvic_incidence and pelvic_radius are weakly correlated.\n- pelvic_tilt numeric and pelvic_radius are weakly correlated.\n\n**Heat map:**\n\nHeat map allows us to look at the correlated data in a visual manner, which might help us quickly point out variables which are highly correlated, hence helping us on a path towards using those variables for further analysis.\n\nWe will create a heat map from the correlated data below:"},{"metadata":{"trusted":true,"_uuid":"f3a174d5b3f520cc1b40970e95f88b3fa0880e60","scrolled":true},"cell_type":"code","source":"f, axis = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(), annot=True, linewidths=0.4, fmt='.2f', ax = axis)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bcbc919ddf99802577bc00a139da443af8e3f22"},"cell_type":"markdown","source":"As you can see from the heat map, pelvic_radius is weakly correlated with most of the other features in the dataset. This is helpful as we would have to spend more time with a tabular dataset to come to the above conclusion.\n\n**Count Plot:**\n\nLets take a look at the counts of abnormal vs normal classes in the dataset.\n"},{"metadata":{"trusted":true,"_uuid":"7e0400e4d63235bd5a0260d86bef1e41768b033d"},"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:, 'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"726639eba4ae7f12c9528ab3bb82e9b8bdd6a129"},"cell_type":"markdown","source":"As we can see from the above plot, abnormal counts are more than double of the of the normal class counts. "},{"metadata":{"_uuid":"23bfb355a7c5f018a257785d33e187ced728edea"},"cell_type":"markdown","source":"**Pair Plot**\n\nWe would be using one of the most common plots used by data scientists when trying to understand the data. Pair plot helps us display correlation between different features via scatter plot and historgrams. \n\nBy default pairplot will return scatter plots in the main entries and a histogram in the diagonal. pairplot is oftentimes the first thing that a data scientist will throw at their data, and it works fantastically well in that capacity, even if sometimes the scatter-and-histogram approach isn't quite appropriate, given the data types."},{"metadata":{"trusted":true,"_uuid":"a413088d714393e58e2ca3322d72a8c8d22419ed"},"cell_type":"code","source":"sns.pairplot(data,hue=\"class\",palette=\"Set2\", diag_kind = 'kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae56b3697507300d9c7468a3ff9033128b149b1"},"cell_type":"markdown","source":"As you can see from the pairplot above, the distribution of classes (Abnormal and Normal) are shown over all the features. \n\nFor example, you can look at pelvic_incidence and degree_spondyiosthesis pair, which shows you the distribution of classes. As the pelvic_incidence increases, the degree_spondyiosthesis does not increase beyond 200 value. \n\nAnother example, pelvic_tilt_numeric and lumbar_lordosis_angle, as pelvic_tilt_numeric increases, so does lumbar_lordosis_angle, hence proving them to be linearly related to each other. \n\nYou can find more information on how to create pairplots and play around with various nobs for pair plot here: https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166"},{"metadata":{"_uuid":"b318c8475bfde04ab6a075d05f51c1373880a8d0"},"cell_type":"markdown","source":"**Linear Regression:**\n\nLets look at the basica regression algorithm, linear regression. Our aim is to plot the values vs predicted values from linear regression."},{"metadata":{"trusted":true,"_uuid":"9b7c2c98f1df52308ab64375d0077079d69ae334"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndata1=data[data['class'] == \"Abnormal\"]\n\nlinear_Reg = LinearRegression()\nx = np.array(data1.loc[:, 'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:, 'sacral_slope']).reshape(-1,1)\n\nlinear_Reg.fit(x, y)\ny_head = linear_Reg.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9a121518fccceb9549f01f19b7ac01cc1b23b0c"},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.scatter(x, y, color=\"green\")\nplt.plot(x, y_head, color=\"black\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacrel_scope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c57f7dca64cf5e04f883ceb94cf70b2aeeb74db4"},"cell_type":"markdown","source":"Based on above plot, the black line is the prediction from linear regression and the green circles are the actual values for correlation between pelvic_incidence and sacrel_scrope. \n\nLets take a another example for linear regression, this time looking at relation between: pelvic_tilt_numeric and lumbar_lordosis_angle."},{"metadata":{"trusted":true,"_uuid":"679c2b2ec884ccce93f05982b913965ca3893a6d"},"cell_type":"code","source":"linear_Reg = LinearRegression()\nx = np.array(data1.loc[:, 'pelvic_tilt numeric']).reshape(-1,1)\ny = np.array(data1.loc[:, 'lumbar_lordosis_angle']).reshape(-1,1)\n\nlinear_Reg.fit(x, y)\ny_head = linear_Reg.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fbf61adaf3b51817aec1ccac7d27e4d871287d70"},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.scatter(x, y, color=\"green\")\nplt.plot(x, y_head, color=\"black\")\nplt.xlabel(\"pelvic_tilt numeric\")\nplt.ylabel(\"lumbar_lordosis_angle\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1a1eca68f4985df98389ea1c007406d83e50bce"},"cell_type":"markdown","source":"**Logistic Regression:**\n\nLogistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature, meaning there are only two possible outcomes or classes. It computes the probability of an event occurring. \n\nIt is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. \n\nLinear Regression Equation:\n\n![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image1_ga8gze.png)\n\ny: dependent variable\nx: features\n\nSigmoid function: \n![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image3_qldafx.png)\n\nApply sigmoid function on linear regression:\n\n![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281880/image3_qldafx.png)\n\nThe above equation is used for logistic regression.\n\nLogistic Regression is estimated using maximum likelihood estimation approach. In this approach, we determine the parameters that are mostly likely to produce the desired outcome. MLE sets the mean and variance parameters in determining the specific parameter value for a given model. This set of parameters can be used for predicting the data needed in a normal distribution.\n\nTypes of logisitc regression:\n- Binary logistic regression: the target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No cancer.\n- Multinominal logistic regression: the target variable has three or more nominal categories such as predicting the type of wine. \n- ordinal logistic regression: the target variable has three or more ordinal categories such as a restaurant or product rating from 1 to 5. \n\nYou can read more about logisitc regression here: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n"},{"metadata":{"trusted":true,"_uuid":"53293729f559571648ac5b4cebbea8b44c593d34"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a4e0f38f0433e2f68c543035d6c2d69980ab90c"},"cell_type":"code","source":"#split dataset in features and target variable\nfeature_cols = ['pelvic_incidence', 'pelvic_tilt numeric', 'lumbar_lordosis_angle',\n       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis']\n\nX = data[feature_cols] # Features\ny = data['class'] # Target variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae09eca32cd676fbe4bb6eec541b3c7edc7453b6"},"cell_type":"markdown","source":"Lets split the data into a training set and testing set by using model selection from sklearn package. "},{"metadata":{"trusted":true,"_uuid":"8416bdd6251d701bf679495d913bd82b6b937a9c"},"cell_type":"code","source":"# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec354eaf088062cdf6736f2ed21fb5aaac8e4bff"},"cell_type":"code","source":"# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\n\n# predict\ny_pred = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2917984df8a460aed3fed88d115fc193f081080d"},"cell_type":"markdown","source":"**Model Evaluation using Confusion Matrix**\n\nA confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."},{"metadata":{"trusted":true,"_uuid":"c36563fae9dbaf4ca93798bc942bb6b3f77e4a56"},"cell_type":"code","source":"# import the metrics class\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac6098b1bf525ee35ceedb2b1a41af7203665c2c"},"cell_type":"markdown","source":"Here. you can see the confusion matrix in the form of the array object. The dimension of this matrix is a 2 by 2 because this model is binary classification. You can have two classes: Abnormal and Normal. \n\nDigonal values represent accurate predictions ( 46, 17) and non-diagonal values are inaccurate predictions (7, 8).\n\n**Visualizing the confusion matrix using heatmap:**\n\nLet's visualize the results of the model in  the form of a confusion matrix using matplotlib and seaborn. "},{"metadata":{"trusted":true,"_uuid":"b89eda1900fef8482dcd21cb4b9ce6808d632c89"},"cell_type":"code","source":"class_names=[\"Abnormal\", \"Normal\"] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17117915c1929beb5ada8d9d90714b94cba03458"},"cell_type":"markdown","source":"**Confusion Matrix Evaluation Metrics**\n\nLet's evaluate the model using model evaluation metrics such as accuracy."},{"metadata":{"trusted":true,"_uuid":"a371a28f61e0a0b77031a2a7fafe6ca969b9cbe7"},"cell_type":"code","source":"print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred, labels=['Normal', 'Abnormal'], average=None))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred, labels=['Normal', 'Abnormal'], average=None))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b6daffb3ab869ce181bdb4065484dad48fbe976"},"cell_type":"markdown","source":"Lets look at the values:\n\nAccuracy of 80%. Our logistic regression model is accurate 80% of time. \n\nPrecision:\nIt expresses the proportion of the data points our model says was relevant actually was relevant. For our use-case, for the class Normal, it was precise 68% of time. For abnormal class, it was precise 86% of time.\n\nRecall:\nIt expresses  the ability of the model to find all relevant instances in a dataset. For our use-case, for the class Normal, it was able to find the class 70% of  time. For the class Abnormal, it was able to find the class 85% of the time. "},{"metadata":{"_uuid":"9ab73cf4dba125890b6778bdfdeaee8320ade7be"},"cell_type":"markdown","source":"**Precision - Recall:**\n    \nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n\n While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.\n \n More information on precision-recall can found here: https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"},{"metadata":{"_uuid":"2d8b5b229075520a52d17ee6199aaa105f86e97c"},"cell_type":"markdown","source":"**ROC Curve (Receiver operating characertisitic):**\n\nIt is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity. "},{"metadata":{"trusted":true,"_uuid":"0931ea28159ee9410e6f9686c8239f0a9b1d08c5"},"cell_type":"code","source":"y_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba, pos_label=\"Abnormal\")\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0cf156928aee47a8a6bbaf34d3201769c2a7d6b"},"cell_type":"code","source":"y_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba, pos_label=\"Normal\")\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1076ba513f9e3c46d0c9557ab0691ed4cc3fa0fd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}