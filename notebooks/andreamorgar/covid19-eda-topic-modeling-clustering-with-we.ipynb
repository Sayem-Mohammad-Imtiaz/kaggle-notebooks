{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COVID19 DATA IMPORT"},{"metadata":{},"cell_type":"markdown","source":"We are using the code created by Simo Bouss for data import"},{"metadata":{},"cell_type":"markdown","source":" Loading libreries"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport pyLDAvis.sklearn\nimport json\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\"ggplot\")\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nimport string  \nimport spacy\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom spacy.lang.en import English\n\nimport sklearn.metrics as metrics\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom sklearn.neighbors.nearest_centroid import NearestCentroid\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nimport importlib\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the directory"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"with open('/kaggle/input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n    data = f.read()\n    print(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Extract all the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dirs = ['/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/',\n        '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/',\n        '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/',\n        '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/']\n\nfilenames=[]\ndocs =[]\nfor d in dirs:\n    for file in os.listdir(d):\n        filename = d +file\n        j = json.load(open(filename, 'rb'))\n        \n        paper_id =j['paper_id']\n        #date =j['date']\n        title = j['metadata']['title']\n        authors = j['metadata']['authors']\n        list_authors =[]\n        for author in authors:\n            if(len(author['middle'])==0):\n                middle =\"\"\n            else :\n                middle = author['middle'][0]\n            _authors =author['first']+ \" \"+ middle +\" \"+ author['last']\n            list_authors.append(_authors)\n            \n        try :\n            abstract =  j['abstract'][0]['text']\n        except :\n            abstract =\" \"\n        \n        full_text =\"\"\n        for text in  j['body_text']:\n            full_text += text['text']\n        \n        docs.append([paper_id,title,list_authors,abstract,full_text])\n\ndf = pd.DataFrame(docs,columns=['paper_id','title','list_authors','abstract','full_text'])\ndf.to_csv('/kaggle/working/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA & Text Cleaning\n\nFirst we're going to eliminate numbers, we're going to make everything small, and we're going to eliminate punctuation marks"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf['abstract_cleaned'] = df['abstract'].apply(lambda x: x.lower())\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: x.translate(string.punctuation))\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: x.translate(string.digits))\n\ndf['full_text_cleaned'] = df['full_text'].apply(lambda x: x.lower())\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: x.translate(string.punctuation))\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: x.translate(string.digits))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we remove stop words and those that are very common or very rare."},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_nlp = spacy.load('en_core_web_sm')\n\nspacy_nlp.Defaults.stop_words |= {\"et\", \"al\", \"novel\", \"data\", \"study\", \"studies\", \"research\", \"authors\"}\n\nspacy_stopwords = spacy_nlp.Defaults.stop_words\n\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (spacy_stopwords)]))\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (spacy_stopwords)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see which words are the most common in the abstract and full text and try to eliminate them if they are not descriptive for the problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(df['abstract_cleaned'], 20)\n    \ndf2 = pd.DataFrame(common_words, columns = ['abstract_cleaned' , 'count'])\ndf2.groupby('abstract_cleaned').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in abstract')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = get_top_n_words(df['full_text_cleaned'], 20)\n    \ndf2 = pd.DataFrame(common_words, columns = ['full_text_cleaned' , 'count'])\ndf2.groupby('full_text_cleaned').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in full text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nIn this section of the notebook we will try to visualize the data in an appropriate way to try to obtain relevant information from it that will help in the subsequent process. \n\n## Tag Cloud\n\nFor tag cloud the first step is add the Word Count Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['abstract_word_count'] = df['abstract_cleaned'].apply(lambda x: len(x.strip().split()))\ndf['body_word_count'] = df['full_text_cleaned'].apply(lambda x: len(x.strip().split()))\ndf.head()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# later in this work we found duplicates with differents id's we drop them by title\n# we keep first as biorxiv papers has more information and abstractS\ndf = df.drop_duplicates(subset='title', keep='first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in ['abstract_cleaned','title','full_text_cleaned']:\n    total_words = df[key].values\n    wordcloud = WordCloud(width=1800, height=1200).generate(str(total_words))\n    plt.figure( figsize=(30,10) )\n    plt.title ('Wordcloud' + key)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boxplot: Word Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['abstract_word_count']].plot(kind='box', title='Boxplot of Word Count', figsize=(10,6))\nplt.show()\n\ndf[['body_word_count']].plot(kind='box', title='Boxplot of Word Count', figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['abstract_word_count'].mean(),df['abstract_word_count'].std())\nprint(df['body_word_count'].mean(),df['body_word_count'].std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Modelling with countvectorizer and lda\n\nTo try to answer the tasks we are going to get the topics discussed in the papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"features  = 5000\n# TODO: probar con TFIDF\ntf_vectorizer = CountVectorizer(max_features=features, stop_words='english', min_df=10)\nX_tf = tf_vectorizer.fit_transform(df['abstract'])\ntf_feat_name = tf_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = 7\nlda_model = LatentDirichletAllocation(learning_method='online',random_state=23,n_components=topics)\nlda_output =lda_model.fit_transform(X_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing for plotting pyLDAvis\n%matplotlib inline\npyLDAvis.enable_notebook()\n# plotting lda\npyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer)\n# if you want to save it \n# P=pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer)\n# pyLDAvis.save_html(p, 'lda.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# func from Anish Pandey\ndef visualizing_topic_cluster(model,stop_len,feat_name):\n    topic={}\n    for index,topix in enumerate(model.components_):\n        topic[index]= [feat_name[i] for i in topix.argsort()[:-stop_len-1:-1]]\n    \n    return topic\n\ntopic_lda =visualizing_topic_cluster(lda_model,10,tf_feat_name)\n# printing \nlen([print('Topic '+str(key),topic_lda[key]) for key in topic_lda])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dirty as fuck, removing words that appears in same topic\nimport copy\ntopic_lda_2 = topic_lda\nfor key in topic_lda_2:\n    for element in topic_lda_2[key]:\n        if element in ['cell','cells','viral','virus','respiratory','study','infection','acute']:\n            topic_lda_2[key].remove(str(element))\n[print('Topic '+str(key),topic_lda_2[key]) for key in topic_lda_2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see if our topics are correlated, first mixing topics with our df\ncolumns=['Topic'+ i for i in list(map(str,list(topic_lda.keys())))]\nlda_df =pd.DataFrame(lda_output,columns=columns).apply(lambda x : np.round(x,3))\nlda_df['Major_topic'] =lda_df[columns].idxmax(axis=1).apply(lambda x: int(x[-1]))\nlda_df['keyword'] = lda_df['Major_topic'].apply(lambda x: topic_lda[x])\nlda_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting results\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(abs(lda_df[columns].corr()),annot=True,fmt ='0.2f',cmap=\"YlGnBu\")\nplt.title(\" Correlation Plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering\n\nWe are going to create a clustering model to check if the clusters are correlated with the obtained topics. First we get the features based on the tf-idf weight of the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"features  = 5000\ntfidf_vectorizer = TfidfVectorizer(max_features=features, stop_words=\"english\")\ntfidf_vectorizer.fit(df['abstract'])\nfeatures_tf_idf = tfidf_vectorizer.transform(df['abstract'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we train and obtain the clusters, we will use in a first version the number of topics and later we will do experiments to refine the k number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = features_tf_idf\ncls = MiniBatchKMeans(n_clusters=7, random_state=0)\nY = cls.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to add visualization of the clusters, first we will have to reduce the dimensionality."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for reducing (PCA) and visualizing cluster results\ndef reducing_and_visualizing_cluster_results(X,Y,centers):\n    # reduce the features to 2D\n    pca = PCA(n_components=2)\n    reduced_features = pca.fit_transform(X)\n    # reduce the cluster centers to 2D\n    reduced_cluster_centers = pca.transform(centers)\n    plt.scatter(reduced_features[:,0], reduced_features[:,1], c=Y)\n    plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=200, c='b')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reducing_and_visualizing_cluster_results(X.toarray(),Y,cls.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering using word embeddings (WE).\n\nIt can be seen that results using tf-idf do not look pretty promising. Now we are going to try with word embeddings-based feature vectors. First, we are going to train and obtain the WE for the abstracts. We will use the Word2Vec model and CBOW architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for training and creating word embedding model\ndef create_word_embedings(texts,method):\n    documents=[]\n    texts = clean_text(texts)\n    for line in texts:\n        documents.append(line.split())\n    # build vocabulary and train model\n    if(str(method)==\"cbow\"):\n        model = Word2Vec(documents, size=50, window=5, min_count=5, workers=10)\n    else:\n        model = Word2Vec(documents, size=50, window=5, min_count=5, workers=10,sg=1)\n    model.train(documents, total_examples=len(documents), epochs=10)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for cleaning and homogenizing the texts\ndef clean_text(text):\n    text = text.apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', ' ', x))\n    text = text.apply(lambda x: re.sub(' +',' ', x))\n    text = text.apply(lambda x: x.lower())\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = df['abstract']\nmodel = create_word_embedings(texts,'cobw')\nword_vectors = model.wv\ndel model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we get the most similar words (top=10) to \"coronavirus\" word."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors.most_similar(positive = 'coronavirus', topn=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering with AgglomerativeClustering and WE."},{"metadata":{},"cell_type":"markdown","source":"Once the WE have been created, we are going to create clusters of words in order to detect clusters of topics. First, we will try using AgglomerativeClustering algorithm. Before creating the model, we are going to perform a quick analysis to obtain the optimal number of clusters (k). In this case, we will use the Silhouette Method."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [token for token in word_vectors.vocab]\nX = [word_vectors[token] for token in word_vectors.vocab]\n(len(X),len(tokens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_list = []\nk_list = ['2','3','4','5','6','7','8','9','10','15','20','25','30']\nfor p in k_list:\n    clusterer = AgglomerativeClustering(n_clusters=int(p), linkage='ward')\n    clusterer.fit(X)\n    # The higher (up to 1) the better\n    s = round(metrics.silhouette_score(X, clusterer.labels_), 4)\n    silhouette_list.append(s)\n    # The higher (up to 1) the better\n\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,silhouette_list)\n#plt.plot(['3','3'], [0,0.12], color=\"red\", linewidth=2.5, linestyle=\"--\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the highest value apart k=2 is reached for k=5. In this case, we chose k=5 as the optimal number of clusters. We have tried with different configurations of metrics and linkages and the better results were obtained with the Euclidean distance and the Ward method.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"clusterer = AgglomerativeClustering(n_clusters=5,  linkage='ward')\nY = clusterer.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use the NearestCentroid classifier to get the centroids of each cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"cls_NC = NearestCentroid()\ncls_NC.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reducing_and_visualizing_cluster_results(X,Y,cls_NC.centroids_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that words of the same cluster are close to each other, although there are still overlaps. Thus we are going to try to improve the results using another clustering algorithm."},{"metadata":{},"cell_type":"markdown","source":"Getting the five most similar words for each centroid."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_NC.centroids_]\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering with KMeans and WE"},{"metadata":{},"cell_type":"markdown","source":"As in the previous case, the first step will be to obtain the optimal number of clusters. In this case, we are going to use the Elbow Method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function returns WSS score for a list of k values\ndef calculate_WSS(points,func):\n  sse = []\n  k_list = ['2','3','4','5','6','7','8','9','10','15','20','25','30']\n  module = importlib.import_module('sklearn.cluster')\n  function = getattr(module, func)\n  for k in k_list:\n    kmeans = function(n_clusters = int(k)).fit(points)\n    centroids = kmeans.cluster_centers_\n    pred_clusters = kmeans.predict(points)\n    curr_sse = 0\n    \n    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n    for i in range(len(points)):\n      curr_center = centroids[pred_clusters[i]]\n      curr_sse += np.linalg.norm(np.array(points[i])-np.array(curr_center)) ** 2\n      \n    sse.append(curr_sse)\n  return sse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = calculate_WSS(X,'KMeans')\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,sse)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have chosen k=3 as the optimal number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"cls_KM =  KMeans(n_clusters=3, n_jobs=4, verbose=10)\ncls_KM.fit(X)\nY = cls_KM.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reducing_and_visualizing_cluster_results(X,Y,cls_KM.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results look quite promising. Following we get the five most representative words for each cluster taking into account the Word2Vec model previously created."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_KM.cluster_centers_]\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering with MiniBathKMeans and WE."},{"metadata":{},"cell_type":"markdown","source":"Again, the first step will be to obtain the optimal number of clusters. We are going to use the Elbow Method as in the previous case."},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = calculate_WSS(X,'MiniBatchKMeans')\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,sse)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We chose k=3."},{"metadata":{"trusted":true},"cell_type":"code","source":"cls_MBKM = MiniBatchKMeans(n_clusters=3)\ncls_MBKM.fit(X)\nY = cls_MBKM.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reducing_and_visualizing_cluster_results(X,Y,cls_MBKM.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, the clustering looks slightly worse than the above case. Notwithstanding, the results still look well. Eventually, we get the representative words for each cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_MBKM.cluster_centers_]\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results obtained using WE clearly outperform the results reached by TF-IDF. However, there are some ideas that could be interesting in order to improve topic analysis:\n\n* At first glance, we can see that the pre-processing step can be improved (cleaning, filtering, etc.).\n* Considering the use of n-grams for texts analyzing.\n* Using another sort of model (Doc2Vec, FastText) for text representation in a low-dimensional space.\n* Improving the method of selecting the most representative words.\n* Considenring the extraction of association rules per detected topic."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}