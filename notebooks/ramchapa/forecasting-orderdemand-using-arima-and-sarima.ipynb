{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the Kaggle \"../input/\" directory.\nimport os\nfor dirname,_,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print (os.path.join(dirname,filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd # Data handling and managing\nimport numpy as np  # Handiling linear Algera\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf = pd.read_csv('../input/productdemandforecasting/Historical Product Demand.csv', parse_dates=['Date'])\ndf.head(100) # Getting the first 100 rows to view the records\n#df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for all the date types and nature.\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the columns which got has the NaN values\nprint(df.isnull().any().sum(), ' / ', len(df.columns))\n# Check any number of data points with NaN\nprint(df.isnull().any(axis=1).sum(),'/', len(df))\n#print(df.isnull().any(axis=1).sum(), ' / ', len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check which column has null values.\nprint (df.isna().sum())\n\n#Print the Null Value to Dataset Ratio for the column obtained in the above line of code\nprint ('Null to Dataset Ratio for \"Dates\" Column '': ',df.isnull().sum()[3]/df.shape[0]*100)\n#So, its an clear indcation that There are missing values in Dates and the ratio is 1 %.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the number of missing values are about 1%, So i will be removing them for cleaner workble data. \n#df.dropna(axis=0, inplace=True) #remove all rows with na's.\n#df.reset_index(drop=True)\n#df.sort_values('Date')[10:20] #Some of the values have () in them.\n\ndf.dropna(axis=0, inplace=True) #Remove all the rows with na's\ndf.reset_index(drop=True)\ndf.sort_values('Date')[1:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can notice Some of the values have () in them for \"Order_Demand\" column, which have to remove.\n#Removing () from the \"Order_Demand\" Column\ndf['Order_Demand']=df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand']=df['Order_Demand'].str.replace(')',\"\")\ndf.head(100)\n#Since the \"()\" has been removed , Now i Will change the data type.\n\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the Hieghest and lowest dates in the dataset.\ndf['Date'].min() , df['Date'].max()\n#There is data for 6 years.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm, skew #Import Norm and skew for some statistics\nfrom scipy import stats #Import stats\nimport statsmodels.api as sm #for decomposing the trends, seasonality etc.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX #for the Seasonal Forecast\n\n\n#Lets check the ditribution of the target variable (Order_Demand)\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 10,5\n\nsn.distplot(df['Order_Demand'],fit=norm)\n\n#Get the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['Order_Demand'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## In case we need Data Normilization, We can use Log Values or use Box Cox. Pick the one that looks MOST like a normal distribution.\n#for i in [1,2,3,4,5,6,7,8]:\n #   plt.hist(df['Order_Demand']**(1/i), bins= 40, normed=False)\n  #  plt.title(\"Box Cox transformation: 1/{}\". format(str(i)))\n   # plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Considering Warehouse, Product Category columns for UniVariate Analysis.\ndf['Warehouse'].value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I will get the amount of orders shipped by each warehouse.\ndf.groupby('Warehouse').sum().sort_values('Order_Demand', ascending = False)\n#Warehouse J is clearly shipping most orders. Although S is shipping more quantity within fewer requested orders.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Product Category analysis\nprint(len(df['Product_Category'].value_counts()))\nrcParams['figure.figsize']=50,14\n#sn.countplot(df['Product_Category'].sort_values(ascending=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Bivariate Analysis for WH and PC with Order Demand as Target Variable.\n\n#Step-01: Check the Order Demand Qty by WareHouse\nfrom matplotlib import rcParams\n\nrcParams['figure.figsize']=20,5 #Figure Size in Inches for Plotting\nf, axes = plt.subplots(1,2)\n\nregDataWH=sn.boxplot(df['Warehouse'],df['Order_Demand'],ax=axes[0]) #Create a variable for Regular Data for WH and OD \n\nlogDataWH=sn.boxplot(df['Warehouse'],np.log1p(df['Order_Demand']),ax=axes[1]) #Craete a Variable with Log Transformation\n\ndel regDataWH, logDataWH\n\n#Step-02: Check the Order Demand Qty by Product Category (PC)\nrcParams['figure.figsize']=20,5\nf,axes =plt.subplots(1,2)\n\nregDataPC=sn.boxplot(df['Product_Category'],df['Order_Demand'],ax=axes[0])\nlogDataPC=sn.boxplot(df['Product_Category'],df['Order_Demand'],ax=axes[1])\n\ndel regDataPC, logDataPC\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring the Data as TIME SERIES\n#Step-01: Lets calculate the Total  Order Qty placed on by Each Day\ndf=df.groupby('Date')['Order_Demand'].sum().reset_index()\n#Step-02: Indexing the Date Column as for further procssing.\ndf = df.set_index('Date')\ndf.index #Lets check the index\n#Step-03:#Averages daily sales value for the month, and we are using the start of each month as the timestamp.\nmonthly_avg_sales = df['Order_Demand'].resample('MS').mean()\n#In case there are Null values, they can be imputed using bfill.\nmonthly_avg_sales = monthly_avg_sales.fillna(monthly_avg_sales.bfill())\n#Visualizing time series.\n\nmonthly_avg_sales.plot(figsize=(20,10))\nplt.show()\n\n#Findings: The sales are always low for the beginning of the year and the highest peak in demand every year is in the\n#last quarter. The observed trend shows that orders were higher during 2014-2016 then reducing down slowly.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate the Seasonality , Trend and Residuals with Decomposition Analysis.\n\n#Using Time Series for Decomposition. \nfrom pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 20, 10\ndecomposition = sm.tsa.seasonal_decompose(monthly_avg_sales, model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ARIMA\n\n#An ARIMA model is characterized by 3 terms: p, d, q where these three parameters account for seasonality (p), trend (d), and noise in data (q):\n\n#p is the order of the AR term (number of lags of Y to be used as predictors). If it rained for the last week, it is likely it will rain tomorrow.\n#q is the order of the MA term (moving average).\n#d is the number of differencing required to make the time series stationary. if already stationary d=0.\n#But when dealing with SEASONALITY, it is best to incorporate it as 's'. ARIMA(p,d,q)(P,D,Q)s. Where 'pdq' are non seasonal params and 's' is the perdiocity of the time series. 4:quarter, 12:yearly etc.\n#If a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, short for ‘Seasonal ARIMA’.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid Search and Random Search\n\n#Since ARIMA has hyper params that can be tuned, the objective here is to find the best params using Grid Search.\n\n#GRID SEARCH for Param Tuning.\n#Sample params for seasonal arima. (SARIMAX).\n\n#STEP-01:\n#For each combination of parameters, we fit a new seasonal ARIMA model with the SARIMAX() function \n#from the statsmodels module and assess its overall quality.\n\nimport itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n#print('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX1: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX2: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX3: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX4: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n\n#STEP-02:\n#Get the best params for the data. Choose the lowest AIC.\n\n# The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a \n# given set of data. \n# AIC measures how well a model fits the data while taking into account the overall complexity of the model.\n# Large AIC: Model fits very well using a lot of features.\n# Small AIC: Model fits similar fit but using lesser features. \n# Hence LOWER THE AIC, the better it is.\n\n#The code tests the given params using sarimax and outputs the AIC scores.\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(monthly_avg_sales,\n                                            order=param,\n                                            seasonal_order=param_seasonal,enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the model with the best params.\n#SARIMA(1, 1, 1)x(0, 1, 1, 12)12 - AIC:1351.1631068717465\n\n\n#The above output suggests that ARIMA(1, 1, 1)x(0, 1, 1, 12)12 yields the lowest AIC value: 1351.1631068717465\n#Therefore we should consider this to be optimal option.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nmod = sm.tsa.statespace.SARIMAX(monthly_avg_sales,\n                                order=(1, 1, 1),\n                                seasonal_order=(0, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Analysis of Co-efficiecnt and Standrad Error by interpeting the above Result.\n#coeff: Shows weight/impotance how each feature impacts the time series. \n#Pvalue: Shows the significance of each feature weight. Can test hypothesis using this. If p value is <.05 then they are statitically significant.\n\n#Refresher on null hyp and pvalues. By default we take the null hyp as 'there is no relationship bw them' If p value < .05 (significance level) then you reject the Null Hypthesis If p value > .05 , then you fail to reject the Null Hypothesis.\n\n#So, if the p-value is < .05 then there is a relationship between the response and predictor. Hence, significant.\n\n#Plotting the diagnostics.\n\n#The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.\nresults.plot_diagnostics(figsize=(20, 10))\nplt.show()\n\n#What are the details for analysis and check?\n#1. Residuals SHOULD be Normally Distributed ; Check\n#Top Right: The (orange colored) KDE line should be closely matched with green colored N(0,1) line. This is the standard notation\n#for normal distribution with mean 0 and sd 1.\n#Bottom Left: The qq plot shows the ordered distribution of residuals (blue dots) follows the linear trend of the samples \n#taken from a standard normal distribution with N(0, 1). \n\n#2. #Residuals are not correlated; Check\n#Top Left: The standard residuals don’t display any obvious seasonality and appear to be white noise. \n#Bottom Right: The autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have \n#low correlation with its own lagged versions.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MODEL Evaluation and Analysis\n#Lets get the predictions and confidence interval for those predictions.\n#Get the predictions. The forecasts start from the 1st of Jan 2017 but the previous line shows how it fits to the data.\npred = results.get_prediction(start=pd.to_datetime('2014-05-01'), dynamic=False) #false is when using the entire history.\n#Confidence interval.\npred_ci = pred.conf_int()\n\n#Plotting real and forecasted values.\nax = monthly_avg_sales['2016':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='blue', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Takeaway: The forecats seems to be fitting well to the data. The Blue/purple thicker plot shows the confidence level in the forecasts. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the Forecast Accuracy\n\n#Calculating the mean squared error (average error of forecasts) and the lower Mean Square Error always reflects the better results \ny_forecasted = pred.predicted_mean\ny_truth = monthly_avg_sales['2016-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('MSE {}'.format(round(mse, 2)))\nprint('RMSE: {}'.format(round(np.sqrt(mse), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can make more changes in the time series by using below steps.\npred_uc = results.get_forecast(steps=75)\npred_ci = pred_uc.conf_int()\nax = monthly_avg_sales.plot(label='observed', figsize=(16, 8))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Far out values are naturally more prone to greater variance. \n#The grey area is the confidence we have in the predictions and the corealtes to .","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}