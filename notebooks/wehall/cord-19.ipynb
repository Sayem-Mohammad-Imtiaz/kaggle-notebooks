{"cells":[{"metadata":{"papermill":{"duration":0.013313,"end_time":"2021-01-20T04:53:56.620449","exception":false,"start_time":"2021-01-20T04:53:56.607136","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Using spaCy lemmatization - maybe more human intelligible"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-20T04:53:56.662132Z","iopub.status.busy":"2021-01-20T04:53:56.657037Z","iopub.status.idle":"2021-01-20T04:54:48.096481Z","shell.execute_reply":"2021-01-20T04:54:48.097226Z"},"papermill":{"duration":51.463277,"end_time":"2021-01-20T04:54:48.097382","exception":false,"start_time":"2021-01-20T04:53:56.634105","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import json\nimport string\nimport spacy\nimport math\nimport os\n\ndef sorted_frequencies(doc):\n    frequencies = []\n    for word in doc:\n        entry = next((e for e in frequencies if e['word'] == word), None)\n        if entry is None:\n            frequencies.append({'word': word, 'count': 1})\n        else:\n            entry['count'] += 1\n    frequencies.sort(key=lambda entry: -entry['count'])\n    return frequencies\n\ndef term_frequency(term, sorted_frequencies):\n    entry = next((e for e in sorted_frequencies if e['word'] == term), None)\n    term_freq = entry['count']\n    max_freq = sorted_frequencies[0]['count']\n    return term_freq / max_freq\n\ndef inverse_document_frequency(term, corpus):\n    num_occurring = 0\n    for doc in corpus:\n        if term in doc:\n            num_occurring += 1\n    return math.log2(len(corpus)/num_occurring)\n\ndef tfidf(term, corpus, sorted_frequencies, word_document_frequencies):\n    idf = math.log2(len(corpus)/word_document_frequencies[term])\n    return term_frequency(term, sorted_frequencies) * idf\n\n##############################\n# THIS IS THE ONLY NEW THING #\n##############################\ndef prep_doc_spacy(filepath, nlp_spacy):\n    with open(filepath) as file:\n        doc = json.load(file)\n\n        # get text and lowercase it, then combine\n        text = ' '.join([chunk['text'].lower() for chunk in doc['body_text']])\n        # remove punctuation\n        text = ''.join([char for char in text if char not in string.punctuation])\n        #print('First 100 words - lower case, no puctuation')\n        #print(text[0:100])\n        # tokenize, remove stopwords\n        doc_spacy = nlp_spacy(text)\n        no_stopwords = [word for word in doc_spacy if not word.is_stop]\n        # lemmatization\n        lemmas = [word.lemma_ for word in no_stopwords]\n        #print('First 20 lemmas')\n        #print(lemmas[0:20])\n        return lemmas\n##############################\n\n##############################\n\ndef gen_corpus_frequencies(corpus):\n    # get all words in corpus\n    word_sets = [set(doc) for doc in docs]\n    corpus_words = set()\n    for doc in word_sets:\n        corpus_words.update(doc)\n    # get number of docs in which each word occurs\n    wordcounts = dict.fromkeys(corpus_words)\n    for word in wordcounts:\n        wordcounts[word] = 0\n        for doc in word_sets:\n            if word in doc:\n                wordcounts[word] += 1\n    return wordcounts\n\nlimit = 10\ni = 0\nfilepaths = []\ndirpath = '../input/CORD-19-research-challenge/document_parses/pdf_json'\nfor path in os.listdir(dirpath):\n    i += 1\n    if i > limit:\n        break\n    filepaths.append(os.path.join(dirpath, path))\n\n#print(filepaths)\nnlp_spacy = spacy.load('en_core_web_sm')\ndocs = [prep_doc_spacy(path, nlp_spacy) for path in filepaths]\n\ndoc = docs[0]\n\ndoc_stats = sorted_frequencies(doc)\nword_document_frequencies = gen_corpus_frequencies(docs)\n\nfor entry in doc_stats:\n    entry['tfidf'] = tfidf(entry['word'], docs, doc_stats, word_document_frequencies)\n# include this in doc_stats method?\n\ndoc_stats.sort(key=lambda entry: -entry['tfidf'])\n\nfor entry in doc_stats[0:10]:\n    print(entry['word'], entry['tfidf'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012987,"end_time":"2021-01-20T04:54:48.123751","exception":false,"start_time":"2021-01-20T04:54:48.110764","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Output from cell above:  \nbrazilian 3.321928094887362  \npardo 2.9264604645436285  \nnorth 2.6100863602686415  \nethnicity 2.3728057820624016  \ncentralsouth 1.977338151718668  \nethnic 1.740057573512428  \nregion 1.654252946824958  \nbrazil 1.5818705213749342  \nrio 1.2654964170999474  \nlikely 1.1864028910312008  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}