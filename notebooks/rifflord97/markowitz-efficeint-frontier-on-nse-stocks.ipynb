{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Markowitz Efficient Frontier on NSE Stocks"},{"metadata":{},"cell_type":"markdown","source":"Source: https://towardsdatascience.com/python-markowitz-optimization-b5e1623060f5\nThe credits for this notebook goes to Fábio Neves and his posts on Medium. This is my first notebook and it is primarily for me learning analytics. The following comments etc. may seem too much, but it is keeping in mind that I may use this notebook for future reference.\nThanks for going through this!\n\n## What Is the Efficient Frontier?\n\nThe efficient frontier is the set of optimal portfolios that offer the highest expected return for a defined level of risk or the lowest risk for a given level of expected return. Portfolios that lie below the efficient frontier are sub-optimal because they do not provide enough return for the level of risk. Portfolios that cluster to the right of the efficient frontier are sub-optimal because they have a higher level of risk for the defined rate of return.\n\nI'm utilizing the data from NSE NIFTY50 stocks, to prepare a Markowitz Efficient Frontier.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Explorartion\nThe data is avialable as independent .csv files in the folder, each with its own Open, Close, etc. since 2000. For this problem I need to compile all the closing prices for the stocks into  a single DataFrame -> stocks.\nFor this, I have utilised pands.join() in a loop to merge all the 5 Closing Values into a sinlge Data Frame.\n\nI am limiting the number of stocks to 5 but, it can be extended to use all the 50 stocks.\nThe entire code has been optimised such that adjusting the below value of 5 to the number of stocks you want to include in the portfolio, would allow you too create the effiecient frontier. It requires no other changes.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#The final dataframe\nstocks=pd.DataFrame()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    #just change the 5 to 50 to utilise all 50 stocks\n    for filename in filenames[:5]:\n        data=pd.DataFrame()\n        #reading the csv files one at a time, and querrying 'Date' and 'Close' columns using a lambda\n        data = pd.read_csv(os.path.join(dirname, filename), usecols=lambda x: x in ['Date', 'Close'], parse_dates=True)\n        #renaming the Close column to the Stock-name i.e., filename\n        data.rename(columns = {'Close':(filename.replace('.csv',''))}, inplace = True)\n        data.set_index('Date',inplace=True)\n        data.index = pd.to_datetime(data.index)\n        #join returns a new dataframe, not inplace of old dataframe\n        stocks=stocks.join(data, how='outer')\n\nstocks.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing Data\nI’m going to use logarithmic returns, since it’s more convenient and it takes care of the normalization for the rest of the project. Converting everything to logarithmic returns is simple. Think of it as the log of an arithmetic daily return (which is obtained by dividing the price at day n, by the price at day n-1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_ret = np.log(stocks/stocks.shift(1))\nlog_ret.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We need to prepare a for loop which will simulate several different combinations of the 5 stocks and save their Sharpe ratio. I’m going to use 10000 portfolios. The random seed at the top of the code is making sure I get the same random numbers every time for reproducibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nnum_ports = 10000\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor x in range(num_ports):\n    # Weights\n    weights = np.array(np.random.random(len(stocks.columns)))\n    weights = weights/np.sum(weights)\n    \n    # Save weights\n    all_weights[x,:] = weights\n    \n    # Expected yearly-return\n    ret_arr[x] = np.sum((log_ret.mean() * weights * 252))\n    \n    # Expected volatility\n    vol_arr[x] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov()*252, weights)))\n    \n    # Sharpe Ratio\n    sharpe_arr[x] = ret_arr[x]/vol_arr[x]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us find out the maximum Sharpe Ratio among the given portfolios."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Max Sharpe Ratio in the array: {}\".format(sharpe_arr.max()))\nprint(\"It's location in the array: {}\".format(sharpe_arr.argmax()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Let’s check the allocation weights in that index number for the maximum SR and save the return and volatility figures to use it in the chart later."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_weights[sharpe_arr.argmax(),:])\n\nmax_sr_ret = ret_arr[sharpe_arr.argmax()]\nmax_sr_vol = vol_arr[sharpe_arr.argmax()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the portfolios:\n\nWe have everything we need to plot a chart that compares all combinations in terms of volatility (or risk) and return, colored by Sharpe ratio. \n\nThe red dot is obtained from the previous calculation above and it represents the return and volatility for the simulation with the maximum Sharpe ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='viridis')\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\nplt.scatter(max_sr_vol, max_sr_ret,c='red', s=50) # red dot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The basic bullet shape can be observed in the given plot, which sort of outlines the efficient frontier for the given stocks."},{"metadata":{},"cell_type":"markdown","source":"## Important Functions\n\nget_ret_vol_sr: Returns an array with the average Returns, Volatility and Sharpe Ratio for the given set of weights\nneg_sharpe: Returns the negative SR for the given weights                                                                                                                                             \ncheck_sum: checks if sum of weights is equal to 1. It will return 0 (zero) if the sum is 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ret_vol_sr(weights):\n    weights= np.array(weights)\n    ret=np.sum(log_ret.mean() * weights)*252\n    vol=np.sqrt(np.dot(weights.T, np.dot(log_ret.cov()*252, weights)))\n    sr=ret/vol\n    return np.array([ret, vol, sr])\n\ndef neg_sharpe(weights):\n    return get_ret_vol_sr(weights)[2]*-1\n\ndef check_sum(weights):\n    return np.sum(weights)-1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moving on, we will need to create a variable to include our constraints like the check_sum. We’ll also define an initial guess and specific bounds, to help the minimization be faster and more efficient. Our initial guess will be 1/n th for each stock (or 1/6, in this case), and the bounds will be a tuple (0,1) for each stock, since the weight can range from 0 to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"#constraints\ncons = ({'type':'eq', 'fun': check_sum})\nn_cols=len(log_ret.columns)\n#bounds\nbnds = (((0,1),)*n_cols)\n#initial guess\ninit_guess = [np.repeat((1/n_cols),n_cols)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enter the minimize function. I chose the method ‘SLSQP’ because it’s the method used for most of the generic minimization problems. In case you are wondering, it stands for Sequential Least Squares Programming. Make sure to pass the initial method, the bounds and the constraints with the variables defined above. If we print the variable it will look like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_results = minimize(neg_sharpe, init_guess, method = 'SLSQP', bounds = bnds, constraints = cons)\nprint(opt_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want the key x from the dictionary, which is an array with the weights of the portfolio that has the maximum Sharpe ratio. If we use our function get_ret_vol_sr we get the return, volatility, and sharpe ratio:"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ret_vol_sr(opt_results.x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we got a better Sharpe ratio than we got with the simulation we did before (0.66 vs 0.62 earlier)"},{"metadata":{},"cell_type":"markdown","source":"We’re now ready to check all optimal portfolios, which is basically our efficient frontier. The efficient frontier is the set of portfolios that gets us the highest expected return for any given risk level. Or from another perspective, the minimum amount of risk for an expected return. To trace this line, we can define a variable frontier_y. Going back to the chart above, we can see the maximum return doesn’t go much higher than 0.25, so frontier_y will be defined from 0 to 0.3."},{"metadata":{"trusted":true},"cell_type":"code","source":"frontier_y = np.linspace(0,0.25,200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To finish the plotting of the frontier, we have define one last function that will help us minimize the volatility. It will return the volatility (index 1) of the given weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"def minimize_volatility(weights):\n    return get_ret_vol_sr(weights)[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now the last bit of code to help us get get our x values for the efficient frontier. We use the same code as above with a few changes to the constraints. The for loop is basically going through every possible value in our previously defined frontier_y and obtaining the minimum result (which is the key ‘fun’) of volatility (our x axis in the chart)."},{"metadata":{"trusted":true},"cell_type":"code","source":"frontier_x = []\n\nfor possible_return in frontier_y:\n    cons = ({'type':'eq', 'fun':check_sum},\n            {'type':'eq', 'fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,init_guess,method='SLSQP', bounds=bnds, constraints=cons)\n    frontier_x.append(result['fun'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we can plot the actual efficient frontier by passing the variables frontier_x and frontier_y."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='viridis')\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\nplt.plot(frontier_x,frontier_y, 'r--', linewidth=3)\nplt.savefig('cover.png')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}