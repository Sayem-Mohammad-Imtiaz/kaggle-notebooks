{"cells":[{"metadata":{},"cell_type":"markdown","source":"# College Placement Dataset ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This dataset is related to the placement statistics of an MBA college. \n* It is available on kaggle : https://www.kaggle.com/benroshan/factors-affecting-campus-placement\n* Thanks Ben Roshan D for providing the Dataset\n* Here we use machine learning to predict the placement chances of placement and the salary offered if placed ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## We can get started by importing the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\n# Read your datasets from the installed directory\ndf.head() #prints the first 5 rows of the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the columns in the DataFrame\n* <b>sl_no</b>          : The id no. of the student \n* <b>gender</b>         : The gender of the student\n* <b>ssc_p</b>          : The percentage of marks obtained in SSC (Senior Secondary Certificate)\n* <b>ssc_b</b>          : The board in which the student has studied SSC\n* <b>hsc_p</b>          : The percentage of marks obtained in HSC (Higher Secondary Certificate)\n* <b>hsc_b</b>          : The board in which the student has studied HSC \n* <b>hsc_s</b>          : The subject chosen for HSC\n* <b>degree_p</b>       : The percentage of marks obtained in Degree\n* <b>degree_t</b>       : The subject chosen for Degree\n* <b>workex</b>         : Work Experience of the student \n* <b>etest_p</b>        : Employability Test Percentage\n* <b>specialisation</b> : Specialization chosen in MBA\n* <b>mba_p</b>          : Percentage of marks obtained in MBA\n* <b>status</b>         : The placement status of the student\n* <b>salary</b>         : The salary offered to the students who are placed ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nreport = ProfileReport(df,title='Summary Report of Student Placements')\nreport","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Takeaways from the Profile Report \n* There are <b>32.1%</b> empty values in Salary i.e 32.1% people have not been placed \n* There are <b>7</b> Numerical Variables \n* There are <b>7</b> Categorical Variables\n* There is <b>1</b> Boolean Variable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## EXPLORATORY DATA ANALYSIS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Plotting the salaries of those who scored more than 60% in all their studies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"greater_70 = (df.ssc_p > 70) & (df.hsc_p > 70) & (df.mba_p >70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_70 = df[greater_70]\ndf_70.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This shows that there are only 12 students who scored more than 70 in their studies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_70.salary,bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the no. of students with and without work ex","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['workex'].value_counts() #Since its a categorical variable it does not require normalization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the types of specializations offered and students enrolled","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['specialisation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the types of degree done in undergraduation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['degree_t'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the background of student in their +1 and +2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hsc_s'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Looking at no. of placed and not placed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the gender difference in the batch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Matplotlib to Visualize the Data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the range of Salaries Offered","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['salary'],bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot for SSC_P vs Salary ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df['ssc_p'],df['salary'])\nplt.xlabel('Percentage in SSC')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt SSC Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot for HSC_P vs Salary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df['hsc_p'],df['salary'])\nplt.xlabel('Percentage in HSC')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt HSC Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot for Degree_P vs Salary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df['degree_p'],df['salary'])\nplt.xlabel('Percentage in Degree')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt Degree Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot for MBA_P vs Salary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df['mba_p'],df['salary'])\nplt.xlabel('Percentage in MBA')\nplt.ylabel('Salary Offered')\nplt.title('Salary offered wrt MBA Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The outliers in the dataset have to be removed so that the algorithm can work equally well on new data\n* Therefore we can remove the data where salary is greater than 5,00,000","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['salary'],bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA PREPROCESSING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Checking Wether the numerical data is Normally Distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Tests to check Normality:</b>\n* The Shapiro-Wilk test\n* The Anderson-Darling test\n* The Kolmogorov-Smirnov test","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Visual measures to be implemented:</b>\n* Box Plots\n* QQ Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Why is Normality Required:</b>\n* It is a (a bit strongly stated) fact that formal normality tests always reject on the huge sample sizes we work with today. It’s even easy to prove that when n gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of randomness, no single dataset will be a perfectly normally distributed sample. But in applied statistics the question is not whether the data/residuals … are perfectly normal, but normal enough for the assumptions to hold.\n* As we can see from the code below, the Shapiro-Wilk test has <b>rejected normality for MBA Percentage</b>. Therefore, we might have to use some additional measure to see if the null hypothesis for MBA Percentage should indeed be rejected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\ndegree_p = stats.norm.rvs(df['degree_p'])\nssc_p = stats.norm.rvs(df['ssc_p'])\nhsc_p = stats.norm.rvs(df['hsc_p'])\nmba_p = stats.norm.rvs(df['mba_p'])\nsalary = stats.norm.rvs(df['salary'])\netest_p = stats.norm.rvs(df['etest_p'])\nprint(\"Stat for degree:\", stats.shapiro(degree_p)) # Null Accepted\nprint(\"Stat for ssc:\", stats.shapiro(ssc_p)) # Null Accepted\nprint(\"Stat for hsc:\", stats.shapiro(hsc_p)) # Null Rejected\nprint(\"Stat for mba:\", stats.shapiro(mba_p)) # Null Accepted\nprint(\"Stat for salary:\", stats.shapiro(salary)) # Null Accepted \nprint(\"Stat for etest:\", stats.shapiro(etest_p)) # Null Rejected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above tests prove that the data is not normal therefore we can scale the Data\n* This can be done using StandardScaler from scikit-learn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncolumns = df[['ssc_p','hsc_p','degree_p','mba_p','etest_p']]\nx_scaled = pd.DataFrame(scaler.fit_transform(columns))\nx_scaled.columns = ['ssc_p','hsc_p','degree_p','mba_p','etest_p']\nx_scaled.reset_index(drop=True, inplace=True)\nx_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding for all the Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cat = df[['gender','ssc_b','hsc_b','hsc_s','degree_t','specialisation']]\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx_cat['gender'] = le.fit_transform(x_cat.gender)\nx_cat['ssc_b'] = le.fit_transform(x_cat.ssc_b)\nx_cat['hsc_b'] = le.fit_transform(x_cat.hsc_b)\nx_cat['hsc_s'] = le.fit_transform(x_cat.hsc_s)\nx_cat['degree_t'] = le.fit_transform(x_cat.degree_t)\nx_cat['specialisation'] = le.fit_transform(x_cat.specialisation)\nx_cat.reset_index(drop=True, inplace=True)\nx_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What are we predicting :\n* We are trying to predict the chance of a person getting a placement \n* Therefore we have to make the training and testing sets accordingly ","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"x = pd.concat([x_cat,x_scaled],join='outer',axis=1)\nx.isnull().sum()\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = le.fit_transform(df.status)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data into Training and Testing sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\n\nx_train,x_test,y_train,y_test = tts(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Classification Algorithms for prediction ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### LOGISTIC REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=42)\nlr.fit(x_train,y_train)\ny_pred = lr.predict(x_test)\nlrscore = lr.score(x_test,y_test)\nlrscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNEIGHBORS CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\nknc.fit(x_train,y_train)\ny_pred = knc.predict(x_test)\nkncscore = knc.score(x_test,y_test)\nkncscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### DECISION TREE CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\ndtr = DecisionTreeClassifier(random_state=42)\ndtr.fit(x_train,y_train)\ny_pred = dtr.predict(x_test)\n\ndtcscore =  metrics.accuracy_score(y_test,y_pred)\nprint(f'Decision Tree Classification Score = {dtcscore:4.1f}%\\n')\nprint(f'Classification Report:\\n {metrics.classification_report(y_test, y_pred)}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RANDOM FOREST CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfr = RandomForestClassifier(n_estimators=10,random_state=42)\nrfr.fit(x_train,y_train)\n\nfrom sklearn import metrics\n\npredicted = rfr.predict(x_test)\nrfcscore =  metrics.accuracy_score(y_test, predicted)\nprint(f'Random Forest Classification Score = {rfcscore:4.1f}%\\n')\nprint(f'Classification Report:\\n {metrics.classification_report(y_test, predicted)}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RIDGE CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\nrc = RidgeClassifier(random_state=42)\nrc.fit(x_train,y_train)\nl_pred = rc.predict(x_test)\nrcscore = rc.score(x_test,y_test)\nrcscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STOCHASTIC GRADIENT DESCENT CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nSGDC = SGDClassifier(random_state=42)\nSGDC.fit(x_train,y_train)\nresult = SGDC.predict(x_test)\nsgdcscore = SGDC.score(x_test,y_test)\nsgdcscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PERCEPTRON","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Perceptron\np = Perceptron(random_state=42)\np.fit(x_train,y_train)\nresult = p.predict(x_test)\npscore = p.score(x_test,y_test)\npscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PASSIVE AGRESSIVE CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\npac = PassiveAggressiveClassifier(random_state=42)\npac.fit(x_train,y_train)\nresult_pac = pac.predict(x_test)\npacscore = pac.score(x_test,y_test)\npacscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SUPPORT VECTOR CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC(random_state=42)\nsvc.fit(x_train,y_train)\ny_pred = svc.predict(x_test)\nsvcscore = svc.score(x_test,y_test)\nsvcscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### BAGGING CLASSIFIER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nbc = BaggingClassifier(random_state=43)\nbc.fit(x_train,y_train)\ny_pred = bc.predict(x_test)\nbcscore = bc.score(x_test,y_test)\nbcscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'Algorithms Used': ['Logistic Regression','K Neighbors Classifier','Decision Tree Classifier','Random Forest Classifier',\n                         'Ridge Classifier','Stochastic Gradient Descent','Perceptron','Passive Aggressive Classifier',\n                        'Support Vector Classifier','Bagging Classifier'],\n    'Accuracy Achieved': [lrscore,kncscore,dtcscore,rfcscore,rcscore,sgdcscore,pscore,pacscore,svcscore,bcscore]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_df = pd.DataFrame(d)\nAccuracy_df = Accuracy_df.sort_values(by=['Accuracy Achieved'],ascending=False)\nAccuracy_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have achieved the highest accuracy using <b>PASSIVE AGGRESSIVE CLASSIFIER</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## FEATURE SELECTION  (MODEL OPTIMIZATION)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We will continue with the Passive Aggressive Classifier for Future processes\n* In feature selection we will understand which variable affects the result the most ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Wrapper Methods - RECURSIVE FEATURE ELIMINATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.feature_selection import RFE\n\npac = PassiveAggressiveClassifier(random_state=42)\nrfe = RFE(pac,1)\nrfe.fit(x_train,y_train)\nfor var, name in sorted(zip(rfe.ranking_,x), key=lambda x: x[0]):\n    print(f'{name:>18} rank = {var}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EVALUATION METRICS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The major evaluation metrics used for a classification problem are \n* <b>Accuracy Score\n* Classification Report\n* Confusion Matrix</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = metrics.confusion_matrix(y_test,result_pac)\nreport = metrics.classification_report(y_test,result_pac)\nprint(f'Classification Report:\\n {metrics.classification_report(y_test,result_pac)}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL SELECTION (HYPERPARAMETER TUNING)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom time import time\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\n# Start clock\nstart = time()\n\npac = PassiveAggressiveClassifier()\nskf = StratifiedKFold(n_splits=10)\n\nfit_intercept = [True]\nvalidation_fraction = [0.1,0.2,0.3,0.4,0.5,0.6]\nloss = ['hinge','squared_hinge']\nrandom_state = [42,33]\nclass_weight = ['weight','balanced',None]\n\n# Create a dictionary of hyperparameters and values\nparams = {'fit_intercept':fit_intercept, 'validation_fraction':validation_fraction,'loss':loss,'random_state':random_state,'class_weight':class_weight}\n\n# Number of random parameter samples\nnum_samples = 20\n\n# Run randomized search\nrscv = RandomizedSearchCV(pac, param_distributions=params, n_iter=num_samples, random_state=23)\n\n# Fit grid search estimator and display results\nrscv.fit(x_train, y_train)\n\nprint(f'Compute time = {time() - start:4.2f} seconds', end='')\nprint(f' for {num_samples} parameter combinations')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get best esimtator\nbe = rscv.best_estimator_\n\n# Display parameter values\nprint(f'Best fit_intercept={be.get_params()[\"fit_intercept\"]:5.4f}')\nprint(f'Best validation_fraction={be.get_params()[\"validation_fraction\"]}')\nprint(f'Best loss={be.get_params()[\"loss\"]}')\nprint(f'Best Class_Weight={be.get_params()[\"class_weight\"]}')\n\n# Display best score\nprint(f'Best CV Score = {rscv.best_score_:4.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}