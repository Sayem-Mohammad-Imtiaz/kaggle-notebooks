{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Birdsong Pytorch Baseline: ResNeSt50-fast (Inference)"},{"cell_type":"markdown","metadata":{},"source":"This is my attempt to refactor and clean [this](https://www.kaggle.com/ttahara/inference-birdsong-baseline-resnest50-fast) excellent notebook. It's work in progress and I'll update it as I go. Am doing this mainly for my own understanding, and am sharing in case it's of any use to anyone else.\n\nType-checking, style checking, and formatting done with `black`, `flake8`, `isort`, and `mypy` via [nbQA](https://github.com/nbQA-dev/nbQA)\n\nConfigurations used:\n\n`.pre-commit-config.yaml`:\n\n```yaml\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 0.1.22\n    hooks:\n      - id: nbqa\n        args: ['black']\n        name: nbqa-black\n        additional_dependencies: ['black']\n      - id: nbqa\n        args: ['flake8']\n        name: nbqa-flake8\n        additional_dependencies: ['flake8']\n        alias: nbqa-flake8\n      - id: nbqa\n        args: ['isort']\n        additional_dependencies: ['isort']\n      - id: nbqa\n        args: ['blackdoc']\n        name: nbqa-blackdoc\n        additional_dependencies: ['blackdoc']\n      - id: nbqa\n        args: ['mypy']\n        name: nbqa-mypy\n        alias: nbqa-mypy\n        additional_dependencies: ['mypy']\n      - id: nbqa\n        args: ['pydocstyle']\n        name: nbqa-pydocstyle\n        additional_dependencies: ['pydocstyle']\n```\n\n`.nbqa.ini`:\n\n```ini\n[black]\naddopts = --line-length=96\nmutate = 1\n\n[flake8]\nconfig=.flake8\n\n[isort]\naddopts = --profile=black\nmutate = 1\n\n[blackdoc]\naddopts = --line-length=96\nmutate = 1\n\n[mypy]\naddopts = --ignore-missing-imports --disallow-untyped-defs\n\n[pydocstyle]\naddopts = --add-ignore=D100,D101,D105,D103,D107\n```"},{"cell_type":"markdown","metadata":{},"source":"## About\n\nIn this notebook, I try ResNeSt, which is the one of state of the art in image recognition.  \n\nThis is a notebook for **_inference & submission_**. I shared training process as another one:  \nhttps://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast  \nIf you want to know experimental details, see it.\n\nMost of this notebook consists of [great baseline](https://www.kaggle.com/hidehisaarai1213/inference-pytorch-birdcall-resnet-baseline) shared by @hidehisaarai1213 .  \nThank you for sharing !"},{"cell_type":"markdown","metadata":{},"source":"## Prepare"},{"cell_type":"markdown","metadata":{},"source":"### import libraries"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"!pip install ../input/resnest50-fast-package/resnest-0.0.6b20200701/resnest/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"outputs":[],"source":"import os\nimport random\nimport time\nimport typing as tp\nimport warnings\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\nimport cv2\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport resnest.torch as resnest_torch\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nfrom fastprogress import progress_bar\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500"},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":"### define utilities"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def set_seed(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\n@contextmanager\ndef timer(name: str) -> tp.Iterator[None]:\n    \"\"\"Timer util.\"\"\"\n    t0 = time.time()\n    print(\"[{}] start\".format(name))\n    yield\n    print(\"[{}] done in {:.0f} s\".format(name, time.time() - t0))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"set_seed(1213)"},{"cell_type":"markdown","metadata":{},"source":"### read data"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\nTEST_AUDIO_DIR = RAW_DATA / \"test_audio\""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"train = pd.read_csv(RAW_DATA / \"train.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"if not TEST_AUDIO_DIR.exists():\n    TEST_AUDIO_DIR = INPUT_ROOT / \"birdcall-check\" / \"test_audio\"\n    test = pd.read_csv(INPUT_ROOT / \"birdcall-check\" / \"test.csv\")\nelse:\n    test = pd.read_csv(RAW_DATA / \"test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":"test.head()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"sub = pd.read_csv(\"../input/birdsong-recognition/sample_submission.csv\")\nsub.to_csv(\n    \"submission.csv\", index=False\n)  # this will be overwritten if everything goes well"},{"cell_type":"markdown","metadata":{},"source":"### set parameters"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"TARGET_SR = 32000\nmodel_config: tp.Dict[str, tp.Union[str, int, bool]] = {\n    \"base_model_name\": \"resnest50_fast_1s1x64d\",\n    \"pretrained\": False,\n    \"num_classes\": 264,\n    \"trained_weights\": \"../input/training-birdsong-baseline-resnest50-fast/best_model.pth\",\n}\n\nmelspectrogram_parameters = {\"n_mels\": 128, \"fmin\": 20, \"fmax\": 16000}"},{"cell_type":"markdown","metadata":{},"source":"## Definition"},{"cell_type":"markdown","metadata":{},"source":"### Dataset\n\nFor `site_3`, I decided to use the same procedure as I did for `site_1` and `site_2`, which is, crop 5 seconds out of the clip and provide prediction on that short clip.\nThe only difference is that I crop 5 seconds short clip from start to the end of the `site_3` clip and aggeregate predictions for each short clip after I did prediction for all those short clips."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":"BIRD_CODE = {\n    \"aldfly\": 0,\n    \"ameavo\": 1,\n    \"amebit\": 2,\n    \"amecro\": 3,\n    \"amegfi\": 4,\n    \"amekes\": 5,\n    \"amepip\": 6,\n    \"amered\": 7,\n    \"amerob\": 8,\n    \"amewig\": 9,\n    \"amewoo\": 10,\n    \"amtspa\": 11,\n    \"annhum\": 12,\n    \"astfly\": 13,\n    \"baisan\": 14,\n    \"baleag\": 15,\n    \"balori\": 16,\n    \"banswa\": 17,\n    \"barswa\": 18,\n    \"bawwar\": 19,\n    \"belkin1\": 20,\n    \"belspa2\": 21,\n    \"bewwre\": 22,\n    \"bkbcuc\": 23,\n    \"bkbmag1\": 24,\n    \"bkbwar\": 25,\n    \"bkcchi\": 26,\n    \"bkchum\": 27,\n    \"bkhgro\": 28,\n    \"bkpwar\": 29,\n    \"bktspa\": 30,\n    \"blkpho\": 31,\n    \"blugrb1\": 32,\n    \"blujay\": 33,\n    \"bnhcow\": 34,\n    \"boboli\": 35,\n    \"bongul\": 36,\n    \"brdowl\": 37,\n    \"brebla\": 38,\n    \"brespa\": 39,\n    \"brncre\": 40,\n    \"brnthr\": 41,\n    \"brthum\": 42,\n    \"brwhaw\": 43,\n    \"btbwar\": 44,\n    \"btnwar\": 45,\n    \"btywar\": 46,\n    \"buffle\": 47,\n    \"buggna\": 48,\n    \"buhvir\": 49,\n    \"bulori\": 50,\n    \"bushti\": 51,\n    \"buwtea\": 52,\n    \"buwwar\": 53,\n    \"cacwre\": 54,\n    \"calgul\": 55,\n    \"calqua\": 56,\n    \"camwar\": 57,\n    \"cangoo\": 58,\n    \"canwar\": 59,\n    \"canwre\": 60,\n    \"carwre\": 61,\n    \"casfin\": 62,\n    \"caster1\": 63,\n    \"casvir\": 64,\n    \"cedwax\": 65,\n    \"chispa\": 66,\n    \"chiswi\": 67,\n    \"chswar\": 68,\n    \"chukar\": 69,\n    \"clanut\": 70,\n    \"cliswa\": 71,\n    \"comgol\": 72,\n    \"comgra\": 73,\n    \"comloo\": 74,\n    \"commer\": 75,\n    \"comnig\": 76,\n    \"comrav\": 77,\n    \"comred\": 78,\n    \"comter\": 79,\n    \"comyel\": 80,\n    \"coohaw\": 81,\n    \"coshum\": 82,\n    \"cowscj1\": 83,\n    \"daejun\": 84,\n    \"doccor\": 85,\n    \"dowwoo\": 86,\n    \"dusfly\": 87,\n    \"eargre\": 88,\n    \"easblu\": 89,\n    \"easkin\": 90,\n    \"easmea\": 91,\n    \"easpho\": 92,\n    \"eastow\": 93,\n    \"eawpew\": 94,\n    \"eucdov\": 95,\n    \"eursta\": 96,\n    \"evegro\": 97,\n    \"fiespa\": 98,\n    \"fiscro\": 99,\n    \"foxspa\": 100,\n    \"gadwal\": 101,\n    \"gcrfin\": 102,\n    \"gnttow\": 103,\n    \"gnwtea\": 104,\n    \"gockin\": 105,\n    \"gocspa\": 106,\n    \"goleag\": 107,\n    \"grbher3\": 108,\n    \"grcfly\": 109,\n    \"greegr\": 110,\n    \"greroa\": 111,\n    \"greyel\": 112,\n    \"grhowl\": 113,\n    \"grnher\": 114,\n    \"grtgra\": 115,\n    \"grycat\": 116,\n    \"gryfly\": 117,\n    \"haiwoo\": 118,\n    \"hamfly\": 119,\n    \"hergul\": 120,\n    \"herthr\": 121,\n    \"hoomer\": 122,\n    \"hoowar\": 123,\n    \"horgre\": 124,\n    \"horlar\": 125,\n    \"houfin\": 126,\n    \"houspa\": 127,\n    \"houwre\": 128,\n    \"indbun\": 129,\n    \"juntit1\": 130,\n    \"killde\": 131,\n    \"labwoo\": 132,\n    \"larspa\": 133,\n    \"lazbun\": 134,\n    \"leabit\": 135,\n    \"leafly\": 136,\n    \"leasan\": 137,\n    \"lecthr\": 138,\n    \"lesgol\": 139,\n    \"lesnig\": 140,\n    \"lesyel\": 141,\n    \"lewwoo\": 142,\n    \"linspa\": 143,\n    \"lobcur\": 144,\n    \"lobdow\": 145,\n    \"logshr\": 146,\n    \"lotduc\": 147,\n    \"louwat\": 148,\n    \"macwar\": 149,\n    \"magwar\": 150,\n    \"mallar3\": 151,\n    \"marwre\": 152,\n    \"merlin\": 153,\n    \"moublu\": 154,\n    \"mouchi\": 155,\n    \"moudov\": 156,\n    \"norcar\": 157,\n    \"norfli\": 158,\n    \"norhar2\": 159,\n    \"normoc\": 160,\n    \"norpar\": 161,\n    \"norpin\": 162,\n    \"norsho\": 163,\n    \"norwat\": 164,\n    \"nrwswa\": 165,\n    \"nutwoo\": 166,\n    \"olsfly\": 167,\n    \"orcwar\": 168,\n    \"osprey\": 169,\n    \"ovenbi1\": 170,\n    \"palwar\": 171,\n    \"pasfly\": 172,\n    \"pecsan\": 173,\n    \"perfal\": 174,\n    \"phaino\": 175,\n    \"pibgre\": 176,\n    \"pilwoo\": 177,\n    \"pingro\": 178,\n    \"pinjay\": 179,\n    \"pinsis\": 180,\n    \"pinwar\": 181,\n    \"plsvir\": 182,\n    \"prawar\": 183,\n    \"purfin\": 184,\n    \"pygnut\": 185,\n    \"rebmer\": 186,\n    \"rebnut\": 187,\n    \"rebsap\": 188,\n    \"rebwoo\": 189,\n    \"redcro\": 190,\n    \"redhea\": 191,\n    \"reevir1\": 192,\n    \"renpha\": 193,\n    \"reshaw\": 194,\n    \"rethaw\": 195,\n    \"rewbla\": 196,\n    \"ribgul\": 197,\n    \"rinduc\": 198,\n    \"robgro\": 199,\n    \"rocpig\": 200,\n    \"rocwre\": 201,\n    \"rthhum\": 202,\n    \"ruckin\": 203,\n    \"rudduc\": 204,\n    \"rufgro\": 205,\n    \"rufhum\": 206,\n    \"rusbla\": 207,\n    \"sagspa1\": 208,\n    \"sagthr\": 209,\n    \"savspa\": 210,\n    \"saypho\": 211,\n    \"scatan\": 212,\n    \"scoori\": 213,\n    \"semplo\": 214,\n    \"semsan\": 215,\n    \"sheowl\": 216,\n    \"shshaw\": 217,\n    \"snobun\": 218,\n    \"snogoo\": 219,\n    \"solsan\": 220,\n    \"sonspa\": 221,\n    \"sora\": 222,\n    \"sposan\": 223,\n    \"spotow\": 224,\n    \"stejay\": 225,\n    \"swahaw\": 226,\n    \"swaspa\": 227,\n    \"swathr\": 228,\n    \"treswa\": 229,\n    \"truswa\": 230,\n    \"tuftit\": 231,\n    \"tunswa\": 232,\n    \"veery\": 233,\n    \"vesspa\": 234,\n    \"vigswa\": 235,\n    \"warvir\": 236,\n    \"wesblu\": 237,\n    \"wesgre\": 238,\n    \"weskin\": 239,\n    \"wesmea\": 240,\n    \"wessan\": 241,\n    \"westan\": 242,\n    \"wewpew\": 243,\n    \"whbnut\": 244,\n    \"whcspa\": 245,\n    \"whfibi\": 246,\n    \"whtspa\": 247,\n    \"whtswi\": 248,\n    \"wilfly\": 249,\n    \"wilsni1\": 250,\n    \"wiltur\": 251,\n    \"winwre3\": 252,\n    \"wlswar\": 253,\n    \"wooduc\": 254,\n    \"wooscj2\": 255,\n    \"woothr\": 256,\n    \"y00475\": 257,\n    \"yebfly\": 258,\n    \"yebsap\": 259,\n    \"yehbla\": 260,\n    \"yelwar\": 261,\n    \"yerwar\": 262,\n    \"yetvir\": 263,\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false},"outputs":[],"source":"SR = 32000\n\n\ndef mono_to_color(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Make 2D image 3D. Normalise and scale to be between 0 and 255.\n\n    Parameters\n    ----------\n    X\n        2D image.\n    eps\n        Epsilon, small number to add to std to avoid dividing by zero.\n\n    Returns\n    -------\n    np.ndarray\n        3D image, normalised, scaled.\n    \"\"\"\n    original_shape = X.shape\n    assert len(X.shape) == 2\n\n    X = np.stack([X, X, X], axis=-1)\n    assert X.shape == (*original_shape, 3)\n\n    mean = X.mean()\n    X = X - mean\n    std = X.std()\n    Xstd = X / (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    if (_max - _min) > eps:\n        V = Xstd\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\n\ndef _get_image_from_audio(\n    y: np.ndarray, melspectrogram_parameters: tp.Dict[str, int], img_size: int\n) -> np.ndarray:\n    \"\"\"\n    Get 3D image from audio.\n\n    Steps are:\n    - Compute the mel-scaled (power) spectrogram of the signal\n    - Convert the power spectrogram to decibel units.\n    - Stack signal so as to get 3D image, and normalise\n\n    Parameters\n    ----------\n    y\n        Audio signal.\n    melspectrogram_parameters\n        Arguments to be passed on to melspectrogram.\n    img_size\n        Desired width of image that'll be obtained by converting audio.\n    \"\"\"\n    melspec = librosa.feature.melspectrogram(y, sr=SR, **melspectrogram_parameters)\n    melspec = librosa.power_to_db(melspec).astype(np.float32)\n\n    image = mono_to_color(melspec)\n    height, width, _ = image.shape\n    image = cv2.resize(image, (int(width * img_size / height), img_size))\n    image = np.moveaxis(image, 2, 0)\n    image = (image / 255.0).astype(np.float32)\n    return image\n\n\nclass TestDataset(data.Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        clip: np.ndarray,\n        img_size: int,\n        melspectrogram_parameters: tp.Dict[str, int],\n    ) -> None:\n        \"\"\"\n        Initialise test dataset.\n\n        Parameters\n        ----------\n        df\n            Rows of test DataFrame corresponding to given audio clip.\n        clip\n            Audio clip as time series.\n        img_size\n            Desired width of image that'll be obtained by converting audio.\n        melspectrogram_parameters\n            Arguments to be passed on to melspectrogram.\n        \"\"\"\n        self.df = df\n        self.clip = clip\n        self.img_size = img_size\n        self.melspectrogram_parameters = melspectrogram_parameters\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> tp.Tuple[np.ndarray, str, str]:\n        \"\"\"\n        Preprocess row of data.\n\n        If the row comes from site 1 or site 2, we just convert the\n        sound file to a 3D image. Otherwise, we split the sound file\n        into 5-second chunks (discarding the last incomplete one) and\n        transform each to an image.\n\n        Parameters\n        ----------\n        idx\n            Index of element to pre-process.\n\n        Returns\n        -------\n        img\n            3D image representation of sound file.\n        row_id\n            Row id corresponding to element.\n        site\n            Site where recording was taken (site 1, site 2, or site 3)\n        \"\"\"\n        sample = self.df.loc[idx, :]\n        site = sample.site\n        row_id = sample.row_id\n\n        if site == \"site_3\":\n            y = self.clip.astype(np.float32)\n            len_y = len(y)\n            start = 0\n            end = SR * 5\n            images = []\n            while len_y > start:\n                y_batch = y[start:end].astype(np.float32)\n                if len(y_batch) != (SR * 5):\n                    break\n                start = end\n                end = end + SR * 5\n                image = _get_image_from_audio(\n                    y_batch, self.melspectrogram_parameters, self.img_size\n                )\n                images.append(image)\n            images = np.asarray(images)\n        else:\n            end_seconds = int(sample.seconds)\n            start_seconds = int(end_seconds - 5)\n\n            start_index = SR * start_seconds\n            end_index = SR * end_seconds\n\n            y = self.clip[start_index:end_index].astype(np.float32)\n            image = _get_image_from_audio(y, self.melspectrogram_parameters, self.img_size)\n\n        return image, row_id, site"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def get_model(args: tp.Dict) -> nn.Module:\n    \"\"\"\n    Get pre-trained model and customise head.\n\n    Parameters\n    ----------\n    args\n        Additional arguments to pass to Pytorch model\n    \"\"\"\n    model = getattr(resnest_torch, args[\"base_model_name\"])(pretrained=args[\"pretrained\"])\n    del model.fc\n    model.fc = nn.Sequential(\n        nn.Linear(2048, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, args[\"num_classes\"]),\n    )\n\n    state_dict = torch.load(args[\"trained_weights\"])\n    model.load_state_dict(state_dict)\n    device = torch.device(\"cuda\")\n    model.to(device)\n    model.eval()\n\n    return model"},{"cell_type":"markdown","metadata":{},"source":"## Prediction loop"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false},"outputs":[],"source":"def prediction_for_clip(\n    test_df: pd.DataFrame,\n    clip: np.ndarray,\n    model: nn.Module,\n    mel_params: tp.Dict[str, int],\n    threshold: float = 0.5,\n) -> tp.Dict[str, str]:\n    \"\"\"\n    Make prediction for single audio clip.\n\n    Audio clip may correspond to multiple rows from the dataset\n    (e.g. seconds 0-5, then seconds 5-10, ...).\n\n    Parameters\n    ----------\n    test_df\n        Portion of test dataset corresponding to this audio clip.\n    clip\n        Audio as floating point time series.\n    model\n        Model (already trained).\n    mel_params\n        Parameters for melspectrogram..\n    threshold\n        Predict all targets whose logits are above this value.\n\n    Returns\n    -------\n    Dict\n        Keys are row_ids, values are list of birds detected.\n    \"\"\"\n    assert clip.ndim == 1\n    pd.testing.assert_index_equal(\n        test_df.columns, pd.Index([\"site\", \"row_id\", \"seconds\", \"audio_id\"])\n    )\n\n    dataset = TestDataset(\n        df=test_df, clip=clip, img_size=224, melspectrogram_parameters=mel_params\n    )\n    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.eval()\n    prediction_dict = {}\n    for image, row_id, site in progress_bar(loader):\n        assert image.ndim == 4\n        # Last dimension is height\n        batch_size, channels, frequency, time = image.shape\n        assert (batch_size, channels, frequency) == (1, 3, 224)\n        assert len(row_id) == 1\n        assert len(site) == 1\n\n        (site,) = site\n        (row_id,) = row_id\n\n        image = image.to(device)\n        with torch.no_grad():\n            prediction = torch.sigmoid(model(image))\n            proba = prediction.detach().cpu().numpy().reshape(-1)\n            assert proba.shape == (264,)\n\n        events = proba >= threshold\n        labels = np.argwhere(events).reshape(-1).tolist()\n\n        if len(labels) == 0:\n            prediction_dict[row_id] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[row_id] = label_string\n    return prediction_dict"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false},"outputs":[],"source":"def prediction(\n    test_df: pd.DataFrame,\n    test_audio: Path,\n    model_config: tp.Dict[str, tp.Union[str, int, bool]],\n    mel_params: tp.Dict[str, int],\n    target_sr: int,\n    threshold: float = 0.5,\n) -> pd.DataFrame:\n    \"\"\"\n    Get predictions.\n\n    For each unique audio ID, predict which bird is present in each 5 second clip.\n\n    Parameters\n    ----------\n    test_df\n        test csv file\n    test_audio\n        Directory containing audio recordings for test dataset.\n    model_config\n        Configs to load model with\n    mel_params\n        Parameters for melspectrogram (used in data loader).\n    target_sr\n        Target sampling rate\n    threshold\n        Predict all targets whose logits are above this value.\n\n    Returns\n    -------\n    DataFrame\n        Predictions for each row of the input data.\n    \"\"\"\n    model = get_model(model_config)\n    unique_audio_id = test_df.audio_id.unique()\n\n    prediction_dfs = []\n    for audio_id in unique_audio_id:\n        with timer(f\"Loading {audio_id}\"), warnings.catch_warnings():\n            # libsndfile doesn't handle mp3\n            # see https://github.com/librosa/librosa/issues/1015\n            warnings.simplefilter(\"ignore\", UserWarning)\n            clip, _ = librosa.load(\n                test_audio / (audio_id + \".mp3\"),\n                sr=target_sr,\n                mono=True,\n                res_type=\"kaiser_fast\",\n            )\n\n        test_df_for_audio_id = test_df.query(f\"audio_id == '{audio_id}'\")\n        test_df_for_audio_id = test_df_for_audio_id.reset_index(drop=True)\n        with timer(f\"Prediction on {audio_id}\"):\n            prediction_dict = prediction_for_clip(\n                test_df_for_audio_id,\n                clip=clip,\n                model=model,\n                mel_params=mel_params,\n                threshold=threshold,\n            )\n        row_id = list(prediction_dict.keys())\n        birds = list(prediction_dict.values())\n        prediction_df = pd.DataFrame({\"row_id\": row_id, \"birds\": birds})\n        prediction_dfs.append(prediction_df)\n\n    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n    assert len(prediction_df) == len(test_df)\n    return prediction_df"},{"cell_type":"markdown","metadata":{},"source":"## Prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":"submission = prediction(\n    test_df=test,\n    test_audio=TEST_AUDIO_DIR,\n    model_config=model_config,\n    mel_params=melspectrogram_parameters,\n    target_sr=TARGET_SR,\n    threshold=0.6,\n)\nsubmission.to_csv(\"submission.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":"submission"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{},"source":"## EOF"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}