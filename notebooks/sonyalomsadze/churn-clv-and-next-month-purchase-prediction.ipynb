{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\ntelco=pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv',skipinitialspace=True)\ntelco.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - #### Examples Y (target variable) of supervised learning on marketing are:\n - Which customers will churn?[Classification]\n - Which customers will buy again?[Classification]\n - How much will customers spend in the next 30 days?[Regression]\n - #### Examples X (feature variable) of supervised learning on marketing are:\n - Purchase patterns prior churning\n - Number of missed loan payments prior defaulting on a loan"},{"metadata":{},"cell_type":"markdown","source":"Data format for supervised learning is \n- X by N+1 matris:\n    * X number of observations (customer, vendor, product)\n    * N+1 number of columns (N features + 1 target variable)\n- Note: In unsupervised learning techniques, dependent variable (target) is not used."},{"metadata":{},"cell_type":"markdown","source":"## Preparation for Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A) Categorical and Numberical Colums Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identifier and target variable\ncustomer_id=['customerID']\ntarget=['Churn']\n#Split categorical and numerical column NAMES as lists\ncategories=telco.nunique()[telco.nunique()<10].keys().tolist()\ncategories.remove(target[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical=[col for col in telco.columns if col not in customer_id+target+categories]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numerical)\nprint(categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B) One-Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco_raw=pd.get_dummies(data=telco,columns=categories,drop_first=True)\ntelco_raw.iloc[:5,5:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C) Scaling Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before scaling lets check types if we have an object scaler will not work\ntelco[numerical].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaled_numerical=scaler.fit_transform(telco[numerical])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_numerical=pd.DataFrame(scaled_numerical,columns=numerical)\nprint(telco[numerical])\nprint(scaled_numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final..Bringing all together"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco_raw=telco_raw.drop(columns=numerical,axis=1)\n# Merge with scaled numerical data on left\ntelco=telco_raw.merge(right=scaled_numerical,how='left',left_index=True,right_index=True)\ntelco.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telco=telco.dropna()\ntelco.Churn=telco.Churn.replace({'No':0,'Yes':1})\nX = telco.drop(columns={'customerID','Churn'})\nY = telco['Churn']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MACHINE LEARNING PART"},{"metadata":{},"cell_type":"markdown","source":"### A) Supervised Learning-Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ntrain_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.25)\n# [0] is for dimension of row number \nprint(train_X.shape[0]/X.shape[0])\nprint(test_X.shape[0]/X.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mytree = tree.DecisionTreeClassifier(max_depth=5)\ntreemodel = mytree.fit(train_X, train_Y)\npred_Y = treemodel.predict(test_X)\npred_Y_ = treemodel.predict(train_X)\nimport numpy as np\nprint('Training accuracy:',np.round(accuracy_score(train_Y, pred_Y_),3))\nprint('Test accuracy:',np.round(accuracy_score(test_Y, pred_Y),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gini Index as impurity criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"mytree = tree.DecisionTreeClassifier(max_depth=7,criterion='gini',splitter='best')\ntreemodel = mytree.fit(train_X, train_Y)\npred_Y = treemodel.predict(test_X)\npred_Y_ = treemodel.predict(train_X)\nprint('Gini Training accuracy:',np.round(accuracy_score(train_Y, pred_Y_),3))\nprint('Gini Test accuracy:',np.round(accuracy_score(test_Y, pred_Y),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B) Unsupervised Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco=telco.drop(columns='customerID')\nfrom sklearn.cluster import KMeans\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(telco)\ntelco=telco.assign(Cluster=kmeans.labels_)\ntelco.groupby('Cluster').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.groupby('Cluster').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What is churn?\nWhen buying and engaging stops by a customer churn happens. There is two main churn typology.\n- Constractual\n   - Customers decide to cancel subscriptions or services.\n- Non-constractual\n   - Happens in settings like grocery or online shopping. Customers just stop buying or using products without contract. It's more harder to track."},{"metadata":{},"cell_type":"markdown","source":"\"In this part, we'll discover telecommunications company under one master agreement which defices whether customer is still active, or has churned, which means they have terminated their contract.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"set(telco['Churn'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.groupby(['Churn']).size()/telco.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, the minority class is higher than 5% so this is not savere case of imbalanced datasets. Otherwise, we could increase the minority class of decrease the majority class with oversampling or undersampling techniques."},{"metadata":{},"cell_type":"markdown","source":"### A) Predict Churn with LogReg\nLogreg is\n- Statistical classification model for binary outcomes\n- Computes the probability of the target. \n- Odds=P(A)/P'(A) or P(A)/1-P(A)\n   - If P(A)=75%, odds ratio=3, log-odd is roughly 0.\n- LogReg helps to find the decision boundary btw 2 classes while keeping the coefficients linearly related to the target variable. "},{"metadata":{},"cell_type":"markdown","source":"Key Metrics:\n- #### Accuracy\n% of correctly predicted labels-1 and 0.\n- #### TP+TN/TP+TN+FP+FN\n- #### Precision\n% of total positive class predictions (here is 1) that are correctly classified.\n- #### TP/TP+FP\n- #### Recall\n% of total positive class samples (all 1's) that were correctly classified.\n- #### TP/TP+FN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(train_X,train_Y)\npred_train_Y=logreg.predict(train_X)\npred_test_Y=logreg.predict(test_X)\ntrain_accuracy=accuracy_score(train_Y,pred_train_Y)\ntest_accuracy=accuracy_score(test_Y,pred_test_Y)\nprint('Training accuracy:', round(train_accuracy,4))\nprint('Test accuracy:', round(test_accuracy,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score,recall_score\ntrain_precision=round(precision_score(train_Y,pred_train_Y),4)\ntest_precision=round(precision_score(test_Y,pred_test_Y),4)\ntrain_recall=round(recall_score(train_Y,pred_train_Y,average=\"binary\"),4)\ntest_recall=round(recall_score(test_Y,pred_test_Y,average=\"binary\"),4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train precision is:',train_precision)\nprint('Test precision is:',test_precision)\nprint('Train recall is:',train_recall)\nprint('Test recall is:',test_recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regularization & Parameter Tuning\nSome regularization techniques keeps all the features, but reduce magnitude/values of parameters to avoid overfitting. LogisticRegression from sklearn already is this type and performs L2 regularization by default. You can call L1 or LASSO explicitly. L1 performs feature selection by shrinking some of the beta parameters to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"C=[1,.5,.25,.1,.05,.025,.01,.005,.0025]\nl1_metrics=np.zeros((len(C),5))\nl1_metrics[:,0]=C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# C tuning\nfor index in range (0, len(C)):\n    logreg=LogisticRegression(penalty='l1', C=C[index], solver='liblinear')\n    logreg.fit(train_X,train_Y)\n    pred_test_Y=logreg.predict(test_X)\n    l1_metrics[index,1]=np.count_nonzero(logreg.coef_)\n    l1_metrics[index,2]=accuracy_score(test_Y,pred_test_Y)\n    l1_metrics[index,3]=precision_score(test_Y,pred_test_Y)\n    l1_metrics[index,4]=recall_score(test_Y,pred_test_Y)\ncol_names=['C','Non-ZeroCoeffs','Accuracy','Precision','Recall']\nprint(pd.DataFrame(l1_metrics, columns=col_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg=LogisticRegression(penalty='l1', C=.025, solver='liblinear')\nlogreg.fit(train_X,train_Y)\npred_test_Y=logreg.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tree depth parameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndepth_list=list(range(2,15))\ndepth_tuning=np.zeros((len(depth_list),4))\n# Storing depth candidates into the first column\ndepth_tuning[:,0]=depth_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depth_tuning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmytree =DecisionTreeClassifier()\ntreemodel=mytree.fit(train_X,train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(len(depth_list)):\n    mytree=DecisionTreeClassifier(max_depth=depth_list[index])\n    mytree.fit(train_X,train_Y)\n    pred_test_Y=mytree.predict(test_X)\n    depth_tuning[index,1]=accuracy_score(test_Y,pred_test_Y)\n    depth_tuning[index,2]=precision_score(test_Y,pred_test_Y)\n    depth_tuning[index,3]=recall_score(test_Y,pred_test_Y)\ncol_names=['Max_Depth','Accuracy','Precision','Recall']\nprint(pd.DataFrame(depth_tuning,columns=col_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing accuracy first increases with more depth and then start decline.The precision declines with more depth, yet the recall increases first, then starts falling.\nAt max_depth of 4, tree solution produces good scores and pretty high recall before it starts declining."},{"metadata":{},"cell_type":"markdown","source":"### The Interpretion of Churn Drivers"},{"metadata":{"trusted":true},"cell_type":"code","source":"mytree =DecisionTreeClassifier(max_depth=4)\ntreemodel=mytree.fit(train_X,train_Y)\ncols=train_X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Displaying Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nexported=tree.export_graphviz(\ndecision_tree=mytree,\nout_file=None,\nfeature_names=cols,\nprecision=1,\nclass_names=['Not churn','churn'],\nfilled=True)\ngraph=graphviz.Source(exported)\ndisplay(graph)\ngraph.format = 'png'\ngraph.render('dtree_render',view=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see customer tenure is the most important variable while predicting churn. You should update mytree on max_depth=4"},{"metadata":{},"cell_type":"markdown","source":"### Logreg coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List of beta coefficients are difficult to interpret since there are in log scale. Solution is to calculate their exponents ."},{"metadata":{"trusted":true},"cell_type":"code","source":"# axis=1 means along the columns, axis=0 means along the rows.\ncoeffs=pd.concat([pd.DataFrame(train_X.columns),\n                 pd.DataFrame(np.transpose(logreg.coef_))],\n                axis=1)\ncoeffs.columns=['Feature','Coefficient']\ncoeffs['Exp_Coefficient']=np.exp(coeffs['Coefficient'])\ncoeffs=coeffs[coeffs['Coefficient']!=0]\ncoeffs.sort_values(by=['Coefficient'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is consistent with our findings from decision tree. In addition, we can intepret effect of odds as follows:\nthe effect of one additional year of tenure decreases the odds of churn by 1-0.429. This roughly 60% decrease in the churn odds."},{"metadata":{},"cell_type":"markdown","source":"# Customer Lifetime\nIt's measurement of how much a company expects to earn from an average customer in a lifetime.\n"},{"metadata":{},"cell_type":"markdown","source":"### a) Historical CLV\nHistorical cvl does not account for:\n- Customer tenure\n- Retention\n- Churn Rates"},{"metadata":{},"cell_type":"markdown","source":"### b) Traditional CLV\nChurn= 1-retention rate\nThe retention to churn ratio gives us a multiplier meaning the expected length of the customer lifespan (loyalty)."},{"metadata":{},"cell_type":"markdown","source":"#### Standard Transactional Dataset - Online Retail"},{"metadata":{"trusted":true},"cell_type":"code","source":"online_retail=pd.read_csv('../input/online-retail-ii-uci/online_retail_II.csv',encoding = 'unicode_escape')\nonline_retail['InvoiceDate'] = pd.to_datetime(online_retail['InvoiceDate']).dt.date\nonline_retail.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time Based Cohort Dataset - Derived from Online Retail\nThis dataset is created by assigning each customer to a monthly cohort, bsed on the month they made their first purchase. We'll use this cohort to calculate retention rates."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\ndef get_month(x): return dt.datetime(x.year,x.month,1)\nonline_retail['InvoiceMonth']=online_retail['InvoiceDate'].apply(get_month)\ngroup=online_retail.groupby('Customer ID')['InvoiceMonth']\nonline_retail['CohortMonth']=group.transform('min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have assigned the acquisition month cohort to each customer."},{"metadata":{},"cell_type":"markdown","source":"#### Extracting year, month and day integer values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_date_int(df, column):\n    year=df[column].dt.year\n    month=df[column].dt.month\n    day=df[column].dt.day\n    return year, month, day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"invoice_year,invoice_month,_=get_date_int(online_retail,'InvoiceMonth')\ncohort_year,cohort_month,_=get_date_int(online_retail,'CohortMonth')\nyears_diff=invoice_year-cohort_year\nmonths_diff=invoice_month-cohort_month\nonline_retail['CohortIndex']=years_diff*12+months_diff+1\nonline_retail.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Count monthly active customers from each cohort"},{"metadata":{"trusted":true},"cell_type":"code","source":"group=online_retail.groupby(['CohortMonth','CohortIndex'])\n#Count the number of customers in each group by applying pandas nunique().\ncohort_data=group['Customer ID'].apply(pd.Series.nunique)\ncohort_data=cohort_data.reset_index()\ncohort_counts=cohort_data.pivot(index='CohortMonth',columns='CohortIndex',values='Customer ID')\ncohort_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Customer Retention Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"cohort_sizes=cohort_counts.iloc[:,0]\n# Divide them along the row axis.\nretention=cohort_counts.divide(cohort_sizes,axis=0)\nretention.round(3)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Other Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouping=online_retail.groupby(['CohortMonth','CohortIndex'])\ncohort_data=group['Quantity'].mean()\ncohort_data=cohort_data.reset_index()\naverage_quantity=cohort_data.pivot(index='CohortMonth',\n                                  columns='CohortIndex',\n                                  values='Quantity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_quantity.round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing cohort analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nplt.title('Retention Rates')\nsns.heatmap(data=retention,\n           annot=True,\n           fmt='.0%',\n           vmin=0.0,\n           vmax=0.5,\n           cmap='BuGn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First month retention is 100% because this is the month when the customers had first started buying. By definition, all of the customers from this cohort are active in their first month."},{"metadata":{},"cell_type":"markdown","source":"####  Churn analysis from cohort & visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"churn=1-retention\nchurn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nplt.title('Churn')\nsns.heatmap(data=churn,\n           annot=True,\n           fmt='.0%',\n           vmin=0.0,\n           vmax=0.5,\n           cmap='YlGn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retention_rate=retention.iloc[:,1:].mean().mean()\nchurn_rate=churn.iloc[:,1:].mean().mean()\nprint('Retention rate: {:.2f};Churn rate: {:.2f}'.format(retention_rate,churn_rate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating Revenue Based (Traditional) CLV\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"online_retail['TotalSum']=online_retail['Price']*online_retail['Quantity']\nmonthly_revenue=online_retail.groupby(['Customer ID','InvoiceMonth'])['TotalSum'].sum()\nmonthly_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_revenue=np.mean(monthly_revenue)\nmonthly_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lifespan_months=36\nclv=monthly_revenue*lifespan_months\nprint('Average Basic  CLV is {:.1f} USD'.format(clv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average revenue per invoice\nrevenue_per_purchase = online_retail.groupby(['Invoice'])['TotalSum'].mean().mean()\n\n# Calculate average number of unique invoices per customer per month\nfrequency_per_month = online_retail.groupby(['Customer ID','InvoiceMonth'])['Invoice'].nunique().mean()\n\n# Define lifespan to 36 months\nlifespan_months = 36\n\n# Calculate granular CLV\nclv_granular = revenue_per_purchase * frequency_per_month * lifespan_months\n\n# Print granular CLV value\nprint('Average granular CLV is {:.1f} USD'.format(clv_granular))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate monthly spend per customer\nmonthly_revenue = online_retail.groupby(['Customer ID','InvoiceMonth'])['TotalSum'].sum().mean()\n\n# Calculate average monthly retention rate\nretention_rate = retention.iloc[:,1:].mean().mean()\n\n# Calculate average monthly churn rate\nchurn_rate = 1 - retention_rate\n\n# Calculate traditional CLV \nclv_traditional = monthly_revenue * (retention_rate / churn_rate)\n\n# Print traditional CLV and the retention rate values\nprint('Average traditional CLV is {:.1f} USD at {:.1f} % retention_rate'.format(clv_traditional, retention_rate*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Traditional CLV formula yield a much lower estimate as it accounts for monthly retention which is quite low for this company."},{"metadata":{},"cell_type":"markdown","source":"# Purchase Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"online_retail.groupby(['InvoiceMonth']).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"online_retail['InvoiceDate'] = pd.to_datetime(online_retail['InvoiceDate'])\n# Exclusion of target variable\nonline_X=online_retail[online_retail['InvoiceMonth']!='2011-11-01']\nNow=dt.datetime(2011,11,1)\nfeatures=online_retail.groupby('Customer ID').agg({\n    'InvoiceDate':lambda x: (Now-x.max()).days,\n    'Invoice':pd.Series.nunique,\n    'TotalSum':np.sum,\n    'Quantity':['mean','sum'],\n    }).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We first calculate the recency by calculating the difference in days between the previously defined snapshot date and the latest invoice date for the customer.\n- Then we falculate frequency by counting the unique number of invoices.\n- We sum the revenue spent by that customer to get the monetary value.\n- In the final step, we calculate both the average and the total quantity purchased by the customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.columns=['CustomerID','Recency','Frequency','Monetary','Quantity_Avg','Quantity_Sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build pivot table with monthly transactions per customer\ncust_month_tx=pd.pivot_table(data=online_retail,index=['Customer ID'],\n                            values='Invoice',\n                            columns=['InvoiceMonth'],\n                            aggfunc=pd.Series.nunique,fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_month_tx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_month_tx.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custid=['Customer ID']\ntarget=['2011-11-01']\nY=cust_month_tx['2011-11-01']\ncols=[col for col in features.columns if col not in custid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=features[cols]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.25, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression Performance Metrics\n- ###### Root Mean Squared Error (RMSE)\n  sqrt of the average squared difference between prediction and actuals. \n- ###### Mean Absolute Error (MAE)\n  avgs absolute differences between predictions and actuals.\n- ###### Mean Absolute Percentage Error (MAPE)\n  avgs percentage difference between prediction and actuals. Actuals must be higher than zero.\n- ###### R-squared (R^2)\n  percentage proportion of variance that is explained by the model. Only applicable to regression, not classification. The higher the R^2 value, the better the model explains variance.\n- ###### Coefficient p-values\n  probability that the regression or classification coefficient is observed toe to chance. Lower is better. Typical thresholds are 5% and 10%."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinreg=LinearRegression()\nlinreg.fit(train_X,train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred_Y=linreg.predict(train_X)\ntest_pred_Y=linreg.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Measuring model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nrmse_train=np.sqrt(mean_squared_error(train_Y,train_pred_Y))\nmae_train=np.sqrt(mean_absolute_error(train_Y,train_pred_Y))\nrmse_test=np.sqrt(mean_squared_error(test_Y,test_pred_Y))\nmae_test=np.sqrt(mean_absolute_error(test_Y,test_pred_Y))\nprint('RMSE train: {:.3f}; RMSE test: {:.3f}\\nMAE train: {:.3f}, MAE test: {:.3f}'.format(rmse_train,rmse_test,mae_train,mae_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MAE is smaller, as it is less sensitive to outliers. MAE means that comparing actual transactions in November to the predicted transactions in November, our model is off by %70 of transactions."},{"metadata":{},"cell_type":"markdown","source":"### Interpret coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\ntrain_Y=np.array(train_Y)\nolsreg=sm.OLS(train_Y,train_X)\nolsreg=olsreg.fit()\nprint(olsreg.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R-squared is 0.609 means that the model explains %60 variance. Coefficient of frequency is 0.12, the customer who  have 1 unit higher frequency or invoice number in pre-november period will have 0.12 invoices more in November on average. If we assume the significance level at 95%, we will interpret coefficients with p-value lower or equal to 1 minus the significance level or 5%. There are only two coefficients with p-value lower or equal to 5%. There are 4 of them recency, frequency, monetary and quantity total."},{"metadata":{},"cell_type":"markdown","source":"REFERENCES: Datacamp, Market Analytics with Python"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}