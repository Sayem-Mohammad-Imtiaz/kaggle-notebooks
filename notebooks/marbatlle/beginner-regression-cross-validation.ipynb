{"cells":[{"metadata":{},"cell_type":"markdown","source":"# California Housing Prices Prediction"},{"metadata":{},"cell_type":"markdown","source":"This project is part of the book Hands-On-Machine Learning with Scikit-Learn, Keras & TensorFlow by Aurélien Géron."},{"metadata":{},"cell_type":"markdown","source":"## 0. Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common imports\nimport sys\nimport sklearn\nimport numpy as np\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Get the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport tarfile\nimport urllib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhousing_path = \"/kaggle/input/california-housing-prices/housing.csv\"\n\nhousing = pd.read_csv(housing_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each row represents one district. There are 10 attributes: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, household, median_income, median_house_value, and ocean_proximity.\n\nThe info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute's type, and the number of nonull values."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 20,640 instances in the dataset. Notice that the total_bedrooms attribute has only 20.433 nonnull values, meaning that 207 districts are missing this feature.\n\nAll attributes are numerical, except ocean_proximity field. Its type is object, so it could hold any kind of Python object, but as we saw earlier, it's a text attribute, and since it's repeated values, it might be it's probably a categorical attribute. We can find out which ones using the value_counts() method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the other fields. The describe() method shows a summary of the numerical attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few things you might notice in these histograms:\n* The median income attribute, the housing median age and the median house value, has been scaled and capped at the highest and lowest value.\n* These attributes have very different scales.\n* Many histograms are tail-heavy."},{"metadata":{},"cell_type":"markdown","source":"### Create a Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"median_income\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making the assumptionthat the median income is a very important attribute to predict median housing prices, we want to ensure that the test set is representative of the various categories of incomes in the whole dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Discover and visualize the data to gain insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a copy of our training set so we can play with it without harming it\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the high-density areas in California\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\nsave_fig(\"better_visualization_plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualize the housing prices\n## the radius of each circle represents the district's population the color reprents the price\n## color map ranges from blue (low values) to red (high prices)\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the California image\nimages_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nfilename = \"california.png\"\nprint(\"Downloading\", filename)\nurl = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(os.path.join(images_path, filename))\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=housing['population']/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar(ticks=tick_values/prices.max())\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nsave_fig(\"california_housing_prices_plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking for Correlations"},{"metadata":{},"cell_type":"markdown","source":"Since the dataset is not too large, we can easily compute the standard correlation coefficient between every pair of attributes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this data we can see that the most promising attribute to predict the median house value is the median income."},{"metadata":{},"cell_type":"markdown","source":"### Experiment with Attribute Combinations"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Prepare the data for Machine Learning algorithms"},{"metadata":{},"cell_type":"markdown","source":"First, let's revert to a clean training set and let's also separate the predictors and the labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Most Machine Learning algorithms cannot work with missing features, so let's create a new function to take care of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scikit-Learn provides a handy class to take care of missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the text attribute because median can only be calculated on numerical attributes\nhousing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the training set and set it back into a pandas DataFrame\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to deal with the text attributes; the ocean_proximity feature, which is a categorical variable with text attributes. Most machine learning algorithms prefer to work with numbers, so let's convert these categories from text to numbers by using the scikit-learn's OrdinalEncoder class."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's preprocess the categorical input feature\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can get a list of categories\nordinal_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But to avoid that the algorithms assume that two nearby values are more similar than two distant ones, we will instead create dummy values with scikit-learn's tools OneHotEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a custom transformer to add extra attributes\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # get the column indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature scaling"},{"metadata":{},"cell_type":"markdown","source":"Machine learning algorithms don't perform well when the input numerical attributes have very different scales, we will fix this as we create pipelines with the past steps to facilitate future analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's build a pipeline for preprocessing the numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can build a pipeline for both numerical and categorical attributes preprocessing\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\n\nhousing_prepared ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Select and train a model"},{"metadata":{},"cell_type":"markdown","source":"We will try three models; Linear Regression, Decision Tree Regressor and Random Forest Regressor."},{"metadata":{},"cell_type":"markdown","source":"### Linear Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Measure the RMSE on the whole training set\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define and fit the model\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's impossible there's no error, it's very likely that the model has badly overfit the data. We will evaluate this assumption with a cross-validations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This step allowed us to estimate the performance of our decision tree model and see that it actually doesn't seem more accurate than the Linear Regression model, but we can compute the same scores to be sure."},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's right, the Decision Tree model is overfitting so badly that it performs worst than the Linear Regression model."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define and fit the model\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation score\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forests look very promising. However, the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions would be to simplify the model, constrain it, or get a lot more training data. But we will leave that for future Mar."},{"metadata":{"trusted":true},"cell_type":"code","source":"## 5. Evaluate the system on the Test Set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = forest_reg.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}