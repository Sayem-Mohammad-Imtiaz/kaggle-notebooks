{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-09T09:59:37.829419Z","iopub.execute_input":"2021-06-09T09:59:37.829945Z","iopub.status.idle":"2021-06-09T09:59:37.832917Z","shell.execute_reply.started":"2021-06-09T09:59:37.829912Z","shell.execute_reply":"2021-06-09T09:59:37.832215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:37.835176Z","iopub.execute_input":"2021-06-09T09:59:37.835625Z","iopub.status.idle":"2021-06-09T09:59:39.328927Z","shell.execute_reply.started":"2021-06-09T09:59:37.835594Z","shell.execute_reply":"2021-06-09T09:59:39.327985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape\n# shape of the dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:39.330257Z","iopub.execute_input":"2021-06-09T09:59:39.330528Z","iopub.status.idle":"2021-06-09T09:59:39.342455Z","shell.execute_reply.started":"2021-06-09T09:59:39.3305Z","shell.execute_reply":"2021-06-09T09:59:39.341498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.columns\n# columns in the dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:39.344402Z","iopub.execute_input":"2021-06-09T09:59:39.344818Z","iopub.status.idle":"2021-06-09T09:59:39.353195Z","shell.execute_reply.started":"2021-06-09T09:59:39.344774Z","shell.execute_reply":"2021-06-09T09:59:39.35251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(n=10)\n# first 10 rows of the dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:39.354538Z","iopub.execute_input":"2021-06-09T09:59:39.355025Z","iopub.status.idle":"2021-06-09T09:59:39.389198Z","shell.execute_reply.started":"2021-06-09T09:59:39.354995Z","shell.execute_reply":"2021-06-09T09:59:39.388501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:39.390257Z","iopub.execute_input":"2021-06-09T09:59:39.390677Z","iopub.status.idle":"2021-06-09T09:59:39.9291Z","shell.execute_reply.started":"2021-06-09T09:59:39.390647Z","shell.execute_reply":"2021-06-09T09:59:39.928027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=dataset['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:39.930723Z","iopub.execute_input":"2021-06-09T09:59:39.931164Z","iopub.status.idle":"2021-06-09T09:59:40.128763Z","shell.execute_reply.started":"2021-06-09T09:59:39.931119Z","shell.execute_reply":"2021-06-09T09:59:40.127527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['sentiment'].value_counts()\n# count of each sentiment","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:40.1303Z","iopub.execute_input":"2021-06-09T09:59:40.130628Z","iopub.status.idle":"2021-06-09T09:59:40.150735Z","shell.execute_reply.started":"2021-06-09T09:59:40.130598Z","shell.execute_reply":"2021-06-09T09:59:40.149521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (2) Stop word Removal and stemming","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n# to dealing with stopwords\n\nfrom nltk.stem import PorterStemmer\n# PorterStemmer --> for stemming the text\n\nimport re\n# re --> regular expression","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:40.153436Z","iopub.execute_input":"2021-06-09T09:59:40.153724Z","iopub.status.idle":"2021-06-09T09:59:41.118595Z","shell.execute_reply.started":"2021-06-09T09:59:40.153696Z","shell.execute_reply":"2021-06-09T09:59:41.117859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sw = stopwords.words('english') # as we needs to remove english stopwords\nps = PorterStemmer() # creating an object of PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:41.120528Z","iopub.execute_input":"2021-06-09T09:59:41.121152Z","iopub.status.idle":"2021-06-09T09:59:41.139972Z","shell.execute_reply.started":"2021-06-09T09:59:41.121108Z","shell.execute_reply":"2021-06-09T09:59:41.139121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(sample):\n    sample = sample.lower()\n    # lowering the entire text\n    \n    sample = sample.replace(\"<br /><br />\",\"\")\n    # as this is an html text. hence it is containing '<br>'\n    # So, we are replacing the <br> with \"\", ie, remove this <br> tags\n    \n    sample = re.sub(\"[^a-zA-Z]+\",\" \",sample)\n    # to remove those characters which are not the alphabets and replacing them with \" \".\n    \n    sample = sample.split()\n    # to apply stopword removal and stemming, we needs to iterate over the text, which is only possible \n    # if we convert this text into a list. Hence, converting this into a list.\n    \n    sample = [ps.stem(s) for s in sample if s not in sw]\n    # iterating over the list to perform stemming\n    \n    sample = \" \".join(sample)\n    # after stemming, re join the list back into a text.\n    \n    return sample","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:41.141223Z","iopub.execute_input":"2021-06-09T09:59:41.141841Z","iopub.status.idle":"2021-06-09T09:59:41.148928Z","shell.execute_reply.started":"2021-06-09T09:59:41.141795Z","shell.execute_reply":"2021-06-09T09:59:41.148066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['review'][0]\n# first review with stopwords and without stemming","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:41.150351Z","iopub.execute_input":"2021-06-09T09:59:41.150822Z","iopub.status.idle":"2021-06-09T09:59:41.164264Z","shell.execute_reply.started":"2021-06-09T09:59:41.150737Z","shell.execute_reply":"2021-06-09T09:59:41.16348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text(dataset['review'][0])\n# first review without stopwords and after stemming","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:41.16561Z","iopub.execute_input":"2021-06-09T09:59:41.165931Z","iopub.status.idle":"2021-06-09T09:59:41.183848Z","shell.execute_reply.started":"2021-06-09T09:59:41.165902Z","shell.execute_reply":"2021-06-09T09:59:41.18315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying the stopword removal and stemming over entire dataset","metadata":{}},{"cell_type":"code","source":"dataset['review'] = dataset['review'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T09:59:41.184889Z","iopub.execute_input":"2021-06-09T09:59:41.185313Z","iopub.status.idle":"2021-06-09T10:03:04.692484Z","shell.execute_reply.started":"2021-06-09T09:59:41.185283Z","shell.execute_reply":"2021-06-09T10:03:04.691564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(n=10)\n# dataset after the removal of stopwords","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:04.694195Z","iopub.execute_input":"2021-06-09T10:03:04.694592Z","iopub.status.idle":"2021-06-09T10:03:04.705915Z","shell.execute_reply.started":"2021-06-09T10:03:04.694547Z","shell.execute_reply":"2021-06-09T10:03:04.70494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (3) Creating the Vocab","metadata":{}},{"cell_type":"code","source":"max_features = 10000\n# the number of words in the vocab = 10000\n# 10000 is basically the number of unique words, ie. vocabulary size is 10000\n# so the first 10000 relevant words will be used.","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:04.707342Z","iopub.execute_input":"2021-06-09T10:03:04.707723Z","iopub.status.idle":"2021-06-09T10:03:04.718537Z","shell.execute_reply.started":"2021-06-09T10:03:04.707684Z","shell.execute_reply":"2021-06-09T10:03:04.717797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:04.719817Z","iopub.execute_input":"2021-06-09T10:03:04.72011Z","iopub.status.idle":"2021-06-09T10:03:04.732667Z","shell.execute_reply.started":"2021-06-09T10:03:04.720083Z","shell.execute_reply":"2021-06-09T10:03:04.731725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features, split=' ')\n# tokenizer --> a vocab of 10000 words\n\ntokenizer.fit_on_texts(dataset['review'].values)\n# applying tokenization on the dataset, it will take first 10000 words","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:04.73425Z","iopub.execute_input":"2021-06-09T10:03:04.734942Z","iopub.status.idle":"2021-06-09T10:03:10.01752Z","shell.execute_reply.started":"2021-06-09T10:03:04.7349Z","shell.execute_reply":"2021-06-09T10:03:10.016821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.index_word\n# it is saying that the first word in vocab is 'movi'\n# second word is 'film'","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:10.018696Z","iopub.execute_input":"2021-06-09T10:03:10.019448Z","iopub.status.idle":"2021-06-09T10:03:10.061944Z","shell.execute_reply.started":"2021-06-09T10:03:10.019401Z","shell.execute_reply":"2021-06-09T10:03:10.060916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.word_counts\n# it means 'one' has occured 55435 times in entire dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:10.063232Z","iopub.execute_input":"2021-06-09T10:03:10.063544Z","iopub.status.idle":"2021-06-09T10:03:10.151765Z","shell.execute_reply.started":"2021-06-09T10:03:10.063512Z","shell.execute_reply":"2021-06-09T10:03:10.148554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:10.152827Z","iopub.execute_input":"2021-06-09T10:03:10.153221Z","iopub.status.idle":"2021-06-09T10:03:10.157099Z","shell.execute_reply.started":"2021-06-09T10:03:10.15319Z","shell.execute_reply":"2021-06-09T10:03:10.155845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now saving this tokenizer\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:10.159049Z","iopub.execute_input":"2021-06-09T10:03:10.159548Z","iopub.status.idle":"2021-06-09T10:03:10.269885Z","shell.execute_reply.started":"2021-06-09T10:03:10.159506Z","shell.execute_reply":"2021-06-09T10:03:10.268984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (4) Creating the X, \n### X --> rows = 50000\n### X --> columns = not specific, it contains different columns for each row, where column number is equals to number of words in that review (or row)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T07:58:19.933913Z","iopub.execute_input":"2021-06-09T07:58:19.934315Z","iopub.status.idle":"2021-06-09T07:58:19.938035Z","shell.execute_reply.started":"2021-06-09T07:58:19.934283Z","shell.execute_reply":"2021-06-09T07:58:19.937069Z"}}},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(dataset['review'].values)\n# X --> in each row it basically contains index of those words which are there in the review","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:10.271281Z","iopub.execute_input":"2021-06-09T10:03:10.271572Z","iopub.status.idle":"2021-06-09T10:03:14.103326Z","shell.execute_reply.started":"2021-06-09T10:03:10.271544Z","shell.execute_reply":"2021-06-09T10:03:14.102225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X)\n# ie, it has 50000 reviews","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.10845Z","iopub.execute_input":"2021-06-09T10:03:14.108778Z","iopub.status.idle":"2021-06-09T10:03:14.114383Z","shell.execute_reply.started":"2021-06-09T10:03:14.108734Z","shell.execute_reply":"2021-06-09T10:03:14.113422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X[0])\n# ie, first review has 153 words","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.116431Z","iopub.execute_input":"2021-06-09T10:03:14.116706Z","iopub.status.idle":"2021-06-09T10:03:14.128865Z","shell.execute_reply.started":"2021-06-09T10:03:14.116679Z","shell.execute_reply":"2021-06-09T10:03:14.128026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X[1])\n# ie, second review has 79 words","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.130081Z","iopub.execute_input":"2021-06-09T10:03:14.130343Z","iopub.status.idle":"2021-06-09T10:03:14.139642Z","shell.execute_reply.started":"2021-06-09T10:03:14.130318Z","shell.execute_reply":"2021-06-09T10:03:14.138794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X[0])\n# they are the indexes of 153 words of first review","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.140919Z","iopub.execute_input":"2021-06-09T10:03:14.141303Z","iopub.status.idle":"2021-06-09T10:03:14.152636Z","shell.execute_reply.started":"2021-06-09T10:03:14.141263Z","shell.execute_reply":"2021-06-09T10:03:14.151958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (5) XT --> taking first 25000 reviews into the training set\n###       Xt --> taking last 25000 reviews into the testing set","metadata":{}},{"cell_type":"code","source":"XT = X[:25000] # XT --> taking first 25000 reviews into the training set\nXt = X[25000:] # Xt --> taking last 25000 reviews into the testing set\n# NOTE --> ie, we have divided X into two parts --> i) XT and ii) Xt","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.153459Z","iopub.execute_input":"2021-06-09T10:03:14.153699Z","iopub.status.idle":"2021-06-09T10:03:14.162823Z","shell.execute_reply.started":"2021-06-09T10:03:14.153674Z","shell.execute_reply":"2021-06-09T10:03:14.161829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(XT))\nprint(len(Xt))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.163956Z","iopub.execute_input":"2021-06-09T10:03:14.164214Z","iopub.status.idle":"2021-06-09T10:03:14.174276Z","shell.execute_reply.started":"2021-06-09T10:03:14.164189Z","shell.execute_reply":"2021-06-09T10:03:14.173359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (6) Vectorization of reviews, ie, making each review of size 10000\n#### (6.1) X_train and X_test","metadata":{}},{"cell_type":"code","source":"def vectorize_sentences(sentences,dim=10000):\n    outputs=np.zeros((len(sentences),dim)) \n    # outputs --> (25000 x 10000) matrix\n    \n    for i,idx in enumerate (sentences):\n        outputs[i,idx]=1\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.175448Z","iopub.execute_input":"2021-06-09T10:03:14.175701Z","iopub.status.idle":"2021-06-09T10:03:14.183456Z","shell.execute_reply.started":"2021-06-09T10:03:14.175677Z","shell.execute_reply":"2021-06-09T10:03:14.182839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = vectorize_sentences(XT)\nX_test = vectorize_sentences(Xt)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:14.184603Z","iopub.execute_input":"2021-06-09T10:03:14.185166Z","iopub.status.idle":"2021-06-09T10:03:17.026034Z","shell.execute_reply.started":"2021-06-09T10:03:14.185125Z","shell.execute_reply":"2021-06-09T10:03:17.025262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\n# basically in each row we have one review and for that review we have 10000 columns\n# and if that index word is present in review then it will be 1 otherwise it will be 0","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.027206Z","iopub.execute_input":"2021-06-09T10:03:17.027795Z","iopub.status.idle":"2021-06-09T10:03:17.033302Z","shell.execute_reply.started":"2021-06-09T10:03:17.027735Z","shell.execute_reply":"2021-06-09T10:03:17.032356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### (6.2) Y_train and Y_test\n#### Encoding positive as 1 and negative as 0","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# we have two types of sentiment, so to encode them with numbers","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.034894Z","iopub.execute_input":"2021-06-09T10:03:17.035277Z","iopub.status.idle":"2021-06-09T10:03:17.049372Z","shell.execute_reply.started":"2021-06-09T10:03:17.035238Z","shell.execute_reply":"2021-06-09T10:03:17.04862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = dataset['sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.050744Z","iopub.execute_input":"2021-06-09T10:03:17.051314Z","iopub.status.idle":"2021-06-09T10:03:17.062276Z","shell.execute_reply.started":"2021-06-09T10:03:17.05127Z","shell.execute_reply":"2021-06-09T10:03:17.061261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = le.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.063466Z","iopub.execute_input":"2021-06-09T10:03:17.064026Z","iopub.status.idle":"2021-06-09T10:03:17.093255Z","shell.execute_reply.started":"2021-06-09T10:03:17.063983Z","shell.execute_reply":"2021-06-09T10:03:17.092155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[:100]\n# 1--> positive\n# 0--> negative\n# value of first 100 reviews","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.09442Z","iopub.execute_input":"2021-06-09T10:03:17.094709Z","iopub.status.idle":"2021-06-09T10:03:17.110565Z","shell.execute_reply.started":"2021-06-09T10:03:17.094681Z","shell.execute_reply":"2021-06-09T10:03:17.109397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = y[:25000] # YT --> training set of y\nY_test = y[25000:] # Yt --> test set of y","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.112036Z","iopub.execute_input":"2021-06-09T10:03:17.112414Z","iopub.status.idle":"2021-06-09T10:03:17.122696Z","shell.execute_reply.started":"2021-06-09T10:03:17.112383Z","shell.execute_reply":"2021-06-09T10:03:17.121502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(Y_train))\nprint(len(Y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.124085Z","iopub.execute_input":"2021-06-09T10:03:17.124459Z","iopub.status.idle":"2021-06-09T10:03:17.135683Z","shell.execute_reply.started":"2021-06-09T10:03:17.124428Z","shell.execute_reply":"2021-06-09T10:03:17.134498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (7) Spliting the training set (25000) into \n### i) train_new (20000)\n### ii) validation (5000)","metadata":{}},{"cell_type":"code","source":"x_train_new = X_train[:20000]\nx_val = X_train[20000:]\n\ny_train_new = Y_train[:20000]\ny_val = Y_train[20000:]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.137208Z","iopub.execute_input":"2021-06-09T10:03:17.137705Z","iopub.status.idle":"2021-06-09T10:03:17.146132Z","shell.execute_reply.started":"2021-06-09T10:03:17.137663Z","shell.execute_reply":"2021-06-09T10:03:17.145041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_val.shape)\nprint(x_train_new.shape)\nprint(y_val.shape)\nprint(y_train_new.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.147711Z","iopub.execute_input":"2021-06-09T10:03:17.1482Z","iopub.status.idle":"2021-06-09T10:03:17.160505Z","shell.execute_reply.started":"2021-06-09T10:03:17.148158Z","shell.execute_reply":"2021-06-09T10:03:17.159471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (8) Defining the model architecture\n#### Using Fully Connected/ Dense Layers with ReLu activation\n#### 2 Hidden layers with 16 units each\n#### 1 output layer with 1 unit (Sigmoid activation)","metadata":{}},{"cell_type":"code","source":"from keras import models\nfrom keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.162376Z","iopub.execute_input":"2021-06-09T10:03:17.163172Z","iopub.status.idle":"2021-06-09T10:03:17.172207Z","shell.execute_reply.started":"2021-06-09T10:03:17.163124Z","shell.execute_reply":"2021-06-09T10:03:17.171215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=models.Sequential()\nmodel.add(Dense(16,activation='relu',input_shape=(10000,))) # first hidden layer having 16 neurons.\nmodel.add(Dense(16,activation='relu')) # second hidden layer having 16 neurons.\nmodel.add(Dense(1,activation='sigmoid')) # output layer having only 1 neuron which can be used for binary classification.","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:07:21.270575Z","iopub.execute_input":"2021-06-09T10:07:21.270945Z","iopub.status.idle":"2021-06-09T10:07:21.305025Z","shell.execute_reply.started":"2021-06-09T10:07:21.270914Z","shell.execute_reply":"2021-06-09T10:07:21.304128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()\n# parameters are nothing but weights and biases\n# 160016 = (10000 * 16) + 16, 10000-->features || 16-->neurons in H1 layer || 16-->bias term\n# 272 = (16 * 16) + 16, 16-->neurons in H1 layer || 16-->neurons in H2 layer || 16-->bias term\n# 17 = (16 * 1) + 1, 16-->neurons in H2 layer || 1-->neurons in O/P layer || 1-->bias term","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:07:22.94602Z","iopub.execute_input":"2021-06-09T10:07:22.946376Z","iopub.status.idle":"2021-06-09T10:07:22.956036Z","shell.execute_reply.started":"2021-06-09T10:07:22.946344Z","shell.execute_reply":"2021-06-09T10:07:22.953999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n# loss function --> binary cross entropy\n# optimizer --> rmsprop (it basically helps to reduce the loss function)\n# metrics --> after each epoch, we can judge our training procedure, for that judgement we are using\n#             'accuracy' parameter here.","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:07:36.219332Z","iopub.execute_input":"2021-06-09T10:07:36.219714Z","iopub.status.idle":"2021-06-09T10:07:36.232543Z","shell.execute_reply.started":"2021-06-09T10:07:36.219681Z","shell.execute_reply":"2021-06-09T10:07:36.231643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Executing the model\nhist = model.fit(\n    x_train_new, \n    y_train_new,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val,y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:17.311159Z","iopub.execute_input":"2021-06-09T10:03:17.311475Z","iopub.status.idle":"2021-06-09T10:03:34.466602Z","shell.execute_reply.started":"2021-06-09T10:03:17.311443Z","shell.execute_reply":"2021-06-09T10:03:34.465447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (9) Visualizing the validation accuracy and loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:34.468543Z","iopub.execute_input":"2021-06-09T10:03:34.468861Z","iopub.status.idle":"2021-06-09T10:03:34.472948Z","shell.execute_reply.started":"2021-06-09T10:03:34.46883Z","shell.execute_reply":"2021-06-09T10:03:34.472018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:34.474034Z","iopub.execute_input":"2021-06-09T10:03:34.474337Z","iopub.status.idle":"2021-06-09T10:03:34.488076Z","shell.execute_reply.started":"2021-06-09T10:03:34.474309Z","shell.execute_reply":"2021-06-09T10:03:34.487103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = hist.history\n# it is a dictionary having keys as 'accuracy' , 'loss', 'val_accuracy' and their values","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:34.48925Z","iopub.execute_input":"2021-06-09T10:03:34.489509Z","iopub.status.idle":"2021-06-09T10:03:34.506325Z","shell.execute_reply.started":"2021-06-09T10:03:34.489484Z","shell.execute_reply":"2021-06-09T10:03:34.505222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### (9.1) Loss vs Epoch","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:06:46.534401Z","iopub.execute_input":"2021-06-09T08:06:46.534777Z","iopub.status.idle":"2021-06-09T08:06:46.538594Z","shell.execute_reply.started":"2021-06-09T08:06:46.534746Z","shell.execute_reply":"2021-06-09T08:06:46.537272Z"}}},{"cell_type":"code","source":"plt.plot(h['val_loss'],label=\"Validation Loss\")\nplt.plot(h['loss'],label=\"Training Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n# ie, after some epochs our validation loss decreases but after that it starts to increase \n# which basically means overfitting","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:34.507581Z","iopub.execute_input":"2021-06-09T10:03:34.508034Z","iopub.status.idle":"2021-06-09T10:03:34.69567Z","shell.execute_reply.started":"2021-06-09T10:03:34.507989Z","shell.execute_reply":"2021-06-09T10:03:34.694737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### (9.2) Accuracy vs Epoch","metadata":{"execution":{"iopub.status.busy":"2021-06-09T08:07:21.545411Z","iopub.execute_input":"2021-06-09T08:07:21.545821Z","iopub.status.idle":"2021-06-09T08:07:21.549635Z","shell.execute_reply.started":"2021-06-09T08:07:21.545788Z","shell.execute_reply":"2021-06-09T08:07:21.548684Z"}}},{"cell_type":"code","source":"plt.plot(h['val_accuracy'],label=\"Validation Acc\")\nplt.plot(h['accuracy'],label=\"Training Acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n# after some epochs, validation accuracy starts to decrease---> overfitting\n# so now we will stop after some 3-4 epochs called as stop early.","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:03:34.698541Z","iopub.execute_input":"2021-06-09T10:03:34.698844Z","iopub.status.idle":"2021-06-09T10:03:34.883908Z","shell.execute_reply.started":"2021-06-09T10:03:34.698815Z","shell.execute_reply":"2021-06-09T10:03:34.882839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (10) So now we will run our model for 4 epochs","metadata":{}},{"cell_type":"code","source":"# when epochs = 4\nhist = model.fit(\n    x_train_new,\n    y_train_new,epochs=4,\n    batch_size=512,\n    validation_data=(x_val,y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:07:46.157623Z","iopub.execute_input":"2021-06-09T10:07:46.158039Z","iopub.status.idle":"2021-06-09T10:07:51.931244Z","shell.execute_reply.started":"2021-06-09T10:07:46.158005Z","shell.execute_reply":"2021-06-09T10:07:51.930351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = hist.history","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:07:55.514595Z","iopub.execute_input":"2021-06-09T10:07:55.515123Z","iopub.status.idle":"2021-06-09T10:07:55.518436Z","shell.execute_reply.started":"2021-06-09T10:07:55.515089Z","shell.execute_reply":"2021-06-09T10:07:55.517724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loss vs Epoch","metadata":{}},{"cell_type":"code","source":"plt.plot(h['val_loss'],label=\"Validation Loss\")\nplt.plot(h['loss'],label=\"Training Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:05.544968Z","iopub.execute_input":"2021-06-09T10:08:05.545312Z","iopub.status.idle":"2021-06-09T10:08:05.734377Z","shell.execute_reply.started":"2021-06-09T10:08:05.545283Z","shell.execute_reply":"2021-06-09T10:08:05.733347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy vs Epoch","metadata":{}},{"cell_type":"code","source":"plt.plot(h['val_accuracy'],label=\"Validation Acc\")\nplt.plot(h['accuracy'],label=\"Training Acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:07.683635Z","iopub.execute_input":"2021-06-09T10:08:07.684008Z","iopub.status.idle":"2021-06-09T10:08:07.868701Z","shell.execute_reply.started":"2021-06-09T10:08:07.683975Z","shell.execute_reply":"2021-06-09T10:08:07.86772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (11) Analyzing the developed model","metadata":{}},{"cell_type":"code","source":"x = model.evaluate(X_test,Y_test)[1]*100\nx = round(x,2)\nprint(\"Accuracy on test set = \",x,\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:09.541014Z","iopub.execute_input":"2021-06-09T10:08:09.541412Z","iopub.status.idle":"2021-06-09T10:08:12.217032Z","shell.execute_reply.started":"2021-06-09T10:08:09.541377Z","shell.execute_reply":"2021-06-09T10:08:12.216005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = model.evaluate(X_train,Y_train)[1]*100\nx = round(x,2)\nprint(\"Accuracy on training set = \",x,\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:12.670795Z","iopub.execute_input":"2021-06-09T10:08:12.671189Z","iopub.status.idle":"2021-06-09T10:08:15.629603Z","shell.execute_reply.started":"2021-06-09T10:08:12.671154Z","shell.execute_reply":"2021-06-09T10:08:15.628572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = model.predict(X_test)\n# applying on test dataset\n# result basically has the probability of each reviews that by what probability it can be positive(ie,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:15.633112Z","iopub.execute_input":"2021-06-09T10:08:15.633419Z","iopub.status.idle":"2021-06-09T10:08:18.282535Z","shell.execute_reply.started":"2021-06-09T10:08:15.633386Z","shell.execute_reply":"2021-06-09T10:08:18.281293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(result)\n# first review has 0.04 probability that it is positive and so on...","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:19.817504Z","iopub.execute_input":"2021-06-09T10:08:19.818411Z","iopub.status.idle":"2021-06-09T10:08:19.82496Z","shell.execute_reply.started":"2021-06-09T10:08:19.818343Z","shell.execute_reply":"2021-06-09T10:08:19.823664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_train = model.predict(X_train)\n# applying model on training dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:21.07544Z","iopub.execute_input":"2021-06-09T10:08:21.07609Z","iopub.status.idle":"2021-06-09T10:08:23.691708Z","shell.execute_reply.started":"2021-06-09T10:08:21.076046Z","shell.execute_reply":"2021-06-09T10:08:23.690602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(result_train)\n# first review has 0.97 probability that it is positive and so on...","metadata":{"execution":{"iopub.status.busy":"2021-06-09T10:08:28.089283Z","iopub.execute_input":"2021-06-09T10:08:28.089671Z","iopub.status.idle":"2021-06-09T10:08:28.096452Z","shell.execute_reply.started":"2021-06-09T10:08:28.089636Z","shell.execute_reply":"2021-06-09T10:08:28.095155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ------------------------------END----------------------------------","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}