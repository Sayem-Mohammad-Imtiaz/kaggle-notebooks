{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T04:58:13.106568Z","iopub.execute_input":"2021-06-05T04:58:13.106912Z","iopub.status.idle":"2021-06-05T04:58:13.117513Z","shell.execute_reply.started":"2021-06-05T04:58:13.106883Z","shell.execute_reply":"2021-06-05T04:58:13.116486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 50px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : #bedcfa; border-radius: 5px 5px;\"><strong>EDA on fake-and-real news</strong></p>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size : 35px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Importing Libraries</strong></p>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:13.130682Z","iopub.execute_input":"2021-06-05T04:58:13.131048Z","iopub.status.idle":"2021-06-05T04:58:13.137182Z","shell.execute_reply.started":"2021-06-05T04:58:13.131016Z","shell.execute_reply":"2021-06-05T04:58:13.136134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Read The Datasets</strong></p>","metadata":{}},{"cell_type":"code","source":"fake=pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\ntrue=pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:13.138742Z","iopub.execute_input":"2021-06-05T04:58:13.139034Z","iopub.status.idle":"2021-06-05T04:58:14.306764Z","shell.execute_reply.started":"2021-06-05T04:58:13.139005Z","shell.execute_reply":"2021-06-05T04:58:14.305684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.308828Z","iopub.execute_input":"2021-06-05T04:58:14.309165Z","iopub.status.idle":"2021-06-05T04:58:14.322771Z","shell.execute_reply.started":"2021-06-05T04:58:14.309135Z","shell.execute_reply":"2021-06-05T04:58:14.321638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.324371Z","iopub.execute_input":"2021-06-05T04:58:14.324682Z","iopub.status.idle":"2021-06-05T04:58:14.343879Z","shell.execute_reply.started":"2021-06-05T04:58:14.324654Z","shell.execute_reply":"2021-06-05T04:58:14.342675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">Now we introduce a new column named as target. Here target 1 shows the real news and target 0 shows the fake news.And then we just concatenate both the dataframes to give a single dataframe and then we can further proceed for exploratory data analysis </p>","metadata":{}},{"cell_type":"code","source":"true['target']=1\nfake['target']=0","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.345535Z","iopub.execute_input":"2021-06-05T04:58:14.345959Z","iopub.status.idle":"2021-06-05T04:58:14.360318Z","shell.execute_reply.started":"2021-06-05T04:58:14.345914Z","shell.execute_reply":"2021-06-05T04:58:14.358857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([true,fake])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.362004Z","iopub.execute_input":"2021-06-05T04:58:14.362486Z","iopub.status.idle":"2021-06-05T04:58:14.378793Z","shell.execute_reply.started":"2021-06-05T04:58:14.362441Z","shell.execute_reply":"2021-06-05T04:58:14.377638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.381969Z","iopub.execute_input":"2021-06-05T04:58:14.38261Z","iopub.status.idle":"2021-06-05T04:58:14.393432Z","shell.execute_reply.started":"2021-06-05T04:58:14.382564Z","shell.execute_reply":"2021-06-05T04:58:14.392336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.395702Z","iopub.execute_input":"2021-06-05T04:58:14.39608Z","iopub.status.idle":"2021-06-05T04:58:14.413075Z","shell.execute_reply.started":"2021-06-05T04:58:14.396045Z","shell.execute_reply":"2021-06-05T04:58:14.411633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>target feature view</strong></p>","metadata":{}},{"cell_type":"code","source":"temp = df.groupby('target').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.414865Z","iopub.execute_input":"2021-06-05T04:58:14.415405Z","iopub.status.idle":"2021-06-05T04:58:14.471789Z","shell.execute_reply.started":"2021-06-05T04:58:14.415356Z","shell.execute_reply":"2021-06-05T04:58:14.470491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='target',data=df,palette=['orange','purple'])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.472969Z","iopub.execute_input":"2021-06-05T04:58:14.473272Z","iopub.status.idle":"2021-06-05T04:58:14.600568Z","shell.execute_reply.started":"2021-06-05T04:58:14.473243Z","shell.execute_reply":"2021-06-05T04:58:14.599404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly import graph_objs as go\nfig = go.Figure(go.Funnelarea(\n    text =temp.target,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Target Distribution\"}\n    ))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.603046Z","iopub.execute_input":"2021-06-05T04:58:14.603737Z","iopub.status.idle":"2021-06-05T04:58:14.616182Z","shell.execute_reply.started":"2021-06-05T04:58:14.603686Z","shell.execute_reply":"2021-06-05T04:58:14.614987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">In the final dataset the count of fake news is greater than real news </p>","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.617504Z","iopub.execute_input":"2021-06-05T04:58:14.618268Z","iopub.status.idle":"2021-06-05T04:58:14.648583Z","shell.execute_reply.started":"2021-06-05T04:58:14.618214Z","shell.execute_reply":"2021-06-05T04:58:14.6478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">No null values are present in the dataset </p>","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:14.649893Z","iopub.execute_input":"2021-06-05T04:58:14.650191Z","iopub.status.idle":"2021-06-05T04:58:15.012987Z","shell.execute_reply.started":"2021-06-05T04:58:14.650161Z","shell.execute_reply":"2021-06-05T04:58:15.012071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:15.014544Z","iopub.execute_input":"2021-06-05T04:58:15.014834Z","iopub.status.idle":"2021-06-05T04:58:15.021898Z","shell.execute_reply.started":"2021-06-05T04:58:15.014806Z","shell.execute_reply":"2021-06-05T04:58:15.021013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>subject vs target</strong></p>","metadata":{}},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(15,5))\nsns.countplot(x='subject',data=df,hue='target')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:15.023179Z","iopub.execute_input":"2021-06-05T04:58:15.023533Z","iopub.status.idle":"2021-06-05T04:58:15.315536Z","shell.execute_reply.started":"2021-06-05T04:58:15.023504Z","shell.execute_reply":"2021-06-05T04:58:15.314475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">This shows that all the real news have the subject politicsNews and worldnews. Apart from these subjects other six subjects lies in the fake news category </p>","metadata":{}},{"cell_type":"code","source":"df['text']=df['text']+\" \"+df['title']","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:15.316852Z","iopub.execute_input":"2021-06-05T04:58:15.317152Z","iopub.status.idle":"2021-06-05T04:58:15.580432Z","shell.execute_reply.started":"2021-06-05T04:58:15.317123Z","shell.execute_reply":"2021-06-05T04:58:15.579407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['title','subject','date'],axis=1,inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:15.581676Z","iopub.execute_input":"2021-06-05T04:58:15.581945Z","iopub.status.idle":"2021-06-05T04:58:15.596089Z","shell.execute_reply.started":"2021-06-05T04:58:15.581917Z","shell.execute_reply":"2021-06-05T04:58:15.595034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>No of words in real & fake news</strong></p>","metadata":{}},{"cell_type":"code","source":"no_words=df[df['target']==1].text.str.split().map(lambda x:len(x))\nno_words.plot(kind='hist',edgecolor='black',color='lightgreen',title='no of words in Real')\nplt.show()\nno_words=df[df['target']==0].text.str.split().map(lambda x:len(x))\nno_words.plot(kind='hist',edgecolor='black',color='lightblue',title='no of words in fake')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:15.5973Z","iopub.execute_input":"2021-06-05T04:58:15.597772Z","iopub.status.idle":"2021-06-05T04:58:18.503935Z","shell.execute_reply.started":"2021-06-05T04:58:15.597736Z","shell.execute_reply":"2021-06-05T04:58:18.502409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">No of words in real news are lying in the range of 0 to 1500\nwhereas in the case of fake news it lies in the range of 0 to 2000.This shows that number of words in the fake news are higher than that of real news</p>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Average length of words in real & fake news</strong></p>","metadata":{}},{"cell_type":"code","source":"avg_len_word=df[df['target']==1].text.str.split().map(lambda x:np.mean([len(word) for word in x]))\navg_len_word.plot(kind='hist',edgecolor='black',color='lightcoral',title='avg length of words in true')\nplt.show()\navg_len_word=df[df['target']==0].text.str.split().map(lambda x:np.mean([len(word) for word in x]))\navg_len_word.plot(kind='hist',edgecolor='black',color='lightyellow',title='avg length of words in false')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:18.509199Z","iopub.execute_input":"2021-06-05T04:58:18.509574Z","iopub.status.idle":"2021-06-05T04:58:27.066088Z","shell.execute_reply.started":"2021-06-05T04:58:18.509534Z","shell.execute_reply":"2021-06-05T04:58:27.065078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:150%; font-family:cursive;\">The average length of word in real news ranging from 4.5 to 6.0\nwhereas in the case of fake news it lies in the range of 0 to 20.This shows that in the fake news most of the words have length greater than the words of real news.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Analyzing the top stop words in the real and fake news</strong></p>","metadata":{}},{"cell_type":"markdown","source":"# First way","metadata":{}},{"cell_type":"code","source":"# creating sample words\ndef create_words(target):\n    words = []\n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            words.append(i)\n    return words","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:27.068186Z","iopub.execute_input":"2021-06-05T04:58:27.068534Z","iopub.status.idle":"2021-06-05T04:58:27.073224Z","shell.execute_reply.started":"2021-06-05T04:58:27.068504Z","shell.execute_reply":"2021-06-05T04:58:27.072237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\ndef analyze_stopwords(data,fun,target):\n    values_list=[]\n    dic=defaultdict(int)\n    for i in range(0,len(target)):\n        corpus=fun(target[i])\n        for word in corpus:\n            dic[word]+=1\n        top=sorted(dic.items(),key=lambda x:x[1],reverse=True)[:10]\n        x_items,y_items=zip(*top)\n        values_list.append(x_items)\n        values_list.append(y_items)\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Real\")\n    \n    ax2.bar(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Fake\")\n            \n    plt.suptitle(\"Top Stop words in text\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:27.074767Z","iopub.execute_input":"2021-06-05T04:58:27.075344Z","iopub.status.idle":"2021-06-05T04:58:27.092358Z","shell.execute_reply.started":"2021-06-05T04:58:27.0753Z","shell.execute_reply":"2021-06-05T04:58:27.091251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_stopwords(df,create_words,[1,0])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:27.093562Z","iopub.execute_input":"2021-06-05T04:58:27.094142Z","iopub.status.idle":"2021-06-05T04:58:36.942274Z","shell.execute_reply.started":"2021-06-05T04:58:27.094099Z","shell.execute_reply":"2021-06-05T04:58:36.941471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Second way","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ndf['temp_list']=df['text'].apply(lambda x: str(x).split())\ntop=Counter([word for li in df['temp_list'] for word in li])\ntemp_1=pd.DataFrame(top.most_common(20))\ntemp_1.columns=[\"most_common_words\",\"frequency\"]\ntemp_1.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:36.943296Z","iopub.execute_input":"2021-06-05T04:58:36.943716Z","iopub.status.idle":"2021-06-05T04:58:44.041121Z","shell.execute_reply.started":"2021-06-05T04:58:36.943684Z","shell.execute_reply":"2021-06-05T04:58:44.040432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as pe\nimport plotly.figure_factory as ff\nfig = pe.bar(temp_1, x=\"frequency\", y=\"most_common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='most_common_words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:44.042051Z","iopub.execute_input":"2021-06-05T04:58:44.042495Z","iopub.status.idle":"2021-06-05T04:58:48.433672Z","shell.execute_reply.started":"2021-06-05T04:58:44.042458Z","shell.execute_reply":"2021-06-05T04:58:48.432509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Analyzing the top punctuation marks in  real & fake news</strong></p>","metadata":{}},{"cell_type":"code","source":"import string\npunctuation_list=list(string.punctuation)\nvalue_list=[]\ndef most_occuring(dataset,fun,target):\n    d=defaultdict(int)\n    for j in range(0,len(target)):\n        words=fun(target[j])\n        for i in words:\n            if i in punctuation_list:\n                d[i]+=1\n        top=sorted(d.items(),key=lambda x: x[1],reverse=True)[:10]\n        x_items,y_counts=zip(*top)\n        value_list.append(x_items)\n        value_list.append(y_counts)\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(value_list[0],value_list[1],color=\"lightcoral\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Real\")\n    \n    ax2.bar(value_list[2],value_list[3],color=\"purple\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Fake\")\n            \n    plt.suptitle(\"Punctuations in text\")\n    plt.show()\n\nmost_occuring(df,create_words,[1,0])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:48.435593Z","iopub.execute_input":"2021-06-05T04:58:48.436022Z","iopub.status.idle":"2021-06-05T04:58:59.250942Z","shell.execute_reply.started":"2021-06-05T04:58:48.435979Z","shell.execute_reply":"2021-06-05T04:58:59.249872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Text preprocessing</strong></p>","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):\n    \"\"\"Process text function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    stopwords_english = stopwords.words('english')\n    text= re.sub('\\[[^]]*\\]', '', text)\n    # remove stock market tickers like $GE\n    text = re.sub(r'\\$\\w*', '', text)\n    #removal of html tags\n    review =re.sub(r'<.*?>',' ',text) \n    # remove old style retweet text \"RT\"\n    text = re.sub(r'^RT[\\s]+', '', text)\n    # remove hyperlinks\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = text.lower()\n    text_tokens =word_tokenize(text)\n\n    text_clean = []\n    for word in  text_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            lem_word =lemmatizer.lemmatize(word)  # lemmitiging word\n            text_clean.append(lem_word)\n    text_mod=[i for i in text_clean if len(i)>2]\n    text_clean=' '.join(text_mod)\n    return  text_clean","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:59.252447Z","iopub.execute_input":"2021-06-05T04:58:59.253022Z","iopub.status.idle":"2021-06-05T04:58:59.264551Z","shell.execute_reply.started":"2021-06-05T04:58:59.252978Z","shell.execute_reply":"2021-06-05T04:58:59.263767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text']=df['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T04:58:59.266087Z","iopub.execute_input":"2021-06-05T04:58:59.266696Z","iopub.status.idle":"2021-06-05T05:03:31.252677Z","shell.execute_reply.started":"2021-06-05T04:58:59.266651Z","shell.execute_reply":"2021-06-05T05:03:31.251644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'][:2]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:03:31.253851Z","iopub.execute_input":"2021-06-05T05:03:31.254152Z","iopub.status.idle":"2021-06-05T05:03:31.261402Z","shell.execute_reply.started":"2021-06-05T05:03:31.254124Z","shell.execute_reply":"2021-06-05T05:03:31.260514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Look at the top words of the dataset</strong></p>","metadata":{}},{"cell_type":"code","source":"# wordcloud\nfrom PIL import Image\nbook_mask = np.array(Image.open('../input/masksforwordclouds/book-logo-1.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=book_mask,\n)\nwc.generate(' '.join(text for text in df.loc[:, 'clean_text']))\nplt.figure(figsize=(18,10))\nplt.title('Top words', \n          fontdict={'size': 28,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:03:31.262533Z","iopub.execute_input":"2021-06-05T05:03:31.262851Z","iopub.status.idle":"2021-06-05T05:04:49.690912Z","shell.execute_reply.started":"2021-06-05T05:03:31.262823Z","shell.execute_reply":"2021-06-05T05:04:49.689993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_temp']=df['clean_text'].apply(lambda x: str(x).split())\ntop=Counter([word for li in df['clean_temp'] for word in li])\ntemp_2=pd.DataFrame(top.most_common(20))\ntemp_2.columns=[\"common_words\",'frequency']\ntemp_2.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:04:49.692257Z","iopub.execute_input":"2021-06-05T05:04:49.692786Z","iopub.status.idle":"2021-06-05T05:04:53.435957Z","shell.execute_reply.started":"2021-06-05T05:04:49.692742Z","shell.execute_reply":"2021-06-05T05:04:53.435234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top.most_common(20)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:04:53.436913Z","iopub.execute_input":"2021-06-05T05:04:53.437299Z","iopub.status.idle":"2021-06-05T05:04:53.469593Z","shell.execute_reply.started":"2021-06-05T05:04:53.437258Z","shell.execute_reply":"2021-06-05T05:04:53.468751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = pe.treemap(temp_2, path=['common_words'], values='frequency',title='Tree of Most Common Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:04:53.47112Z","iopub.execute_input":"2021-06-05T05:04:53.471565Z","iopub.status.idle":"2021-06-05T05:04:53.564122Z","shell.execute_reply.started":"2021-06-05T05:04:53.471523Z","shell.execute_reply":"2021-06-05T05:04:53.563176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Top 50 words in real & fake news</strong></p>","metadata":{}},{"cell_type":"code","source":"# analyze top 50 words in Real and false texts\ndata_1=df[df['target']==1]\npd.Series(' '.join([i for i in data_1.clean_text]).split()).value_counts()[:50].plot(kind='bar',figsize=(20,8),color='yellow'\n                                                                                       ,edgecolor='black',title='Real')\nplt.show()\n\ndata_0=df[df['target']==0]\npd.Series(' '.join([i for i in data_0.clean_text]).split()).value_counts()[:50].plot(kind='bar',figsize=(20,8),color='green'\n                                                                                       ,edgecolor='black',title='Fake')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:04:53.565322Z","iopub.execute_input":"2021-06-05T05:04:53.565593Z","iopub.status.idle":"2021-06-05T05:05:00.054342Z","shell.execute_reply.started":"2021-06-05T05:04:53.565566Z","shell.execute_reply":"2021-06-05T05:05:00.053193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 35px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightblue; border-radius: 5px 5px;\"><strong>N-Grams</strong></p>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>Generating N-Grams</strong></p>","metadata":{"execution":{"iopub.status.busy":"2021-05-29T17:48:41.781014Z","iopub.execute_input":"2021-05-29T17:48:41.781537Z","iopub.status.idle":"2021-05-29T17:48:41.787677Z","shell.execute_reply.started":"2021-05-29T17:48:41.781505Z","shell.execute_reply":"2021-05-29T17:48:41.785842Z"}}},{"cell_type":"code","source":"data=' '.join([sentance for sentance in df['clean_text']])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:05:00.055649Z","iopub.execute_input":"2021-06-05T05:05:00.056039Z","iopub.status.idle":"2021-06-05T05:05:00.128254Z","shell.execute_reply.started":"2021-06-05T05:05:00.056002Z","shell.execute_reply":"2021-06-05T05:05:00.127269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n \n# Function to generate n-grams from sentences.\ndef extract_ngrams(data, num):\n    n_grams = ngrams(nltk.word_tokenize(data), num)\n    return [ ' '.join(grams) for grams in n_grams]\n \nunigrams=extract_ngrams(data, 1)\nbigrams= extract_ngrams(data, 2)\ntrigrams= extract_ngrams(data, 3)\nfourgrams=extract_ngrams(data, 4)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:05:00.129561Z","iopub.execute_input":"2021-06-05T05:05:00.129872Z","iopub.status.idle":"2021-06-05T05:11:54.395197Z","shell.execute_reply.started":"2021-06-05T05:05:00.12984Z","shell.execute_reply":"2021-06-05T05:11:54.394341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightgreen; border-radius: 5px 5px;\"><strong>counting the frequency of n- grams</strong></p>","metadata":{}},{"cell_type":"code","source":"freq_uni = nltk.FreqDist(unigrams)\nfreq_bi = nltk.FreqDist(bigrams)\nfreq_tri = nltk.FreqDist(trigrams)\nfreq_four = nltk.FreqDist(fourgrams)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:11:54.396651Z","iopub.execute_input":"2021-06-05T05:11:54.397255Z","iopub.status.idle":"2021-06-05T05:12:38.523448Z","shell.execute_reply.started":"2021-06-05T05:11:54.397209Z","shell.execute_reply":"2021-06-05T05:12:38.522569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Unigrams</strong></p>","metadata":{}},{"cell_type":"code","source":"# top 20 uigrams\ntop_20_uni=freq_uni.most_common(20)\ntop_20_uni_words,top_20_uni_freq=list(zip(*top_20_uni))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_uni_words, top_20_uni_freq, color ='maroon',\n        width = 0.4)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:12:38.524749Z","iopub.execute_input":"2021-06-05T05:12:38.525349Z","iopub.status.idle":"2021-06-05T05:12:40.33135Z","shell.execute_reply.started":"2021-06-05T05:12:38.525304Z","shell.execute_reply":"2021-06-05T05:12:40.330226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Bigrams</strong></p>","metadata":{}},{"cell_type":"code","source":"top_20_bi=freq_bi.most_common(20)\ntop_20_bi_words,top_20_bi_freq=list(zip(*top_20_bi))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_bi_words, top_20_bi_freq, color ='lightcoral',\n        width = 0.4)\nplt.xticks(rotation=90) ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:12:40.332627Z","iopub.execute_input":"2021-06-05T05:12:40.333015Z","iopub.status.idle":"2021-06-05T05:12:41.210034Z","shell.execute_reply.started":"2021-06-05T05:12:40.332983Z","shell.execute_reply":"2021-06-05T05:12:41.209244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Trigrams</strong></p>","metadata":{}},{"cell_type":"code","source":"top_20_tri=freq_tri.most_common(20)\ntop_20_tri_words,top_20_tri_freq=list(zip(*top_20_tri))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_tri_words, top_20_tri_freq, color ='lightgreen',\n        width = 0.4)\nplt.xticks(rotation=90) ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:12:41.211151Z","iopub.execute_input":"2021-06-05T05:12:41.211763Z","iopub.status.idle":"2021-06-05T05:12:42.575376Z","shell.execute_reply.started":"2021-06-05T05:12:41.211712Z","shell.execute_reply":"2021-06-05T05:12:42.574296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size : 25px; color : #532e1c ; font-family : 'Comic Sans MS'; text-align : center; background-color : lightcoral; border-radius: 5px 5px;\"><strong>Top 20 Fourgrams</strong></p>","metadata":{}},{"cell_type":"code","source":"top_20_four=freq_four.most_common(20)\ntop_20_four_words,top_20_four_freq=list(zip(*top_20_four))\nplt.figure(figsize=(20,7))\nplt.bar(top_20_four_words, top_20_four_freq, color ='blue',\n        width = 0.4)\nplt.xticks(rotation=90) ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:12:42.576573Z","iopub.execute_input":"2021-06-05T05:12:42.577021Z","iopub.status.idle":"2021-06-05T05:12:44.074224Z","shell.execute_reply.started":"2021-06-05T05:12:42.576982Z","shell.execute_reply":"2021-06-05T05:12:44.073499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words","metadata":{}},{"cell_type":"code","source":"# creating the bag of words\ndata=' '.join([sent for sent in df['clean_text']])\nwords_count={}\nwords=nltk.word_tokenize(data)\nfor word in words:\n    if word in words_count.keys():\n        words_count[word]+=1\n    else:\n        words_count[word]=1\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:12:44.075347Z","iopub.execute_input":"2021-06-05T05:12:44.075647Z","iopub.status.idle":"2021-06-05T05:14:25.390058Z","shell.execute_reply.started":"2021-06-05T05:12:44.075618Z","shell.execute_reply":"2021-06-05T05:14:25.389007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import heapq\nfreq_words=heapq.nlargest(100,words_count,key=words_count.get)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:14:25.391349Z","iopub.execute_input":"2021-06-05T05:14:25.391645Z","iopub.status.idle":"2021-06-05T05:14:25.430876Z","shell.execute_reply.started":"2021-06-05T05:14:25.391617Z","shell.execute_reply":"2021-06-05T05:14:25.429859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_words","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:14:25.432203Z","iopub.execute_input":"2021-06-05T05:14:25.432687Z","iopub.status.idle":"2021-06-05T05:14:25.44637Z","shell.execute_reply.started":"2021-06-05T05:14:25.432654Z","shell.execute_reply":"2021-06-05T05:14:25.445306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x=[]\n# for sentence in df['clean_text']:\n#     vector=[]\n#     for word in freq_words:\n#         if word in nltk.word_tokenize(sentence):\n#             vector.append(1)\n#         else:\n#             vector.append(0)\n#     x.append(vector)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:14:25.447641Z","iopub.execute_input":"2021-06-05T05:14:25.448019Z","iopub.status.idle":"2021-06-05T05:14:25.460665Z","shell.execute_reply.started":"2021-06-05T05:14:25.447988Z","shell.execute_reply":"2021-06-05T05:14:25.459399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tf-Idf","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:14:25.46238Z","iopub.execute_input":"2021-06-05T05:14:25.462861Z","iopub.status.idle":"2021-06-05T05:14:25.473355Z","shell.execute_reply.started":"2021-06-05T05:14:25.462817Z","shell.execute_reply":"2021-06-05T05:14:25.47238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the term frequency\ndef term_freq(words_count):\n    term={}\n    for sen in df['clean_text']:\n        for word in nltk.word_tokenize(sen):\n            term[word]=words_count[word]/len(nltk.word_tokenize(sen))\n    return term\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:14:25.474874Z","iopub.execute_input":"2021-06-05T05:14:25.475554Z","iopub.status.idle":"2021-06-05T05:14:25.487374Z","shell.execute_reply.started":"2021-06-05T05:14:25.475507Z","shell.execute_reply":"2021-06-05T05:14:25.486405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# term_freq(words_count)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:40:51.185198Z","iopub.execute_input":"2021-06-05T05:40:51.185554Z","iopub.status.idle":"2021-06-05T05:40:51.189714Z","shell.execute_reply.started":"2021-06-05T05:40:51.185523Z","shell.execute_reply":"2021-06-05T05:40:51.188705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# document per words\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T05:40:43.597554Z","iopub.status.idle":"2021-06-05T05:40:43.597983Z"},"trusted":true},"execution_count":null,"outputs":[]}]}