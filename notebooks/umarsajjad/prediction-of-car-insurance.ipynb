{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting of Car Insurance\nStep by Step Guide:\n1. Importing the Necessary Libraries\n2. Importing the Dataset \n3. Dataset Analysis\n   * 3.1 Observing the data  \n   * 3.2 Determining missing values\n   * 3.3 Joining Train/Test Data\n4. Visualizing and Comparing Features\n   * 4.1 Correlation heatmap \n   * 4.2 Comparing the effect of different categorical features and features with 2 categories on the target(Response)\n       * 4.2.1 Gender\n       * 4.2.2 Vehicle Age\n       * 4.2.3 Vehicle Damage \n       * 4.2.4 Driving_License\n       * 4.2.5 Previoulsy_Insured\n   * 4.3 Comparing the effect of different numerical features on the target(Response)   \n5. Feature Engineering\n   * 5.1 Converting categorical columns to numerical values\n       * 5.1.1 Mapping categorical Vehicle_Age feature\n       * 5.1.2 Mapping categorical Gender feature\n       * 5.1.3 Mapping categorical Vehicle_Damage feature \n   * 5.2 Dropping non-essential columns\n       * 5.2.1 Dropping categorical columns\n       * 5.2.2 Dropping Id column and Vintage column\n6. Building/Training/Evaluating our models\n   * 6.1 Seperating Train/Test dataset\n   * 6.2 Modelling various classifiers\n   * 6.3 Hyperparameter tuning\n   * 6.4 Submitting"},{"metadata":{},"cell_type":"markdown","source":"# 1-Importing the Necessary Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing the data analysis libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n#Importing the visualization libraries\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#Ensuring that we don't see any warnings while running the cells\nimport warnings\nwarnings.filterwarnings('ignore') \n\n#Importing the counter\nfrom collections import Counter\n\n#Importing sci-kit learn libraries that we will need for this project\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2-Importing the Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/train.csv\")\ntest = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3-Data Analysis"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 3.1-Observing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Obervations/Discussion:\n* As we can see from the sampling that the dataset has a mixture of both Quantitative variables and Categorical Variables\n* This mix up of variables will cause problems while training our model\n* We need to convert the categorical varibales into Quantitative variables so that our ML model doesn't encounter trouble is training and predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(train).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Obervations/Discussion:\n* Diving deeper into the details of the dataset, we can observe that the dataset has no missing values\n* We will still need to ensure whether the values are correct though\n* The NaNs represent Cateogorical features which we will convert to Quantitative variables"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 - Joining Train/Test"},{"metadata":{},"cell_type":"markdown","source":"First we will combine the train and test data to ensure that we implement the feature engineering on all data, and we don't have discrepancies when modeling and evaluating. We will split the dataframe again after the feature engineering process"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat(objs = [train, test], axis = 0).reset_index(drop=True)\ndf.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(df).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion\n* The missing values represent the missing responses from the dataset\n* Apart from the response missing values, there are no missing values in the dataset"},{"metadata":{},"cell_type":"markdown","source":"### Seperating categorical and numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data = df.select_dtypes(include='number')\ncategorical_data = df.select_dtypes(exclude='number')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 - Visualizing and Comparing the Features"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 - Correlation heatmap\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sn = sns.heatmap(df[[\"Response\",\n                \"Age\",\n                \"Driving_License\", \n                \"Region_Code\", \n                \"Previously_Insured\", \n                \"Vehicle_Age\", \n                \"Vehicle_Damage\", \n                \"Annual_Premium\",\n                \"Policy_Sales_Channel\",\n                \"Vintage\"]].corr(), cmap = 'coolwarm', annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 - Comparing the effect of different categorical features and features with 2 categories on the target(Response)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A function to visualize and determine the fraction of responses in each category for a certain feature\ndef bar_plot(feature):\n    \n    feature_categories = df[feature].sort_values().unique()\n    for category in feature_categories:\n        temp_series = df[\"Response\"][df[feature] == category].value_counts(normalize = True)\n        #This code is used to solve problem when there are no Responses for a category, which causes an error in runtime\n        if temp_series.shape == (1,):\n            temp_series = temp_series.append(pd.Series([0], index=[1]))\n        elif temp_series.shape == (0,):\n            continue\n        print(\"Percentage of individuals having {}: {}, who got the insurance: {:.2f} %\".format(feature, category, temp_series[1]*100))\n    #visualize\n    sns.barplot(x = df[feature],y = df[\"Response\"],  data = df).set_title('Fraction Who Got Insurance With Respect To {}'.format(feature))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2.1 - Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot(\"Gender\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* From this graph it can be deduced that of those that did get the insurance, the fraction of males was slighty higher than the fraction of females\n* Reasons for this difference cannot be determined just now and require further analysis\n* From this observation it can be duduced that males have a 4% greater chance of getting insurance as compared to females\n* Thus Gender also plays a part in determining whether an individual will get the insurance or not"},{"metadata":{},"cell_type":"markdown","source":"### 4.2.2 - Vehicle Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot(\"Vehicle_Age\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* The rate of people getting insurance gets higher as the age of the vehicle increases\n* Almost 30% of people who have vehicles that are older than 2 years, got the insurance\n* This shows that Vehicle_Age plays a huge part in whether people will get insurance or not"},{"metadata":{},"cell_type":"markdown","source":"### 4.2.3 - Vehicle Damage"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot(\"Vehicle_Damage\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* The rate of people getting insurance is significantly higher for people with vehicle damage\n* Almost 25% of people who have vehicles that are damaged, got the insurance\n* Just 0.5% of people who have vehicles that are not damaged, got the insurance\n* This shows that Vehicle Damage plays a huge part in whether people will get insurance or not"},{"metadata":{},"cell_type":"markdown","source":"### 4.2.4 - Driving License"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot(\"Driving_License\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* The rate of people getting insurance is more than double for people who have a Driving License as compared to those who don't\n* Almost 12% of people who have a Driving License, got the insurance\n* This shows that Driving License plays a significant part in whether people will get insurance or not"},{"metadata":{},"cell_type":"markdown","source":"### 4.2.5 - Previously Insured"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_plot(\"Previously_Insured\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* The rate of people getting insurance is significantly higher for people without prior insurance\n* Almost 23% of people who didn't have previous insurance, got the insurance\n* As compared to only 0.09% of people who got the insurance, who already had a previous insurance\n* This shows that Previous Insurance status plays a huge part in whether people will get insurance or not\n* This makes sense that people who already have an insurance will not be looking for further insurance or a new insurance program\n* People who are uninsured will be looking for insurance and thus explaining the difference in rate"},{"metadata":{},"cell_type":"markdown","source":"## 4.3 - Comparing the effect of different numerical features on the target(Response)"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.1 - Correlation heatmap for numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sn = sns.heatmap(df[[\"Response\",\n                    \"Age\", \n                    \"Region_Code\",\n                    \"Vehicle_Age\",  \n                    \"Policy_Sales_Channel\",\n                    \"Vintage\"]].corr(), cmap = 'coolwarm', annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function that takes in a feature and returns the histogram\ndef histograms(feature):\n    fig = px.histogram(\n        train, \n        feature, \n        color='Response',\n        nbins=100, \n        title=('{} Vs Response'.format(feature)), \n        width=700,\n        height=500\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3.2 Age Vs Response Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"histograms(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* The 1st graph shows that majority of those who do not go for insurance are in their 20s \n* The 2nd graph shows that the majority of people who do go for an insurance are between the ages of 38 and 50.\n* Thus age plays a significant role in determining whether an individual will get vehicle insurance or not"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.3 Vintage Vs Response Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"histograms(\"Vintage\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations and Discussion:\n* From this graph it is pretty evident that Vintage features is evenly distributed\n* This verifies the observation from the heatmap which states that Vintage plays an insignificant role in determining the response of the individual to getting an insurance or not\n* This is one of the features that can be removed from modelling, again because of the fact that it plays a very insignificant role"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.4 Region Code Vs Response Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"histograms(\"Region_Code\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n* Region code = 28 has a very large number of counts as compared to other regions\n* There are small spikes everywhere else but nothing as substantial as Region Code = 28"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.5 Policy_Sales_Channel Vs Response Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"histograms(\"Policy_Sales_Channel\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n* There are certain spikes at certain Policy Channels\n* Most policy Channels have very few customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"histograms(\"Annual_Premium\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n* The vast majority of the customers have an annual premium of less than 100k\n* As the annual premium increases, the chance of response = Yes(0), decreases\n* Higher premiums usually deter customers from the insurance offer which can explain the point stated above"},{"metadata":{},"cell_type":"markdown","source":"# 5 - Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 - Converting categorical columns to numerical values"},{"metadata":{},"cell_type":"markdown","source":"### 5.1.1 - Vehicle_Age Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Vehicle_Age_Encoded\"] = df[\"Vehicle_Age\"].map({\"< 1 Year\": 0, \"1-2 Year\": 1, \"> 2 Years\": 2})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1.2 - Gender Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Gender_Encoded\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1.3 - Vehicle_Damage Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Vehicle_Damage_Encoded\"] = df[\"Vehicle_Damage\"].map({\"No\": 0, \"Yes\": 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 - Dropping non-essential columns"},{"metadata":{},"cell_type":"markdown","source":"### 5.2.1 - Dropping categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"Vehicle_Age\", \"Vehicle_Damage\", \"Gender\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comments:\n* I have already converted these categorical columns into numerical representations, thus these columns can now be dropped"},{"metadata":{},"cell_type":"markdown","source":"### 5.2.2 - Dropping Id column and Vintage column"},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_ID = pd.Series(df[\"id\"], name = \"CustomerId\")\ndf = df.drop([\"id\", \"Vintage\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comments:\n* The Id of the particilar customer plays no part in determining the outcome of the response thus must be dropped\n* Vintage feature has a neglible correlation with the response and other features as evident from the confusion matrix, thus it also can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6 - Building/Training/Evaluating our models"},{"metadata":{},"cell_type":"markdown","source":"### 6.1 - Seperating Train/Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:].drop([\"Response\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 6.2 - Modelling various classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#StratifiedKFold aims to ensure each class is (approximately) equally represented across each test fold\nk_fold = StratifiedKFold(n_splits=5)\n\nX_train = train.drop(labels=\"Response\", axis=1)\ny_train = train[\"Response\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n\n# Creating objects of each classifier\nLG_classifier = LogisticRegression(random_state=0)\nSVC_classifier = SVC(kernel=\"rbf\", random_state=0)\nKNN_classifier = KNeighborsClassifier()\nNB_classifier = GaussianNB()\nDT_classifier = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\nRF_classifier = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", random_state=0)\n\n#putting the classifiers in a list so I can iterate over there results easily\ninsurance_classifiers = [LG_classifier]\n\n#This dictionary is just to grad the name of each classifier\nclassifier_dict = {\n    0: \"Logistic Regression\",\n    1: \"Support Vector Classfication\",\n    2: \"K Nearest Neighbor Classification\",\n    3: \"Naive bayes Classifier\",\n    4: \"Decision Trees Classifier\",\n    5: \"Random Forest Classifier\",\n}\n\ninsurance_results = pd.DataFrame({'Model': [],'Mean Accuracy': [], \"Standard Deviation\": []})\n\n#Iterating over each classifier and getting the result\nfor i, classifier in enumerate(insurance_classifiers):\n    classifier_scores = cross_val_score(classifier, X_train, y_train, cv=k_fold, n_jobs=2, scoring=\"accuracy\")\n    insurance_results = insurance_results.append(pd.DataFrame({\"Model\":[classifier_dict[i]], \n                                                           \"Mean Accuracy\": [classifier_scores.mean()],\n                                                           \"Standard Deviation\": [classifier_scores.std()]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (insurance_results.to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3 - Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# RF_classifier = RandomForestClassifier()\n\n\n# ## Search grid for optimal parameters\n# RF_paramgrid = {\"max_depth\": [None],\n#                   \"max_features\": [1, 3, 10],\n#                   \"min_samples_split\": [2, 3, 10],\n#                   \"min_samples_leaf\": [1, 3, 10],\n#                   \"bootstrap\": [False],\n#                   \"n_estimators\" :[100,200,300],\n#                   \"criterion\": [\"entropy\"]}\n\n\n# RF_classifiergrid = GridSearchCV(RF_classifier, param_grid = RF_paramgrid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose=1)\n\n# RF_classifiergrid.fit(X_train,y_train)\n\n# RFC_optimum = RF_classifiergrid.best_estimator_\n\n# # Best Accuracy Score\n# RF_classifiergrid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IDtest = customer_ID[train.shape[0]:].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4 - Submitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = train.drop(labels=\"Response\", axis=1)\ny_train = train[\"Response\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(test)\n\nLG_classifier.fit(X_train, y_train)\n\ntest_predictions = pd.Series(LG_classifier.predict(X_test).astype(int), name=\"Response\")\ninsurance_results = pd.concat([IDtest, test_predictions], axis = 1)\ninsurance_results.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}