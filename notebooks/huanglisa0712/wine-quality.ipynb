{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [Introduction](#introduction)\n* [Data at a glance](#glance-data)\n* [Input Data](#read-data)\n* [Data Cleaning](#data-cleaning)\n* [Data Preprocessing](#data-preprocessing)\n* [XGBoost](#XGBoost)\n    - [XGBClassifier](#XGBClassifier)\n    - [XGBRegressor](#XGBRegressor)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n# Introduction\n\n![picture resource: https://mountaintoplodge.com/blog/wine-tastings-poconos/](https://mountaintoplodge.com/wp-content/uploads/2018/09/wine-tasting-1500x609.jpg)\n(taken from https://mountaintoplodge.com/)\n\n**goal**: The objectives of this notebook is to model wine quality with XGBoost and Random forest, and examine the results with correspoinding test, such as r2, auc curve... etc. \n\n**about dataset**\nThe dataset is related to red and white variants of the Portuguese wine. We consider good wine as the quality is greater than 6.5. \n* **fixed acidity** means most acids involved with wine\n* **volatile acidity** shows the amount of acetic acid in wine, which at <mark>too high of levels can lead to an unpleasant, vinegar taste</mark>\n* **citric acid** found in <mark>small quantities, citric acid can add 'freshness'</mark> and flavor to wines\n* **residual sugar** is the amount of sugar remaining after fermentation stops, it's <mark>rare to find wines with less than 1 gram/liter</mark> \n* **chlorides** means the amount of salt in the wine\n* **free sulfur dioxide** shows the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion\n* **total sulfur dioxide** is amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine\n* **density** is the density of water is close to that of water depending on the percent alcohol\n* **pH** describes how acidic or basic a wine is on a <mark>scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 </mark>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n# for correlation matrix\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import cm \n\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n#k-fold validation\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.metrics import confusion_matrix,classification_report, auc,roc_curve, r2_score\nfrom collections import OrderedDict \nfrom sklearn import metrics\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\"\"\"\n<a id='data-at-a-glance'></a>\n# Data at a glance\nIn the following table, we give a peek at the data to have a general understanding of the data. \n* All of the features are numeric and the dependent variable( wine quality ) is \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\"\"\"\n<a id='XGBoost-review' ></a>\n# A Brief Review on XGBoost\nXGBoost stands for eXtreme Gradient Boosting and was first proposed by [Tianqi Chen](https://arxiv.org/abs/1603.02754). \nXGBoost is a model, dealing with supervised learning problems, where we predict the target variable with features. \n\n### Boosting Algorithm \nEnsemble learning (or boosting) technique combines weak (base) learners into a strong learners. Though the base learners tends to have limited power, we can use ensemble learning to form more accurate model. *We can consider XGBoost is a special case of Newton Boosting. We can considered it as the interpretation of Newton method is functional space. It is numerical optimization algorithms in function space. *\n\nGiven a finite training set $\\{ x_i, y_i \\},i=1...n$, we hope to solve the objective function:\n$$\\hat{\\Phi}(F)=\\sum^{n}_{i=1}L(y_i,F(x_i))$$\n\n### Numerical Optimization in Function Space\n\nAssume the true risk R(f) is known to us, and we iterative risk minimization procedures in function space. We are minimizing\n$$R(f(x))=E[L(Y,f(x))|X=x] , \\forall x \\in \\mathcal{X}$$ \nat each iteration \n\n#### Construct $F(x)$\nPractically, We view boosting algorithms as numerical optimization in function space. Boosting algorithms implements a sequantial process, generates weak learners and combines them into strong learners. We can consider the weak learners as a set of basis functions. Boosting algorithm sequentially add base models to improves the fitting. \n\n$$f^{(m)}(x)=f^{(m-1)}(x)+f_m(x)$$\n$f^{(m-1)}$ is current estimate. We take the \"step\" $f_m$ in function space, and get $f^{(m)}$. Therefore, we can view f(x) as the combination of initial guess and all successive \"steps\" taken previously in function space. \n\n$$F(x)\\equiv f^{(M)}(x)\\equiv \\sum_{m=0}^{M}f_m(x)$$\n\n, where $f(x)$ is the prediction, M is the number of weak learners and $f_0(x)$\n\nforward stagewise additive modeling (FSAM)$\\hat{f}(x)=\\hat{f}^{(M)}(x)=\\sum_{m=1}^{M}\\hat{\\theta}_m \\hat{c}_m(x_i)$\n\n\n\n### System Features:\n* Parallelization : The parallelisatio happens during the tree construction and enables distributed training and predicting across nodes. \n* Cache Optimization : XGBoost keeps all of the immediate calculation \n* Distributed Computing\n* Out-of-Core Computing\n\n### Model Features\n* Gradient Boosting\n* Stochastic Gradient Boosting\n* Regularizaed Gradient Boosting\n\n### Algorithm Features\n* Sparse Aware\n* Block Structure\n* Continued Training\n\n### CART algorithm \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"read-data\"></a>\n### Read the data "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(dirname,filename))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we read the data, we want to make sure our system has successfully get the data. After we run the read_csv function from panda library, nothing will appear. \n\nSo we have to find a way to see the data. <mark>Instead of seeing all the data, we can only see the first few row by using head function. </mark>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-cleaning\"></a>\n## Data Cleaning\nIn the data cleaning we are going to conduct the following process to make sure our data is concise and neat. \n1. Dropping unnecessary columns in a DataFrame : If we don't eliminate the unnecessary column, we will add redundant parameter model. This will make the model become unnecessarily complex. \n2. Detecting missing values: \n3. Fixing expected types\n* 4. Detecting outlier"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"outliner\"></a>\n### Outlier\nThere is no missing value in this data. This is a good news, because it means that our dataset is clean. However, we can not relax our guard now, because <mark>there is still a possibility that the recorder input the wrong value. In other words, we are going to detect for outlier</mark>  \n\n**We can observe it by seeing if the maximum or minimum value for every variables are in the normal range. Another possible way is to see the scatter plot.** If a point behaves significantly different from others, than it has possibility to be an outlier. \n\nIn regression model, outlier can sometimes alter the result. In tree models, however, decision trees are robust to outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table above, it is not hard to find that all of the variable are in the normal range. For example, it's rate to observe wines to have residual sugar less than 1gram/liter, and our minimum residual sugar data is 0.9 gram/liter, which is also close to 1 gram/liter. \n\nSo we can say that, in our first glance, our data set may not have outlier! \n\nWoo-hoo!"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"histogram\"></a>\n## Density Plot and Histogram\nWe hope to discover the underlying frequency distribution of every variables through histogram and denstiy plot. Besides underlying distribution, histogram can also double checks our conclusion on outliers. Noting that since all of our variables are continuous, we construct histogram to plot the frequency of score occurrences in a continuous data set that has divided into classes. Suppose our data is discrete, then we have to choose bar charts. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure()\nkey = df.keys()\nf, axes = plt.subplots(3,4,figsize = (20,25)) # rows, columns\nfor j in range(4): \n    for i in range(3):\n        sns.distplot(df[key[4*i+j]],ax = axes[i,j]) \n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe density plot draws a smooth line around the histogram and guess the underlying distribution of variables. From the histograms and density plots above, we can know that most of the features follows right-skewed distribution.\n\nFortunetely, **tree models are insensitive to skew data**. <mark>Since tree models are non-parametric method, which doesn't make any assumption on the population distribution and sample size, tree models are more robust to outliers, nonlinear relationship.</mark> \n\nDecision trees tries to find the variable that maximize difference in two brances, and the data distribution doesn't play an important role. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n## Data Preprocessing\nFrom the dataset description, we know that good wine is defined to be quality greater than 6.5. Since \">=\" ( greater than or equal to ) is an comparison operators, we will have boolean result. <mark>We hope our response variable to be integer, so we have to change change the data type after the comparison. </mark>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'] = df['quality'].values.astype(np.int)\ndf['good_bad_wine'] = df['quality']>=6.5\ndf['good_bad_wine'] = df['good_bad_wine'].values.astype(np.int)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, after the preprocessing, we want to know if we are in the right direction and have a look at the dataset. We use the head function to check the dataset. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"scatter-plot\"></a>\n## Scatter Plot : Relationship between variables\nWe take a look at the raw data by using scatter plot. In scatter plot, we can observe the relationship between two variables. In regression analysis, we care about multicollinearity. If the degree of correlation between independent variables are high, it can cause problems when interpreting the results. \n\nA regression coefficient can be interpreted as ,for each 1 unit change in an independent variable, the mean change in the dependent variable when you hold other independent variables constant. The higher the correlated degree, the harder to change one variable without change another, and variables change in unison. \n\nFortunately, **decision trees and boosted trees algorithms are immune to multicollenearity**. When the tree split, they choose one prefect correlated features. <mark>The splitting only take place while the tree choosing from one of the prefectly correclated features. </mark>"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"import plotly.express as px\ndef hide_current_axis(*args, **kwds):\n    plt.gca().set_visible(False)\n    \n\n    \nplt.figure(figsize = (60,60))\ncmap = cm.get_cmap('gnuplot')\nplt.figure(figsize = (10,10))\ngrid = sns.PairGrid(df,hue = 'good_bad_wine')\ngrid = grid.map_lower(sns.scatterplot, alpha=0.3, edgecolor='none')\n\ngrid.map_diag(plt.hist)\ngrid.map_upper(hide_current_axis)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We give weach point a distinct hue to show membership of each point and distinguish its wine quality. \n\nThe graph shows that good wine have some obvious pattern in some dependent variable, even if cheap plonk doesn't have any pattern on that variable. For example, good wine tends to have low volatine acidity, slightly higher sulphates than average wine ,low total sulfur dioxide, high alcohol. \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"correlation-matrix\"></a>\n### correlation matrix\nWe use correlation matrix to show the correlation coefficients between variables. We are using a matrix of Pearson-type correlations. In most cases, these correlations are influenced by outliers, unequal variances, nonnormality, and nonlinearities. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr(),annot = True,cmap='PuBuGn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation graph, we observe the following :\n1. The quality of wine is somewhat related to volatile acidity. The higher the wine quality, the lower the volatile acidity. \n2. The citric acid, aulphates and alcohol degree are also related to wine quality. The alcohol degree relates to the wine quality the most, and sulphates and citric acid degree are less related. The higher the wine quality, the higher the citric acid, sulphates, and alcohol degree. \n3. fixed acidity, residual sugar, free sulfur dioxide and pH doesn't have apparent pattern accross different quality. "},{"metadata":{},"cell_type":"markdown","source":"we only consider wine as good or bad. To balance data and fairness, for quality larger than five, we classify as 'good'. Otherwise, the wine is consider as bad. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgboost\"></a>\n## XGBoost\nAs we know, xgboost provides classifier and regressor. Classifier deals with discrete target, and Regressor deals with continuous outcome. In this project, we will first train the data with <mark>XGBClassifier and use ROC curve and AUC score to examinate the outcome</mark>. We then use <mark>XGBRegressor to solve the problem again, and examinate the model with r2 score. </mark>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"XGBClassifier\"></a>\n### XGBClassifier\nWe first initialize the XGBoost, and use the GridSearchCV to build pipline to find the best model. \n\n[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\nGridSearchCV object inside the pipeline will be reinitialized after fit(). When refit=True, the GridSearchCV will be refitted with the best scoring parameter combination on the whole data that is passed in fit()\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df[df.columns[0:11]].values\ny = df['good_bad_wine'].values.astype(np.int)\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)\nprint('X train size: ', x_train.shape)\nprint('X test size: ', x_test.shape)\nprint('y train size: ', y_train.shape)\nprint('y test size: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the parameters, Gamma specifies the minimum loss reduction required to make a split. \"reg_alpha\" and \"reg_lambda\" are regularize parameter. In the \"tuned_parameters\", the labels start with the stage of the pipline, following with two underdash. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\nmy_pipeline = Pipeline([('pca', PCA()),  ('xgbrg', XGBClassifier(random_state = 2))])\ntuned_parameters = {'pca__n_components': [5, 10,None],'xgbrg__gamma':[0,0.5,1],'xgbrg__reg_alpha':[0,0.5,1], 'xgbrg__reg_lambda':[0,0.5,1],\"xgbrg__learning_rate\": [0.1, 0.5, 1]}\n\nxgb = GridSearchCV(my_pipeline, cv=5,param_grid = tuned_parameters,  scoring='roc_auc')\nxgb.fit(x_train, y_train)\nprint('The best model is: ', xgb.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GridSearchCV results tells us that it is better not to perform PCA in preprocessing. The best mobel has parameters gamma as 0.5, learning rate 0.5 and reg\nThe following is the ROC curve on the default model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = xgb.predict_proba(x_test)[:,1]\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,prediction)\n\nplt.subplots(figsize=(6,6))\nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.xlabel('False positive rate', fontsize = 14)\nplt.ylabel('True positive rate', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An ROCcurve can show the performance of cassification problems. It plots both true positive rate and false positive rate. Recall that True Positve Rate(TPR) and False Positive Rate(FPR) are defined as following:\n\n$$TPR = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP) + False Negative (FN)}}$$\n$$FPR = \\frac{FP}{FP+TN}$$\n\nMore items are positive when lowering the classification threshold. This will result in both FP and TP increase. \n![](http://)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AUC is: \", auc(false_positive_rate, true_positive_rate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first fit the training data with default model, and get the AUC score 0.902. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"XGBRegressor\"></a>\n### XGBRegressor\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nx = df[df.columns[0:11]].values\ny = df['quality'].values.astype(np.int)\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)\nprint('X train size: ', x_train.shape)\nprint('X test size: ', x_test.shape)\nprint('y train size: ', y_train.shape)\nprint('y test size: ', y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = XGBRegressor(random_state = 2)\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"tuned_parameters = {'gamma':[0,0.5,1], 'reg_lambda': [1,5,10], 'reg_alpha':[0,1,5], 'subsample': [0.6,0.8,1.0]}\n\nreg = GridSearchCV(XGBRegressor(random_state = 2, n_estimators=500), tuned_parameters, cv=5, scoring='r2')\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The best model is: ', reg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\"\"\"\nprediction = reg.predict(x_test)\nplt.plot(y_test, prediction, linestyle='', marker='o')\nplt.xlabel('true values', fontsize = 16)\nplt.ylabel('predicted values', fontsize = 16)\nplt.show()\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = reg.predict(x_test)\nprint('The r2_score on the test set is: ',r2_score(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"random-forest\"></a>\n## Random Forest\n"},{"metadata":{},"cell_type":"markdown","source":"Random forest split the training data into several parts, and training a decision tree for all parts. The final prediction is the voting result of all tree's individual prediction. By voting, averaging all prediction result, random forest improves the problem that decision tree is easily overfitting"},{"metadata":{},"cell_type":"markdown","source":"Here we split the data into training and testing set. Like we have done before, the percentage of the test size is 33% of total data, and we use the same random state as previous model. We set the random state because we hope to have the same result everytime we run the program. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df[df.columns[0:11]].values\ny = df['good_bad_wine'].values.astype(np.int)\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the scikit-learn's library, RandomFOrestClassifier has all parameters that DecisionTreeClassifier has."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"\ntuned_parameters = {'n_estimators':[500], 'max_depth': [2,3,5,7], 'max_features': [0.5,0.7,0.9],'n_jobs':[-1],'min_samples_leaf':[1,5,10]} \n#,'random_state':[14]\nclf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, scoring='roc_auc')\nclf.fit(X_train, y_train)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"\n# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"OOB error rate\")\nplt.legend(loc=\"upper right\")\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest classifier with log2 performs the best. The model performs the best when the number of estimators is around 150 to 250. "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 100,oob_score = True)\n\nmax_features = ['sqrt','log2',None]\nn_estimators = [int(x) for x in np.linspace(start = 100,stop = 250,num = 16 )]\n\nparam_grid = {'max_features':max_features,'n_estimators' : n_estimators }\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search.fit(x_train,y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Reference\nhttps://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\nhttps://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}