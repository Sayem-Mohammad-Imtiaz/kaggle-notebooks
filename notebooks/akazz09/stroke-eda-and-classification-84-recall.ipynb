{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 0. Import libraries and Read data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'\nx_df = pd.read_csv(DATA_PATH)\nx_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will analyze our features and target distribution to get some raw insights on the potential relationships to helps us for the features selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='stroke',data=x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is unbalanced because the number of people having strokes are much lower than people who have not. Obviously that is natural since a study in 2010 showed that 0.25% of the world population had a stroke during that year"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Residence_type',data=x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a fair representation between rural and urban people.\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\nsns.violinplot(ax=axes[0], x=\"stroke\", y=\"avg_glucose_level\", data=x_df)\nsns.violinplot(ax=axes[1], x=\"stroke\", y=\"bmi\", data=x_df)\nsns.violinplot(ax=axes[2], x=\"stroke\", y=\"age\", data=x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The distribution of **average glucose level** between the two classes is almost similar with the only difference being that there are slightly more people with stroke who have an average glucose level above 150.\n\n* There is no significant difference between the distribution of **BMI** between the two classes. So the BMI has not a big impact in having a stroke here. However we may notice potential outliers as a bmi over 65 is quite rare.\n\n* The difference of **age** distribution between the two classes is significant with people having strokes who are much older than the rest of the population. We can deduce that age is an important factor of stroke. The more people are old the more chance to have a stroke.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_df = x_df[~(x_df['gender'] == 'Other')]\nsns.violinplot(x=\"stroke\", y=\"age\", data=x_df ,hue='gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Among people having strokes, the ages at which the risk of stroke is significant are almost similar for men and women. The only difference is men are more at risk when they are around 60 years old."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"stroke\", y=\"age\", data=x_df ,hue='smoking_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n* Among people having strokes, the ones who formerly smoked or smokes are more likely to have a stroke earlier than others. So smoking might have an impact on chances of stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nsns.violinplot(ax=axes[0], x=\"stroke\", y=\"age\", data=x_df ,hue='Residence_type')\nsns.violinplot(ax=axes[1], x=\"stroke\", y=\"age\", data=x_df ,hue='hypertension')\nsns.violinplot(ax=axes[2], x=\"stroke\", y=\"age\", data=x_df ,hue='heart_disease')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Among people having strokes, the ages at which the risk of stroke is significant are similar for rural and urban people. So living in a city or a rural area does not have an impact on strokes.\n\n* Among people having strokes, the ages at which the risk of stroke is significant are similar for people with hypertension and people without hypertension. So it is hard to tell if hypertension alone has an impact on strokes.\n\n* It is also hard to tell whether heart_disease alone has an impact on strokes or not\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 2. Missing values BMI"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Missing values :\\n{}\".format(x_df.isnull().sum()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# source : https://en.wikipedia.org/wiki/Body_mass_index\n\nNORMAL = 18.5 # All bmi values under 18.5 refer to underweight\nOVERWEIGHT = 25\nOBESE_1 = 30\nOBESE_2 = 35\nOBESE_3 = 40\nMAX_BMI = 55\n\n\nx_df = x_df[(x_df['bmi'].isnull()) | (x_df['bmi'] < MAX_BMI)] # Filter out BMI outliers\nx_overweight_df = x_df[x_df['bmi'] > OVERWEIGHT]\nx_underweight_df = x_df[x_df['bmi'] <= NORMAL]\nx_test_df = x_df[(x_df['bmi'] > NORMAL) & (x_df['bmi'] <= OVERWEIGHT )]\nx_null_df = x_df[x_df['bmi'].isnull()]\n\n\n\nprint('Ratio of overweight ppl over positive class : {:.02f}%'.format(100 * (x_overweight_df['stroke'].sum() / x_df['stroke'].sum())))\nprint('Ratio of underweight ppl over positive class : {:.02f}%'.format(100 * (x_underweight_df['stroke'].sum() / x_df['stroke'].sum())))\nprint('Ratio of normal ppl over positive class : {:.02f}%'.format(100 * (x_test_df['stroke'].sum() / x_df['stroke'].sum())))\nprint('Ratio of MISSING BMI ppl over positive class : {:.02f}%'.format(100 * (x_null_df['stroke'].sum() / x_df['stroke'].sum())))\n\nsns.violinplot(data=x_df,x='stroke',y='bmi')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Replace missing BMI with median "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1 :\n#x_df['bmi'] = x_df['bmi'].fillna(x_df['bmi'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Replace missing BMI using DecisionTree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 2 :\n# Predict missing 'bmi' with other values based on 'age' and 'gender' attributes with a simple Decision Tree\nbmi_pipe = Pipeline([('scaler', StandardScaler()), \n                     ('dtr', DecisionTreeRegressor(random_state=42))\n                    ])\n\nx_pipe_df = x_df[['age','gender','bmi']].copy()\nx_pipe_df['gender'] = x_pipe_df['gender'].replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\nx_missing_df = x_pipe_df[x_pipe_df['bmi'].isnull()].drop(columns='bmi')\n\nx_pipe_df = x_pipe_df[~x_pipe_df['bmi'].isnull()]\ny_pipe_df = x_pipe_df.pop('bmi')\n\nbmi_pipe.fit(x_pipe_df,y_pipe_df)\nx_df.loc[x_missing_df.index, 'bmi'] = bmi_pipe.predict(x_missing_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Numerical features"},{"metadata":{},"cell_type":"markdown","source":"\nA child stroke is a 'very' rare event and half of them cannot be precisely explained ([source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3255104/)).  \nThe dataset contains only 2 children stroke samples which are certainly not enough to grasp children stroke event and including these two samples might drift our model training process. Therefore we have decided to remove them to predict **ONLY adults strokes**.   \nNote that if we wanted to predict all type of strokes removing these children stroke introduces a strong bias.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop children stroke\nx_children_stroke_df = x_df[(x_df['age'] < 20 ) & (x_df['stroke'] == 1)]\nx_df = x_df.drop(x_children_stroke_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop id column\nif 'id' in x_df.columns:\n    x_df = x_df.drop(columns='id')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = ['gender','ever_married','work_type','Residence_type','smoking_status']\n\nif 'Residence_type' in x_df.columns:\n    x_df = x_df.drop(columns='Residence_type') # From our experiments : 'Residence_type' add more noise than relevant information\nx_df = pd.get_dummies(x_df)\nx_df.head() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Split dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = x_df.drop(columns='stroke'), x_df['stroke']\n\n\nx_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.2, random_state = 42, shuffle = True, stratify=y)\nprint(\"Train shape : {}\\nValidation Shape : {}\".format(x_train.shape, x_val.shape))\nprint(\"Positive # samples : {}\".format(np.count_nonzero(y_val == 1)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Balancing dataset\n\nBalacing the dataset did not result in better performance (as for recall) so for now we have commented out the below code. Note that upsampling and downsampling should be done on the training set and the final model should always be evaluated on original (not synthetic) data samples."},{"metadata":{},"cell_type":"markdown","source":"## Upsampling : SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1 : Upsampling with SMOTE\n# from imblearn.over_sampling import SMOTE\n# oversample = SMOTE()\n# x_train, y_train = oversample.fit_resample(x_train, y_train)\n\n# print(\"Input shape after SMOTE : {}\".format(x_train.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Downsampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 2 : Downsample\n# FRAC = 0.9 # Drop 70% of negative samples\n# y_train_to_drop = y_train[y_train == 0].sample(frac = FRAC,random_state = 42)\n# x_train = x_train.drop(y_train_to_drop.index)\n# y_train = y_train.drop(y_train_to_drop.index)\n\n# print(\"Input shape after downsampling by {}% : {}\".format(FRAC*100,x_train.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Classification"},{"metadata":{},"cell_type":"markdown","source":"## Evaluation metrics\n\n\nDepending on business goals, the evaluation metrics to be optimized might be different. From our experiments, identifying accurately positive cases (people that had stroke) is difficult for many several reasons : unsufficent data samples, unsufficient relevant features...\nAlso this classification problem is different from other classical classification problems such as fraud detection or image classification where a sample has a unique target label regardless of the 'time'. However in the stroke classification problem, we might have samples having a lot of stroke-correlated features but that haven't had any stroke YET. Therefore they are labelled as negative samples but they might actually have a stroke anytime soon.\n\nTLDR : In this notebook, we want to focus on the recall metric instead of accuracy as we have an imbalanced dataset with positive samples as minority. We also consider that having False Negative predictions in this dataset is more dangerous than False Positive."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score, average_precision_score\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom matplotlib import pyplot as plt\n\ndef plot_metrics(targets,predictions):\n    print(\"Validation accuracy : {:.4f}\".format(accuracy_score(targets, predictions)))\n    print(\"Validation recall : {:.4f}\".format(recall_score(targets,predictions)))\n    print(\"Validation precision : {:.4f}\".format(precision_score(targets,predictions)))\n    print(\"Validation f1-score : {:.4f}\".format(f1_score(targets,predictions)))\n    precision, recall, _ = precision_recall_curve(targets, predictions)\n    pr_auc = auc(recall,precision) # NB : average_precision_score(y_val, predictions) also gives \"AUC\" precision/recall\n    print(\"Validation auc : {:.4f}\".format(pr_auc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale features\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(C=1,random_state=42,class_weight='balanced')\nclf.fit(x_train, y_train)\npredictions = clf.predict(x_val)\nplot_metrics(y_val, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features impotance\nplt.xticks(rotation=90)\nplt.bar(x.columns, clf.coef_[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* The model seems to give too much importance to the 'age' attribute.\n* The above non detected cases all have relatively low ages compared to person that had stroke (80 yo vs 58 yo)\n* if the individual is a child, it helps the model predicting the sample as non-stroke (high negative peak for work_type = children)"},{"metadata":{},"cell_type":"markdown","source":"## Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndt_clf = DecisionTreeClassifier(random_state=42, class_weight='balanced')\ndt_clf.fit(x_train,y_train)\npredictions = dt_clf.predict(x_val)\nplot_metrics(y_val, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nsvm_clf = LinearSVC(C=0.01, class_weight ='balanced')\nsvm_clf.fit(x_train, y_train)\n\n# Evaluation\npredictions = svm_clf.predict(x_val)\nplot_metrics(y_val, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features impotance\nplt.xticks(rotation=90)\nplt.bar(x.columns, svm_clf.coef_[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Non linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_clf = SVC(C=1, class_weight ='balanced', kernel='rbf',gamma='auto')\nsvm_clf.fit(x_train, y_train)\n\n# Evaluation\npredictions = svm_clf.predict(x_val)\nplot_metrics(y_val, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Gridsearch and cross-validation with SVM\nIn the last section, simple models were evaluated with manually chosen parameters using a fix training/validation sets. \nTo automate the evaluation process and to get a better estimate on how accurate our model will be in practice we will use **GridSearch** and **cross-validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nskf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nsvc = SVC()\nsvc_pipe = Pipeline(steps=[('scaler', scaler), ('svc', svc)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'svc__kernel':['linear', 'rbf'], \n              'svc__C':[0.1, 1, 10],\n             'svc__class_weight' : ['balanced']}\n\n\nclf = GridSearchCV(svc_pipe, parameters,cv=skf, scoring=['recall','precision'], refit='recall')\nclf.fit(x,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = clf.best_estimator_\nprint('Best params :{}\\nBest CV score(recall) : {}'.format(clf.best_params_,clf.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finally,we end up with a high CV recall 0.84 traded with accuracy/precision meaning that our models predict a lot of non-positive sample as positive. By ignoring the classication problem, a positive sample predicted by our model can also be considered as an individual that has more than 50% to get a stroke. Thinking about probabilities might be better as the predictions are given to people that did not have a stroke yet so they can get a preventive treatment depending on the risks."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}