{"cells":[{"metadata":{"_uuid":"716fa6095cfff789b47e773878f4b82bdd4e31f9","_cell_guid":"49842f61-a98e-405a-974f-59e94da78ce8"},"cell_type":"markdown","source":"# Spam and Ham classification\n\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"},{"metadata":{"_uuid":"0259bd59e4fad41ffd78aa5a510a83ef07daab7f","_cell_guid":"d6b1ab40-34da-4645-98c4-4b61c97895f6","collapsed":true},"cell_type":"markdown","source":"Import library"},{"execution_count":null,"metadata":{"_uuid":"214d949f0f90e0aba892b1b9d3a8219490eb6ce1","_cell_guid":"0e2adfc4-c735-4694-8e5f-cc672ab5b24d","collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer","outputs":[]},{"metadata":{"_uuid":"9578802e04faef0abd3502cbcb8f60d3f9871f67","_cell_guid":"5478ea31-4a3f-45bf-9e2f-86d5b797a180"},"cell_type":"markdown","source":"Read data"},{"execution_count":null,"metadata":{"_uuid":"4429a082ddabcb23261daec29ee1ae9e68ba2a2e","_cell_guid":"a89760f4-9075-42be-b4c4-c8c87d175c88","collapsed":true},"cell_type":"code","source":"data_dir = \"../input/\"\ndf = pd.read_csv(data_dir + \"/spam.csv\", encoding = 'latin-1')\n\nprint (df.shape)\nprint (df.head())","outputs":[]},{"metadata":{"_uuid":"4c97a2d598fdc678ebac37a28485e6b56cd2a0dd","_cell_guid":"fb7d1a73-39d6-43d9-b23a-171968ab0344"},"cell_type":"markdown","source":"split data into train set and test set"},{"execution_count":null,"metadata":{"_uuid":"8d418776458bcdd5cb059dedf505aad4ad503901","_cell_guid":"ace4225f-7d4b-429f-86ef-b29091f4b920","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.v2, df.v1, test_size=0.2, random_state=0)\nprint (X_train.head())\nprint (y_train.head())","outputs":[]},{"metadata":{"_uuid":"483d58d9fa3bf5d4e034d7807ff0aed9b9a7675f","_cell_guid":"920ceb77-3694-45e3-896d-6a6abf7089f7"},"cell_type":"markdown","source":"CountVectorizer example"},{"execution_count":null,"metadata":{"_uuid":"5116f876800e6e1647a25e9ef6dfe74f02318c06","_cell_guid":"a2e96086-3318-45cf-931d-20971df3a9ca","collapsed":true},"cell_type":"code","source":"vect = CountVectorizer()\nexample = ['I love you, good bad bad', 'you are soo good']\nexample2 = ['hope not me, hey hey wu suo', 'qian mian de luoo']\n\nresult = vect.fit_transform(example)\nresult2 = vect.transform(example2)\nprint (result.shape)\nprint (result)\nprint (result2.shape)\nprint (result2)","outputs":[]},{"metadata":{"_uuid":"08ee317a74217303f5f20f74e4788e1590cabb4e","_cell_guid":"bb6ae7f3-5cb5-4f67-9a5e-7f92b8c56f4d"},"cell_type":"markdown","source":"Count words number in the training set"},{"execution_count":null,"metadata":{"_uuid":"bc7497787d1d19eee918fec0faabe29669a8b3fb","_cell_guid":"76be376a-0c66-4001-ac99-9b968ce62781","collapsed":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nX_train_count = vectorizer.fit_transform(X_train)\nX_test_count = vectorizer.transform(X_test)\nprint (X_train_count.shape)\nprint (X_train_count)\n#print (vectorizer.vocabulary_)\nprint (X_test_count.shape)\nprint (X_test_count)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"542b5e7daee7e2c14a560b9e01ba6520e78ee5fa","_cell_guid":"8a93e06e-0c02-41ce-8766-5bb4ab3c9ca4","collapsed":true},"cell_type":"code","source":"# show how word count looks like\nword_freq_df = pd.DataFrame({'word': vectorizer.get_feature_names(), 'occurrences': X_train_count.toarray().sum(axis=0)})\nplt.plot(word_freq_df.occurrences)\nplt.show()\n\nword_freq_df['frequency'] = word_freq_df['occurrences'] / np.sum(word_freq_df['occurrences'])\nword_freq_df_sort = word_freq_df.sort_values(by=['occurrences'], ascending=False)\nword_freq_df_sort.head()","outputs":[]},{"metadata":{"_uuid":"1c236f43a4915a4f3567c7d91b297f0214d57610","_cell_guid":"ab63bd91-b913-4ec5-bd4e-2c731e67cfac"},"cell_type":"markdown","source":"选择比较重要的feature"},{"metadata":{"_uuid":"d1ccbf1a3bfbdd521c52db477e9d40e9d4268212","_cell_guid":"fd0114c9-7e15-4da0-8b1c-e87c0643e073"},"cell_type":"markdown","source":"Train model"},{"execution_count":null,"metadata":{"_uuid":"ad1c1cf6085fdf9d83c34c68b324f1c73e710315","_cell_guid":"708ea854-0c20-4ac0-97b7-90075d8f63c4","collapsed":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train_count, y_train)\ny_pred = classifier.predict(X_test_count)\nprint (y_pred)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"d2dd8d2b739e46a895f63cbd55077d47f81a7429","_cell_guid":"9930bb73-6ffb-4de2-a933-b43b329e34cd","collapsed":true},"cell_type":"code","source":"# Check model accuracy\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nprint (accuracy_score(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint (confusion_matrix(y_test, y_pred))\n\n# this time just calculate the train set\ncross_val = cross_val_score(classifier, X_train_count, y_train, cv=20, scoring='accuracy')\nprint (cross_val)\nprint (np.mean(cross_val))","outputs":[]},{"metadata":{"_uuid":"3e59b3f2f239d8be210975d12598f590866337c3","_cell_guid":"d9cc3ecb-0f94-42d9-80bc-ab3369c3a616"},"cell_type":"markdown","source":"# hand written Naive Bayes"},{"execution_count":null,"metadata":{"_uuid":"d26e952060201ceabf1868e57ca195346f23cff3","_cell_guid":"4cc9d40d-2987-4976-998a-2c30627d2c36","collapsed":true},"cell_type":"code","source":"def get_vocabulary(data):\n    vocabulary_set = set([])\n    for document in data:\n        words = document.split()\n        for word in words:\n            vocabulary_set.add(word)\n    return list(vocabulary_set)\n\nvocabulary_list = get_vocabulary(df.v2)\nprint (df.v2.shape)\n","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"365008e4886eab2bf027134d616ef951595d62fe","_cell_guid":"f1d0dfd3-0206-4701-86d8-e5236265b8d2","collapsed":true},"cell_type":"code","source":"def document_2_vector(vocabulary_list, data):\n    word_vector = np.zeros(len(vocabulary_list))\n    words = data.split()\n    for word in words:\n        if word in vocabulary_list:\n            word_vector[vocabulary_list.index(word)] += 1\n    return word_vector","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"1f90d225834fbcb6306f2c01b10740f08786486e","_cell_guid":"115734e8-a356-40f1-8fba-76f19addd5bf","collapsed":true},"cell_type":"code","source":"train_matrix = []\nfor document in X_train:\n    word_vector = document_2_vector(vocabulary_list, document)\n    train_matrix.append(word_vector)\nprint (len(train_matrix))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"d018b19416e454c051f2b85bdd8d424f9a1d2fac","_cell_guid":"11b32b37-f63e-4b51-be2f-58830ff3901e","collapsed":true},"cell_type":"code","source":"def naive_bayes_train(train_matrix, y_train):\n    docs_num = len(train_matrix)\n    words_num = len(train_matrix[0])\n    \n    spam_vector_count = np.ones(words_num)\n    ham_vector_count = np.ones(words_num)\n    spam_total_count = words_num\n    ham_total_count = words_num\n    spam_count = 0\n    ham_count = 0\n    \n    for i in range(docs_num):\n#         if i > 100:\n#             break\n        if y_train[i] == 'spam':\n            spam_vector_count += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n        else:\n            ham_vector_count += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n    p_spam_vector = np.log(spam_vector_count/spam_total_count)\n    p_ham_vector = np.log(ham_vector_count/ham_total_count)\n    return p_spam_vector, np.log(spam_count / docs_num), p_ham_vector, np.log(ham_count / docs_num)\np_spam_vector, p_spam, p_ham_vector, p_ham = naive_bayes_train(train_matrix, y_train.values)\n\nprint (p_spam_vector)\n    ","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"cc45900c7cdc53055ce6c5b085c59cd86415374f","_cell_guid":"2395958f-73d2-41e6-b1ea-344ce2a3a7ec","collapsed":true},"cell_type":"code","source":"def doc_predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham):\n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if (spam > ham):\n        return 'spam'\n    else:\n        return 'ham'\n    \ndef predict(X_test, p_spam_vector, p_spam, p_ham_vector, p_ham):\n    predictions = []\n    for document in X_test.values:\n        test_word_vector = document_2_vector(vocabulary_list, document)\n        ans = doc_predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n        predictions.append(ans)\n    return predictions\n\ny_pred = predict(X_test, p_spam_vector, p_spam, p_ham_vector, p_ham)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"1a33ff57efe6eab6beed8937f8fe4c5b5bbe351c","_cell_guid":"9864edd3-1d1f-4f6c-986e-1da32dd9714b","collapsed":true},"cell_type":"code","source":"# Check model accuracy\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nprint (accuracy_score(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint (confusion_matrix(y_test, y_pred))","outputs":[]}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}}}