{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Credits to [Bahy](https://www.kaggle.com/bahyhelmihp/kudo-bahy-title-translation)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\ndatadir = '../input/shopee-code-league-20/_DS_Title_Translation'\n\ndev_en = pd.read_csv(os.path.join(datadir, 'dev_en.csv'))\ndev_en.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_tcn = pd.read_csv(os.path.join(datadir,'dev_tcn.csv'))\ndev_tcn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\nimport jieba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_tcn = pd.concat([dev_en['translation_output'], dev_tcn['text']], axis=1).values\nen_tcn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove punctuation, symbols, any non words\nen_tcn[:,0] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,1]]\nen_tcn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert text to lowercase\nfor i in range(len(en_tcn)):\n    en_tcn[i,0] = en_tcn[i,0].lower()\n    en_tcn[i,1] = en_tcn[i,1].lower()\nen_tcn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Cut with jieba before tokenize with keras tokenizer, remove extra spaces\nen_tcn[:,0] = [re.sub(' +', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(' +', ' ', \" \".join(jieba.cut(s, cut_all=False))) for s in en_tcn[:,1]]\nen_tcn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# empty lists\neng_l = []\ntcn_l = []\n\n# populate the lists with sentence lengths\nfor i in en_tcn[:,0]:\n      eng_l.append(len(i.split()))\n\nfor i in en_tcn[:,1]:\n      tcn_l.append(len(i.split()))\n\nlength_df = pd.DataFrame({'eng':eng_l, 'tcn':tcn_l})\n\nlength_df.hist(bins = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(eng_l), max(tcn_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare english tokenizer\neng_tokenizer = tokenization(en_tcn[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = max(eng_l)\nprint('English Vocabulary Size: %d' % eng_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare Chinese tokenizer\ntcn_tokenizer = tokenization(en_tcn[:, 1])\ntcn_vocab_size = len(tcn_tokenizer.word_index) + 1\n\ntcn_length = max(tcn_l)\nprint('Chineese Vocabulary Size: %d' % tcn_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(en_tcn, test_size=0.2, random_state = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare training data\ntrainX = encode_sequences(tcn_tokenizer, tcn_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(tcn_tokenizer, tcn_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model compilation\nmodel = define_model(tcn_vocab_size, eng_vocab_size, tcn_length, eng_length, 512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'model.h1.31_jul_20'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('model.h1.31_jul_20')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\npred_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q sacrebleu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# product_title_translation_eval_script.py\n\"\"\"Sample evaluation script for product title translation.\"\"\"\nfrom typing import List\nimport regex\n# !pip install sacrebleu\nfrom sacrebleu import corpus_bleu\n\nOTHERS_PATTERN: re.Pattern = regex.compile(r'\\p{So}')\n\n\ndef eval(preds: List[str], refs: List[str]) -> float:\n    \"\"\"BLEU score computation.\n\n    Strips all characters belonging to the unicode category \"So\".\n    Tokenize with standard WMT \"13a\" tokenizer.\n    Compute 4-BLEU.\n\n    Args:\n        preds (List[str]): List of translated texts.\n        refs (List[str]): List of target reference texts.\n    \"\"\"\n    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n    return corpus_bleu(\n        preds, [refs],\n        lowercase=True,\n        tokenize='13a',\n        use_effective_order= False\n    ).score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval(pred_df['actual'], pred_df['predicted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use all val data to predict test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(os.path.join(datadir,'test_tcn.csv'))\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_test_data(arr):\n    ## Remove punctuation, symbols, extra spaces\n    arr = arr.apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n    arr = arr.apply(lambda x: re.sub(' +', ' ', \" \".join(jieba.cut(x, cut_all=False))))\n    ## To lowercase\n    arr = arr.apply(lambda x: x.lower())\n    ## Get max length\n    tcn_l = []\n    for i in arr:\n        tcn_l.append(len(i.split()))\n    tcn_length = max(tcn_l)\n    \n    return arr, tcn_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tcn_arr, tcn_length_test = process_test_data(test_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tcn_arr.head())\nprint(tcn_length_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare test data\ntestX_test = encode_sequences(tcn_tokenizer, tcn_length_test, tcn_arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_classes(testX_test.reshape((testX_test.shape[0],testX_test.shape[1])))\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame({\"translation_output\": preds_text})\nres.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str_zero = res['translation_output'].value_counts().index[0]\nstr_zero\nstr_most = 'baby'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res['translation_output'] = res['translation_output'].apply(lambda x: \"baby\" if x == str_zero else x)\nres['translation_output'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv(\"./submission.csv\").shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}