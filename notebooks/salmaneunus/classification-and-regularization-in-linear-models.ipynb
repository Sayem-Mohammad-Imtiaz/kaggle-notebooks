{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mglearn==0.1.9","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:43.587979Z","iopub.execute_input":"2021-06-17T14:07:43.588481Z","iopub.status.idle":"2021-06-17T14:07:52.513128Z","shell.execute_reply.started":"2021-06-17T14:07:43.588446Z","shell.execute_reply":"2021-06-17T14:07:52.512217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport sys\nfrom scipy import sparse\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport mglearn\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:52.514543Z","iopub.execute_input":"2021-06-17T14:07:52.514824Z","iopub.status.idle":"2021-06-17T14:07:52.908006Z","shell.execute_reply.started":"2021-06-17T14:07:52.514794Z","shell.execute_reply":"2021-06-17T14:07:52.907321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Models For Binary Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nX, y = mglearn.datasets.make_forge()\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n    clf = model.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n    ax=ax, alpha=.7)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\naxes[0].legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:52.909312Z","iopub.execute_input":"2021-06-17T14:07:52.909724Z","iopub.status.idle":"2021-06-17T14:07:53.330932Z","shell.execute_reply.started":"2021-06-17T14:07:52.909682Z","shell.execute_reply":"2021-06-17T14:07:53.330025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the figure above, we display the first feature of the forge dataset on the x axis and 2nd feature on the y-axis. The decision boundaries found by LinearSVC and LogisticRegression are displayed as straight lines, separating the area classifies as class 1 and 0.**","metadata":{}},{"cell_type":"markdown","source":"**Here regularization parameter is applied where the strength of the regularization is called 'C'. Higher values of 'C' corresponds to less regulariztion.**","metadata":{}},{"cell_type":"code","source":"mglearn.plots.plot_linear_svc_regularization()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:53.332244Z","iopub.execute_input":"2021-06-17T14:07:53.332575Z","iopub.status.idle":"2021-06-17T14:07:53.48664Z","shell.execute_reply.started":"2021-06-17T14:07:53.332545Z","shell.execute_reply":"2021-06-17T14:07:53.485589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**On the lefthand figure, small C means a lot of regularization. In the centre plot, C is slightly higher and the very high value of C in the model tilts the decision boundary a lot, now correctly classifying all the points in class 0. One of the points in class 1 is still classified, as it is not possible to correctly classify all points in this dataset using a straight\nline.**","metadata":{}},{"cell_type":"markdown","source":"**Let's analyze LinearLogisticRegression in more detail on the Breast Cancer dataset.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\ncancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:53.487976Z","iopub.execute_input":"2021-06-17T14:07:53.488237Z","iopub.status.idle":"2021-06-17T14:07:53.563257Z","shell.execute_reply.started":"2021-06-17T14:07:53.488212Z","shell.execute_reply":"2021-06-17T14:07:53.561909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The default value of C=1 provides quite good performance, with 95% accuracy on both the training and the test set. But as training and test set performance are very close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible model.**","metadata":{}},{"cell_type":"markdown","source":"**Using C=100 results in higher training set accuracy, and also a slightly increased test set accuracy, confirming our intuition that a more complex model should perform better.**","metadata":{}},{"cell_type":"code","source":"logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:53.568817Z","iopub.execute_input":"2021-06-17T14:07:53.56931Z","iopub.status.idle":"2021-06-17T14:07:53.633436Z","shell.execute_reply.started":"2021-06-17T14:07:53.569254Z","shell.execute_reply":"2021-06-17T14:07:53.632545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can also investigate what happens if we use an even more regularized model than the default of C=1, by setting C=0.01:**","metadata":{}},{"cell_type":"code","source":"logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:53.63475Z","iopub.execute_input":"2021-06-17T14:07:53.635228Z","iopub.status.idle":"2021-06-17T14:07:53.700596Z","shell.execute_reply.started":"2021-06-17T14:07:53.635184Z","shell.execute_reply":"2021-06-17T14:07:53.699494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's look at the coefficients learned by the models with the three different settings of the regularization parameter C**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.ylim(-5, 5)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:53.706504Z","iopub.execute_input":"2021-06-17T14:07:53.707004Z","iopub.status.idle":"2021-06-17T14:07:54.188655Z","shell.execute_reply.started":"2021-06-17T14:07:53.706956Z","shell.execute_reply":"2021-06-17T14:07:54.187691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As Logistic Regression applies L2 regularization by default, the results look similar to that produced by ridge regularization. Stronger regularization pushes coefficients more and more toward zero. By observing the plot more closely, we can also see an interesting effect in the third coefficient, for “mean perimeter.” For C=100 and C=1, the coefficient\nis negative, while for C=0.001, the coefficient is positive, with a magnitude that is even larger than for C=1. Interpreting a model like this, one might think the coefficient tells us which class a feature is associated with. For example, one might think that a high “texture error” feature is related to a sample being “malignant.” However, the change of sign in the coefficient for “mean perimeter” means that depending on which model we look at, a high “mean perimeter” could be taken as being either indicative of “benign” or indicative of “malignant.”**","metadata":{}},{"cell_type":"markdown","source":"**Coefficient plot and classification accuracies for L2 regularization**","metadata":{}},{"cell_type":"code","source":"for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l2 = LogisticRegression(C=C, penalty=\"l2\").fit(X_train, y_train)\n    print(\"Training accuracy of l2 logreg with C={:.3f}: {:.2f}\".format(C, lr_l2.score(X_train, y_train)))\n    print(\"Test accuracy of l2 logreg with C={:.3f}: {:.2f}\".format(C, lr_l2.score(X_test, y_test)))\n    plt.plot(lr_l2.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:54.189988Z","iopub.execute_input":"2021-06-17T14:07:54.190295Z","iopub.status.idle":"2021-06-17T14:07:54.669973Z","shell.execute_reply.started":"2021-06-17T14:07:54.190254Z","shell.execute_reply":"2021-06-17T14:07:54.668896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Models For Multi-Class Classification","metadata":{}},{"cell_type":"markdown","source":"**Two-dimensional toy dataset shows dataset containing 3 classes**","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:54.671222Z","iopub.execute_input":"2021-06-17T14:07:54.671495Z","iopub.status.idle":"2021-06-17T14:07:54.827563Z","shell.execute_reply.started":"2021-06-17T14:07:54.671468Z","shell.execute_reply":"2021-06-17T14:07:54.826799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, we train a LinearSVC classifier on the dataset**","metadata":{}},{"cell_type":"code","source":"linear_svm = LinearSVC().fit(X, y)\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:54.828475Z","iopub.execute_input":"2021-06-17T14:07:54.828837Z","iopub.status.idle":"2021-06-17T14:07:54.835243Z","shell.execute_reply.started":"2021-06-17T14:07:54.82881Z","shell.execute_reply":"2021-06-17T14:07:54.834376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n'Line class 2'], loc=(1.01, 0.3))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:54.836617Z","iopub.execute_input":"2021-06-17T14:07:54.836995Z","iopub.status.idle":"2021-06-17T14:07:55.016922Z","shell.execute_reply.started":"2021-06-17T14:07:54.836953Z","shell.execute_reply":"2021-06-17T14:07:55.01586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**You can see that all the points belonging to class 0 in the training data are above the  line corresponding to class 0, which means they are on the “class 0” side of this binary classifier. The points in class 0 are above the line corresponding to class 2, which means they are classified as “rest” by the binary classifier for class 2. The points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Therefore, any point in this area will be classified as class 0 by the final classifier the result of the classification confidence formula for classifier 0 is greater than zero, while it is smaller than zero for the other two classes.**","metadata":{}},{"cell_type":"code","source":"mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T14:07:55.018344Z","iopub.execute_input":"2021-06-17T14:07:55.018694Z","iopub.status.idle":"2021-06-17T14:07:55.325021Z","shell.execute_reply.started":"2021-06-17T14:07:55.018662Z","shell.execute_reply":"2021-06-17T14:07:55.324071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The above figure shows Multiclass decision boundaries derived from the three one-vs.-rest classifiers**","metadata":{}}]}