{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Principal Component Analysis\n* Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more\n* In this tutorial, we will see that PCA is not just a “black box”, and we are going to unravel its internals in 3 basic steps.\n* Large data  is useful but on computation it causes a lot of issue\n* PCA helps us to retain the correlation among the most important feature of the dataset\n# Linear Decomposition Analysis\n* LDA is linear transformation methods\n* LDA helps to find the direction which maximize the separation of different classes.\n* It helps in pattern classification\n* LDA determines the suitable feature of the dataset\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# PCA and Dimensionality Reduction\n* he desired goal is to reduce the dimensions of a d-dimensional dataset by projecting it onto a (k)-dimensional subspace (where k<d)\n*  in order to increase the computational efficiency while retaining most of the information\n* An important question is “what is the size of k that represents the data ‘well’?”\n* Later, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix\n* If some eigenvalues have a significantly larger magnitude than others, then the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the “less informative” eigenpairs is reasonable.\n","metadata":{}},{"cell_type":"markdown","source":"# A Summary of the PCA Approach\n* Standardize the data.\n* Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition.\n* Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k≤d).\n* Construct the projection matrix W from the selected k eigenvectors.\n* Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/iris-flower-dataset/IRIS.csv', sep=',')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['species'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df.iloc[:,0:4].values\ny=df.iloc[:,4].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict={1: 'Iris-setosa',\n           2: 'Iris-virginica',\n           3: 'Iris-versicolor'}\nfeature_dict={0: 'sepal_length',1: 'sepal_width',2: 'petal_length',3: 'petal_width'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for i in range(4):\n        plt.subplot(2,2,i+1)\n        for lab in ('Iris-setosa','Iris-virginica','Iris-versicolor'):\n            plt.hist(X[y==lab, i],\n                    label=lab,\n                    bins=10,\n                    alpha=0.3)\n        plt.xlabel(feature_dict[i])\n    plt.legend(loc='upper right', fancybox=True,fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_std.shape[0]-1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_vec=np.mean(X_std,axis=0)\ncov_mat=(X_std-mean_vec).T.dot((X_std-mean_vec))/(X_std.shape[0]-1)\nprint(\"Covariance Matrix \\n%s\" %cov_mat)\n                 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Numpy Covariance matrix \\n%s\" %np.cov(X_std.T))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we perform an eigendecomposition on the covariance matrix:","metadata":{}},{"cell_type":"code","source":"cov_mat=np.cov(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(cov_mat)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Matrix","metadata":{}},{"cell_type":"code","source":"corr_mat1=np.corrcoef(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(corr_mat1)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eigendecomposition of the raw data based on the correlation matrix","metadata":{}},{"cell_type":"code","source":"cor_mat2=np.corrcoef(X.T)\neig_vals, eig_vecs=np.linalg.eig(cor_mat2)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We can clearly see that all three approaches yield the same eigenvectors and eigenvalue pairs:\n\n* Eigendecomposition of the covariance matrix after standardizing the data.\n* Eigendecomposition of the correlation matrix.\n* Eigendecomposition of the correlation matrix after standardizing the data.","metadata":{}},{"cell_type":"markdown","source":"# Singular Value Decomposition","metadata":{}},{"cell_type":"markdown","source":"While the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Value Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:","metadata":{}},{"cell_type":"code","source":"u, s, v=np.linalg.svd(X_std.T)\nu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sorting Eigenpairs\nThe typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code","metadata":{}},{"cell_type":"code","source":"for ev in eig_vecs.T:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint(\"Everithing is ok\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explained Variance: \nAfter sorting the eigenpairs, the next question is “how many principal components are we going to choose for our new feature subspace?” A useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.","metadata":{}},{"cell_type":"code","source":"tot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above clearly shows that most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information.","metadata":{}},{"cell_type":"markdown","source":"# Projection Matrix","metadata":{}},{"cell_type":"markdown","source":"It’s about time to get to the really interesting part: The construction of the projection matrix that will be used to transform the Iris data onto the new feature subspace. Although, the name “projection matrix” has a nice ring to it, it is basically just a matrix of our concatenated top k eigenvectors.","metadata":{}},{"cell_type":"code","source":"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('Matrix W:\\n', matrix_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Projection Onto the New Feature Space\nn this last step we will use the 4×2-dimensional projection matrix W to transform our samples onto the new subspace via the equation\nY=X×W, where Y is a 150×2 matrix of our transformed samples.","metadata":{}},{"cell_type":"code","source":"Y = X_std.dot(matrix_w)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y[y==lab, 0],\n                    Y[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shortcut - PCA in scikit-learn","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y_sklearn[y==lab, 0],\n                    Y_sklearn[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='upper center')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \n* It helps us to reduce the dimesion of tha dataset\n* Here the helpful resources \n>https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html","metadata":{}}]}