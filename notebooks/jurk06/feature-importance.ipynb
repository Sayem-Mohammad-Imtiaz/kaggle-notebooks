{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-31T07:22:46.387904Z","iopub.execute_input":"2021-08-31T07:22:46.388241Z","iopub.status.idle":"2021-08-31T07:22:46.398348Z","shell.execute_reply.started":"2021-08-31T07:22:46.388212Z","shell.execute_reply":"2021-08-31T07:22:46.397549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\n*  techniques that assign a score to input features based on how useful they are at predicting a target variable\n* There are many types and sources of feature importance scores, although popular examples include statistical correlation scores, coefficients calculated as part of linear models, decision trees, and permutation importance scores\n* Feature importance scores play an important role in a predictive modeling project, including providing insight into the data, insight into the model\n* Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction\n* Feature importance scores can be calculated for problems that involve predicting a numerical value, called regression, and those problems that involve predicting a class label, called classification.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:22:52.885702Z","iopub.execute_input":"2021-08-31T07:22:52.886373Z","iopub.status.idle":"2021-08-31T07:22:52.897386Z","shell.execute_reply.started":"2021-08-31T07:22:52.886336Z","shell.execute_reply":"2021-08-31T07:22:52.896516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:22:53.811367Z","iopub.execute_input":"2021-08-31T07:22:53.811883Z","iopub.status.idle":"2021-08-31T07:22:53.853157Z","shell.execute_reply.started":"2021-08-31T07:22:53.811853Z","shell.execute_reply":"2021-08-31T07:22:53.852451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop('Unnamed: 32', axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:22:54.368564Z","iopub.execute_input":"2021-08-31T07:22:54.369063Z","iopub.status.idle":"2021-08-31T07:22:54.374411Z","shell.execute_reply.started":"2021-08-31T07:22:54.369033Z","shell.execute_reply":"2021-08-31T07:22:54.373487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:22:55.390558Z","iopub.execute_input":"2021-08-31T07:22:55.391147Z","iopub.status.idle":"2021-08-31T07:22:55.394638Z","shell.execute_reply.started":"2021-08-31T07:22:55.391108Z","shell.execute_reply":"2021-08-31T07:22:55.393684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Joint PLot with Pearson Coefficent","metadata":{}},{"cell_type":"code","source":"r, p=stats.pearsonr(df.loc[:,'concavity_worst'], df.loc[:,'concave points_worst'])\ngraph=sns.jointplot(df.loc[:,'concavity_worst'], df.loc[:,'concave points_worst'], kind=\"reg\", color=\"#ce1414\",)\nphantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\ngraph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:01.082595Z","iopub.execute_input":"2021-08-31T07:23:01.083137Z","iopub.status.idle":"2021-08-31T07:23:01.852509Z","shell.execute_reply.started":"2021-08-31T07:23:01.083096Z","shell.execute_reply":"2021-08-31T07:23:01.851717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:03.460595Z","iopub.execute_input":"2021-08-31T07:23:03.461119Z","iopub.status.idle":"2021-08-31T07:23:03.466234Z","shell.execute_reply.started":"2021-08-31T07:23:03.461088Z","shell.execute_reply":"2021-08-31T07:23:03.465533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:04.788425Z","iopub.execute_input":"2021-08-31T07:23:04.788958Z","iopub.status.idle":"2021-08-31T07:23:04.796614Z","shell.execute_reply.started":"2021-08-31T07:23:04.788925Z","shell.execute_reply":"2021-08-31T07:23:04.795648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:05.838655Z","iopub.execute_input":"2021-08-31T07:23:05.83907Z","iopub.status.idle":"2021-08-31T07:23:05.84743Z","shell.execute_reply.started":"2021-08-31T07:23:05.839036Z","shell.execute_reply":"2021-08-31T07:23:05.84633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:09.844064Z","iopub.execute_input":"2021-08-31T07:23:09.844439Z","iopub.status.idle":"2021-08-31T07:23:09.86189Z","shell.execute_reply.started":"2021-08-31T07:23:09.84441Z","shell.execute_reply":"2021-08-31T07:23:09.860547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.dtype","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:10.804807Z","iopub.execute_input":"2021-08-31T07:23:10.805186Z","iopub.status.idle":"2021-08-31T07:23:10.811883Z","shell.execute_reply.started":"2021-08-31T07:23:10.805156Z","shell.execute_reply":"2021-08-31T07:23:10.810916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regresion","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:14.427786Z","iopub.execute_input":"2021-08-31T07:23:14.428132Z","iopub.status.idle":"2021-08-31T07:23:14.433304Z","shell.execute_reply.started":"2021-08-31T07:23:14.428101Z","shell.execute_reply":"2021-08-31T07:23:14.432134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:16.428283Z","iopub.execute_input":"2021-08-31T07:23:16.428792Z","iopub.status.idle":"2021-08-31T07:23:16.433826Z","shell.execute_reply.started":"2021-08-31T07:23:16.428759Z","shell.execute_reply":"2021-08-31T07:23:16.433075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LogisticRegression(solver=\"liblinear\")\nlr.fit(X, y)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:19.306701Z","iopub.execute_input":"2021-08-31T07:23:19.307227Z","iopub.status.idle":"2021-08-31T07:23:19.322319Z","shell.execute_reply.started":"2021-08-31T07:23:19.307195Z","shell.execute_reply":"2021-08-31T07:23:19.321537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:20.695756Z","iopub.execute_input":"2021-08-31T07:23:20.696213Z","iopub.status.idle":"2021-08-31T07:23:20.704223Z","shell.execute_reply.started":"2021-08-31T07:23:20.696179Z","shell.execute_reply":"2021-08-31T07:23:20.702686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = lr.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:23.351241Z","iopub.execute_input":"2021-08-31T07:23:23.351629Z","iopub.status.idle":"2021-08-31T07:23:23.831776Z","shell.execute_reply.started":"2021-08-31T07:23:23.351598Z","shell.execute_reply":"2021-08-31T07:23:23.830519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature imporatnce after Standarization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:33.298762Z","iopub.execute_input":"2021-08-31T07:23:33.299149Z","iopub.status.idle":"2021-08-31T07:23:33.310669Z","shell.execute_reply.started":"2021-08-31T07:23:33.299111Z","shell.execute_reply":"2021-08-31T07:23:33.309468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LogisticRegression(solver=\"liblinear\")\nlr.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:35.906035Z","iopub.execute_input":"2021-08-31T07:23:35.906427Z","iopub.status.idle":"2021-08-31T07:23:35.918316Z","shell.execute_reply.started":"2021-08-31T07:23:35.906393Z","shell.execute_reply":"2021-08-31T07:23:35.917176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = lr.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:23:37.307811Z","iopub.execute_input":"2021-08-31T07:23:37.308195Z","iopub.status.idle":"2021-08-31T07:23:37.570578Z","shell.execute_reply.started":"2021-08-31T07:23:37.308162Z","shell.execute_reply":"2021-08-31T07:23:37.569356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* Standarization affect the features importance of the dataset\n* As in above graph we can see that there wwre few features whose values were either too negative or positive. Rest of the features had almost 0\n* After standarization we can see that a lot of the features wholse coefficeint values are much better. \n* Here negative values indicate that it tries to push the model towards the negative side\n* Same case with the positive value which tends to push the model in positive side.","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:24:06.983785Z","iopub.execute_input":"2021-08-31T07:24:06.984131Z","iopub.status.idle":"2021-08-31T07:24:06.992911Z","shell.execute_reply.started":"2021-08-31T07:24:06.984103Z","shell.execute_reply":"2021-08-31T07:24:06.991894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:24:13.920179Z","iopub.execute_input":"2021-08-31T07:24:13.92056Z","iopub.status.idle":"2021-08-31T07:24:13.924421Z","shell.execute_reply.started":"2021-08-31T07:24:13.920531Z","shell.execute_reply":"2021-08-31T07:24:13.923629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:24:15.831584Z","iopub.execute_input":"2021-08-31T07:24:15.831976Z","iopub.status.idle":"2021-08-31T07:24:16.35718Z","shell.execute_reply.started":"2021-08-31T07:24:15.831945Z","shell.execute_reply":"2021-08-31T07:24:16.356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* The most important features are \"radius_worst\"\n* The lkist of the important featuires are --Radius_worst, texture_worst, concave point worst, texture mean, concavity features, etc\n* Those features wholse value is almost equal to 0 , are least important features. \n* Least features can be removed form the dataset and the dataset will be used for prediction. ","metadata":{}},{"cell_type":"markdown","source":"# Feature importance after the standarization ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:24:25.216806Z","iopub.execute_input":"2021-08-31T07:24:25.217157Z","iopub.status.idle":"2021-08-31T07:24:25.229735Z","shell.execute_reply.started":"2021-08-31T07:24:25.217128Z","shell.execute_reply":"2021-08-31T07:24:25.228468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar([x for x in range(len(importance))], importance)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:24:28.024876Z","iopub.execute_input":"2021-08-31T07:24:28.025238Z","iopub.status.idle":"2021-08-31T07:24:28.333124Z","shell.execute_reply.started":"2021-08-31T07:24:28.025208Z","shell.execute_reply":"2021-08-31T07:24:28.332112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conlusion\n* Since we are getting same number of the important features.\n","metadata":{}},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:25:10.596364Z","iopub.execute_input":"2021-08-31T07:25:10.596731Z","iopub.status.idle":"2021-08-31T07:25:10.606222Z","shell.execute_reply.started":"2021-08-31T07:25:10.596701Z","shell.execute_reply":"2021-08-31T07:25:10.605173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:25:11.80794Z","iopub.execute_input":"2021-08-31T07:25:11.808308Z","iopub.status.idle":"2021-08-31T07:25:11.812993Z","shell.execute_reply.started":"2021-08-31T07:25:11.808277Z","shell.execute_reply":"2021-08-31T07:25:11.811794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:25:12.826409Z","iopub.execute_input":"2021-08-31T07:25:12.826964Z","iopub.status.idle":"2021-08-31T07:25:13.651962Z","shell.execute_reply.started":"2021-08-31T07:25:12.826928Z","shell.execute_reply":"2021-08-31T07:25:13.65087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* The most important features is \"Concave Point Wrost\"\n* The list of the most imporatnt feature that we obtained using Random Forest Classifier are----Perimetr Wrost, Concave point worst, radius worst, concave point mean, area worst, texture mean etc.\n* SInce we can see that there are llarge number of important features ae available\n* Those feature which have least value can be removed from the dataset. ","metadata":{}},{"cell_type":"markdown","source":"# Feature importance before Standarization","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:03.276728Z","iopub.execute_input":"2021-08-31T07:30:03.277223Z","iopub.status.idle":"2021-08-31T07:30:03.290045Z","shell.execute_reply.started":"2021-08-31T07:30:03.277181Z","shell.execute_reply":"2021-08-31T07:30:03.288886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:04.730057Z","iopub.execute_input":"2021-08-31T07:30:04.730643Z","iopub.status.idle":"2021-08-31T07:30:04.739772Z","shell.execute_reply.started":"2021-08-31T07:30:04.730581Z","shell.execute_reply":"2021-08-31T07:30:04.738967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestClassifier()\nmodel.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:27:05.963826Z","iopub.execute_input":"2021-08-31T07:27:05.9642Z","iopub.status.idle":"2021-08-31T07:27:06.257049Z","shell.execute_reply.started":"2021-08-31T07:27:05.964164Z","shell.execute_reply":"2021-08-31T07:27:06.255797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:27:29.375866Z","iopub.execute_input":"2021-08-31T07:27:29.376425Z","iopub.status.idle":"2021-08-31T07:27:30.122801Z","shell.execute_reply.started":"2021-08-31T07:27:29.376392Z","shell.execute_reply":"2021-08-31T07:27:30.121799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* It is same as done above \n* Without and with standarization has less impact or almost no impact on feature importance","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Feature Importance","metadata":{}},{"cell_type":"code","source":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:21.446719Z","iopub.execute_input":"2021-08-31T07:30:21.447263Z","iopub.status.idle":"2021-08-31T07:30:21.455721Z","shell.execute_reply.started":"2021-08-31T07:30:21.4472Z","shell.execute_reply":"2021-08-31T07:30:21.454898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:22.813049Z","iopub.execute_input":"2021-08-31T07:30:22.813421Z","iopub.status.idle":"2021-08-31T07:30:22.817668Z","shell.execute_reply.started":"2021-08-31T07:30:22.813391Z","shell.execute_reply":"2021-08-31T07:30:22.816848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:23.858611Z","iopub.execute_input":"2021-08-31T07:30:23.859151Z","iopub.status.idle":"2021-08-31T07:30:24.462943Z","shell.execute_reply.started":"2021-08-31T07:30:23.859106Z","shell.execute_reply":"2021-08-31T07:30:24.461914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification\n* The most important features is Radius Worst\n* The number of important features decreaseas.\n* As we can see that Perimeter Worst is 2nd important features.","metadata":{}},{"cell_type":"markdown","source":"# Permutation Feature Importance\n* technique for calculating relative importance scores that is independent of the model used.\n* First, a model is fit on the dataset, such as a model that does not support native feature importance scores. \n* This approach can be used for regression or classification and requires that a performance metric be chosen as the basis of the importance score, such as the mean squared error for regression and accuracy for classification.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import permutation_importance","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:30.779657Z","iopub.execute_input":"2021-08-31T07:30:30.780224Z","iopub.status.idle":"2021-08-31T07:30:30.785075Z","shell.execute_reply.started":"2021-08-31T07:30:30.780175Z","shell.execute_reply":"2021-08-31T07:30:30.784191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:36.619174Z","iopub.execute_input":"2021-08-31T07:30:36.619912Z","iopub.status.idle":"2021-08-31T07:30:36.631054Z","shell.execute_reply.started":"2021-08-31T07:30:36.619852Z","shell.execute_reply":"2021-08-31T07:30:36.629711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KNeighborsClassifier()\n# fit the model\nmodel.fit(X, y)\n# perform permutation importance\nresults = permutation_importance(model, X, y, scoring='neg_mean_squared_error')\n# get importance\nimportance = results.importances_mean\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T07:30:37.665055Z","iopub.execute_input":"2021-08-31T07:30:37.665511Z","iopub.status.idle":"2021-08-31T07:30:42.519879Z","shell.execute_reply.started":"2021-08-31T07:30:37.66547Z","shell.execute_reply":"2021-08-31T07:30:42.518832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* The most imprtant features is Area worst\n* The 2nd most important feature is Area Mean\n* as we can see that there is only 2 features which is important","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}