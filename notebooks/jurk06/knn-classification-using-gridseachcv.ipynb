{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get the dataset and information","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding the Null Value","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical Table","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Matrix","metadata":{}},{"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap \n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), cmap='Blues', annot=True)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kde Plot for visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.kdeplot(df['Age'], shade=True, Label=\"Age\")\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Inference- Age is within 20 to 37 years ","metadata":{}},{"cell_type":"markdown","source":"# Age Statistical Value","metadata":{}},{"cell_type":"code","source":"df['Age'].describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Outcome'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of the Outcomes","metadata":{}},{"cell_type":"code","source":"print(\"The outcome values\",df['Outcome'].value_counts())\nplt.figure(figsize=(10,5))\nsns.countplot(x='Outcome', data=df)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting the arrya of the feature and Target columns","metadata":{}},{"cell_type":"code","source":"X=df.drop(['Outcome'], axis=1).values\ny=df['Outcome'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.45, random_state=45, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling ","metadata":{}},{"cell_type":"markdown","source":"# KNeighbours Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nneighbors=np.arange(1,9)\ntrain_accuracy=np.empty(len(neighbors))\ntest_accuracy=np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    \n    knn.fit(X_train, y_train)\n    \n    train_accuracy[i]=knn.score(X_train, y_train)\n    \n    test_accuracy[i]=knn.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"K-NN varying number of neighbour\")\nplt.plot(neighbors, test_accuracy, label=\"Testing Accuracy\")\nplt.plot(neighbors, train_accuracy, label=\"Train Accuracy\")\nplt.legend()\nplt.xlabel(\"Number of Neighbour\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference \nAs test accracy is high at k=7, We adopt the KNeighborsClassifier with number of neighbours as 7","metadata":{}},{"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(knn.score(X_test, y_test)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=knn.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of the model","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of the test dataset","metadata":{}},{"cell_type":"code","source":"# Predicting the Test data with model \ny_test_pred=knn.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Model","metadata":{}},{"cell_type":"code","source":"knn_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",knn_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix\n* Calculation","metadata":{}},{"cell_type":"code","source":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix\", cfm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visulalize the Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cfm_df=pd.DataFrame(cfm, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(cfm_df, cmap='Reds', annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Table of confusion Matrix","metadata":{}},{"cell_type":"code","source":"pd.crosstab(y_test, y_test_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy ","metadata":{}},{"cell_type":"code","source":"print(\"correct prediction\", \n      round((trueNegative+truePositive)/len(y_test_pred)*100, 1),'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are four ways to check if the predictions are right or wrong:\n* TN / True Negative: the case was negative and predicted negative\n* TP / True Positive: the case was positive and predicted positive\n* FN / False Negative: the case was positive but predicted negative\n* FP / False Positive: the case was negative but predicted positive\n* Precision — What percent of your predictions were correct?\n* Precision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive.\n* Precision:- Accuracy of positive predictions.\n* Precision = TP/(TP + FP)\n* Recall — What percent of the positive cases did you catch?\n* Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.\n* Recall:- Fraction of positives that were correctly identified.\n* Recall = TP/(TP+FN)\n* F1 score — What percent of positive predictions were correct?\n* The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n* F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n* Support\n* Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process.\n* ","metadata":{}},{"cell_type":"markdown","source":"# Summary table of Precision, Recall, F1-Score, Support","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC(Reciever Operating Curve)","metadata":{}},{"cell_type":"code","source":"y_test_pred_prob=knn.predict_proba(X_test)[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='knn')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC (k_nn=7)\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n* The k-nn is very godd as we can see that it is not dumb model as AUC is more than 0.5 , Hence it is performing very good.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test, y_test_pred_prob)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_range = list(range(1, 26))\nscores = []\nfor i in k_range:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, y_pred))\nplt.plot(k_range, scores)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation\n\nNow before getting into the details of Hyperparamter tuning, let us understand the concept of Cross validation.\n\nThe trained model's performance is dependent on way the data is split. It might not representative of the model’s ability to generalize.\n\nThe solution is cross validation.\n\nCross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it.\n# K-fold cross-validation\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning\n\nThe value of k (i.e 7) we selected above was selected by observing the curve of accuracy vs number of neighbors. This is a primitive way of hyperparameter tuning.\n\nThere is a better way of doing it which involves:\n\n1) Trying a bunch of different hyperparameter values\n\n2) Fitting all of them separately\n\n3) Checking how well each performs\n\n4) Choosing the best performing one\n\n5) Using cross-validation every time\n\nScikit-learn provides a simple way of achieving this using GridSearchCV i.e Grid Search cross-validation.","metadata":{}},{"cell_type":"markdown","source":"In almost any Machine Learning project, we train different models on the dataset and selecting the one with the best performance. However, there is almost a room for improvement as we cannot say for sure that this particular model is best for the problem at hand, hence our aim is to improve the model in any way possible. One important factor in the performances of these models are their hyperparameters, once we set appropriate values for these hyperparameters, the performance of a model can improve significantly. In this article, we will find out how we can find optimal values for the hyperparameters of a model by using GridSearchCV.","metadata":{}},{"cell_type":"markdown","source":"# GridSearchCV\n* It is the process of performing hyperparameter tuning in order to determine the optimal values for a given model\n* As mentioned above, the performance of a model significantly depends on the value of hyperparameters.\n*  Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values.\n* Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.\n* This method of classifier is optimized by cross-validation, which is done using the GridSearchCV object on a development set that comprises only half of the available labeled data.\n* The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.\n","metadata":{}},{"cell_type":"markdown","source":"# How does GridSearchCV work?\n* GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. \n* Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance.","metadata":{}},{"cell_type":"markdown","source":"# Use of GridSearch done below","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import  GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid={'n_neighbours':np.arange(1,50)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_range = list(range(1, 31))\nprint(k_range)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = dict(n_neighbors=k_range)\nprint(param_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False)\ngrid.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. estimator: Pass the model instance for which you want to check the hyperparameters.\n2. params_grid: the dictionary object that holds the hyperparameters you want to try\n3. scoring: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n4. cv: number of cross-validation you have to try for each selected set of hyperparameters\n5. verbose: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n6. n_jobs: number of processes you wish to run in parallel for this task if it -1 it will use all available processors. ","metadata":{}},{"cell_type":"code","source":"grid_mean_scores = grid.cv_results_['mean_test_score']\nprint(grid_mean_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* As we can see that the accuracy score after the GridSerach is a bit high\n* Gridseach worked in our case\n* ROC-AUC is more than 0.5 , hence it is very good\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}