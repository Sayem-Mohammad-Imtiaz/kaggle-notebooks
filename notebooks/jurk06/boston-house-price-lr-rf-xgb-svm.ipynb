{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"To predict the house price using the boston house price dataset.\nI have used multiple regressor model in order to achieve the best accuracy result according to that we wil use that model whose performance is quite high and eassy implementation.\n* . Linear Regression\n*.  Random Forest\n*.  XGBoost \n*.  SVM\nThe visulaization has been done in order to get better about the feature,\nHere the description of the dataset\nThe Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per $10,000\n* PTRATIO - pupil-teacher ratio by town\n\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in $1000's","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_boston","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston=load_boston()\ndata=pd.DataFrame(boston.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns=boston.feature_names\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns=boston.feature_names\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n>1https://archive.ics.uci.edu/ml/datasets/Housing 123 20.2. Load the Dataset 124\n* \n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to ﬁve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per $10,000\n* PTRATIO: pupil-teacher ratio by town 12.\n* B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 13.\n* LSTAT: % lower status of the population\n* MEDV: Median value of owner-occupied homes in $1000s\n* We can see that the input attributes have a mixture of units.","metadata":{}},{"cell_type":"code","source":"#Adding target variable \ndata['Price']=boston.target\ndata.head()\n# Median value of owner-occupied homes in $1000s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization \n* Pairplot helps to know the relationship among the numerical features of dataset i.e. we get the correlation among the features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding null values","metadata":{}},{"cell_type":"code","source":"data.isna().sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unique value of each featured columns","metadata":{}},{"cell_type":"code","source":"data.nunique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical table ","metadata":{}},{"cell_type":"code","source":"data.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variation of the Target columns","metadata":{}},{"cell_type":"code","source":"data['Price'].describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference \nSince the 90% price is within $ 34*1000 . This is the distrinution of the price ","metadata":{}},{"cell_type":"markdown","source":"# Correlation Matrix-Visualization\n* A correlation matrix is a table containing correlation coefficients between variables.\n* Each cell in the table represents the correlation between two variables.\n","metadata":{}},{"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = data.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap\n*  graphical representation of data using colors to visualize the value of the matrix.\n* to represent more common values or higher activities brighter colors basically blues colors are used and to represent less common or activity values, light colors are preferred\n* Heatmap is also defined by the name of the shading matrix.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(), cmap='Blues', annot=True)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kde-Plot \n* Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable.\n* depicts the probability density at different values in a continuous variable.\n* KDE Plot to visualize the PRICE of the dataset.\n* Maximum Desnsity of PRICE of house is distributed within 15 to 30","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.kdeplot(data.Price, shade=True, Label=\"Price\")\nplt.xlabel('Price')\nplt.ylabel('Probability Density')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Trying to visualize PRICE vs RM as both are correlated ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.kdeplot(data['Price'], data['RM'], color='b', shade=True, Labels='Price', shade_lowest=False, cmap='Reds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.kdeplot(data['Price'], data['RM'], color='b', shade=True, Labels='Price', shade_lowest=False, cmap='Reds')\n\nsns.kdeplot(data['Price'], data['LSTAT'], color='b', shade=True, Labels='Price', shade_lowest=False, cmap='Greens')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The above plot is not quite useful as we re not getting any infference ","metadata":{}},{"cell_type":"markdown","source":"# Box-plot\n* It is used to visualize the outlier of price \n* Anything above and below the whiskers are called outliers","metadata":{}},{"cell_type":"code","source":"data['Price'].plot(kind='box')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method of calculating the outliers\n* Q1- 25% percentile data\n* Q2- 50% percentile data\n* Q3- 75% percentile data\n* IQR- Inter qurtile range ","metadata":{}},{"cell_type":"code","source":"Q1 = np.percentile(data['Price'], 25, interpolation = 'midpoint') \nQ2 = np.percentile(data['Price'], 50, interpolation = 'midpoint') \nQ3 = np.percentile(data['Price'], 75, interpolation = 'midpoint') \n  \nprint('Q1 25 percentile of the given data is, ', Q1)\nprint('Q1 50 percentile of the given data is, ', Q2)\nprint('Q1 75 percentile of the given data is, ', Q3)\n  \nIQR = Q3 - Q1 \nprint('Interquartile range is', IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_lim = Q1 - 1.5 * IQR\nup_lim = Q3 + 1.5 * IQR\nprint('low_limit is', low_lim)\nprint('up_limit is', up_lim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Counting of the outliers","metadata":{}},{"cell_type":"code","source":"outlier =[]\nfor x in data['Price']:\n    if ((x> up_lim) or (x<low_lim)):\n         outlier.append(x)\nprint(' outlier in the dataset is', outlier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spliting the dataset\nX=data.drop(['Price'], axis=1)\ny=data['Price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3, random_state=7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm=LinearRegression()\n\nlm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coefficients=pd.DataFrame([X_train.columns, lm.coef_]).T\ncoefficients=coefficients.rename(columns={0:'Attributes',1:'Coefficients'})\ncoefficients","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=lm.predict(X_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"R^2\",metrics.r2_score(y_train, y_pred))\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.max_error(y_train, y_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics.mean_squared_log_error(y_train, y_pred)--can't use as target has negative value\nmetrics.mean_absolute_percentage_error(y_train, y_pred)\n# metrics.mean_poisson_deviance(y_train, y_pred)--can't use as target has negative value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# R-squared\n>R-squared = (TSS-RSS)/TSS\n\n                = Explained variation/ Total variation\n\n                = 1 – Unexplained variation/ Total variation\n*A higher R-squared value indicates a higher amount of variability being explained by our model and vice-versa. If we had a really low RSS value, it would mean that the regression line was very close to the actual points. high RSS value, it would mean that the regression line was far away from the actual points. independent variables fail to explain the majority of variation in the target variable. Give us a really low R-squared value.","metadata":{}},{"cell_type":"markdown","source":"# Problems with R-squared statistic\n* The R-squared statistic isn’t perfect. In fact, it suffers from a major flaw. Its value never decreases no matter the number of variables we add to our regression model.\n* That is, even if we are adding redundant variables to the data, the value of R-squared does not decrease.\n* It either remains the same or increases with the addition of new independent variables.\n* This clearly does not make sense because some of the independent variables might not be useful in determining the target variable. \n# Adjusted R-squared statistic\n* The Adjusted R-squared takes into account the number of independent variables used for predicting the target variable.\n* In doing so, we can determine whether adding new variables to the model actually increases the model fit.\n* if R-squared does not increase significantly on the addition of a new independent variable, then the value of Adjusted R-squared will actually decrease.\n* On the other hand, if on adding the new independent variable we see a significant increase in R-squared value, then the Adjusted R-squared value will also increas","metadata":{}},{"cell_type":"code","source":"\n# visualize the difference between the actual and predicted price \nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting the residual Values\nplt.scatter(y_pred, y_train-y_pred)\nplt.xlabel(\"Prediced price\")\nplt.ylabel(\"Residual values\")\nplt.title(\"Residual Vs Predicted Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* No pattern is observed as datas are equally distributed around the zero","metadata":{}},{"cell_type":"code","source":"# Checking for the Normality Errors\nsns.distplot(y_train-y_pred)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of residula\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Errors are normally distributed . Hence Normality Assumption is satisfied","metadata":{}},{"cell_type":"markdown","source":"# Test dataset ","metadata":{}},{"cell_type":"code","source":"# Predicting the Test data with model \ny_test_pred=lm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_acc=metrics.r2_score(y_test, y_test_pred)\nprint(\"R^2\",lin_acc)\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The model evaluation of the test data is almost same as the model evealuation of the training data , As the difference is very less.\n* Now We can say that the model is not overfitting","metadata":{}},{"cell_type":"markdown","source":"# Random Forest Regressor\n> As it is regression problem so we use regressor\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nreg=RandomForestRegressor()\n\nreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* bootstrap- Method of generating a new dataset form the orginal one (dim new < original dim) done by sampling with replacement. some of feature will be common to new datasets . it is called bootstrap samples\n* Entropy - Measure of randomness. Entropy can be defined as a measure of the purity of the sub split. Entropy always lies between 0 to 1.The algorithm calculates the entropy of each feature after every split and as the splitting continues on, it selects the best feature and starts splitting according to it\n* Gini Impurity- Gini impurity is also somewhat similar to the working of entropy. both are used for building the tree by splitting as per the appropriate features but there is quite a difference in the computation of both the methods. \n","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred=reg.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation for Random Forest\nprint(\"R^2\",metrics.r2_score(y_train, y_pred))\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the difference between the actual and predicted price \nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting the residual Values\nplt.scatter(y_pred, y_train-y_pred)\nplt.xlabel(\"Prediced price\")\nplt.ylabel(\"Residual values\")\nplt.title(\"Residual Vs Predicted Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for the Normality Errors\nsns.distplot(y_train-y_pred)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of residula\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Follow the Normality Assumption","metadata":{}},{"cell_type":"markdown","source":"# test Dataset","metadata":{}},{"cell_type":"code","source":"\n# Predicting the Test data with model \ny_test_pred=reg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_acc=metrics.r2_score(y_test, y_test_pred)\nprint(\"R^2\",rf_acc)\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the difference between the actual and predicted price \nplt.scatter(y_test, y_test_pred)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting the residual Values\nplt.scatter(y_test_pred, y_test-y_test_pred)\nplt.xlabel(\"Prediced price\")\nplt.ylabel(\"Residual values\")\nplt.title(\"Residual Vs Predicted Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for the Normality Errors\nsns.distplot(y_test-y_test_pred)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of residula\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The model is performing very good as train dataset . Hence again No overfitting","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxreg=XGBRegressor()\n\nxreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Prediction and Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred=xreg.predict(X_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation for Random Forest\nprint(\"R^2\",metrics.r2_score(y_train, y_pred))\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the difference between the actual and predicted price \nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting the residual Values\nplt.scatter(y_pred, y_train-y_pred)\nplt.xlabel(\"Prediced price\")\nplt.ylabel(\"Residual values\")\nplt.title(\"Residual Vs Predicted Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for the Normality Errors\nsns.distplot(y_train-y_pred)\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of residula\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For test Data- XGBoost","metadata":{}},{"cell_type":"code","source":"# Predicting the Test data with model \ny_test_pred=xreg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_acc=metrics.r2_score(y_test, y_test_pred)\nprint(\"R^2\",xgb_acc)\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM Regressor","metadata":{}},{"cell_type":"code","source":"#Spliting the dataset\nXX=data.drop(['Price'], axis=1)\nyy=data['Price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXX_train, XX_test, yy_train, yy_test=train_test_split(XX,yy, test_size=0.3, random_state=7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nXX_train=sc.fit_transform(XX_train)\nXX_test=sc.fit_transform(XX_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\n\nregg=svm.SVR()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regg.fit(XX_train,yy_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred=regg.predict(XX_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Evaluation for Random Forest\nprint(\"R^2\",metrics.r2_score(yy_train, y_pred))\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(yy_train, y_pred))*(len(yy_train)-1)/(len(yy_train)-XX_train.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(yy_train, y_pred))\nprint(\"MSE\", metrics.mean_squared_error(yy_train, y_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(yy_train, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the difference between the actual and predicted price \nplt.scatter(yy_train, y_pred)\nplt.xlabel(\"Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting the residual Values\nplt.scatter(y_pred, yy_train-y_pred)\nplt.xlabel(\"Prediced price\")\nplt.ylabel(\"Residual values\")\nplt.title(\"Residual Vs Predicted Prices\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For test Data","metadata":{}},{"cell_type":"code","source":"y_test_pred=regg.predict(XX_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_acc=metrics.r2_score(yy_test, y_test_pred)\nprint(\"R^2\",svm_acc)\nprint(\"Adusted R^2\", 1-(1-metrics.r2_score(yy_test, y_test_pred))*(len(yy_test)-1)/(len(yy_test)-XX_test.shape[1]-1))\nprint(\"MAE\", metrics.mean_absolute_error(yy_test, y_test_pred))\nprint(\"MSE\", metrics.mean_squared_error(yy_test, y_test_pred))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(yy_test, y_test_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Comparisoon of the all model","metadata":{}},{"cell_type":"code","source":"models=pd.DataFrame({\n    'Model':['Linear Regression', 'Random Forest', 'XGBoost', 'Support Vector Machine'],\n    'R_squared Score':[lin_acc*100, rf_acc*100, xgb_acc*100, svm_acc*100]\n})\nmodels.sort_values(by='R_squared Score', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion '\nAs we can se that \n* Least performing model is Linear Regression as form there we cam conclude that the dataset is not linear \n* Best performing model is XGBoost .XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n* XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n* Random forest is a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time.\n* It is also one of the most used algorithms, because of its simplicity and diversity \n* One of the biggest advantages of random forest is its versatility. It can be used for both regression and classification tasks\n* it’s also easy to view the relative importance it assigns to the input features.\n* Random forest is also a very handy algorithm because the default hyperparameters it uses often produce a good prediction result.\n\n","metadata":{}}]}