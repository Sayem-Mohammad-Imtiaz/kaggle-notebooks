{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0acf2adb-baa4-6efa-3392-58b4d9b573f9"},"source":"In this notebook,KNN algorithm are used on the voice.csv dataset which consists of of various features of voice:mean frequency etc. Here we will use feature_selection from sklearn to improve our learning dataset.\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59855808-788c-46be-e2b9-f4f7721f18fd"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"133ccbac-177a-73af-f120-91feb8b9f5bd"},"source":"Importing:\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed2d5732-2655-9f4d-d297-602508ffb9c5"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing, neighbors\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b21b40f4-84df-c100-a6d9-9fa63b6fccfb"},"outputs":[],"source":"# Reading and uploading the file\ndf = pd.read_csv('../input/voice.csv')\ndf.head(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e4a08dfc-dc7c-4371-28dd-fd55d10d10d9"},"source":"Visualizing the correlation among the features.\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3a15083-96c8-1134-4c97-815ae61336ce"},"outputs":[],"source":"corrmat=df.corr()\nsns.heatmap(corrmat,linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa48133a-1735-c4d4-51f2-28911dc8b883"},"outputs":[],"source":"# Name of the columns\ncol_names = list(df.columns.values)\nprint(col_names)\nprint (type(df.columns.values))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b483eca-41cf-2caf-0e81-4159fe1a9c63"},"outputs":[],"source":"df = df.rename(columns={'label': 'gender'})\ndf.columns.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17cad688-8873-5822-1ce2-04f2d5aa3474"},"outputs":[],"source":"#Lets use logistic Regression:\n\n#Producing X and y\nX = np.array(df.drop(['gender'], 1))\ny = np.array(df['gender'])\n\n#Dividing the data randomly into training and test set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nmodel=LogisticRegression()\nmodel.fit(X_train,y_train)\n\nprint('Accuracy1 :',model.score(X_train,y_train))\nprint('Accuracy2 :',model.score(X_test,y_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fb14366-99c3-c325-1e1f-fcd9d60ab41c"},"outputs":[],"source":"\n#KNN Classifier\n#Producing X and y\nX = np.array(df.drop(['gender'], 1))\ny = np.array(df['gender'])\n\n#Dividing the data randomly into training and test set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nmodel = neighbors.KNeighborsClassifier()\nmodel.fit(X_train, y_train)\n\naccuracy = model.score(X_test, y_test)\nprint('Accuracy='+str(accuracy))\n\n\n#The above was without any tuning ,now we will drop some columns which does not make any sense\n#We will drop col=median,mode,Q25,Q75,IQR.\n#next edit use only few=meanfreq,sd,median,gender(for no error)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a047ba9-b380-ea45-59a3-4279b8303d53"},"outputs":[],"source":"df1=df[['meanfreq','sd','median','meanfun','gender']]\nX = np.array(df1.drop(['gender'], 1))\ny = np.array(df1['gender'])\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nmodel = neighbors.KNeighborsClassifier()\nmodel.fit(X_train, y_train)\n\naccuracy2 = model.score(X_test, y_test)\nprint('Accuracy2='+str(accuracy2))\n\n#All the models should be above the base_line model:Base line model acc=50:50\n#But this is not very helpful,have to find new ways for k-nearest neibhors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c65d017d-2514-9310-3ef4-9c7daec5d9d3"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e970aa7-7eea-fd5b-05c1-47e9a0ddfda7"},"outputs":[],"source":"df2=df[['meanfreq','sd','meanfun','gender']]\nX = np.array(df2.drop(['gender'], 1))\ny = np.array(df2['gender'])\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nmodel = neighbors.KNeighborsClassifier()\nmodel.fit(X_train, y_train)\n\naccuracy2 = model.score(X_test, y_test)\nprint('Accuracy='+str(accuracy2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"646b22f4-f422-23ca-da3b-2b0e74e6e08c"},"outputs":[],"source":"#print(X_train.shape,y_train.shape,)\nprint(X_test.shape,y_test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f0dff59-5462-bc9d-4be2-a6015aff88c1"},"source":"Improving the model using feature_selection from sklearn\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f9f1deb-f9c1-2a82-ba90-42a6c024ee24"},"outputs":[],"source":"\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\ndef select_kbest_clf(data_frame, target, k=5):\n    \"\"\"\n    Selecting K-Best features for classification\n    :param data_frame: A pandas dataFrame with the training data\n    :param target: target variable name in DataFrame\n    :param k: desired number of features from the data\n    :returns feature_scores: scores for each feature in the data as \n    pandas DataFrame\n    \"\"\"\n    feat_selector = SelectKBest(f_classif, k=k)\n    _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n    \n    return feat_scores\nk=select_kbest_clf(df, 'gender', k=5).sort(['F Score'],ascending=False)\n\nk"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"603b55ef-c2df-d2ac-cd9a-4b8105d7a4bb"},"outputs":[],"source":"k1=sns.barplot(x=k['F Score'],y=k['Attribute'])\nk1.set_title('Feature Importance')"},{"cell_type":"markdown","metadata":{"_cell_guid":"91502612-86d6-4579-33ee-64cd4634b84f"},"source":"k-Nearest Neighbors\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa6172f7-8dc1-fce0-89ac-8e861c58b302"},"outputs":[],"source":"df3=df[['meanfun','IQR','Q25','sp.ent','sd','sfm','meanfreq','gender']]\nX = np.array(df3.drop(['gender'], 1))\ny = np.array(df3['gender'])\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nmodel = neighbors.KNeighborsClassifier()\nmodel.fit(X_train, y_train)\n\naccuracy3 = model.score(X_test, y_test)\nprint('Accuracy3='+str(accuracy3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc84bd8e-6fd7-c024-a499-d371bd856266"},"source":"Decision Tree with Boosting(AdaBoostClassifier)\n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f409bab5-b26f-c4d8-9c7d-773591be9b30"},"outputs":[],"source":"df.replace({'male':0,'female':1},inplace=True)\nX = np.array(df.drop(['gender'], 1))\ny = np.array(df['gender'])\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3df7346-2b8e-6613-1a60-7d225166a754"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier\n#DecisionTreeClassifier\ndt=DecisionTreeClassifier(max_depth=3,min_samples_leaf=int(0.5*len(X_train)))\nboosted_dt=AdaBoostClassifier(dt,algorithm='SAMME',n_estimators=800,learning_rate=0.5)\nboosted_dt.fit(X_train,y_train)\ny_predicted=boosted_dt.predict(X_test)\n\nprint (\"Area under ROC curve: %.4f\"%(roc_auc_score(y_test, y_predicted)))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"4a9f3a85-154e-c28d-8f29-62f049ab7d34"},"source":"Support Vector Machine \n======="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4fb7d47-f0fe-8d28-e4bf-5f6759565600"},"outputs":[],"source":"from sklearn import svm\nsvc = svm.SVC(kernel='rbf', C=1,gamma='auto').fit(X_train, y_train)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca1699ea-e780-b609-3ece-95f85fff3d1d"},"outputs":[],"source":"#To be continued"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}