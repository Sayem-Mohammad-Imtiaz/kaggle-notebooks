{"cells":[{"metadata":{"_uuid":"871e630d61a85c7383c0706d3a13d8b72229c35e"},"cell_type":"markdown","source":"**INTRODUCTION**\n* In this kernel, I've demonstrated the KNN Classification to a biomechanical dataset. As you can see, I didn't go into details; so just some visualizations, dataset normalization and finally the KNN Classification.\n* There are 2 csv files in our dataset, but I've just used one of them. \n* Each dataset contains 6 columns which are represeting patients' biomechanical features."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c12e2f69bc21273673cb33dbc17b7697785c2b"},"cell_type":"markdown","source":"**1) Deal With Data**\n* In this part, I've manipulated the data set, so that make things clear. I also plotted some graphs to easily understand the data."},{"metadata":{"trusted":true,"_uuid":"bc8afc0057d190e26f606c59046e7b77b2a9e15a"},"cell_type":"code","source":"data = pd.read_csv(\"../input/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc5d4701ddd2565a88aed45534cf2c421f70235e"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262637acb189ab47d7bdd34ecaef1ec2fc0fabfe"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc7a073560d2d9291a9eef359fb69296e7e05b1"},"cell_type":"code","source":"#Division of normal and abnormal parts to use them in plotting seperately\nAbnormal = data[data[\"class\"] == \"Abnormal\"]\nNormal = data[data[\"class\"] == \"Normal\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"831150cadf49b7835586541e5bbac8751eddc129"},"cell_type":"code","source":"#Size of each seperated subdata sets.\nsns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b176c14d96d46a07306af0ccc02f6d127707437"},"cell_type":"code","source":"#correlation map\n#just to find which features I should use.\nf,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=.8, fmt= '.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847d40a96418ef77387813f2ee768787c2eb844b"},"cell_type":"code","source":"#here is the visualization part. \nf,ax = plt.subplots(figsize=(12, 8))\nplt.scatter(Abnormal.pelvic_incidence, Abnormal.pelvic_radius, color=\"red\", alpha=0.5, label=\"Abnormal\")\nplt.scatter(Normal.pelvic_incidence, Normal.pelvic_radius, color=\"green\", alpha=0.8, label=\"Normal\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"pelvic_radius\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f48da2e276458c53e4ae4a95a03db288eecc49fb"},"cell_type":"markdown","source":"**2) Manipulate The Data**\n* In this part, I've applied some manipulation techniques to make ready my data set. (label convertion, dataset split,)"},{"metadata":{"trusted":true,"_uuid":"8e7843e042eb280ab76096884d59afef87ac4146"},"cell_type":"code","source":"#To make things easy, I've converted \"object\" types into \"integers\". 0 and 1\ndata[\"class\"] = [1 if i == \"Abnormal\" else 0 for i in data[\"class\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cafe3fb3c06e14fbdde63d5c9fa46b066b89b695"},"cell_type":"code","source":"#Determination of our label and input data.\ny = data['class'].values\nx_data = data.drop(['class'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e73bedf6c3a58ff7fc0eaec862dd81d727fe262"},"cell_type":"code","source":"#Normalization of the data. Normally I would use it but as I've seen that it reduces my test accuracy, I won't use it. \n#But use should run it to see what I'm trying to explain. \n#x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8ef93fccb80732e057270139423a98a7d025d1a"},"cell_type":"code","source":"#Simply dividing data into some percentages, like 30% for test and 70% for train\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbc13d994f607dabc7cc1b5f650ea5c96c48edc"},"cell_type":"markdown","source":"**3) K-Nearest Neighbour Classification**\n* I'll try to explain the KNN Classification basically:\n        According to KNN neighbour value (which determines how many distance comparisons will be made) in the function, the algorithm finds the closest point to our input point. After finding them, it checks their classification -whether they are let say man or women, good or bad, normal or abnormal- and our point's classification will be the dominant one. (For example: There are 3 woman points and 1 man points close to our input point. (3+1) = 4 is our neighbour value. Our point's classification will be no doubt \"woman\", since it is the dominant one). "},{"metadata":{"trusted":true,"_uuid":"57b167557e75d98a6745f28c6de5a8900e0ad186"},"cell_type":"code","source":"#Here is the KNN part. \nfrom sklearn.neighbors import KNeighborsClassifier\nneighbors = 13\nknn = KNeighborsClassifier(n_neighbors=neighbors)\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)\nprint(\"score for {} neighbors is: {}\".format(neighbors, knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d141e25cad77e9aa7905759d0c0167ac9f60f0ce"},"cell_type":"code","source":"#To find the best neighbour value for our KNN, we use for loop and simply try each neighbour values.\ntrain = []\ntest = []\nfor i in range(1,15):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(x_train,y_train)\n    test.append(knn.score(x_test,y_test))\n    train.append(knn.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63d562ed2e83db7f077fc4cb60235f99304bab4e"},"cell_type":"code","source":"#After obtaining the result, we are just gonna plot them to see the result clearly.\nf,ax = plt.subplots(figsize=(12, 8))\nplt.plot(range(1,15),train, color=\"red\", label=\"train\")\nplt.plot(range(1,15),test, color=\"green\", label=\"test\")\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42838a2c9136a2b8a6260948767b35aafc6604d8"},"cell_type":"markdown","source":"**CONCLUSION**\n* Thats it for this kernal. If you have any question, or if you saw any logical problem(s) in my code, don't hesitate to comment them to me. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}