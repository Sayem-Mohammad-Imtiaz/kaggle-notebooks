{"cells":[{"metadata":{"_uuid":"399faa409e382be0ea1ea73b2a8c31e1aa9b4d66"},"cell_type":"markdown","source":"**INTRODUCTION **\n* In this kernel, what I'm going to demonstrate is \"Logistic Regression\"\n* Using logistic regression, we can create our own model for the situations that have two results which makes it binary (0 and 1).\n* At first, I'm going to have some idea about our dataset which is about identification of voice gender. As we have two genders (results), I'll convert them to integer values like 0 or 1. \n* So what we do in here is just the long way of logistic regression, but since our purpose is to learn it from basics, I've decided this way."},{"metadata":{"_uuid":"53475eda71857c94a6b7fe1b3f4007ce0bd3a69a"},"cell_type":"markdown","source":"Below chart is the basics of this dataset's voice determination system. As you can see, its's like decision tree."},{"metadata":{"_uuid":"5c5aac5aae6168b8506af0dbd7a50f218c19e84b"},"cell_type":"markdown","source":"![](http://www.primaryobjects.com/images/gender/voice-plot-1.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for visualization\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ade1bd64969aeef91c073c3e469cd4db3f243d7"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02c541d65c8918cdcf57f5384eb1417a3f44651c"},"cell_type":"code","source":"#Just having some information about our dataset\n\n# import data, see feature names, label count and data info\nprint(df.columns)\nlabel_value_count = df.label.value_counts()\nprint(label_value_count)\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2508d6bf8222ff301ee38d0c29f2a4b323fce7c7"},"cell_type":"markdown","source":"**1. Visualization**"},{"metadata":{"trusted":true,"_uuid":"b21bef27c9a12f9b5467870e6da8b2f891face07"},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(df.corr(), annot=True, linewidths=.8, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d1623bc1cd8de0ea887f4d33486e0d4d6ba4f08"},"cell_type":"code","source":"sns.jointplot(df['meanfreq'], df['sfm'], kind=\"regg\", color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb67e5f87d67beeed4e94f23a5816841cb717576"},"cell_type":"markdown","source":"* In this part of the kernel below, I've tried to find average values for each column in \"male\" and \"female\" labels. "},{"metadata":{"trusted":true,"_uuid":"6f85a725a42904ac52c02b9a9ba32df75d610aca"},"cell_type":"code","source":"male = []\nfemale = []\nfor i in range(0,3167):\n    for j in range(0,20):\n        if df.iloc[i,20] == \"male\":\n            male.append([df.iloc[i,j]])\n        elif df.iloc[i,20] == \"female\":\n            female.append([df.iloc[i,j]])      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"966456f6e760a1802b8f22282c3c6788c541f48e"},"cell_type":"code","source":"sum_male = 0\nsize_male = len(male) #1584\nfor i in range(0,size_male,20):\n    sum_male = np.add(sum_male,male[i:i+20])\n\nsum_female =0\nsize_female = len(female) #1583\nfor i in range(0,size_female,20):\n    sum_female = np.add(sum_female,female[i:i+20])\n\nsum_male = sum_male/1584\nsum_female = sum_female/1583","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b538c70f5dcbd1c29683c0dc1f62ca1c8fa0995"},"cell_type":"code","source":"data = {\"Male\":[sum_male],\"Female\":[sum_female]}\ndata = pd.DataFrame(data)\ndata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34745e680cece5c5270c156d20b6c17a406477e9"},"cell_type":"markdown","source":"**2. Logistic Regression Brief Look**"},{"metadata":{"_uuid":"2ce6512f7c3d2cc922a4ce4821bf09890734d539"},"cell_type":"markdown","source":"Basicly there are 2 steps for logistic regression. These are Forward and Backward Propagations. From left to right, we call it \"Forward Propagation\", and from right to left we call it \"Backward Propagation\". At first we multiply our each feature with its weight value and add a bias value to it. After doing that, we obtain the 'z' and we put it to a function called \"Sigmoid function\". After this process, we simply get the label value but it's not that easy. We do these steps until our cost value -which is obtained by the sum of all loss values -is close to zero. In order to update our weight and bias, we apply backward propagation steps. From there, we just take the derivative of cost function for each step and substract it from the current bias and weight values. I didn't go through the details, for a close look to logistic regression you can look: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners"},{"metadata":{"_uuid":"829f4d5b22e38910a1542153550a66b7a1c05181"},"cell_type":"markdown","source":"![](https://image.ibb.co/jYevxc/5.jpg)\n"},{"metadata":{"_uuid":"eec818b05f193aed214e90773dade3961eb05cf8"},"cell_type":"markdown","source":"**2.1 Logistic Regression Part (Long Method)**"},{"metadata":{"trusted":true,"_uuid":"d16fea017a4a83d5355f678698057df6ed6b31d2"},"cell_type":"code","source":"#changing \"male\" and \"female\" names by '1' and '0' \ndf.label = [1 if i == \"male\" else 0 for i in df.label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b32d86bd225472b5ee1a2f9c1e490b8dc884b075"},"cell_type":"code","source":"y = df.label.values   #we store the labels in y\nx_data = df.drop([\"label\"], axis=1)  #we put everything except label into x_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d1bd43c9e3ded4dc4a5a6a199faa48173fd9211"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))  #normalization of the data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf0491ec93994fe2324a4b3db4e77f08660819b"},"cell_type":"code","source":"#we divide our data into some percentages that we desired, like %20 for test and %80 for train\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f93441fdd049e47f8e18b5c0fbca5bf3085781"},"cell_type":"code","source":"#since we apply matrix multiplication, we take transforms of our matrices\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbdf81acc516a67707a545e55528d13118789728"},"cell_type":"code","source":"print(\"x_train shape: \", x_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"969896576ef6074dab761983d977472f1da3089c"},"cell_type":"code","source":"#We initialize the weightt and bias values by default\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1), 0.01) #'w' is a vector which has the same dimension with our features\n    b = 0.0\n    \n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62cedbd72969c26975b9d11171c8eb730ca76bdd"},"cell_type":"code","source":"#we define our sigmoid function which gives us the resulting labels.\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    \n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deefef915d1acc59e1bb968e11ce9a78e93e5a42"},"cell_type":"code","source":"#as we can see from the picture at the beginning, we do the forward propagation first and,\n#we apply backward propagation steps to get the parameters for updating the weight and bias values.\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13a71191c7ba58d25d76ced4fc8c8092c1f8a8c3"},"cell_type":"code","source":"#we use the resulting values in the previous funtion to update our 'w' and 'b'\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c36cc502a2051d91cf8c0ae4f89cdf7007eefea3"},"cell_type":"code","source":"#Here is the prediction part, we determine whether the result should be 1 or 0 according to our 'w' and 'bias' values\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc81f4a45eae1e2d3f0319d4765c94e7406b43a6"},"cell_type":"code","source":"#This is the final part for logistic regression. \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd45213d9d05ba1a96f1526763cd8bfb62558b8"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1.5, num_iterations = 501)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1582de0afe901595ab0761632dde95d364ccd4b1"},"cell_type":"markdown","source":"**2.2 Logistic Regression (Short Method)**"},{"metadata":{"trusted":true,"_uuid":"f544251dbef491df96dfa3d1c0c54e8caf81bf3f"},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(logistic_regression.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41039c481bb27bfafbf7de0f7523a1f49a373abd"},"cell_type":"markdown","source":"**CONCLUSION**"},{"metadata":{"_uuid":"d02edc8efdc740c75bbece004bc8661c5161fff8"},"cell_type":"markdown","source":"* Long and Short logistic regression methods' results are almost close to each other. But of course the sklearn library applys some other high accurated parameters in it. That is why the two result are not exactly the same. \n* From the dataset, what we've learnt that there are some features for men and women voices. In terms of our dataset features, we've created a basic machine learning algorithm by just using logistic regression method to distinguish men and women voices. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}