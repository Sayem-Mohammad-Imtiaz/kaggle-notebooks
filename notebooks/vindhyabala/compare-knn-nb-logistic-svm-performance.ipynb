{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Jun 28 21:56:27 2021\n\n@author: Vindhya\nCalculate accuracy of stock data using logistic\n\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn . linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn . preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom tabulate import tabulate\nfrom sklearn . neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n##set filenames\ninput_dir = '../input/pima-indians-diabetes-database'\n\n#root_dir = os.getcwd()\n\ndiabetes_file ='diabetes'\n#details_file = 'BAC_weekly_return_volatility_detailed'\ndiabetes_path_file = os.path.join(input_dir, diabetes_file+'.csv')\n#details_path_file = os.path.join(input_dir,details_file+'.csv')\n\n\ntry:\n   ## Load the data\n   diabetes_orig_df = pd.read_csv(diabetes_path_file)\n   print(\"\\nSuccessfully read the dataset\")        \n   print(tabulate(diabetes_orig_df.head(1),headers = 'keys', tablefmt = \"fancy_grid\"))\nexcept Exception as e:\n  print(e)\n  print('Failed to read input files', diabetes_file)\n  \n\n## copy df\ndiabetes_df = diabetes_orig_df.copy()\n\nprint(\"\\nData prep begins\")\n## Drop null rows\nprint(\"\\n\\tDropping null rows\")\ndiabetes_df.dropna(how=\"all\", inplace=True) \n\n## check if class variable distribution is balanced or unbalanced\nprint(\"\\n\\tFrom the plot, we see that the class distribution is balanced.\")\nsns.countplot(diabetes_df['Outcome'])\nplt.title(\"Class label distribution\")\n\n\n\ncol_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness','Insulin','DiabetesPedigreeFunction','Age']\n\n## class distribution by feature\nprint(\"\\n\\tGenerating plots to see how features measure up against the class atttribute\")\nplt.figure(figsize=(15, 15))\nplt.title(\"Class Distribution of Feature\")\nfor i, column in enumerate(col_names, 1):\n    plt.subplot(3, 3, i)\n    diabetes_df[diabetes_df[\"Outcome\"] == 0][column].hist(bins=35, color='blue', label='Not Diabetic', alpha=0.6)\n    diabetes_df[diabetes_df[\"Outcome\"] == 1][column].hist(bins=35, color='red', label='Diabetic', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\n    plt.suptitle('Analysis of Features vs Class')\n\nprint(\"\\n\\t From the plot, we see that the class labels are well distributed across all features\")\n\n## Pair plot of diabetes dataset\nprint(\"\\n\\tGenerating pairplot for all features\")\nsns.pairplot(diabetes_df, hue = 'Outcome', vars = col_names)\nplt.title(\"Pairplot of all features in Pima dataset\")\nplt.show()\n\n## determine correlation \nprint(\"\\n\\tCorrelation among features is plotted\")\nmask = np.zeros_like(diabetes_df.corr(), dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(diabetes_df.corr(), mask = mask,annot = True, cmap ='coolwarm', linewidths=2)\nplt.title(\"Correlation Analysis of diabetes data\")\nplt.show()\n\n## get stats of all features\nprint(\"\\n\\tPrinting stats of the dataset\")\nprint(tabulate(diabetes_df.describe(),headers = \"keys\",tablefmt = \"fancygrid\"))\n## boxplot to describe features\nax=sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(diabetes_df))\nplt.setp(ax.get_xticklabels(), rotation=45)\nplt.title(\"PIMA dataset stats\")\nplt.show()\n\n## describe df stats\ndiabetes_df.describe()\nprint(\"\\n\\tFrom the box plot,one can see that there are a few features that have 0 values which means that there is some error in capturing those attributes.\\n\\tCalculating the % of records with 0 values in the dataset.\")\n\nfor col in [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"BMI\", \"Insulin\"]:\n    print(\"{}: {}\".format(col, round((diabetes_df[col].value_counts()[0]/diabetes_df.shape[0])*100,2 )))\n    \nprint(\"\\n\\tAs the % of 0 value records is negligible for Glucose, BP and BMI, it makes sense to drop these records from further processing.\")\n\ndiabetes_df = diabetes_df[ (diabetes_df[\"Glucose\"]!=0) & (diabetes_df[\"BloodPressure\"]!=0) & (diabetes_df[\"BMI\"]!=0)]\n\nprint(\"\\n\\tAs over 30% of dataset doesn't have info on Insulin and SkinThickness, it makes sense to impute the missing records. Used the approach of imputing the records with mean value for Insulin and linear regression for SkinThickness.\")    \n\n# create df with 0 and non 0 values for SkinThickness\nthicknessnot0_df = diabetes_df[diabetes_df[\"SkinThickness\"]!=0]\nthickness0_df = diabetes_df[diabetes_df[\"SkinThickness\"]==0]\n    \n## Use Linear Regression for calculating 0 values\nlinreg = LinearRegression()\nlinreg.fit(thicknessnot0_df.drop([\"SkinThickness\", \"Outcome\"], axis=1), thicknessnot0_df[\"SkinThickness\"])\n## Predict SkinThickness for 0 value records using fit from above\nthickness0_df[\"SkinThickness\"] = linreg.predict(thickness0_df.drop([\"SkinThickness\",\"Outcome\"], axis=1))\n### Merge the imputed datas, then check\ndiabetes_df = thicknessnot0_df.append(thickness0_df)\ndiabetes_df.describe()\ndiabetes_df.boxplot()\nplt.show()\n\n## Fill 0 values for Insulin with mean\ndiabetes_df['Insulin']=diabetes_df['Insulin'].replace(0,diabetes_df['Insulin'].mean())\n\n## Calculate knn accuracy\ndef calc_knn_acc(xtrain,ytrain,xtest,ytest,n):\n    knn_classifier = KNeighborsClassifier (n_neighbors =n)\n    knn_classifier.fit ( xtrain,np.ravel(ytrain ))\n    knn_pred = knn_classifier.predict (xtest )\n    print(\"Classification report for {}:\\n{}\".format(knn_classifier, classification_report(ytest, knn_pred)))\n    return round(accuracy_score(ytest,knn_pred)*100,2)\n\n## Calculate logistic accuracy\ndef calc_log_acc(xtrain,ytrain,xtest,ytest):\n    log_reg_clf = LogisticRegression ()\n    log_reg_clf.fit (xtrain,np.ravel(ytrain))\n    log_pred = log_reg_clf.predict(xtest)\n    print(\"Classification report for {}:\\n{}\".format(log_reg_clf, classification_report(ytest, log_pred)))\n    return round(accuracy_score(ytest,log_pred)*100,2) \n\n## Calculate NB accuracy\ndef calc_NB_acc(xtrain,ytrain,xtest,ytest):\n    gnb = GaussianNB()\n    NB_classifier = gnb.fit(xtrain, np.ravel(ytrain))\n    NB_pred = NB_classifier.predict(xtest)\n    print(\"Classification report for {}:\\n{}\".format(gnb, classification_report(ytest, NB_pred)))\n    return round(accuracy_score(ytest,NB_pred)*100,2)\n\n## Calculate SVM accuracy\ndef calc_SVM_acc(xtrain,ytrain,xtest,ytest):\n    svm_clf = svm.SVC(kernel='linear')\n    svm_clf.fit (xtrain,np.ravel(ytrain))\n    svm_pred = svm_clf.predict(xtest)\n    print(\"Classification report for {}:\\n{}\".format(svm_clf, classification_report(ytest, svm_pred)))\n    return round(accuracy_score(Y_test,svm_pred)*100,2)  \n\n## split dataframe into train and test\nprint(\"\\nRun different classifiers and compare accuracy\")\nprint(\"\\n\\tSplit the data 50-50 for training and testing\")\nx = diabetes_df.drop(['Outcome'],axis = 1)\ny = diabetes_df['Outcome']\nX_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.5,random_state =5)\n\n## Scale the data\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nscaler = StandardScaler().fit(X_test)\nX_test_scaled = scaler.transform(X_test)\n\n## For knn, determine best hyperparameter\nk_dict = {}\nfor k in range(3,19,2):\n    k_dict[k] = calc_knn_acc(X_train_scaled, Y_train, X_test_scaled, Y_test, k)\n\n## Accuracy grid    \nprint('\\n\\tKNN Classifier - accuracy rates for k = 3,5,7,9,11,13,15,17  ')\nknn_accuracy_df = pd.DataFrame(k_dict,index = ['Accuracy'])\nknn_accuracy_df = knn_accuracy_df.transpose()\nprint(tabulate(knn_accuracy_df.T, headers=\"keys\", tablefmt=\"fancy_grid\")) \nprint('\\n\\t\\tMost optimal k value for 2019 stock data is 15 as it seems to have the best accuracy.')\n\n## Accuracy plot for k values\nprint(\"\\n\\tAccuracy plot for diffferent k values\")\nplt.figure( figsize =(10 ,4))\nax = plt. gca ()\nax. xaxis.set_major_locator(plt.MaxNLocator( integer = True ))\nplt.plot ([3,5,7,9,11,13,15,17],knn_accuracy_df, color ='blue')\nplt.title ('Accuracy vs. k')\nplt.xlabel ('Number of Neighbors : k')\nplt.ylabel ('Accuracy ')\nplt.show()\n\nbest_k = knn_accuracy_df['Accuracy'].idxmax()\n\n## Calculate accuracy using all features in dataset across multiple classifiers\n\n# declare dict to store accuracies\nall_accuracy_dict = {}\nprint(\"\\n\\tAccuracy rates for different classifiers\")\n\nall_accuracy_dict['KNN'] = calc_knn_acc(X_train_scaled,Y_train,X_test_scaled,Y_test,best_k) \n\nall_accuracy_dict['Log'] = calc_log_acc(X_train_scaled,Y_train,X_test_scaled,Y_test)\n\nall_accuracy_dict['NB'] = calc_NB_acc(X_train_scaled,Y_train,X_test_scaled,Y_test)\n\nall_accuracy_dict['SVM'] = calc_SVM_acc(X_train_scaled,Y_train,X_test_scaled,Y_test)\n\nall_accuracy_df = pd.DataFrame(all_accuracy_dict,index=['All Features'])\n\nprint(tabulate(all_accuracy_df, headers = \"keys\", tablefmt = \"fancy_grid\"))\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nclassifiers_list = ['Knn','Log','NB','SVM']\nax.bar(classifiers_list,all_accuracy_df.transpose()['All Features'])\nplt.title(\"Accuracy rates for classifiers\")\nplt.show()\nprint(\"From the plot and the grid, we see that the highest accuracy rates are observed for SVM classifier.\")\n\n## calculate accuracy by dropping features\nprint(\"\\nDetermine each feature's contribution to accuracy\")\ndef calc_acc_dropped_features(log_all_accuracy,knn_all_accuracy,NB_all_accuracy,SVM_all_accuracy):\n    log_dict = {}\n    knn_dict = {}\n    nb_dict = {}\n    svm_dict = {}\n    for col in col_names:\n        ## create new train and test dataset with dropped feature\n        dropped_feature =  col_names.copy()\n        dropped_feature.remove(col)\n        x_train_dropped =  X_train[dropped_feature].values\n        x_test_dropped =  X_test[dropped_feature].values\n        scaler = StandardScaler().fit(x_train_dropped)\n        x_train_dropped = scaler.transform(x_train_dropped)\n        scaler = StandardScaler().fit(x_test_dropped)\n        x_test_dropped = scaler.transform(x_test_dropped)\n        # run logistic on dropped feature dataset \n        log_dict[col] = log_all_accuracy - calc_log_acc(x_train_dropped,Y_train,x_test_dropped,Y_test)\n        # run knn on dropped feature dataset\n        knn_dict[col] = knn_all_accuracy -calc_knn_acc(x_train_dropped,Y_train,x_test_dropped,Y_test,best_k)\n        # run NB on dropped feature dataset\n        nb_dict[col] = NB_all_accuracy - calc_NB_acc(x_train_dropped,Y_train,x_test_dropped,Y_test)\n        # run SVM on dropped feature dataset\n        svm_dict[col] = SVM_all_accuracy - calc_SVM_acc(x_train_dropped,Y_train,x_test_dropped,Y_test)\n  \n    dropped_knn_df = pd.DataFrame(knn_dict,index = ['KNN']).transpose()\n    dropped_log_df = pd.DataFrame(log_dict,index = ['Log']).transpose()\n    dropped_NB_df = pd.DataFrame(nb_dict,index = ['NB']).transpose()\n    dropped_SVM_df = pd.DataFrame(svm_dict,index = ['SVM']).transpose()\n    drop_df = dropped_knn_df.join(dropped_log_df,how=\"inner\")\n    drop_df = drop_df.join(dropped_NB_df,how =\"inner\")\n    drop_df = drop_df.join(dropped_SVM_df,how=\"inner\")\n    frames = [all_accuracy_df,drop_df]\n    drop_df = pd.concat(frames)\n    return drop_df\n   \ndropped_df = calc_acc_dropped_features(all_accuracy_df.loc['All Features','Log'], all_accuracy_df.loc['All Features','KNN'],  all_accuracy_df.loc['All Features','NB'], all_accuracy_df.loc['All Features','SVM'])\n\n## print results of feature contribution analysis\nprint(\"\\n\\tFeature contribution to accuracy for each classifier\")\nprint(tabulate(dropped_df,headers = \"keys\",tablefmt = \"fancy_grid\"))\nprint(\"\\n\\tFrom the above grid, one can conclude that Glucose is the most significant contributor towards the accuracy rates of a classifier.\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T18:33:10.918983Z","iopub.execute_input":"2021-08-13T18:33:10.919329Z","iopub.status.idle":"2021-08-13T18:33:28.297731Z","shell.execute_reply.started":"2021-08-13T18:33:10.919292Z","shell.execute_reply":"2021-08-13T18:33:28.296709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}