{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Length of Stay Prediction\n\nTask is to predict Length of Stay (LOS) Category from the below data. Increased importance to accurately those patients with higher LOS.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras as ks\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (7, 6)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/av-healthcare-analytics-ii/healthcare/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\nUsing the training dataset to evaluate the available features and their correlation with the length of stay feature, which will be the predictor for this task. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training dataset\ntrain = pd.read_csv(\"train_data.csv\")\n\nprint(\"Available Features : {}\".format(train.columns))\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Stay\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting certain features to categorical variables for ease of analysis\n\ncat_cols = list(set(list(train.columns)) - set(['Available Extra Rooms in Hospital', 'Visitors with Patient', 'Admission_Deposit']))\nordered_cols = ['Bed Grade', 'Age', 'Stay', \"Severity of Illness\"]\nstay_order = ['0-10', '11-20', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90','91-100', 'More than 100 Days']\ny_feature = \"Stay\"\n\nfor c in cat_cols:\n    if c in ordered_cols:\n        if c in ordered_cols[-2:]:\n            if c == \"Age\":\n                # Age \n                train[c] = pd.Categorical(train[c], ordered=True, categories=stay_order[:-1])\n            elif c == \"Stay\":\n                # Stay\n                train[c] = pd.Categorical(train[c], ordered=True, categories=stay_order)\n            elif c == \"Severity of Illness\":\n                train[c] = pd.Categorical(train[c], ordered=True, categories=[\"Minor\", \"Moderate\", \"Extreme\"])\n        else:\n            # Bed Grade\n            train[c] = pd.Categorical(train[c], ordered=True)\n    else:\n        train[c] = pd.Categorical(train[c])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly shows that there relatively few examples in the training set with high LOS (61 + days). Therefore, we might need to consider aggregating the classes in the higher LOS for improved representation of the higher LOS classes. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class Distribution\n\nsns.countplot(train[\"Stay\"])\n\nplt.title(\"Class Distribution of Training Dataset (n = {})\".format(len(train)))\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial thoughts on what could correlate with high LOS from features present in dataset:\n* Severity of Illness\n* Type of Admission\n* Age\n* Department \n* Visitors with Patient (?)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Severity of Illness\n\nHere, it is apparent that the levels of extreme and minor severities of illnesses do change with a change in length of stay categories. \n\nFor longer LOS, we see that typically they have noticeably lower proportions of minor and higher extreme illnesses than shorter LOS. Therefore, this feature would be particularly helpful in this task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Severity of Illness\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Severity of Illness feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Severity of Illness\", \"Stay\"]).size()\nc = (c/c.groupby(level=1).sum()).reset_index()\n\nsns.lineplot(x = \"Stay\", y = 0, hue = \"Severity of Illness\", data = c, ax = ax2)\n\nax2.set_title(\"\"\"Proportion of Length of Stay Category by Severity of Illness\n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Proportion of Length of Stay Category\")\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Type of Admission\n\nFrom the above, it appears that emergency and trauma admissions could be particularly to disseminate shorter LOS (e.g. 0 - 20 days) from longer LOS (21 + days). We also see that the proportion of urgent admissions decreasing over the LOS, which could also be potentially useful in the model. \nOverall, it is not yet clear how effective this feature will be to add in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Type of Admission\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Type of Admission feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Type of Admission\", \"Stay\"]).size()\nc = (c/c.groupby(level=1).sum()).reset_index()\n\nsns.lineplot(x = \"Stay\", y = 0, hue = \"Type of Admission\", data = c, ax = ax2)\n\nax2.set_title(\"\"\"Proportion of Length of Stay Category by Type of Admission Category\n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Proportion of Type of Admission Category\")\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age\n\nAge does seem to be a relatively helpful feature in distinguishing LOS. This could be due to the lack of representation from minority age groups (e.g. 0-10, and 91-100). However, we do see that older patients tend to skew toward longer LOS than younger patients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Age\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Age feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Age\", \"Stay\"]).size()\nc = (c/c.groupby(level=0).sum()).reset_index()\nc = c.values[:,-1].reshape(c.Age.unique().size, c[\"Stay\"].unique().size).astype(float)\n\nsns.heatmap(c, ax = ax2, cmap = \"Blues\")\n\nax2.set_title(\"\"\"Proportion of Age Category by Length of Stay Category\n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Age Category\")\nax2.set_yticklabels(stay_order[:-1], rotation = 0)\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Department\n\nHere we see again that there is no noticeable change in distribution of cases by Department over LOS. We do see a marginal difference in surgery department's distribution of cases, whereby it is slightly more skewed to more longer LOS than other departments. This could be confounded by lack of surgery entries. Maybe we could aggregate TB, anesthesia, gynecology and radiotherapy together into a super category and leave surgery as separate category?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Department\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Department feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Department\", \"Stay\"]).size()\nc = (c/c.groupby(level=0).sum()).reset_index()\nyticks = c.Department.unique()\nc = c.values[:,-1].reshape(c.Department.unique().size, c[\"Stay\"].unique().size).astype(float)\n\nsns.heatmap(c, ax = ax2, cmap = \"Blues\")\n\nax2.set_title(\"\"\"Proportion of Department Category by Length of Stay Category \n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Department\")\nax2.set_yticklabels(yticks, rotation = 0)\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visitors with Patient\n\nHere, we can see a relatively vague increase in median visitors with patients as LOS becomes longer. This could be a useful feature for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.distplot(train[\"Visitors with Patient\"], kde = False, ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Visitors with Patient feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nsns.boxplot(x = \"Stay\", y = \"Visitors with Patient\", data = train, ax = ax2)\n\nax2.set_title(\"\"\"Distribution of Visitors with Patient by Length of Stay Category\n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Visitors with Patient\")\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hospital Code\n\nHospital code does show different distributions of LOS, which proves to be relatively useful to classify LOS in combination with other features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Hospital_code\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Hospital code feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Hospital_code\", \"Stay\"]).size()\nc = (c/c.groupby(level=0).sum()).reset_index()\nyticks = c.Hospital_code.unique()\nc = c.values[:,-1].reshape(c.Hospital_code.unique().size, c.Stay.unique().size).astype(float)\n\nsns.heatmap(c, ax = ax2, cmap = \"Blues\")\n\nax2.set_title(\"\"\"Proportion of Hospital code Category by Length of Stay Category \n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Hospital code\")\nax2.set_yticklabels((np.arange(yticks.size) * 2) + 1, rotation = 0)\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ward Type\n\nIt seems that there are discrepancies in distribution of proportions of Ward Type by LOS. For example, those patients admitted to a U ward have extremely short LOS, whilst those admitted to S or T wards tend to have longer LOS. U distribution could be confounded by the fact there are very few U ward-type entries (9).\n\nMight be worth concatenating ward types into the following super-categories:\n1. P, Q, R, U\n2. S, T","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"Ward_Type\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of Ward Type feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"Ward_Type\", \"Stay\"]).size()\nc = (c/c.groupby(level=0).sum()).reset_index()\nyticks = c.Ward_Type.unique()\nc = c.values[:,-1].reshape(c.Ward_Type.unique().size, c.Stay.unique().size).astype(float)\n\nsns.heatmap(c, ax = ax2, cmap = \"Blues\")\n\nax2.set_title(\"\"\"Proportion of Ward Type by Length of Stay Category \n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"Ward_Type\")\nax2.set_yticklabels(yticks, rotation = 0)\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Updating Dataset as per EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_features = [\"Severity of Illness\", \"Age\", \"Type of Admission\", \"New Ward_Type\",\n                  \"Hospital_code\", \"Department\", \"Visitors with Patient\"]\n\nordered_cats = input_features[0:2]\ncats = input_features[2:-1]\nnum = [input_features[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating Ward types as explained in EDA\n\ntrain.loc[:, \"New Ward_Type\"] = train.loc[:, \"Ward_Type\"].astype(str)\ncombine_cats = {\"PQRU\": [\"P\", \"Q\", \"R\", \"U\"], \"ST\": [\"S\", \"T\"]}\n\nfor k in combine_cats:\n    _idxs = train[train[\"Ward_Type\"].isin(combine_cats[k])].index.values\n    train.at[_idxs, \"New Ward_Type\"] = k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14,6))\n\n# ax1 -------\nsns.countplot(train[\"New Ward_Type\"], ax = ax1)\n\nax1.set_title(\"\"\"Distribution of New Ward Type feature in training dataset\n(n = {})\"\"\".format(len(train)))\nax1.set_ylabel(\"Frequency\")\n\n# ax2 -------\nc = train.groupby([\"New Ward_Type\", \"Stay\"]).size()\nc = (c/c.groupby(level=0).sum()).reset_index()\nyticks = c[\"New Ward_Type\"].unique()\nc = c.values[:,-1].reshape(c[\"New Ward_Type\"].unique().size, c.Stay.unique().size).astype(float)\n\nsns.heatmap(c, ax = ax2, cmap = \"Blues\")\n\nax2.set_title(\"\"\"Proportion of New Ward Type by Length of Stay Category \n(n = {})\"\"\".format(len(train)))\nax2.set_ylabel(\"New Ward_Type\")\nax2.set_yticklabels(yticks, rotation = 0)\nax2.set_xticklabels(stay_order, rotation = 45)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.loc[:, input_features + [y_feature]]\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def balance_classes(data, y):\n    \"\"\"\n    Balances class in dataset\n    \n    :param data: (Pandas DataFrame) dataset to balance\n    :param y: (String) class column name\n    \"\"\"\n    d = pd.DataFrame(data[y].value_counts())\n    max_class = d.idxmax(axis = 0).values[0]\n    max_class_count = d.loc[max_class][0]\n\n    new_data = data[data[y] == max_class]\n\n    for c in list(set(data[y].unique()) - set([max_class])):\n        \n        try:\n            c_idxs = data[data[y] == c].index.values\n            c_idxs = np.random.choice(c_idxs, max_class_count)\n            new_data = pd.concat([new_data, data.loc[c_idxs,:]], ignore_index = True)\n        except Exception as e:\n            print(e)\n            pass\n        \n    return new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = balance_classes(X_train, \"Stay\")\nX_train.Stay.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical \nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_dataset():\n\n    # Convert categorical variables to indices\n    X_trn = None\n    m = MinMaxScaler()\n\n    for c in X_train.columns:\n        print(c)\n\n        if c in cats or c in ordered_cats or c == y_feature:\n            _ = pd.factorize(X_train[c], sort=True)[0]\n            if c in cats:\n                _ = to_categorical(_, num_classes=X_train[c].unique().size)\n            elif c in ordered_cats:\n                _ = _/np.max(_)\n        else:\n            #print(c)\n            _ = m.fit_transform(X_train[c].values.reshape(-1,1))[:, 0]\n\n        try:\n            print(\"Xtrn: \", X_trn.shape) \n        except:\n            pass\n        print(\"_: \",_.shape)\n\n        try:\n            if len(_.shape) == 1:\n                    print(\"1D\")\n                    X_trn = np.hstack((X_trn, _.reshape(-1,1)))\n            else:\n                X_trn = np.hstack((X_trn, _))  \n        except Exception as e:\n            print(\"Error\")\n            if len(_.shape) == 1:\n                    print(\"1D\")\n                    X_trn = _.reshape(-1,1)\n            else:\n                X_trn = _\n\n    X_trn = X_trn\n\n\n    y_trn = X_trn[:, -1]\n    X_trn = X_trn[:, :-1]\n\n    # adding Visitors Number ^ 2 to add another feature\n    #X_trn = np.vstack((X_trn.T, m.fit_transform(((X_train[\"Visitors with Patient\"]**2).values).reshape(-1,1))[:, 0])).T\n\n    print(X_trn.shape)\n    \n    y_trn = to_categorical(y_trn, num_classes=train.Stay.unique().size)\n    \n    X_trn, X_val, y_trn, y_val = train_test_split(X_trn, y_trn, test_size=0.3)\n    \n    return X_trn, X_val, y_trn, y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trn, X_val, y_trn, y_val = prepare_dataset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Archtitectures","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"METRICS = [ \n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n    tf.keras.metrics.AUC(name='auc'),\n    tf.keras.metrics.Accuracy(name='acc')\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline = tf.keras.Sequential([\n    layers.Dense(input_dim = X_trn.shape[1], units = 128, activation = \"relu\"),\n    layers.Dense(128, activation = \"relu\"),\n    layers.Dense(train.Stay.unique().size, activation = \"softmax\")\n], name = \"baseline\")\n\nbaseline.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=METRICS)\n\nbaseline.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs_history = baseline.fit(X_trn, y_trn,\n                          epochs=20,\n                          batch_size = 700,\n                          validation_data=(X_val, y_val),\n                          validation_steps=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(history, label, n):\n    # Use a log scale to show the wide range of values.\n    plt.semilogy(history.epoch,  history.history['loss'],\n               color=colors[n], label='Train '+label)\n    plt.semilogy(history.epoch,  history.history['val_loss'],\n          color=colors[n], label='Val '+label,\n          linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(bs_history, \"Baseline\", 0)\nplot_loss(inc_layers_feat_history, \"Inc layers feature\", 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cm(model, classes = None, datasets = None):\n    \n    if datasets is None:\n        datasets = [X_val, y_val]\n        \n    prediction = np.argmax(model.predict(datasets[0]), axis = 1)\n    tst = np.vstack((np.argmax(datasets[1], axis = 1), prediction)).T\n\n    cm = np.zeros((len(classes),len(classes)))\n\n    for i in tst:\n        cm[i[0], i[1]] += 1\n\n    f = plt.figure(figsize = (7,6))\n    \n    f = sns.heatmap(cm)#, annot = True)\n    \n    \n    plt.title(\"Confusion Matrix over validation set\")\n    plt.yticks(np.arange(len(classes)) + 0.5, classes, rotation = 0)\n    plt.ylabel(\"Actual\")\n    plt.xticks(np.arange(len(classes)) + 0.5, classes, rotation = 45)\n    plt.xlabel(\"Predicted\")\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see from the confusion matrix that the 31-40 day class has a low sensitivity. Most of these examples are classified at 41-50.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Post Baseline Model Analysis\n\nHere we can see that the 31-40 and 41-50 share similar severity fo illness distributions, which could have contributed to the results shown in the confusion matrix above. It is also not helping the proportions of moderate illnesses is similar across all classes. Maybe collapse this feature space such that moderate and extreme (or minor) are aggregated?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize = (14, 6))\n\n# ax1 ---\n\nc = X_train.groupby([\"Stay\", \"Severity of Illness\"]).size()\nc = (c/c.groupby(level = 1).sum()).reset_index()\n\nsns.barplot(x = \"Stay\", y = 0, hue = \"Severity of Illness\", data = c, ax = ax1)\nax1.set_title(\"\"\"Proportion of Severity of Illnesses by LOS in model training data\n(n = {})\"\"\".format(X_train.shape[0]))\n#ax1.set_xticks(rotation = 45)\nax1.set_ylabel(\"Proportion of Severity of Illnesses\")\n\n# ax2 ---\n\n# Aggregating extreme and moderate categories into above moderate category\nX_train.loc[:, \"Severity of Illness_v2\"] = X_train[\"Severity of Illness\"].astype(str)\nnon_minor_idxs = X_train[X_train[\"Severity of Illness_v2\"] != \"Minor\"].index.values\nX_train.at[non_minor_idxs, \"Severity of Illness_v2\"] = \"Above Moderate\"\n\nc = X_train.groupby([\"Stay\", \"Severity of Illness_v2\"]).size()\nc = (c/c.groupby(level = 1).sum()).reset_index()\n\nsns.barplot(x = \"Stay\", y = 0, hue = \"Severity of Illness_v2\", data = c, ax = ax2)\nax2.set_title(\"\"\"Proportion of Severity of Illnesses v2 by LOS in model training data\n(n = {})\"\"\".format(X_train.shape[0]))\n#ax2.set_xticks(rotation = 45)\nax2.set_ylabel(\"Proportion of Severity of Illnesses\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Updated Baseline\n\nTrying baseline model with update to feature space as shown above, which is used to create discrimination between the LOS classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_features = [\"Severity of Illness_v2\", \"Age\", \"Type of Admission\", \"New Ward_Type\",\n                  \"Hospital_code\", \"Department\", \"Visitors with Patient\"]\n\nordered_cats = input_features[0:2]\ncats = input_features[2:-1]\nnum = [input_features[-1]]\n\nX_train[input_features[0]] = pd.Categorical(X_train[input_features[0]], ordered=True, categories=[\"Minor\", \"Above Moderate\"])\n\nX_train = X_train.loc[:, input_features + [y_feature]]\n\nX_trn, X_val, y_trn, y_val = prepare_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nearlystop_callback = tf.keras.callbacks.EarlyStopping(\n  monitor='val_acc', min_delta=0.0001, mode = \"min\",\n  patience=10, verbose=1)\n\"\"\"\n\ninc_layers_feat = tf.keras.Sequential([\n    layers.Dense(input_dim = X_trn.shape[1], units = 128, activation = \"relu\"),\n    layers.Dense(128, activation = \"relu\"),\n    layers.Dense(128, activation = \"relu\"),\n    layers.Dense(128, activation = \"relu\"),\n    layers.Dense(train.Stay.unique().size, activation = \"softmax\")\n], name = \"inc_layers_feat\")\n\ninc_layers_feat.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=METRICS)\n\n\n\ninc_layers_feat.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inc_layers_feat_history = inc_layers_feat.fit(X_trn, y_trn,\n                          epochs=20,\n                          batch_size = 700,\n                          validation_data=(X_val, y_val),\n                          validation_steps=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_feat.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm(baseline, classes = stay_order)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm(baseline_feat, classes = stay_order)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}