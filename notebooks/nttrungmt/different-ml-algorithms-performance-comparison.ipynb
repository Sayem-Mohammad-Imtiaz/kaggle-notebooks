{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5b85b6f1-a4ca-a1b0-a4fd-bb714fc98d0a"},"source":"In this notebook I will try to use different ML algorithms such as Random Forest, Boosting, SVM, NN to build classifier models, test and compare performance between them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b44325c-5aee-24bd-460f-ed4550671a01"},"outputs":[],"source":"import pandas as pd \nfrom sklearn.utils import shuffle\n# reading data from csv files and converting to matrix \ntest  = pd.read_csv(\"../input/test.csv\")  \ntrain = pd.read_csv(\"../input/train.csv\") \n\n# suffling data \ntest  = shuffle(test)\ntrain = shuffle(train)\n\n# separating data inputs and output lables \ntrainData  = train.drop('Activity' , axis=1).values\ntrainLabel = train.Activity.values\n\ntestData  = test.drop('Activity' , axis=1).values\ntestLabel = test.Activity.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f0886a2-4eba-20a2-5e39-e50dbf85683f"},"outputs":[],"source":"# encoding labels \nfrom sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\n\n# encoding test labels \nencoder.fit(testLabel)\ntestLabelE = encoder.transform(testLabel)\n\n# encoding train labels \nencoder.fit(trainLabel)\ntrainLabelE = encoder.transform(trainLabel)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52007070-1ff4-bd99-6667-eae4e95adb46"},"outputs":[],"source":"import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd5c7435-6dff-9667-a875-d56c1b0a2094"},"outputs":[],"source":"\n#train and test with Random Forest\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=200,  n_jobs=4, min_samples_leaf=10)    \n#train\nrf.fit(trainData, trainLabelE)\n#test\np_te = rf.predict_proba(testData)\n#auc_te = roc_auc_score(testLabelE, p_te)\ny_te_pred = rf.predict(testData)\nacc = accuracy_score(testLabelE, y_te_pred)\nprec = precision_score(testLabelE, y_te_pred, average=\"macro\")\nrec = recall_score(testLabelE, y_te_pred, average=\"macro\")\ncfs = confusion_matrix(testLabelE, y_te_pred)\nprint(\"Acc: %3.5f, P: %3.5f, R: %3.5f\" % (acc, prec, rec))\n#print(\"Confusing Matrix:\\n\", cfs)\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = encoder.classes_\nplot_confusion_matrix(cfs, classes=class_names,\n                      title='RF Confusion matrix, without normalization')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7989b8bd-a743-6077-afaa-0af899e2b264"},"outputs":[],"source":"print(trainData.shape)\nprint(trainLabelE.shape)\nprint(testLabelE.shape)\nprint(p_te.shape)\nprint(y_te_pred.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c4ac8bf-c5b8-ae85-8184-28494c922919"},"outputs":[],"source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import svm\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n#linear kernel gave Acc: 0.96437, P: 0.96695, R: 0.96396\n#classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True, random_state=0))\n#RBF kernel gave Acc: 0.92806, P: 0.93173, R: 0.92578\nclassifier.fit(trainData, trainLabelE)\ny_score = classifier.predict_proba(testData)\ny_te_pred = classifier.predict(testData)\nacc = accuracy_score(testLabelE, y_te_pred)\nprec = precision_score(testLabelE, y_te_pred, average=\"macro\")\nrec = recall_score(testLabelE, y_te_pred, average=\"macro\")\ncfs = confusion_matrix(testLabelE, y_te_pred)\nprint(\"Acc: %3.5f, P: %3.5f, R: %3.5f\" % (acc, prec, rec))\n#print(\"Confusing Matrix:\\n\", cfs)\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = encoder.classes_\nplot_confusion_matrix(cfs, classes=class_names,\n                      title='SVM Confusion Matrix, without normalization')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"838098e6-731b-6588-1d06-d6d14f14bb4e"},"outputs":[],"source":"#perf test with Adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nbdt_discrete = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n                                    n_estimators=200,\n                                    learning_rate=1.5,\n                                    algorithm=\"SAMME\")\nbdt_discrete.fit(trainData, trainLabelE)\ny_score = bdt_discrete.predict_proba(testData)\ny_te_pred = bdt_discrete.predict(testData)\nacc = accuracy_score(testLabelE, y_te_pred)\nprec = precision_score(testLabelE, y_te_pred, average=\"macro\")\nrec = recall_score(testLabelE, y_te_pred, average=\"macro\")\ncfs = confusion_matrix(testLabelE, y_te_pred)\nprint(\"Acc: %3.5f, P: %3.5f, R: %3.5f\" % (acc, prec, rec))\n#print(\"Confusing Matrix:\\n\", cfs)\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = encoder.classes_\nplot_confusion_matrix(cfs, classes=class_names,\n                      title='AdaBoost Confusion Matrix, without normalization')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d190eda3-f788-9fc0-2c06-9d004521d4eb"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()\nscaler.fit(trainData)  \ntrainData_scaled = scaler.transform(trainData)  \n# apply same transformation to test data\ntestData_scaled = scaler.transform(testData) \n#test with NN\nimport sklearn.neural_network as nn\nmlpSGD  =  nn.MLPClassifier(hidden_layer_sizes=(50,50)\n                            , solver='adam' \n                            , max_iter=1000\n                            , early_stopping=True\n                            , random_state=0) \nnnSGDModel  = mlpSGD.fit(trainData_scaled , trainLabelE)\ny_score = nnSGDModel.predict_proba(testData_scaled)\ny_te_pred = nnSGDModel.predict(testData_scaled)\nacc = accuracy_score(testLabelE, y_te_pred)\nprec = precision_score(testLabelE, y_te_pred, average=\"macro\")\nrec = recall_score(testLabelE, y_te_pred, average=\"macro\")\ncfs = confusion_matrix(testLabelE, y_te_pred)\nprint(\"Acc: %3.5f, P: %3.5f, R: %3.5f\" % (acc, prec, rec))\n#print(\"Confusing Matrix:\\n\", cfs)\n# Plot non-normalized confusion matrix\nplt.figure()\nclass_names = encoder.classes_\nplot_confusion_matrix(cfs, classes=class_names,\n                      title='MLP-SCG Confusion Matrix, without normalization')\n#test with SGD no scaled HL(30,60) max_iter=1000 => Acc: 0.94673, P: 0.94860, R: 0.94456\n#test with Adam no scaled HL(30,60) max_iter=1000 => Acc: 0.94537, P: 0.94720, R: 0.94511\n#test with Adam scaled HL(20,10) max_iter=1000, EarlyStop => Acc: 0.94062, P: 0.94493, R: 0.93926\n#test with Adam scaled HL(20,20) max_iter=1000, EarlyStop => Acc: 0.94774, P: 0.94912, R: 0.94670\n#test with Adam scaled HL(20,30) max_iter=1000, EarlyStop => Acc: 0.94333, P: 0.94470, R: 0.94221\n#test with Adam scaled HL(20,40) max_iter=1000, EarlyStop => Acc: 0.94808, P: 0.95020, R: 0.94749\n#test with Adam scaled HL(30,30) max_iter=1000, EarlyStop => Acc: 0.94197, P: 0.94488, R: 0.94052\n#test with Adam scaled HL(30,60) max_iter=1000, EarlyStop => Acc: 0.94096, P: 0.94463, R: 0.93973\n#test with Adam scaled HL(40,20) max_iter=1000, EarlyStop => Acc: 0.93349, P: 0.93825, R: 0.93124\n#test with Adam scaled HL(40,30) max_iter=1000, EarlyStop => Acc: 0.94774, P: 0.94872, R: 0.94674\n#test with Adam scaled HL(40,40) max_iter=1000, EarlyStop => Acc: 0.94808, P: 0.95087, R: 0.94699\n#test with Adam scaled HL(40,60) max_iter=1000, EarlyStop => Acc: 0.94605, P: 0.94655, R: 0.94495\n#test with Adam scaled HL(40,80) max_iter=1000, EarlyStop => Acc: 0.94231, P: 0.94519, R: 0.94061\n#test with Adam scaled HL(50,50) max_iter=1000, EarlyStop => Acc: 0.95080, P: 0.95176, R: 0.94989"},{"cell_type":"markdown","metadata":{"_cell_guid":"87a20f70-fccf-9e02-1602-c430a0fce6cd"},"source":"So far, we can see that SVM (linear kernel) gave the best result: 96% accuracy with high precision and recall also.\nIn the future we can try tuning different parameters of those ML algorithms to find better model. \n - SVM-linear:             Acc: 0.96437, P: 0.96695, R: 0.96396\n - MLP-SCG(50,50):   Acc: 0.95080, P: 0.95176, R: 0.94989"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}