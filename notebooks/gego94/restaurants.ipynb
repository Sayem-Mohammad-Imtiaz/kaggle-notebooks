{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import metrics \nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# fetching data annd removing useless columns like name, url, ...\ndata = pd.read_csv('../input/zomato-bangalore-restaurants/zomato.csv').drop(['url','address','name','phone','location',\n                          'reviews_list','menu_item','listed_in(city)'], axis=1)\n\n# cleaning datam by removing null values and categorize other data\ndata.dropna(inplace=True)\ndata.drop(data[ data['rate'] == 'NEW'].index , inplace=True)\ndata.drop(data[data['rate'].str.len() < 2].index , inplace=True)\ndata.online_order = data.online_order.astype('category').cat.codes\ndata.book_table = data.book_table.astype('category').cat.codes\ndata.rest_type = data.rest_type.astype('category').cat.codes\ndata['listed_in(type)'] = data['listed_in(type)'].astype('category').cat.codes\ndata['approx_cost(for two people)'] = data['approx_cost(for two people)'].str.replace(',','',regex=True).astype(float)\n\n# for these kind of data i took the number of possibilities they gave, because they're lists of elements\ndata.dish_liked = data.dish_liked.str.split(',').str.len().astype('category').cat.codes + 1\ndata['cuisines'] = data.cuisines.str.split(',').str.len().astype('category').cat.codes + 1\n\n# representing the score as the float number *10, because the model needs integer values as targets\ndata['rate'] = (data.rate.str.slice(stop=-2).astype(float) * 10).astype(int)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most of the columns have not much possibile values, so decision trees are good.\n\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create train and test set and take 10% of data as test\nx = data.drop('rate', axis=1).values\ny = data['rate'].values\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying prediction with entropy model. Max depth of the tree is free.\nres = pd.DataFrame(columns=['Accuracy'])\nfor i in range(2,28):\n    entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n    entropy = entropy.fit(X_train,y_train)\n    y_pred = entropy.predict(X_test)\n    res = res.append({'Accuracy': metrics.accuracy_score(y_test, y_pred)}, ignore_index=True)\n# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# print(entropy.get_depth())\nplt.plot(range(2,28), res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying prediction with gini model. Max depth of the tree is free.\nres1 = pd.DataFrame(columns=['Accuracy'])\nfor i in range(2,28):\n    gini = DecisionTreeClassifier(criterion=\"gini\", max_depth=i)\n    gini = gini.fit(X_train,y_train)\n    y_pred = gini.predict(X_test)\n    res1 = res1.append({'Accuracy': metrics.accuracy_score(y_test, y_pred)}, ignore_index=True)\nplt.plot(range(2,28), res1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see, decision trees performs really well in these kind of tasks, where there isn't a big variance in data values. (only in 1)\n# Further more the differnce between the 2 methods is really small and depends principally on how train data is taken. Generally entropy \n# methods performs a bit better.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}