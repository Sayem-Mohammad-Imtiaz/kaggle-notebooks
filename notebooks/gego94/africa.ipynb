{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import delle librerie necessarie"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import tanh\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport math\nimport timeit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import dei valori, rimozione dei valori nulli e mapping {-1, 1} degli output"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 50)\nafr = pd.read_csv('../input/africa-economic-banking-and-systemic-crisis-data/african_crises.csv').drop(['case','cc3','country','year'], axis=1)\nafr.dropna(inplace=True)\nafr.banking_crisis = afr.banking_crisis.map({'crisis':1,'no_crisis':-1})\nafr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split del dataset in train e test"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = afr.drop('banking_crisis', axis=1).values\ny = afr.banking_crisis.values\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definizione delle funzione per il successivo utilizzo della classe perceptron, in particolare, le funzioni per eseguire i calcoli di accuratezza del perceptron e per disegnare riusltati e funzioni di attivazione"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['batch', 'stochastic']\nlearnings = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n# funzione per eseguire i calcoli\n# passati un array di percettroni, il tipo di perceptron desiderato e l'errore da raggiungere, addestra i percettroni con i learning_rates\n# sopra definiti e restituisce 2 array con i calcoli della precisione e degli errori commessi sul test set\ndef calc(p, t, eps = 0.2):\n    start = timeit.default_timer()\n    ris = []; er = []\n    for l in learnings:\n        ris.append([]); er.append([]); ind = learnings.index(l)\n        for i in range(len(p)):    \n            p[i].train(x=X_train, y=y_train, x_test=X_test, y_test=y_test, learning_rate=l, no_batch=[i], t=t, eps=eps)\n            ris[ind].append(p[i].acc); er[ind].append(p[i].errors)\n    stop = timeit.default_timer()\n    print('Time: ', stop - start)  \n    return ris, er\n# funzioni per disegnare\nplt.rcParams.update({'font.size': 22})\n\n# disegna le funzioni segno, sigmoid, relu, tanh\ndef dis_func(t=0, names=[]):\n    plt.rcParams[\"figure.figsize\"] = (20, 5)\n    if t == 0:\n        fun = lambda x:x\n    elif t == 1:\n        fun = sigmoid\n    elif t == 2:\n        fun = tanh\n    else:\n        fun = relu\n    deriv = derivative(t)\n    l = np.linspace(-5, 5, 100)\n    f, axx = plt.subplots(1,2)\n    yd = []; yy = []\n    for u in l:\n        yy.append(fun(u))\n        yd.append(deriv(fun(u)))\n    axx[0].plot(l, yy)\n    axx[0].grid(True)\n    axx[0].set_title(names[0])\n    axx[1].plot(l, yd); axx[1].grid(True); axx[1].set_title(names[1])\n    plt.rcParams[\"figure.figsize\"] = (25, 20)\n# disegna accuratezza ed errori commessi\ndef dis(p, r, e):\n    plt.rcParams[\"figure.figsize\"] = (25, 20)\n    fig, ax = plt.subplots(len(learnings), 2) \n    for l in range(len(learnings)):\n        ax[l][0].set_title('accuratezza con learning_rate = {} '.format(learnings[l]))\n        ax[l][1].set_title('errore quadratico medio con learning_rate = {}'.format(learnings[l]))\n        for i in range(len(p)):\n            ax[l][0].plot(range(1,len(r[l][i]) + 1), r[l][i], label=labels[i])\n            ax[l][0].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n            ax[l][1].plot(range(1,len(e[l][i]) + 1), e[l][i], label=labels[i])\n            ax[l][1].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n    plt.legend()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definizione delle funzioni di supporto per la classe perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Funzione per il calcolo della sigmoide\ndef sigmoid(gamma):\n    if gamma < 0:\n        return 1 - 1/(1 + math.exp(gamma))\n    else:\n        return 1/(1 + math.exp(-gamma))\n    \n# funzione per il calcolo della relu\ndef relu(x):\n    return max(0.001*x, x)\n\n# Serve per fornire alla classe perceptron la giusta funzione segno in base alla funzione di attivazione \ndef sign(t=0):\n    if t == 1:\n        return lambda x: np.where(x >= 0.5, 1, -1)\n    return lambda x: np.where(x >= 0.0, 1, -1)\n    \n# Fornisce alla classe perceptron la derivata della funzione di attivazione in base alla funzione di attivazione\ndef derivative(t=0):\n    if t == 0:\n        return lambda x: 1\n    if t == 1:\n        return lambda x: x * ( 1 - x )   \n    if t == 2:\n        return lambda x: 1.0 - x**2\n    if t == 3:\n        return lambda x: 1 if x >= 0 else 0.001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implementazione del perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# classe perceptron\nclass perceptron:\n    \n    # In base al parametro definsice la funzione di attivazione\n    def act(self, t=0):\n        if t == 0:\n            return lambda x: np.dot(x, self.w[1:]) + self.w[0]  \n        if t == 1:\n            return lambda x: sigmoid(np.dot(x, self.w[1:]) + self.w[0])\n        if t == 2:\n            return lambda x: tanh(np.dot(x, self.w[1:]) + self.w[0])\n        if t == 3:\n            return lambda x: relu(np.dot(x, self.w[1:]) + self.w[0])\n    \n    # predice applicando la funzione segno all'attivazione\n    def predict(self,x): \n        return self.sign(self.activation(x)) \n    \n    # calcolo dell'errore quadratico medio\n    def err_calc(self, x, y):\n        ris = []; m = 0\n        for xi, yi in zip(x, y):\n            p = self.sign(self.activation(xi))\n            m += ( yi - p ) ** 2\n        return m / ( 2 * len(x) )\n    \n    # calcolo dell' accuratezza\n    def accuracy(self, x, y):\n        ris = []\n        for e in x:\n            ris.append(self.predict(e.tolist()))\n        return (sum(np.array(ris) == np.array(y)) / len(y))\n    \n    # inizializza i valori\n    # x è il train set, learning_rate = eta, no_batch = 0 crea un perceptron di tipo batch, t il tipo di attivazione\n    def initialize(self, x, learning_rate, no_batch, t=0):\n        # assegno le funzioni in base al tipo desiderato\n        self.sign = sign(t); self.activation = self.act(t); self.derivative = derivative(t)  \n        self.errors = []; self.no_batch = no_batch; self.acc = []; self.inputs_dim = len(x[0])\n        self.w = np.random.uniform(low=-0.05, high=0.05, size=(self.inputs_dim + 1,))\n        self.learning_rate = learning_rate; self.w[0] = 1 # bias iniziale\n        \n    # aggiorna pesi\n    def update(self, xi, yi):\n        s = self.activation(xi)\n        p = self.sign(s)\n        # aggiorno solo se ha sbagliato la predizione\n        if p - yi != 0: \n            err = yi - s \n            # se tipo batch aggiorna i delta\n            if self.no_batch == 0: \n                self.dw[1:] += self.learning_rate * xi * err * self.derivative(s)\n                self.dw[0] += self.learning_rate * err * self.derivative(s)\n            else: \n                # se non batch aggiorna i pesi\n                self.w[1:] += self.learning_rate * xi * err * self.derivative(s)\n                self.w[0] += self.learning_rate * err * self.derivative(s)\n                \n    # addestra x,y sono il train , x_test, y_test il test, epochs il numero massimo di epoche se non raggiunge l'errore desiderato,\n    # eps l'errore da raggiungere, no_batch=0 crea un perceptron di tipo batch, t = tipo di attivazione\n    def train(self, x, y, x_test, y_test, learning_rate = 0.001, epochs=10000, eps=0.2, no_batch=1, t=0):\n        # inizializza i dati\n        self.initialize(x=x, learning_rate=learning_rate, no_batch=no_batch, t=t) \n        # itera sule epoche\n        for e in range(epochs): \n            # se batch inizializza i delta\n            if self.no_batch == 0:\n                self.dw = np.zeros(self.inputs_dim + 1)\n            # chiama la funzione di update per ogni esempio    \n            for xi, yi in zip(x, y): \n                self.update(xi, yi)\n            # se batch somma delta ai pesi\n            if self.no_batch == 0: \n                self.w -= self.dw / len(x)\n            # crea gli array di accuratezza ed errore    \n            self.acc.append(self.accuracy(x=x_test, y=y_test)) \n            er = self.err_calc(x=x, y=y)\n            self.errors.append(er) \n            #se raggiunto l'errore interrompe\n            if er < eps and e > 10: \n                break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applicazione del perceptron con funzione di attivazione segno(x)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_func(t=0,names=['sign(x)', 'sign(x) derivata'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_n = [perceptron(), perceptron()]\nris_n,er_n = calc(perc_n, t=0)\ndis(perc_n, ris_n, er_n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applicazione del perceptron con funzione di attivazione sigmoide(x).\n\n    def sigmoid(gamma):\n\n        if gamma < 0:\n    \n            return 1 - 1/(1 + math.exp(gamma))\n       \n        else:\n    \n            return 1/(1 + math.exp(-gamma))\n\nVantaggi:\n* Molto comoda per rappresentare probabilità in quanto ha range di output [0,1].\n* Buona per predire relazione binarie.\n\nProblemi :\n* Vanishing gradient : il gradiente diventa talmente piccolo che causa un cambiamento dei pesi talemente piccolo da bloccare quasi l'apprendimento nei primi strati delle reti neurali.\n* Computazionalmente espensiva per via dell'exp.\n\nDimostrazione della derivata della sigmoide : \n\n(d/dx)σ(x)=\n\n(d/dx)( 1 / (1+e^(-x)) )=\n\n(d/dx)(1+e^(−x))^(−1)=\n\n−(1+e^(−x))^(−2) * (−e^(−x))=\n\ne^(−x) / (1+e^(−x))^2=\n\n1 / (1+e^(−x)) * (e^(−x) / (1+e^(−x))=\n\n1 / (1+e^(−x)) * ((1+e^(−x)−1) / (1+e^(−x)))=\n\n1 / (1+e^(−x)) * (((1+e^(−x)) / (1+e^(−x))) − (1 / (1+e^(−x))))=\n\n1 / (1+e^(−x)) * (1 − (1 / (1+e^(−x))=\n\nσ(x)⋅(1−σ(x))"},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_func(t=1,names=['sigmoid(x)', 'sigmoid(x) derivata']) # disegna la funzione","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_s = [perceptron(), perceptron()]\nris_s,er_s = calc(perc_s, t=1)\ndis(perc_s, ris_s, er_s)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applicazione del perceptron con funzione di attivazione tanh(x).\n\n    def tanh(x):\n        \n        return sinh(x) / cosh(x)\n        \nVantaggi:\n* Range [-1,1].\n* Gradiente più incisivo della sigmoide.\n\nProblemi:\n* Vanishing gradient.\n\nDimostrazione della derivata della tanh : \n\n(d/dx) (tanh(x)) =\n\n(d/dx) (sinh(x) / cosh(x)) =\n\n((sinh(x))' * cosh(x) - sinh(x) * (cosh(x))') / (cosh(x)) ^ 2 = \n\n(cosh(x) * cosh(x) - sinh(x) * sinh(x)) / (cosh(x)) ^ 2 =\n\n(cosh(x) ^ 2 - sinh(x) ^ 2) / (cosh(x)) ^ 2 =\n\n(cosh(x) ^ 2 / cosh(x) ^ 2) - ( sinh(x) ^ 2 / cosh(x) ^ 2) =\n\n1 - ( sinh(x) ^ 2 / cosh(x) ^ 2) =\n\n1 - ( sinh(x) / cosh(x) ) ^ 2 = \n\n1 - tanh(x) ^ 2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_func(t=2,names=['tanh(x)', 'tanh(x) derivata'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_t = [perceptron(), perceptron()]\nris_t,er_t = calc(perc_t, t=2)\ndis(perc_t, ris_t, er_t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applicazione del perceptron con funzione di attivazione relu(x).\n\n    def relu(x):\n\n        return max(0.001*x, x)\n        \nDella funzione relu, ne esistono diverse varianti, in particolare, quella normale differisce da quella utilizzata in quanto definita come max(0, x), ma rischia di far morire il perceptron, ovvero una volta che il arriva a predire 0 non si riesce a recuperarlo, per questo ho usato una variante detta leaky relu, che per valori negativi restituisce max(0.001 * x, x). Altre varianti sono ad esempio la parametric relu definita come return max(a * x, x) con a > 0 , di cui la leaky relu ne è a sua volta una variante o la exponential relu definita come max(a * (e ^x - 1) * x, x).\n\nIn generale, la relu :\n\n* E' più veloce da calcolare rispetto alle altre funzioni.\n* Converge più velocemente.\n* No vanishing gradient.\n\nProblemi:\n* Dying ReLU : neuroni che muoiono arrivando a predire sempre 0.\n* Non usata in certe applicazioni in quanto può restituire valori molto elevati.\n* Valori troppo elevati del learning rate possono portare a non convergere.\n* Di solito viene utilizzata negli strati interni delle reti neurali in quanto ha un risultato lienare se positivo.\n\n\nDimostrazione della derivata della relu : \n\n(d/dx) (relu(x)) =\n\ncaso 1 : x >= 0\n\n(d/dx) x = 1\n\ncaso 2 : x < 0\n\n(d/dx) 0.001 * x = 0.001\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_func(t=3,names=['relu(x)', 'relu(x) derivata'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_r = [perceptron(), perceptron()]\nris_r,er_r = calc(perc_r, t=3)\ndis(perc_r, ris_r, er_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come si può notare, ReLu e identità, tendono a convergere più in fretta ad una soluzione, il problema è che a learning rate più alti, non arrivano a convergere in tempo ragionevole. Le altre 2 funzioni, risultano invece essere più stabili, in quanto convergono al risultato anche se con più lentezza. Inoltre, sigmoide e tanh risultano avere un apprendimento molto instabile all'aumentare del learning rate, a differenza di ReLu e identità."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}