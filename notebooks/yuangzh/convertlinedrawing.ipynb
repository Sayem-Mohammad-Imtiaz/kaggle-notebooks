{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/didriknielsen/survae_flows.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport cv2\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom survae.distributions import DataParallelDistribution\nfrom survae.distributions import StandardNormal, ConditionalNormal, ConditionalBernoulli, Distribution, ConditionalDistribution\nimport torch\nimport numpy as np\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom skimage import color\nimport zipfile\nimport os\nimport io\nfrom PIL import Image\nimport cv2\nfrom pathlib import Path\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import grad\n\nfrom survae.distributions import StandardNormal, ConditionalNormal, ConditionalBernoulli, Distribution, ConditionalDistribution\nfrom survae.utils import sum_except_batch, mean_except_batch\nfrom torch.distributions import Normal\nfrom torchvision.models import resnet50, resnet18\nfrom torchvision.models.segmentation import fcn\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torchvision.models.utils import load_state_dict_from_url","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linedraw(x):\n    x = x.mean(1, keepdims=True)\n    dilated = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n    diff = torch.abs(x - dilated)\n    return diff\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\nunnormalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n                                   std=[1/0.229, 1/0.224, 1/0.225])","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ANIME(torchvision.datasets.VisionDataset):\n\n    def __init__(self, img_size, train=True):\n        transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()])\n        super().__init__('../input/moeimouto-faces/moeimouto-faces', transform=transform)\n        self.img_size = img_size\n        self.samples = list(Path(self.root).glob(\"0*/*.png\" if train else \"1*/*.png\"))\n\n    def __getitem__(self, index: int):\n        img = Image.open(str(self.samples[index])).convert('RGB')\n        img = self.transform(img)\n        return img\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\ndef get_data_loaders(batch_size, img_size=256):\n    tr_loader = DataLoader(ANIME(img_size, train=True), batch_size, shuffle=True, num_workers=2)\n    va_loader = DataLoader(ANIME(img_size, train=False), batch_size, num_workers=2)\n    return tr_loader, va_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomizedResnet(nn.ModuleDict):\n    def __init__(self, out_channels=20, build_fn=resnet50, fpn=True, aux_in=0, ori_out=False, spp=False):\n        self.aux_in = aux_in\n        self.fpn = fpn\n        model = build_fn(True)\n        model.conv1.in_channels = 1\n        model.conv1.weight.data = model.conv1.weight.data.sum(1, keepdims=True)\n        super(CustomizedResnet, self).__init__(model.named_children())\n        if aux_in:\n            self.decode = nn.Sequential(nn.Conv2d(aux_in, 256 if build_fn == resnet50 else 64, 1))\n        if fpn:\n            fpn_dim = 256 if build_fn == resnet50 else 64\n            self.out4 = nn.Conv2d(2048 if build_fn == resnet50 else 512, fpn_dim, 1)\n            self.out3 = nn.Conv2d(1024 if build_fn == resnet50 else 256, fpn_dim, 1)\n            self.out2 = nn.Conv2d(512 if build_fn == resnet50 else 128, fpn_dim, 1)\n\n            self.up3 = nn.Conv2d(fpn_dim, fpn_dim, 3, 1, 1)\n            self.up2 = nn.Conv2d(fpn_dim, fpn_dim, 3, 1, 1)\n            out = [\n                nn.Conv2d(fpn_dim, fpn_dim//2, 3, 1, 1), nn.BatchNorm2d(fpn_dim//2), nn.ReLU(),\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(fpn_dim//2, fpn_dim//4, 3, 1, 1), nn.BatchNorm2d(fpn_dim//4), nn.ReLU(),\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(fpn_dim//4, fpn_dim//8, 3, 1, 1), nn.BatchNorm2d(fpn_dim//8), nn.ReLU(),\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(fpn_dim//8, out_channels, 3, 1, 1)\n            ]\n            self.out = nn.Sequential(*out)\n        else:\n            self.fc = nn.Conv2d(2048 if build_fn == resnet50 else 512, out_channels, 1)\n\n    def forward(self, x):\n        if self.aux_in:\n            x, z = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x2 = self.layer2(x + self.decode(z) if self.aux_in else x)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        if self.fpn:\n            z4 = self.out4(x4)\n            z3 = self.up3(self.out3(x3) + F.interpolate(z4, scale_factor=2))\n            z2 = self.up2(self.out2(x2) + F.interpolate(z3, scale_factor=2))\n            return self.out(z2)\n        return self.fc(self.avgpool(x4))\n\n\nclass ConditionalNormalMean(ConditionalNormal):\n    def sample(self, context):\n        return self.mean(context)\n\nclass VAE(Distribution):\n    def __init__(self, prior:ConditionalNormal, latent_size=20, vae=True):\n        super().__init__()\n        self.prior = prior\n        self.vae = vae\n        self.encoder = ConditionalNormal(CustomizedResnet(latent_size*2, resnet18, aux_in=3, fpn=False), 1)\n        self.decoder = ConditionalNormalMean(CustomizedResnet(3*2, aux_in=latent_size, ori_out=True, spp=True), 1)\n\n    def log_prob(self, x, l):\n        z, log_qz = self.encoder.sample_with_log_prob(context=(l, F.avg_pool2d(x, 4, 4)))\n        log_px = self.decoder.log_prob(x, context=(l, z))\n        log_p = self.prior.log_prob(z, l) + log_px - log_qz\n        return log_p\n\n    def sample(self, l, num_samples=1):\n        z = self.prior.sample(l)\n        x = self.decoder.sample(context=(l, z))\n        return x\n\nclass Discriminator(nn.ModuleDict):\n    def __init__(self):\n        super(Discriminator, self).__init__(resnet18(True).named_children())\n        self.fc = nn.Linear(512, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x2 = self.layer2(x)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n        return self.fc(self.avgpool(x4).flatten(1))\n\nclass RejVAE(VAE):\n    def __init__(self, prior, latent_size=20, vae=True):\n        super().__init__(prior, latent_size, vae)\n        self.sampler = Discriminator()\n        self.register_buffer('rej_prob', torch.tensor(0.5))\n\n    def _grad_pen(self, G: torch.Tensor): \n        return grad_pen\n\n    def log_prob(self, x, l):\n        log_posterior = self.sampler(x).flatten()\n        G = super().sample(l)\n        G = 1.01 * G.detach() - 0.01 * G\n        log_prior = self.sampler(G).flatten()\n        alpha = torch.rand((x.shape[0], 1, 1, 1), device=x.device)\n        x_hat = alpha * x + (1 - alpha) * G\n        x_hat = x_hat.detach().requires_grad_()\n        lipschitz_grad = grad(\n                outputs=self.sampler(x_hat).sum(), \n                inputs=x_hat,\n                create_graph = True, \n                retain_graph = True)[0].view(G.shape[0], -1)\n        grad_pen = (torch.sum(lipschitz_grad**2, dim=1) - 1).relu()\n\n        self.rej_prob = log_prior.detach().mean()\n        log_prior = torch.logsumexp(log_prior - math.log(log_prior.size(0)), 0)\n        rej_log_prob = log_posterior - log_prior + grad_pen.detach() - grad_pen\n        return super().log_prob(x, l) + rej_log_prob * 20000 - rej_log_prob.detach() * 19999\ndef get_model(pretrained_backbone=True, vae=True, rej=True) -> VAE:\n    prior = ConditionalNormal(CustomizedResnet(64 * 2, resnet18, fpn=False), 1)\n    Model = RejVAE if rej else VAE\n    return Model(prior, 64, vae=vae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import MultiStepLR\n\nclass LinearWarmupScheduler(MultiStepLR):\n    \"\"\" Linearly warm-up (increasing) learning rate, starting from zero.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        warmup: target learning rate is reached at warmup.\n    \"\"\"\n\n    def __init__(self, optimizer, warmup, milestones, gamma=0.1, verbose=False, last_epoch=-1):\n        self.warmup = warmup\n        super(LinearWarmupScheduler, self).__init__(optimizer, milestones, gamma, last_epoch, verbose)\n\n    def get_lr(self):\n        if self.last_epoch >= self.warmup:\n            return super().get_lr()\n        return [0.1 * base_lr + 0.9 * base_lr * self.last_epoch / self.warmup for base_lr in self.base_lrs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport argparse\nimport os\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef log_img(model, args, wandb, writer):\n    with torch.no_grad():\n        l = linedraw(X_test)\n        lab = unnormalize(model.sample(l))\n        img = torch.cat([1 - l.repeat(1, 3, 1, 1), lab], -1)\n        if args.vis_mode == 'tensorboard':\n            writer.add_images(\"result\", img.transpose(0, 3, 1, 2), gIter)\n        elif args.vis_mode == 'wandb':\n            wandb.log({'result': [wandb.Image(i) for i in img]})\n        else:\n            save_plt_img(img, title='result')\n\nclass ARGS:\n    def __init__(self):\n        self.batch_size = 24\n        self.img_size = 256\n        self.num_epoch = 64\n        self.lr = 0.001\n        self.warmup = 1000\n        self.vae = True\n        self.rej = True\n        self.vis_mode = 'wandb'\n        self.param_path = 'models/'\n        self.exp_name = 'vae'\n        self.adam = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############\n##  Data  ##\n############\nargs = ARGS()\nos.makedirs(args.param_path, exist_ok=True)\n\ntorch.manual_seed(0)\ntr_loader, va_loader = get_data_loaders(args.batch_size, args.img_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#############\n##  Model  ##\n#############\n\nmodel = get_model(vae=args.vae, rej=args.rej).to(device)\n\nif args.adam:\n    optim = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\nelse:\n    optim = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\nsched = LinearWarmupScheduler(optim, args.warmup, [\n    args.num_epoch * 7 * len(tr_loader) // 10, args.num_epoch * 9 * len(tr_loader) // 10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############\n##  Logging  ##\n###############\n\nif args.vis_mode == 'tensorboard':\n    from tensorboardX import SummaryWriter\n    writer = SummaryWriter(flush_secs=30)\n    wandb = None\nelif args.vis_mode == 'wandb':\n    import wandb\n    wandb.login(key='9cb14069f28e84706771023c650bfbed5c65f65c')\n    wandb.init(project='linedraw')\n    wandb.config.update(args)\n    wandb.watch(model)\n    writer = None\ngIter = 0\n\nX_test = next(iter(va_loader))\nX_test = X_test.to('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_iters = [100 * i for i in range(1, 10)] + [1000 * i for i in range(1, 1000)]\nfor epoch in range(args.num_epoch):\n    cum_loss = 0.0\n    pbar = tqdm(tr_loader)\n    for i, img in enumerate(pbar):\n        img = img.to(device)\n        l = linedraw(img)\n        img = normalize(img)\n        loss = -model.log_prob(img, l).mean() / (3 * args.img_size ** 2)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        sched.step()\n        cum_loss += loss.item()\n        pbar.set_description_str(f\"Epoch {epoch}, nll {cum_loss / (i+1):.4f}\")\n        if args.vis_mode == 'tensorboard':\n            writer.add_scalar(\"Train/nll\", loss, gIter)\n            if args.rej:\n                writer.add_scalar(\"Train/rej\", model.rej_prob, gIter)\n        elif args.vis_mode == 'wandb':\n            logs = {\"Train/nll\": loss}\n            if args.rej:\n                logs.update({\"Train/rej\": model.rej_prob})\n            wandb.log(logs)\n        if gIter in log_iters:\n            log_img(model, args, wandb, writer)\n        gIter += 1\n\n#     # model.eval()\n#     with torch.no_grad():\n#         cum_loss = 0.0\n#         pbar = tqdm(va_loader)\n#         for i, img in enumerate(pbar):\n#             img = img.to(device)\n#             l = linedraw(img)\n#             img = normalize(img)\n#             loss = -model.log_prob(img, l).mean() / (3 * args.img_size ** 2)\n#             cum_loss += loss.item()\n#             pbar.set_description_str(f\"Test nll {cum_loss / (i+1):.4f}\")\n#     if args.vis_mode == 'tensorboard':\n#         writer.add_scalar(\"Val/nll\", cum_loss / len(va_loader), gIter)\n#     elif args.vis_mode == 'wandb':\n#         wandb.log({\"Val/nll\": cum_loss / len(va_loader)})\n\n    log_img(model, args, wandb, writer)\n\n    torch.save(model.state_dict(), os.path.join(args.param_path, args.exp_name+'_model.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r wandb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}}]}