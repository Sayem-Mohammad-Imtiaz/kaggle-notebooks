{"cells":[{"metadata":{"_uuid":"38881f9d7c2ea8e9f2624385054dafd37611f10a"},"cell_type":"markdown","source":"# Mushrooms classifier with tensorflow neural networks\n\n\nThis is a very basic implementation of a simple neural network algorithm for solving a mushroom classification task. The goal here is just to provide an intuition on the basics of tensorflow and neural networks.\n\nThe dataset was got from Kaggle and is available here: <a href=\"https://www.kaggle.com/uciml/mushroom-classification/data\">Mushroom Classification</a>. More details about it are available under the <a href=\"https://www.kaggle.com/uciml/mushroom-classification/\">Overview</a> tab.\n\n### The problem\nClassify mushrooms between Poisonous or Eligible for eating\n\n\n### Method   \n1. Prepare the data;\n2. Create models;\n3. Evaluate and compare the models."},{"metadata":{"trusted":true,"_uuid":"59e293c0bc315cc30e4bd8dd410511330457201c"},"cell_type":"code","source":"# For preparing and analyzing the dataset\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\nfrom time import time\n\n# For creating the model\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dedf5f503e963db20e79dde7e38a4016c423be2"},"cell_type":"markdown","source":"## Loading and preparing dataset"},{"metadata":{"trusted":true,"_uuid":"14fe7141de987abb9df2e62c5cafd36486d4901d"},"cell_type":"code","source":"# Open file\n\ndata = pd.read_csv(\"../input/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f12bcdb0f72b68873bc7c735ff4653c3c2d8eb6a"},"cell_type":"markdown","source":"###  How many samples and features do we have?"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2e0d0de8ce17eb41a7372bed03eeb4fb55b8b899"},"cell_type":"code","source":"samples = data.shape[0]\nfeatures = data.shape[1]\nprint(f\"{samples} samples and {features} features\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd58d22148433ea0d44e2c2641dd8a345474457e"},"cell_type":"markdown","source":"### Getting intuition about the data"},{"metadata":{"_uuid":"8bd905cfd391f995956cd68e7565ade06a13ce9d"},"cell_type":"markdown","source":"This dataset has categorical data in all its columns."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"3731aeca7abd33396bdc62e3b074d46f6f45d156"},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"987059a5a9b35ea7f08baab4c65cd88730f2b283"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac98ef29079c9d6645019f14bbbefa7c9c162707"},"cell_type":"markdown","source":"### Getting numeric categories\nNow we have a dataset with numbers for each category.\n\nFor the class column, which will be our labels we have:\n* 1: poisonous (\"p\" in the orinigal dataset)\n* 0: eligible (\"e\" in the original dataset)"},{"metadata":{"trusted":true,"_uuid":"454821506d3c080cd2353aaf6f8972ebcdb53637"},"cell_type":"code","source":"# Casting the column types to categorical.\ncolumns = list(data.columns)\n\n# For that, we will need to create a dictionary where the keys are\n# the name of the column and the values are strings with the type we want.\ncolumns_dtype = {column:\"category\" for column in columns}\n\ndata = data.astype(columns_dtype)\n\n# Getting category codes\ndata = data[columns].apply(lambda x: x.cat.codes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f28e2bb3b7f3277f7b5e2604dabaa28eb4f4bb82"},"cell_type":"markdown","source":"### Shuffling the data\nThis step is important. This dataset is provided in some kind of order, if we don't shuffle it, we will get a training set that will not generalize well the entire dataset, causing the model to perform very badly."},{"metadata":{"trusted":true,"_uuid":"779c2daad2800b30ed0a5b7b3c7d05dad637b5e2"},"cell_type":"code","source":"# Shuffle data\ndata = data.sample(frac=1, random_state=3)\ndata.reset_index(inplace=True, drop=True)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4b09cbf605e6430b29797ed439176a9d96d1860"},"cell_type":"markdown","source":"Because we are using softmax cross entropy loss, we need to apply one-hot encodig."},{"metadata":{"trusted":true,"_uuid":"3d47dc7ed41ee127848c9777c56a07c5b1108fc9"},"cell_type":"code","source":"# One-hot labels\n# Create dataframe with the labels\nY_labels = pd.DataFrame(data['class'].copy())\n# Create dataframe with the data\nX_data = pd.DataFrame(data.iloc[:,1:])\n\n# Create 'class2' column that is the opposite of 'class' column\nY_labels['class2'] = Y_labels['class'] == 0\n# Cast 'class' values to bool\nY_labels['class'] = Y_labels['class'].astype('bool', copy=True)\n\nY_labels.reset_index(inplace=True, drop=True)\nY_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6693641488079e2d9ea920e27884260c38738d0"},"cell_type":"markdown","source":"### Zero-mean normalization"},{"metadata":{"trusted":true,"_uuid":"7f4f6076cd5b6875ebbaf5a244c58f2e4e86e689"},"cell_type":"code","source":"# Normalize columns\nfor column in X_data.columns:\n    mean = X_data[column].mean()\n    standard_deviation = X_data[column].std()\n    X_data[column] = (X_data[column] - mean) / (standard_deviation) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54690c6098b69f7d7d236c758ae648d0c7fbb27c"},"cell_type":"markdown","source":"Because the \"veil-type\" column has the same value in all its extension, when we apply the normalization all its values become NaN. I decided not to drop this column because the performance turned up to be better when I just filled its fields with zero."},{"metadata":{"trusted":true,"_uuid":"e838fa00d4ebd72476bbdd57fe7513b204d9bdbe"},"cell_type":"code","source":"X_data = X_data.fillna(0)\n#X_data.drop(columns=\"veil-type\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c869a30c1c9b427faee41d0ff23b953bd56e174"},"cell_type":"markdown","source":"### Creating dataset splits\n\nIt is important to use the data we have available wisely. Here we got a dataset with 8124 samples, so what we are going to do is split this in training set, development set (also known as validation set) and test set. This will help us detecting problems like high variance and high bias, helping us to deal with underfitting or overfitting.\n\nWe will train our model with the training set, then the dev set will help us to tune the hyperparameters (learning rate, mini-batch size, regularization parameters, etc.). The last step is to evaluate our model on a data it has never seen, our test set. \n\nBy comparing the performance (accuracy and cost) in all these sets, we can see if our model is generalizing well the dataset.\n\nThe sets are going to be divided as follows:\n* training set: 60% --> 4874 samples\n* dev set: 20%  -------> 1625 samples\n* test set: 20% -------> 1625 samples\n\n"},{"metadata":{"trusted":true,"_uuid":"61f0ca4b8d2e868241b622a2579ab916b98c21f1"},"cell_type":"code","source":"# Calculating splits\ntrain_size = int(8124*.60)\ndev_size = (8124 - train_size) // 2\ntest_size = (8124 - (train_size+dev_size))\nprint(train_size, dev_size, test_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe2b021e441973805001edae67e18e32e0e5299d"},"cell_type":"code","source":"X_train = X_data.values[:train_size,:]\nY_train = Y_labels.values[:train_size,:]\nX_dev = X_data.values[train_size:train_size+dev_size,:]\nY_dev = Y_labels.values[train_size:train_size+dev_size,:]\nX_test = X_data.values[train_size+dev_size:,:]\nY_test = Y_labels.values[train_size+dev_size:,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"658628b872a664264a6a08b8c9abca53696e9f10"},"cell_type":"markdown","source":"## Neural Network Model"},{"metadata":{"_uuid":"2f195c723dbcb2ab82d45832f07c76a6478762e4"},"cell_type":"markdown","source":"This model helps us to see the basics functions in a feed-forward part of a neural network\n\n* ***parameters* function:** initialize the Weights (W) and biases for a new layer, for that you need to provide the expected **input shape** for that layer and the **layer_name**\n*  ***linear_op* function:** receives the **Weights (W)**, **Biases (b)** and the **Input** from the previous layer **(X)**.\n*  ***activation* function:** receives as input the output values of the linear function and the activation function that will be applied to these values.\n*  ***layer* function:** using the previous functions, it creates a layer Neural Network layer. It receives the values from the previous layer or input **X**, the intended **shape** for that layer, the **layer name** and an **activation** function.\n* ***basic_model* function:** Receives the **Input (X)** for the network. Here we define our Neural Network model which in this case has 4 hidden layers\n\n**note:** These functions allow only densely connected layers"},{"metadata":{"trusted":true,"_uuid":"e5e59cb7c313465c6af3c98878e1ae7037ec94a9"},"cell_type":"code","source":"def parameters(shape, layer_name):\n    \"\"\" Initialize the parameters W and b of a layer with name ´layer_name´ and a shape ´shape´. \"\"\"\n    W = tf.get_variable(shape=shape, initializer=tf.glorot_normal_initializer(3),\n                        regularizer=tf.contrib.layers.l2_regularizer(0.2), name=layer_name+\"_weight\")\n    b = tf.Variable(tf.constant(0.1, shape=[shape[1]]), name=layer_name+\"bias\")\n    return W,b\n        \ndef linear_op(W,X,b):\n    \"\"\" Run a linear function ´X´ * ´W´ + ´b´ \"\"\"\n    linear = tf.add(tf.matmul(X,W), b)\n    return linear\n\ndef activation(linear, activate):\n    \"\"\" Apply an activation function ´activate´ in the value passed to ´linear´ \"\"\"\n    activation = activate(linear)\n    return activation\n\ndef layer(X, shape, layer_name, activate=tf.nn.relu):\n    \"\"\" Create a neural network layer \"\"\"\n    W, b = parameters(shape, layer_name)\n    linear = linear_op(W,X,b)\n    activation = activate(linear)\n    return activation\n\ndef basic_model(X):\n    \"\"\" Create a neural network model in the current Graph \"\"\"\n    graph = tf.get_default_graph()\n    with graph.as_default():\n        l = layer(X, [22, 32], 'hidden1')\n        l = layer(l, [32, 16], 'hidden3')\n        l = layer(l, [16, 8], 'hidden4')\n        output = layer(l, [8, 2], 'output', activate=tf.identity)\n    \n    return output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6e9dce57baf226ba8421c0966271e13da0c945e"},"cell_type":"markdown","source":"Same as above, but using predefined Tensorflow layers. We will see in the results that this have a better performance than our hand made model (basic_model)."},{"metadata":{"trusted":true,"_uuid":"9920334f0c446349bb0c6a7fcd2f94bf07cd0472"},"cell_type":"code","source":"def other_model(X):\n    \"\"\" Create a neural network model in the current Graph with tensorflow layers \"\"\"\n    graph = tf.get_default_graph()\n    with graph.as_default():\n        l = tf.keras.layers.Input(shape=(22,), tensor=X)\n        l = tf.layers.dense(l, 32, activation=tf.nn.relu, kernel_initializer=tf.glorot_normal_initializer(3),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(0.2))\n        l = tf.layers.dense(l, 16, activation=tf.nn.relu, kernel_initializer=tf.glorot_normal_initializer(3),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(0.2))\n        l = tf.layers.dense(l, 8, activation=tf.nn.relu, kernel_initializer=tf.glorot_normal_initializer(3),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(0.2))\n        output = tf.layers.dense(l, 2, activation=tf.identity, kernel_initializer=tf.glorot_normal_initializer(3),\n                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(0.2))\n\n    return output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c76915d11454e1ff1faec8fb3ef4681e7ca33234"},"cell_type":"markdown","source":"A one layer model"},{"metadata":{"trusted":true,"_uuid":"f792a1de6b2bc2c701b06aeb67fcf9654dccab80"},"cell_type":"code","source":"def simpler_model(X):\n    \"\"\" Create a simple neural network model in the current Graph with tensorflow layers \"\"\"\n    graph = tf.get_default_graph()\n    with graph.as_default():\n        l = tf.keras.layers.Input(shape=(22,), tensor=X)\n        output = tf.layers.dense(l, 2, activation=tf.identity, kernel_initializer=tf.glorot_normal_initializer(3))\n\n    return output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38d0f40a3d20b99bd5d8e357e0992174ce59e5e5"},"cell_type":"markdown","source":"### Mini-batches\n\nHere we are dividing our training set in mini-batches of size 128. This is meant to make our training faster and with better results."},{"metadata":{"trusted":true,"_uuid":"64b82254986925e91525e973a5a09d3a9be9bfba"},"cell_type":"code","source":"batch = 128\n\nbatches = []\nfor j in range(0, X_train.shape[0], batch):\n    X_train_batch = X_train[j:j+batch,:]\n    Y_train_batch = Y_train[j:j+batch,:]\n    batches.append((X_train_batch, Y_train_batch))\n\nbatches = tuple(batches)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"226961e035da3c199d4672714256c6dd485c9ddf"},"cell_type":"markdown","source":"### Graph builder\nWhen testing models it is common to rerun the building function of the model (like our ***model()*** and ***other_model()*** fuctions) after making changes. If we rerun that builder without starting a new graph it will raise an error saying we already have an element with that name in the graph. This happens because we are using the default graph provided by Tensorflow.\n\nI don't know if there is a better way for solving this problem, but I decided to start a new graph everytime I made a change in the models. After that I had to get the default graph of the current scope inside the model functions with the ***tf.get_default_graph()***  tensorflow method (look at ***model()*** and ***other_model()*** functions for reference)."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7c0027e297eff1257a66b095bb6ffa9877bfac0f"},"cell_type":"code","source":"def build_graph(model_):\n    \"\"\" Create a new graph for training and evaluating models \"\"\"\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input and Labels\n        X = tf.placeholder(tf.float32, [None, 22], name=\"Input\")\n        Y = tf.placeholder(tf.float32, [None, 2], name=\"Labels\")\n\n        # Neural Network\n        output = model_(X)\n\n        # Operation for calculating accuracy\n        with tf.name_scope(\"accuracy\"):\n            with tf.name_scope(\"correct_predictions\"):\n                correct = tf.equal(tf.argmax(output,1),tf.argmax(Y,1))\n            with tf.name_scope(\"accuracy\"):\n                accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n\n        # Cost OP\n        with tf.name_scope(\"cost\"):\n            cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=Y)\n            cost = tf.reduce_mean(cost,axis=0)\n        # Train OP\n        with tf.name_scope(\"optimization\"):\n            global_step = tf.Variable(0, trainable=False)\n            initial_learning_rate = 0.03\n            learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, 2048, decay_rate=.97, staircase=True)\n            train = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n        \n        # Initialize variables\n        init = tf.global_variables_initializer()        \n    \n        return graph, init, X, Y, learning_rate, cost, accuracy, train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f67422ec1d9d1745563e77e943c3dea3d1a11e7b"},"cell_type":"markdown","source":"### Session\n\nSessions are how tensorflow run and evaluate the graphs.\n\nA Session with our last graph started is created here, then we initialize our variables and run our training steps, always keeping track of the current model performance. The perfomance in the test set is printed here but it is not meant to be when we are still tuning and developing our model. The best practice is to look at the training and dev set performance while tunning and just after finnishing this tunning, evaluate the model in the test set and compare the results. Since our models are performing well already, we can look at the test set."},{"metadata":{"trusted":true,"_uuid":"2c53621508da7c8cf2cb2a27740e3445a200c93d"},"cell_type":"code","source":"epochs = 64\nmdls = [basic_model, other_model, simpler_model]\nfor mdl in mdls:\n    print(f\"\\nModel used: {mdl.__name__}\\t\\t#Epochs: {epochs}\")\n    graph, init, X, Y, learning_rate, cost, accuracy, train = build_graph(mdl)\n    # Session\n    with tf.Session(graph=graph) as sess:\n        sess.run(init)\n\n        # Train iterations\n        for i in range(epochs):\n            if i%32 == 0:\n                lr, cos_dev, acc_dev = sess.run([learning_rate, cost, accuracy], feed_dict={X: X_dev, Y: Y_dev})\n                cos_train, acc_train = sess.run([cost, accuracy], feed_dict={X: X_train, Y: Y_train})\n                print(f\"epoch {i:4d}: train_cost= {cos_train:0.10f} | train_accuracy= {acc_train:0.2f} || dev_cost= {cos_dev:0.10f} | dev_accuracy= {acc_dev:0.2f} | lr: {lr:0.10f}\")\n\n            # Running training steps through our mini-batches\n            for X_batch, Y_batch in batches:\n                # Training Step\n                sess.run([train], feed_dict={X: X_batch, Y: Y_batch})\n        \n        # Evaluate the last learing rate and our model cost and accuracy with train, dev and test sets\n        lr, cos_train, acc_train = sess.run([learning_rate, cost, accuracy], feed_dict={X: X_train, Y: Y_train})\n        cos_dev, acc_dev = sess.run([cost, accuracy], feed_dict={X: X_dev, Y: Y_dev})\n        cos_test, acc_test = sess.run([cost, accuracy], feed_dict={X: X_test, Y: Y_test})\n        \n        # Print the last training step values\n        print(f\"epoch {epochs-1:4d}: train_cost= {cos_train:0.10f} | train_accuracy= {acc_train:0.2f} || dev_cost= {cos_dev:0.10f} | dev_accuracy= {acc_dev:0.2f} | lr: {lr:0.10f}\")\n        \n        # Print the performance in all sets\n        print(f\"\\nPERFORMANCE:\\ntrain_cost\\t= {cos_train:0.10f} | train_accuracy\\t= {acc_train:0.2f} |\\ndev_cost\\t= {cos_dev:0.10f} | dev_accuracy\\t= {acc_dev:0.2f} |\\ntest_cost\\t= {cos_test:0.10f} | test_accuracy\\t= {acc_test:0.2f} |\")\n        print(\"=\"*96)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"893bfc044f0e5e4130d32500038e88f1d1534223"},"cell_type":"markdown","source":"## Observations\n\nThis is a very simple problem and our models are quite an overkill to it. In fact, with very simple neural network model, without tuning its parameters and using the same we used for the other models we achieved 97% accuracy. There are also different machine learining models that are not neural networks that can achieve 100% accuracy easily and providing better intuition for the analysis process."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}