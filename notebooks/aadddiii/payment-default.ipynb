{"cells":[{"metadata":{"_uuid":"09c2e954471f1cfa29138363be91bda4e073d3f3"},"cell_type":"markdown","source":"##### \nThis notebook attempts to find the best classification algorithm to predict credit card payment defaults.\n\nThe data set is available in UCI repository: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n     \nClassification algorithms used for model estimations are:\n\n1. LogisticRegression\n2. Decision Tree\n3. Random Forest\n4. K Nearest Neighbor\n5. SVM\n        \n        \n#### GOAL: Notebook should be able to conclude which particular model performs best for the given data set with justifications        "},{"metadata":{"trusted":true,"_uuid":"14c654894028bd6f5ec4ed0a8e6fc63d3dac7c90"},"cell_type":"code","source":"import pandas as panda\n\nfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, classification_report, confusion_matrix, f1_score, roc_curve, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom matplotlib import pyplot as plot\nimport seaborn as sns\n\n\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\n\nimport itertools, time, datetime\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6e727e631dd57c564cc1b71b47963f40d1ed09"},"cell_type":"code","source":"remote_location = \"../input/UCI_Credit_Card.csv\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e65655c3080159d98f8a0ee58727cd199096f30"},"cell_type":"code","source":"def downLoadData():\n    \"\"\"\n    \n    Downloads the excel data from remote location. Reads one particular sheet, converts all columns names \n    to lower case and then returns the data\n    \n    \"\"\"\n\n    data = panda.read_csv(remote_location)\n\n    data.rename(str.lower, inplace = True, axis = 'columns')\n\n    print(data.dtypes)\n\n    return data\n\ndata = downLoadData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ee9d8088289658fbd3fa4f8d83f7fc1f858f4f5"},"cell_type":"code","source":"## check for varied data types. there may be alphabetical data types or numeric data written as string eg \"4\"\".\n## in such cases reformatting may be required\nprint(data.dtypes.value_counts()) ## all values are numeric and no formatting of data types are required in that case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8d05c8550037d566c1ba770c7897af20810307"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e007137f2d4d1d46df7adc20088c0b1b55448d3d"},"cell_type":"code","source":"\n## since all fields are numeric we can get away with normal describe. else we would have required describe(include='all')\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"420eca9281c013a4a5eb21d7d2f572635ed6e793"},"cell_type":"code","source":"print(data['default.payment.next.month'].value_counts())\n\ndata.drop(['id'], inplace=True, axis =1)\n\n'id' not in data.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ab9ed024a81d866ce30c2874ccbbd906c17af4"},"cell_type":"code","source":"## divide up our x and y axis\n\n_y_target = data['default.payment.next.month'].values\n\ncolumns = data.columns.tolist()\ncolumns.remove('default.payment.next.month')\n\n_x_attributes = data[columns].values\n\n\n## meaning of stratify = _y_target. returns test and training data having the same proportions of class label '_y_target'\n_x_train,_x_test,_y_train, _y_test = train_test_split(_x_attributes, _y_target, test_size =0.30, stratify = _y_target, random_state = 1)\n\n## lets check the distribution. we can see 4times the lower value as was the case before as well. train/test set distributed well\nprint(\"label counts in y train %s\" %bincount(_y_train))\nprint(\"label counts in y test %s\" %bincount(_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40396d46c68a759e057c247defaee017df04886e"},"cell_type":"code","source":"class CodeTimer:\n    \n    \"\"\"\n        Utility custom contextual class for calculating the time \n        taken for a certain code block to execute\n    \n    \"\"\"\n    def __init__(self, name=None):\n        self.name = \" '\"  + name + \"'\" if name else ''\n\n    def __enter__(self):\n        self.start = time.clock()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (time.clock() - self.start) * 1000.0\n        time_taken = datetime.timedelta(milliseconds = self.took)\n        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78ec6236871c418ea4514584483a69e681cfdb3c"},"cell_type":"code","source":"## cv is essentially value of K in k fold cross validation\n    \n## n_jobs = 1 is  non parallel execution    , -1 is all parallel , any other number say 2 means execute in 2 cpu cores\n\ndef plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n    \n    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n                                                                X = _x_train, \\\n                                                                y = _y_train, \\\n                                                                train_sizes = training_sample_sizes, \\\n                                                                cv = k_fold, \\\n                                                                n_jobs = jobsInParallel) \n\n\n    training_mean = mean(training_score, axis = 1)\n    training_std_deviation = std(training_score, axis = 1)\n    testing_std_deviation = std(testing_score, axis = 1)\n    testing_mean = mean(testing_score, axis = 1 )\n\n\n\n    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n\n    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n\n    plot.title(\"Scoring of our training and testing data vs sample sizes\")\n    plot.xlabel(\"Number of Samples\")\n    plot.ylabel(\"Accuracy\")\n    plot.legend(loc= 'best')\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b98b11962aa0f3e83177a02f4b5f7bcd32fffe03"},"cell_type":"code","source":"def runGridSearchAndPredict(pipeline, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n    \n    response = {}\n    training_timer       = CodeTimer('training')\n    testing_timer        = CodeTimer('testing')\n    learning_curve_timer = CodeTimer('learning_curve')\n    predict_proba_timer  = CodeTimer('predict_proba')\n    \n    with training_timer:\n        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n\n        search = gridsearch.fit(x_train,y_train)\n\n        print(\"Grid Search Best parameters \", search.best_params_)\n        print(\"Grid Search Best score \", search.best_score_)\n            \n    with testing_timer:\n        y_prediction = gridsearch.predict(x_test)\n            \n    print(\"Accuracy score %s\" %accuracy_score(y_test,y_prediction))\n    print(\"F1 score %s\" %f1_score(y_test,y_prediction))\n    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n    \n    with learning_curve_timer:\n        plotLearningCurve(_x_train, _y_train, search.best_estimator_)\n        \n    with predict_proba_timer:\n        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n            \n            y_probability = gridsearch.predict_proba(x_test)\n            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:,1])\n            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n            response['roc_curve'] = (false_positive_rate, true_positive_rate)\n    \n        else: \n            \n            response['roc_auc_score'] = 0\n            response['roc_curve'] = None\n    \n    response['learning_curve_time'] = learning_curve_timer.took\n    response['testing_time'] = testing_timer.took\n    response['_y_prediction'] = y_prediction\n    response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n    response['training_time'] = training_timer.took\n    response['f1_score']  = f1_score(y_test, y_prediction)\n    \n    \n    return response\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0011c1237ba984f1735d6584b288521ea4be9c"},"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plot.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n    plot.title(title)\n    plot.colorbar()\n    tick_marks = arange(len(classes))\n    plot.xticks(tick_marks, classes, rotation=45)\n    plot.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plot.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plot.ylabel('True label')\n    plot.xlabel('Predicted label')\n#     plot.tight_layout()\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a86ddacc1c51ded5fd16d3e73ef57a4632b1f501"},"cell_type":"code","source":"\n\nclassifiers = [\n\n    LogisticRegression(random_state = 1),\n    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),\n    RandomForestClassifier(random_state = 1, criterion = 'gini'),\n    KNeighborsClassifier(metric = 'minkowski'),\n    SVC(random_state = 1, kernel = 'rbf'),    \n]\n\n\nclassifier_names = [\n          \n            'logisticregression',\n            'decisiontreeclassifier',\n            'randomforestclassifier',\n            'kneighborsclassifier',\n            'svc',               \n    \n]\n\nclassifier_param_grid = [\n            \n           \n            {'logisticregression__C':[100,200,300,50,20,600]},\n            {'decisiontreeclassifier__max_depth':[6,7,8,9,10,11]},\n            {'randomforestclassifier__n_estimators':[1,2,3,5,6]} ,\n            {'kneighborsclassifier__n_neighbors':[4,6,7,8]},\n            {'svc__C':[1], 'svc__gamma':[0.01]},\n    \n]\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde3b6981c3d81bccce7512d2e6100d0132646aa"},"cell_type":"code","source":"\ntimer = CodeTimer(name='overalltime')\nmodel_metrics = {}\n\nwith timer:\n    for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n\n        pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                (model_name, model)\n        ])\n\n        result = runGridSearchAndPredict(pipeline, _x_train, _y_train, _x_test, _y_test, model_param_grid , score = 'f1')\n\n        _y_prediction = result['_y_prediction']\n\n        _matrix = confusion_matrix(y_true = _y_test ,y_pred = _y_prediction)\n\n        model_metrics[model_name] = {}\n        model_metrics[model_name]['confusion_matrix'] = _matrix\n        model_metrics[model_name]['training_time'] = result['training_time']\n        model_metrics[model_name]['testing_time'] = result['testing_time']\n        model_metrics[model_name]['learning_curve_time'] = result['learning_curve_time']\n        model_metrics[model_name]['accuracy_score'] = result['accuracy_score']\n        model_metrics[model_name]['f1_score'] = result['f1_score']\n        model_metrics[model_name]['roc_auc_score'] = result['roc_auc_score']\n        model_metrics[model_name]['roc_curve'] = result['roc_curve']\n        \n        \nprint(timer.took)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d016e8658fcae004f0ff1a80e69318634ebd682d"},"cell_type":"code","source":"\n\nmodel_estimates = panda.DataFrame(model_metrics).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb2848ead02de59914b3f6534e61fc0a4718e00"},"cell_type":"code","source":"\n## convert model_metrics into panda data frame\n## print out across model estimations and accuracy score bar chart\n\n\nmodel_estimates['learning_curve_time'] = model_estimates['learning_curve_time'].astype('float64')\nmodel_estimates['testing_time'] = model_estimates['testing_time'].astype('float64')\nmodel_estimates['training_time'] = model_estimates['training_time'].astype('float64')\nmodel_estimates['f1_score'] = model_estimates['f1_score'].astype('float64')\nmodel_estimates['roc_auc_score'] = model_estimates['roc_auc_score'].astype('float64')\n\n#scaling time parameters between 0 and 1\nmodel_estimates['learning_curve_time'] = (model_estimates['learning_curve_time']- model_estimates['learning_curve_time'].min())/(model_estimates['learning_curve_time'].max()- model_estimates['learning_curve_time'].min())\nmodel_estimates['testing_time'] = (model_estimates['testing_time']- model_estimates['testing_time'].min())/(model_estimates['testing_time'].max()- model_estimates['testing_time'].min())\nmodel_estimates['training_time'] = (model_estimates['training_time']- model_estimates['training_time'].min())/(model_estimates['training_time'].max()- model_estimates['training_time'].min())\n\nprint(model_estimates)\nmodel_estimates.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a90647f4150dec5bca187fcdf4af8d16e010992"},"cell_type":"code","source":"def plotROCCurveAcrossModels(positive_rates_sequence, label_sequence):\n    \n\n    for plot_values, label_name in zip(positive_rates_sequence, label_sequence):\n        \n        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n        \n    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n    plot.title('ROC Curve across models')\n    plot.xlabel('False Positive Rate')\n    plot.ylabel('True Positive Rate')\n    plot.legend(loc='best')\n    plot.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfcb230ed15cbab733e7cd419c74e99d3f1baffc"},"cell_type":"code","source":"\nroc_curve_input = {}\nfor i , j in enumerate(model_metrics):\n    \n    _matrix = model_metrics[j]['confusion_matrix']\n    plot_confusion_matrix(_matrix, classes = [0,1], title = 'Confusion Matrix for %s'%j)\n    if model_metrics[j]['roc_curve']:\n        roc_curve_input[j]= model_metrics[j]['roc_curve']\n    \n\nplotROCCurveAcrossModels(list(roc_curve_input.values()), list(roc_curve_input.keys()))\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"863e657a031b37fc017c00690c727f4cb849dd29"},"cell_type":"markdown","source":"### Conclusion\n\nBased on our datasets, the model selected for best performance would be Decision Tree with a max depth of 7.\nThese are the metrics we get for DecisionTreeClassifier:\n\n1. accuracy score : 0.81\n\n2. f1 score: 0.47\n\n3. roc_auc_score : 0.75\n\n4. training time: 29 secs\n\n5. testing time: miniscule\n\n\n"},{"metadata":{"trusted":true,"_uuid":"1fefa6c063f09236ab1c88e7ae3749f86e1fff96"},"cell_type":"code","source":"classifier = DecisionTreeClassifier(random_state = 1, criterion = 'gini', max_depth = 7, class_weight = 'balanced')\n\npipeline = Pipeline([('scaler', StandardScaler()), ('decisiontreeclassifier', classifier)])\n\npipeline.fit(_x_train,_y_train)\n\ny_prediction = pipeline.predict(_x_test)\n\ny_probability = pipeline.predict_proba(_x_test)\n\nprint(\"Accuracy score accounting for unbalanced classes\", accuracy_score(_y_test, y_prediction))\nprint(\"F1 score accounting for unbalanced classes\", f1_score(_y_test, y_prediction))\nprint(\"ROC AUC score accounting for unbalanced classes\", roc_auc_score(_y_test, y_probability[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c74e98bdd4c7cc3336e4d5aafcbd8a53c3d2f549"},"cell_type":"markdown","source":"Seeing that we have not been able to increase our estimates as much , lets see if we can boost our model using any of the boosting techniques\n"},{"metadata":{"trusted":true,"_uuid":"fd8a1af19c32221874922ceb4767b4bda2ab4aff"},"cell_type":"code","source":"\nfrom sklearn.ensemble import  AdaBoostClassifier\n\nboostClassifier = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state = 1, criterion = 'gini', max_depth = 7, class_weight = 'balanced'), random_state = 1)\n\npipeline = Pipeline([('scaler', StandardScaler()), ('boostClassifier', boostClassifier)])\n\nparam_grid = [{'boostClassifier__n_estimators':[ 500], 'boostClassifier__learning_rate' :[0.1]}]\n\nresult = runGridSearchAndPredict(pipeline, _x_train, _y_train, _x_test, _y_test, param_grid , score = 'f1')\n\nprint(\"Accuracy score accounting for unbalanced classes\", result['accuracy_score'])\nprint(\"F1 score accounting for unbalanced classes\", result['f1_score'])\nprint(\"ROC AUC score accounting for unbalanced classes\", result['roc_auc_score'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}