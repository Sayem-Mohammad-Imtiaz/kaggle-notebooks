{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INDEX\n- ## 1. Library Management\n- ## 2. Data Sourcing\n- ## 3. Data Cleaning\n- ## 4. Data Preparation (For Modelling)\n- ## 5. Data Modelling"},{"metadata":{},"cell_type":"markdown","source":"# 1. Library Management"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nsns.set(rc={'figure.figsize': (15, 10)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Sourcing"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\nraw_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_row_count, raw_data_column_count = raw_data.shape\nprint('Row Count:', raw_data_row_count)\nprint('Column Count:', raw_data_column_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. ID Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.id.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(raw_data.id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We see that number number of unique values equals total items in the row.\n##### This can also indicate that, it might be Patient / Customer ID. We will drop this, as we already have a unique identifier for the dataframe\n##### Thus, we will drop this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = raw_data.drop(columns='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.gender.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Gender needs to be categorized as Categorical Nominal Variable. For this, we would be using Dummy Variable Method.\n##### Also, from the analysis perspective, it will be tedious to create another dummy variable just for one row vlue (of Others). Therefore, we will impute this other value with mode in this column.\n#### Therefore, conversion will be as follows:\n- 1. Male: 1\n- 2. Female: 0\n- 3. Others: Mode Value of column"},{"metadata":{},"cell_type":"markdown","source":"##### Replacing Other value with mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['gender'] = raw_data['gender'].replace('Other', list(raw_data.gender.mode().values)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Verifying if the value was imputed appropriately"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.gender = raw_data.gender.map({'Male': 1, 'Female': 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.age.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.age.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.cut(raw_data['age'], bins=np.arange(0, 100, 10)).value_counts(sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(raw_data.age)\nplt.title('Age Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('Age', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.age)\nplt.title('Age Distribution Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('Age', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4. HyperTension"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.hypertension.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.hypertension.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Nothing needs to be done with this data column"},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Heart Disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.heart_disease.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.heart_disease.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Nothing needs to be done with this data column"},{"metadata":{},"cell_type":"markdown","source":"## 3.6. Ever Married"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.ever_married.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.ever_married.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.ever_married = raw_data.ever_married.map({'Yes': 1, 'No': 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.7. Work Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.work_type.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.work_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### This data represent a bit like categorical nominal variable. Hence we will keep them as it is."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_train_df = pd.get_dummies(raw_data['work_type'], drop_first=True)\nraw_data = pd.concat([raw_data, dummy_train_df], axis=1)\nraw_data = raw_data.drop(columns=['work_type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.8. Residence Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.Residence_type.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.Residence_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.Residence_type = raw_data.Residence_type.map({'Rural': 0, 'Urban': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.9. Average Glucose Level"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.avg_glucose_level.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.avg_glucose_level.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.avg_glucose_level.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.cut(raw_data['avg_glucose_level'], bins=np.arange(50, 300, 25)).value_counts(sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(raw_data.avg_glucose_level)\nplt.title('Average Glucose Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('Average Glucose', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.avg_glucose_level)\nplt.title('Average Glucose Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('Average Glucose', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.10. BMI"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.bmi.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.bmi.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.bmi.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Count of missing values is bit high enough to drop respective value. for the same, we will use he mean value to impute these null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[\"bmi\"].fillna(raw_data.bmi.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.bmi.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.cut(raw_data['bmi'], bins=np.arange(10, 110, 10)).value_counts(sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(raw_data.bmi)\nplt.title('BMI Distribution Plot', fontdict={'fontsize': 20})\nplt.xlabel('BMI', fontdict={'fontsize': 12})\nplt.ylabel('Patient Count', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(raw_data.bmi)\nplt.title('BMI Box Plot', fontdict={'fontsize': 20})\nplt.xlabel('BMI', fontdict={'fontsize': 12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the box plot, we can see that there are many outliers prsent in the higher region. Technically we can neglect the top oultier to predict output.\n##### But from application perspective, we can have pateints with that BMI level and if we neglect these values, our model wont exterpolate higher values. Therefore, we will continue with this missing values."},{"metadata":{},"cell_type":"markdown","source":"## 3.11. Smoking Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.smoking_status.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.smoking_status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### These values dont have any definate order. Hence we will asume them to be Categorical Nominal Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_train_df = pd.get_dummies(raw_data['smoking_status'], drop_first=True)\nraw_data = pd.concat([raw_data, dummy_train_df], axis=1)\nraw_data = raw_data.drop(columns=['smoking_status'])\nraw_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.12. Stroke  Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.stroke.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.stroke.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 4. Data Preparation (For Modelling)"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Defining Input / Output Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = raw_data.drop(columns=['stroke'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = raw_data.stroke","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Splitting Train & Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Data Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# std_scaler = StandardScaler()\n# X_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns = X_train.columns)\n# X_test = pd.DataFrame(std_scaler.transform(X_test),columns = X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Modelling"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Data Modelling on Trained Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel = model.fit(X_train, y_train)\npred_probs_train = model.predict_proba(X_train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame(y_train)\ny_train_pred_final['stroke_probability'] = pred_probs_train[:,1]\nnumbers = np.arange(0.0, 1.0, 0.001)\nfor i in numbers:\n    y_train_pred_final[i] = y_train_pred_final.stroke_probability.map(lambda x: 1 if x > i else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoff_df = pd.DataFrame(columns=['prob', 'accuracy', 'sensi', 'speci'])\nfor i in numbers:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.stroke, y_train_pred_final[i])\n    accuracy = (cm1[0, 0] + cm1[1, 1]) / sum(sum(cm1))\n    speci = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n    sensi = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n    cutoff_df.loc[i] = [i, accuracy, sensi, speci]\ncutoff_df[(cutoff_df['sensi'] < 0.8) & (cutoff_df['sensi'] > 0.7)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.xlabel('Probability', fontdict={'fontsize': 15})\nplt.title('Cut-Off for Logisitic Regression Model', fontdict={'fontsize': 20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the above graph, probability cut off of 0.064 seems to be respectable enough to behave as threshold value above which patient is likely to have stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_off = 0.064","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix = metrics.confusion_matrix(y_train_pred_final.stroke, y_train_pred_final[cut_off])\n\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\nFN = conf_matrix[1, 0]\nTP = conf_matrix[1, 1]\n\naccuracy_score = metrics.accuracy_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\naccuracy_score = round(accuracy_score*100, 2)\n\nprecision_score = metrics.precision_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nprecision_score = round(precision_score*100, 2)\n\nrecall_score = metrics.recall_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nrecall_score = round(recall_score*100, 2)\n\nsensitivity = TP / float(FN + TP)\nsensitivity = round(sensitivity*100, 2)\n\nspecificity = TN / float(TN + FP)\nspecificity = round(specificity*100, 2)\n\nf1_score = metrics.f1_score(y_train_pred_final.stroke, y_train_pred_final[cut_off])\nf1_score = round(f1_score*100, 2)\n\nauc_score = metrics.roc_auc_score(y_train_pred_final.stroke, y_train_pred_final.stroke_probability)\nauc_score = round(auc_score*100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.stroke, y_train_pred_final.stroke_probability, drop_intermediate=False )\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]', fontdict={'fontsize': 15})\nplt.ylabel('True Positive Rate', fontdict={'fontsize': 15})\nplt.title('ROC (Receiver Operating Characteristic) Curve - Train Data\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({'Parameter': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'],\n                     'Value': [accuracy_score, sensitivity, specificity, precision_score, recall_score, f1_score, auc_score]}, index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'])   \ndata = data.groupby(by='Parameter').Value.sum().sort_index()\ngraph = sns.barplot(x=data.index, y=data.values)\nplt.title('Model Metrices (With Train Data)\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.xlabel('Parameters', fontdict={'fontsize': 15})\nplt.ylabel('Score Value (In Percent)', fontdict={'fontsize': 15})\nlabel_deviation_above_y_axis = data.max() * 0.015\nfor index, value in enumerate(data.iteritems()):\n    graph.text(index, value[1] + label_deviation_above_y_axis, str(round(value[1], 1))+'%', color='black', ha=\"center\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Data Modelling on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_probs_test = model.predict_proba(X_test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_final = pd.DataFrame(y_test)\ny_test_pred_final['stroke_probability'] = pred_probs_test[:,1]\ny_test_pred_final['stroke_predicted'] = y_test_pred_final.stroke_probability.map(lambda x: 1 if x > cut_off else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix_test = metrics.confusion_matrix(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\n\nTN_test = conf_matrix[0, 0]\nFP_test = conf_matrix[0, 1]\nFN_test = conf_matrix[1, 0]\nTP_test = conf_matrix[1, 1]\n\naccuracy_score_test = metrics.accuracy_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\naccuracy_score_test = round(accuracy_score_test*100, 2)\n\nprecision_score_test = metrics.precision_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nprecision_score_test = round(precision_score_test*100, 2)\n\nrecall_score_test = metrics.recall_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nrecall_score_test = round(recall_score_test*100, 2)\n\nsensitivity_test = TP_test / float(FN_test + TP_test)\nsensitivity_test = round(sensitivity_test*100, 2)\n\nspecificity_test = TN_test / float(TN_test + FP_test)\nspecificity_test = round(specificity_test*100, 2)\n\nf1_score_test = metrics.f1_score(y_test_pred_final.stroke, y_test_pred_final.stroke_predicted)\nf1_score_test = round(f1_score_test*100, 2)\n\nauc_score_test = metrics.roc_auc_score(y_test_pred_final.stroke, y_test_pred_final.stroke_probability)\nauc_score_test = round(auc_score_test*100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test_pred_final.stroke, y_test_pred_final.stroke_probability, drop_intermediate=False )\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score_test)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]', fontdict={'fontsize': 15})\nplt.ylabel('True Positive Rate', fontdict={'fontsize': 15})\nplt.title('ROC (Receiver Operating Characteristic) Curve - Test Data\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({'Parameter': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'],\n                     'Value': [accuracy_score_test, sensitivity_test, specificity_test, precision_score_test, recall_score_test, f1_score_test, auc_score_test]}, index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision Score', 'Recall Score', 'F1 Score', 'AUC Score'])   \ndata = data.groupby(by='Parameter').Value.sum().sort_index()\ngraph = sns.barplot(x=data.index, y=data.values)\nplt.title('Model Metrices (With Test Data)\\nLogisitic Regression Model', fontdict={'fontsize': 20})\nplt.xlabel('Parameters', fontdict={'fontsize': 15})\nplt.ylabel('Score Value (In Percent)', fontdict={'fontsize': 15})\nlabel_deviation_above_y_axis = data.max() * 0.015\nfor index, value in enumerate(data.iteritems()):\n    graph.text(index, value[1] + label_deviation_above_y_axis, str(round(value[1], 1))+'%', color='black', ha=\"center\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_score_df = pd.DataFrame({'train': [accuracy_score, sensitivity, specificity, precision_score, recall_score, f1_score, auc_score], \n                                    'test': [accuracy_score_test, sensitivity_test, specificity_test, precision_score_test, recall_score_test, f1_score_test, auc_score_test]},\n                                   index=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1 Score', 'AUC Score'])   \n\nfig, ax = plt.subplots()\nx = np.arange(len(train_test_score_df.index))\nwidth = 0.35\n\n\nax.set_ylabel('Score (In %)', fontdict={'fontsize': 15})\nax.set_xlabel('Parameters', fontdict={'fontsize': 15})\nax.set_title('Logisitic Regression Model', fontdict={'fontsize': 20})\nax.set_xticks(x)\nax.set_xticklabels(train_test_score_df.index)\n\nrects1 = ax.bar(x - width/2, train_test_score_df['train'], width, label='Train Score')\nfor rect in rects1:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n\nrects2 = ax.bar(x + width/2, train_test_score_df['test'], width, label='Test Score')\nfor rect in rects2:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Model Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}