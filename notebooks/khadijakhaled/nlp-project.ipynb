{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob # finds all pathnames matching a specified pattern\nimport json\nimport nltk\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Input data files are available in the \"../input/\" directory.\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# loading the metadata into a dataframe\n\nmetadata_path = '/kaggle/input/CORD-19-research-challenge/metadata.csv'\n\n# dtype indicates the type of the specified columns (all ids are strings)\nmetadata = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\n# head shows the first 5 rows of the dataframe\nmetadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the index of the metadata will be the unique cord_uid of the papers\n\nmetadata.index = metadata['cord_uid']\n\nmetadata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# info gives a statistical overview of the dataframe\nmetadata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata['abstract'].describe(include = 'all')\n\n# statistics on the abstract column show us that there are some abstract duplicates\n# the authors may have submitted their papers to more than 1 journal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the duplicate entries\n\nmetadata.drop_duplicates(['abstract'], inplace = True)\nmetadata.drop_duplicates(['cord_uid'], inplace = True)\n\nmetadata['abstract'].describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all entries with null abstracts\n\nmetadata = metadata[metadata.abstract.notnull()]\n\nmetadata.info()\n\n# now all our text entries have unique non-null abstracts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data preprocessing\n\n# step 1: specifying the language of each text entry\n\n# installing the langdetect module\n!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_index = []\n\nfor x in metadata.index:\n    metadata_index.append(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# holds the language label for each metadata entry\nlanguages = []\n\n# loop through each entry in the metadata and detect the language of its abstract\nfor x in tqdm(range(len(metadata_index))):\n    \n    try:\n        lang = detect(metadata['abstract'][metadata_index[x]])\n        \n        languages.append(lang)\n          \n    except Exception as e:\n        languages.append(\"unknown\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dictionary with the number of occurances of each language in our dataset\n\nlanguages_dict = {}\n\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(languages_dict) # the majority of papers are in english","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a language column to our metadata dataframe\nmetadata['language'] = languages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all entries that are not english\n\nmetadata = metadata[metadata['language'] == 'en']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the language column\n\nmetadata = metadata.drop(['language'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install spacy; a package for natural language processing\n\n!pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data preprocessing\n# step 2: natural language processing\n\nimport spacy ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop words\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport string\n\n# generating lists of punctuations and stop words that are going to be eliminated from the text\npunctuations = list(string.punctuation)\nstopwords = list(STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scientific papers have stopwords that don't contribute to the meaning but are not often used in everyday english\n# therefore we need to add these stopwords to our existing list of stopwords\n\ncustom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor word in custom_stop_words:\n    if(word not in stopwords):\n        stopwords.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_sci_lg # module for parsing scientific texts\n\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1) reducing to lowercase\n# 2) removing stop words and punctuation\n# 3) lemmatization\n\nfrom nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() \n\nmetadata_index = []\n\nfor x in metadata.index:\n    metadata_index.append(x)\n\n# a new column that is going to carry out processed abstracts\nmetadata['processed_abstract'] = ''\n\nfor i in tqdm(range(len(metadata_index))):\n    \n    # put all sentences in the abstract into a list\n    sentences = list(parser(metadata['abstract'][metadata_index[i]]).sents)\n\n    for x in range(len(sentences)):\n        mytokens = parser(str(sentences[x])) # parse each sentence into tokens\n\n        # convert each token to lowercase \n        mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n        \n        # do not include the token if it is a stopword or a punctuation\n        mytokens = [word for word in mytokens if str(word) not in stopwords and word not in punctuations]\n        \n        # lemmatize token\n        mytokens = [lemmatizer.lemmatize(str(word)) for word in mytokens]\n        \n        new_sentence = \"\"\n        for token in mytokens:\n            new_sentence = new_sentence + token + \" \"\n\n        new_sentence = new_sentence[:len(new_sentence)-1]\n\n        sentences[x] = new_sentence\n\n    new_abstract = \"\"\n    for sentence in sentences:\n        new_abstract = new_abstract + sentence + \". \"\n\n    new_abstract = new_abstract[:len(new_abstract)-1]\n    \n    metadata['processed_abstract'][metadata_index[i]] = new_abstract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find all papers that don't mention covid-19 in their abstracts\n\nfrom tqdm import tqdm\n\n# a list that will carry the ids of the papers that do not mention covid-19\nnot_covid = []\n\nfor i in tqdm(range(len(metadata_index))):\n    \n    sentences = list(parser(str(metadata['processed_abstract'][metadata_index[i]])).sents)\n\n    myList = []\n    for sentence in sentences:\n        sentence = sentence[:len(sentence)-1]\n\n        for word in sentence:\n            myList.append(str(word))\n    \n    # check if the processed abstract has any mention of the words covid-19 or sars-cov-2 \n    if 'covid-19' not in myList and 'sars-cov-2' not in myList:\n        not_covid.append(str(metadata['cord_uid'][metadata_index[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all entries that do not mention covid-19 from the metadata\n\nfor x in tqdm(range(len(not_covid))):\n    try:\n        metadata = metadata.drop(not_covid[x], axis = 0)\n    except:\n        print('cannot find id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_index = []\n\nfor index in metadata.index:\n    metadata_index.append(index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write the new, filtered and processed metadata into a csv file\nmetadata.to_csv('filtered-processed-metadata.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a method that generates a list of ngrams for a given text\n# only works for unigrams, bigrams and trigrams\n\ndef generate_ngram(ngram, text):\n \n    # ngrams are generated for each separate sentence in the text\n    sentences = text.split(\".\")\n    sentences = [sentence for sentence in sentences if sentence != '']\n    \n    myList = []\n    for sentence in sentences:\n\n        myTokens = sentence.split()\n        myTokens = [token for token in myTokens if token != '']\n\n        if(myTokens != []):\n            if(ngram == 'unigram'):\n                myList.extend(myTokens)\n            elif(ngram == 'bigram' and len(myTokens) >= 2): # cannot generate a bigram for less than 2 words\n                myList.append(list(nltk.bigrams(myTokens)))\n            elif(ngram == 'trigram' and len(myTokens) >= 3): # cannot generate a trigram for less than 3 words\n                myList.append(list(nltk.trigrams(myTokens)))\n        \n    return myList","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a method that runs the unigrams, bigrams and trigrams against each abstract in the matadata\n# in our implementation, abstracts that contain at least one of the unigrams, bigrams or trigrams are considered to be relevant\n\ndef get_relevant_uids(unigrams, bigrams, trigrams):\n    \n    # a list containing the unique cord_uids of entries that are considered relevant\n    relevant_uids = []\n\n    # prioritize entries that contain trigrams as they are less likely to be irrelevant\n    if(trigrams != []):\n        for i in tqdm(range(len(metadata_index))):\n\n            myList = generate_ngram('trigram', str(metadata['processed_abstract'][metadata_index[i]]))\n\n            for trigram in trigrams:\n                for trigramsList in myList:\n                    if trigram in trigramsList:\n                        if str(metadata['cord_uid'][metadata_index[i]]) not in relevant_uids:\n                            relevant_uids.append(str(metadata['cord_uid'][metadata_index[i]]))\n\n    # prioritize bigrams over unigrams\n    if(bigrams != []):\n        for i in tqdm(range(len(metadata_index))):\n\n            myList = generate_ngram('bigram', str(metadata['processed_abstract'][metadata_index[i]]))\n\n            for bigram in bigrams:\n                for bigramsList in myList:\n                    if bigram in bigramsList:\n                        if str(metadata['cord_uid'][metadata_index[i]]) not in relevant_uids:\n                            relevant_uids.append(str(metadata['cord_uid'][metadata_index[i]]))\n\n    # finally check for the availability of unigrams \n    if(unigrams != []):\n        for i in tqdm(range(len(metadata_index))):\n\n            myList = generate_ngram('unigram', str(metadata['processed_abstract'][metadata_index[i]]))\n\n            for unigram in unigrams:\n                if unigram in myList:\n                    if str(metadata['cord_uid'][metadata_index[i]]) not in relevant_uids:\n                        relevant_uids.append(str(metadata['cord_uid'][metadata_index[i]]))\n\n    return relevant_uids    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a method that calculates the precision @ k that takes the list of relevant uids as input as well as k\n\ndef precision_at_k(relevant_documents, k):\n    \n    return len(relevant_documents)/k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 1\n# Data on potential risks factors \n# Smoking, pre-existing pulmonary disease\n\n# for all tasks, we attempt to focus more on the trigrams, and minimize bigrams and unigrams as they are more prone to give us irrelevant documents\n\ntask1_unigrams = ['smoking', 'smoke', 'smoker', 'tobacco', 'cigarettes', 'cigarette', 'asthma']\n\ntask1_bigrams = [('heavy', 'smoker')]\n\ntask1_trigrams = [('pre-existing', 'pulmonary', 'disease'), ('pre-existing', 'lung', 'disease'), \n                  ('pre-existing', 'respiratory', 'disease'), ('pre-existing', 'pulmonary', 'condition'),\n                  ('pre-existing', 'lung', 'condition'), ('pre-existing', 'respiratory', 'condition'),\n                  ('pre-existent', 'pulmonary', 'disease'), ('pre-existent', 'lung', 'disease'), \n                  ('pre-existent', 'respiratory', 'disease'), ('pre-existent', 'pulmonary', 'condition'),\n                  ('pre-existent', 'lung', 'condition'), ('pre-existent', 'respiratory', 'condition'),\n                  ('existing', 'pulmonary', 'disease'), ('existing', 'lung', 'disease'), \n                  ('existing', 'respiratory', 'disease'), ('existing', 'pulmonary', 'condition'),\n                  ('existing', 'lung', 'condition'), ('existing', 'respiratory', 'condition')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 2\n# Data on potential risks factors \n# Co-existing respiratory/viral infections\n\ntask2_unigrams = ['co-infection', 'co-infections', 'co-morbidity', 'co-morbidities']\n\ntask2_bigrams = [('co-existing', 'infection'), ('co-existent', 'infection'), ('existing', 'infection')]\n\ntask2_trigrams = [('co-existing', 'viral', 'infection'), ('co-existing', 'lung', 'infection'),\n                  ('co-extisting', 'respiratory', 'infection'),\n                  ('existing', 'viral', 'infection'), ('existing', 'lung', 'infection'),\n                  ('existing', 'respiratory', 'infection')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 3\n# Data on potential risks factors \n# Neonates and pregnant women\n\ntask3_unigrams = ['baby', 'neonate', 'newborn', 'newborns', 'infant', \n                'pregnant', 'pregnancy']\n\ntask3_bigrams = [('pregnant', 'woman'), ('pregnant', 'patient')]\n\ntask3_trigrams = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 4\n# Socio-economic and behavioral factors \n# to understand the economic impact of the virus and whether there were differences\n\ntask4_unigrams = []\n\ntask4_bigrams = [('socio-economic', 'factor'), ('socioeconomic', 'factor'), ('behavioral', 'factor'), ('economic', 'impact'), \n                 ('social', 'factor'), ('economic', 'factor'), ('social', 'impact')]\n\ntask4_trigrams = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 5\n# Transmission dynamics of the virus\n# basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n\ntask5_unigrams = []\n\ntask5_bigrams = [('transmission', 'dynamic'), ('reproductive', 'number'), ('incubation', 'period'), \n                 ('serial', 'interval'), ('environmental', 'factor'), ('mode', 'transmission')]\n\ntask5_trigrams = [('modes', 'of', 'transmission')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 6\n# Severity of disease \n# including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n\ntask6_unigrams = ['severity', 'high-risk']\n\ntask6_bigrams = [('risk', 'fatality'), ('patient', 'risk'), ('higher', 'risk'), ('high-risk', 'patient'),\n                 ('fatality', 'risk'), ('mortality', 'risk'), ('death', 'risk')]\n\ntask6_trigrams = [('severity', 'of', 'disease'), ('risk', 'of', 'fatality'), ('patient', 'at', 'risk'), \n                  ('symptomatic', 'hospitalized', 'patients'), ('high', 'risk', 'patient'),\n                  ('high', 'fatality', 'risk')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 7\n# Susceptibility of populations\n\ntask7_unigrams = ['susceptibility']\n\ntask7_bigrams = [('susceptibility', 'populations'), ('susceptible', 'group'), ('susceptible', 'population'), \n                 ('high-risk', 'population'), ('vulnerable', 'population'), ('vulnerable', 'people')]\n\ntask7_trigrams = [('susceptibility', 'of', 'populations'), ('high', 'risk', 'population')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 8\n# Public health mitigation measures that could be effective for control\n\ntask8_unigrams = ['mitigation', 'containment']\n\ntask8_bigrams = [('mitigation', 'method'), ('mitigation', 'strategy'), \n                 ('health', 'strategy'), ('prevention', 'method')]\n\ntask8_trigrams = [('public', 'health', 'mitigation'), ('public', 'health', 'prevention'), ('public', 'health', 'strategy')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a list of cord_uids for task 1 that are retrieved by our system\n\ntask1_retrieved_uids = get_relevant_uids(task1_unigrams, task1_bigrams, task1_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task1_retrieved_data = pd.DataFrame(task1_retrieved_uids, columns = ['cord_ui'])\n\ntask1_retrieved_data.to_csv('task1_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task2_retrieved_uids = get_relevant_uids(task2_unigrams, task2_bigrams, task2_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task2_retrieved_data = pd.DataFrame(task2_retrieved_uids, columns = ['cord_ui'])\n\ntask2_retrieved_data.to_csv('task2_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task3_retrieved_uids = get_relevant_uids(task3_unigrams, task3_bigrams, task3_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task3_retrieved_data = pd.DataFrame(task3_retrieved_uids, columns = ['cord_ui'])\n\ntask3_retrieved_data.to_csv('task3_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task4_retrieved_uids = get_relevant_uids(task4_unigrams, task4_bigrams, task4_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task4_retrieved_data = pd.DataFrame(task4_retrieved_uids, columns = ['cord_ui'])\n\ntask4_retrieved_data.to_csv('task4_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task5_retrieved_uids = get_relevant_uids(task5_unigrams, task5_bigrams, task5_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task5_retrieved_data = pd.DataFrame(task5_retrieved_uids, columns = ['cord_ui'])\n\ntask5_retrieved_data.to_csv('task5_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_retrieved_uids = get_relevant_uids(task6_unigrams, task6_bigrams, task6_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_retrieved_data = pd.DataFrame(task6_retrieved_uids, columns = ['cord_ui'])\n\ntask6_retrieved_data.to_csv('task6_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task7_retrieved_uids = get_relevant_uids(task7_unigrams, task7_bigrams, task7_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task7_retrieved_data = pd.DataFrame(task7_retrieved_uids, columns = ['cord_ui'])\n\ntask7_retrieved_data.to_csv('task7_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task8_retrieved_uids = get_relevant_uids(task8_unigrams, task8_bigrams, task8_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task8_retrieved_data = pd.DataFrame(task8_retrieved_uids, columns = ['cord_ui'])\n\ntask8_retrieved_data.to_csv('task8_retrieved_data', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate precision @ k \n# k is the top 10 documents\n\n# task 1\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task1_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in an attempt to measure the performance of our nlp methods, we took the initiative of manually labeling the first 10 documents that are retrieved by our system\n\n# top 10 retrieved documents:\n# ['95fty9yi','6tso9our','8g70j0qw','1gnoo0us','t73oioqv','y778k3hs','5xyt8d5u','emwu5g5f','ygnxmcl1','ygnxmcl1','z0476b47','ubqexcof']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask1_top_10_retrieved = ['95fty9yi', '8g70j0qw', '1gnoo0us', 'y778k3hs', '5xyt8d5u', 'emwu5g5f']\n\n# we realize that this method is very subjective, however, we only took the documents whose main subject was related to our question\n# and since the documents aren't labeled, this is the method we resorted to for measurement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask1_precision = precision_at_k(task1_top_10_retrieved, 10)\n\nprint(task1_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 2\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task2_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents:\n# ['vqqqip1m','b2vjrskm','fzpz9oaj','1igydu3y','rjqq1lxx','nzjz0q8b','bjykd232','2z5ixtk3','dqs21e0q','50iju57j']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask2_top_10_relevant = ['b2vjrskm', 'fzpz9oaj', '1igydu3y', 'bjykd232', '2z5ixtk3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask2_precision = precision_at_k(task2_top_10_relevant, 10)\n\nprint(task2_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 3\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task3_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents:\n# ['fce2kdzw', 'un1y8r5a', 'c27qh5y7', 's0vo7dlk', '5itjbp2j', '7xe3fvd5', 'vqz6b7zy', '9ae8qhcg', 'n6sq2jz9', 'wncv7qvm']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask3_top_10_relevant = ['fce2kdzw', 'un1y8r5a', 'c27qh5y7', '7xe3fvd5', '5itjbp2j', 'n6sq2jz9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask3_precision = precision_at_k(task3_top_10_relevant, 10)\n\nprint(task3_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 4\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task4_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents:\n# ['dg213uf6', 'ogfwlr4m', 'q114cle2', '1zyeusat', 'd7at4wp4', 's8fvqcel', 'p4q64ksa', 'jtwb17u8', '8ozauxlk', 'pm6ta0qy']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask4_top_10_relevant = ['ogfwlr4m', 'q114cle2', 's8fvqcel', '8ozauxlk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask4_precision = precision_at_k(task4_top_10_relevant, 10)\n\nprint(task4_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 5\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task5_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents in our last run\n# ['zph6r4il', 'lh3x6wat', 'jkyltjap', 'ohrqw3ac', '16rgt4ca', 'efk2jj50', 'dl7vrb5k', '4bzw76lt', 'zd8c1no7', 'vhphlccl']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask5_top_10_relevant = ['lh3x6wat', 'ohrqw3ac', 'efk2jj50', 'dl7vrb5k', 'zd8c1no7', 'vhphlccl']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask5_precision = precision_at_k(task5_top_10_relevant, 10)\n\nprint(task5_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 6\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task6_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents in our last run\n# ['a2mqvbj2', '8g70j0qw', 'lh3x6wat', '4bzw76lt', 'v83fgykh', 'mez1k8t0', '142qz8of', 'o3b2yvbs', 'bj142e9b', 'xn8jzzsd']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask6_top_10_relevant = ['8g70j0qw', '4bzw76lt', 'v83fgykh', 'mez1k8t0', 'o3b2yvbs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask6_precision = precision_at_k(task6_top_10_relevant, 10)\n\nprint(task6_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 7\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task7_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents in our last run\n# ['1tkbwk8u', '41cqkmra', 'lp4nhezz', '810fmd8e', 'flzaqw4s', 'b4e3y9u3', 'po1bu4v3', '2ftw85xw', 'c9ecid5e', 'hkm8yspk']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask7_top_10_relevant = ['1tkbwk8u', '41cqkmra', 'lp4nhezz', '810fmd8e', '2ftw85xw', 'hkm8yspk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask7_precision = precision_at_k(task7_top_10_relevant, 10)\n\nprint(task7_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# task 8\n# the following are the top 10 retrieved documents (cord_uid + abstract)\n\nfor x in range(10):\n    \n    uid = task8_retrieved_uids[x]\n    \n    for i in range(len(metadata_index)):\n        if(uid == metadata['cord_uid'][metadata_index[i]]):\n            print(metadata['cord_uid'][metadata_index[i]])\n            print(metadata['abstract'][metadata_index[i]])\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 10 retrieved documents in our last run\n# ['nnl1txhi', 'swc2pitd', 'syt4r964', 'qwngx01h', 'v6wt9mhh', '2w0zr9c0', 'gb9rkv9c', '8niqpwvc', '9iwiv5m4']\n\n# the following are the relevant documents that we manually labeled from the top 10 retrieved entires:\ntask8_top_10_relevant = ['nnl1txhi', 'swc2pitd', 'qwngx01h', '8niqpwvc', '9iwiv5m4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision @ k\ntask8_precision = precision_at_k(task8_top_10_relevant, 10)\n\nprint(task8_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}