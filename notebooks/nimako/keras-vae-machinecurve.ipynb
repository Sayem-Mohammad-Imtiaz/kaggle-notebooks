{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nimport matplotlib.image as mpimg\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\n\n### To avoid compatibility issues between Keras and Tensorflow\ntf.compat.v1.disable_eager_execution()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_glob = glob.glob('../input/coco2017/train2017/train2017/*.jpg')\ntest_images_glob = glob.glob('../input/coco2017/val2017/val2017/*.jpg')\n\n\"\"\"Test whether the data is loaded correctly by checking the length\"\"\"\nprint(len(train_images_glob))\nprint(len(test_images_glob))\n\n\"\"\"Display one of the train and test images\"\"\"\nplt.imshow(mpimg.imread(train_images_glob[2]))\nplt.show()\nplt.imshow(mpimg.imread(test_images_glob[2]))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Copy data into numpy arrays"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Define train and test data and their sizes\"\"\"\nx_train = []\nx_val = []\ntrain_size = 1000\nval_size = 200\nimage_size = 128\n\n\"\"\"Load the images into lists and then convert into np.array\"\"\"\nfor i in tqdm(train_images_glob[0:train_size]):\n  img = image.load_img(i, target_size=(image_size,image_size,3))\n  img = image.img_to_array(img)\n  x_train.append(img)\n\nfor i in tqdm(test_images_glob[0:val_size]):\n  img = image.load_img(i, target_size=(image_size,image_size,3))\n  img = image.img_to_array(img)\n  x_val.append(img)\n\n\"\"\"Check the length for confirmation\"\"\"\nx_train = np.array(x_train)\nx_val = np.array(x_val)\nprint(len(x_train), len(x_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width, img_height = x_train.shape[1], x_train.shape[2]\nbatch_size = 128\nno_epochs = 50\nvalidation_split = 0.2\nverbosity = 1\nlatent_dim = 100\nnum_channels = 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\nx_val = x_val.reshape(x_val.shape[0], img_height, img_width, num_channels)\ninput_shape = (img_height, img_width, num_channels)\n\nx_train = x_train.astype(\"float32\")\nx_val = x_val.astype(\"float32\")\n\nx_train = x_train / 255\nx_val = x_val / 255\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configure the VAE"},{"metadata":{},"cell_type":"markdown","source":"# Sampling Function\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define sampling with reparameterization trick\ndef sample_z(args):\n  mu, sigma = args\n  batch     = K.shape(mu)[0]\n  dim       = K.int_shape(mu)[1]\n  eps       = K.random_normal(shape=(batch, dim))\n  return mu + K.exp(sigma / 2) * eps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the encoder layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"i       = Input(shape=input_shape, name='encoder_input')\ncx      = Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(i)\ncx      = BatchNormalization()(cx)\ncx      = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx      = BatchNormalization()(cx)\ncx      = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx      = BatchNormalization()(cx)\ncx      = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx      = BatchNormalization()(cx)\ncx      = Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx      = BatchNormalization()(cx)\nx       = Flatten()(cx)\nx       = Dense(100, activation='relu')(x)\nx       = BatchNormalization()(x)\nmu      = Dense(latent_dim, name='latent_mu')(x)\nsigma   = Dense(latent_dim, name='latent_sigma')(x)\n\n# Get Conv2D shape for Conv2DTranspose operation in decoder\nconv_shape = K.int_shape(cx)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use reparameterization trick\nz       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n\n# Instantiate encoder\nencoder = Model(i, [mu, sigma, z], name='encoder')\nencoder.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definition\nd_i   = Input(shape=(latent_dim, ), name='decoder_input')\nx     = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\nx     = BatchNormalization()(x)\nx     = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\ncx    = Conv2DTranspose(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(x)\ncx    = BatchNormalization()(cx)\ncx    = Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx    = BatchNormalization()(cx)\ncx    = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx    = BatchNormalization()(cx)\ncx    = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(cx)\ncx    = BatchNormalization()(cx)\ncx    = Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same',  activation='relu')(cx)\ncx    = BatchNormalization()(cx)\no     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(cx)\n\n# Instantiate decoder\ndecoder = Model(d_i, o, name='decoder')\ndecoder.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define loss\ndef kl_reconstruction_loss(true, pred):\n  # Reconstruction loss\n  reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n  # KL divergence loss\n  kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n  kl_loss = K.sum(kl_loss, axis=-1)\n  kl_loss *= -0.5\n  # Total loss = 50% rec + 50% KL divergence loss\n  return K.mean(reconstruction_loss + kl_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate VAE\nvae_outputs = decoder(encoder(i)[2])\nvae         = Model(i, vae_outputs, name='vae')\nvae.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compile the VAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.compile(optimizer='RMSprop', loss=kl_reconstruction_loss)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model\n### having multiple layers of train helps us stop frequently and change run different parts of the code as needed without keyboard interrupt"},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 10, batch_size = batch_size, validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 40, batch_size = batch_size, validation_split = validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 10, batch_size = batch_size, validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Predictions on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = vae.predict(x_val, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 5\nplt.figure(figsize= (30,20))\n\nfor i in range(n):\n  ax = plt.subplot(2, n, i+1)\n#   plt.imshow(val_x_px[i+20])\n  plt.imshow(x_val[i+40])  \n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False) \n  ax = plt.subplot(2, n, i+1+n)\n  plt.imshow(predictions[i+40])\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try CelebA Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_glob = glob.glob('../input/celeba-dataset/img_align_celeba/img_align_celeba/*.jpg')\n\nprint(len(train_images_glob))\nplt.imshow(mpimg.imread(train_images_glob[2]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = []\nimage_size = 128\ntrain_size = 6000\n\nfor i in tqdm(train_images_glob[0:train_size]):\n  img = image.load_img(i, target_size=(image_size,image_size,3))\n  img = image.img_to_array(img)\n  all_images.append(img)\n\"\"\"Change the data into an np.array\"\"\"\nall_images = np.array(all_images)\nprint(len(all_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split into train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test = train_test_split(\n                                     all_images, \n                                     shuffle=True,\n                                     test_size = 0.2,\n                                     random_state=42\n                                  )\n\"\"\"Reshape the data into proper form and normalize\"\"\"\nx_train = x_train.reshape(x_train.shape[0], image_size, image_size, num_channels)\nx_test = x_test.reshape(x_test.shape[0], image_size, image_size, num_channels)\ninput_shape = (image_size, image_size, num_channels)\n\nx_train = x_train.astype(\"float32\")\nx_test = x_test.astype(\"float32\")\n\nx_train = x_train / 255\nx_test = x_test / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model on New Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 10, batch_size = batch_size, validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 30, batch_size = batch_size, validation_split = validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae.fit(x_train, x_train, epochs = 10, batch_size = batch_size, validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Predictions on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = vae.predict(x_test, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 5\nplt.figure(figsize= (30,20))\n\nfor i in range(n):\n  ax = plt.subplot(2, n, i+1)\n#   plt.imshow(val_x_px[i+20])\n  plt.imshow(x_test[i+40])  \n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False) \n  ax = plt.subplot(2, n, i+1+n)\n  plt.imshow(predictions[i+40])\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save the Weights for Future Use"},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import randint as r\nvae.save_weights(\"vae-weights_6k_celeba\"+str(r(0,3653))+\".h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}