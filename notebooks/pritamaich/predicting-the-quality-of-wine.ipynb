{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objectives: ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The objectives of this project are as follows:\n\n#### 1. To experiment with different classification methods to see which yields the highest accuracy\n#### 2. To determine which features are the most indicative for predicting the quality of  wine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\n\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data description","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_data.copy()\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Checking missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are no missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.  Outlier Detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,15))\nsns.boxplot(data = pd.melt(data) , x = 'variable', y = 'value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So there are some outliers in the 'total sulfer dioxide' column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,6))\nsns.distplot(data['total sulfur dioxide'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing outliers from data :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['total sulfur dioxide']<180]\nplt.figure(figsize = (8,6))\nsns.distplot(data['total sulfur dioxide'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking the data again with a boxplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nsns.boxplot(data = pd.melt(data) , x = 'variable', y = 'value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the distribution of quality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,6))\nsns.distplot(data['quality'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let's first check which features are correlated with each other with a correclation heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(12,12))\nsns.heatmap(data.corr(), cmap = 'Blues', annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The density and citric acid are highly correlated with fixed acidity. Again, total sulfur dioxide and free sulfer dioxide are highly correlated to each other. \n\nThis is the multicollinearity. This results in unstable parameter estimates of regression which makes it very difficult to assess the effect of independent variables on dependent variables.\n\nWe will use Variance inflation factor to analyze which variable has a high correlation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data[['density', 'citric acid', 'total sulfur dioxide', 'free sulfur dioxide']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'free sulfur dioxide' has a correlation above 5 and needs to be dropped","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('free sulfur dioxide', axis = 1)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check the Variance inflation factor for the remaining 3 variables after dropping the 'free sulfur dioxide' column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = data[['density', 'citric acid', 'total sulfur dioxide']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As the vif of these variables are below 5, there's no multicolinearity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, let's check which features are most important for our quality predictions. For this we will use Extra Tree classifier,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier \n\nX = data.drop('quality', axis = 1)\nY = data['quality']\n\nmodel =  ExtraTreesClassifier()\nmodel.fit(X,Y)\n\nfeatures = pd.DataFrame()\nfeatures['Features'] = X.columns\nfeatures['Importance'] = model.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features,  order=features.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order)\", size=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can observe that except the pH column , all other features comprises 95% of the data that influences a wine quality. So we are going to use these 9 features for our models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = data.drop('pH', axis = 1)\ndata_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since the features are measured in different units , wee need to standardize the values. for that, we use Standard Scaler. \n##### Standard scaler scales the values with mean = 0 and standard deviation = 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nx = data_new.drop('quality', axis = 1)\nscaler = StandardScaler()\nscaler.fit(x)\nx_scaled = scaler.transform(x)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"x_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's observe the distribution of wine quality in the data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(16,5))\nplt.subplot(1,2,1)\nsns.distplot(data['quality'])\nplt.subplot(1,2,2)\nsns.countplot(data['quality'])#Showing the frequency of occurence of a particular quality rating\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we will categorize the ratings into 3 categories , 'Bad', 'Normal' , 'Good'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"category = [] # Defining an empty array\nfor x in data['quality']:\n    if x>=1 and x<=3:\n        category.append('Bad')\n    elif x>=4 and x<=6:\n        category.append('Normal')\n    elif x>=7 and x<=10:\n        category.append('Good')\n        \n        \ndata_new['category'] = category #Assigning a new column\ndata_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final = data_new.copy()\ndata_final = data_final.drop('quality',axis =1)\ndata_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['category'].value_counts() #Checking the number of ratings in each category","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#defining inputs(independent) and targets(dependent) variables\ninputs = x_scaled\ntargets = data_final['category']\n\n#splitting into training and testing data\n\nx_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Since this a Classification problem , we are mainly going to use :\n\n#### 1. Logistic Regression\n\n#### 2. Decision Tree Classifier\n\n#### 3. Support Vector Classifier\n\n#### 4. Random Forest Classifier\n\n#### 5. K-Nearest Neighbours\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining a method or function that will print the cross validation score and accuracy for each model\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\ndef model_report(cl):\n    \n    cl.fit(x_train, y_train)\n\n    print('Cross Val Score: ',(cross_val_score(cl,x_train,y_train, cv=5).mean()*100).round(2))#using a 5-Fold cross validation\n\n    y_pred = cl.predict(x_test)\n\n    print('Accuracy Score: ', (accuracy_score(y_test,y_pred)*100).round(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nmodel_report(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\nmodel_report(dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Support Vector Classifer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nmodel_report(svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nmodel_report(rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. K-Nearest Neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier(algorithm ='auto')\n\nmodel_report(kn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lets try to tune our models and see if we can improve accuracy. For this we will use GridSearchCV \n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n#Defining a function that will calculate the best parameters and accuracy of the model based on those parameters\n#Using GridSearchCV\n\ndef grid_search(classifier,parameters):\n    \n    grid = GridSearchCV(estimator = classifier,\n                        param_grid = parameters,\n                        scoring = 'accuracy',\n                        cv = 5,\n                        n_jobs = -1\n                        )\n    \n    grid.fit(x_train,y_train)\n\n    print('Best parameters: ', grid.best_params_) #Displaying the best parameters of the model\n\n    print(\"Accuracy: \", ((grid.best_score_)*100).round(2))#Accuracy of the model based on those parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Support Vector Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_svc = {\n    'C': [0.1, 1, 10, 100],  \n    'gamma': [0.0001, 0.001, 0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], \n    'kernel': ['linear','rbf']\n    }\nsvc = SVC()\n\ngrid_search(svc,param_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model again with the best parameters we got\nsvc = SVC(C = 10, gamma = 0.3, kernel='rbf')\n\nmodel_report(svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### So, the accuracy of our Support Vector Classifier model increased from 89.38% to 90.62% ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Random Forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_rf = {\n    'n_estimators': [10,50,100,500,1000],\n    'min_samples_leaf': [1,10,20,50]\n    }\nrf = RandomForestClassifier(random_state = 0)\ngrid_search(rf,param_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model again with the best parameters we got\nrf = RandomForestClassifier(n_estimators = 1000, min_samples_leaf = 1,random_state = 0)\nmodel_report(rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### So, the accuracy of our Random Forest Classifier model increased from 91.25% to 92.19%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. K-Nearest Neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = list(range(5,10))#This is basically the value of k\n                   \nparam_knn = {\n    'n_neighbors' : n_neighbors,\n    'p' : [1,2]\n    \n    }\n\nknn = KNeighborsClassifier(algorithm ='auto', n_jobs = -1)\ngrid_search(knn,param_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model again with the best parameters we got\nknn = KNeighborsClassifier(n_neighbors = 7, p = 2, algorithm ='auto', n_jobs = -1)\nmodel_report(knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Looks like hyper parameter tuning did'nt changed the accuracy of K-Nearest Neighbors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lastly we will use some boosting algorithms mainly :\n#### 1. AdaBoost\n#### 2. Gradient Boost\n#### 3. XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  AdaBoostClassifier\n\nab = AdaBoostClassifier(random_state = 42)\n\nmodel_report(ab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### So, AdaBoost is'nt a good model to perform on this dataset ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Gradient Boost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(random_state = 42, learning_rate = 0.2)\n\nmodel_report(gb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Gradient boost gives quite better accuracy than AdaBoost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxg = XGBClassifier(random_state = 42, learning_rate = 0.2)\n\nmodel_report(xg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So , comparing all the models , Random Forest(92.19% accuracy) and XGBoost(92.81%) seems to give the highest accuracy.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Finally , let's see which features contributed most in each of these 2 models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_rf = pd.DataFrame()\nx_rf = data_final.drop('category',axis=1)\nfeatures_rf['Features'] = x_rf.columns\nfeatures_rf['Importance'] = rf.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features_rf,  order=features_rf.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order) for Random Forest\", size=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## XGBoost:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_xg = pd.DataFrame()\nx_xg = data_final.drop('category',axis=1)\nfeatures_xg['Features'] = x_xg.columns\nfeatures_xg['Importance'] = xg.feature_importances_\n\nplt.figure(figsize =(15,6))\nsns.barplot(y='Importance', x='Features', data=features_xg,  order=features_xg.sort_values('Importance',ascending = False).Features)\nplt.xlabel(\"Features\", size=15)\nplt.ylabel(\"Importance\", size=15)\nplt.title(\"Features Importance(Descending order) for XGBoost\", size=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusuion:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### 1. For determing the quality of wines , alcohol plays a significant role followed by sulphates and volatile acididty.\n##### 2. For predicting quality of wines, we can either use Random Forest or XGBoost model. However,  XGBoost has a slightly better accuracy(0.62% more accurate) over Random Forest.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### If you find this notebook useful, please upvote it! And let me know in the comments if i did anything wrong or if i could've done this better. Means a lot! :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}