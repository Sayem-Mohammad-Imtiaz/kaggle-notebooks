{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","version":"3.6.3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3"}},"cells":[{"outputs":[],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport sklearn\nimport random\n \nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom sklearn import cross_validation\nfrom sklearn.cross_validation import train_test_split\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\n\n# Definitions\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n#-----------------------------------------------------------------------------\n#                     Create Training & Testintg Set\n#-----------------------------------------------------------------------------\ncsv =  pd.read_csv('../input/nyc-rolling-sales.csv')\ntrain, test = sklearn.cross_validation.train_test_split(csv, train_size = 0.8)\n\n\n#-----------------------------------------------------------------------------\n#                     Data Analysis - Clean up\n#-----------------------------------------------------------------------------\ncol = csv.columns\ntrain['SALE PRICE'].describe()\nsns.distplot(train['SALE PRICE']);\ntrain = train.drop(train[train['SALE PRICE'] > 110000000].index) #Drop extreme values\ntrain = train.drop(train[train['TOTAL UNITS'] < 1].index) #Drop null values\n\n#correlation matrix (Check variables that are effecting Sales Price)\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n\n#saleprice correlation matrix (Sorted by significance of correlation)\nk = 12 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SALE PRICE')['SALE PRICE'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=2)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.1f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()  \n\n#Scatter plots between 'SalePrice' and correlated variables \nsns.set()\ncols = ['SALE PRICE', 'GROSS SQUARE FEET', 'TOTAL UNITS', 'RESIDENTIAL UNITS', 'COMMERCIAL UNITS']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()\n\n#standardizing data (Check significance of outliers: expect normal data to be around 0)\nsaleprice_scaled = StandardScaler().fit_transform(train['SALE PRICE'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)\n\n#       Bivariate analysis SALE PRICE VS GROSS SQUARE FEET\n#       check which points are off the trend (aka outliers)\nvar = 'GROSS SQUARE FEET'\ndata = pd.concat([train['SALE PRICE'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SALE PRICE');\n\n#Identify the position of outliers then remove them\ntrain.sort_values(by = 'GROSS SQUARE FEET', ascending = False)[:3]\ntrain = train.drop(train[train['GROSS SQUARE FEET'] == 1021752].index)\n\n#       Bivariate analysis SALE PRICE VS TOTAL UNITS\n#       check which points are off the trend (aka outliers)\nvar = 'TOTAL UNITS'\ndata = pd.concat([train['SALE PRICE'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SALE PRICE');\n\ntrain.sort_values(by = 'TOTAL UNITS', ascending = False)[:5]\ntrain = train.drop(train[train['GROSS SQUARE FEET'] == 555954].index)\n\n#-----------------------------------------------------------------------------\n#                     Data Analysis - Transformation\n#-----------------------------------------------------------------------------\n\n#Normality Check on Sales price-heavy tail, not even close to normal\nsns.distplot(train['SALE PRICE'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SALE PRICE'], plot=plt)\n\n#apply lognormal transformation (Still not normal, but alot better)\ntrain['SALE PRICE'] = np.log(train['SALE PRICE'])\n\n#Apply lognormal transformation on other factors for the same reason\ntrain['GROSS SQUARE FEET'] = np.log(train['GROSS SQUARE FEET'])\nsns.distplot(train['GROSS SQUARE FEET'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SALE PRICE'], plot=plt)\n\ntrain['TOTAL UNITS'] = np.log(train['TOTAL UNITS'])\nsns.distplot(train['TOTAL UNITS'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SALE PRICE'], plot=plt)\n\n#-----------------------------------------------------------------------------\n#                     Data Analysis - Regression\n#-----------------------------------------------------------------------------\ny = train['SALE PRICE']\ngrossSize = train['GROSS SQUARE FEET'] \nunitNum = train['TOTAL UNITS']\nborough = train['BOROUGH']#Caterlogical Vars\n\n\n# Linear Regression\nlr = LinearRegression()\nlr.fit(grossSize, y)\n","execution_count":null,"cell_type":"code","metadata":{"_kg_hide-input":true,"_cell_guid":"408ad6a7-0e13-4f40-924a-356cbf51c61b","_uuid":"9b2bcfa1471e104b7750d1559c33109a7a7c80d4"}}]}