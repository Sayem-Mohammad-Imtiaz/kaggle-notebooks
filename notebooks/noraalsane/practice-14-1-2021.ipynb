{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Apply the classification algorithms Naive Bayes, Logistic regression and K-Nearest neighbours on the attached imdb dataset of review texts and review sentiment.\n\nConvert the review text to Bag-of-Word (BOW) model with TF-IDF weights (text preprocessing should be applied first) and predict the review sentiment (positive or negative). Use label encoding to convert the sentiment feature to numerical values. The training/test split for the dataset should be 80/20.\n\nprint the accuracy score for each algorithm on the test dataset to find the most accurate model among the three created models."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport operator\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the datafile\ndata = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a 10% sample of the dataset as recommended\n\nimdb = data.sample(frac =.1)\nimdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# replace values in sentiment feature from (positive, negative) to (1,0)\nimdb['sentiment'].replace({'positive':1,'negative':0},inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove stopping words\n\nstop = stopwords.words('english')\nreviews_wo_stop = []\n\nfor rev in imdb['review']:\n    text = []\n    for word in rev.split():\n        if word.lower() not in stop:\n            text.append(word)\n            \n    reviews_wo_stop.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the review text to Bag-of-Word (BOW) model with TF-IDF weights \n#(text preprocessing should be applied first) \n\n#text pre-processing and creating TFIDF vector.\n#conver text to lowercase\n#apply stemming\n#apply lematization\n#create the bag of words and tfidf vector\n\nwordnet_lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef tokenize(str_input):\n    \n    words = re.sub(r\"(?u)[^A-Za-z]\", \" \", str_input).lower().split(\" \")\n    words = [stemmer.stem(word) for word in words if len(word)>2]\n    words = [wordnet_lemmatizer.lemmatize(word) for word in words if len(word)>2]\n    return words\n\n\nvectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n\nvectors = vectorizer.fit_transform(imdb['review'])\n\nfeature_names = vectorizer.get_feature_names()\n\nimdb_reviews_tfidf = pd.DataFrame(vectors.toarray(),columns=feature_names)\n\n#print(imdb_reviews_tfidf.head())\n#print(feature_names)\n#print(\"number of words = \", len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#and predict the review sentiment (positive or negative). \n#Use label encoding to convert the sentiment feature to numerical values. \n#The training/test split for the dataset should be 80/20.\n\n#training/ test split\nX=pd.DataFrame(vectors.toarray(),columns=feature_names) #imdb_reviews_tfidf\ny = imdb.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Multinomial Naieve Bayes Classification \n#multinomial classification model  applied for text \n#Gaussian classification model applied for in case of using numbers. \n\n\nNB = MultinomialNB()\nNB.fit(X_train,y_train)\ny_NB=NB.predict(X_test)\nprint(\"Multinomial Accuracy=\",accuracy_score(y_test,y_NB))\nA=accuracy_score(y_test,y_NB) #SAVE MULTINOMIAL ACCURACY SCORE HERE FOR COMMPARISON ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#P.S: This block of code takes nearly 3 hours to run\n#Applying Nearest Neighbour Classification  \n#numNeighbors = [1, 5, 10, 15, 20, 25, 30]\n#trainAcc = []\n#testAcc = []\n\n#for k in numNeighbors:\n#    clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n#    clf.fit(X_train, y_train)\n#    Y_predTrain = clf.predict(X_train)\n#    Y_predTest = clf.predict(X_test)\n#    trainAcc.append(accuracy_score(y_train, Y_predTrain))\n#    testAcc.append(accuracy_score(y_test, Y_predTest))\n\n#clf = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n#clf.fit(X_train, y_train)\n\n#print(\"accuracy= \", clf.score(X_test,y_test))\n#B= clf.score(X_test,y_test) #SAVE NEAREST NEIGHBOUR SCORE HERE FOR COMPARISON\n#plt.plot(numNeighbors, trainAcc, 'ro-', numNeighbors, testAcc,'bv--')\n#plt.legend(['Training Accuracy','Test Accuracy'])\n#plt.xlabel('Number of neighbors')\n#plt.ylabel('Accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Logistic regression\nC = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\nLRtrainAcc = []\nLRtestAcc = []\n\n\nfor param in C:\n    clf = linear_model.LogisticRegression(C=param)\n    clf.fit(X_train, y_train)\n    Y_predTrain = clf.predict(X_train)\n    Y_predTest = clf.predict(X_test)\n    LRtrainAcc.append(accuracy_score(y_train, Y_predTrain))\n    LRtestAcc.append(accuracy_score(y_test, Y_predTest))\n\nclf = linear_model.LogisticRegression(C=1.0)  \nprint(\"Accuracy= \",clf.fit(X_train, y_train).score(X_test,y_test))\nC= clf.fit(X_train, y_train).score(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison between all classification methods. \n#print the accuracy score for each algorithm \n#on the test dataset to find the most accurate model \n#among the three created models.\n\n\n\n#due to very long run time of k-nearest neighbour I was not able to figure out the accuracy value. \n#Also, due to this code generating different result each time it runs \n#(because it is selecting a random sample from the reviews database )\n#the Accuracy result is different after each run \n\n#to compare the three models, the best model is the one that provides the highest accuracy score\n\nprint('Final Results - Accuracy Scores:')\nprint ('Multinomial Accuracy= ', A)\n#print ('K- Nearest Neighbor accuracy= ', B) # This line has been commented out because it takes a very long time to run.(3 hrs)\nprint ('Logistic Regresssion Accuracy= ',C)\nprint (\"the highest value represents the most accurate model.\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}