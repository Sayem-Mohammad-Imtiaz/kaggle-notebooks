{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DIR = \"../input/medium-articles-dataset/medium_data.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport cv2\n\nIMAGE_SIZE = (150, 150)\n\nfrom skimage import io\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nimport re\ndf = pd.read_csv(DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['url'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(subset = ['image'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image(x):\n    try:\n        if type(x) != str:\n            return None\n        \n        img_path = \"../input/medium-articles-dataset/images/\" + x\n\n        image = io.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, IMAGE_SIZE) \n        if type(image) == float:\n            return None\n        return image\n    except:\n#         img_path = \"../input/medium-articles-dataset/images/1\"\n\n#         image = io.imread(img_path)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#         image = cv2.resize(image, IMAGE_SIZE) \n        return None\n        \n    \n    \ndf.image = df.image.apply(lambda x: image(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.image = df.image.replace(to_replace='None', value=np.nan).dropna()\n\n\ndf.image = df.image.dropna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef clean(text):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n    return text\ndf.title = df.title.apply(lambda x: clean(x))\ndf.subtitle = df.subtitle.apply(lambda x: clean(x))\n\n# df.responses  = df.responses.apply(lambda x: int(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaner(num):\n    if (num > 500):\n        return 500\n    else:\n        return num\ndf.claps = df.claps.apply(lambda x: cleaner(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 20\n\ntrain_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.title)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.title),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.title),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nsubtitle_train = pad_sequences(tokenizer.texts_to_sequences(train_data.subtitle),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nsubtitle_test = pad_sequences(tokenizer.texts_to_sequences(test_data.subtitle),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nresponses_train = train_data.responses\nresponses_test = test_data.responses\n\n\nreading_time_train = train_data.reading_time\nreading_time_test = test_data.reading_time\n\n\ny_train = train_data.claps\ny_test = test_data.claps\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_train = train_data.image\nimg_test = test_data.image\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(img_train.shape)\n\nfor i in img_train:\n    print(i.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(img_test.shape)\n\nfor i in img_test:\n    print(i.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GLOVE_DIR = '../input/glove6b300dtxt/glove.6B.300d.txt'\n\n\nembeddings_index = {}\nf = open(GLOVE_DIR)\nprint('Loading GloVe from:', GLOVE_DIR,'...', end='')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nprint(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n\nembedding_matrix = np.random.random((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint(\" Completed!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_train = np.array(img_train, dtype = 'float32')\n# img_test = np.array(img_test, dtype = 'float32')\n# img_train=np.asarray(img_train).astype(np.float32)\n# img_test=np.asarray(img_test).astype(np.float32)\n\nimg_train=tf.convert_to_tensor(list(img_train), dtype = tf.float32)\nimg_test=tf.convert_to_tensor(list(img_test), dtype = tf.float32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n# from tensorflow.keras.layers import *\n\n\n# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n# embedding_layer = Embedding(len(word_index) + 1,\n#                            300,\n#                            weights = [embedding_matrix],\n#                            input_length = MAX_SEQUENCE_LENGTH,\n#                            trainable=False,# prevent re-training the glove vector\n#                            name = 'embeddings')\n# embedded_sequences = embedding_layer(sequence_input)\n# x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n# x = GlobalMaxPool1D()(x)\n# x = Dropout(0.1)(x)\n# x = Dense(50, activation=\"relu\")(x)\n# x = Dropout(0.1)(x)\n# preds = Dense(1)(x)\n\n\n# ####\n\n# from tensorflow.keras.models import Model\n\n# model = Model(sequence_input, preds)\n# model.compile(loss='mean_squared_error', optimizer='adam',\n#              metrics = ['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = keras.Sequential([\n#       norm,\n#       layers.Dense(64, activation='relu'),\n#       layers.Dense(64, activation='relu'),\n#       layers.Dense(1)\n#   ])\n\n#   model.compile(loss='mean_absolute_error',\n#                 optimizer=tf.keras.optimizers.Adam(0.001))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#1\nfrom tensorflow.keras.layers import *\n\n# Title\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(len(word_index) + 1,\n                           300,\n                           weights = [embedding_matrix],\n                           input_length = MAX_SEQUENCE_LENGTH,\n                           trainable=False,# prevent re-training the glove vector\n                           name = 'embeddings')\nembedded_sequences = embedding_layer(sequence_input)\nx = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n# x = Conv1D(64, 5, activation='relu')(x)\nx = GlobalMaxPool1D()(x)\n\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\npreds = Dense(10)(x)\n\n#Subtitle\nsequence_input2= Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedded_sequences2 = embedding_layer(sequence_input2)\nx2 = LSTM(60, return_sequences=True,name='lstm_layer2')(embedded_sequences2)\n# x = Conv1D(64, 5, activation='relu')(x)\nx2 = GlobalMaxPool1D()(x2)\nx2 = Dropout(0.1)(x2)\nx2 = Dense(50, activation=\"relu\")(x2)\nx2 = Dropout(0.1)(x2)\npreds2 = Dense(10)(x2)\n\n#reading_time\ninput_y = Input(shape = (1,), dtype='int32')\ny = Dense(2, activation=\"relu\")(input_y)\n\n#Image\ninput_image = Input(shape = (150,150,3)) # , dtype='object'\nz = Conv2D(32, (3, 3), activation = 'relu')(input_image)\nz = MaxPooling2D(2,2)(z)\nz = Conv2D(32, (3, 3), activation = 'relu')(z)\nz = MaxPooling2D(2,2)(z)\nz = Flatten()(z)\nz = Dense(128, activation=\"relu\")(z)\n\n\n\nconcat = Concatenate()([preds, preds2, y, z])\n\noutput = Dense(1)(concat)\n\n####\n\nfrom tensorflow.keras.models import Model\n\nmodel = Model(inputs = [sequence_input, sequence_input2, input_y, input_image], outputs = [output])\n\nmodel.compile(loss='mean_absolute_error', optimizer='adam',\n             metrics = ['mae'])\n\n######\n\nprint('Training progress:')\n\nhistory = model.fit([x_train,subtitle_train, reading_time_train, img_train], y_train, epochs = 15, batch_size=64, validation_data=([x_test,subtitle_test, reading_time_test, img_test], y_test))\n\nmae = history.history['mae']\nval_mae = history.history['val_mae']\nepochs = range(1, len(mae)+1)\n\nplt.plot(epochs, mae, label='Training mae')\nplt.plot(epochs, val_mae, label='Validation MAE')\nplt.title('Training and validation MAE')\nplt.ylabel('MAE/val_MAE')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (type(sequence_input))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #2 \n# from tensorflow.keras.layers import *\n\n\n# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\n# embedding_layer = Embedding(len(word_index) + 1,\n#                            300,\n#                            weights = [embedding_matrix],\n#                            input_length = MAX_SEQUENCE_LENGTH,\n#                            trainable=False,# prevent re-training the glove vector\n#                            name = 'embeddings')\n# embedded_sequences = embedding_layer(sequence_input)\n# x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n# x = Dense(64, activation=\"relu\")(x)\n# x = Dense(64, activation=\"relu\")(x)\n# x = Dense(64, activation=\"relu\")(x)\n\n# preds = Dense(10)(x)\n\n\n\n# ####\n\n# from tensorflow.keras.models import Model\n\n# model = Model(sequence_input, preds)\n# model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.001),\n#              metrics = ['mae'])\n\n\n\n# ######\n\n# print('Training progress:')\n# history = model.fit(x_train, y_train, epochs = 50, batch_size=64, validation_data=(x_test, y_test))\n\n# mae = history.history['mae']\n# val_mae = history.history['val_mae']\n# epochs = range(1, len(mae)+1)\n\n# plt.plot(epochs, mae, label='Training mae')\n# plt.plot(epochs, val_mae, label='Validation MAE')\n# plt.title('Training and validation MAE')\n# plt.ylabel('MAE/val_MAE')\n# plt.xlabel('Epochs')\n# plt.legend()\n# plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict([x_test,subtitle_test, reading_time_test, img_test]).flatten()\n\n# a = plt.axes(aspect='equal')\nplt.scatter(y_test, test_predictions)\nplt.xlabel('True Values [Claps]')\nplt.ylabel('Predictions [Claps]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\nmae = history.history['mae']\nval_mae = history.history['val_mae']\nepochs = range(1, len(mae)+1)\n\nplt.plot(epochs, mae, label='Training mae')\nplt.plot(epochs, val_mae, label='Validation MAE')\nplt.title('Training and validation MAE')\nplt.ylabel('MAE/val_MAE')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(x_test.shape)\n# print(y_test.shape)\n# test_predictions = model.predict(x_test)\n# print(test_predictions.flatten().shape)\n# print(test_predictions.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# input input\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"User_title = input(\"Your Title is: \")\nUser_subtitle = input(\"Your Subtitle is: \")\n\n\n#User_title_pred = []\n#User_subtitle_pred = []\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nUser_title = re.sub(text_cleaning_re, ' ', str(User_title).lower()).strip()\nUser_subtitle = re.sub(text_cleaning_re, ' ', str(User_subtitle).lower()).strip()\n\nUser_title_tokenised = pad_sequences(tokenizer.texts_to_sequences(User_title),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nUser_subtitle_tokenised = pad_sequences(tokenizer.texts_to_sequences(User_subtitle),\n                        maxlen = MAX_SEQUENCE_LENGTH)\n\nUser_title_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nUser_subtitle_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\n#User_title_pred.append(User_title_tokenised)\n#User_subtitle_pred.append(User_subtitle_tokenised)\n\n\nUser_image_path = \"../input/medium-articles-dataset/images/1.png\"\nUser_image = io.imread(User_image_path)\nUser_image = cv2.cvtColor(User_image, cv2.COLOR_BGR2RGB)\nUser_image = cv2.resize(User_image, IMAGE_SIZE) \nUser_image =tf.convert_to_tensor(list(User_image), dtype = tf.float32)\nprint(User_image.shape)\n\nReading_time = 10\n\nUser_title_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\nUser_subtitle_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\nReading_time_input = Input(shape = (1,))\nUser_image_input = Input(shape = (150,150,3)) # , dtype='object'\n\n\ntest_predictions_user = model.predict([User_title_input,User_subtitle_input, Reading_time_input, User_image_input])\n\nprint (test_predictions_user)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}