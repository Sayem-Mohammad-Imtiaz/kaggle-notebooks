{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Heart Diseases Prediction \nI have collected this dataset from Kaggale ('https://www.kaggle.com/ronitf/heart-disease-uci')\nI analyzed the data in two Machine Learning Algorithosms & also derived which model gives the best prediction about the probabilty of heart diseases. \nTwo ML Algorithms:\n1. KNeighboursClassifier\n2. Random Forest Classifier\n\nI'm also trying to apply rest of the ML algorithm for this data set."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('C:/Users/USER/Downloads/Predicting-Heart-Disease-master/Predicting-Heart-Disease-master/dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"df.info()"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='target',data=df,palette='RdBu_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Processing\n\nCategorical variables are converted into dummy variable & as these variables have different different values they are scaled down."},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nknn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    score=cross_val_score(knn_classifier,X,y,cv=10)\n    knn_scores.append(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_classifier = KNeighborsClassifier(n_neighbors = 12)\nscore=cross_val_score(knn_classifier,X,y,cv=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"randomforest_classifier= RandomForestClassifier(n_estimators=10)\n\nscore=cross_val_score(randomforest_classifier,X,y,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, for this data set KNeighbour Classifier is more efficient than Random Forest Classifier. \n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}