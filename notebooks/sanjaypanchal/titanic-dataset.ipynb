{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-10T16:24:58.635731Z","iopub.execute_input":"2021-08-10T16:24:58.636311Z","iopub.status.idle":"2021-08-10T16:24:58.659449Z","shell.execute_reply.started":"2021-08-10T16:24:58.636216Z","shell.execute_reply":"2021-08-10T16:24:58.658296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import some necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:58.660997Z","iopub.execute_input":"2021-08-10T16:24:58.661305Z","iopub.status.idle":"2021-08-10T16:24:59.677924Z","shell.execute_reply.started":"2021-08-10T16:24:58.661277Z","shell.execute_reply":"2021-08-10T16:24:59.67686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import dataset\n\ntitanic_data = pd.read_csv(\"/kaggle/input/titanicdataset-traincsv/train.csv\")\ntitanic_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.679839Z","iopub.execute_input":"2021-08-10T16:24:59.680152Z","iopub.status.idle":"2021-08-10T16:24:59.738987Z","shell.execute_reply.started":"2021-08-10T16:24:59.680121Z","shell.execute_reply":"2021-08-10T16:24:59.738026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the dataset\n\ntitanic_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.740543Z","iopub.execute_input":"2021-08-10T16:24:59.740855Z","iopub.status.idle":"2021-08-10T16:24:59.747113Z","shell.execute_reply.started":"2021-08-10T16:24:59.740827Z","shell.execute_reply":"2021-08-10T16:24:59.745793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 891 Rows & 12 Columns, According to me columns=[\"PassangerId\",\"Name\",\"Ticket\"] are not useful for the models. So, let's first drop it.","metadata":{}},{"cell_type":"code","source":"# Droping Unusable Columns\n\ntitanic_data.drop(columns = [\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n\nprint(\"Shape of the data --> \",titanic_data.shape)\ntitanic_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.748783Z","iopub.execute_input":"2021-08-10T16:24:59.749188Z","iopub.status.idle":"2021-08-10T16:24:59.774027Z","shell.execute_reply.started":"2021-08-10T16:24:59.749156Z","shell.execute_reply":"2021-08-10T16:24:59.772841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our data is ready for EDA","metadata":{}},{"cell_type":"code","source":"# Quick information about dataset\n\ntitanic_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.775289Z","iopub.execute_input":"2021-08-10T16:24:59.775572Z","iopub.status.idle":"2021-08-10T16:24:59.799347Z","shell.execute_reply.started":"2021-08-10T16:24:59.775545Z","shell.execute_reply":"2021-08-10T16:24:59.798106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that there are missing values in our dataset. Let's check how many missing values are available","metadata":{}},{"cell_type":"code","source":"# Missing Values\n\ntitanic_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.802558Z","iopub.execute_input":"2021-08-10T16:24:59.803193Z","iopub.status.idle":"2021-08-10T16:24:59.813365Z","shell.execute_reply.started":"2021-08-10T16:24:59.803157Z","shell.execute_reply":"2021-08-10T16:24:59.812183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns = [\"Age\", \"Cabin\", \"Embarked\"] have null values. Let's check how many percentages (%) they are covering from the datset.","metadata":{}},{"cell_type":"code","source":"column_data = titanic_data.isna().sum().keys().tolist()\nvalue_data = titanic_data.isna().sum().values.tolist()\n\nmissing_data = pd.DataFrame(list(zip(column_data, value_data)),columns=[\"Columns\", \"Missing Values\"],index=list(range(1,10)))\nmissing_data[\"Missing %\"] = round(((missing_data[\"Missing Values\"]/titanic_data.shape[0])*100),2)\n\nprint(\"\\nMissing Values for Titanic Data :-\")\nmissing_data","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.815539Z","iopub.execute_input":"2021-08-10T16:24:59.815895Z","iopub.status.idle":"2021-08-10T16:24:59.856389Z","shell.execute_reply.started":"2021-08-10T16:24:59.815862Z","shell.execute_reply":"2021-08-10T16:24:59.855643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Cabin\" is covering 77.10 % of null values which is very high. and other are covering less null values.","metadata":{}},{"cell_type":"code","source":"# Let's describe the dataset\n\ntitanic_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.857488Z","iopub.execute_input":"2021-08-10T16:24:59.857917Z","iopub.status.idle":"2021-08-10T16:24:59.890703Z","shell.execute_reply.started":"2021-08-10T16:24:59.857886Z","shell.execute_reply":"2021-08-10T16:24:59.889523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Survive is our target column.\n\nIt seems that some have age and some doesn't have age.\n\nIt seems that Age and Fare column have skewness. Survived is our target column and Pclass,SibSp & Parch looks as classification. So we need to focus on Age & Fare columns for filling null values.","metadata":{}},{"cell_type":"code","source":"# Let's check the \"0\" values if there are present\n\nfor col in titanic_data:\n           print(col +\"  \"+str(titanic_data[titanic_data[col]==0].shape[0]))\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.892405Z","iopub.execute_input":"2021-08-10T16:24:59.892851Z","iopub.status.idle":"2021-08-10T16:24:59.908186Z","shell.execute_reply.started":"2021-08-10T16:24:59.892805Z","shell.execute_reply":"2021-08-10T16:24:59.90739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I discussed about all column in which 0 is present but Fare column is not a classification column and it has 0 values. Let's analyze it.","metadata":{}},{"cell_type":"code","source":"# Let's check Fare column where 0 values are present\n\ntitanic_data[titanic_data[\"Fare\"]==0]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.909416Z","iopub.execute_input":"2021-08-10T16:24:59.909889Z","iopub.status.idle":"2021-08-10T16:24:59.93224Z","shell.execute_reply.started":"2021-08-10T16:24:59.909857Z","shell.execute_reply":"2021-08-10T16:24:59.931177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's describe the categorical data\n\ntitanic_data.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.93364Z","iopub.execute_input":"2021-08-10T16:24:59.933941Z","iopub.status.idle":"2021-08-10T16:24:59.957375Z","shell.execute_reply.started":"2021-08-10T16:24:59.933915Z","shell.execute_reply":"2021-08-10T16:24:59.956492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The frequency of Male is higher than Female, Males have covered 65% data. In Embarked S covered the 644 out of 889.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Let's check pairplot with Survived\n\nplt.figure(figsize=(7,7))\nsns.pairplot(titanic_data,hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:24:59.958547Z","iopub.execute_input":"2021-08-10T16:24:59.95898Z","iopub.status.idle":"2021-08-10T16:25:08.188233Z","shell.execute_reply.started":"2021-08-10T16:24:59.95895Z","shell.execute_reply":"2021-08-10T16:25:08.186861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing \n\nsns.catplot(x=\"Age\",y=\"Sex\",data=titanic_data,hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.189551Z","iopub.execute_input":"2021-08-10T16:25:08.190004Z","iopub.status.idle":"2021-08-10T16:25:08.603032Z","shell.execute_reply.started":"2021-08-10T16:25:08.189961Z","shell.execute_reply":"2021-08-10T16:25:08.601898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that Females are more survive than Males","metadata":{}},{"cell_type":"code","source":"# Let's Handle the Missing Data\n\n\n# Dropna Embarked\ntitanic_data.dropna(subset=[\"Embarked\"],inplace=True)\n\n# Filling Age with its mean value\ntitanic_data.fillna(titanic_data[\"Age\"].mean(), inplace=True)\n\n# Filling Cabin with 0\ntitanic_data.fillna(titanic_data[\"Cabin\"].mode(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.604568Z","iopub.execute_input":"2021-08-10T16:25:08.604925Z","iopub.status.idle":"2021-08-10T16:25:08.61712Z","shell.execute_reply.started":"2021-08-10T16:25:08.604893Z","shell.execute_reply":"2021-08-10T16:25:08.615798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Null Values again\n\ntitanic_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.618967Z","iopub.execute_input":"2021-08-10T16:25:08.619314Z","iopub.status.idle":"2021-08-10T16:25:08.635096Z","shell.execute_reply.started":"2021-08-10T16:25:08.619284Z","shell.execute_reply":"2021-08-10T16:25:08.633941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the skewness\n\ntitanic_data.skew()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.636565Z","iopub.execute_input":"2021-08-10T16:25:08.636925Z","iopub.status.idle":"2021-08-10T16:25:08.654522Z","shell.execute_reply.started":"2021-08-10T16:25:08.636889Z","shell.execute_reply":"2021-08-10T16:25:08.653752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we discussed that SibSp & Parch are classifiation columns, So I have to go with Fare column for making skewness correct","metadata":{}},{"cell_type":"code","source":"# Let's separate the numerica columns and categorical columns\n\nnumerical = titanic_data.drop(columns=[\"Sex\", \"Cabin\", \"Embarked\"])\ncategorical = titanic_data[[\"Sex\", \"Cabin\", \"Embarked\"]]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.655818Z","iopub.execute_input":"2021-08-10T16:25:08.656135Z","iopub.status.idle":"2021-08-10T16:25:08.670135Z","shell.execute_reply.started":"2021-08-10T16:25:08.656102Z","shell.execute_reply":"2021-08-10T16:25:08.669186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualizing the skewness with Distplot\n\nplt.figure(figsize = (10,15), facecolor='white')\ngraphplot = 1\nfor column in numerical:\n    if graphplot <=6:\n        ax = plt.subplot(2,3,graphplot)\n        sns.distplot(numerical[column])\n        plt.xlabel(column, fontsize=15)\n    graphplot+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:08.674169Z","iopub.execute_input":"2021-08-10T16:25:08.674768Z","iopub.status.idle":"2021-08-10T16:25:09.912915Z","shell.execute_reply.started":"2021-08-10T16:25:08.674723Z","shell.execute_reply":"2021-08-10T16:25:09.911909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fare column has skewness and other column have classification problems","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Let's check the outliers\n\nplt.figure(figsize = (15,10), facecolor='white')\ngraphplot = 1\nfor column in numerical:\n    if graphplot<=6:\n        ax = plt.subplot(2,3,graphplot)\n        sns.boxplot(numerical[column])\n        plt.xlabel(column,fontsize=15)\n    graphplot+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:09.915994Z","iopub.execute_input":"2021-08-10T16:25:09.916603Z","iopub.status.idle":"2021-08-10T16:25:10.517183Z","shell.execute_reply.started":"2021-08-10T16:25:09.916556Z","shell.execute_reply":"2021-08-10T16:25:10.516384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers are present in Fare column.","metadata":{}},{"cell_type":"code","source":"# Let's remove the outliers\n\nfrom scipy.stats import zscore\n\nz = np.abs(zscore(numerical))\ntitanic_data = titanic_data[(z<3).all(axis=1)]\ntitanic_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.518201Z","iopub.execute_input":"2021-08-10T16:25:10.518596Z","iopub.status.idle":"2021-08-10T16:25:10.527306Z","shell.execute_reply.started":"2021-08-10T16:25:10.518566Z","shell.execute_reply":"2021-08-10T16:25:10.526351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check skewness again\n\ntitanic_data.skew()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.528531Z","iopub.execute_input":"2021-08-10T16:25:10.528843Z","iopub.status.idle":"2021-08-10T16:25:10.54336Z","shell.execute_reply.started":"2021-08-10T16:25:10.528805Z","shell.execute_reply":"2021-08-10T16:25:10.542201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Skewness is still pending in Fare column. Let's apply another method","metadata":{}},{"cell_type":"code","source":"# Let's remove the pending skewness\n\nfor index in titanic_data.skew().index:\n    if titanic_data.skew().loc[index]>0.5:\n        titanic_data[index] = np.sqrt(titanic_data[index])\n    if titanic_data.skew().loc[index]<-0.5:\n        titanic_data[index] = np.cbrt(titanic_data[index])","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.54473Z","iopub.execute_input":"2021-08-10T16:25:10.545047Z","iopub.status.idle":"2021-08-10T16:25:10.583177Z","shell.execute_reply.started":"2021-08-10T16:25:10.545019Z","shell.execute_reply":"2021-08-10T16:25:10.582054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check skewness again..\n\ntitanic_data.skew()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.584605Z","iopub.execute_input":"2021-08-10T16:25:10.584992Z","iopub.status.idle":"2021-08-10T16:25:10.596031Z","shell.execute_reply.started":"2021-08-10T16:25:10.584958Z","shell.execute_reply":"2021-08-10T16:25:10.594664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Skewness is still pending in Fare column. Let's apply Power Transformer to remove it...","metadata":{}},{"cell_type":"code","source":"# Let's apply Power Transformer to remove the skewness\n\nfrom sklearn.preprocessing import PowerTransformer\n\nfeatures = [\"Pclass\",\"Age\", \"SibSp\", \"Parch\",\"Fare\"]\n\nscaler = PowerTransformer(method='yeo-johnson')\ntitanic_data[features] = scaler.fit_transform(titanic_data[features].values)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.598321Z","iopub.execute_input":"2021-08-10T16:25:10.598882Z","iopub.status.idle":"2021-08-10T16:25:10.749952Z","shell.execute_reply.started":"2021-08-10T16:25:10.59881Z","shell.execute_reply":"2021-08-10T16:25:10.749089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data.skew()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.751198Z","iopub.execute_input":"2021-08-10T16:25:10.752011Z","iopub.status.idle":"2021-08-10T16:25:10.763038Z","shell.execute_reply.started":"2021-08-10T16:25:10.75196Z","shell.execute_reply":"2021-08-10T16:25:10.761997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that skewness has gone from Fare column. Now Let's deal with categorical column.","metadata":{}},{"cell_type":"code","source":"# Let's separate the input and output\n\nx = titanic_data.drop(\"Survived\", axis=1)\ny = titanic_data[\"Survived\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.764431Z","iopub.execute_input":"2021-08-10T16:25:10.764966Z","iopub.status.idle":"2021-08-10T16:25:10.771775Z","shell.execute_reply.started":"2021-08-10T16:25:10.764932Z","shell.execute_reply":"2021-08-10T16:25:10.770359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the correlation between features and target\n\nx.corrwith(y)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.773377Z","iopub.execute_input":"2021-08-10T16:25:10.773763Z","iopub.status.idle":"2021-08-10T16:25:10.797346Z","shell.execute_reply.started":"2021-08-10T16:25:10.773722Z","shell.execute_reply":"2021-08-10T16:25:10.796551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation analysis by ploting a graph between features and target\n\nx.corrwith(y).plot(kind='bar',grid=True, figsize=(8,5),rot=60,title=\"Correlation Between Features & Targer\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:10.798661Z","iopub.execute_input":"2021-08-10T16:25:10.799286Z","iopub.status.idle":"2021-08-10T16:25:11.006671Z","shell.execute_reply.started":"2021-08-10T16:25:10.799238Z","shell.execute_reply":"2021-08-10T16:25:11.005909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 features is correlated in negative and 3 features in correlated in positive but not any features is highly correlated to the target. Now, let's check the multicollinearity between features","metadata":{}},{"cell_type":"code","source":"# Multicollinearity between features\n\ncorr = titanic_data.corr()\ncorr","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.007971Z","iopub.execute_input":"2021-08-10T16:25:11.008554Z","iopub.status.idle":"2021-08-10T16:25:11.027935Z","shell.execute_reply.started":"2021-08-10T16:25:11.008509Z","shell.execute_reply":"2021-08-10T16:25:11.026873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot a heatmap for better analysis the multicollinearity\n\nplt.figure(figsize = (10,10))\nsns.heatmap(corr,cbar=True,square=True,annot=True,cbar_kws={\"shrink\":.82},\n            annot_kws={\"size\":10},cmap=\"Blues\",fmt=\".2f\",linewidths=.2)\nplt.xlabel(\"Multicollinearity between features\",fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.029696Z","iopub.execute_input":"2021-08-10T16:25:11.030153Z","iopub.status.idle":"2021-08-10T16:25:11.455836Z","shell.execute_reply.started":"2021-08-10T16:25:11.030103Z","shell.execute_reply":"2021-08-10T16:25:11.454573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is clear that no any single feature is correlated to another feature. Let's check VIF Score for more analysis.","metadata":{}},{"cell_type":"code","source":"# Converting categorical column to numerical column using OneHotEncoder\n\ncat_to_conv = [\"Sex\", \"Cabin\", \"Embarked\"]\n\ntitanic_data = pd.get_dummies(titanic_data, columns=cat_to_conv)\n\ntitanic_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.457303Z","iopub.execute_input":"2021-08-10T16:25:11.457797Z","iopub.status.idle":"2021-08-10T16:25:11.477148Z","shell.execute_reply.started":"2021-08-10T16:25:11.457746Z","shell.execute_reply":"2021-08-10T16:25:11.476031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset after converting categorical columns to numerical columns\n\ntitanic_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.478876Z","iopub.execute_input":"2021-08-10T16:25:11.479288Z","iopub.status.idle":"2021-08-10T16:25:11.510144Z","shell.execute_reply.started":"2021-08-10T16:25:11.479191Z","shell.execute_reply":"2021-08-10T16:25:11.509144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's separate the input and output again..\n\nX = titanic_data.drop(columns = [\"Survived\"],axis=1)\nY = titanic_data[\"Survived\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.511441Z","iopub.execute_input":"2021-08-10T16:25:11.511803Z","iopub.status.idle":"2021-08-10T16:25:11.518811Z","shell.execute_reply.started":"2021-08-10T16:25:11.51177Z","shell.execute_reply":"2021-08-10T16:25:11.517691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do Scaling first before proceeding to the VIF Score\n\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\nx_scaled = scalar.fit_transform(X)\n\nx_scaled","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.520109Z","iopub.execute_input":"2021-08-10T16:25:11.520414Z","iopub.status.idle":"2021-08-10T16:25:11.543411Z","shell.execute_reply.started":"2021-08-10T16:25:11.520378Z","shell.execute_reply":"2021-08-10T16:25:11.542328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the VIF Score first..\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif[\"VIF Score\"] = [variance_inflation_factor(x_scaled,i) for i in range(x_scaled.shape[1])]\nvif[\"Features\"] = X.columns\n                    \nvif.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:11.544808Z","iopub.execute_input":"2021-08-10T16:25:11.545103Z","iopub.status.idle":"2021-08-10T16:25:16.681329Z","shell.execute_reply.started":"2021-08-10T16:25:11.545068Z","shell.execute_reply":"2021-08-10T16:25:16.680245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the VIF Scores are less than 5. Now it is cross verified that multicollinearity is not exists between the features. Let's do the analysis of target column now.","metadata":{}},{"cell_type":"code","source":"# Target Column (Survived) Analysis\n\ntitanic_data[\"Survived\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:16.686378Z","iopub.execute_input":"2021-08-10T16:25:16.689046Z","iopub.status.idle":"2021-08-10T16:25:16.705462Z","shell.execute_reply.started":"2021-08-10T16:25:16.688972Z","shell.execute_reply":"2021-08-10T16:25:16.704217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot countplot for better analysis\n\nsns.countplot(titanic_data[\"Survived\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:16.711986Z","iopub.execute_input":"2021-08-10T16:25:16.714619Z","iopub.status.idle":"2021-08-10T16:25:16.869069Z","shell.execute_reply.started":"2021-08-10T16:25:16.714563Z","shell.execute_reply":"2021-08-10T16:25:16.868302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that class is not balance. Let's check the percentage of each using pie chart","metadata":{}},{"cell_type":"code","source":"# let's plot a pie chart to check the contribution of each\n\ntitanic_data[\"Survived\"].value_counts().plot.pie(figsize=(5,5), autopct=\"%1.2f\",startangle=90,\n                                                 labels=[\"Not Survived\",\"Survived\"],fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:16.870462Z","iopub.execute_input":"2021-08-10T16:25:16.871115Z","iopub.status.idle":"2021-08-10T16:25:16.974979Z","shell.execute_reply.started":"2021-08-10T16:25:16.871073Z","shell.execute_reply":"2021-08-10T16:25:16.973941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is big difference between classes. Let's do the Oversampling to balance the classes","metadata":{}},{"cell_type":"code","source":"# Oversampling for class Imbalance\n\n\nfrom imblearn.over_sampling import SMOTE\n\nSM = SMOTE()\n\nX,Y = SM.fit_resample(X,Y)\n\nY.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:16.976056Z","iopub.execute_input":"2021-08-10T16:25:16.976324Z","iopub.status.idle":"2021-08-10T16:25:17.431287Z","shell.execute_reply.started":"2021-08-10T16:25:16.976298Z","shell.execute_reply":"2021-08-10T16:25:17.430307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check countplot again...\n\nsns.countplot(Y)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.432847Z","iopub.execute_input":"2021-08-10T16:25:17.433258Z","iopub.status.idle":"2021-08-10T16:25:17.564064Z","shell.execute_reply.started":"2021-08-10T16:25:17.433217Z","shell.execute_reply":"2021-08-10T16:25:17.563046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now It is clear that class are balanced.","metadata":{}},{"cell_type":"code","source":"# Let's do scaling again for the latest oversampled data\n\nfrom sklearn.preprocessing import StandardScaler\n\nsca = StandardScaler()\nx_scal = sca.fit_transform(X)\n\nx_scal","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.565617Z","iopub.execute_input":"2021-08-10T16:25:17.566048Z","iopub.status.idle":"2021-08-10T16:25:17.585939Z","shell.execute_reply.started":"2021-08-10T16:25:17.566005Z","shell.execute_reply":"2021-08-10T16:25:17.584887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of final data\n\ntitanic_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.587428Z","iopub.execute_input":"2021-08-10T16:25:17.58787Z","iopub.status.idle":"2021-08-10T16:25:17.594723Z","shell.execute_reply.started":"2021-08-10T16:25:17.587828Z","shell.execute_reply":"2021-08-10T16:25:17.593529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are so many columns. So, Let's use the PCA\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import explained_variance_score\n\nx_pca = PCA()\nx_pca.fit(x_scal)\n\nvar_cumu = np.cumsum(x_pca.explained_variance_ratio_)*100\n\nk = np.argmax(var_cumu>95)\nprint(\"Number of component explaining 95% variance : \",k)\n\nplt.xlabel(\"Principle Component\", fontsize=15)\nplt.ylabel(\"Cumulative Explained Variance\", fontsize=15)\nplt.axvline(x=k, color='k', linestyle='--')\nplt.axhline(y=95, color='r', linestyle='--')\nplt.plot(var_cumu)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.596572Z","iopub.execute_input":"2021-08-10T16:25:17.597122Z","iopub.status.idle":"2021-08-10T16:25:17.857298Z","shell.execute_reply.started":"2021-08-10T16:25:17.597072Z","shell.execute_reply":"2021-08-10T16:25:17.856162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finalysing the features with PCA for model buildings\n\nPca = PCA(n_components = 131)\ndf_x = Pca.fit_transform(x_scal)\n\ndf_x","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.858874Z","iopub.execute_input":"2021-08-10T16:25:17.859237Z","iopub.status.idle":"2021-08-10T16:25:17.879868Z","shell.execute_reply.started":"2021-08-10T16:25:17.859207Z","shell.execute_reply":"2021-08-10T16:25:17.878783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's import the necessary libraries for models\n\nfrom sklearn.metrics import accuracy_score, plot_roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score\n","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.881103Z","iopub.execute_input":"2021-08-10T16:25:17.881387Z","iopub.status.idle":"2021-08-10T16:25:17.885717Z","shell.execute_reply.started":"2021-08-10T16:25:17.88136Z","shell.execute_reply":"2021-08-10T16:25:17.884427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's find the best random state\n\ndef model(mod):\n    max_auc = 0\n    max_state = 0\n    for i in range(1,201):\n        x_train,x_test,y_train,y_test = train_test_split(df_x,Y,test_size = 0.25, random_state =i)\n        mod.fit(x_train,y_train)\n        y_pred = mod.predict(x_test)\n        auc_scor = accuracy_score(y_test, y_pred)\n        if auc_scor>max_auc:\n            max_auc=auc_scor\n            max_state=i\n    print(\"Best Accuracy Score corresponding to \",max_state,\"is \",auc_scor)\n    print(\"Cross Validation Score : \",cross_val_score(mod,df_x,Y,cv=5).mean())\n    print(\"Classifiation Report-->\\n\\n\",classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.88702Z","iopub.execute_input":"2021-08-10T16:25:17.887357Z","iopub.status.idle":"2021-08-10T16:25:17.902939Z","shell.execute_reply.started":"2021-08-10T16:25:17.887328Z","shell.execute_reply":"2021-08-10T16:25:17.901619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nmodel(LR)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:17.90454Z","iopub.execute_input":"2021-08-10T16:25:17.904919Z","iopub.status.idle":"2021-08-10T16:25:27.73216Z","shell.execute_reply.started":"2021-08-10T16:25:17.904886Z","shell.execute_reply":"2021-08-10T16:25:27.731121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Models Format\n\ndef models(mods):\n    x_train,x_test,y_train,y_test = train_test_split(df_x,Y,test_size = 0.25, random_state =41)\n    mods.fit(x_train,y_train)\n    y_preds = mods.predict(x_test)\n    auc_score = accuracy_score(y_test, y_preds)\n    print(\"Best Accuracy Score corresponding to \",auc_score)\n    print(\"Cross Validation Score : \",cross_val_score(mods,df_x,Y,cv=5).mean())\n    print(\"Classifiation Report-->\\n\\n\",classification_report(y_test,y_preds))          ","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:27.733654Z","iopub.execute_input":"2021-08-10T16:25:27.734261Z","iopub.status.idle":"2021-08-10T16:25:27.742952Z","shell.execute_reply.started":"2021-08-10T16:25:27.734209Z","shell.execute_reply":"2021-08-10T16:25:27.741266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDTR = DecisionTreeClassifier()\n\nmodels(DTR)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:27.744475Z","iopub.execute_input":"2021-08-10T16:25:27.745054Z","iopub.status.idle":"2021-08-10T16:25:28.117565Z","shell.execute_reply.started":"2021-08-10T16:25:27.745012Z","shell.execute_reply":"2021-08-10T16:25:28.116519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Neighbors Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nKNC = KNeighborsClassifier()\n\nmodels(KNC)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:28.118989Z","iopub.execute_input":"2021-08-10T16:25:28.119555Z","iopub.status.idle":"2021-08-10T16:25:28.436573Z","shell.execute_reply.started":"2021-08-10T16:25:28.119512Z","shell.execute_reply":"2021-08-10T16:25:28.43555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nGBC = GradientBoostingClassifier()\n\nmodels(GBC)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:28.4379Z","iopub.execute_input":"2021-08-10T16:25:28.43822Z","iopub.status.idle":"2021-08-10T16:25:43.233032Z","shell.execute_reply.started":"2021-08-10T16:25:28.438188Z","shell.execute_reply":"2021-08-10T16:25:43.231792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bagging Classifier\n\nfrom sklearn.ensemble import BaggingClassifier\n\nBC = BaggingClassifier()\n\nmodels(BC)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:43.235985Z","iopub.execute_input":"2021-08-10T16:25:43.236291Z","iopub.status.idle":"2021-08-10T16:25:45.15043Z","shell.execute_reply.started":"2021-08-10T16:25:43.236262Z","shell.execute_reply":"2021-08-10T16:25:45.149533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\n\nmodels(RFC)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:45.152855Z","iopub.execute_input":"2021-08-10T16:25:45.153454Z","iopub.status.idle":"2021-08-10T16:25:48.491713Z","shell.execute_reply.started":"2021-08-10T16:25:45.153402Z","shell.execute_reply":"2021-08-10T16:25:48.490576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Let's check ROC AUC Curve for the fitted model****","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(df_x,Y,test_size = 0.25, random_state =41)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:48.497447Z","iopub.execute_input":"2021-08-10T16:25:48.497779Z","iopub.status.idle":"2021-08-10T16:25:48.504701Z","shell.execute_reply.started":"2021-08-10T16:25:48.49775Z","shell.execute_reply":"2021-08-10T16:25:48.503554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = plot_roc_curve(LR, x_test, y_test)\nplot_roc_curve(DTR, x_test, y_test, ax=disp.ax_)\nplot_roc_curve(KNC, x_test, y_test, ax=disp.ax_)\nplot_roc_curve(BC, x_test, y_test, ax=disp.ax_)\nplot_roc_curve(RFC, x_test, y_test, ax=disp.ax_)\nplot_roc_curve(GBC, x_test, y_test, ax=disp.ax_)\n\nplt.legend(prop={\"size\":10},loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:48.506128Z","iopub.execute_input":"2021-08-10T16:25:48.506408Z","iopub.status.idle":"2021-08-10T16:25:48.857763Z","shell.execute_reply.started":"2021-08-10T16:25:48.506382Z","shell.execute_reply":"2021-08-10T16:25:48.856509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After analyzing ROC AUC Curve the Accuray Scores & Cross Validatio Scores of above 6 models, I choose Logistic Regression becasue this is the only model which is not getting the difference in accuracy score after cross validation and It is looking stable with it's score. Now let's try to increase the score by applying hyperparameter tuning**","metadata":{}},{"cell_type":"code","source":"# Hyperparameter Tuning with Logistic Regression\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\"penalty\" : ['l1','l2'], \"intercept_scaling\" : [1,2,3],\n             \"random_state\" : [81,131,151], \"max_iter\" : [100,200]}\n\ngrid_srch = GridSearchCV(LR, param_grid=param_grid)\ngrid_srch.fit(df_x,Y)\ngrid_srch.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:48.859373Z","iopub.execute_input":"2021-08-10T16:25:48.859811Z","iopub.status.idle":"2021-08-10T16:25:53.590972Z","shell.execute_reply.started":"2021-08-10T16:25:48.859767Z","shell.execute_reply":"2021-08-10T16:25:53.58997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Model**","metadata":{}},{"cell_type":"code","source":"Final_Model = LogisticRegression(intercept_scaling=1, max_iter=100, penalty='l2', random_state=81)\n\nx_train,x_test,y_train,y_test = train_test_split(df_x,Y,test_size = 0.25, random_state =41)\nFinal_Model.fit(x_train,y_train)\ny_preds = Final_Model.predict(x_test)\nauc_scr = accuracy_score(y_test, y_preds)\nprint(\"Best Accuracy Score corresponding to \",auc_scr)\nprint(\"Classifiation Report-->\\n\\n\",classification_report(y_test,y_preds))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:53.592627Z","iopub.execute_input":"2021-08-10T16:25:53.593382Z","iopub.status.idle":"2021-08-10T16:25:53.666722Z","shell.execute_reply.started":"2021-08-10T16:25:53.593335Z","shell.execute_reply":"2021-08-10T16:25:53.665617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Saving The Model**","metadata":{}},{"cell_type":"code","source":"import joblib\n\njoblib.dump(Final_Model,\"Final_Titanic_Project_Model.plk\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:25:53.668582Z","iopub.execute_input":"2021-08-10T16:25:53.66935Z","iopub.status.idle":"2021-08-10T16:25:53.680794Z","shell.execute_reply.started":"2021-08-10T16:25:53.6693Z","shell.execute_reply":"2021-08-10T16:25:53.679451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}