{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9ef5e42b-f505-a34f-e3cd-e2de94862fe1"},"source":"## Breast cancer data analysis:  colinearity and dimensional reduction"},{"cell_type":"markdown","metadata":{"_cell_guid":"5d7737eb-30ea-6979-5401-4563c3149f50"},"source":"Data source: Kaggle (https://www.kaggle.com/uciml/breast-cancer-wisconsin-data).\nAlso can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number \n2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. \n* In this blog, analyses of collinearity and dimensional reduction are discussed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3dca1759-d849-1ce7-b6a1-b977f85938a8"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6775499-768b-c3d5-5e28-cc6e946a10c1"},"outputs":[],"source":"# First, we import the downloaded cvs data \nbc=pd.read_csv(\"../input/data.csv\")\nbc.head(10)        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e48e2f64-8ff0-5984-3696-1e29be3f2361"},"outputs":[],"source":"# slicing out the predictor variables/features\nX=bc.iloc[:,2:32]\nprint(type(X))\n# displaying the first five rows of X\nX.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93c91de9-420c-698b-264f-9ed334cfef7a"},"outputs":[],"source":"# response variable: 'diagnosis'\nyr=bc.diagnosis\nprint(type(yr))\nyr.head()\n# we use pandas get_dummies function to convert the qualitative values in yr to quantitative \nyd=pd.get_dummies(yr) # this creates two columns: one for B and another for M\nprint(yd.head())\n# take the yd.M as the response variable (y) since it assigns 1 for M and 0 for B\n# this makes sense as we are interested more in predicting M cases than B\ny=yd.M \ny.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b58e45f-1ec9-5aa2-98ae-76cfc18d9734"},"source":"### 1.  Identifying Collinearity\n* Now, looking at the covariance matrix of the features, we can identify variables with more than 85% correlation \n(85% selected as a threshold here, but other values can be used as well)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c5e5dc7-682b-f823-09b0-7c9366d447a1"},"outputs":[],"source":"cor=np.corrcoef(X.T)\ntype(cor)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(10, 8))\nsns.heatmap(cor, vmin=0.85,vmax=1,\\\n            cmap=plt.cm.Spectral_r)\n#cmap='coolwarm'#"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f861f31-32eb-f45e-2b4a-d7d20f34903f"},"source":"*From the above plot, we can see the following features are highly correlated:*\n\n* feature 0,2,3, 20, 22, 23 (radius, area, perimeter),this makes sense since area and perimeter are computed from radius. We keep 0 (mean radius) and 20 (worst radius), and discard the rest.\n* feature 10, 12, 13 are  highly correlated to eachother. Keeping feature 10, we can leave the rest out. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4bacb4c8-f826-3df0-fa2d-905e9ecbe3de"},"source":"**Summary:**\n\n* covariance matrix can only detect one-to-one collinearity leaving the issue of multi-collinearity\n* High multi-collinearity can be detected by inspecting the eigen values of correlation matrix. A very low eigen value shows that the data are collinear, and the corresponding eigen vector shows which variables are collinear.\n* If there is no collinearity in the data, you would expect that none of the eigen values are close to zero"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f3c091d-fc8c-8a01-19ad-460b67b3b2f8"},"source":"### 2. Dimensional reduction using Principla Component Analysis (PCA)\n\n* PCA is a technique that extracts a smaller but valuable set of predictors by linear combination of the original *p* predictors (x1,x2,x3,...,xp) in a dataset. The new set of predictors captures as much information as possible usually 95% or more of the variance in the original data.  \n* The new predictors can be written as: \n    * PC1 = Φ¹¹X¹ + Φ²¹X² + Φ³¹X³ + .... +Φp¹Xp\n    * PC2 = Φ¹²X¹ + Φ²²X² + Φ³²X³ + .... + Φp2Xp\n    ...\n    * Φp¹, and Φp2 are the loading vectors of the first and the second  principal components,respectively. \n* In this blog, we apply PCA to the original data, and differnet versions of it and try to understand how results from PCA vary.\n** Data to be used: raw/original data, scaled or standardized data, and scaled data with the removal of collinear predictors."},{"cell_type":"markdown","metadata":{"_cell_guid":"0be01103-1bbb-235a-91e0-b0aa3a6abeba"},"source":"#### 2.1. PCA on original data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9795c1ab-0f01-6b81-7e74-303ca165d1d6"},"outputs":[],"source":"from sklearn.decomposition import PCA\npca=PCA(n_components=10)\nx_pca=pca.fit_transform(X)\nprint (x_pca.shape)\nprint(pca.explained_variance_ratio_) \nprint (pca.explained_variance_ratio_.sum())\nloadings = pca.components_"},{"cell_type":"markdown","metadata":{"_cell_guid":"609e13f9-5646-4533-cb4e-1c9ed48ab15b"},"source":"* The above result shows more than 95% of the variance in the data can be expressed just by the first component. This happens in cases where there is high collinearity; the presence of collinearity can cause the PCA to overemphasize the contribution of the variance from the highly correlated (or redundant) varibles and gives less weight to the variables to the uncorrelated variables. This ultimately influence modeling/prediction results if such data is used. \n\n* It is also important to note that **we used PCA on unscaled data**. When the data is not scaled, PCA might  give us a result where only one component explains almost all the variance in the data.  "},{"cell_type":"markdown","metadata":{"_cell_guid":"33d20789-bbf6-b2df-e0a0-cd2e12b3a8c7"},"source":"#### 2.2. PCA on scaled/standardized data\n* Standardizing transforms the data to a data with zero mean and a standard deviation of one (unit variance).\n* In cases where the original predictors may have different scales, it is obvious the resulting variances in these predictors will be large. Therefore, performing PCA on unstandardized predictors will result a biased result; PCA assigns large loadings on the predictors with high variances."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e605b48-9174-076a-2b90-8d3044f5a574"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled=scaler.fit_transform(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d404c49-b169-ac37-5ee3-ffcfc79f1896"},"outputs":[],"source":"pca=PCA(n_components=0.95)\nx_pca=pca.fit_transform(X_scaled)\nprint (x_pca.shape)\nprint(pca.explained_variance_ratio_) \nprint (pca.explained_variance_ratio_.sum())\nloadings = pca.components_"},{"cell_type":"raw","metadata":{"_cell_guid":"b10b233f-9a77-5530-2a46-1da8ac1c8953"},"source":"* Here, it can be noted, 10 components explain 95% of the variance in the data compared to just 1 when applying PCA on the unscaled data. By using the scaled data PCA projects more features/variables which are crucial to breast cancer prediction. \n* From this, we can conclude that PCA with out scaling can hide features with smaller magnitude which are usable to the prediction. \n* We can further see how removing collinear features from the scaled can affect PCA here below."},{"cell_type":"markdown","metadata":{"_cell_guid":"a9fd8ef4-c0a6-1432-52f7-1738f8dbe42e"},"source":"#### 2.3. PCA after removing collinear variables\n* Here, PCA is applied on the saled data after removing some of the collinear features as identified in section 1.  We aim to see how removing redundant/colinear variables can improve PCA results."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4696481b-c8eb-62c9-d766-91c7e04bea0b"},"outputs":[],"source":"# Remove area, perimeter,perimeter_worst, area_worst, perimeter_se, area_se  ( columns: 2,3,12,13,22,23,) from the original data: \nX_scaled=pd.DataFrame(X_scaled)\nxx=X_scaled.drop(X_scaled.columns[[2, 3, 22, 23, 12, 13]], axis=1) \npca=PCA(n_components=0.95)\nx_pca=pca.fit_transform(xx)\nprint (x_pca.shape)\nprint(pca.explained_variance_ratio_) \nprint (pca.explained_variance_ratio_.sum())\nloadings = pca.components_"},{"cell_type":"markdown","metadata":{"_cell_guid":"c59de6ca-2ba5-31b9-c5de-a798a5a849f3"},"source":"##### Variance plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b65b308-2860-c1d3-e792-a87644543b83"},"outputs":[],"source":"import matplotlib.pyplot as plt\n\nn=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\nindex = np.arange (10)\n\nvar_1=np.array([9.82044672e-01,1.61764899e-02,1.55751075e-03,1.20931964e-04,8.82724536e-05,6.64883951e-06,4.01713682e-06,8.22017197e-07,3.44135279e-07,1.86018721e-07])                \nvar_sc=np.array([0.44272026,0.18971182,0.09393163,0.06602135,0.05495768,0.04024522,0.02250734,0.01588724,0.01389649,0.01168978])\nvar_sc_coll=np.array([0.42661046,0.15932139,0.10294428,0.07788731,0.06489774,0.05015242,0.02145044,0.0187846,0.01505759,0.01197751])\nvar=np.vstack([var_1,var_sc,var_sc_coll]).T\n\n# creating pandas datframe from numpy array 'var'\ndf=pd.DataFrame(var,index=n,columns=['original','scaled','scaled_no_colli'])\n\n# plotting variance data \n%matplotlib inline\nfig, ax= plt.subplots(1,3,sharey=True,figsize=(20,4))\nr1 = ax[0].bar(index,df['original'],width = 0.6,align='center')\nax[0].set_title('PCA on raw data',fontsize=14)\n\nr2 = ax[1].bar(index,df['scaled'],width = 0.6,align='center')\nax[1].set_title('PCA on scaled data',fontsize=14)\n\nr3 = ax[2].bar(index,df['scaled_no_colli'],width= 0.6,align='center')\n#ax[2].set_xticklabels(n)\nax[2].set_title('PCA after removing collinearity',fontsize=14)"},{"cell_type":"markdown","metadata":{"_cell_guid":"20f00f9d-62e0-9dac-6acc-d8fef67cc558"},"source":"**Summary:**\n\n* By stanrdizing the data, PCA performance has improved quite well in capturing variance of the raw data\n* Removing collinear features allowed the variance from other variables which were previously ignored to be captured by PCA. "},{"cell_type":"markdown","metadata":{"_cell_guid":"09a9b871-28c2-4946-95d2-a0210c372423"},"source":"#### Ploting first and second principal components against diagnosis\n* First principal component (PC1) is a linear combination of the features/variables which represents the maximum variance in the data set. \n* Second principal component (PC2) is also a linear combination of the features/variables which represents the remaining variance in the data set and is uncorrelated with the first principal component (PC1)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"075e2ad6-38fe-072a-dbe7-bea8bdfa2bda"},"outputs":[],"source":"import pandas as pd\nn=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11']\nxx_pca=pd.DataFrame(x_pca,columns = n, index=bc.diagnosis)\nxx_pca.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43e71f07-f4de-4a99-97fd-a80a45e5cfd2"},"outputs":[],"source":"#plotting the first 2 pca compenents vs diagnosis\n%matplotlib inline\nax = xx_pca.plot(kind='scatter', x='PC1', y='PC2', figsize=(16,8))\nfor i, diagnosis in enumerate(xx_pca.index):\n    ax.annotate(diagnosis,(xx_pca.iloc[i].PC1, xx_pca.iloc[i].PC2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4580fc67-4940-e8ff-758a-c993a9db0b9e"},"source":"**Summary:**\n\n* PCA resulted clear regions of benine and malignant cases (as the above plot shows). This helps the modeling/prediction be more effective and accurate.\n* After this, a model can be fitted on this data."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}