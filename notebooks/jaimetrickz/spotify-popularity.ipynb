{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Spotify song popularity prediction\n\nBased on the notebook:\n\nhttps://www.kaggle.com/thomaskonstantin/top-spotify-songs-analysis-modeling-and-prediction\n\nI tried to elaborate multiple models to predict the data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.autograd import Variable as V","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/top50spotify2019/top50.csv\", encoding=\"ISO-8859-1\", index_col= [\"Unnamed: 0\"])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Genre_Enc= LabelEncoder().fit_transform(df[\"Genre\"])\ndf.insert(3,'Genre_Enc',Genre_Enc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nheatmap = sns.heatmap(df.corr(), vmin=-1,vmax=1, annot=True, cmap='viridis')\n\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Popularity is:\nCorrelated directly with:\n* Genre\n* Speechiness\n* Beats.Per.Minute\n\nInverse correlated with:\n* Valence\n\nNon correlated to others."},{"metadata":{},"cell_type":"markdown","source":"## Model Popularity "},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df.Popularity\nX = df.drop(['Popularity','Track.Name','Artist.Name','Genre'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df, x= range(0,df.shape[0]), y = sorted(Y),\n             hover_name = \"Track.Name\",hover_data= ['Artist.Name', 'Genre'])\n\nfig.update_layout(title= 'Popularity sorted', \n                  xaxis =dict(title='Song index'),\n                  yaxis =dict(title='Popularity'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFR = RandomForestRegressor(random_state=42,n_jobs=2)\n\nRFR_error = -1*cross_val_score(RFR,X,Y,cv=10, scoring='neg_mean_squared_error')\nRFR.fit(X,Y)\n\nprint('RFR max error by CV is: {:.3f} \\nRFR min error by CV is: {:.3f} \\nRFR mean error by CV is: {:.3f}'.\n      format(RFR_error.max(),RFR_error.min(),RFR_error.mean()))\n\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(RFR.predict(X)), label='Random Forest Prediction')\n\nplt.title(\"Random Forest prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\n\nRFR_mse = mse(RFR.predict(X), Y)\nprint(\"RFR mean squared error is: {:.4f}\".format(RFR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBR = GradientBoostingRegressor(random_state=42)\n\nGBR_error = -1*cross_val_score(GBR,X,Y,cv=10, scoring='neg_mean_squared_error')\nGBR.fit(X,Y)\n\nprint('GBR max error by CV is: {:.3f} \\nGBR min error by CV is: {:.3f} \\nGBR mean error by CV is: {:.3f}'.\n      format(GBR_error.max(),GBR_error.min(),GBR_error.mean()))\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(GBR.predict(X)), label='Gradien Boosting Prediction')\n\nplt.title(\"Gradient Boosting prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\n\nGBR_mse = mse(GBR.predict(X), Y)\nprint(\"GBR mean squared error is: {:.4f}\".format(GBR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Neighbours Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNR = KNeighborsRegressor(n_jobs=2)\n\nKNR_error = -1*cross_val_score(KNR,X,Y,cv=10, scoring='neg_mean_squared_error')\nKNR.fit(X,Y)\n\nprint('KNR max error by CV is: {:.3f} \\nKNR min error by CV is: {:.3f} \\nKNR mean error by CV is: {:.3f}'.\n      format(KNR_error.max(),KNR_error.min(),KNR_error.mean()))\n\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(KNR.predict(X)), label='K Neighbors Prediction')\n\nplt.title(\"K Neighbors prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\n\nKNR_mse = mse(KNR.predict(X), Y)\nprint(\"KNR mean squared error is: {:.4f}\".format(KNR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVRR = SVR(kernel='linear')\n\nSVRR_error = -1*cross_val_score(SVRR,X,Y,cv=10, scoring='neg_mean_squared_error')\nSVRR.fit(X,Y)\n\nprint('SVR max error by CV is: {:.3f} \\nSVR min error by CV is: {:.3f} \\nSVR mean error by CV is: {:.3f}'.\n      format(SVRR_error.max(),SVRR_error.min(),SVRR_error.mean()))\n\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(SVRR.predict(X)), label='Support Vector Prediction')\n\nplt.title(\"Support Vector prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\n\nSVRR_mse = mse(SVRR.predict(X), Y)\nprint(\"SVR mean squared error is: {:.4f}\".format(SVRR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LinearRegression()\n\nLR_error = -1*cross_val_score(LR,X,Y,cv=10, scoring='neg_mean_squared_error')\nLR.fit(X,Y)\n\nprint('LR min error by CV is: {:.3f} \\nLR min error by CV is: {:.3f} \\nLR mean error by CV is: {:.3f}'.\n      format(LR_error.max(),LR_error.min(),LR_error.mean()))\n\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(LR.predict(X)), label='Linear Prediction')\n\nplt.title(\"Linear prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.75,0.15))\nplt.show()\n\nLR_mse = mse(LR.predict(X), Y)\nprint(\"RFR mean squared error is: {:.4f}\".format(LR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obtaining a model\n### Model of models by Cross Validation error"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(16,9))\n\nplt.plot(RFR_error, label='Random Forest')\nplt.plot(GBR_error, label='Gradient Boosting')\nplt.plot(KNR_error, label='K Neighbours')\nplt.plot(SVRR_error, label= 'Support Vector')\nplt.plot(LR_error, label= 'Linear')\n\n\nfig.suptitle(\"Cross Validation Error\")\nfig.legend(loc='upper right',bbox_to_anchor=(0.8,0.75))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Giving weights to prediction models based on their accuracy\nmean_errors = np.array([RFR_error.mean(),GBR_error.mean(),KNR_error.mean(),SVRR_error.mean(),LR_error.mean()])\nmin_errors = np.array([RFR_error.min(),GBR_error.min(),KNR_error.min(),SVRR_error.min(),LR_error.min()])\nmax_errors = np.array([RFR_error.max(),GBR_error.max(),KNR_error.max(),SVRR_error.max(),LR_error.max()])\n\nRFR_w0,GBR_w0,KNR_w0,SVRR_w0,LR_w0 = mean_errors/(mean_errors.sum())\nRFR_w1,GBR_w1,KNR_w1,SVRR_w1,LR_w1 = min_errors/(min_errors.sum())\nRFR_w2,GBR_w2,KNR_w2,SVRR_w2,LR_w2 = max_errors/(max_errors.sum())\n\nRFR_w = (RFR_w0+RFR_w1+RFR_w2)/3\nGBR_w = (GBR_w0+GBR_w1+GBR_w2)/3\nKNR_w = (KNR_w0+KNR_w1+KNR_w2)/3\nSVRR_w = (SVRR_w0+SVRR_w1+SVRR_w2)/3\nLR_w = (LR_w0+LR_w1+LR_w2)/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RGKSL0_Ypred = RFR_w0*RFR.predict(X)+GBR_w0*GBR.predict(X)+KNR_w0*KNR.predict(X)+SVRR_w0*SVRR.predict(X)+LR_w0*LR.predict(X)\nRGKSL0_mse = mse(RGKSL0_Ypred,Y)\n\nRGKSL1_Ypred = RFR_w1*RFR.predict(X)+GBR_w1*GBR.predict(X)+KNR_w1*KNR.predict(X)+SVRR_w1*SVRR.predict(X)+LR_w1*LR.predict(X)\nRGKSL1_mse = mse(RGKSL1_Ypred,Y)\n\nRGKSL2_Ypred = RFR_w2*RFR.predict(X)+GBR_w2*GBR.predict(X)+KNR_w2*KNR.predict(X)+SVRR_w2*SVRR.predict(X)+LR_w2*LR.predict(X)\nRGKSL2_mse = mse(RGKSL2_Ypred,Y)\n\nRGKSL_Ypred = RFR_w*RFR.predict(X)+GBR_w*GBR.predict(X)+KNR_w*KNR.predict(X)+SVRR_w*SVRR.predict(X)+LR_w*LR.predict(X)\nRGKSL_mse = mse(RGKSL_Ypred,Y)\n\n\nprint('RGKSL0 model has a mean squared error of : {:.3f}'.format(RGKSL0_mse) )\nprint('RGKSL1 model has a mean squared error of : {:.3f}'.format(RGKSL1_mse) )\nprint('RGKSL2 model has a mean squared error of : {:.3f}'.format(RGKSL2_mse) )\nprint('RGKSL model has a mean squared error of : {:.3f}'.format(RGKSL_mse) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value')\nplt.plot(sorted(RGKSL0_Ypred), label='w with Mean Error')\nplt.plot(sorted(RGKSL1_Ypred), label='w with Min Error')\nplt.plot(sorted(RGKSL2_Ypred), label='w with Max Error')\nplt.plot(sorted(RGKSL_Ypred), label= 'w with Mean(Min,Max,Mean) Error')\n\n\nfig.suptitle(\"Comparison between weights choosing method\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best option for building a linear model of previous models is to choose the weights based on the mean error or min error of the *mean_squared_error* function from cross validation."},{"metadata":{},"cell_type":"markdown","source":"### Model of models by MSE error"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Giving weights to prediction models based on their mse\nmse_errors = np.array([RFR_mse,GBR_mse,KNR_mse,SVRR_mse,LR_mse])\n\nRFR_wmse,GBR_wmse,KNR_wmse,SVRR_wmse,LR_wmse = (1/mse_errors)/((1/mse_errors).sum())\n\n# Building the model\n\nRGKSL_mse_Ypred = RFR_wmse*RFR.predict(X)+GBR_wmse*GBR.predict(X)+KNR_wmse*KNR.predict(X)+SVRR_wmse*SVRR.predict(X)+LR_wmse*LR.predict(X)\nRGKSL_mse_MSE = mse(RGKSL_mse_Ypred,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value',lw=2, alpha= 0.7)\nplt.plot(sorted(RGKSL_mse_Ypred), label='w with previous mse')\n\nplt.title(\"Model of models with MSE prediction\")\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.show()\nprint('RGKSL_mse model has a mean squared error of : {:.3f}'.format(RGKSL_mse_MSE) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nx = torch.from_numpy(X.values.astype(np.float32)).reshape(-1,10).to(device)\ny = torch.from_numpy(np.array(Y).astype(np.float32)).reshape(-1,1).to(device)\n\ninput_shape, hidden_shape1, hidden_shape2, output_shape, EPOCH = 10, 500, 10, 1, 3001\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(input_shape, hidden_shape1),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_shape1, hidden_shape2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_shape2, output_shape),\n).to(device)\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\nloss_fn = torch.nn.MSELoss()\n\n\nfor epoch in range(EPOCH):\n\n    pred = model(x)\n    loss = loss_fn(pred, y)\n    \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if epoch % 500 == 0:\n        print('EPOCH: {}. Training loss: {:.4f}'.format(epoch,loss_fn(model(x), y)))\n\nNNR_Ypred = model(x).detach().numpy()\n\nfig= plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),sorted(Y), label='Actual Value')\nplt.plot(sorted(NNR_Ypred), '+-',label='Neural Network')\n\nfig.suptitle(\"Neural Network Regression\")\nfig.legend(loc='lower right',bbox_to_anchor=(0.85,0.15))\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nplt.show()\nNNR_mse = mse(NNR_Ypred,Y)\nprint(\"NNR has a mean squared error of: {:.4f}\".format(NNR_mse))\nprint(\"Applying a Neural Network of two hidden layers seems to be the best option but may suffer from overfitting.\\nThe error is clearly the best, dropping from 0.034 to {:.4f}.\".format(NNR_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\n\nplt.plot(range(0,50),Y, label='Actual Value',lw=10,alpha= 0.5,color='black')\nplt.plot(NNR_Ypred,label='Neural Network',color='green')\nplt.plot(RGKSL0_Ypred, label='Regressor of CV error',color='orange')\nplt.plot(RGKSL_mse_Ypred, label='Regressor of MSE error',color='magenta')\nplt.plot(RFR.predict(X), label='Random Forest',color= 'blue')\nplt.plot(GBR.predict(X), label='Gradient Boosting',color='red')\nplt.plot(KNR.predict(X), label='K Neighbors',color='pink')\nplt.plot(SVRR.predict(X), label='Support Vector', color='yellow')\nplt.plot(LR.predict(X), label='Linear')\n\nplt.title(\"Model predicted values\")\nplt.legend(loc='best')\nplt.xlabel(\"Items\")\nplt.ylabel(\"Popularity\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\n\nheight=np.array([RFR_mse,GBR_mse,KNR_mse,SVRR_mse,LR_mse,RGKSL0_mse,RGKSL_mse_MSE,NNR_mse])\nx_mse = ['Random_Forest', 'Gradient_Boosting','K_Neighbours','Support_Vector','Linear',\n        'Reg_of_CV','Reg_of_MSE','Neural_Network']\n\ndf_sns = pd.DataFrame({'Model':x_mse,\n                      'Mean Squared Error':height})\ndf_sns.sort_values(by=['Mean Squared Error'],inplace=True)\nsns.barplot(data=df_sns,y='Model',x='Mean Squared Error')\n\nplt.title('Model ranking by MSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm starting with neural networks. If you have any advice please comment it so I can improve.\n\nThank you for reading. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}