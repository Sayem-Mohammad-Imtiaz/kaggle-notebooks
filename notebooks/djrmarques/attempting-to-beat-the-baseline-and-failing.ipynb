{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Estonia Disaster analysis\nIn this notebook I try, and fail, to beat the 86% baseline using XGBOOST. I still decided to publish this just because I feel that i managed to create a nice enough pipeline that someone can just tweak the XGBOOST, or custom transformer, and try to get a better result. \n\nI have a few ideas for other models that I will try as soon as I have the time. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nimport xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Custom transformer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataProcess(BaseEstimator, TransformerMixin):\n    \"\"\" This is just a transformer that I will feed into a pipeline  \"\"\"\n    \n    def __init__(self):\n        self.columns_to_drop = [\"Firstname\", \"Lastname\", \"PassengerId\"]\n        self.country_ratio_param = pd.Series(np.nan)  ## This will be se tby fit\n        self.has_family_map = pd.Series(np.nan)\n    \n    def transform(self, X, y=None):\n        X_ = X.copy()\n        X_[\"Sex\"] = X_[\"Sex\"].map({\"M\": 0, \"F\": 1})\n        X_[\"Category\"] = X_[\"Category\"].map({\"C\": 0, \"P\": 1})\n        \n        X_[\"swedish\"] = X_[\"Country\"].apply(lambda x: x == \"Sweden\")\n        X_[\"estonian\"] = X_[\"Country\"].apply(lambda x: x == \"Estonia\")\n        X_[\"Country\"] = X_[\"Country\"].map(self.country_ratio_param)\n        \n        X_[\"has_family\"] = X_[\"Lastname\"].map(self.has_family_map)\n        \n        X_.drop(self.columns_to_drop, inplace=True, axis=1)\n        \n        assert not X_.isna().any().any(), f\"Missing values found: {X_.isna().any()}\"\n        \n        return X_\n    \n    def fit(self, X, y):\n        \"\"\" There is not anything to fit here \"\"\"\n        X_ = X.copy()\n        X_[\"Survived\"] = y\n        self.country_ratio_param = df.groupby(\"Country\")[\"Survived\"].apply(lambda x: x.sum()/x.shape[0]).sort_values()\n        self.has_family_map = df.groupby(\"Lastname\", as_index=False)[\"Firstname\"].apply(lambda x: x.shape[0] > 1).set_index(\"Lastname\").squeeze()\n    \n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n    \nprint(\"Processed DataFrame\")\nDataProcess().fit_transform(X=df.drop(\"Survived\", axis=1), y=df[\"Survived\"]).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nFor now, let's just create some visualizations!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Number of deaths per country\nWhich countries suffered the highest casualties? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"survivors_by_country = df.groupby([\"Country\", \"Survived\"])[\"Age\"].count().sort_values().reset_index()\nsurvivors_by_country[\"Survived\"] = survivors_by_country[\"Survived\"].astype(str)\nsurvivors_by_country.rename(columns={\"Age\": \"nPeople\"}, inplace=True)\nfig = px.bar(survivors_by_country, x=\"Country\", y=\"nPeople\", color=\"Survived\")\nfig.layout.yaxis.title = \"# People\"\nfig.layout.title = \"# Survivors and Total passenders by country\"\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Country Survivor Ratio\nThis is the ratio of survivors. In some instances this might not be at all significant, given that there are countries that only have 1 or two people on board.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country_survivor_ratio = df.groupby(\"Country\")[\"Survived\"].apply(lambda x: x.sum()/x.shape[0]).sort_values()\nfig = px.bar(country_survivor_ratio.reset_index(), x=\"Country\", y=\"Survived\", title=\"Surviving Ratio per Country\")\nfig.layout.yaxis.title = \"Survivor Ratio [%]\"\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of survivors per Category and Gender. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = df.groupby([\"Category\", \"Sex\"])[\"Survived\"].sum().reset_index()\npx.bar(grouped, y=\"Survived\", x=\"Category\", color=\"Sex\", title=\"Number of survivors per Category and Gender\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting an xbgoost classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# I used a bigger param grid on previous commits. \nparams = {\n 'xgboost__booster': ['gbtree'], \n 'xgboost__colsample_bytree': [0.8], \n 'xgboost__eta': [0.05],\n 'xgboost__eval_metric': ['error'], \n 'xgboost__gamma': [0.5], \n 'xgboost__max_depth': [5],\n 'xgboost__min_child_weight': [1], \n 'xgboost__n_estimators': [100], \n 'xgboost__subsample': [1.0]}\n\npipeline = Pipeline([(\"data_process\", DataProcess()), (\"xgboost\", xgboost.XGBRFClassifier())])\nclf = GridSearchCV(pipeline, cv=StratifiedShuffleSplit(6, random_state=1), n_jobs=-1, scoring=[\"f1\", \"accuracy\"], refit=\"accuracy\",  param_grid=params)\n\nX = df.drop(\"Survived\", axis=1)\ny = df[\"Survived\"]\nclf.fit(X, y)\nprint(clf.best_params_)\npd.DataFrame(clf.cv_results_).sort_values(\"rank_test_f1\").head(1).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance\nNow let's see what out classifier considers the most important features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importance\n_, ax = plt.subplots(figsize=(20, 5))\nxgboost.plot_importance(clf.best_estimator_[-1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(30, 30))\nxgboost.plot_tree(clf.best_estimator_[-1], ax=ax, num_trees=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nI did not manage the beat this baseline, I got 86.7% accuracy. I decide to publish this notebook now because I ran out of ideas for now, but there is a separate model that I want to try (maybe next week). \n\nThe model performed very poorly with an f1 of 12.5%. I don't think it is possible to obtain a great classifier with this data, but maybe I can increase the accuracy until 90% using another type of model. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}