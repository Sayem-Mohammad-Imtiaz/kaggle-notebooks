{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3>Given:</h3>\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not\nscaled). The data has 8 quantitative input variables, and 1 quantitative output\nvariable, and 1030 instances (observations).\n<h3>Objective:</h3>\nModeling of strength of high performance concrete using Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"<h2>1. Reading the data</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Importing the libraries\nimport pandas as pd        # for data manipulation\nimport seaborn as sns      # for statistical data visualisation\nimport numpy as np         # for linear algebra\nimport matplotlib.pyplot as plt      # for data visualization\nfrom scipy import stats        # for calculating statistics\n\n# Importing various machine learning algorithm from sklearn\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.metrics import mean_absolute_error,roc_curve,auc,accuracy_score,mean_squared_error,r2_score\nfrom scipy.stats import zscore\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor,RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.utils import resample\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe= pd.read_csv(\"concrete (1).csv\")  # Reading the data\ndataframe.head()   # showing first 5 datas","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data given has  9 columns and consist of 1030 data. And all the data is read correctly.<br>\nThere are eight independent variables and one dependent variable (i.e strenght)"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n    The above information shows the following<br>\n    a. The attributes are either int or float  <br>\n    <br>\n       \n"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"dataframe.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values. "},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.apply(lambda x: len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"dataframe.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>We can infer that:</h3>\nWe can see that cement,slag,ash are left skewed.<br>\n\n\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\nWe can see that<br>\nAge is positively skewed (right skewed)<br>\nCoarseagg and Fineagg is slightly left skewed<br>\nOthers are slightly right skewed<br>\n\n<h2>Univariate analysis</h2>\n<h2>Cement</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Range of values',dataframe.cement.max()-dataframe.cement.min())\nprint('Interquartile range: ',dataframe.cement.describe()['75%']-dataframe.cement.describe()['25%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"q3=dataframe.cement.describe()['75%']\nq1=dataframe.cement.describe()['25%']\niqr=dataframe.cement.describe()['75%']-dataframe.cement.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of Upper outliers are : ',dataframe[dataframe['cement']>586.4375].cement.count(),'(',dataframe[dataframe['cement']>586.4375].cement.count()/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['cement']<-44.0625].cement.count(),'(',dataframe[dataframe['cement']<-44.0625].cement.count()/len(dataframe),'%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no outliers in Cement data."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.cement,ax=a1)\na1.set_title('Cement')\na1.set_ylabel('strengh')\nsns.boxplot(x='cement',data=dataframe,ax=a2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>There is nothing much to infer but we can see cement attribute have almost normal curve.</li>\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Slag</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Range of values',dataframe.slag.max()-dataframe.slag.min())\nprint('Interquartile range: ',dataframe.slag.describe()['75%']-dataframe.slag.describe()['25%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"q3=dataframe.slag.describe()['75%']\nq1=dataframe.slag.describe()['25%']\niqr=dataframe.slag.describe()['75%']-dataframe.slag.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of Upper outliers are : ',dataframe[dataframe['slag']>357.357].slag.count(),'(',dataframe[dataframe['slag']>357.375].slag.count()/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['slag']<-214.42499999999998].slag.count(),'(',dataframe[dataframe['slag']<-214.42499999999998].slag.count()/len(dataframe),'%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 2 outliers on the right"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.slag,ax=a1)\na1.set_title('Slag')\na1.set_ylabel('strengh')\nsns.boxplot(x='slag',data=dataframe,ax=a2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>Slag has two gausssians and rightly skewed.It shows the presence of outlies.</li>\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Ash</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Range of values',dataframe.ash.max()-dataframe.ash.min())\nprint('Interquartile range: ',dataframe.ash.describe()['75%']-dataframe.ash.describe()['25%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"q3=dataframe.ash.describe()['75%']\nq1=dataframe.ash.describe()['25%']\niqr=dataframe.ash.describe()['75%']-dataframe.ash.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of Upper outliers are : ',dataframe[dataframe['ash']>295.75].ash.count(),'(',dataframe[dataframe['ash']> 295.75].ash.count()/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['ash']<-177.45].ash.count(),'(',dataframe[dataframe['ash']<-177.45].ash.count()/len(dataframe),'%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" there are no outliers."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.ash,ax=a1)\na1.set_title('Ash ')\na1.set_ylabel('strengh')\nsns.boxplot(x='ash',data=dataframe,ax=a2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>Ash has two gaussians and rightly skewed.</li>\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Water</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Range of values',dataframe.water.max()-dataframe.water.min())\nprint('Interquartile range: ',dataframe.water.describe()['75%']-dataframe.water.describe()['25%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"q3=dataframe.water.describe()['75%']\nq1=dataframe.water.describe()['25%']\niqr=dataframe.water.describe()['75%']-dataframe.water.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of Upper outliers are : ',dataframe[dataframe['water']>232.64999999999998].water.count(),'(',dataframe[dataframe['water']> 232.64999999999998].water.count()/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['water']<124.25000000000001].water.count(),'(',dataframe[dataframe['water']<124.25000000000001].water.count()/len(dataframe),'%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 4 outliers on the right and 5 outliers on left."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.water,ax=a1)\na1.set_title('Water')\na1.set_ylabel('strengh')\nsns.boxplot(x='water',data=dataframe,ax=a2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>Water has atleast guassians and slighly left skewed.It shows the presence of outlies.</li>\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Super Plastic</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Range of values',dataframe.superplastic.max()-dataframe.superplastic.min())\nprint('Interquartile range: ',dataframe.superplastic.describe()['75%']-dataframe.superplastic.describe()['25%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"q3=dataframe.superplastic.describe()['75%']\nq1=dataframe.superplastic.describe()['25%']\niqr=dataframe.superplastic.describe()['75%']-dataframe.superplastic.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of Upper outliers are : ',dataframe[dataframe['superplastic']>25.5].superplastic.count(),'(',dataframe[dataframe['superplastic']> 25.5].superplastic.count()/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['superplastic']<-15.299999999999999].superplastic.count(),'(',dataframe[dataframe['superplastic']<-15.299999999999999].superplastic.count()/len(dataframe),'%)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 10 outliers on the right"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.superplastic,ax=a1)\na1.set_title('Super plastic')\na1.set_ylabel('strengh')\nsns.boxplot(x='superplastic',data=dataframe,ax=a2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>superplastic has multiple gaussians and rightly skewed.It shows the presence of outlies.</li>\n"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"fig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(dataframe['cement'],ax=ax2[0][0])\nsns.distplot(dataframe['slag'],ax=ax2[0][1])\nsns.distplot(dataframe['ash'],ax=ax2[0][2])\nsns.distplot(dataframe['water'],ax=ax2[1][0])\nsns.distplot(dataframe['superplastic'],ax=ax2[1][1])\nsns.distplot(dataframe['coarseagg'],ax=ax2[1][2])\nsns.distplot(dataframe['fineagg'],ax=ax2[2][0])\nsns.distplot(dataframe['age'],ax=ax2[2][1])\nsns.distplot(dataframe['strength'],ax=ax2[2][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Multivariate Analysis</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Cement Vs Strength</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.regplot(x='cement',y='strength',data=dataframe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" It is linearly related to the cement. The relationship is positive and we can see that for a given value of cement we have a multiple values of strength. Which one should we pick we don't know. Hence Cement though it has positive relationship with the strength, it is not a very good predictor. It is a weak predictor."},{"metadata":{},"cell_type":"markdown","source":"<h3>Slag Vs Strength</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.regplot(x='slag',y='strength',data=dataframe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No much findings."},{"metadata":{},"cell_type":"markdown","source":"<h3>Age Vs Strength</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.regplot(x='age',y='strength',data=dataframe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a given value of age, we have different values of strength. Hence, It is not a good predictor."},{"metadata":{},"cell_type":"markdown","source":"<h3>Super Plastic Vs Strength</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.regplot(x='superplastic',y='strength',data=dataframe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a given value of Superplastic, we have different values of strength. Hence, It is not a good predictor."},{"metadata":{},"cell_type":"markdown","source":"Hence no independent attribute is useful to predict dependent values single handedly."},{"metadata":{},"cell_type":"markdown","source":"<h2>PairPlot</h2>"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"sns.pairplot(dataframe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<li><b>Cement vs other independent attributes:</b> This attribute does not have any significant relation with slag, ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.</li>\n<li><b>Slag vs other independent attributes: </b> This attribute also does not have any significant relation with ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.</li>\n<li><b>Ash vs other independent attributes:</b>  This attribute also does not have any significant relation with water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.</li>\n<li><b>Water vs other independent attributes: </b> This attribute have negative linear relationship with superplastic and fineagg. It does not have any significant relationship with other independent atributes. This is true as Superplasticizers allows the reduction of water in the concrete upto the extent of 30% without reducing the workability.</li>\n<li><b>Superplastic vs other independent attributes:</b> This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\ncoarseagg vs other independent attributes:This attribute also does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.</li>\n<li><b>Fineagg vs other independent attributes:</b> It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.</li>"},{"metadata":{},"cell_type":"markdown","source":"\n<h2>Corelation of Attributes</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"corelation=dataframe.corr()\ncorelation","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,20))\na=sns.heatmap(corelation,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Obv</h3>\n<li>Here most of the column share high collinearity both negatively and positively. </li>\n<li>water shows significant negative relationship with superplastic and fineagg. It also shows some kind of positive relationship with slag and age.</li>\n<li>This should not be the case in the model.</li>\n<li>As data are correlated, we can drop some of the columns which are corelated.</li>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h3>Managing the Outliers</h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.boxplot(figsize=(35,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for columns in dataframe.columns[:-1]:\n    q1 = dataframe[columns].quantile(0.25)\n    q3 = dataframe[columns].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    dataframe.loc[(dataframe[columns] < low) | (dataframe[columns] > high), columns] = dataframe[columns].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ndataframe.boxplot(figsize=(35,15))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Preparing the data for modeling</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataframe.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see all the attributes in the same scale(unit) except the age attribute. Therefore, we need to scale  the attributes. "},{"metadata":{"trusted":false},"cell_type":"code","source":"z_dataframe=dataframe.apply(zscore)\nz_dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalising the data"},{"metadata":{},"cell_type":"markdown","source":"<h3> Splitting the Data</h3>\nSplitting the model in 7:3 ratio"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_X.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Linear Regression Model</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"linear_reg=LinearRegression(n_jobs=1)\nlinear_reg","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"l_model=linear_reg.fit(train_X,train_y)\npredict=l_model.predict(test_X)\nprint(predict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Performance on training data using Linear Model:',linear_reg.score(train_X,train_y))\nprint('Performance on testing data using Linear Model:',linear_reg.score(test_X,test_y))\nacc_DT=r2_score(test_y, predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results = pd.DataFrame({'Method':['Linear Regression'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Ridge Regression Model</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"R_model=Ridge(alpha=.3)\nR_model=R_model.fit(train_X , train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"R_predict=R_model.predict(test_X)\nR_predict[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Performance on training data using Ridge Model:',R_model.score(train_X,train_y))\nprint('Performance on testing data using Ridge Model:',R_model.score(test_X,test_y))\nacc_DT=r2_score(test_y, R_predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, R_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"L_results = pd.DataFrame({'Method':['Ridge Regression'], 'accuracy': acc_DT},index={'2'})\nresults=pd.concat([results,L_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Lasso Regression Model</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"L_model=Lasso(alpha=.2)\nL_model=L_model.fit(train_X , train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"L_predict=L_model.predict(test_X)\nL_predict[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Performance on training data using Lasso Model:',L_model.score(train_X,train_y))\nprint('Performance on testing data using Lasso Model:',L_model.score(test_X,test_y))\nacc_DT=r2_score(test_y, L_predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, L_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"L_results = pd.DataFrame({'Method':['Lasso Regression'], 'accuracy': acc_DT},index={'3'})\nresults=pd.concat([results,L_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Dtree</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree = DecisionTreeRegressor(random_state=1)\ndtree=dtree.fit(train_X , train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree_predict=dtree.predict(test_X)\ndtree_predict[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Performance on training data using Lasso Model:',dtree.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree.score(test_X,test_y))\nacc_DT=r2_score(test_y, dtree_predict)\nprint('Accuracy : ',acc_DT)\nprint('Mean Sqaure Error: ',mean_squared_error(test_y, dtree_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a overfitting model as the dataset is performing 99% accurate in trainnig data.<br>The accuracy on test data is less."},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'4'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_val_score(dtree,train_X,train_y,cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(dtree,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Decision Tree'], 'accuracy': kfold_acc},index={'5'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"imp=pd.DataFrame(data=dtree.feature_importances_,index=list(X.columns))\nimp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, cement, age and water are significant attributes.\nHere, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column."},{"metadata":{"trusted":false},"cell_type":"code","source":"df=z_dataframe.copy()\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Dropping some features</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"x=df.drop(['ash','coarseagg','fineagg','strength'] , axis=1)\ny=df.strength\ntrain_X,test_X,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=1)\ntrain_X.count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree2 = DecisionTreeRegressor(random_state=1)\ndtree2=dtree2.fit(train_X , train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree_predict2=dtree2.predict(test_X)\ndtree_predict2[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Performance on training data using DT:',dtree2.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree2.score(test_X,test_y))\nacc_DT=r2_score(test_y, dtree_predict2)\nprint('Accuracy : ',acc_DT)\nprint('Mean Sqaure Error: ',mean_squared_error(test_y, dtree_predict2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still a overfitting model without any signficant change in accuracy score."},{"metadata":{"trusted":false},"cell_type":"code","source":"dt2_results = pd.DataFrame({'Method':['2nd Decision Tree'], 'accuracy': acc_DT},index={'6'})\nresults=pd.concat([results,dt2_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Pruning the DTree</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         \ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree_reg = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\ndtree_reg.fit(train_X, train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"dtree_reg_pred = dtree_reg.predict(test_X)\nprint('Performance on training data using DT:',dtree_reg.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree_reg.score(test_X,test_y))\nacc_RDT=r2_score(test_y, dtree_reg_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y,dtree_reg_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"RegDtree_result = pd.DataFrame({'Method':['Regularised Decision Tree'], 'accuracy': [acc_RDT]},index={'7'})\nresults = pd.concat([results, RegDtree_result])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(dtree_reg,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Regularised Decision Tree'], 'accuracy': kfold_acc},index={'8'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x=df.drop(['ash','coarseagg','fineagg','strength'] , axis=1)\ny=df.strength\ntrain_X,test_X,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=1)\ntrain_X.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtree_reg = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\ndtree_reg.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"re_dtree_predict = dtree_reg.predict(test_X)\nprint('Performance on training data using DT:',dtree_reg.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree_reg.score(test_X,test_y))\nacc_RDT=r2_score(test_y, re_dtree_predict)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, re_dtree_predict))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['2nd Pruned Decision Tree '], 'accuracy': acc_RDT},index={'9'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Random Forest</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         \ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfr_model=RandomForestRegressor(random_state=1)\nrfr_model=rfr_model.fit(train_X,train_y)\nrfr_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfr_predict = rfr_model.predict(test_X)\nprint('Performance on training data using Lasso Model:',rfr_model.score(train_X,train_y))\nprint('Performance on testing data using RF:',rfr_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, rfr_predict)\nprint('Accuracy RF: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, rfr_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model is also overfitting but the accuracy score is pretty good."},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Random Forest Regressor '], 'accuracy': acc_RDT},index={'10'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(rfr_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Random Forest Regressor '], 'accuracy': kfold_acc},index={'11'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Ada Boosting</h2>\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"ada_model=AdaBoostRegressor(random_state=1)\nada_model=ada_model.fit(train_X,train_y)\nada_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ada_predict = ada_model.predict(test_X)\nprint('Performance on training data using Ada Boosting:',ada_model.score(train_X,train_y))\nprint('Performance on testing data using Ada Boosting:',ada_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, ada_predict)\nprint('Accuracy Ada Boostig: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, ada_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Ada Boosting'], 'accuracy': acc_RDT},index={'12'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(ada_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Ada Boosting '], 'accuracy': kfold_acc},index={'13'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Gradient Boosting</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_model=GradientBoostingRegressor(random_state=1)\ngb_model=gb_model.fit(train_X,train_y)\ngb_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_predict = gb_model.predict(test_X)\nprint('Performance on training data using Gradient Boosting:',gb_model.score(train_X,train_y))\nprint('Performance on testing data using Gradient Boosting:',gb_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, gb_predict)\nprint('Accuracy Gradient Boostig: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, gb_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Gradient Boosting'], 'accuracy': acc_RDT},index={'14'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(gb_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Gradient Boosting '], 'accuracy': kfold_acc},index={'15'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> SVR </h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"svr_model=SVR(kernel=\"linear\")\nsvr_model=svr_model.fit(train_X,train_y)\nsvr_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svr_predict = svr_model.predict(test_X)\nprint('Performance on training data using SVR:',svr_model.score(train_X,train_y))\nprint('Performance on testing data using SVR:',svr_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, svr_predict)\nprint('Accuracy SVR: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, svr_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['SVR '], 'accuracy': acc_RDT},index={'16'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(svr_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val SVR '], 'accuracy': kfold_acc},index={'17'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Bagging Regression</h2>\n    "},{"metadata":{"trusted":false},"cell_type":"code","source":"bag_model=BaggingRegressor(random_state=1)\nbag_model=bag_model.fit(train_X,train_y)\nbag_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bag_predict = bag_model.predict(test_X)\nprint('Performance on training data using Bagging Regressor:',bag_model.score(train_X,train_y))\nprint('Performance on testing data using Bagging Regressor :',bag_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, bag_predict)\nprint('Accuracy Bagging Regressor : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, bag_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Bagging Regressor '], 'accuracy': acc_RDT},index={'18'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(bag_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Bagging Regressor '], 'accuracy': kfold_acc},index={'19'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>KNN</h2>"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"error=[]\nfor i in range(1,30):\n    knn_model = KNeighborsRegressor(n_neighbors=i)\n    knn_model.fit(train_X,train_y)\n    pred_i = knn_model.predict(test_X)\n    error.append(np.mean(pred_i!=test_y))\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nplt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see n_neighnors=3 is suitable"},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors=3)\nknn_model.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_predict = knn_model.predict(test_X)\nprint('Performance on training data using KNN:',knn_model.score(train_X,train_y))\nprint('Performance on testing data using KNN :',knn_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, knn_predict)\nprint('Accuracy KNN : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, knn_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['KNN Regressor '], 'accuracy': acc_RDT},index={'20'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(knn_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val KNN Regressor '], 'accuracy': kfold_acc},index={'21'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Ensemble</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"\nL=LinearRegression()\nK=KNeighborsRegressor(n_neighbors=3)\nS=SVR(kernel='linear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ebl=VotingRegressor(estimators=[('L',L),('K',K),('S',S)])\nebl.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ebl_predict = ebl.predict(test_X)\nprint('Performance on training data using Ensemble technique:',ebl.score(train_X,train_y))\nprint('Performance on testing data using Ensemble technique :',ebl.score(test_X,test_y))\nacc_RDT=r2_score(test_y, ebl_predict)\nprint('Accuracy Ensemble : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, ebl_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Ensemble '], 'accuracy': acc_RDT},index={'22'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(ebl,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cr_results = pd.DataFrame({'Method':['Cross Val Ensemble '], 'accuracy': kfold_acc},index={'23'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a. After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.<br><br>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Boot Strap Sampling</h2>"},{"metadata":{"trusted":false},"cell_type":"code","source":"values=z_dataframe.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"values","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"scores=list()   \nfor i in range(1000):\n    # prepare train and test sets\n    train = resample(values, n_samples=len(z_dataframe))  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbm_model = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbm_model.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbm_model.score(test[:, :-1] , y_test)\n    predictions = gbm_model.predict(test[:, :-1])  \n\n    scores.append(score)\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(scores)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"alpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(scores, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(scores, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":false},"cell_type":"code","source":"scores=list()   \nfor i in range(1000):\n    # prepare train and test sets\n    train = resample(values, n_samples=len(z_dataframe))  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbm_model = RandomForestRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbm_model.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbm_model.score(test[:, :-1] , y_test)\n    predictions = gbm_model.predict(test[:, :-1])  \n\n    scores.append(score)\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(scores)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"alpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(scores, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(scores, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>The bootstrap random forest classification model performance is between 84.3%-90.3% which is better than other classification algorithms.</h3>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}