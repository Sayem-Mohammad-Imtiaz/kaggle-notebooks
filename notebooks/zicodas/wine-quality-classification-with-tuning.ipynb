{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_validate\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df['residual sugar'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nz = np.abs(stats.zscore(df))\nred_wines = df[(z < 3).all(axis=1)]\nred_wines.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red_wines.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(red_wines['residual sugar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(red_wines['residual sugar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='quality', data=red_wines)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### corrleation between variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15, 10))\nsns.heatmap(red_wines.corr(), annot = True, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check class imbalances"},{"metadata":{"trusted":true},"cell_type":"code","source":"red_wines['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### balancing imbalanced Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = red_wines.drop('quality',axis=1)\ny = red_wines['quality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resampling**\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling)."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncounter = Counter(y)\nfor k,v in counter.items():\n\tper = v / len(y) * 100\n\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n# plot the distribution\nplt.bar(counter.keys(), counter.values())\nplt.show()\nX.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = StandardScaler().fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)\nprint ('Train set:', X_train.shape, y_train.shape)\nprint ('Test set:', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the machine learning classifiers\nlog_model = LogisticRegression()\nKNN_model = KNeighborsClassifier()\nSVC_model = LinearSVC(dual=False)\nGauss_model = GaussianProcessClassifier()\nDecision_model = DecisionTreeClassifier()\nRandFor_model = RandomForestClassifier()\nAda_model = AdaBoostClassifier()\nGaussNB_model = GaussianNB()\nQDA_model = QuadraticDiscriminantAnalysis()\nmlp_model = MLPClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define dictionary with performance metrics\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score,average='macro'),\n           'recall':make_scorer(recall_score,average='macro'), \n           'f1_score':make_scorer(f1_score,average='macro')}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log = cross_validate(log_model,X,y,cv=5,scoring=scoring)\nknn = cross_validate(KNN_model,X,y,cv=5,scoring=scoring)\nsvc = cross_validate(SVC_model,X,y,cv=5,scoring=scoring)\ngauss = cross_validate(Gauss_model,X,y,cv=5,scoring=scoring)\ndecision = cross_validate(Decision_model,X,y,cv=5,scoring=scoring)\nrandom = cross_validate(RandFor_model,X,y,cv=5,scoring=scoring)\nada = cross_validate(Ada_model,X,y,cv=5,scoring=scoring)\ngaussNB = cross_validate(GaussNB_model,X,y,cv=5,scoring=scoring)\nqda = cross_validate(QDA_model,X,y,cv=5,scoring=scoring)\nmlp = cross_validate(mlp_model,X,y,cv=5,scoring=scoring)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_scores_table_new = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n                                                               log['test_precision'].mean(),\n                                                               log['test_recall'].mean(),\n                                                               log['test_f1_score'].mean()],\n                                                           \n                                      'KNeighbors Classifier':[knn['test_accuracy'].mean(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  knn['test_precision'].mean(),\n                                                              knn['test_recall'].mean(),\n                                                              knn['test_f1_score'].mean()],\n                                       \n                                      'SVC Classifer':[svc['test_accuracy'].mean(),\n                                                       svc['test_precision'].mean(),\n                                                       svc['test_recall'].mean(),\n                                                       svc['test_f1_score'].mean()],\n                                       \n                                      'GaussianProcess Classifier':[gauss['test_accuracy'].mean(),\n                                                                   gauss['test_precision'].mean(),\n                                                                   gauss['test_recall'].mean(),\n                                                                   gauss['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n                                      'DecisionTree Classifier':[decision['test_accuracy'].mean(),\n                                                                   decision['test_precision'].mean(),\n                                                                   decision['test_recall'].mean(),\n                                                                   decision['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'RandomForest Classifier':[random['test_accuracy'].mean(),\n                                                                   random['test_precision'].mean(),\n                                                                   random['test_recall'].mean(),\n                                                                   random['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'AdaBoost Classifier':[ada['test_accuracy'].mean(),\n                                                                   ada['test_precision'].mean(),\n                                                                   ada['test_recall'].mean(),\n                                                                   ada['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'GaussianNB':[gaussNB['test_accuracy'].mean(),\n                                                                   gaussNB['test_precision'].mean(),\n                                                                   gaussNB['test_recall'].mean(),\n                                                                   gaussNB['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'Quadratic Discriminant Analysis':[qda['test_accuracy'].mean(),\n                                                                   qda['test_precision'].mean(),\n                                                                   qda['test_recall'].mean(),\n                                                                   qda['test_f1_score'].mean()],\n                                      'MLP Classifer':[mlp['test_accuracy'].mean(),\n                                                                   mlp['test_precision'].mean(),\n                                                                   mlp['test_recall'].mean(),\n                                                                   mlp['test_f1_score'].mean()]},\n\t\t\t\t\t\t\t\t\t  \n                                       index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_scores_table_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_scores_table_new['Best Score'] = models_scores_table_new.idxmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_scores_table_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier has the highest score, so it is selected for futher processing** "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import multilabel_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandFor_model.fit(X_train,y_train)\ny_pred = RandFor_model.predict(X_test)\ny_test.shape,y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel_confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Parameters currently in use:\\n')\npprint(RandFor_model.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features)*"},{"metadata":{},"cell_type":"markdown","source":"### Random Hyperparameter Grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = RandFor_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the random search model\nrf_random.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = RandomForestClassifier()\nbase_model.fit(X_train,y_train)\nbase_accuracy = evaluate(base_model,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search with Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [60, 70, 80, 90, 100, 110],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1,2,3],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [2000]\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}