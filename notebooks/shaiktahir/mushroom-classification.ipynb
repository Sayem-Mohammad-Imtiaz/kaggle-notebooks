{"cells":[{"metadata":{"id":"y35z2HqrLYoF","outputId":"a6659e1d-6159-4b56-835e-aa96c3c4d964","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)","execution_count":null,"outputs":[]},{"metadata":{"id":"UZktTvicL3xt","outputId":"80234a2a-4505-4603-d5df-1755b342e1ec","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/mushroom-classification/mushrooms.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"iealmzn9MFTm","outputId":"e9e1a7fe-1649-4e2a-bdab-ecd25dcd5021","trusted":true},"cell_type":"code","source":"print('The Mushroom data set has rows :{} and columns :{}'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"GBmxjbxPMQmO","outputId":"f36f6dd1-a24d-43ff-d4e5-e9e3b859fc83","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"1A4MUwIkNNRm"},"cell_type":"markdown","source":"> * All 23 atttributes in the data set is of object type ","execution_count":null},{"metadata":{"id":"hqdBhnz8Macv","outputId":"30594be5-6a3c-43a4-da97-9afcca2d7a8b","trusted":true},"cell_type":"code","source":"df.describe()  # 5 point summary is not avaialble as all features here are of object type","execution_count":null,"outputs":[]},{"metadata":{"id":"axUj6_DbM4NT","outputId":"9ac47ac4-e8fc-4dba-d43c-8f753763783a","trusted":true},"cell_type":"code","source":"pd.DataFrame({'Count':df.isnull().sum(),'% Missing':df.isnull().sum()/df.shape[0]})*100 # Checking for null values","execution_count":null,"outputs":[]},{"metadata":{"id":"_M2gyLOzN_xb"},"cell_type":"markdown","source":"\n\n> # Exploratory Data Anlysis \n\n","execution_count":null},{"metadata":{"id":"51EdwIVkOG0R","outputId":"987ffc7a-ae12-4b3f-f6a5-752b80b13acf","trusted":true},"cell_type":"code","source":"for i in df.select_dtypes(include='O'):        \n    print(i,'unique values are:',df[i].nunique(),'--- they are:',df[i].unique(),'---and the % of observation in each feaure is: \\n',df[i].value_counts(normalize=True)*100)\n    print('*'*50)","execution_count":null,"outputs":[]},{"metadata":{"id":"eT5N16h6OY_7","trusted":true},"cell_type":"code","source":"df.replace('?',np.nan,inplace=True)  # replacing as Nan value","execution_count":null,"outputs":[]},{"metadata":{"id":"a2DSAaFYPhFo","outputId":"e943a2e8-1b9e-4f86-c84c-111b86f7b2d4","trusted":true},"cell_type":"code","source":"pd.DataFrame({'Count':df.isnull().sum(),'% Missing':df.isnull().sum()/df.shape[0]*100}) # Checking for null values","execution_count":null,"outputs":[]},{"metadata":{"id":"36MuU_5PP64l"},"cell_type":"markdown","source":"\n\n> We notice that stalk-root has 30% missing values \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['stalk-root']=df['stalk-root'].fillna(df['stalk-root'].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"KX7O2sNWPl6p","outputId":"59b51c26-3c35-48f2-e07e-2eb7428dcd0e","trusted":true},"cell_type":"code","source":"# Checking for outliers in the dataset \n\nfor i in df.describe(include='O').columns:\n    plt.subplots()\n    plt.title(i)\n    print(sns.boxplot(x=df[i].value_counts(),hue='class',data=df)) # have to use value counts \n","execution_count":null,"outputs":[]},{"metadata":{"id":"raFYTy5rYGq3"},"cell_type":"markdown","source":"> - Yes , the target column in distributed evenly ","execution_count":null},{"metadata":{"id":"x1i6--45mN6J","outputId":"689d0bdb-ca74-492f-e06f-96eea8e6fc69","trusted":true},"cell_type":"code","source":"for i in df.columns:\n    plt.subplots()            # using value counts to count each observation in each feature\n    sns.countplot(df[i],hue=df['class'],palette='hot')","execution_count":null,"outputs":[]},{"metadata":{"id":"jtBvsyENnC7j","outputId":"f87adebe-88e3-414e-cba4-237189de2d95","trusted":true},"cell_type":"code","source":"for i in df.columns:        # veil-type has only one value p. we can eliminate it.\n    if df[i].nunique()==1:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"id":"LFC6IL3Mbnyc","trusted":true},"cell_type":"code","source":"df = df.drop('veil-type', axis=1)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Typecasting (Data Munging)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.get_dummies(data=df,drop_first=True)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"aSZd_SXzl_Ek","outputId":"707e175f-ac36-4187-833e-0199740cd535","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,10))\nsns.heatmap(df1.corr(),annot=True,fmt='0.2f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"OCw55Z-9qh0-","outputId":"8240a94d-bd84-4a1c-aa8d-ed182fafb53a","trusted":true},"cell_type":"code","source":"corr=df1.corr()                              # Top 15 features that high high correlation with class \ncols=corr.nlargest(15,'class_p').index\ncm = np.corrcoef(df1[cols].values.T)\nplt.figure(figsize=(20,12))\nsns.heatmap(cm,annot=True, yticklabels = cols.values, xticklabels = cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"w2W7xJIJtfdR"},"cell_type":"markdown","source":"\n\n> # Statistical Analysis \n\n","execution_count":null},{"metadata":{"id":"lmQNPUchuNRF","trusted":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency,f_oneway","execution_count":null,"outputs":[]},{"metadata":{"id":"oc1e5hYls8ZX","trusted":true},"cell_type":"code","source":"cat_cols=df.describe(include='O').columns   # Used df here as all features are categorical\n\nchi_stat=[]\np_value=[]\nfor i in cat_cols:\n    chi_res=chi2_contingency(np.array(pd.crosstab(df[i],df['class'])))\n    chi_stat.append(chi_res[0])\n    p_value.append(chi_res[1])\nchi_square=pd.DataFrame([chi_stat,p_value])\nchi_square=chi_square.T\ncol=['Chi Square Value','P-Value']\nchi_square.columns=col\nchi_square.index=cat_cols","execution_count":null,"outputs":[]},{"metadata":{"id":"c2-qQ5ExuKAe","outputId":"f1ceafb6-675a-419c-d85a-7ebe919835e4","trusted":true},"cell_type":"code","source":"chi_square  # Need to interpret the Pvalue correctly here ","execution_count":null,"outputs":[]},{"metadata":{"id":"zzsNW_TJuQbi","outputId":"fbf95e69-824a-4cc0-da82-da77faa9fa19","trusted":true},"cell_type":"code","source":"features_p = list(chi_square[chi_square[\"P-Value\"]==0.00].index)  # Selected 13 important features based on chi2 test \nprint(\"Significant categorical Features:\\n\\n\",features_p)","execution_count":null,"outputs":[]},{"metadata":{"id":"q8G6Rqp1vrsx","trusted":true},"cell_type":"code","source":"num_cols=df1.describe(exclude='O')    # After Typecasting the data using pd.get_dummies and then using ANOVA\nnum_cols.columns\n\nf_stat=[]\np_val=[]\nfor i in num_cols:\n    edible=df1[df['class']=='e'][i]  # make sure to use df as target variable is categorical and only then can use ANOVA\n    poison=df1[df['class']=='p'][i]\n    a=f_oneway(edible,poison)\n    f_stat.append(a[0])\n    p_val.append(a[1])\nanova=pd.DataFrame([f_stat,p_val])\nanova=anova.T\ncols=['F-STAT','P-VALUE']\nanova.columns=cols\nanova.index=num_cols.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"nrhtCya3vz3i","outputId":"2d553b65-d8d1-4376-8145-c77840442753","trusted":true},"cell_type":"code","source":"anova","execution_count":null,"outputs":[]},{"metadata":{"id":"CxxqWCs9vdFE","outputId":"0d58d30c-c183-4712-eb97-bb83a0c90430","trusted":true},"cell_type":"code","source":"features_p_n = list(anova[anova[\"P-VALUE\"]==0.00].index)  # Anova chose 4 significant features\nprint(\"Significant numerical Features:\\n\\n\",features_p_n)","execution_count":null,"outputs":[]},{"metadata":{"id":"1ZXq0Kddxqh-"},"cell_type":"markdown","source":"\n\n> # Splitting the data set into Train and Test with df1 \n\n","execution_count":null},{"metadata":{"id":"857TOL3sx88M","trusted":true},"cell_type":"code","source":"X=df1.drop('class_p',axis=1)\ny=df1['class_p']","execution_count":null,"outputs":[]},{"metadata":{"id":"c0E8nW9Ow7UV","outputId":"3cc5a8ba-e916-42be-d8ac-aa48bf3390ab","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"K6JX2vTryI80","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , accuracy_score , roc_auc_score , roc_curve,classification_report,precision_score,recall_score,f1_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score,KFold\n\n\n\nlr = LogisticRegression(fit_intercept=True)\ngnb= GaussianNB()\nknn = KNeighborsClassifier()\ndtc = DecisionTreeClassifier(ccp_alpha=0.01) # to increase pruning and avoid overfitting\nsvm= SVC(probability=True)\nlda=LinearDiscriminantAnalysis()","execution_count":null,"outputs":[]},{"metadata":{"id":"EhaTZFeHyWVr","trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression(random_state=123)))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state=123)))\nmodels.append(('SVM', SVC(probability=True,random_state=123)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))","execution_count":null,"outputs":[]},{"metadata":{"id":"LPG4dvHAyWYd","outputId":"9c6b55b3-2e1d-40f3-8156-f9284db75739","trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"id":"MAlYIFkly-cg","outputId":"1258c34b-fbcd-4e6c-ba8f-49ebbff0dc25","trusted":true},"cell_type":"code","source":"# compare algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison',fontsize=20)\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(20,8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"pWPLaMp60RgR","trusted":true},"cell_type":"code","source":"def model_eval(algo,X_train,X_test,y_train,y_test):\n    algo.fit(X_train,y_train)\n\n    y_train_pred=algo.predict(X_train)             # Finding the positives and negatives \n    y_train_prob=algo.predict_proba(X_train)[:,1]  # we are intersted only in the second column\n\n    #overall accuracy for train model\n    print('Confusion Matrix- Train:','\\n',confusion_matrix(y_train,y_train_pred))\n    print('Overall Accuracy-Train:',accuracy_score(y_train,y_train_pred))\n    print('AUC-Train',roc_auc_score(y_train,y_train_prob))\n\n    \n    y_test_pred=algo.predict(X_test)\n    y_test_prob=algo.predict_proba(X_test)[:,1]\n    print('*'*50)\n    \n    #overall accuracy of test model\n    print('Confusion matrix - Test :','\\n',confusion_matrix(y_test,y_test_pred))\n    print('Overall Accuracy - Test :',accuracy_score(y_test,y_test_pred))\n    print('AUC - Test:',roc_auc_score(y_test,y_test_prob))\n\n    print('*'*50)\n    scores=cross_val_score(algo,X,y,cv=3,scoring='roc_auc')\n    print('3 Fold Cross Validation Scores')\n    print(scores)\n    print('Bias Error:',100-scores.mean()*100)\n    print('Variance Error:',scores.std()*100)\n          \n          \n    print('*'*50)\n    print('Classification Report for test model: \\n', classification_report(y_test,y_test_pred))\n          \n    fpr,tpr,threshold=roc_curve(y_test,y_test_prob,pos_label=[2])\n    plt.figure(figsize=(15,8))\n    plt.plot(fpr,tpr)\n    plt.plot(fpr,fpr,color='r')\n    plt.xlabel('Fpr')\n    plt.ylabel('Tpr')","execution_count":null,"outputs":[]},{"metadata":{"id":"WkP2EBqf1qfx"},"cell_type":"markdown","source":"\n\n> ### -Logistic Regression\n","execution_count":null},{"metadata":{"id":"UYoCYPAL1maa","outputId":"9ada71dd-248d-4174-bc96-c5f8613cb488","trusted":true},"cell_type":"code","source":"model_eval(lr,X_train,X_test,y_train,y_test)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"fc0ZDlM64L01"},"cell_type":"markdown","source":"> * other metric to evaluate the model output are roc_auc_score,confusion matrix , bias and vraince error ,classification report , given above\n> * Also talk about sensitivity and specifity ,TPR and FPR to get a whollistic view","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)\ny_test_pred=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"VQPPfHq-2rAk","outputId":"74862db6-0062-4680-dcaf-cfece8e46586","trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\ndummy=DummyClassifier(random_state=123)\ndummy.fit(X_train,y_train)\nprint('The dummy classifier gives us a basic score',dummy.score(y_test,y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"02uYmL4S2k0A"},"cell_type":"markdown","source":"This means that a dumb model that always predicts 0 would be right 50% of the time\n\nThis shows how classification accuracy is not that good as it's close to a dumb model.\nIt's a good way to know the minimum we should achieve with our models\n","execution_count":null},{"metadata":{"id":"VyM3ZIxJwQgg","outputId":"7b2c9292-81aa-498b-8391-b3fa34124a01","trusted":true},"cell_type":"code","source":"print('The Precision Score for Logistic Regression Model is:',precision_score(y_test,y_test_pred))\nprint('The Recall Score for Logistic Regression Model is:',recall_score(y_test,y_test_pred))\nprint('The F1 Score for Logistic Regression Model is:',f1_score(y_test,y_test_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"RfQeebxivlZD"},"cell_type":"markdown","source":"\n\n1. The optimisation objective is to maximise likelihood or minimise MSE.\n2. We see that the overall accuracy of the model is 99.83 %. This is a fantastic score but can be further improved. \n\n","execution_count":null},{"metadata":{"id":"6jnMwEcizkjF","outputId":"5fc608ee-6b65-45de-dfb4-fc1c83f2c677","trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(X.columns) # Variables that are significant in prediction based on their coef vals\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(lr.coef_[0])\npd.Series(lr.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"Kz_V4f0vyFt3","outputId":"3beb6d93-2d20-41fd-f75c-d59030e36cc5","trusted":true},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score  # Very Good Score\ncohen_kappa_score(y_test,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"hTDmhbOnzfeG"},"cell_type":"markdown","source":"Cohen Kappa is always less than or equal to 1. A value of 1 implies perfect agreement and values less than 1 imply less than perfect agreement. In rare situations, Kappa can be negative. This is a sign that the two observers agreed less than would be expected just by chance.\n\n*   0 = agreement equivalent to chance.\n*   0.1 – 0.20 = slight agreement.\n*   0.21 – 0.40 = fair agreement.\n*  0.41 – 0.60 = moderate agreement.\n*  0.61 – 0.80 = substantial agreement.\n*  0.81 – 0.99 = near perfect agreement\n*  1 = perfect agreement.\n\nEvaluates the level of agreement betwen observers. In this case between y_test and predicted values for X_test","execution_count":null},{"metadata":{"id":"1ThDWRVP4X_o"},"cell_type":"markdown","source":"\n\n> # Splitting the data set to Train and Test with df2 , using backward elimination \n\n","execution_count":null},{"metadata":{"id":"wsspBVML_hA3","trusted":true},"cell_type":"code","source":"df2=df1.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"W3uMQ-uAAKIb","trusted":true},"cell_type":"code","source":"X=df2.drop('class_p',axis=1)\ny=df2['class_p']","execution_count":null,"outputs":[]},{"metadata":{"id":"KEPMWOKw_t1M","outputId":"e931a223-6a6a-43bd-8be4-bb2f77b5f4a4","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nX_1=sm.add_constant(X)\nmodel=sm.OLS(y,X_1).fit()\nmodel.pvalues ","execution_count":null,"outputs":[]},{"metadata":{"id":"KZHHUVZz_g8I","outputId":"375ee78e-3f2b-4968-8030-7f79622d6746","trusted":true},"cell_type":"code","source":"cols=list(X.columns)\npmax=1\nwhile (len(cols)>0):            # Using Backwad Elimination \n    p=[]\n    X_1=X[cols]\n    X_1=sm.add_constant(X_1)\n    model=sm.OLS(y,X_1).fit()\n    p=pd.Series(model.pvalues.values[1:],index=cols)\n    pmax=max(p)\n    feature_with_p_max=p.idxmax()\n    if(pmax > 0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE=cols\nprint(selected_features_BE)","execution_count":null,"outputs":[]},{"metadata":{"id":"k_3R9yHXAdYf","trusted":true},"cell_type":"code","source":"df2=df2[['cap-shape_k',\n 'odor_c',\n 'odor_f',\n 'odor_m',\n 'odor_n',\n 'odor_p',\n 'odor_s',\n 'odor_y',\n 'gill-size_n',\n 'gill-color_e',\n 'gill-color_g',\n 'gill-color_h',\n 'gill-color_k',\n 'gill-color_n',\n 'gill-color_o',\n 'gill-color_p',\n 'gill-color_r',\n 'gill-color_u',\n 'gill-color_w',\n 'gill-color_y',\n 'stalk-shape_t',\n 'stalk-root_c',\n 'stalk-root_e',\n 'stalk-root_r',\n 'stalk-surface-above-ring_y',\n 'stalk-surface-below-ring_y',\n 'stalk-color-above-ring_c',\n 'stalk-color-above-ring_o',\n 'stalk-color-below-ring_o',\n 'stalk-color-below-ring_w',\n 'veil-color_w',\n 'ring-number_o',\n 'ring-number_t',\n 'ring-type_f',\n 'ring-type_l',\n 'ring-type_n',\n 'ring-type_p',\n 'spore-print-color_h',\n 'spore-print-color_r',\n 'spore-print-color_w',\n 'habitat_w',\n 'class_p']]","execution_count":null,"outputs":[]},{"metadata":{"id":"Tsaa5UkSCn5J","trusted":true},"cell_type":"code","source":"X=df2.drop('class_p',axis=1)\ny=df2['class_p']","execution_count":null,"outputs":[]},{"metadata":{"id":"x2eCwkiFBGW6","outputId":"83d42b7e-b695-43f0-c45a-2dfd75ecf45c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"dpot_81TBGRc","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , accuracy_score , roc_auc_score , roc_curve,classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score,KFold\n\nlr2 = LogisticRegression(fit_intercept=True)\ngnb= GaussianNB()\nknn = KNeighborsClassifier()\ndtc = DecisionTreeClassifier(ccp_alpha=0.01) # to increase pruning and avoid overfitting\nsvm= SVC(probability=True)\nlda=LinearDiscriminantAnalysis()","execution_count":null,"outputs":[]},{"metadata":{"id":"2LsRgceqBGMS","trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR2', LogisticRegression(random_state=123)))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state=123)))\nmodels.append(('SVM', SVC(probability=True,random_state=123)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))","execution_count":null,"outputs":[]},{"metadata":{"id":"57D0wtyZC0xK","outputId":"abd97e87-8b71-4dfc-abd5-1f4fb1e17617","trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"id":"1c4LdNpNC7DX","outputId":"8a452a27-7fc9-49a6-8d85-c6ec634e2d85","trusted":true},"cell_type":"code","source":"model_eval(lr2,X_train,X_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"W1tN9J1NQrJA","outputId":"61774bdc-d30d-412e-c709-40cf7d664784","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\ndf2.corr()['class_p'].sort_values().plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eamBry_bRCY_","outputId":"5bf7184e-85f9-4b4e-c5e1-9fdf5096e684","trusted":true},"cell_type":"code","source":"corr=df2.corr()                   # Top 15 features that high high correlation with class after feature selection \ncols=corr.nlargest(15,'class_p').index\ncm = np.corrcoef(df2[cols].values.T)\nplt.figure(figsize=(20,12))\nsns.heatmap(cm,annot=True, yticklabels = cols.values, xticklabels = cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"f4A_UF_Ic5VV"},"cell_type":"markdown","source":"- The buiness interpretation of the model would be to implement this model in the field of biology and agriculture where we can classify consumer products. \n- Here we were dealing with mushrooms being either edible or poisonous and we see such kind of analysis being made majorly in the agri-tech industry.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* # Working with DF3 , using Backward Elimination and VIF to remove multicollinearity ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df2.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df3.iloc[:,:-1]\ncalc_vif(X).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.drop(['odor_c','odor_f','odor_m','odor_n','spore-print-color_w','gill-size_n','gill-color_e','gill-color_g','gill-color_h','gill-color_k','gill-color_n','gill-color_o','gill-color_p','gill-color_r'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df3.iloc[:,:-1]\ncalc_vif(X).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.drop(['stalk-color-above-ring_c','stalk-color-above-ring_o','ring-type_n'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df3.iloc[:,:-1]\ncalc_vif(X).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.drop(['veil-color_w','stalk-color-below-ring_o'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df3.iloc[:,:-1]\ncalc_vif(X).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X\ny=y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , accuracy_score , roc_auc_score , roc_curve,classification_report\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score,KFold\n\nlr3 = LogisticRegression(fit_intercept=True)\ngnb= GaussianNB()\nknn = KNeighborsClassifier()\ndtc = DecisionTreeClassifier(ccp_alpha=0.01) # to increase pruning and avoid overfitting\nsvm= SVC(probability=True)\nlda=LinearDiscriminantAnalysis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR3', LogisticRegression(random_state=123)))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state=123)))\nmodels.append(('SVM', SVC(probability=True,random_state=123)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_eval(lr3,X_train,X_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working with DF4 , using Backward Elimination and PCA and Logistic Regression ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4=df2.copy(deep=True)\n\nX=df4.drop('class_p',axis=1)\ny=df4['class_p']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA # Scaling not required here \npca=PCA(n_components=25)       # Applying PCA to Logistic Regression\nX_train_pca=pca.fit_transform(X_train) \nX_test_pca=pca.transform(X_test)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cov_matirx=np.cov(X.T)\neig_vals,eig_vectors=np.linalg.eig(cov_matirx)\neig_vals   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot=sum(eig_vals)\nvar_exp=[(i/tot)*100 for i in sorted(eig_vals,reverse=True)]\ncum_var_exp=np.cumsum(var_exp)\nprint('Cumulative variance Explained:',(cum_var_exp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.bar(range(X.shape[1]),var_exp,alpha=0.5,align='center',label='Individual explained variance')\nplt.step(range(X.shape[1]),cum_var_exp,where='mid',label='cummulative explained variance')\nplt.ylabel(\"explained variance ratio\")\nplt.xlabel(\"principal components\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr4=LogisticRegression(fit_intercept=True)\n\nmodel_eval(lr4,X_train_pca,X_test_pca,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"sEd9n26DR0c9"},"cell_type":"markdown","source":"### The changes in the base model that has the most effect on model performance are:-\n\n1. Feature selection reduced the features from a total of 95 to 44 which is a huge imrovement.\n2. Label encoding creates biasness towards certain observations in features and therefore one hot encoding performed better by increasing the overall accuracy. \n3. These changes gave us a perfect overall score where bias and variance tradeoff could be met and at the same time achieve the desired results.\n4. Eventually we got a score of 99.9 and 99.8 in train and test for Logistic Regression after using Backward elimination and VIF and helped reduce the variance error and bias error.\n\n","execution_count":null},{"metadata":{"id":"dKYWbtFvRqX2"},"cell_type":"markdown","source":"### The key risks and to the results are:\n\n1. Presence of multicollinearity is a high risk as we have 95 features after one hot encoding. Feature selection using backward elimination method was helpful but doesnt gurantee removal of multicollinearity.\n2. Alot of assumptions were made on normality and equality of variance of distribution and certain statistical tests ought to be performed to make sure our assumptions are right.\n3. Overall the prediction score given by Logistic Regression is fantastic and need not worry about overfitting as logisitc regression takes care of this automatically by penalising certain features using ridge functionality which is a default in-built parametric.\n\n","execution_count":null},{"metadata":{"id":"96SB0Q7c5CZO"},"cell_type":"markdown","source":"# THE END","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}