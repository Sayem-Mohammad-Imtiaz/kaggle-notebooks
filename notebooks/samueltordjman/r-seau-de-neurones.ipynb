{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Définition de la classe abstraite\n\nc'est une couche \"type\" qu'il faut remplir/définir par la suite en fonction de la couche que l'on veut coder  "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Layer:\n    def __init__(self):\n        self.input = None\n        self.output = None\n\n    # calcule de la sortie Y pour une entrée X donnée \n    def forward_propagation(self, X):\n        raise NotImplementedError #Donc on ne définit pas encore la fonction, ce sera fait par la suite\n\n    # calcule de dE/dX \n    def backward_propagation(self, dE_dY , alpha):\n        raise NotImplementedError #Donc on ne définit pas encore la fonction, ce sera fait par la suite","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Couche FC (Fully Connected)\n\nOn se limite à des couche FC (càd des couches où tous les neurones d'entrée sont connectés avec tous ceux de sortie)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remarque pour la synthaxe : dE_dY = dE/dY, idem pour dE_dX et dE_dW \n\nclass FCLayer(Layer):\n    # définitions des données \n    def __init__(self, nb_neurones_X, nb_neurones_Y):\n        self.weights = np.random.rand(nb_neurones_X, nb_neurones_Y) - 0.5\n        self.bias = np.random.rand(1, nb_neurones_Y) - 0.5\n\n    # calcule de la sortie Y pour une entrée X donnée     \n    def forward_propagation(self, input_data):\n        self.input = input_data \n        #on calcule la sortie avec la formule classique Y = XW + B\n        self.output = np.dot(self.input, self.weights) + self.bias \n        return self.output\n\n    # calcule de dE/dX \n    def backward_propagation(self, dE_dY, alpha): \n        #dE/dX et dE/dW servent à ajuster le Poids (weight) et le Biais afin d'augmenter les performances du réseau \n        dE_dX = np.dot(dE_dY, self.weights.T) \n        dE_dW = np.dot(self.input.T, dE_dY)\n\n        # ajustement des paramètres comme annoncé précédement\n        self.weights -= alpha * dE_dW\n        self.bias -= alpha * dE_dY\n        return dE_dX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Couche activation\n\nqui par la fonction d'activation produit un effet de seuil permettant une modification des poids seulement si certaines valeurs sont dépassées"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ActivationLayer (Layer) : #On prend en entrée un variable de type de la classe Layer\n\n    #On initialise la couche avec la fonction d'activation, sa dérivée pour le calcul de l'erreur et param sera l'objet contenant les infos\n    def __init__(self, activation, activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n\n\n    #On code la passe avant, c'est un simple calcul d'image par la fonction d'activation   \n    def forward_propagation(self, input_data):\n        self.input = input_data\n        self.output = self.activation(self.input) #On calcul la sortie en donnant l'entrée à  la fonction d'activ \n        return self.output\n\n    def backward_propagation(self, output_error, learning_rate):\n        #Le calcul de dE/dY nous permet de définir la fonction pour la régression\n        return self.activation_prime(self.input) * output_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tangente hyperbolique"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#De manière pas très originale, on peut reprendre la même fonction d'activation que lui, dont la forme modélise bien l'effet de seuil recherché pour une telle fonction.\ndef tanh(x): \n    return np.tanh(x)\n\ndef tanh_prime(x):\n    return 1-np.tanh(x)**2"},{"metadata":{},"cell_type":"markdown","source":"### Sigmoïd"},{"metadata":{},"cell_type":"markdown","source":"import math\n\ndef sig(x):\n    return 1 / (1 + math.exp(-x))\n\ndef sig_prime(x) : \n    return math.exp(x)/(1+math.exp(x))**2"},{"metadata":{},"cell_type":"markdown","source":"### ReLU"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ReLU(x):\n    return x * (x > 0)\n\ndef dReLU(x):\n    return 1. * (x > 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Avec un fonction de perte classique (MSE)\n#Rq : on la modifiera pour optimiser le programme, comme la fonction d'activation\n\ndef mse(y_true, y_pred):\n    return np.mean(np.power(y_true-y_pred, 2));\n\ndef mse_prime(y_true, y_pred):\n    return 2*(y_pred-y_true)/y_true.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classe Network\n\nc'est la construction du réseau de neurones en utilisant "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Network:\n    def __init__(self):\n        self.layers = []              # On créé dans self une liste qui contiendra toutes les couches de neurones\n        self.loss = None              # On définira la fonction de perte et sa dérivée ultérieurment\n        self.loss_prime = None\n\n    def add(self, layer):             # On définit la fonction d'addition d'une couche au réseau de neurones\n        self.layers.append(layer)\n\n    def use(self, loss, loss_prime):  # On créé la fonction permettant de définir la fonction de perte à utiliser pour le réseau\n        self.loss = loss              # On choisit l'argument loss renseigné dans la fonction comme fonction de perte, de même pour sa dérivée\n        self.loss_prime = loss_prime\n\n    def predict(self, input_data):    # Création de la fonction de prédiction d'une sortie result pour une entrée input_data donnée\n        samples = len(input_data)     # La taille de la sortie doit faire la taille de l'entrée\n        result = []                   # Création de la liste dans laquelle sera stockée la sortie\n\n        for i in range(samples):      # Boucle de la prédiction des valeurs en sortie\n            output = input_data[i]\n            for layer in self.layers: # On repéte l'itération pour toutes les couches afin de propager l'entrée\n                output = layer.forward_propagation(output) # On utilise la passe avant pour obtenir la sortie\n            result.append(output)     # On obtient la sortie en dernière couche\n\n        return result\n\n    def fit(self, x_train, y_train, epochs, learning_rate) : #La fonction fit va servir à entraîner le réseau de neuronnes\n#On donne le nombre d'aller-retour qu'on veut effectuer avec la variable epoch\n\n        samples = len(x_train) #On prend les dimensions des paramètre en input\n\n        for i in range(epochs):\n            err = 0 #On va devoir calculer l'erreur, qu'on initialise à 0\n\n            for j in range(samples):\n\n                #1ère étape : passe avant dans le réseau :\n                output = x_train[j] #On va calculer la sortie pour chaque x_j de l'entrée\n                for layer in self.layers:#En bouclant sur les couches, on parcourt le réseau en profondeur\n                    output = layer.forward_propagation(output) #Pas besoin de redonner self qui est global dans la structure de class\n                err += self.loss(y_train[j], output)\n                #2e étape : passe arrière dans le réseau :\n                error = self.loss_prime(y_train[j], output) #Fondamentalement pour la descente de gradient, il faut l'erreur, que l'on calcule :\n                for layer in reversed(self.layers): #On parcourt depuis la fin donc on doit retourner la liste des couches\n                    error = layer.backward_propagation(error,learning_rate) #On utilise les fontions héritées de la structure de couche pour calculer l'erreur\n\n            # erreur moyenne sur tous les échantillons étudiés\n            err /= samples\n            print('epoch %d/%d   error=%f' % (i+1, epochs, err))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XOR"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# training data\nx_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\ny_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n\n# network\nnet = Network()\nnet.add(FCLayer(2, 3))\nnet.add(ActivationLayer(ReLU, dReLU))\nnet.add(FCLayer(3, 1))\nnet.add(ActivationLayer(ReLU, dReLU))\n\n# train\nnet.use(mse, mse_prime)\nnet.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n\n# test\nout = net.predict(x_train)\nprint(out)"},{"metadata":{},"cell_type":"markdown","source":"# Chargement du dataset des ramens"},{"metadata":{"trusted":true},"cell_type":"code","source":"ram = pd.read_csv('../input/ramen-ratings/ramen-ratings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ram = ram.drop(['Variety'], axis = 1)\nram = ram.drop(['Review #'], axis = 1)\nram = ram.drop(['Brand'], axis = 1)\nram = ram.drop(['Top Ten'], axis = 1)\n\n#On remplace Cup, Pack, Tray, Bowl par 1, 2, 3, 4\n#On drop les Unrated\n#On remplace Japan, Taiwan, USA, South Korea, Singapoure, India /// on drop les autres\n\nstyl = list(ram.Style)\ncount = list(ram.Country)\nstars = list(ram.Stars)\nc=0\n\nfor i in range(len(styl)) :\n    i = i - c\n    if styl[i] == 'Cup' :\n        styl[i] = 1\n    elif styl[i] == 'Pack' :\n        styl[i] = 2\n    elif styl[i] == 'Tray' :\n        styl[i] = 3\n    elif styl[i] == 'Bowl' :\n        styl[i] = 4\n    else :\n        del(styl[i])\n        del(count[i])\n        del(stars[i])\n        c +=1\n        \n    if count[i] == 'Japan' :\n        count[i] = 1\n    elif count[i] == 'Taiwan' :\n        count[i] = 2\n    elif count[i] == 'USA' :\n        count[i] = 3\n    elif count[i] == 'South Korea' :\n        count[i] = 4\n    elif count[i] == 'Singapoure' :\n        count[i] = 5\n    elif count[i] == 'India' :\n        count[i] = 6\n    else :\n        del(styl[i])\n        del(count[i])\n        del(stars[i])\n        c+=1\n    \n    if stars[i] == 'Unrated' :\n        del(styl[i])\n        del(count[i])\n        del(stars[i])\n        c += 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Style = pd.DataFrame(styl)\nCountry = pd.DataFrame(count)\nStars = pd.DataFrame(stars)\n\nram = pd.DataFrame({'Style': styl, 'Country': count, 'Stars': stars})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ram_train = ram.sample(frac=0.8, random_state=1) # 80% des données avec frac=0.8\nram_test = ram.drop(ram_train.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"donnee = []\n\nfor i in range(len(styl)) :\n    donnee.append([styl[i], count[i]])\n    #On créer les données d'entraînements\nx_train = donnee    \ny_train = stars\n\nx_test = ram_test.drop(['Stars'], axis=1) #Puis celles de test\ny_test = ram_test.drop(['Style','Country'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lancement du réseau de neurones sur le dataset des ramens"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train = np.array(x_train)\ny_train = np.array(y_train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(x_train)) :\n    for j in range(0,2) : \n         x_train[i][j] = float(x_train[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(y_train)) : \n    y_train[i] = float(y_train[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.array(x_train)\ny_train = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], 1, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = Network()\nnet.add(FCLayer(2, 30))               \nnet.add(ActivationLayer(ReLU, dReLU))\nnet.add(FCLayer(30, 10))                   \nnet.add(ActivationLayer(ReLU, dReLU))\n\n\nnet.use(mse, mse_prime)\nnet.fit(x_train, y_train, epochs=20, learning_rate=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}