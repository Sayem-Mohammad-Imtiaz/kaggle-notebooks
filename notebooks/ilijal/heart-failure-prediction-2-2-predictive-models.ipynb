{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Heart Failure Prediction EDA</h1></center>\n\n<hr/>\n\n<img src=\"https://storage.googleapis.com/kaggle-datasets-images/727551/1263738/b480e9c8a7b4efd0026dff1a2aeb98df/dataset-cover.png?t=2020-08-18-10-19-56\" />\n\n<hr/>\n\nThis is EDA on Kaggle [dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data/activity) regarding heart failure diagnosing.\n\nThis is the second of two notebooks, in which we are going to build models to predict if patient is in danger of having heart failure.\n\n[First](https://www.kaggle.com/ilijal/heart-failure-prediction-1-2-eda) notebook focused on exploratory data analysis.\n\nWhithout further ado, lets start!"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n* [Conclusion from previous part](#Conclusions-from-previous-part)\n* [Loading libraries and modules](#Loading-libraries-and-modules)\n* [Importing data](#Importing-data)\n* [Preparing dataset](#Preparing-dataset)\n* [Inspect outliers](#Inspect-outliers)\n* [Predictive modeling](#Predictive-modeling)\n* [Conclusions](#Conclusions)\n* [References](#References)"},{"metadata":{},"cell_type":"markdown","source":"## Conclusions from previous part\n\n[TOC](#Table-of-contents)\n\nAfter the exploratory data analysis we have found out that:\n- there is **no missing** values,\n- there is **no duplicate** entries,\n- dataset is **moderately imbalanced**,\n- **none** of the **boolean** predictor variables correlates to patient death,\n- 4 of the **numerical predictor variables seems to indicate if patient will die**. **Those are `serum_creatinine`, `ejection_fraction`, `serum_sodium` and `age`**, with age having the least effect.\n- there are multiple numeric variables that contain **outliers** (`creatinine_phosphokinase`, `platelets`, and `serum_creatinine`). However, we will handle outliers for **`serum_creatinine`** since it indicates patient death.   \n\nBased on these conclusions we want to:\n- use 4 numerical predictor variables `serum_creatinine`, `ejection_fraction`, `serum_sodium` and `age`,\n- want to handle outliers in `serum_creatinine` variable,\n- use scaling of variable values since some of variables have value ranges orders of magnitude bigger than others,\n- use Decision Trees, Random Forest and Logistic Regression models for classification.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Loading libraries and modules\n\n[TOC](#Table-of-contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.metrics import plot_roc_curve, accuracy_score, make_scorer\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import learning_curve, validation_curve, cross_val_score, train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing data\n\n[TOC](#Table-of-contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = '/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\n\ndf = pd.read_csv(file_name)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing dataset \n[TOC](#Table-of-contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert variables to bool\n\ndf['DEATH_EVENT'] = df.DEATH_EVENT.astype('bool')  \n\nBOOL_COLUMNS = ['anaemia', 'diabetes', 'high_blog_pressure', 'sex', 'smoking']\n# df['anaemia'] = df.anaemia.astype('bool')\n# df['diabetes'] = df.diabetes.astype('bool')\n# df['high_blood_pressure'] = df.high_blood_pressure.astype('bool')\n# df['sex'] = df.sex.astype('bool')\n# df['smoking'] = df.smoking.astype('bool')\n\ndf = df.drop(columns=['time'])  # drop time column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspect outliers\n[TOC](#Table-of-contents)\n\nIt is good to view possible outliers in variables again."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = c = 0\n\ncolumns = 3\nrows = int(np.ceil(len(df.columns)/columns))\n\nfig, axs = plt.subplots(nrows=rows, ncols=columns, figsize=(20, 20))\nplt.subplots_adjust(hspace=0.4)\n\nfor n, i in enumerate(df.columns):\n    if i in BOOL_COLUMNS:\n        sns.countplot(x=i, data=df, ax=axs[r, c])\n    else: \n        sns.boxenplot(x='DEATH_EVENT', y=i, data=df, ax=axs[r, c])\n    axs[r, c].set_title(i.upper(), y=1.02)\n    axs[r, c].set_ylabel(None)\n    \n    c += 1\n    if (n + 1) % columns == 0:\n        r += 1\n        c = 0\n\n\nfig.suptitle('Box plots and count plots for variables grouped by DEATH_EVENT', size=20, y=0.95)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at couple of articles on the web, we have found out that:\n- Ejection Fraction (EF) has normal range 55%-70%, slightly below normal 40%-54%, moderately below normal 35%-39% and **severely below normal less than 35%**. More [here](https://my.clevelandclinic.org/health/articles/16),\n- Normal serum creatinine levels are 0.9-1.3 mg/dL for males and 0.6-1.1 mg/dL for females. More [here](https://www.medicalnewstoday.com/articles/322380#what-does-the-test-involve) and [here](https://www.urmc.rochester.edu/encyclopedia/content.aspx?ContentTypeID=167&ContentID=creatinine_serum),\n- Serum sodium has normal levels between 135 to 145 mEq/L. Values **below 135 mEq/L may indicate health issues**. More [here](https://www.mayoclinic.org/diseases-conditions/hyponatremia/symptoms-causes/syc-20373711#:~:text=A%20normal%20blood%20sodium%20level,Certain%20medications)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.serum_creatinine.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.distplot(a=df.serum_creatinine, color='blue', hist=True, kde=False, ax=axs[0]);\naxs[0].set_title('serum_creatinine distribution plot');\nsns.distplot(a=df.loc[df.DEATH_EVENT == True, 'serum_creatinine'], color='red', hist=True, kde=False, ax=axs[1])\nsns.distplot(a=df.loc[df.DEATH_EVENT == False, 'serum_creatinine'], color='blue', hist=True, kde=False, ax=axs[1])\naxs[1].set_title('serum_creatinine distribution plot per DEATH_EVENT');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets use `median + 3*std` as a threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"serum_creatinine_thresh = df.serum_creatinine.median() + df.serum_creatinine.std() * 3\n\nprint('Threshold value:', serum_creatinine_thresh)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the records to be excluded, as they are marked as outliers by `serum_creatinine` values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.serum_creatinine > serum_creatinine_thresh]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictive modeling\n\n[TOC](#Table-of-contents)\n\nHere we want to handle everything along with hyperparameter tuning, comparing models and choosing best one.\n\nHere are the things we will do:\n- Separate predictor variable,\n- Divide dataset in training and test sets,\n- Define CV scheme and use traning data to find best hyperparameters and compare algorithms,\n- Use best performing model and tune hyperparameters with whole training data, then test generalization error on test set,\n- Display accuracy of classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"CHOSEN_COLUMNS = ['age', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']\n\nclass FeaturesChooser(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        X = X.copy()\n        return X[CHOSEN_COLUMNS]\n    \nclass CreatinineOutlierHandler(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        X = X.copy()\n        serum_creatinine_thresh = X.serum_creatinine.median() + X.serum_creatinine.std() * 3\n        return X[X.serum_creatinine <= serum_creatinine_thresh]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate predictor label"},{"metadata":{"trusted":true},"cell_type":"code","source":"HANDLE_OUTLIERS = True\n\n_df = df.copy()\n\nif HANDLE_OUTLIERS is True:\n    _df = CreatinineOutlierHandler().fit_transform(df)\n\nX, y = _df.drop(columns=['DEATH_EVENT']), _df.DEATH_EVENT\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate trainin and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=42)\n\nX.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets briefly display each variable distributions per created dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=4, figsize=(20, 20))\n\nr, c = 0, 0\n\nfor n, i in enumerate(X_train.columns):\n    X_train[i].hist(ax=axs[r, c])\n    X_test[i].hist(ax=axs[r, c])\n    axs[r, c].set_title('Distribution of {}'.format(i))\n\n    c += 1\n    if (n + 1) % columns == 0:\n        r += 1\n        c = 0\n        \nfig.suptitle('Distributions of variables for train and test sets.', size=20, y=0.925);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining CV scheme and hyperparameter tuning\n\nThe idea is to compare results of:\n- LogisticRegression,\n- DecisionTreeClassifier and\n- RandomForestClassifier,\n\nas introductory algorithms for classification.\n\nWhat we want to do here is to:\n- tune hyperparameters and take best ones by measuring how stable each model is,\n- compare each model based on stability,\n- use test set to estimage how good is final model at generalizing.\n\nSo basically, we want to:\n- take part of the data as test set (20%),\n- take rest of the data as train set (80%),\n- use nested 5x4 CV scheme. 5 folds of outter loop for estimating generalization and 4 folds of inner loop for hyperparameter tuning. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"INNER_SPLITS = 4\nOUTTER_SPLITS = 5\n\nprint(f\"Train set size {X_train.shape[0]}\\n Test set size {X_test.shape[0]}\\n\")\n\nprint('Train/test size = {}/{} in outer splits.'.format(int(np.ceil(X_train.shape[0]*(OUTTER_SPLITS-1)/OUTTER_SPLITS)), X_train.shape[0]//OUTTER_SPLITS))\noutter_train_size = int(np.ceil(X_train.shape[0]*(OUTTER_SPLITS-1)/OUTTER_SPLITS))\n\ninner_train_size = int(np.ceil(outter_train_size*(INNER_SPLITS-1)/INNER_SPLITS))\ninner_test_size = int(outter_train_size-inner_train_size)\nprint('Train/test size = {}/{} in inner splits.'.format(inner_train_size, inner_test_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose scoring function for CV scheme\nscoring_function = 'roc_auc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_gridsearchcvs(stratified_folds=INNER_SPLITS):\n    names, pipes, params = [], [], []\n    \n    pipe = Pipeline((\n                    ('stds', StandardScaler()),\n                    ('est', LogisticRegression(solver='liblinear'))))\n\n    pipe_params = {\n        'est__C': [0.0001, 0.001, .01, 0.1, 1, 10, 100]\n    }\n    \n    names.append('LogisticRegression_1')\n    pipes.append(pipe)\n    params.append(pipe_params)\n\n    pipe = Pipeline((\n                    ('stds', MinMaxScaler()),\n                    ('est', LogisticRegression(solver='liblinear'))))\n\n    pipe_params = {\n        'est__C': [0.0001, 0.001, .01, 0.1, 1, 10, 100]\n    }\n    \n    names.append('LogisticRegression_2')\n    pipes.append(pipe)\n    params.append(pipe_params)\n    \n    pipe = Pipeline((('less_fts', FeaturesChooser()),\n                    ('stds', StandardScaler()),\n                    ('est', LogisticRegression(solver='liblinear'))))\n\n    pipe_params = {\n        'est__C': [0.0001, 0.001, .01, 0.1, 1, 10, 100]\n    }\n\n    names.append('LogisticRegression_3')\n    pipes.append(pipe)\n    params.append(pipe_params)\n\n    \n    pipe = Pipeline((\n        ('est', DecisionTreeClassifier(criterion='entropy', random_state=42)),\n    ))\n\n    pipe_params = {\n        'est__max_depth': [3, 4, 5, 6], \n        'est__min_samples_leaf': [5, 10, 15, 20, 30]\n    }\n\n    names.append('DecisionTree_1')\n    pipes.append(pipe)\n    params.append(pipe_params)\n\n    pipe = Pipeline((\n        ('poly', PolynomialFeatures(interaction_only=True, include_bias=False)),\n        ('est', DecisionTreeClassifier(criterion='entropy', random_state=42)),\n    ))\n\n    pipe_params = {\n        'poly__degree': [1, 2, 3],\n        'est__max_depth': [3, 4, 5, 6], \n        'est__min_samples_leaf': [5, 10, 15, 20, 30]\n    }\n\n    names.append('DecisionTree_2')\n    pipes.append(pipe)\n    params.append(pipe_params)\n    \n    pipe = Pipeline((\n        ('est', RandomForestClassifier(criterion='entropy', random_state=42)),\n    ))\n\n    pipe_params = {\n        'est__n_estimators': [50, 100, 150, 200],\n        'est__max_depth': [3, 4],\n        'est__min_samples_leaf': [6, 10, 13]\n    }\n\n    names.append('RandomForest_1')\n    pipes.append(pipe)\n    params.append(pipe_params)\n    \n    gcvs = []\n\n    for name, pipe, params in zip(names, pipes, params):\n        print(f'Adding GridSearchCV for {name} estimator...')\n        gcv = GridSearchCV(pipe, \n                           param_grid=params, \n                           scoring=scoring_function, \n                           refit=True, \n                           n_jobs=-1, \n                           cv=stratified_folds, \n                           return_train_score=True)\n        gcvs.append({'name': name, 'gs': gcv})\n    \n    print('\\n')\n    return gcvs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcvs = get_gridsearchcvs()\n\nfor gcv in gcvs:\n    scores = cross_val_score(gcv['gs'], X_train, y_train, scoring=scoring_function, cv=OUTTER_SPLITS)\n    print('{:>20s}, {:.2f}, std +/- {:.2f}%'.format(gcv['name'], 100 * np.mean(scores), 100 * np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcvs = get_gridsearchcvs(stratified_folds=4)\n\nstats = []\n    \nfor gcv in gcvs:\n    gcv['gs'].fit(X_train, y_train)\n    print(f\"Estimator: {gcv['name']}, params: {gcv['gs'].best_params_}\")\n    cols = ['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']\n    cv_stats = pd.DataFrame(gcv['gs'].cv_results_)[cols].describe().loc['mean', :]\n\n    print(f\"CV train score {cv_stats['mean_train_score']:.5f} +/- {cv_stats['std_train_score']:.2f}%\")\n    print(f\"CV validation score {cv_stats['mean_test_score']:.5f} +/- {cv_stats['std_test_score']:.2f}%\")\n\n    whole_train_score = gcv['gs'].score(X_train, y_train)\n    test_score = gcv['gs'].score(X_test, y_test)\n    \n    stats.append({'name': gcv['name'],\n                  'folds': 4,\n                  'train_score_mean': cv_stats['mean_train_score'], \n                  'train_score_std': cv_stats['std_train_score'],\n                  'train_score': whole_train_score,\n                  \n                  'validation_score_mean': cv_stats['mean_test_score'], \n                  'validation_score_std': cv_stats['std_test_score'],\n                  'test_score': test_score})\n    \n    print(f\"Whole train score: {whole_train_score:.5f}\")\n    print(f\"Whole test score: {test_score:.5f}\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcvs = get_gridsearchcvs(stratified_folds=5)\n\nfor gcv in gcvs:\n    gcv['gs'].fit(X_train, y_train)\n    print(f\"Estimator: {gcv['name']}, params: {gcv['gs'].best_params_}\")\n    cols = ['mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']\n    cv_stats = pd.DataFrame(gcv['gs'].cv_results_)[cols].describe().loc['mean', :]\n\n    print(f\"CV train score {cv_stats['mean_train_score']:.5f} +/- {cv_stats['std_train_score']:.2f}%\")\n    print(f\"CV validation score {cv_stats['mean_test_score']:.5f} +/- {cv_stats['std_test_score']:.2f}%\")\n\n    whole_train_score = gcv['gs'].score(X_train, y_train)\n    test_score = gcv['gs'].score(X_test, y_test)\n    \n    stats.append({'name': gcv['name'],\n                  'folds': 5,\n                  'train_score_mean': cv_stats['mean_train_score'], \n                  'train_score_std': cv_stats['std_train_score'],\n                  'train_score': whole_train_score,\n                  \n                  'validation_score_mean': cv_stats['mean_test_score'], \n                  'validation_score_std': cv_stats['std_test_score'],\n                  'test_score': test_score})\n    \n    print(f\"Whole train score: {whole_train_score:.5f}\")\n    print(f\"Whole test score: {test_score:.5f}\")\n    print()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats = pd.DataFrame(stats)\ndf_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20, 5));\n\nsns.barplot(y='name', x='train_score_mean', hue='folds', data=df_stats, ax=axs[0])\naxs[0].set_xlim(min(df_stats['train_score_mean'] - 0.05), 1);\n\nsns.barplot(y='name', x='train_score_std', hue='folds', data=df_stats, ax=axs[1])\naxs[1].set_xlim(min(min(df_stats['train_score_std']), 0));\n\nsns.barplot(y='name', x='validation_score_mean', hue='folds', data=df_stats, ax=axs[2])\naxs[2].set_xlim(min(df_stats['validation_score_mean'])- 0.1, 1);\n\nsns.barplot(y='name', x='validation_score_std', hue='folds', data=df_stats, ax=axs[3])\naxs[3].set_xlim(min(0, min(df_stats['validation_score_std'])-0.005));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20, 5));\nsns.barplot(x='train_score', y='name', hue='folds', data=df_stats, ax=axs[0]);\naxs[0].set_xlim(.5, .9);\nsns.barplot(x='test_score', y='name', hue='folds', data=df_stats, ax=axs[1]);\naxs[1].set_xlim(.5, .9);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=len(gcvs), figsize=(20, 5), sharey=True)\n\ncv=10\n\nfor i, gcv in enumerate(gcvs):\n#     print(gcv['name'])\n    x, y1, y2 = learning_curve(gcv['gs'].best_estimator_, X_train, y_train, cv=cv, scoring=scoring_function)\n    axs[i].plot(x, y1.mean(1), label='train');\n    axs[i].plot(x, y2.mean(1), label='test');\n    axs[i].set_title(f\"{gcv['name']}\")\n    axs[i].axhline((y1.mean(1)[-1]+y2.mean(1)[-1])/2, c='r', linewidth=.5, linestyle='--')\n    axs[i].legend()\n    \nfig.suptitle('Learning curves for our models', size=20, y=1.02);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=len(gcvs), figsize=(20, 4))\n\nfor i, gcv in enumerate(gcvs):\n    ac_score = accuracy_score(y_test, gcv['gs'].predict(X_test))\n    plot_confusion_matrix(confusion_matrix(y_test, gcv['gs'].predict(X_test)), axis=axs[i]);\n    axs[i].set_title(f\"{gcv['name']}\\n{ac_score:.4f}\")\n    axs[i].set_ylabel(None)\n    axs[0].set_ylabel('true label')\n    axs[i].set_xlabel('predicted label')\n\nfig.suptitle('Confusion matrices for our models', size=20, y=1.02);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\n[TOC](#Table-of-contents)\n\n* Removing outliers improves accuracy score for almost all classifiers,\n* Logistic Regression happens to be the best algorithm for this problem with maximal accuracy of 0.763 accuracy score,\n* Logistic Regression is more stable in its predictions than the others. We can see this by comparing how similar validation and test scores for each of them is. \n\n## References\n\n[TOC](#Table-of-contents)\n\n- Research [paper](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5) pointed in Kaggle dataset description,\n- [Sebastian Raschka's great paper on **Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning**](https://arxiv.org/pdf/1811.12808.pdf),\n- [Great presentation on Bias-Variance tradeoff](https://stdm.github.io/downloads/courses/ML/V06_BiasVariance-LearningCurves.pdf)\n\n<hr>\n\nI hope you have enjoyed this EDA notebook and that you have learned something new about the dataset and Data Science in general. \n\nCheers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}