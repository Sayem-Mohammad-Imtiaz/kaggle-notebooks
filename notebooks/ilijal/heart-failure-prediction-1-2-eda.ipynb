{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Heart Failure Prediction EDA</h1></center>\n\n<hr/>\n\n<img src=\"https://storage.googleapis.com/kaggle-datasets-images/727551/1263738/b480e9c8a7b4efd0026dff1a2aeb98df/dataset-cover.png?t=2020-08-18-10-19-56\" />\n\n<hr/>\n\nThis is EDA on Kaggle [dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data/activity) regarding heart failure diagnosing.\n\nThis is the first of two notebooks, in which we are going to conduct EDA and check which predictor variables indicate undesirable event of patient's death.\n\n[Second](https://www.kaggle.com/ilijal/heart-failure-prediction-1-2-eda) notebook will focus on using this notebook's insights for creating predictive models.\n\nWhithout further ado, lets start!"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n* [Loading libraries and modules](#Loading-libraries-and-modules)\n* [Importing data](#Importing-data)\n* [Brief data overview](#Brief-data-overview)\n* [Detailed data overview](#Detailed-data-overview)\n* [Conclusions](#Conclusions)\n* [References](#References)"},{"metadata":{},"cell_type":"markdown","source":"## Loading libraries and modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import ks_2samp\nfrom scipy.spatial.distance import jensenshannon\n\nfrom IPython.display import display, Markdown, Latex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing data\n[TOC](#Table-of-contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = '/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\n\ndf = pd.read_csv(file_name)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 13 columns, 12 of which are predictor and 1 that is predicted one (DEATH_EVENT).\n\nOne of predictor variables (`time`) can be missleading since it is something that was known for the time of conducting the experiment. Therefore, it is obviously highly indicative of predicted variable. However, this is not something we will know for any future patient that we want to diagnose, and that is why we do not want to use it for predictive model.\n\nOne idea is to use other 11 predictor variables to predict this numerical variable (`time`), through regression model. Food for thought."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Brief data overview\n[TOC](#Table-of-contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously some of variable types can be more specific e.g. boolean ones.\n\nIt would be nice to convert this for further using."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of missing values per column:')\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, there is no missing values at all.\n\nLets check for duplicated ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Is there any duplicated rows:', any(df.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no duplicated rows. Great. Very well prepared data set so far.\n\nLets now check how balanced this set is."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.DEATH_EVENT.value_counts())\nsns.countplot(x='DEATH_EVENT',data=df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude that this set is imbalanced, but its not extremly.\n\nBased on this imbalance we will be careful how we build and choose models and how we assess their accuracy.\n\n[TOC](#Table-of-contents)"},{"metadata":{},"cell_type":"markdown","source":"## Detailed data overview\n\n[TOC](#Table-of-contents)\n\nLets first convert data types to more usable ones.\n\nAs you can remember, there are 6 variables of boolean type. \n\nLets convert them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['anaemia'] = df.anaemia.astype('bool')\ndf['diabetes'] = df.diabetes.astype('bool')\ndf['high_blood_pressure'] = df.high_blood_pressure.astype('bool')\ndf['sex'] = df.sex.astype('bool')\ndf['smoking'] = df.smoking.astype('bool')\ndf['DEATH_EVENT'] = df.DEATH_EVENT.astype('bool')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column `time` is somethig we should not use for predicting death event since this variable is obviously indicative. \n\nMore so, its not clear wheter this variable will be known for future patients."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_column = df['time']\ndf.drop(columns=['time'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bool_columns = df.columns[df.dtypes.apply(pd.api.types.is_bool_dtype)].values\nnum_columns = df.columns[~df.columns.isin(bool_columns)].values.tolist() + ['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data=df[num_columns], hue='DEATH_EVENT', height=1.5);\ng.map_lower(sns.kdeplot, levels=4, color=\".2\");\ng.fig.suptitle('Pairplot of numerical variables with scatter and KDE plots\\nabove and below diagonal.', size=15, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution plots on diagonal might prove to be useful. Lets plot those separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 0\nc = 0\n\nfig, axs = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace=0.3)\n\nfor n, i in enumerate(num_columns[:-1]):\n    class0 = df.loc[df.DEATH_EVENT == False, i].sort_values()\n    class1 = df.loc[df.DEATH_EVENT == True, i].sort_values()\n    \n    sns.distplot(class0, color='blue', ax=axs[r, c]);\n    sns.distplot(class1, color='red', ax=axs[r, c]);\n    \n    sns.rugplot(class0, color='blue', alpha=.3, ax=axs[r, c])\n    sns.rugplot(class1, color='red', alpha=.3, ax=axs[r, c])\n    \n    axs[r, c].legend(title='DEATH_EVENT', labels=['False', 'True']);\n    axs[r, c].set_title(i.upper(), y=1.02)\n    axs[r, c].set_ylabel(None)\n    c += 1\n    if (n + 1) % 3 == 0:\n        r += 1\n        c = 0\n    \nfig.suptitle('Distribution plots for numerical variables by DEATH_EVENT', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are some wide tails that show potential outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 0\nc = 0\n\nfig, axs = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace=0.3)\n\nfor n, i in enumerate(num_columns[:-1]):\n    class0 = df.loc[df.DEATH_EVENT == False, i].sort_values()\n    class0N = len(class0)\n    class1 = df.loc[df.DEATH_EVENT == True, i].sort_values()\n    class1N = len(class1)\n    \n    axs[r, c].plot(class0, np.array(range(class0N))/float(class0N), c='b')\n    axs[r, c].plot(class1, np.array(range(class1N))/float(class1N), c='r')\n    axs[r, c].set_ylim(0, 1)\n    \n    axs[r, c].legend(title='DEATH_EVENT', labels=['False', 'True']);\n\n    axs[r, c].set_title(i.upper(), y=1.02)\n    axs[r, c].set_ylabel(None)\n    c += 1\n    if (n + 1) % 3 == 0:\n        r += 1\n        c = 0\n    \nfig.suptitle('Cumulative distribution plots for numerical variables by DEATH_EVENT', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) functions can be used to inspect difference between multiple distributions. Of course, this is only for visual inspection.\n\nWe can use more rigorous methods like performing [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) test and see if there is a difference and how significant is it."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = []\n\nfor n, i in enumerate(num_columns[:-1]):\n    s, p = ks_2samp(df.loc[df.DEATH_EVENT == 0, i], df.loc[df.DEATH_EVENT == 1, i])\n    if p < 0.05: a= '--> '\n    else: a = '    '\n    print('{}KS s={:.2f}, p={:.3f} for {}'.format(a, s, p, i))\n    r.append({'predictor': i, 'statistic': s, 'p': p})\n    \nprint()\npd.DataFrame(r).sort_values(by=['p']).query('p < 0.05')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on results of Kolmogorov-Smirnov tests and p-values, only 4 predictor variables appear to have different distributions between groups based on DEATH_EVENT, where difference is statistically significant. \n\nThose are: `serum_creatinine`, `ejection_fraction`, `serum_sodium` and `age`. We will have those in mind.\n\nLets try using [Jensen-Shannon](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) distribution distances for numerical variables and see what value per variable we get. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace=0.4)\n\nr, c = 0, 0\ndistances = []\n\nfor i, col in enumerate(num_columns[:-1]):\n    _, bins = np.histogram(df[col], bins='fd')\n    a, _ = np.histogram(df.loc[df.DEATH_EVENT == True, col], bins=bins)\n    b, _ = np.histogram(df.loc[df.DEATH_EVENT == False, col], bins=bins)\n\n    jsd = jensenshannon(a, b)\n    \n    sns.barplot(x=bins[:-1], y=a, color='b', alpha=0.2, ax=axs[r, c]);\n    g = sns.barplot(x=bins[:-1], y=b, color='r', alpha=0.3, ax=axs[r, c]);\n    g.set_title('Distance for {} is {:.3f}'.format(col, jsd), size=12, y=1.05);\n    g.set_xticklabels(['%.2f' %(float(z.get_text()),) for z in g.get_xticklabels()], rotation=75)\n   \n    distances.append({'column': col, 'distance': jsd})\n    c += 1\n    if (i + 1) % 3 == 0:\n        r += 1\n        c = 0\n    \nfig.suptitle('Jensen-Shannon distances for numerical variables grouped by DEATH_EVENT', size=20)\nplt.show()\n\nprint('Jensen-Shannon distances for numerical variables, sorted in descending order:')\npd.DataFrame(distances).sort_values(by='distance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, above mentioned variables also have distances bigger than the rest. JS distance may be useful in similar scenarios.\n\nAnyway, we proceed with 4 above mentioned variables.\n\nLets see how numerical variables are distributed by using box plots. \n\nThis will give us additional perspective of comparing medians, quantiles and outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"r = c = 0\n\nfig, axs = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace=0.3)\n\nfor n, i in enumerate(num_columns[:-1]):\n    sns.boxenplot(x='DEATH_EVENT', y=i, data=df, ax=axs[r, c])\n    axs[r, c].set_title(i.upper(), y=1.02)\n    axs[r, c].set_ylabel(None)\n    c += 1\n    if (n + 1) % 3 == 0:\n        r += 1\n        c = 0\n    \nfig.suptitle('Box plots for numerical variables grouped by DEATH_EVENT', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('DEATH_EVENT').describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, here we can see outliers too. We should probably do something about it.\n\nLets check correlations between these numerical predictors and DEATH_EVENT.\n\nWe will use Pearson, Spearman and Kendall coefficients."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nsns.heatmap(df[num_columns].corr(method='pearson'), \n            annot=True,\n            fmt='.2f');\nplt.title('Pearson correlation between numeric predictor variables\\nand with predicted variable.', size=15, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nsns.heatmap(df[num_columns].corr(method='spearman'), \n            annot=True,\n            fmt='.2f');\n\nplt.title('Spearman correlation between numeric predictor variables\\nand with predicted variable.', size=15, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nsns.heatmap(df[num_columns].corr(method='kendall'), \n            annot=True,\n            fmt='.2f');\n\nplt.title('Kendall correlation between numeric predictor variables\\nand with predicted variable.', size=15, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the correlation coefficients show pretty much the same thing.\n\nLets get this verbosely."},{"metadata":{"trusted":true},"cell_type":"code","source":"s_p_corr_variables = df.corr(method='pearson')['DEATH_EVENT'].abs().sort_values(ascending=False)\ns_s_corr_variables = df.corr(method='spearman')['DEATH_EVENT'].abs().sort_values(ascending=False)\ns_k_corr_variables = df.corr(method='kendall')['DEATH_EVENT'].abs().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pearson correlations')\nprint(s_p_corr_variables[s_p_corr_variables >= .2])\nprint('')\nprint('Spearman correlations')\nprint(s_s_corr_variables[s_s_corr_variables >= .2])\nprint('')\nprint('Kendall correlations')\nprint(s_k_corr_variables[s_k_corr_variables >= .2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After this we may take couple of variables into consideration, based on correlation coefficients above. These variables might be indicative of death event.\n\nThese variables are: `serum_creatinine`, `ejection_fraction`, `age` and `serum_sodium`.\n\nSame to what we already discovered. This looks promising.\n\nLets take boolean predictor variables and calculate Pearson correlation coefficient with predicted variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nsns.heatmap(df[bool_columns].corr(method='pearson'), \n            annot=True,\n            fmt='.2f');\n\nplt.title('Pearson correlation between boolean predictor variables\\nand with predicted variable.', size=15, y=1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's no any correlation between boolean predictor variables and predicted one. Interesting...\n\nHow about we compare proportions of patients per boolean predictor variable, based on the class."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 0\nc = 0\n\nfig, axs = plt.subplots(2, 3, figsize=(20, 10))\nplt.subplots_adjust(hspace=0.3)\n\nfor n, i in enumerate(bool_columns[:-1]):\n    ct = pd.crosstab(columns=df[i], index=df.DEATH_EVENT, normalize=\"columns\")\n    \n    ct.T.plot(kind='bar', stacked=True, ax=axs[r, c]);\n\n    axs[r, c].set_title(i.upper(), y=1.02)\n    axs[r,c].set_ylabel(\"% of observations\")\n\n    c += 1\n    if (n + 1) % 3 == 0:\n        r += 1\n        c = 0\n    \naxs[r,c].axis(\"off\")\n\nfig.suptitle('Stacked bar plots for boolean variables grouped by DEATH_EVENT', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By inspecting visually, only `high_blood_pressure` might be indicative of death event, since there is higher percent of death events when patient suffers from high blood pressure. \n\nHowever, this analysis should be conducted more rigorously. \n\nLets check contigency tables for each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in bool_columns[:-1]:\n    ct = pd.crosstab(df[c], df.DEATH_EVENT)\n    print('Contigency table for {}'.format(c))\n    print(ct)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are couple of options for testing associations between categorical variables in contigency tables.\n\nSome of the tests that can be used for this purpose are:\n- Chi2 contigency test,\n- Fisher's test\n\nThere are certain pros and cons for each of them, but most notable are:\n- Chi2 provides an aproximation and gets more precise when sample is bigger. The approximation is inadequate when sample sizes are small, or the data are very unequally distributed among the cells of the table, resulting in the cell counts predicted on the null hypothesis (the “expected values”) being low.\n- Fishers test is exact, but is used with smaller samples. It becomes difficult to calculate with large samples or well-balanced tables, but fortunately these are exactly the conditions where the Chi2 test is appropriate.\n\nBased on this, we will use Chi2 contigency test.\n\nFor more info on this, check Wiki [article](https://en.wikipedia.org/wiki/Fisher%27s_exact_test)."},{"metadata":{"trusted":true},"cell_type":"code","source":"h0 = 'H0: There is no relationship between {} and DEATH_EVENT'\nh1 = 'H1: There is a relationship between {} and DEATH_EVENT'\n\nfor c in bool_columns[:-1]:\n    ct = pd.crosstab(columns=df[c],index=df.DEATH_EVENT)\n    stat, p, dof, expected = chi2_contingency(ct) \n\n    print('Contigency table for {}\\n'.format(c))\n    print(ct)\n    print()\n    print(h0.format(c))\n    print(h1.format(c))\n    print('\\np value:', np.round(p,2))\n    if p < 0.05:\n        message = Markdown('<b style=\"color: red;\">Reject H0</b>')\n    else:\n        message = Markdown('<b style=\"color: green;\">Failed to reject H0</b>')\n    display(message)\n    print('\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results show that none of the binary predictor variables has significant effect on DEATH_EVENT."},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\nAfter the exploratory data analysis we have found out that:\n- there is **no missing** values,\n- there is **no duplicate** entries,\n- dataset is **moderately imbalanced**,\n- **none** of the **boolean** predictor variables correlates to patient death,\n- 4 of the **numerical predictor variables seems to indicate if patient will die**. **Those are `serum_creatinine`, `ejection_fraction`, `serum_sodium` and `age`**, with age having the least effect.\n- there are multiple numeric variables that contain **outliers** (`creatinine_phosphokinase`, `platelets`, and `serum_creatinine`). However, we will handle outliers for **`serum_creatinine`** since it indicates patient death.   \n\n[TOC](#Table-of-contents)"},{"metadata":{},"cell_type":"markdown","source":"## References\n\n- Research [paper](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5) pointed in Kaggle dataset description\n- Expected values of serum creatinine [calculator](http://www.scymed.com/en/smnxps/psxdf212_c.htm)\n- Another [description](https://www.mayoclinic.org/tests-procedures/creatinine-test/about/pac-20384646) on serum creatinine expected test values\n- [Article](https://medium.com/datalab-log/measuring-the-statistical-similarity-between-two-samples-using-jensen-shannon-and-kullback-leibler-8d05af514b15) on measuring the statistical similarity between two samples using Jensen-Shannon and Kullback-Leibler divergences\n- [Article](https://towardsdatascience.com/how-to-compare-two-distributions-in-practice-8c676904a285) on how to compare two distributions in practice\n- Great [tutorial](https://machinelearningmastery.com/divergence-between-probability-distributions/) on how to calculate the KL divergence for machine learning \n- [Wiki](https://en.wikipedia.org/wiki/Fisher%27s_exact_test) article on Fishers test and when to use it.\n\n[TOC](#Table-of-contents)\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n\nI hope you have enjoyed this EDA notebook and that you have learned something new about the dataset and Data Science in general. \n\nCheers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}