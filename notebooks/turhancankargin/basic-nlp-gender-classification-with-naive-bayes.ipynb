{"cells":[{"metadata":{},"cell_type":"markdown","source":"Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\n* In this notebook, we will learn the basics of the NLP by using Twitter User Gender Classification dataset.\n* We will classify the dataset by using Naive Bayes Algorithm"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Twitter Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(r\"/kaggle/input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv\",encoding = \"latin1\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our aim is to classify the gender from tweets so, we just need gender and description columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data.gender,data.description],axis=1) # New data contains just two columns\ndata.dropna(axis = 0,inplace = True) # Drop NaN values\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender] # 1 for female, 0 for male\ndata.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Data "},{"metadata":{},"cell_type":"markdown","source":"### Regular Expression:\n* Regular Expression, is a sequence of characters that forms a search pattern.\n* RegEx can be used to check if a string contains the specified search pattern."},{"metadata":{},"cell_type":"markdown","source":"Firstly, I will show you whole process from one tweet. Then, it will be applied for whole tweets in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_description = data.description[4] \nfirst_description","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndescription = re.sub(\"[^a-zA-Z]\",\" \",first_description)  # Except from a to z, and from A to Z will be transform to space\ndescription = description.lower()   # Make whole words lowercase\ndescription","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stopwords (Irrelavent Words)\n* In computing, stop words are words that are filtered out before or after the natural language data (text) are processed. While “stop words” typically refers to the most common words in a language, all-natural language processing tools don’t use a single universal list of stop words."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk # natural language tool kit\nnltk.download(\"stopwords\")      \nfrom nltk.corpus import stopwords  \ndescription = nltk.word_tokenize(description) # To split words\ndescription = [ word for word in description if not word in set(stopwords.words(\"english\"))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatazation\n* For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n\n* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n\n* *  am, are, is $\\Rightarrow$ be\n* * car, cars, car's, cars' $\\Rightarrow$ car\n* * The result of this mapping of text will be something like:\n* * the boy's cars are different colors $\\Rightarrow$ the boy car be differ color"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk as nlp\n\nlemma = nlp.WordNetLemmatizer()\ndescription = [ lemma.lemmatize(word) for word in description] \n\ndescription = \" \".join(description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply these to all tweets with for loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"description_list = []\nfor description in data.description:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n    description = description.lower()   \n    description = nltk.word_tokenize(description)\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n    lemma = nlp.WordNetLemmatizer()\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words\n* A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n* The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n* A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n    1. A vocabulary of known words.\n    1. A measure of the presence of known words.\n    \n\n* It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n* A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer # for bag of words\nmax_features = 5000\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\nprint(\"Most Common {} word is {}\".format(max_features,count_vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Our Machine Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.iloc[:,0].values   # male or female classes (output)\nx = sparce_matrix # our input\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# visualize number of digits classes\nplt.figure(figsize=(15,7))\nsns.countplot(y)\nplt.title(\"Number of Gender\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n\n\n# naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n# prediction\ny_pred = nb.predict(x_test)\n\nprint(\"Accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}