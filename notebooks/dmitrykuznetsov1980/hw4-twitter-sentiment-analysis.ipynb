{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest  = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[::10].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[::10].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['tweet'].str.contains(\">\")].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Negative class count in train dataset: ', len(train[train['label'] == 1]))\nprint('Neutral class count in train dataset: ', len(train[train['label'] == 0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_train = train.tweet\ntext_test = test.tweet\nprint(text_train)\ny_train = train.label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying model Bag-of-words for the list of tweets","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets look at the vocabulary:\nfeature_names = vect.get_feature_names()\nprint(\"Features quantity: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n {}\".format(feature_names[:20]))\nprint(\"Each 100th feature:\\n {}\".format(feature_names[::100]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nscores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\nprint(\"Average accuracy in cross val: {:.2f}\".format(np.mean(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets we try to improve it using GridSearch:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.001,0.01,0.1,1,10]}\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"The best value for cross val: {:.2f}\".format(grid.best_score_))\nprint(\"The best parameters: \", grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result 0.96 is the same as in previous step..","metadata":{}},{"cell_type":"markdown","source":"Let we set minimum of documents where each token appears:","metadata":{}},{"cell_type":"code","source":"vect = CountVectorizer(min_df=5).fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with min_df:\\n{}\".format(repr(X_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = vect.get_feature_names()\nprint(\"Features quantity: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n {}\".format(feature_names[:20]))\nprint(\"Each 100th feature:\\n {}\".format(feature_names[::100]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"The best value for cross val: {:.2f}\".format(grid.best_score_))\nprint(\"The best parameters: \", grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result 0.96 is the same as in previous step.. Probably parameter min_df doesn't metter for the dataset.  ","metadata":{}},{"cell_type":"markdown","source":"# Stop words\nLets improve results by using stop-words removal.","metadata":{}},{"cell_type":"code","source":"vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with stop words removing :\\n{}\".format(repr(X_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"The best value for cross val: {:.2f}\".format(grid.best_score_))\nprint(\"The best parameters: \", grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best value the same - 0.96. Stop words remaval also doesn't matter for result improving.","metadata":{}},{"cell_type":"markdown","source":"# TF-IDF \nNow lets try to use TF-IDF scaling and look how it can improve results","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001,0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"The best result: {:.2f}\".format(grid.best_score_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result is the same again.\nBut let we look at the most relevant and the least relevant tokens: ","metadata":{}},{"cell_type":"code","source":"vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\nX_train = vectorizer.transform(text_train)\nmax_value = X_train.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\nfeature_names = np.array(vectorizer.get_feature_names())\nprint(\"Features with minimum value of tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[:100]]))\nprint(\"Features with maximum value of tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[-100:]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_by_idf = np.argsort(vectorizer.idf_)\nprint(\"Features with minimum value idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}