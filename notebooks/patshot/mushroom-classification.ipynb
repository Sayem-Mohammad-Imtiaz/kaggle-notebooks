{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1"}},"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{},"source":"# Mushroom Classification\n\n    a rather short analysis!\n\nWelcome! The motivation for this dataset is already well defined. Namely, \n\n1. What types of machine learning models perform best on this dataset?\n2. Which features are most indicative of a poisonous mushroom?\n\nSo, let's begin!"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":9},{"cell_type":"code","outputs":[],"metadata":{},"source":"#Read the data using pandas\ndata=pd.read_csv(\"../input/mushrooms.csv\")","execution_count":10},{"cell_type":"markdown","metadata":{},"source":"Let's see how it looks like!"},{"cell_type":"code","outputs":[],"metadata":{},"source":"data.head()","execution_count":11},{"cell_type":"markdown","metadata":{},"source":"To know more about the data, let's use the *describe* method of pandas."},{"cell_type":"code","outputs":[],"metadata":{},"source":"data.describe().transpose()","execution_count":12},{"cell_type":"markdown","metadata":{},"source":"There are lots of features! Also, there are 8124 entries, but the good news is that"},{"cell_type":"code","outputs":[],"metadata":{},"source":"total_null_values = sum(data.isnull().sum())\nprint(total_null_values)","execution_count":13},{"cell_type":"markdown","metadata":{},"source":"There are no null values! So, we can easily skip some Feature Engineering. All we now need to do to the data is encode it such a way that our algorithms will understand. We need to encode the given labels, using Scikit-Learn's Label Encoder."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.preprocessing import LabelEncoder\nEnc = LabelEncoder()\ndata_tf = data.copy()\nfor i in data.columns:\n    data_tf[i]=Enc.fit_transform(data[i])","execution_count":14},{"cell_type":"markdown","metadata":{},"source":"Although it's just fine to proceed with transforming all the data, let's just transform the features and leave the targets alone. "},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"X = data_tf.drop(['class'], axis=1)\nY = data_tf['class']","execution_count":15},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"","execution_count":null},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42)","execution_count":16},{"cell_type":"code","outputs":[],"metadata":{},"source":"X_train.head(5)","execution_count":17},{"cell_type":"markdown","metadata":{},"source":"That looks good so far.\n\n## Classifiers\n\nSince there are only two classes, we can use some standard Binary Classifiers. We can start with Logistic Regression, but I'll use that just to contrast with the RandomForestClassifier. Other Kernels have clearly shown that RandomForestClassifiers and Decision Trees are a better fit with this data set. "},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.linear_model import LogisticRegression","execution_count":18},{"cell_type":"code","outputs":[],"metadata":{},"source":"from sklearn.metrics import accuracy_score\nlog_clf = LogisticRegression()\nlog_clf.fit(X_train, Y_train)\nLR_pred=log_clf.predict(X_test)\n\naccuracy_score(Y_test, LR_pred)","execution_count":19},{"cell_type":"code","outputs":[],"metadata":{},"source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(LR_pred, Y_test)","execution_count":20},{"cell_type":"markdown","metadata":{},"source":"Whoa! That's a lot of False Positives, and a lot of True Negatives! Yes, it's a default values, but can we somehow improve somewhere else? Let's try to cross validate this."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.model_selection import cross_val_predict\n\ny_pred_cv = cross_val_predict(log_clf, X_test, Y_test, cv=5)","execution_count":21},{"cell_type":"code","outputs":[],"metadata":{},"source":"confusion_matrix(y_pred_cv, Y_test)","execution_count":22},{"cell_type":"code","outputs":[],"metadata":{},"source":"from sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(log_clf, X_train, Y_train, cv = 30)\nprint(cv_score)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (cv_score.mean(), cv_score.std() * 2))","execution_count":23},{"cell_type":"markdown","metadata":{},"source":"It's at best 95% accurate. Can Random Forest be better than this?\n\n### Random Forest Classifier"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier()\nrnd_clf.fit(X_train, Y_train)\nY_pred = rnd_clf.predict(X_test)","execution_count":24},{"cell_type":"code","outputs":[],"metadata":{},"source":"accuracy_score(Y_test, Y_pred)","execution_count":25},{"cell_type":"markdown","metadata":{},"source":"What! 100% accuracy? What does the confusion matrix look like?"},{"cell_type":"code","outputs":[],"metadata":{},"source":"confusion_matrix(Y_pred, Y_test)","execution_count":26},{"cell_type":"markdown","metadata":{},"source":"It really works well! All predictions are correct! We should still cross validate this, this time over the entire set."},{"cell_type":"code","outputs":[],"metadata":{},"source":"scores = cross_val_score(rnd_clf, X, Y, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":27},{"cell_type":"markdown","metadata":{},"source":"The Random Forest Classifier (RFC) is really good! Comparing it to the Logistic Classifier, it seems like an improvement. After all, it makes all the predictions correctly. The question is now whether RFC is overfitting; to ensure it is not, we can try to fine tune the model using 10-Fold Cross Validation and using Grid Search to get the best parameters over:\n\n1. n_estimators "},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"from sklearn.model_selection import GridSearchCV","execution_count":28},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"param_grid = [\n{'n_estimators': [3, 10, 30, 50, 100], 'max_features': [2, 4, 6, 8]},\n]\n\ngrid_search = GridSearchCV(rnd_clf, param_grid, cv=10, scoring='f1')","execution_count":29},{"cell_type":"code","outputs":[],"metadata":{},"source":"grid_search.fit(X, Y)","execution_count":30},{"cell_type":"code","outputs":[],"metadata":{},"source":"grid_search.best_params_","execution_count":31},{"cell_type":"markdown","metadata":{},"source":"Let's use these to find which of the Features are the important ones?\n\n## Feature Importance"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"feat_score=[]\nfor name, score in zip(X_train.columns, grid_search.best_estimator_.feature_importances_):\n    feat_score.append([name, score])","execution_count":32},{"cell_type":"code","outputs":[],"metadata":{},"source":"feat_score.sort(reverse=True, key= lambda x:x[1])\nfor char in feat_score:\n    print(char)","execution_count":33},{"cell_type":"code","outputs":[],"metadata":{},"source":"grid_search.best_estimator_.fit(X_train, Y_train)\ny_pred = grid_search.best_estimator_.predict(X_test)\nconfusion_matrix(y_pred, Y_test)","execution_count":34},{"cell_type":"markdown","metadata":{},"source":"I have seen that the f1 (Harmonic mean between the True Positives, False Positives, and False Negatives) is a bit unstable. By unstable, I mean that upon doing different iterations, I get different feature importances. One can try out different scoring systems. Maybe use a single decision tree?"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"param_grid = [\n{'n_estimators': [3, 10, 30, 50, 100], 'max_features': [2, 4, 6, 8]},\n]\n\ngrid_search = GridSearchCV(rnd_clf, param_grid, cv=10, scoring='roc_auc')\n#using ROC Area Under Curve to evaluate this time round","execution_count":35},{"cell_type":"code","outputs":[],"metadata":{},"source":"grid_search.fit(X, Y)\ngrid_search.best_params_","execution_count":36},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"feat_score=[]\nfor name, score in zip(X_train.columns, grid_search.best_estimator_.feature_importances_):\n    feat_score.append([name, score])","execution_count":37},{"cell_type":"code","outputs":[],"metadata":{},"source":"feat_score.sort(reverse=True, key= lambda x:x[1])\nfor char in feat_score:\n    print(char)","execution_count":38},{"cell_type":"markdown","metadata":{},"source":"## Some Important Features"},{"cell_type":"markdown","metadata":{},"source":"So, how to identify a wild mushroom? Three of the most important characteristics of mushrooms are their odor, gill colors, and gill size. Let's look at them in a bit more detail.\n\n### Odor"},{"cell_type":"code","outputs":[],"metadata":{},"source":"odor_labels = data['odor'].value_counts().axes[0]\nedible_o =[]\npoi_o = []\nN =0\nfor odor in odor_labels:\n    size = len(data[data['odor'] == odor].index)\n    edibles = len(data[(data['odor'] == odor) & (data['class'] == 'e')].index)\n    edible_o.append(edibles)\n    poi_o.append(size-edibles)\n    N=N+1\n\n#Plotting\nind = np.arange(N)\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nrects1 = ax.bar(ind, poi_o, width, color='r')\nrects2 = ax.bar(ind + width, edible_o, width, color='y')\n\n# Labels and Ticks along the axes.\nax.set_ylabel('Instances')\nax.set_title('Poisonous and Edible Mushrooms by their Odors')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(('None', 'Foul', 'Fishy', 'Spicy', 'Almond', 'Anise', 'Pungent', 'Creosote', 'Musty'))\n\nax.legend((rects1[0], rects2[0]), ('Poisonous', 'Edible'))\n\n\ndef autolabel(rects):\n    \n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","execution_count":39},{"cell_type":"markdown","metadata":{},"source":"Clearly, the odor of a mushroom is a good indicator of whether it will be edible or otherwise. If the mushroom gives of an odor that seems concerning, it probably is poisonous! As on can clearly see from the graph, that if the smells coming off aren't Almond-like or Anise-like, it probably is poisonous!"},{"cell_type":"markdown","metadata":{},"source":"### Gill-size\nGill size is the next feature of importance. I'll do pretty much the same thing."},{"cell_type":"code","outputs":[],"metadata":{},"source":"gillsize_labels = data['gill-size'].value_counts().axes[0]\n\nedible_o =[]\npoi_o = []\nN =0\nfor gs in gillsize_labels:\n    size = len(data[data['gill-size'] == gs].index)\n    edibles = len(data[(data['gill-size'] == gs) & (data['class'] == 'e')].index)\n    edible_o.append(edibles)\n    poi_o.append(size-edibles)\n    N=N+1\n    \n#Plotting\nind = np.arange(N)\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nrects1 = ax.bar(ind, poi_o, width, color='r')\nrects2 = ax.bar(ind + width, edible_o, width, color='y')\n\n# Labels and Ticks along the axes.\nplt.ylim(0,4500)\nax.set_ylabel('Instances')\nax.set_title('Poisonous and Edible Mushrooms by the size of their Gills')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(('Broad','Narrow'))\n\nax.legend((rects1[0], rects2[0]), ('Poisonous', 'Edible'))\n\n\ndef autolabel(rects):\n     #To plot the labels on top of the bars.\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","execution_count":40},{"cell_type":"markdown","metadata":{},"source":"Similarly, one can see that if Mushrooms have narrow gills, they might just be poisonous. That's not to say broad gilled mushrooms are all okay: there is a 30.15% chance of them being poisonous."},{"cell_type":"markdown","metadata":{},"source":"### Gill Color"},{"cell_type":"code","outputs":[],"metadata":{},"source":"gillcolor_labels = data['gill-color'].value_counts().axes[0]\nedible_o =[]\npoi_o = []\nN =0\nfor gc in gillcolor_labels:\n    size = len(data[data['gill-color'] == gc].index)\n    edibles = len(data[(data['gill-color'] == gc) & (data['class'] == 'e')].index)\n    edible_o.append(edibles)\n    poi_o.append(size-edibles)\n    N=N+1\n    \n#Plotting\nind = np.arange(N)\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nrects1 = ax.bar(ind, poi_o, width, color='r')\nrects2 = ax.bar(ind + width, edible_o, width, color='y')\n\n# Labels and Ticks along the axes.\nplt.ylim(0,2000)\nax.set_ylabel('Instances')\nax.set_title('Poisonous and Edible Mushrooms by the color of their Gills')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(('Buff', 'Pink', 'White', 'Brown', 'Gray', 'Chocolate', 'Purple', 'Black', 'Red', 'Yellow', 'Orange','Green'))\n\nax.legend((rects1[0], rects2[0]), ('Poisonous', 'Edible'))\n\n\ndef autolabel(rects):\n    #To plot the labels on top of the bars.\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","execution_count":41},{"cell_type":"markdown","metadata":{},"source":"So, based on these factors, I can tell you that you should stay away from *Foul smelling, Chocolate colored, Narrow gilled* Mushrooms. On the other hand, *Odorless, Brown colored, Broad gilled* mushrooms do quite well for cooking purposes. "},{"cell_type":"markdown","metadata":{},"source":"Thank you for reading! Feel free to critique me in the comments. I will try and update this later if possible."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"","execution_count":null}]}