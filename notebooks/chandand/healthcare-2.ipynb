{"cells":[{"metadata":{},"cell_type":"markdown","source":" >  **Public LB: 43.48**   \n \n>   **Private LB: 43.30**\n\nLink to LeaderBoard: (http://datahack.analyticsvidhya.com/contest/janatahack-healthcare-analytics-ii/#LeaderBoard)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Problem Statement**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Recent Covid-19 Pandemic has raised alarms over one of the most overlooked area to focus: Healthcare Management. While healthcare management has various use cases for using data science, patient length of stay is one critical parameter to observe and predict if one wants to improve the efficiency of the healthcare management in a hospital. \n\nThis parameter helps hospitals to identify patients of high LOS risk (patients who will stay longer) at the time of admission. Once identified, patients with high LOS risk can have their treatment plan optimized to miminize LOS and lower the chance of staff/visitor infection. Also, prior knowledge of LOS can aid in logistics such as room and bed allocation planning.\n\nSuppose you have been hired as Data Scientist of HealthMan â€“ a not for profit organization dedicated to manage the functioning of Hospitals in a professional and optimal manner.\nThe task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.\n\n**Eval Metric** : 100*accuracy score\n ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\nfrom keras.layers import Dense,Dropout\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import load_model\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path='../input/av-healthcare-analytics-ii/healthcare'\ntrain_df=pd.read_csv(os.path.join(path,'train_data.csv'))\ntest_df=pd.read_csv(os.path.join(path,'test_data.csv'))\nsubmission_df=pd.read_csv(os.path.join(path,'sample_sub.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Stay'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df['City_Code_Patient'].fillna(-99,inplace=True)\n# train_df['Bed Grade'].fillna(5,inplace=True)\n\ntrain_df=train_df.drop_duplicates(subset=[ele for ele in list(train_df.columns) if ele not in ['case_id']])\n#Adding more Features\ncombine_set=pd.concat([train_df,test_df],axis=0)\ncombine_set['City_Code_Patient'].fillna(-99,inplace=True)\ncombine_set['Bed Grade'].fillna(-99,inplace=True)\ncombine_set['Unique_Hospital_per_patient']=combine_set.groupby(['patientid'])['Hospital_code'].transform('nunique')\ncombine_set['Unique_patient_per_hospital']=combine_set.groupby(['Hospital_code'])['patientid'].transform('nunique')\ncombine_set['Unique_patient_per_Department']=combine_set.groupby(['Department'])['patientid'].transform('nunique')\ncombine_set['Unique_patient_per_Ward']=combine_set.groupby(['Ward_Type'])['patientid'].transform('nunique')\ncombine_set['Unique_Ward_per_patient']=combine_set.groupby(['patientid'])['Ward_Type'].transform('nunique')\ncombine_set['Unique_Hospital_per_ward']=combine_set.groupby(['Ward_Type'])['Hospital_code'].transform('nunique')\ncombine_set['Unique_Hospital_per_city']=combine_set.groupby(['City_Code_Hospital'])['Hospital_code'].transform('nunique')\ncombine_set['Unique_patients_per_city']=combine_set.groupby(['City_Code_Patient'])['patientid'].transform('nunique')\n\n#creating Aggregate columns\ncombine_set['Total_available_rooms_per_hospital_per_department']=combine_set.groupby(['Hospital_code','Department'])['Available Extra Rooms in Hospital'].transform('sum')\ncombine_set['Total_deposit_paid_by_patient_in_each_hospital']=combine_set.groupby(['Hospital_code','patientid'])['Admission_Deposit'].transform('sum')\ncombine_set['Total_number_visitors_per_patient']=combine_set.groupby(['patientid'])['Visitors with Patient'].transform('sum')\ncombine_set['Total_Amount_paid_per_Bed_grade_during_stay']=combine_set.groupby(['patientid','Bed Grade'])['Admission_Deposit'].transform('sum')\ncombine_set['Total_number_of_visitors_per_ward_during_stay']=combine_set.groupby(['patientid','Ward_Type'])['Visitors with Patient'].transform('sum')\ncombine_set['Number_of_times_patient_joined_with_same_reason']=combine_set.groupby(['patientid','Type of Admission','Severity of Illness'])['Hospital_code'].transform('count')\ncombine_set.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **OHE and Label Encoding**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding Categorical Columns\nle=LabelEncoder()\nfor col in combine_set.select_dtypes(include='object').columns:\n    if col not in ['Age','Stay']:\n#         fe=combine_set.groupby([col]).size()/len(combine_set)\n#         combine_set[col]=combine_set[col].apply(lambda x: fe[x])\n        df=pd.get_dummies(combine_set[col],drop_first=True)\n        combine_set=pd.concat([combine_set,df],axis=1).drop([col],axis=1)\n          \n    elif col!='Stay':\n        combine_set[col]=le.fit_transform(combine_set[col].astype(str))\n    else:\n        pass\n        \ncombine_set.head(5)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=combine_set[combine_set['Stay'].isnull()==False].drop(['case_id','Stay','patientid'],axis=1)\ny=le.fit_transform(combine_set[combine_set['Stay'].isnull()==False]['Stay'])\ny=pd.DataFrame(y,columns=['Stay'])\nX_main_test=combine_set[combine_set['Stay'].isnull()==True].drop(['case_id','Stay','patientid'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Standardizing all Features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat=to_categorical(y)\ny_hat=pd.DataFrame(y_hat)\nsc_X=StandardScaler()\nX=sc_X.fit_transform(X)\nX=pd.DataFrame(X)\nsc_X_main=StandardScaler()\nX_main_test=sc_X_main.fit_transform(X_main_test)\nX.head(5)\n\n# for col in X.select_dtypes(exclude='float64').columns:\n#     X[col]=X[col].astype(int)\n# y_total={}\n# for i in range(0,11):\n#     y_total[i+1]=y_hat[:, i:i+1]\n#     y_total[i+1]=pd.DataFrame(y_total[i+1],columns=[i+1])\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y_hat,test_size=0.2,random_state=294)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Sequential NN**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier=Sequential()\n\n# classifier.add(Dense(512,activation='relu', kernel_initializer='uniform',input_shape=(X_train.shape[1],)))\n# classifier.add(Dropout(0.2))\n# classifier.add(Dense(256,activation='relu',kernel_initializer='uniform'))\n# # classifier.add(Dense(200,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n# # classifier.add(Dense(64,activation='relu',kernel_initializer='uniform'))\n# # classifier.add(Dense(32,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(11,activation='softmax'))\n\n# classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n# callback_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.3,min_lr=0.00001)\n# callback_mc=ModelCheckpoint(filepath='model_repli.hdf5',monitor='val_accuracy',save_best_only=True,mode='max')\n\n# classifier.fit(X_train,y_train,epochs=50,batch_size=32,validation_data=(X_val,y_val),callbacks=[callback_lr,callback_mc])\n\n# classifier=load_model('model_repli.hdf5')\n# pred_val=classifier.predict(X_val)\n\n# preds=classifier.predict(X_main_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier=Sequential()\n\n# classifier.add(Dense(512,activation='relu', kernel_initializer='uniform',input_shape=(X_train.shape[1],)))\n# classifier.add(Dropout(0.2))\n# classifier.add(Dense(256,activation='relu',kernel_initializer='uniform'))\n# # classifier.add(Dense(200,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n# # classifier.add(Dense(32,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(11,activation='softmax'))\n\n# classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n# callback_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.5,min_lr=0.00001)\n# callback_mc=ModelCheckpoint(filepath='model_reli2.hdf5',monitor='val_accuracy',save_best_only=True,mode='max')\n\n# classifier.fit(X_train,y_train,epochs=50,batch_size=32,validation_data=(X_val,y_val),callbacks=[callback_lr,callback_mc])\n\n# classifier=load_model('model_reli2.hdf5')\n# preds2_val=classifier.pedict(X_val)\n\n# preds2=classifier.predict(X_main_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Stacking**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntotal_val_preds=pd.concat([pd.DataFrame(pred_val,columns=[col for col in range(0,11)]),pd.DataFrame(preds2_val,columns=[col for col in range(11,22)])],axis=1)\ntotal_test_preds=pd.concat([pd.DataFrame(preds,columns=[col for col in range(0,11)]),pd.DataFrame(preds2,columns=[col for col in range(11,22)])],axis=1)\n\n\nclassifier=Sequential()\n\nclassifier.add(Dense(128,activation='relu', kernel_initializer='uniform',input_shape=(total_val_preds.shape[1],)))\nclassifier.add(Dropout(0.1))\nclassifier.add(Dense(64,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(200,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n# classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\nclassifier.add(Dense(64,activation='relu',kernel_initializer='uniform'))\nclassifier.add(Dense(11,activation='softmax'))\n\nclassifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\ncallback_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.2,min_lr=0.00001)\ncallback_mc=ModelCheckpoint(filepath='model_test.hdf5',monitor='accuracy',save_best_only=True,mode='max')\n\nclassifier.fit(total_val_preds,y_val,epochs=50,batch_size=16,callbacks=[callback_lr,callback_mc])\n\nclassifier=load_model('model_test.hdf5')\n\nfinal_preds=classifier.predict(total_test_preds)\n\nfinal_preds=pd.DataFrame(final_preds).idxmax(axis=1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Permutation Importance**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nperm = PermutationImportance(lg,random_state=294).fit(X_val, y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **KFOLD (LGBM)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# class_weight=compute_class_weight('balanced',np.unique(y['Stay']), y['Stay'])\n# class_weight=dict(zip(np.unique(y['Stay']),class_weight))\n\nkf=KFold(n_splits=10,shuffle=True,random_state=2020)\n# sc_X=StandardScaler()\n# X=pd.DataFrame(sc_X.fit_transform(X))\npreds={}\nacc_score=0\n\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X)):    \n\n    X_train, y_train = X.iloc[train_idx,:], y_hat.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y_hat.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n    #12,0.8,1000\n    lg=LGBMClassifier(boosting_type='gbdt',learning_rate=0.08,depth=12,objective='multiclass',n_estimators=1000,num_class=11,\n                     metric='multi_error',colsample_bytree=0.5,reg_alpha=2,reg_lambda=2,random_state=294,n_jobs=-1)\n\n#     X_train,y_train=SMOTETomek(random_state=294).fit_resample(X_train,y_train)\n    lg.fit(X_train,y_train)\n\n    print(accuracy_score(y_val,lg.predict(X_val)))\n\n    acc_score+=accuracy_score(y_val,lg.predict(X_val))\n    \n    preds[i+1]=lg.predict(X_main_test)\n#     classifier=Sequential()\n\n#     classifier.add(Dense(512,activation='relu', kernel_initializer='uniform',input_shape=(X_train.shape[1],)))\n#     classifier.add(Dropout(0.2))\n#     classifier.add(Dense(256,activation='relu',kernel_initializer='uniform'))\n#     # classifier.add(Dense(200,activation='relu',kernel_initializer='uniform'))\n#     classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n#     # classifier.add(Dense(64,activation='relu',kernel_initializer='uniform'))\n#     # classifier.add(Dense(32,activation='relu',kernel_initializer='uniform'))\n#     classifier.add(Dense(11,activation='softmax'))\n\n#     classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n#     callback_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.2,min_lr=0.00001)\n#     callback_mc=ModelCheckpoint(filepath='model_'+str(i+1)+'.hdf5',monitor='val_accuracy',save_best_only=True,mode='max')\n\n#     classifier.fit(X_train,y_train,epochs=30,batch_size=32,validation_data=(X_val,y_val),callbacks=[callback_lr,callback_mc])\n    \n#     classifier=load_model('model_'+str(i+1)+'.hdf5')\n\n#     preds+=classifier.predict(X_main_test)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **KFOLD (NN)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kf=KFold(n_splits=10,shuffle=True,random_state=2019)\n# sc_X=StandardScaler()\n# X=pd.DataFrame(sc_X.fit_transform(X))\npreds=0\nacc_score=0\n\n\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X)):    \n\n    X_train, y_train = X.iloc[train_idx,:], y_hat.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y_hat.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n    #12,0.8,1000\n#     lg=LGBMClassifier(boosting_type='gbdt',learning_rate=0.08,depth=12,objective='multiclass',n_estimators=1000,num_class=11,\n#                      metric='multi_error',colsample_bytree=0.5,reg_alpha=2,reg_lambda=2,random_state=294,n_jobs=-1)\n\n# #     X_train,y_train=SMOTETomek(random_state=294).fit_resample(X_train,y_train)\n#     lg.fit(X_train,y_train)\n\n#     print(accuracy_score(y_val,lg.predict(X_val)))\n\n#     acc_score+=accuracy_score(y_val,lg.predict(X_val))\n    classifier=Sequential()\n\n    classifier.add(Dense(512,activation='relu', kernel_initializer='uniform',input_shape=(X_train.shape[1],)))\n    classifier.add(Dropout(0.1))\n#     classifier.add(Dense(256,activation='relu',kernel_initializer='uniform'))\n    classifier.add(Dense(200,activation='relu',kernel_initializer='uniform'))\n    classifier.add(Dropout(0.05))\n    classifier.add(Dense(128,activation='relu',kernel_initializer='uniform'))\n    classifier.add(Dense(64,activation='relu',kernel_initializer='uniform'))\n#     classifier.add(Dense(32,activation='relu',kernel_initializer='uniform'))\n    classifier.add(Dense(11,activation='softmax'))\n\n    classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n    callback_lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.5,min_lr=0.00001)\n    callback_mc=ModelCheckpoint(filepath='model_'+str(i+1)+'.hdf5',monitor='val_accuracy',save_best_only=True,mode='max')\n\n    classifier.fit(X_train,y_train,epochs=30,batch_size=32,validation_data=(X_val,y_val),callbacks=[callback_lr,callback_mc])\n    \n    classifier=load_model('model_'+str(i+1)+'.hdf5')\n\n    preds+=classifier.predict(X_main_test)\n  \n    \npreds=preds/10\npreds=pd.DataFrame(preds).idxmax(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Applying Mode on Output Predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# d = pd.DataFrame()\n# for i in range(1, 11):\n#     d = pd.concat([d,pd.DataFrame(preds[i])],axis=1)\n# d.columns=['1','2','3','4','5','6','7','8','9','10']\n# re = d.mode(axis=1)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Submission File**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['Stay']=le.inverse_transform(preds.astype(int))\nsubmission_df.to_csv('/kaggle/working/main_test.csv',index=False)\nsubmission_df.head(5)\n# le.inverse_transform(re.astype(int))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}