{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\npd.options.display.max_columns = None\n%config Completer.use_jedi = False\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/carinsurance/carInsurance_train.csv')\ntest_data = pd.read_csv('../input/carinsurance/carInsurance_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(train_data, columns):\n    dummies = pd.get_dummies(train_data[columns], prefix = columns)\n    train_data = pd.concat([train_data, dummies], axis =1)\n    train_data = train_data.drop(columns, axis=1)\n    \n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_inputs(train_data):\n    \n    train_data = train_data.copy()\n    \n    # Days passed have more than 3000 values as -1 replacing it with nan to drop.\n    \n    train_data['DaysPassed'] = train_data['DaysPassed'].replace(-1, np.nan)\n    \n    # Droping unwanted features\n    \n    train_data.drop('Id', axis=1, inplace=True)\n    \n    null_columns = train_data.loc[:, train_data.isna().mean() > 0.25].columns\n    train_data.drop(null_columns, axis =1, inplace=True)\n            \n    \n    # maping and treating the null values\n    \n    train_data['Communication'] = train_data.Communication.map({'telephone' : 0, 'cellular' : 1})\n    train_data['Communication'] = train_data['Communication'].fillna(train_data['Communication'].median())\n    train_data['Communication'] = train_data['Communication'].astype('int')\n    \n    train_data['Education'] =train_data.Education.map({'tertiary' : 3, 'primary' : 1 , 'secondary' : 2,})\n    train_data['Education'] = train_data['Education'].fillna(train_data['Education'].median())\n    train_data['Education'] = train_data['Education'].astype('int')\n    \n    # To drop only 19 missing values from jobs\n    \n    train_data=train_data.dropna()\n    \n    # mapping categorical columns\n    \n    train_data['Marital'] = train_data.Marital.map({'single' : 1, 'married' : 2, 'divorced': 3})\n    train_data['LastContactMonth'] = train_data.LastContactMonth.map({'jan' :1, 'may' :5, 'jun' : 6, 'mar' : 3, 'nov' :11, 'jul' : 7, 'aug' :8, 'sep':9, 'apr'  :4,\n       'feb' : 2, 'oct' :10, 'dec' :12})\n    \n    # encode jobs feature\n    \n    train_data = onehot_encode(train_data, 'Job')\n    \n    \n    # Duratin column\n\n    train_data['callduration'] = (pd.to_datetime(train_data['CallEnd']) - pd.to_datetime(train_data['CallStart'])).apply(lambda x : x.seconds)\n    \n    train_data.drop(['CallEnd', 'CallStart'], axis=1, inplace=True)\n    \n    return train_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = preprocess_inputs(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling and Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data.drop('CarInsurance',axis =1)\ny = train_data['CarInsurance']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y, train_size = 0.7, random_state =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc.fit(X_train)\n\nX_train = pd.DataFrame(sc.transform(X_train), columns =X.columns)\nX_test = pd.DataFrame(sc.transform(X_test), columns =X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'Knn' : KNeighborsClassifier(),\n    'lr' : LogisticRegression(),\n    'LSVC': LinearSVC(), \n    'svc' : SVC(),\n    'GB' : GradientBoostingClassifier()\n    \n}\n\nfor name,model in models.items():\n    model.fit(X_train,y_train)\n    print(name + ' trained')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models.items():\n    print(name + 'Acuuracy : {:.2f}%'. format(model.score(X_test,y_test)* 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}