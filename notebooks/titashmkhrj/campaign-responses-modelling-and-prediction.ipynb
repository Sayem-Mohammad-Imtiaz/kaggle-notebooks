{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Description.\n\n<b>Problem Statement :</b>\nA response model can provide a significant boost to the efficiency of a marketing campaign by increasing responses or reducing expenses. The objective is to predict who will respond to an offer for a product or service.\n\n<b>Objective :</b>\n* We are required to model the data relating to the various customer attributes and their response towards the marketting campaigns, and report the key-drivers of those responses.\n* The predictive model will be used to predict the response of a new customer towards a marketting campaign, in order to do targetted marketting which would lead to a better response-ratio for the compaigns and hence will cut down unnecessary costs.  "},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries\nOkay so prima facie, lets import some of the libraries we will be needing for this project.\n(even if we are missing out on some library, we can always import it later on in the project.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\n# library to handle vectorized data \nimport numpy as np \n# library for data analsysis and manupulation\nimport pandas as pd \n# so that the output is not trunacated by pandas when we actually want to see it \n# pd.set_option('display.max_columns', 100)\n# pd.set_option('display.max_rows', 1000)\n\n# for visualisations\nimport seaborn as sns\n\n%matplotlib inline \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/arketing-campaign/marketing_campaign.csv', sep = ';')\nprint('The dimension of our data is :',data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's have a look at the features and the corresponding data types of those features\\n\",\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis and data cleaning."},{"metadata":{},"cell_type":"markdown","source":"### Feature : ' ID '"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so we can see that the feature ID represents the customer IDs of the various customers over a period of time, but the data does not reflect a consecutive collection of data from all the customers, which is why the length of our data is no inline with the range of the customer IDs in this feature.  \nDue to the fact mentoined above, the feature is distorting the data a bit, so we will be dropping the feature from our data as it is not adding any value to our data and hence not required for our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('ID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature : ' Year_Birth '"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Year_Birth'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so from above we can observe the the feature 'Year_Birth' represents the different years of birth of the customers. The data collected contains some gaps in this feature; in terms of particular years in which customers were born. So our data does not contain all the years starting from 1893 to 1996."},{"metadata":{},"cell_type":"markdown","source":"### Feature : ' Education '"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Education'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are  classes in this feature, but there are 2 among them which mean the same but are represented in two different ways; i.e. '2n Cycle' and 'Master'. So we will replace all occurances of he class'2n Cycle' with 'Master', for a better value representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Education'] = data[\"Education\"].replace('2n Cycle', \"Master\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature : ' Marital_Status '"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Marital_Status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"okay so this feature has 8 classes inour data, for the purpose of a better value representation in this feature, we will be doing the following transformations to some of the classes :\n* 'Together' > replaced by 'live_in',\n* 'Alone', 'YOLO' and 'Absurd' > replaced by 'single' ."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Marital_Status'] = data['Marital_Status'].replace('Together', 'Live_in')\ndata['Marital_Status'] = data['Marital_Status'].replace(['YOLO', 'Alone', 'Absurd'], 'Single')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature : ' Dt_Customer '"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Dt_Customer'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature Dt_Customer represents dates of customerâ€™s enrolment with the company. The data in this feature is represented in type str. In order to improve the value representation of this feture, and to do meaningful feature engineering with this feature, we will be converting the values in this feaure to data type datetime."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'], format=\"%Y-%m-%d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Dt_Customer'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the data type has been converted into datetime64[ns], without effecting the data itself."},{"metadata":{},"cell_type":"markdown","source":"# Descriptive analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from above the numerical features are on very different scales with respect to each other, this tells us that we will need to scale the numerical features in the future before using for modelling the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the correlation between the features.\ncorr_matrix = data.corr()\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from above that a lot of values in the heat map representing the correlations between the features are null values (excluding the diagonal values); this is due to the presence of imbalanced data in some of the features in our data which basically leads to 0 variance, resulting in a null value for correlation with those features.  \nThe features with imbalanced features wll be handled in the later stage of the modelling."},{"metadata":{},"cell_type":"markdown","source":"# Handling missing-values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check for missing values in our data\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's visualize the fragmentation of the data feature-wise due to the presence of the missing values.\nimport missingno as msno\nmsno.matrix(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data is fairly oh high quality due to the fact that very less number of missing values are present in the data.  \nGiven that, we do see 24 missing values in the feature 'Income'. The best ways to impute the missing values in this feature, is strategies which utilise relative imputation strategies.  \nFor exampe we can use the mode of the feature for imputation purposes of the missing values in this feature if we assume that the data is a high degree representation of the bigger population, which means that any new data point will be in line with the distribution of the data we have.  \nOtherwise, we can use the **sklearn.impute.KNNImputer** for imputation purposes of the missing values in this feature, which utilises the K-nearest-neighbors algorithm to figure out the value with highest probality, and uses it for imputation.  \nBecause of the fact we cannot be sure of how representative our data is of the larger population, we will be using KNNImputer in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the range of the feature Income\ndata['Income'].max() - data['Income'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\ndata['Income'] = KNNImputer(n_neighbors=4).fit_transform(data['Income'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Income'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers detection and removal."},{"metadata":{},"cell_type":"markdown","source":"Removing the following features :\n* categorical feature with only one class\n* feature with only a singular value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# filtering the features with only one class or singular value\nfor col in data.columns :\n    if len(data[col].value_counts()) == 1 :\n        print(data[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are two features in our dataset have constant values, hence dropping them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the features with binary categorical classes to check for imbalanced data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# filtering the features with binary classes to check for presence of imbalanced data \nfor col in data.columns :\n    if len(data[col].value_counts()) == 2 :\n        print(data[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from above, the feature 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Complain', and 'Response', are binary categorical features with imbalanced data.  \nIn order to prevent false outlier detection of the minority classes in the above mentioned features, we will be excluding them from the outlier detection and removal step.\nAnd in order to prevent majority class prediction by the classification algorithms, we will be handling the above features to balance the frequency of the binary classes later in the process."},{"metadata":{},"cell_type":"markdown","source":" Mean and standard deviation of the features, before removing outliers, for aiding the analysis of the presence and absence of outliers in our data."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"outliers_effect = pd.DataFrame(data.describe().iloc[1:3, :]).rename(index={'mean':'initial_mean', 'std':'initial_SD'})\noutliers_effect = outliers_effect.T\noutliers_effect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outlier detection using Local Outlier Factor."},{"metadata":{"trusted":true},"cell_type":"code","source":"# filtering the features to apply outlier detection and removal step. \ncol_to_exclude = []\n'''\nFiltering the features with binary classes to exclude from outlier detection and removal step.\nThe reason we can use a simple filter like this is,\ndue to the fact that we know this filter screens out all the desired features,\nas already demonstrated above.\n'''\nfor col in data.columns :\n    if len(data[col].value_counts()) == 2 :    \n        col_to_exclude.append(col)\ncol_to_include = (data.select_dtypes(include=['float64', 'int64'])).drop(col_to_exclude, axis=1).columns\ncol_to_include","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's figure out the optimum number of neghbors for the LocalOutlierFactor, for our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n# we are taking a range of odd numbers, because of the use of 'VOTING' n the algorithm.\nneighbors_LOF = np.arange(1, 40, 2)\nnum_outliers = []\nfor n in neighbors_LOF :\n    outlier_detector = LocalOutlierFactor(n_neighbors = n)\n    outliers = pd.Series(outlier_detector.fit_predict(data[col_to_include]))\n    num_outliers.append(outliers.value_counts()[-1])\n    \nsns.barplot(x = neighbors_LOF, y = num_outliers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from above that, the number of outliers detected by the LOF algorithm decreases as we increase the number of neighbor, which is inline with our expectations.  \nFrom above we can clearly see that, the 21 nearest neighbor is the optimum numner of neighbors to detect outliers in this dataset, because number of nearest neighbors more than 21 is not bringing ay significant resuts.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_outlier_detector = LocalOutlierFactor(n_neighbors = 21)\noutliers = pd.Series(final_outlier_detector.fit_predict(data[col_to_include]))\nprint('The number of outliers detected by LOF are > ',outliers.value_counts()[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_indices = [ x for x in range(len(outliers)) if outliers[x] == -1]\ndata.drop(data.index[outliers_indices], inplace = True)\nprint('The dimension of the data after removing outliers is', data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the effect of removing the outliers from our data in terms of the mean and SD of the features before and after removal of the outliers."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"outliers_effect[\"mean_after_outlier_removal\"] = data.describe().iloc[1,:]\noutliers_effect[\"SD_after_outlier_removal\"] = data.describe().iloc[2,:]\noutliers_effect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering."},{"metadata":{},"cell_type":"markdown","source":"Creating the feature 'Customer_age' from the feature 'Year_birth'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating age as it is in the year 2020\ndata['Customer_age'] = 2020 - data['Year_Birth']\n# dropping the 'year_Birth' feature from the data as it is now redundant\ndata.drop('Year_Birth', axis=1, inplace=True)\ndata['Customer_age'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further exploring the feature 'Customer_age'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Customer_age'].value_counts().index.sort_values(ascending = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so we can see that age of the customers in our dataset is ranging from 25 years to 80 years old. That gives us a range of 55 years of age gap between our youngest and oldest targeted customer.  \nIn order to improve the signal-noise ratio in our data, we will be further discretizing this feature into 11 bins, where each bin would represent a range of 5 years. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Customer_age'] = pd.cut(data['Customer_age'], bins=11, labels=False, include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature 'Kidhome' represents the number of small children in customerâ€™s household.  \nAnd the feature 'Teenhome' represents the number of teenagers in customerâ€™s household.  \nCreating the feature 'n_kids' from the above two features, and transformig the above two feature into percentage of the new feature 'n_kids'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['n_kids'] = data['Kidhome'] + data['Teenhome'] \ndata['Kidhome'] = (data['Kidhome']/data['n_kids'])*100\ndata['Teenhome'] = (data['Teenhome']/data['n_kids'])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['n_kids', 'Kidhome', 'Teenhome']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are null-values in the transformed features 'Kidhome' and Teenhome', this is due to the fact that there were 0 in that rows of that feature, whch is why after the transformation, it is showing as NaN.  \nHence we will be filling the NaN values in the feature 'Kidhome' and 'Teenhome' with 0,  which would also be inline with the true data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Kidhome'].fillna(0, inplace=True)\ndata['Teenhome'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming the feature 'Kidhome' and 'Teenhome'\ndata = data.rename(columns= {'Kidhome':'percent_kids', 'Teenhome':'percent_teenagers'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['percent_kids'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['percent_teenagers'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rounding up the values in percent_kids and percent_teenagers to 0 decimals.\ndata['percent_kids'] = data['percent_kids'].apply(lambda x: round(x, 0))\ndata['percent_teenagers'] = data['percent_teenagers'].apply(lambda x: round(x, 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature 'Dt_custome' represents the date of customerâ€™s enrolment with the company.  \nCreating new feature 'Days_with_company' representing the number of days the customer has been associated with the company, calculated from the customer's registration date with the company."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nfrom datetime import datetime, date\n\nfor i in range(0, len(data)):\n    data['Days_with_company'] = datetime.today().date()-data['Dt_Customer'].dt.date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['Days_with_company'].dtype)\ndata['Days_with_company'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in the value representation of the feature 'Days_with_company', the values have been represented in the data type timedelta64[ns], which is why we will be converting the values in the feature to integers, as doing so will not result in any kind of information loss, and in turn it will be a better value representation for the purposes of predictive modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Days_with_company'] = data['Days_with_company'].apply(lambda x: int(x/np.timedelta64(1, 'D')))\ndata['Days_with_company'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dopping the feature 'Dt_Customer' as it is not essential for our analysis\ndata.drop('Dt_Customer', axis=1, inplace=True)\ndata['Days_with_company'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further exploring the new feature 'Days_with_company'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Days_with_company'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the number of days the customers are associted with the company ranges from 2167 days to 2864 days, that is a range of 697 days.  \nIn order to improve the signal-noise ratio in the data we can further discreticize the feature based on the 4 quartiles.  \nThis would in turn also maintain balanced classes in the feature.  \nEach bin representing the following : \n* bin 0 > legacy customer\n* bin 1 > old customer\n* bin 2 > new customer\n* bin 3 > current customer"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Days_with_company'] = pd.qcut(data['Days_with_company'], q=4, labels=False, precision=0)\ndata['Days_with_company'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature 'Recency' represents the number of days since the last purchase, i.e. in other terms it shows the last activity in terms of a purchase.  \nCreating new feature 'last_purchase_day_type', which will represent the type of day of the last purchase, i.e. 'Weekday' or 'Weekend'."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\n\ndays_list = []\nfor val in data['Recency'].values :\n     days_list.append((datetime.today().date() - timedelta(days=int(val))).strftime('%A'))\ndata['last_purchase_day_type'] = days_list\ndata['last_purchase_day_type'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['last_purchase_day_type'] = data['last_purchase_day_type'].apply(lambda x: 'Weekend' if x in ['Saturday', 'Sunday'] else 'Weekday')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['last_purchase_day_type'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'; represent the total amount spent on wine, fruits, meat, fish, sweet, gold products in the last 2 years respectively.  \nCreating new feature 'Total_amnt_spent', representing the total amount spent by a customer in the last 2 years.  \nAnd transforming the above mention original features to represent the percentage of money spent on those products with respect to the total amount spent, in the last 2 years.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Tot_amnt_spent'] = (data['MntWines']+data['MntFruits']+data['MntMeatProducts']+data['MntFishProducts']+data['MntSweetProducts']+data['MntGoldProds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MntWines'] = round((data['MntWines']/data['Tot_amnt_spent'])*100, 2)\ndata['MntFruits'] = round((data['MntFruits']/data['Tot_amnt_spent'])*100, 2)\ndata['MntMeatProducts'] = round((data['MntMeatProducts']/data['Tot_amnt_spent'])*100, 2)\ndata['MntFishProducts'] = round((data['MntFishProducts']/data['Tot_amnt_spent'])*100, 2)\ndata['MntSweetProducts'] = round((data['MntSweetProducts']/data['Tot_amnt_spent'])*100, 2)\ndata['MntGoldProds'] = round((data['MntGoldProds']/data['Tot_amnt_spent'])*100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Income'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from above that monthly income of the customers in our dataset range from 3502 (minimum) to 162397 (maximum).  \nIn order to improve the signal-noise ratio of our data we will be discretizing the feature into 5 bins according to the quantiles of the feature, where each bin represents the following :  \n* bin 0 > represents low income\n* bin 1 > represents below average income\n* bin 2 > represents average income\n* bin 3 > represents above average income\n* bin 4 > represents high income\n\nUsing the quantiles to discretize the feature would also make sure that the discrete bins are of fairly same count, hence maintaining a balanced feruency of the discrete classes in the feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Income'] = pd.qcut(data['Income'], q=5, labels=False, precision=0)\ndata['Income'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data analysis utilising visualizations."},{"metadata":{},"cell_type":"markdown","source":"Visualization of Education and Income with Response of the customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='Income', col='Education', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : \n* customers who have a education level of 'Graduation' show the highest rejection levels to the last campaign, where the effect of 'Income' was insignificant towards the kind of response.\n* customers who have a educational background of 'PhD' and 'Master', show approximately similar levels of rejection towards the last campaign. \n* Among the customers having a 'PhD', we can see the lowest levels of rejection response towards the last campaign is being showed by the customers having a low income, and the level of rejection responses gradually grows as the income levels increase peaking at the customers with above average income and then a slight decrease of rejection levels by customers with high income.\n* The level of the customer's income is insignificant towards their levels of a rejection response, for the customers having a 'Master' in education.\n* Fr the customers with a 'Basic' educational background we can see tht the rejection levels are reletively lower than for customers with other educational backgrounds; but within these class of customers, the ones having a low level of income show significantly higher level of a rejection response towards the last campaign."},{"metadata":{},"cell_type":"markdown","source":"Visualizatoin of Marital_Status and n_kids with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='n_kids', col='Marital_Status', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observtions : \n* We can clearly see that a majority of the rejection responses towards the last campaign is from customers who are married, followed by customers who are in a live-in relationship and who are single.\n* We can also see a clear pattern, that customers having 1 kid irrespective of their maritl status, show the relatively highest rejection responses across the population."},{"metadata":{},"cell_type":"markdown","source":"Visualization of Customer_age, Days_with_company and Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='Customer_age', col='Days_with_company', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations :\n* We can see a similar level of  relatively higher rejection responses from customers who are legecy and old customers.\n* We can also see a similar level of relatively lower but independently higher, rejection levels from new and current customers.\n* A common pttern among all kinds of customers is that the highest levels of rejection responses are shown by the cutomers in the age bin of 3 to 7.\n"},{"metadata":{},"cell_type":"markdown","source":"Visualization of percent_kids, percent_teenager with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='percent_kids', col='percent_teenagers', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : \n* We can see high levels of rejection responses from customers :\n    * having 0 teenagers or kids\n    * having only kids and o teenagers\n    * having equally kids and teenagers\n    * having only teenagers\n* On the contrary, customers with 70% kids and 30% teenagers are showing lower rejection responses towards the last campaign.\n* To summarize the above observations, customers with all kinds of combinations of kids and teenagers (except 30-70 ratio) in their family show similarly high rejection responses."},{"metadata":{},"cell_type":"markdown","source":"Visualization of NumDealsPurchases, NumWebPurchases with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='NumDealsPurchases', col='NumWebPurchases', data=data, kind='count', legend_out = True, col_wrap=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that there is a high level of rejection responses from customers belonging to the type : \n* customers who did 1 to 4 purchases from the company website\n    * and customers who did 1 to 4 purchases with discount deals."},{"metadata":{},"cell_type":"markdown","source":"Visualization of NumCatalogPurchases, NumStorePurchases with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='NumCatalogPurchases', col='NumStorePurchases', data=data, kind='count', legend_out=True, col_wrap=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from above that there is a high level of rejection responses coming from customers who did 2 to 4 purchases directly from the stores and among them the customers who did 1 to 3 purchases from the catalog, are showing relatively higher rejection responses."},{"metadata":{},"cell_type":"markdown","source":"Visualization of NumWebVisitsMonth, last_purchase_day_type with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='NumWebVisitsMonth', col='last_purchase_day_type', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see fromabove tat the customers who were visiting the website from 1 to 8 times a month on a weekday, are showing the relatively highest rejection responses; and the same apples for weekends but the magnitude of the rejection responses are relatively lower than on the weendays."},{"metadata":{},"cell_type":"markdown","source":"Visualization of AcceptedCmp1, Complain with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='AcceptedCmp1', col='Complain', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that, customers who do not complain are showing a higher level of rejection responses."},{"metadata":{},"cell_type":"markdown","source":"Visualization of AcceptedCmp2, Complain with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='AcceptedCmp2', col='Complain', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that, customers who do not complain are showing a higher level of rejection responses."},{"metadata":{},"cell_type":"markdown","source":"Visualization of AcceptedCmp3, Complain with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='AcceptedCmp3', col='Complain', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that, customers who do not complain are showing a higher level of rejection responses."},{"metadata":{},"cell_type":"markdown","source":"Visualization of AcceptedCmp4, Complain with Response."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='AcceptedCmp4', col='Complain', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that, customers who do not complain are showing a higher level of rejection responses, but they ae also showing a relatively lower but globally higher levels of acceptance responses to the campaign."},{"metadata":{},"cell_type":"markdown","source":"Visualization of AcceptedCmp5, Complain with Response."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.catplot(x='Response', hue='AcceptedCmp5', col='Complain', data=data, kind='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from above that, customers who do not complain are showing a higher level of rejection responses, but they ae also showing a relatively lower but globally higher levels of acceptance responses to the campaign."},{"metadata":{},"cell_type":"markdown","source":"# Feature encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoding the nominal categorical feature 'last_purchase_day_type'\nenc_feature_df = pd.get_dummies(data['last_purchase_day_type'],prefix='last_purchase_day_type', prefix_sep='_')\ndata = pd.concat([enc_feature_df,data], axis=1)\ndata.drop('last_purchase_day_type', axis=1, inplace=True)\n\n# label encoding\n# importing required libraies\nfrom sklearn.preprocessing import LabelEncoder\ndata['Education'] = LabelEncoder().fit_transform(data['Education'].values.reshape(-1,1))\ndata['Marital_Status'] = LabelEncoder().fit_transform(data['Marital_Status'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the feature and target space."},{"metadata":{"trusted":true},"cell_type":"code","source":"# making our x and y data\nx_data = data.drop('Response', axis=1)\ny_data = data['Response']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the required libraries, for the purposes of modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"import imblearn \nfrom imblearn.over_sampling import SMOTENC\nfrom imblearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import (LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import (RandomizedSearchCV, train_test_split, cross_val_score)\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Finished importing the libraries.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models objects and their parameter grid."},{"metadata":{"trusted":true},"cell_type":"code","source":"# models as per the sequence in the parameter grid \nmodel_objects = [LogisticRegression(),\n                 LogisticRegression(),\n\t\t\t\t\t\t\t\t LogisticRegression(),\n\t\t\t\t\t\t\t\t PassiveAggressiveClassifier(),\n\t\t\t\t\t\t\t\t RidgeClassifier(),\n\t\t\t\t\t\t\t\t KNeighborsClassifier(),\n\t\t\t\t\t\t\t\t SVC(),\n\t\t\t\t\t\t\t\t DecisionTreeClassifier(),\n\t\t\t\t\t\t\t\t RandomForestClassifier()]\n\n\n\n# hyper-parameter dictionary for the tunningof the models\nparameter_grid = {'LR_l1' : {'model__penalty' : ['l1'],\n                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n                              'model__random_state' : [42],\n                              'model__solver' : ['liblinear', 'saga'],\n                              'model__max_iter' : [100000]\n                          },\n\t\t\t\t\n                  'LR_l2' : {'model__penalty' : ['l2'],\n                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n                              'model__random_state' : [42],\n                              'model__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n                              'model__max_iter' : [100000]\n                          },\n\n                  'LR_ElNet' : {'model__penalty' : ['elasticnet'],\n                                'model__l1_ratio' : [0.3, 0.5, 0.7],\n                                'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n                                'model__random_state' : [42],\n                                'model__solver' : ['saga'],\n                                'model__max_iter' : [100000]\n                              },\n\n                  'Pass_Agg_clif' : {'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n                                      'model__random_state' : [42],\n                                      'model__loss' : ['hinge', 'squared_hinge'],\n                                      'model__class_weight' : ['balanced', None]\n                                  },\n                  \n                  'Ridge_clif' : {'model__alpha' : [500.0, 50.0, 5.0, 0.5, 0.05, 0.005],\n                                  'model__fit_intercept' : ['True', 'False'],\n                                  'model__normalize' : ['True', 'False'],\n                                  'model__class_weight' : ['balanced', None],\n                                  'model__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg']\n                              },\n                  \n                  'KN_classif' : {'model__n_neighbors' : [1,3,5,7,9],\n                                  'model__p' : [1,2,5]                     \n                              },\n                  \n                  'SVC' : {'model__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                           'model__gamma' : ['scale', 'auto'],                     \n                      },\n                  \n                  'DT_clif' : {'model__criterion': ['gini','entropy'],\n                                'model__max_features': ['sqrt','log2',None],\n                                'model__min_samples_leaf': [1,2,5,10],\n                                'model__min_samples_split' : [2,5,10,15,100],\n                                'model__max_depth': [5,8,15,25,30,None]\n                          },\n                  \n                  'RF_clif' : {'model__n_estimators' : [120,300,500,800,1200],\n                               'model__max_features': ['sqrt','log2',None],\n                                'model__min_samples_leaf': [1,2,5,10],\n                                'model__min_samples_split' : [2,5,10,15,100],\n                                'model__max_depth': [5,8,15,25,30,None]                      \n                          }\n              }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data for the purpose of hyper-parameter optimisation and model selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 42)\nx_optimization, x_validation, y_optimization, y_validation = train_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n\nprint(\"Finished splitting the data.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper-parameter optimization."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# initiating an empty list for storing the optimized models\nhyper_parameter_optimized_models = []\n\n\n'''\nresampling our optimization datasets, in order to prevent overfitting of our models on the majority class of the target feature in our\nfor the purpose above stated we will be using SMOTENC, which requires us to give the column indices of the categrical features\n'''\nnum_features = ['Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'Tot_amnt_spent']\ncatg_features = x_train.drop(num_features, axis=1).columns.tolist()\ncatg_idx_list = []\nfor feature in catg_features:\n  catg_idx_list.append(x_data.columns.get_loc(feature))\n\n# making the resampling and standardising objects\nover_sampler = SMOTENC(categorical_features = catg_idx_list, random_state=42)\nscaler = StandardScaler()\n\n# initiating the random search\nfor grid, model in zip(parameter_grid.values(), model_objects) :\n  # the only change that i have done is remove the comma \",\" from the end of the very next line i.e classif_model = ......\n  classif_model = Pipeline([('resampler', over_sampler), ('scaler', scaler), ('model', model)])\n  # the nex thing tht we can do is remove the over_sampler an scaler objects and define them in te pipeline itself\n  optimizer = RandomizedSearchCV(estimator = classif_model,\n\t\t\t\t\t\t\t\tparam_distributions = grid,\n\t\t\t\t\t\t\t\trandom_state = 42,\n\t\t\t\t\t\t\t\tcv = 3,\n\t\t\t\t\t\t\t\terror_score = -1,\n\t\t\t\t\t\t\t\tverbose = 10,\n\t\t\t\t\t\t\t\tn_jobs = -1,\n\t\t\t\t\t\t\t\t)\n  optimizer.fit(x_optimization, y_optimization.values.ravel())\n\t# appending the best estimator to a list\n  hyper_parameter_optimized_models.append(optimizer.best_estimator_)\n\nprint('Hyper parameter tunning is finished.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiating an empty list to stre the validation scores of the optimized models\noptimized_model_validation_scores = []\n\nfor optimized_model in hyper_parameter_optimized_models :\n  optimized_model_pipeline = Pipeline([('resampler', over_sampler), ('scaler', scaler), ('optimized_model', optimized_model)])\n  model_validation_scores = cross_val_score(optimized_model_pipeline, x_validation, y_validation.values.ravel(), cv=3, n_jobs = -1)\n  optimized_model_validation_scores.append(np.mean(model_validation_scores))\n\n# making a dictionary to store the results of the hyper-parameter optimization and the model selection process.\nresults_dict = {'optimized_model':hyper_parameter_optimized_models,\n                'validation_score':optimized_model_validation_scores\n                }\n\noptimized_model_results = pd.DataFrame(results_dict)\n# # saving the results of the hyper-parameter optimization and model_selection in a csv file\n# optimized_model_results.to_csv('/content/drive/My Drive/data_for_HPO&MS/Marketing_response/model_optimizaion_report.csv')\nprint('Model selection is finished')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best performing hyper-parameter optimised model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the best model by its index for the final predictions\nbest_model_idx = optimized_model_results['validation_score'].idxmax(axis=0)\nbest_model = optimized_model_results.iloc[best_model_idx,0]\n\nprint('The best model to our finding is ', best_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the best model from the above findings, that will be futher used for the final prediction making."},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the classifier algorithm from the pipeline of the best model found.\nfinal_model = best_model[2]\nfinal_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are utilizing the whole training dataset for training the fianl model before making predictions on the test set.\n# resampling our training datasets, in order to prevent overfitting of our models on the majority class of the target feature in our training set\nx_train_resampled, y_train_resampled = over_sampler.fit_resample(x_train, y_train)\n# dropping the sythetic feature after resampling is done\ny_train_resampled = pd.DataFrame(y_train_resampled)\nx_train_resampled = pd.DataFrame(x_train_resampled, columns = x_train.columns)\n\n# scaling our features in the training dataset\nscaler = StandardScaler().fit(x_train_resampled)\nx_train_scaled = scaler.transform(x_train_resampled)\nx_test_scaled = scaler.transform(x_test)\n\n# re-fitting out best found optimized model to the whole training set\nfinal_model.fit(x_train_scaled, y_train_resampled.values.ravel())\nout_of_sample_predictions = final_model.predict(x_test_scaled)\n\nfinal_score = accuracy_score(y_test, out_of_sample_predictions)\n\nprint('The final average out-of-sample performance score of our best optimized model is', round(final_score, 3)*100, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}