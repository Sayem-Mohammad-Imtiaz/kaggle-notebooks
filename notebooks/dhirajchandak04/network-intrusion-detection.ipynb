{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler #column standardization\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom scipy.stats import chi2_contingency\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data into train and test dataframes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/network-intrusion-detection/Train_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/network-intrusion-detection/Test_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking number of columns and type of each columnb\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation - 4 columns are object, rest all are numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the first 10 records of train dataset\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Exploratory Data Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['protocol_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['flag'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_row', None)\ntrain['service'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Observation** - We have approx. 53-47% of class label records. Therefore, we can pass this data to ML algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"pro_flg_serv = train.groupby(['protocol_type','service','class'])['class'].count()\npro_flg_serv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Observation** -\n\n1. All records are anomaly when\n\n    - protocol_type = \"tcp\" and service as \"Z39_50\", \"bgp\", \"courier\", \"csnet_ns\", \"ctf\", \"daytime\", \"discard\", \"echo\", \"efs\",\"exec\", \"gopher\", \"hostnames\", \"http_443\", \"http_8001\", \"imap4\", \"iso_tsap\", \"klogin\", \"kshell\", \"ldap\", \"link\", \"login\", \"mtp\", \"name\", \"netbios_dgm\", \"netbios_ns\", \"netbios_ssn\", \"netstat\", \"nnsp\", \"nntp\", \"pm_dump\", \"pop_2\", \"printer\", \"private\", \"remote_job\", \"rje\", \"sql_net\", \"ssh\", \"sunrpc\", \"supdup\", \"systat\", \"uucp\", \"uucp_path\", \"vmnet\", \"whois\"\n    - protocol_type = \"icmp\" and service = \"tim_i\"\n\n2. All records are normal when\n\n    * protocol_type = \"tcp\" and with service = \"IRC\"\n    * protocol_type = \"icmp\" and service as \"red_i\", \"urh_i\"\n    * protocol_type = \"udp\" and service = \"ntp_u\""},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/28576540/how-can-i-normalize-the-data-in-a-range-of-columns-in-my-pandas-dataframe\n# Normalize the data across numeric columns in dataset. Therefore, removing 4 object columns from list\ncols_to_norm = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', \n                'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n                'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', \n                'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', \n                'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', \n                'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', \n                'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\ntrain[cols_to_norm] = StandardScaler().fit_transform(train[cols_to_norm])\ntest[cols_to_norm] = StandardScaler().fit_transform(test[cols_to_norm])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Identify collinearity between columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/38913965/make-the-size-of-a-heatmap-bigger-with-seaborn\ncorr_df = train[cols_to_norm].corr(method='pearson')\nfig, ax = plt.subplots(figsize=(12,12)) \nsns.heatmap(corr_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_column',None)\ncorr_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since num_outbound_cmds and is_host_login is NAN value, we are dropping it from column and row.\ncorr_df.drop(index=['is_host_login','num_outbound_cmds'], columns=['is_host_login','num_outbound_cmds'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **There are 2 methods which i have used to drop either of the correlated column.**\n\n#### ***Method 1***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://thispointer.com/python-pandas-how-to-add-rows-in-a-dataframe-using-dataframe-append-loc-iloc/\ncorr_col_df = pd.DataFrame(columns=[\"Column1\",\"Column2\",\"Corr_value\"])\nfor i in corr_df.columns:\n    for j in corr_df.index:\n        if (i != j) and (corr_df[i][j] > 0.7):\n            corr_col_df = corr_col_df.append({ \"Column1\" : i, \"Column2\" : j, \"Corr_value\" : corr_df[i][j] }, ignore_index=True)\n            #print(i, \"\\t\", j, \"\\t\", corr_df[i][j])\n            \ncorr_col_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# records are repeating while is creating unnecessary complexity in analysis. Will try to remove duplicate records\n\nind_list = []\nfor i in range(len(corr_col_df)):\n    for j in range(len(corr_col_df)):\n        #print(\"j\", j)\n        #print(\"corr\", uni_corr_col_df['Corr_value'][j])\n        #print(\"columns\", uni_corr_col_df['Column1'][i], uni_corr_col_df['Column2'][j])\n        if ((i!=j) and (corr_col_df['Corr_value'][i] == corr_col_df['Corr_value'][j]) \n            and (corr_col_df['Column1'][i] == corr_col_df['Column2'][j]) \n            and (corr_col_df['Column2'][i] == corr_col_df['Column1'][j])):\n            ind_list.append([i,j])\n\n# Unique pair value from list - \n# https://www.geeksforgeeks.org/python-remove-duplicates-from-nested-list/\n# https://stackoverflow.com/questions/47051854/remove-duplicates-based-on-the-content-of-two-columns-not-the-order\nfor i in ind_list:\n    i.sort()\nuni_ind_list = list(set(tuple(i) for i in ind_list)) \n\n# store unique records into dataframe\nuni_corr_col_df = pd.DataFrame(columns=[\"Column1\",\"Column2\",\"Corr_value\"])\nfor i in uni_ind_list:\n    uni_corr_col_df = uni_corr_col_df.append(corr_col_df.iloc[i[0]], ignore_index=True)\n    \nuni_corr_col_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identifying columns to delete from dataframe\ncol_corr = set() # Set of all the names of deleted columns\nfor i in range(len(uni_corr_col_df)):\n    if (uni_corr_col_df['Column1'][i] not in col_corr):\n        colname = uni_corr_col_df['Column2'][i] # getting the name of column\n        col_corr.add(colname)\n\ncol_corr = list(col_corr)\nprint(col_corr)\n\n# dropping identified columns from train and test dataset\ntrain.drop(col_corr, axis=1, inplace=True)\ntest.drop(col_corr, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Method 2 - To identify correlated columns and drop them from train and test dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_corr = set() # Set of all the names of deleted columns\nfor i in range(len(corr_df.columns)):\n    for j in range(i):\n        if (corr_df.iloc[i, j] >= 0.7) and (corr_df.columns[j] not in col_corr):\n            colname = corr_df.columns[i] # getting the name of column\n            col_corr.add(colname)\ncol_corr = list(col_corr)\nprint(col_corr)\n\n# dropping identified columns from train and test dataset\n\n#Uncomment below code if you are using Method 2.\n#train.drop(col_corr, axis=1, inplace=True)\n#test.drop(col_corr, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***When to use Chi-Squared Test:***\nWhen the data type of our feature to be tested and the target variable are both categorical (i.e. we have a classification problem) we can use Chi-Squared test. Now we will identify correlation between categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://towardsdatascience.com/chi-squared-test-for-feature-selection-with-implementation-in-python-65b4ae7696db\n# we need to pass data in cross tabular format to chi2_contingency. Therefore, using pd.crosstab\n# we are assuming that significant value is 0.05\nalpha = 0.05\nstat, p, dof, expected = chi2_contingency(pd.crosstab(train['protocol_type'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nprotocol_type and class columns are dependent\")\nelse:\n    print(\"\\nprotocol_type and class columns are independent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['service'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nservice and class columns are dependent\")\nelse:\n    print(\"\\nservice and class columns are independent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['flag'], train['class']))\nprint(\"p\", p)\n\nif p<=alpha:\n    print(\"\\nflag and class columns are dependent\")\nelse:\n    print(\"\\nflag and class columns are independent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Encoding the categorical data and splitting it into 75-25**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert dependent variable to number.\nlabel_encoder = LabelEncoder()\ntrain['class'] = label_encoder.fit_transform(train['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['class']\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('class', axis=1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Approach 1 - using get_dummies to encode categorical values into numeric."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=45).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Approach 2 - using labelEncoder to encode categorical values to numerical and then convert it to type as category. To verify the results with get_dummies function"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX['protocol_type'] = label_encoder.fit_transform(X['protocol_type'])\nX['service'] = label_encoder.fit_transform(X['service'])\nX['flag'] = label_encoder.fit_transform(X['flag'])\n\nX['protocol_type'] = X['protocol_type'].astype('category')\nX['service'] = X['service'].astype('category')\nX['flag'] = X['flag'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=45).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations -\n1. With get dummies on all columns of type object, F1 score is 0.9736\n2. With converting object type columns to ascategory and then doing label encoding, F1 score is 0.9576\n\n#### From the F1 score, we can say that Approach 1 is better than Approach 2 in this case. Therefore, we are goinf ahead with Approach 1."},{"metadata":{},"cell_type":"markdown","source":"### Predicting test dataset values\n\nBefore passing test dataset values to the generated model, we need to perform same preprocessing and operations which we did on train dataset.\n1. Drop correlated columns from test dataset --> We have already drop correlated columns from test dataset along with train dataset.\n2. Normalized the columns --> We have already completed this activity with train dataset.\n3. get_dummies on categorical columns of test dataset --> We need to perform this activity now."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.get_dummies(test)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\n\nWe don't have same number of columns as train dataset. Therefore we need to identify missing columns from test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are checking if there is any column which is present in test dataset but not in train dataset. \n# If yes, then we will delete it from test dataset because model is not trained on those columns.\nfor i in list(test.columns):\n    if i not in list(X.columns):\n        print(i)\n        test.drop(i, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in order to fetch particular column index, we can use df.columns.get_loc()\n# https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas\n# to add a column on particular index loc in dataframe, we can use df.insert()\n# https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas\n\n# Here, we are identifying missing columns from test dataset.\nfor i in list(X.columns):\n    if i not in list(test.columns):\n        print(i)\n        ind = X.columns.get_loc(i)\n        test.insert(loc=ind, column=i,value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### End of the notebook!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}