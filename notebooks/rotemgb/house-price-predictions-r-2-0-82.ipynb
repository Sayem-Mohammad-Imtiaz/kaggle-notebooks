{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About the Notebook"},{"metadata":{},"cell_type":"markdown","source":"In this kernel, I will present a simple linear regression model combined with more advanced concepts, such as *data manipulation, data distribution, multicollinearity, polynomial relationships* etc., and present ways to deal with these problems.\n\nI chose a simple data set to allow a better understanding of these concepts. Notice how applying these tools will improve the fitting  test results and the quality of your predictions.\n\nI am sometimes using different terms that might imply the same thing. For example, features and variables, or target and response. It is important to know all of the terms that are being used by the community.\n\nIf you have any question or suggestions, please don't hesitate to comment!\n\n**If you like this kernel, please don't hesitate to UPVOTE :)**"},{"metadata":{},"cell_type":"markdown","source":"## The Importance of Predicting House Prices"},{"metadata":{},"cell_type":"markdown","source":"Are you planning on buying a house for investment, and wondering which house would be the best buy? Are you planning to renovate your house to increase its value on the market, but don't know where to invest the most to get the best results? Do you have a real-estate company that wants to give the best machine-learning based solutions to its customers? You are in the right place!\n\nIn this notebook, I will analize house sales in King County, WA, USA between 2014 to 2015.  \n\nLet's start by looking the maps below: the top image is the King County region; the bottom image is downtown Seattle, the capital of Washington."},{"metadata":{},"cell_type":"markdown","source":"<img align=\"center\" src=\"https://imgur.com/O1ImtR8.png\" width=\"700\" hight=\"550\" title=\"King County Region, WA, USA\" />\n<img align=\"center\" src=\"https://imgur.com/culbAe4.png\" width=\"700\" hight=\"250\" title=\"Downtown Seattle\" />"},{"metadata":{},"cell_type":"markdown","source":"# Explore the Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom geopy import distance\nfrom scipy.stats import skew\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nplt.style.use('seaborn-whitegrid')\nsns.set_style('whitegrid')\nplt.rcParams[\"axes.labelsize\"] = 15\n\nfrom bokeh.plotting import figure, show, output_notebook, output_file\nfrom bokeh.tile_providers import CARTODBPOSITRON\nfrom bokeh.transform import log_cmap\nfrom bokeh.models import ColumnDataSource, LogTicker, ColorBar #, HoverTool, CategoricalColorMapper, LogColorMapper\nfrom bokeh.models.formatters import BasicTickFormatter, NumeralTickFormatter\nimport bokeh.palettes as bp\n\noutput_notebook()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's read the data, look at the head of the table and the information about the features."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe contains 19 house features, plus the price and the ID columns, along with 21613 observations.\n\nThe ID column doesn't contribute any insight into the data, and neither does the date, as all of the data is from 2014 to 2015. Let's drop those columns."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.drop(['id', 'date'], axis=1, inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the different features and their relation to the price of the house, starting with the discrete variables."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(3,2, figsize=(18,16))\nfor xcol, ax in zip(['floors', 'waterfront', 'view', 'condition',\n                     'grade', 'bedrooms'], axes.flatten()):\n    sns.boxplot(xcol, 'price', data=df, ax=ax)\n    \n\nfig = plt.figure(figsize=(16, 8))\nsns.boxplot('bathrooms', 'price', data=df)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that some of the features have a linear relation to the target (*price*), but some of them have a non-linear relation, such as the *grade*, which looks more like an exponential relation to price. This seems a bit problematic, considering we are going to use a *linear* regression model. We will learn how to deal with this problem later in this kernel.\n\nLet's have a look at the continuous variables and their relation to the target."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features_cont = ['sqft_living', 'sqft_lot', 'sqft_above','sqft_basement', \n                'sqft_living15', 'sqft_lot15']\n\nfig, axes = plt.subplots(3,2, figsize=(14,14))\n\nfor xcol, ax in zip(features_cont, axes.flatten()):\n    sns.scatterplot(xcol, 'price', data=df, ax=ax)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, we can see some features with a more linear relation to *price*, but some of the features reveal more complex relationships, like polynomial, exponential or even a square-root relation. We can see the complex relations clearly with the features such as *sqft_lot* and *sqft_lot15*.\n\nLet's have a look at the price of houses according to their location in Seattle."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def lgn2x(a):\n    return a * (np.pi/180) * 6378137\n\ndef lat2y(a):\n    return np.log(np.tan(a * (np.pi/180)/2 + np.pi/4)) * 6378137\n\n\n# project coordinates\ndf['x_coor'] = df['long'].apply(lambda row: lgn2x(row))\ndf['y_coor'] = df['lat'].apply(lambda row: lat2y(row))\n\n# creating the map\noutput_file(\"tile.html\")\nxmin, xmax =  df['x_coor'].min(), df['x_coor'].max() \nymin, ymax =  df['y_coor'].min(), df['y_coor'].max() \n\n# range bounds supplied in web mercator coordinates\nmap_kc = figure(x_range=(xmin, xmax), y_range=(ymin, ymax),\n           x_axis_type=\"mercator\", y_axis_type=\"mercator\", title=\"House Price on King County, USA\",\n           plot_width=700, plot_height=500,)\n\nmap_kc.title.text_font_size = '16pt'\nmap_kc.add_tile(CARTODBPOSITRON)\n\nsource = ColumnDataSource({'x':df['x_coor'], 'y':df['y_coor'], 'z':df['price']})\ncolormapper = log_cmap('z', palette=bp.Inferno256, low=df['price'].min(), high=df['price'].max())\n\nmap_kc.circle(x ='x', y='y', source=source, color=colormapper)\n\ncolor_bar = ColorBar(color_mapper=colormapper['transform'], width=18, location=(0,0), \n                     ticker=LogTicker(), label_standoff=12)\n\ncolor_bar.formatter = NumeralTickFormatter(format='0,0')\n# color_bar.formatter = BasicTickFormatter(precision=3)\n\nmap_kc.add_layout(color_bar, 'right')\n\nshow(map_kc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can make two observations from this plot:\n1. The northern part of King County region has a higher house prices.\n2. The closer the house is to downtown Seattle, the price of the houses increases.\n\nAs for the first observation, we can assume that the *lat* feature is important in our model. We can check our assumption by using feature engineering methods, but since we don't have many features compared to the number of data points, we see better results using the entire data set, which negates the need for feature engineering. \n\nAs for the second observation, we can create a new feature that measures the *distance* from each house to downtown Seattle. This feature is a *nonlinear combination* of the *lat* and *long* features, so it doesn't increase the multicollinearity* (see next session).\n\nLet's add the *distance* feature (in km) and have a look at the head of the table."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"location = tuple(map(tuple, df[['lat', 'long']].values))\n# the distance of every house from downtowm seattle\nseattle_dt = (47.6050, -122.3344)\n\ndf['distance'] = [distance.distance(seattle_dt, loc).km for loc in location]\n\n# df.drop(['lat', 'long', 'x_coor', 'y_coor'], axis=1, inplace=True)\ndf.drop(['x_coor', 'y_coor'], axis=1, inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multicollinearity and Data Manipulation"},{"metadata":{},"cell_type":"markdown","source":"Collinearity between variables can produce misleading results. While the \\\\(R^2\\\\) might not be affected by collinearity, the interpretation of the results are highly affected by it. The presence of collinearity can pose problems in\nthe regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since two correlated variables tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response (in our case, the price of the house). This phenomena can completely change the coefficient values (and therefore the interpretation of their importance), and in some cases it can even change the sign of the coefficient value. \n\nUnfortunately, it is not enough to check the correlation matrix, as multicollinearity can occur between three or more variables, even when there is no indication of collinearity between two variables. A better way to assess multicollinearity is by computing the *[Variance Inflation Factor](http://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/)* (\\\\(VIF\\\\)). As a rule of thumb, we would like to keep the \\\\(VIF\\\\) under 5, as  \\\\(VIF > 5\\\\) suggests medium multicollinearity, while \\\\(VIF > 10\\\\) suggests high multicollinearity.\n\nLet's have a look at the \\\\(VIF\\\\) value of our variables."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_vif(data):\n    \n    X = data.iloc[:,1:]\n    vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n\n    return vif.round(1)\n\nget_vif(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You'll see that most of the variables suggest strong multicollinearity; some of the values even go to infinity. To solve this problem, we can create another variable, which will be a linear combination of two highly correlated variables. \n\nComputing a correlation heatmap will help us choose the features we would like to combine together. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nsns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, we have high correlation between *sqft_living* and *sqft_above*. We can also see high correlation between *sqft_living15* and both *sqft_living* and *sqft_above*. \n\nLet's create a new variable called *sqft* that will be a linear combination of the three predictors, and will replace these predictors. We can do the same with *sqft_lot* and *sqft_lot15*, creating a new variable called *sqft_lot_comb*. \n\nLet's have a look at the new table's head."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['sqft'] = df['sqft_living'] + df['sqft_above'] + df['sqft_living15']\ndf.drop(['sqft_living', 'sqft_above', 'sqft_living15'], axis=1, inplace=True)\n\ndf['sqft_lot_comb'] = df['sqft_lot'] + df['sqft_lot15']\ndf.drop(['sqft_lot', 'sqft_lot15'], axis=1, inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the affect on the \\\\(VIF\\\\) table."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"get_vif(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a big improvement: we no longer have infinite \\\\(VIF\\\\) values!  However, we still have some more data manipulation to do for better results. For example, *lat, long, zipcode* and *yr_built* have high \\\\(VIF\\\\) values. \n\nLet's have a look at the data distribution and deal with the polynomial relationships we obsereved earlier between the variables and the target."},{"metadata":{},"cell_type":"markdown","source":"# Data Distributuion and Polynomial Relationships"},{"metadata":{},"cell_type":"markdown","source":"In the plot below, you can see the data distribution of the target. Notice that the target has a right-skewed distribution, meaning, it has a \"tail\" on the right side. Skewing the data by log-transformation will transform the right-skewed distribution to a normal distribution.\n\nTo be clear, there is no need for the target to be normally distributed in order to fit a linear regression model. The normallity assumption in linear regression refers to the error terms between the target and the predicted values (\\\\(\\epsilon\\\\)), and not the distribution of the target itself. \n\nHowever, a log transformation can sometimes solve multiple problems simultaneously: it will linearize some of the polynomial relationships, and help us create a more flexible model that allows [non-linear relashionship with the target](http://stats.stackexchange.com/questions/107610/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution) (see Bill's answer, poin 2). It will also [reduce outlier influences](https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07). \n\nPlus, we get normally distributed data, which is always nice to have, and it's important for some statistical hypothesis tests. \n\nWe will use the normality assumption in the next section, when we will carry on with the high multicollinearity issue."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(11,5))\nfig = sns.distplot(df['price'])\nfig.set(yticks=[]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's look at the skewness factor of the target and the features. \n\nIn order to be normally distributed, the skewness should be zero. A positive skewness is a right-skewed data. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# computing skewness factor\nskewness = df.apply(lambda x: skew(x))\nskewness","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do a log transformation on the data where *skewness*  \\\\(> 0.75\\\\), and have a look on the affect on the *price* distribution."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# converting longtitude to positive values to enable us using the log function on all data\n# this operation doesn't affect results, as all the whole 'long' column is negative\ndf['long'] = abs(df['long'])\n\nskewed = skewness[skewness > 0.75].index\n\ndf[skewed] = np.log1p(df[skewed])\n\n# plot the new target ditribution\nfig = plt.figure(figsize=(11,5))\nfig = sns.distplot(df['price'])\nfig.set(yticks=[]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the *price* is now normally distributed.\n\nLet's have a look at some of the predictors and their relation to the target."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,2, figsize=(16,10))\n\nsns.scatterplot('sqft', 'price', data=df, ax=axes[0,0])\nsns.scatterplot('sqft_lot_comb', 'price', data=df, ax=axes[0,1])\nsns.boxplot('bedrooms', 'price', data=df, ax=axes[1,0])\nsns.boxplot('grade', 'price', data=df, ax=axes[1,1])\naxes[1,0].set_xticks([])\naxes[1,1].set_xticks([])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see the change in the relationship between some of the features and the target. We also see that what once was a nonlinear relation between the feature to the *price* now have a more linear relationship to it. Moreover, we've reduced the outliers' affect. \n\nThis dramatically increases the accuracy of our model. \n\nNotice that it has no affect on the multicollinearity."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"get_vif(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Standardization "},{"metadata":{},"cell_type":"markdown","source":"Going back dealing with the high  \\\\(VIF\\\\), we can use [data standardization to reduce multicollinearity](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/). In fact, only by centering the data, we will lower the \\\\(VIF\\\\) values (the scaling part is a matter of preference). \n\nUnder the assumption that the data has a symetric distribution (like normal distribution), the correlation created by interaction terms will be zero. For the full mathematical explanation, see [this link](https://psychometroscar.com/why-does-centering-in-linear-regression-reduces-multicollinearity/)."},{"metadata":{},"cell_type":"markdown","source":"Let's see the affect of standardizing the data on the \\\\(VIF\\\\)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Standardizing the data\ndf = (df - df.mean()) / df.std()\n\nget_vif(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, standardizing the data has a huge affect on the \\\\(VIF\\\\) values. \n\nNow we are ready to fit the model and get predictions!"},{"metadata":{},"cell_type":"markdown","source":"# House Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"Now, we will predict house prices using a simple linear regression and a k-fold cross-validation. The fitted model will be tested with the R-squared adjusted test, so it will not be affected by the number of features I chose to use in the model (like in the R-squared test)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def split_kfold(folds, i):    \n    train = folds.copy() \n    test = folds[i]\n    del train[i]\n    train = np.concatenate(train, axis=0)\n    d = train.shape[1]-1\n    x_train, y_train = train[:, :d], train[:, d]\n    x_test, y_test = test[:, :d], test[:, d]\n    \n    return x_train, x_test, y_train, y_test\n\n\ndef get_error(Y, Yhat):\n    N = len(Y)   \n    d1 = Y - Yhat\n    d2 = Y - Y.mean()\n    r2 = 1 - (d1.dot(d1) / d2.dot(d2))\n    r2_adj = 1 - (1 - r2)*((N - 1) / (N - D - 1))\n    mse = d1.dot(d1) / N\n    return r2_adj, mse\n\n\ndef fit_kfold(X, Y, X_test, Y_test):\n    w = np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n    Yhat = X.dot(w)\n    Yhat_test =  X_test.dot(w)\n    r2_test, mse_test = get_error(Y_test, Yhat_test)\n    \n    return r2_test, w\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# df_array = df[features].values\nX = df.iloc[:,1:]\nY = df.iloc[:,0]\n\ndf_array = np.c_[X.values, Y.values]\nk = 7\nD = X.shape[1]\nfolds = np.array_split(df_array, k)\n\nr2_test = []\ncoef = []\n\nfor i in range(k):\n    x_train, x_test, y_train, y_test = split_kfold(folds, i)\n    # prepare the array\n    x_train = np.c_[np.ones(x_train.shape[0]), x_train]\n    x_test = np.c_[np.ones(x_test.shape[0]), x_test]\n    \n    r2_test_temp, w = fit_kfold(x_train, y_train, x_test, y_test)\n    r2_test.append(r2_test_temp)\n    coef.append(w)\n    \nr2_test_kfold = sum(r2_test) / len(r2_test)\ncoef = np.sum(coef, axis=0) / len(coef)\n\nindx = list(df.columns)\nindx[0] = 'bias'\ncoef = pd.DataFrame(coef, index=indx, columns=['coef'])\n\nprint('Using  k-fold cross-validation where k = ', k,':')\nprint('R2_adjusted of the test data, using a simple linear regression, is: ', r2_test_kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the \\\\(R^2\\\\) is higher than the everage \\\\(R^2\\\\) from other notebooks that used a linear regression model. \n\nLet's have a look at the coefficients."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"coef.reindex(coef['coef'].abs().sort_values(ascending=False).index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above you can see the coefficients sorted by their importance. We can see that the features *sqft, distance, grade* and *lat* have the greatest affect on the *price*. The negative coefficients imply that the increase of these features lowers the price of the house. For example, as the houses' distance from downtwon Seattle increases, the price of the house decreases.\n\nBecause we used log-transformation on the data and standardized it, it cannot be interpreted using traditional or straightforward methods. More information on how to interpret the coefficients will be added soon."},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"### We fit a simple linear model and got a great \\\\(R^2\\\\) result! \n**How did we do it?**\n1. Data manipulation: we created the *distance* feature.\n2. Log-transformation of the data: created a more flexible model that allows non-linear relationships with the target.\n\n### Lowered multicollinearity and got statistically significant results! \n**How did we do it?**\n1. Created new features to replace highly correlated ones with their linear combination. \n2. Standardizing the data to reduce correlations caused by interaction terms."},{"metadata":{},"cell_type":"markdown","source":"**I hope you enjoy my notebook! If you did, please UPVOTE!**\n\nIf you have a comment or ideas for improvement, please leave a comment below.\n\nSee you on my next kernel :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}