{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.pixabay.com/photo/2017/03/04/12/10/avocado-2115922_960_720.jpg\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1>Prediction of Average Prices of Avocado in USA</h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Context</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements.\n\nClearly, they aren't buying home because they are buying too much Avocado Toast!\n\nBut maybe there's hope… if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Content</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The data represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. Starting in 2013, the data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the data reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Columns in our dataset</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- index\n- Date: The date of the observation\n- AveragePrice: The average price of a single avocado\n- Total Volume: Total number of avocados sold\n- 4046: Total number of avocados with PLU 4046 sold\n- 4225: Total number of avocados with PLU 4225 sold\n- 4770: Total number of avocados with PLU 4770 sold\n- Total Bags\n- Small Bags\n- Large Bags\n- XLarge Bags\n- type: conventional or organic\n- year: The year\n- region: The city or region of the observation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 1: Importing our Dataset</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndataset = pd.read_csv(\"/kaggle/input/avocado-prices/avocado.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using the pandas library to read the the csv file named avocado.The file contains information on the following features: • Date - The date of the observation • AveragePrice - the average price of a single avocado • Total Volume - Total number of avocados sold • 4046 - Total number of avocados with PLU 4046 sold • 4225 - Total number of avocados with PLU 4225 sold • 4770 - Total number of avocados with PLU 4770 sold • Total Bags • Small Bags • Large Bags • XLarge Bags • type - conventional or organic • year - the year • region - the city or region of the observation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop('Unnamed: 0',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Feature \"Unnamed:0\" is just a representation of the indexes, so it's useless to keep it, lets remove it !**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Well as a first observation we can see that we are lucky, we dont have any missing values (18249 complete data) and 13 columns. Now let's do some Feature Engineering on the Date Feature so we can be able to use the day and the month columns in building our machine learning model later.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Date']=pd.to_datetime(dataset['Date'])\ndataset['Month']=dataset['Date'].apply(lambda x:x.month)\ndataset['Day']=dataset['Date'].apply(lambda x:x.day)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we add two more columns month and day where 6 in Month implies the month \"June\" & 15 in Day implies the 15th day of a month.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 2: Analysis of Average Prices</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nbyDate=dataset.groupby('Date').mean()\nplt.figure(figsize=(17,8),dpi=250)\nbyDate['AveragePrice'].plot()\nplt.title('Average Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hence the plot shows the average price of avocado at various points of time**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"byMonth = dataset.groupby(\"Month\").mean()\nplt.figure(figsize=(17,8),dpi=250)\nplt.plot([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"],byMonth['AveragePrice'])\nplt.title('Average Price Per Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above graph plotted for average price of avocado per month we can observe that the price rises for a while in February to March then it falls in April and then the month of May witnesses a rise in the average price. This rise reaches its zenith in the month of October and henceforth it starts to fall.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"byDay = dataset.groupby(\"Day\").mean()\nplt.figure(figsize=(17,8),dpi=250)\nbyDay['AveragePrice'].plot()\nplt.title('Average Price Per Day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above graph for average price per day implies that the price fluctuates in a similar manner at a regular interval.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nbyRegion=dataset.groupby('region').mean()\nbyRegion.sort_values(by=['AveragePrice'], ascending=False, inplace=True)\nplt.figure(figsize=(17,8),dpi=250)\nsns.barplot(x = byRegion.index,y=byRegion[\"AveragePrice\"],data = byRegion,palette='rocket')\nplt.xticks(rotation=90)\nplt.xlabel('Region')\nplt.ylabel('Average Price')\nplt.title('Average Price According to Region')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The barplot shows the average price of avocado at various regions in a ascending order. Clearly Hartford Springfield, SanFrancisco, NewYork are the regions with the highest avocado prices.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10),dpi=250)\ndataset[\"AveragePrice\"].plot(kind=\"hist\",color=\"blue\",bins=30,grid=True,alpha=0.65,label=\"Average Price\")\nplt.legend()\nplt.xlabel(\"Average Price\")\nplt.title(\"Average Price Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above histogram for the average price of avocado suggests that its distribution is somewhat positively skewed.**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import numpy as np\n\ncorr_df = dataset.corr(method='pearson')\nplt.figure(figsize=(12,6),dpi=250)\nsns.heatmap(corr_df,cmap='coolwarm',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can from the heatmap above, all the Features are not correlated with the Average Price column, instead most of them are correlated with each other.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('AveragePrice','region',data=dataset,\n                   hue='year',\n                   aspect=0.8,\n                   height=15,\n                   palette='magma',\n                   join=False,\n              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A factor plot is simply the same plot generated for different response and factor variables and arranged on a single page. The underlying plot generated can be any univariate or bivariate plot. The scatter plot is the most common application.\nThe above plot is a factor plot of average avocado price for different regions classified by year.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_vif = dataset.copy()\ndataset_vif.drop(columns=['Date','type','region'],inplace = True)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nXf = add_constant(dataset_vif)\npd.Series([variance_inflation_factor(Xf.values, i) \n               for i in range(Xf.shape[1])], \n              index=Xf.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above code snippet calculates the variable inflation factor for the displayed variables.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 3: Taking Care of the Outliers</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7),dpi=250)\nsns.boxplot(data = dataset[[\n 'AveragePrice',\n 'Total Volume',\n '4046',\n '4225',\n '4770',\n 'Total Bags',\n 'Small Bags',\n 'Large Bags',\n 'XLarge Bags']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clearly the boxplot indicates that all the variables contains outliers. Now we need to take care of the outliers.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(columns=[\"Date\"],inplace=True)\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Before we go on to taking care of the outliers we removed the \"Date\" variable from our dataset as it is useless now.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import percentile\n\ncolumns = dataset.columns\nfor j in columns:\n    if isinstance(dataset[j][0], str) :\n        continue\n    else:\n        for i in range(len(dataset)):\n            #defining quartiles\n            quartiles = percentile(dataset[j], [25,75])\n            # calculate lower/upper whisker\n            lower_fence = quartiles[0] - (1.5*(quartiles[1]-quartiles[0]))\n            upper_fence = quartiles[1] + (1.5*(quartiles[1]-quartiles[0]))\n            if dataset[j][i] > upper_fence:\n                dataset[j][i] = upper_fence\n            elif dataset[j][i] < lower_fence:\n                dataset[j][i] = lower_fence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In the following code snippet we have we replaced the outliers higer than the upper whisker by the value of the upper whisker and the outliers lower than the lower whisker by the value of the lower whisker.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7),dpi=250)\nsns.boxplot(data = dataset[[\n 'AveragePrice',\n 'Total Volume',\n '4046',\n '4225',\n '4770',\n 'Total Bags',\n 'Small Bags',\n 'Large Bags',\n 'XLarge Bags']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now clearly our data is free from outliers. Now we can fit our data to appropriate models.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 4: Taking Care of the Categorical Variables</h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now since our data contains categorical variables like \"type\", \"month\" and \"region\" we apply one-hot encoding to our variables \"region\",\"month\" and apply label encoding in variable \"type\".**\n\n**One hot encoding creates equal number of columns, with 1's and 0's, as the number of categories in a categorical variable a column for a specific category contains 1's where the category is present and 0's elsewhere.**\n\n**As for label encoding it asssigns numerical value to the categories of a categorical variable in their alphabetical order, the indexing starts with 0.**\n\n**OneHotEncoder in Python can encode a specific number of categories since for the variable 'region' we have crossed that threshold we have used pandas.get_dummies instead. Had we use OneHotEncoder we would have eliminated one column to avoid dummy variable trap but here we have no use for that.**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dataset['region'] = pd.Categorical(dataset['region'])\ndfDummies_region = pd.get_dummies(dataset['region'], prefix = 'region')\ndfDummies_region","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([dataset, dfDummies_region], axis=1)\ndataset.drop(columns=\"region\",inplace=True)\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adding the one hot encoded columns for region into our data and dropping the region column from our dataset.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Month'] = pd.Categorical(dataset['Month'])\ndfDummies_month = pd.get_dummies(dataset['Month'], prefix = 'month')\ndfDummies_month","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similarly applying one hot encoding on months.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([dataset, dfDummies_month], axis=1)\ndataset.drop(columns=\"Month\",inplace=True)\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adding the one hot encoded columns for Month into our data and dropping the Month column from our dataset.**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \n \nlabel_encoder = preprocessing.LabelEncoder() \ndataset['type']= label_encoder.fit_transform(dataset['type']) \ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now label encoding on the variable \"type\"**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Hence our preprocessing ends here!!!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now its time that we fit multiple linear regression, decision tree regression and random forest regression onto our data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 5: Model Fitting</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Having a look at our data after complete preprocessing.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Splitting our dataset to training and test numpy arrays with the names having their intended meaning. Where we are using 80% of our dataset for training and 20% of the data for testing.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.iloc[:,1:78]\ny=dataset['AveragePrice']\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=50)\ny_test = np.array(y_test,dtype = float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalizing our X_train and X_test using standard scaler**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The funtion regression_results defined below calculates and prints the following features of a model: explained_variance, r2, adjusted_r2, MAE, MSE, RMSE. It accepts the original and predicted values as its arguments.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\n\ndef regression_results(y_true, y_pred):\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    r2=metrics.r2_score(y_true, y_pred)\n    adjusted_r2 = 1 - (1-r2)*(len(y_true)-1)/(len(y_true)-X_test.shape[1]-1)\n\n    print('Explained_variance: ', round(explained_variance,4))    \n    print('R2: ', round(r2,4))\n    print('Adjusted_r2: ', round(adjusted_r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below is a function to find the accuracy of each model on the basis of K-fold cross validation.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ndef model_accuracy(model,X_train=X_train,y_train=y_train):\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10)\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Fitting Multiple Linear Regression Model</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The following code snippet fits the multiple linear regression model on X_train and y_train and predicts the values for X_test and stores it in y_pred. It also prints the outputs of the functions defined above. Hence giving us a useful summary for the multiple linear regression model.**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\n\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nregression_results(y_test,y_pred)\nmodel_accuracy(regressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 12),dpi=250)\nred = plt.scatter(range(len(X_test)),y_pred,c='r',s = 10)\nblue = plt.scatter(range(len(X_test)),y_test,c='b', s = 10)\nplt.title(\"Scatter Plot of y_pred and y_test for Regression\",fontsize=15)\nplt.legend((red,blue),(\"y_pred\",\"y_test\"),scatterpoints=1, loc='upper right',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above scatterplot comprises of the original and predicted values of the multiple linear regression model.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Fitting Random Forest Regression Model</h3>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The following code snippet fits the random forest regression model on X_train and y_train and predicts the values for X_test and stores it in y_pred_rf. It also prints the outputs of the functions defined above. Hence giving us a useful summary for the random forest regression model.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nclassifier = RandomForestRegressor()\nclassifier.fit(X_train, y_train)\ny_pred_rf = classifier.predict(X_test)\nregression_results(y_test,y_pred_rf)\nmodel_accuracy(classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 12),dpi=250)\nred=plt.scatter(range(len(X_test)),y_pred_rf,c='r',s = 10)\nblue=plt.scatter(range(len(X_test)),y_test,c='b', s = 10)\nplt.title(\"Scatter Plot of y_pred and y_test for Random Forest Regression\",fontsize=15)\nplt.legend((red,blue),(\"y_pred\",\"y_test\"),scatterpoints=1, loc='upper right',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above scatterplot comprises of the original and predicted values of the random forest regression model.**\n\n<h3>Fitting Decision Tree Regression Model</h3>\n\n**The following code snippet fits the decision tree regression model on X_train and y_train and predicts the values for X_test and stores it in y_pred_dt. It also prints the outputs of the functions defined above. Hence giving us a useful summary for the decision tree regression model.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndecision_tree=DecisionTreeRegressor(criterion='mse',splitter='random',random_state=10)\ndecision_tree.fit(X_train, y_train)\ny_pred_dt = decision_tree.predict(X_test)\nregression_results(y_test,y_pred_dt)\nmodel_accuracy(decision_tree)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 12),dpi=250)\nred=plt.scatter(range(len(X_test)),y_pred_dt,c='r',s = 10)\nblue=plt.scatter(range(len(X_test)),y_test,c='b', s = 10)\nplt.title(\"Scatter Plot of y_pred and y_test for Decision Tree Regression\",fontsize=15)\nplt.legend((red,blue),(\"y_pred\",\"y_test\"),scatterpoints=1, loc='upper right',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above scatterplot comprises of the original and predicted values of the random forest regression model.**\n\n<h2>As our conclusion we proclaim that, using k-fold cross validation as the basis for model selection we declare random forest model as the best suited model for our purpose of predicting average avocado prices</h2>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}