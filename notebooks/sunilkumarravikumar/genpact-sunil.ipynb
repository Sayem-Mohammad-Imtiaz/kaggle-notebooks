{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation of useful libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport random \nimport datetime as dt\nimport re\nimport pickle\nimport nltk, warnings\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom string import digits, punctuation\n\n\nfrom scipy.stats import chi2_contingency\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')\n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Create the dataframe by encoding it\ndataset=pd.read_csv(r'/kaggle/input/ecommerce-data/data.csv',encoding='ISO-8859-1')\n\n#view the sample records\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information about the dataset i.e column name, data types and no of non null counts\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the missing values\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset shape\n\nrows, columns = dataset.shape\nprint(\"Dataset Info\")\nprint(\"*************\")\nprint(\"Number of Observations  :\", rows)\nprint(\"Number of Dimensions    :\", columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the column names and datatypes\ndash = '-' * 70\n\nprint (dash)\nprint ('{:<40s}{:<2s}{:>13s}{:>3s}{:>10s}'.format('              COLUMN NAME', '|', 'UNIQUE VALUES','|','DATA TYPE'))\nprint (dash)\nfor col in dataset.columns:\n    print ('{:<40s}{:<2s}{:>8s}{:>8s}{:>8s}'.format(col, '|', str(len(dataset[col].unique())), '|', str(dataset[col].dtype)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns list\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Statistics info of dataset\ndataset.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if there is duplicate observations\ndataset.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop all duplicate observations which is not needed\ndataset.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information without having duplicate obserations inplace\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the null values\nnan_obs = dataset[dataset.isnull().T.any().T]\nnan_obs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the observations where customerid is null\ndataset=dataset.dropna(subset=['CustomerID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset shape after removing nan records\n\nrows, columns = dataset.shape\nprint(\"Dataset Info\")\nprint(\"*************\")\nprint(\"Number of Observations  :\", rows)\nprint(\"Number of Dimensions    :\", columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage(%) of the removed records : {}%\".format(((541909-401604)/541909)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploratory data analsys\n#Number contries present in the dataset\ndataset['Country'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observations count on country wise\ncountrywise=dataset[['Country','CustomerID']].drop_duplicates()\ncountrywise.groupby(['Country'])['CustomerID'].aggregate('count').reset_index().sort_values('CustomerID', ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Conculsion - More than 90% data are from UK\n#Analze the numerical feature\ndataset.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[(dataset['Quantity']<0)].head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Observations:\na)The stock code values aren't only numerical\nb)The InvoiceNo aren't also only numerical since there is a C before the other numbers for every negative value in the quantity column, this could mean that the order was canceled.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analyzing Invoice\ntemp = dataset.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nnb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\nnb_products_per_basket.InvoiceNo = nb_products_per_basket.InvoiceNo.astype(str)\nnb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\nlen(nb_products_per_basket[nb_products_per_basket['order_canceled']==1])/len(nb_products_per_basket)*100\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like 16% of the transactions were canceled"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Few customers who cancelled the products. Same customers perfomed the cancellation repeatedly \nnb_products_per_basket[nb_products_per_basket['order_canceled']==1][:5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets analyze the Stockcode\nlist_special_codes = dataset[dataset['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"These are specific operations which doesn't characterize our customers so I'll just drop these transactions from our database\nDropping incorrect transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset[dataset['StockCode']!= 'POST']\ndataset = dataset[dataset['StockCode']!= 'D']\ndataset = dataset[dataset['StockCode']!= 'C2']\ndataset = dataset[dataset['StockCode']!= 'M']\ndataset = dataset[dataset['StockCode']!= 'BANK CHARGES']\ndataset = dataset[dataset['StockCode']!= 'PADS']\ndataset = dataset[dataset['StockCode']!= 'DOT']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outliers\ndataset.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#unit price contains the min value of 0, Sample of them\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[(dataset['UnitPrice'] == 0)].head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_counts(data):\n   for i in data.columns:\n       count = data[i].nunique()\n       print(i, \": \", count)\nunique_counts(dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#No of invoces per country\nNo_invoice_per_country = dataset.groupby([\"Country\"])[\"InvoiceNo\"].count().sort_values()\nNo_invoice_per_country.plot(kind='barh', figsize=(15,12))\nplt.title(\"Number of Invoices per Country\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding - Country\nle = LabelEncoder()\nle.fit(dataset['Country'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = [i for i in range(37)]\ndict(zip(list(le.classes_), l))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Country'] = le.transform(dataset['Country'])\n\nwith open('labelencoder.pickle', 'wb') as g:\n    pickle.dump(le, g)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RFM Analysis\ndataset['InvoiceDate'].min()\ndataset['InvoiceDate'].max()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll just fix the date to be one day after the last entry in the databse\n\nNOW = dt.datetime(2011,12,10)\ndataset['InvoiceDate'] = pd.to_datetime(dataset['InvoiceDate'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\n\n\nrfmTable = dataset.groupby(\"InvoiceNo\").agg(custom_aggregation)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfmTable[\"Recency\"] = NOW - rfmTable[\"InvoiceDate\"]\nrfmTable[\"Recency\"] = pd.to_timedelta(rfmTable[\"Recency\"]).astype(\"timedelta64[D]\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"rfmTable.head(5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\n\ncustom_aggregation[\"Recency\"] = [\"min\", \"max\"]\ncustom_aggregation[\"InvoiceDate\"] = lambda x: len(x)\n\nrfmTable_final = rfmTable.groupby(\"CustomerID\").agg(custom_aggregation)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfmTable_final.columns = [\"min_recency\", \"max_recency\", \"frequency\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrfmTable_final.head(5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nquantiles = rfmTable_final.quantile(q=[0.25,0.5,0.75])\nquantiles = quantiles.to_dict()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm = rfmTable_final\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4\n    \ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm['r_quartile'] = segmented_rfm['min_recency'].apply(RScore, args=('min_recency',quantiles,))\nsegmented_rfm['f_quartile'] = segmented_rfm['frequency'].apply(FMScore, args=('frequency',quantiles,))\nsegmented_rfm.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str)\nsegmented_rfm.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm = segmented_rfm.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segmented_rfm.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned = pd.merge(dataset,segmented_rfm, on='CustomerID')\ndf_cleaned.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned = df_cleaned.drop(columns=['r_quartile', 'f_quartile'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Time Features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['Month'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.month)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['Month'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['Weekday'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.weekday())\ndf_cleaned['Day'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.day)\ndf_cleaned['Hour'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.hour)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Product Categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_cleaned[\"Description\"].unique()\n\nstemmer = nltk.stem.porter.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\n\ndef stem_and_filter(doc):\n    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n    return [token for token in tokens if token.isalpha()]\n\nanalyzer = TfidfVectorizer().build_analyzer()\nCV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\nTF_IDF_matrix = CV.fit_transform(X)\nprint(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\n\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\nprint(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_tfidf = []\n\nx = list(range(5, 155, 10))\n\nfor n_clusters in x:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n    kmeans.fit(TF_IDF_embedded)\n    clusters = kmeans.predict(TF_IDF_embedded)\n    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)\n\n    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n    score_tfidf.append(silhouette_avg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,16))\n\nplt.subplot(2, 1, 1)\nplt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\nplt.title(\"Evolution of the Silhouette Score\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The highest value for the silhouette score is when there are 135 clusters. So we'll chose this value.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 135\n\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\nproj = kmeans.fit_transform(TF_IDF_embedded)\nclusters = kmeans.predict(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"ACP with 135 clusters\", fontsize=\"20\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2)\nproj = tsne.fit_transform(TF_IDF_embedded)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nwc = WordCloud()\n\nfor num, cluster in enumerate(random.sample(range(100), 12)) :\n    plt.subplot(3, 4, num+1)\n    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(\"Cluster {}\".format(cluster))\n    plt.axis(\"off\")\nplt.figure()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(clusters).hist(bins=100)\n\n\ndict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('product_clusters.pickle', 'wb') as h:\n    pickle.dump(dict_article_to_cluster, h)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}