{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div ><h1 style = \"borden-padding:20px\"> Learning clustering algorithms  </h1>\n<h2> Clustering the redwine </h2>\n\n<h3> Checking the various clusters using clustering algorithms </h3>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<div><h1 style = \"borden-padding:20px\"> Learning clustering algorithms  </h1>\n<h2> The ten top clustering algorithms the scikit-learn library </h2>\n\n<ul>\n    <li>Affinity Propagation</li>\n    <li>Agglomerative Clustering</li>\n    <li>BIRCH</li>\n    <li>DBSCAN</li>\n    <li>K-Means</li>\n    <li>Mini-Batch K-Means</li>\n    <li>Mean Shift</li>\n    <li>OPTICS</li>\n    <li>Spectral Clustering</li>\n    <li>AMixture of Gaussians</li>\n    \n</ul>\n</div>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n<div style = \"background-color: #FF005F; color:white; font:Arial\"><h1 style = \"borden-padding:20px\"> Conclusion  </h1>\n<h2> It was concluded that the qualification of the wines does not say much about their quality, all models presented groups of wines the opposite of those informed by the dataset. </h2>\n\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df.corr()\ndf_corr['quality'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(df.quality)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,:11]\ny = df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA\n\nThe PCA will combine statistics and linear algebra to produce a pre-processing step that can help reduce dimensionality, which can be the enemy of a simple model.The PCA works by mapping the original data set into a new space where each of the new column array vectors is orthogonal. From a data analysis perspective, the PCA transforms the covariance matrix of the data into column vectors that can explain certain percentages of the variance."},{"metadata":{},"cell_type":"markdown","source":"In this case, 12 columns of this base, 8 components explain 94% of the data, which statistically solves, and thus, we can improve the processing by working with less information."},{"metadata":{},"cell_type":"markdown","source":"To represent the graphs we chose the two vectors with the greatest representativeness, which would be the vectors sulphates and alcohol."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_normalized = StandardScaler().fit(X).transform(X) \npca = PCA(n_components = 8).fit(X_normalized)\nX_pca = pca.transform(X_normalized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"candi = ['nota_3','nota_4','nota_5','nota_6','nota_7','nota_8']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grupo = np.unique(df.quality)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grupo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor i in grupo:\n    row_ix = np.where(df.quality == i)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n  \n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the number of cluster"},{"metadata":{},"cell_type":"markdown","source":"You can see that the data shows that we can work with 6 clusters until the distances of the quadratic errors practically stabilize."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\n\nplt.figure(figsize=(18,10))\nfor i in range(1, 20):\n    kmeans = KMeans(n_clusters = i, init = 'random')\n    kmeans.fit(X_pca)\n    print (i,kmeans.inertia_)\n    wcss.append(kmeans.inertia_)  \nplt.plot(range(1, 20), wcss)\nplt.plot([6],[8437], 'ro')\nplt.annotate('number cluster', xy=(6, 8437), xytext=(7, 8000),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             )\nplt.title('O Metodo Elbow')\nplt.xlabel('Numero de Clusters')\nplt.ylabel('WSS') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Affinity Propagation\n\n"},{"metadata":{},"cell_type":"markdown","source":"The algorithmic complexity of affinity propagation is quadratic in the number of points.\n\nWhen fit does not converge, cluster_centers_ becomes an empty array and all training samples will be labelled as -1. In addition, predict will then label every sample as -1.\n\nWhen all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, fit will result in a single cluster center and label 0 for every sample. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"candi = ['cluste_1','cluste_2','cluste_3','cluste_4','cluste_5','cluste_6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AffinityPropagation\nmodel = AffinityPropagation()\nmodel.fit(X_normalized)\nyhat = model.predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Agglomerative Clustering"},{"metadata":{},"cell_type":"markdown","source":"The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n\nWard minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n\nMaximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n\nAverage linkage minimizes the average of the distances between all observations of pairs of clusters.\n\nSingle linkage minimizes the distance between the closest observations of pairs of clusters.\n\nAgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nmodel = AgglomerativeClustering(n_clusters=6)\nyhat = model.fit_predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BIRCH"},{"metadata":{},"cell_type":"markdown","source":"The Birch builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\n\nThe CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:\n\nNumber of samples in a subcluster.\n\nLinear Sum - A n-dimensional vector holding the sum of all samples\n\nSquared Sum - Sum of the squared L2 norm of all samples.\n\nCentroids - To avoid recalculation linear sum / n_samples.\n\nSquared norm of the centroids.\n\nThe Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.\n\nThis algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import Birch\nmodel = Birch(threshold=0.01, n_clusters=6)\nmodel.fit(X)\nyhat = model.predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DBSCAN"},{"metadata":{},"cell_type":"markdown","source":"The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster.\n\nMore formally, we define a core sample as being a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.\n\nAny core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm.\n\nWhile the parameter min_samples primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter eps is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot.\n\nfonte:scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nmodel = DBSCAN(eps=0.30, min_samples=4)\nyhat = model.fit_predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix,1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means"},{"metadata":{},"cell_type":"markdown","source":"The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of N samples X into K disjoint clusters , each described by the mean of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from X, although they live in the same space.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=6)\nmodel.fit(X)\nyhat = model.predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mini-Batch K-Means"},{"metadata":{},"cell_type":"markdown","source":"The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.\n\nThe algorithm iterates between two major steps, similar to vanilla k-means. In the first step,  samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nmodel = MiniBatchKMeans(n_clusters=6)\nmodel.fit(X)\nyhat = model.predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Shift"},{"metadata":{},"cell_type":"markdown","source":"MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MeanShift\nmodel = MeanShift()\nyhat = model.fit_predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OPTICS"},{"metadata":{},"cell_type":"markdown","source":"The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a reachability_ distance, and a spot within the cluster ordering_ attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for max_eps, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given eps value using the cluster_optics_dbscan method. Setting max_eps to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import OPTICS\nmodel = OPTICS(eps=0.3, min_samples=9)\nyhat = model.fit_predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spectral Clustering"},{"metadata":{},"cell_type":"markdown","source":"SpectralClustering performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space. It is especially computationally efficient if the affinity matrix is sparse and the amg solver is used for the eigenvalue problem (Note, the amg solver requires that the pyamg module is installed.)\n\nfonte: scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\nmodel = SpectralClustering(n_clusters=6)\nyhat = model.fit_predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Mixture Model"},{"metadata":{},"cell_type":"markdown","source":"This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the BayesianGaussianMixture class models with a Dirichlet distribution prior (weight_concentration_prior_type='dirichlet_distribution') and a Dirichlet process prior (weight_concentration_prior_type='dirichlet_process'). On each figure, we plot the results for three different values of the weight concentration prior.\n\nThe BayesianGaussianMixture class can adapt its number of mixture components automatically. The parameter weight_concentration_prior has a direct link with the resulting number of components with non-zero weights. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.\n\nfonte:scikit-learn.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nmodel = GaussianMixture(n_components=6)\nmodel.fit(X)\nyhat = model.predict(X)\nclusters = np.unique(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nfor cluster in clusters:\n    row_ix = np.where(yhat == cluster)\n    pyplot.scatter(X_pca[row_ix, 0], X_pca[row_ix, 1])\n    plt.legend(candi,loc='upper right')\n\npyplot.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}