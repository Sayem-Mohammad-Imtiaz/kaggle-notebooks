{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Letter Embeddings\n* Notebook:https://github.com/RasaHQ/algorithm-whiteboard-resources/blob/master/letter-embeddings/algo_whiteboard_letter_embeddings_v2.ipynb\n* Video: https://www.youtube.com/watch?v=mWvnlVw_LiY&amp=&amp;index=5\n\nFor some reason doesn't use GPU. Even if GPU accelerator is enabled and tensorflow can see GPU ¯\\_(ツ)_/¯","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"P-TZJ-GsQ6Ux","outputId":"d772196d-e332-4750-f89c-3533795db3ed","trusted":true},"cell_type":"code","source":"try:\n  # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{"id":"jP3k0ljEQgp1","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"id":"mqLHeTh_UpTJ"},"cell_type":"markdown","source":"## Fetching the Data \n\nThis is a bit annoying. But to download from kaggle we need to upload the API key here. Then we need to move the file to the correct folder after which we need to change the permissions. The error messages will not provide helpful information. Then again, this code works;","execution_count":null},{"metadata":{"id":"kYxYOwUhU8YC","trusted":true},"cell_type":"code","source":"import pandas as pd\nheadlines = pd.read_csv('/kaggle/input/million-headlines/abcnews-date-text.csv')['headline_text']","execution_count":null,"outputs":[]},{"metadata":{"id":"ttXDwNveVRt6"},"cell_type":"markdown","source":"## Sequence of Letters \n\nLet's now take these headlines and grab sequences of letters out of them.","execution_count":null},{"metadata":{"id":"banoSM0VWuRG","outputId":"7a4a5969-6a6a-4bbf-8ee0-5e86d994c2d9","trusted":true},"cell_type":"code","source":"headlines[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"EwVCjxOPV1Hi","trusted":true},"cell_type":"code","source":"import itertools as it \n\ndef sliding_window(txt):\n    for i in range(len(txt) - 2):\n        yield txt[i], txt[i + 1], txt[i + 2]\n\nwindow = list(it.chain(*[sliding_window(_) for _ in headlines[:10000]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for win in window[:10]:\n    print(win)","execution_count":null,"outputs":[]},{"metadata":{"id":"K_Elh9ezX1c6","trusted":true},"cell_type":"code","source":"mapping = {c: i for i, c in enumerate(pd.DataFrame(window)[0].unique())}\nintegers_in = np.array([[mapping[w[0]], mapping[w[1]]] for w in window])\nintegers_out = np.array([mapping[w[2]] for w in window]).reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping","execution_count":null,"outputs":[]},{"metadata":{"id":"Zaj-gjdAYzoA","outputId":"3f5d2198-6004-4f16-f53c-72a23d8aee1b","trusted":true},"cell_type":"code","source":"integers_in.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Pmt1k0YhQi0m","outputId":"47cd0975-f3c0-470c-f2dd-3aa12f9b23f5","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, Dense, Flatten\nfrom tensorflow.keras.models import Sequential\n\nnum_letters = len(mapping) # typically 36 -> 26 letters + 10 numbers\n\n# this one is so we might grab the embeddings\nmodel_emb = Sequential()\nembedding = Embedding(num_letters, 2, input_length=2)\nmodel_emb.add(embedding)\noutput_array = model_emb.predict(integers_in)\noutput_array.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initalized Letters Visualization\nShows the \"random\" distribtuion of letters after initalization","execution_count":null},{"metadata":{"id":"6SCmddYQjiTq","outputId":"3a85314e-fd5d-420a-edb2-680cc4f11cdf","trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\n\nidx_to_calc = list(mapping.values())\nidx_to_calc = np.array([idx_to_calc, idx_to_calc]).T\n\ntranslator = {v:k for k,v in mapping.items()}\npreds = model_emb.predict(idx_to_calc)\n\nplt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\nfor i, idx in enumerate(idx_to_calc):\n    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])","execution_count":null,"outputs":[]},{"metadata":{"id":"pEmpigx8aNEu","outputId":"0f81f5f7-87ee-4115-b523-74681b24fbfd","trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# this one is so we might learn the mapping\nmodel_pred = Sequential()\nmodel_pred.add(embedding)\nmodel_pred.add(Flatten())\nmodel_pred.add(Dense(num_letters, activation=\"softmax\"))\n\nadam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nmodel_pred.compile(adam, 'categorical_crossentropy', metrics=['accuracy'])\n\noutput_array = model_pred.predict(integers_in)\noutput_array.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"c5e5KjaSRfIn","outputId":"5a024763-db4e-478b-aec3-d9644e31aafd","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nto_predict = OneHotEncoder(sparse=False).fit_transform(integers_out)\nmodel_pred.fit(integers_in, to_predict, epochs=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learned Letters Relationship Visualization\nShows the learned distribtuion of letters after using previous 2 letters to predict 3rd","execution_count":null},{"metadata":{"id":"15mlBdDOcmNx","outputId":"5d4682e4-fd64-484b-e2cc-ff86bdaf2871","trusted":true},"cell_type":"code","source":"preds = model_emb.predict(idx_to_calc)\nplt.scatter(preds[:, 0, 0], preds[:, 0, 1], alpha=0)\nfor i, idx in enumerate(idx_to_calc):\n    plt.text(preds[i, 0, 0], preds[i, 0, 1], translator[idx[0]])","execution_count":null,"outputs":[]},{"metadata":{"id":"QQp8gi3fftUz","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}