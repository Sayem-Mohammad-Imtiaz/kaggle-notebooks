{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of contents\n\n1. Problem description\n2. Environment preparation, data import\n3. Data review\n4. Data transformation\n    - Bag of Words approach\n    - TF-IDF approach\n    - Word2Vec approach\n5. Baseline modelling\n6. Baseline result analysis\n7. Dataset adjustment\n8. Selecting the best model\n    - Naive Bayes classifier\n    - Logistic Regression classifier\n    - LightGBM classifier\n9. Best model result analysis\n10. Retrieving sample unbalance problem\n    - Undersampling\n        - Naive random cut\n        - NearMiss\n    - Oversampling\n        - SMOTE\n        - ADASYN\n11. Best model result analysis\n12. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"# 1. Problem description"},{"metadata":{},"cell_type":"markdown","source":"<b>Diagnostics prediction</b> is a project which aims to automatically predict diagnostics needed for a patient with certain anamnesis. \n\nThe anamnesis is represented by a raw text file with doctor's notes about the patient, including his/her age, compaints described in free way, patient's history and so on. It is unstructured - different sections of oen patient anamnesis may abscent in another's.\n\nThe target labels are represented by the name of needed diagnostics procedure.\n\n<b>The value</b> on the solution might be found in helping a doctor to find the optimal solution for diasnostics order. Patient can save time and money, and doctor can serve a patient more efficiently on sparing time for unnecessary diagnostics. Moreover, in difficult cases the algorithm may help a doctor to find a diagnosys faster, which in some cases may be extremely valuable, up to saving lives.\n\nTheoretically some regularities found by the algorithm may help medical researchers to find the idea of treating some deseases, based on their unobvious interconnections with some symptoms."},{"metadata":{},"cell_type":"markdown","source":"# 2. Environment preparation, data import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic libraries\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport nltk\nimport random\nimport itertools\nfrom collections import defaultdict\n\n# Preprocessing\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom itertools import combinations\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# Models\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegressionCV\nimport lightgbm as lgb\n\n# Evaluation\n\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\nfrom lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nfrom lime.lime_text import LimeTextExplainer\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mtsamples.csv')\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains of several categories, but we will need only <b>transcription</b>, which is the anamnesis, and <b>medical_specialty</b>, which is the target disgnostics type."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['transcription', 'medical_specialty']]\ndata = data.drop(data[data['transcription'].isna()].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data review"},{"metadata":{},"cell_type":"markdown","source":"Let's look at some item of the transcription"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[4]['transcription']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are some constructions, which must be treated in order to clean the data.\n\nNow look at the target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['medical_specialty'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes are deeply imbalanced. For 5000 objects it might be too much. We will check it later.\n\nLet's check the sentence structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\n# Create a list of tokens for each sentence\ntokenizer = RegexpTokenizer(r'\\w+')\ndata[\"tokens\"] = data[\"transcription\"].apply(tokenizer.tokenize)\n\nall_words = [word for tokens in data[\"tokens\"] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in data[\"tokens\"]]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10)) \nplt.title('Sentence length histogram')\nplt.xlabel('Sentence length')\nplt.ylabel('Number of sentences')\nplt.hist(sentence_lengths, edgecolor = 'black', bins = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Median sentence length {np.median(sentence_lengths)}\")\nprint(f\"Mean sentence length {round(np.mean(sentence_lengths), 2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average sentence is long enough: median and mean values are over 400 words."},{"metadata":{},"cell_type":"markdown","source":"# 4. Data transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text preparation\n\ndef basic_preprocessing(df):\n    \n    df_temp = df.copy(deep = True)\n    \n    df_temp = df_temp.rename(index = str, columns = {'transcription': 'text'})\n    \n    df_temp.loc[:, 'text'] = [text_prepare(x) for x in df_temp['text'].values]\n    \n    le = LabelEncoder()\n    le.fit(df_temp['medical_specialty'])\n    df_temp.loc[:, 'class_label'] = le.transform(df_temp['medical_specialty'])\n    \n    tokenizer = RegexpTokenizer(r'\\w+')\n\n    df_temp[\"tokens\"] = df_temp[\"text\"].apply(tokenizer.tokenize)\n    \n    return df_temp\n\ndef text_prepare(text):\n\n    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n    STOPWORDS = set(stopwords.words('english'))\n    \n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub('', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    words = text.split()\n    i = 0\n    while i < len(words):\n        if words[i] in STOPWORDS:\n            words.pop(i)\n        else:\n            i += 1\n    text = ' '.join(map(str, words))# delete stopwords from text\n    \n    return text\n\n# Introduce evaluation metrics\n\ndef get_metrics(y_test, y_predicted):  \n\n    precision = precision_score(y_test, y_predicted, average='weighted')             \n\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    \n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    \n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Bag of words"},{"metadata":{},"cell_type":"markdown","source":"First we apply basic preprocessing function, which splits the words, removes unvaluable symbols and drop stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"def BOW(data):\n    \n    df_temp = data.copy(deep = True)\n    df_temp = basic_preprocessing(df_temp)\n\n    count_vectorizer = CountVectorizer()\n    count_vectorizer.fit(df_temp['text'])\n\n    list_corpus = df_temp[\"text\"].tolist()\n    list_labels = df_temp[\"class_label\"].tolist()\n    \n    X = count_vectorizer.transform(list_corpus)\n    \n    return X, list_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"For TD-IDF we use the same approach as before. The difference is that we use TF-IDF vectorizer instead of count vectorizer. We will check 1-grams only and mix of 1- and 2-grams."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(data, ngrams = 1):\n\n    df_temp = data.copy(deep = True)\n    df_temp = basic_preprocessing(df_temp)\n    \n    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngrams))\n    tfidf_vectorizer.fit(df_temp['text'])\n\n    list_corpus = df_temp[\"text\"].tolist()\n    list_labels = df_temp[\"class_label\"].tolist()\n\n    X = tfidf_vectorizer.transform(list_corpus)\n    \n    return X, list_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Word2Vec"},{"metadata":{},"cell_type":"markdown","source":"For Word2Vec the approach is the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n                                                                                generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def w2v(data):\n    \n    df_temp = data.copy(deep = True)    \n    df_temp = basic_preprocessing(df_temp)\n    \n    embeddings = get_word2vec_embeddings(word2vec, df_temp)\n    list_labels = df_temp[\"class_label\"].tolist()\n    \n    return embeddings, list_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Baseline modelling"},{"metadata":{},"cell_type":"markdown","source":"As baseline model we'll choose LogisticRegression as one of the most verstily one. Calculating the score metrics for 4 ways described before: Bag of words, TF-IDF with 1-gram, TF-IDF with 2-gram, word2vec."},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=3, shuffle=True, random_state = 40)\n\nclf = LogisticRegressionCV(cv = folds, solver = 'saga', \n                           multi_class = 'multinomial', n_jobs = -1, random_state = 40)\n\ndf_res = pd.DataFrame(columns = ['Preprocessing', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\n# Bag of words approach\nX, y = BOW(data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\ndf_res = df_res.append({'Preprocessing': 'Bag of words',\n                       'Precision': precision,\n                       'Recall': recall,\n                       'F1-score': f1,\n                       'Accuracy': accuracy}, ignore_index = True)\n\n# TF_IDF approach. 1-gram\nX, y = tfidf(data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\ndf_res = df_res.append({'Preprocessing': 'TF-IDF 1-gram',\n                       'Precision': precision,\n                       'Recall': recall,\n                       'F1-score': f1,\n                       'Accuracy': accuracy}, ignore_index = True)\n\n# TF_IDF approach. 2-gram\nX, y = tfidf(data, ngrams=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\ndf_res = df_res.append({'Preprocessing': 'TF-IDF 2-gram',\n                       'Precision': precision,\n                       'Recall': recall,\n                       'F1-score': f1,\n                       'Accuracy': accuracy}, ignore_index = True)\n\n# Word2vec\nX, y = w2v(data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\ndf_res = df_res.append({'Preprocessing': 'Word2vec',\n                       'Precision': precision,\n                       'Recall': recall,\n                       'F1-score': f1,\n                       'Accuracy': accuracy}, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Baseline result analysis"},{"metadata":{},"cell_type":"markdown","source":"As we can see, quality metrics vary a lot. So we need to choose the one we will be targeting on.\n\n- Accuracy is not a good choice as due to very unbalanced class maximizing it will force alorithm to predict the major one;\n- Precision indicates how much we can trust the algorithm when it predicts a certain class;\n- Recall shows how many of the true labels of the class are predicted correctly by the algorithm;\n- F1-score is the harmonic mean of precision and recall.\n\nAs F1-score is the most balanced one of the metrics above, we will use it primarily, but still keeping an eye on others."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The major classes are predicted quite well, but minor ones are confused a lot. Most possibly the reason is not only in unbalanceness of the classes, but in a very few absolute number of minor classes. Doubtfully the algorithm can gain some dependancies on 6 or 10 objects of one class.\n\nSo in order to make the model better we will get rid on the classes which are represented by less than 100 objects. For those we would need to get additional data."},{"metadata":{},"cell_type":"markdown","source":"# 7. Dataset adjustment"},{"metadata":{},"cell_type":"markdown","source":"Dropping object with number of observations less than 100."},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = data['medical_specialty'].value_counts()\ndata_100 = data[data['medical_specialty'].isin(counts[counts > 100].index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_100['labels'] = LabelEncoder().fit_transform(data_100['medical_specialty'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_100['medical_specialty'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imbalance is still big, but this measure should help us get better results"},{"metadata":{},"cell_type":"markdown","source":"# 8. Selecting the best model"},{"metadata":{},"cell_type":"markdown","source":"Now having the baseline model on full dataset we will try to improve it using adjusted dataset and three of the most suitable algorithms: Naive Bayes, Logistic Regression and LightGBM."},{"metadata":{},"cell_type":"markdown","source":"## 8.1. Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_naive(X_train_naive, X_test_naive, y_train_naive, y_test_naive, preproc):\n    \n    clf = MultinomialNB()\n    clf.fit(X_train_naive, y_train_naive)\n\n    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n    \n    y_pred = clf.predict(X_test_naive)\n    \n    f1 = f1_score(y_pred, y_test_naive, average = 'weighted')\n    pres = precision_score(y_pred, y_test_naive, average = 'weighted')\n    rec = recall_score(y_pred, y_test_naive, average = 'weighted')\n    acc = accuracy_score(y_pred, y_test_naive)\n    \n    res = res.append({'Preprocessing': preproc, 'Model': 'Naive Bayes', 'Precision': pres, \n                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.2. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_logreg(X_train_log, X_test_log, y_train_log, y_test_log, preproc):\n    \n    folds = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 40)\n    \n    clf = LogisticRegressionCV(cv = folds, solver = 'saga', multi_class = 'multinomial', n_jobs = -1)\n    \n    clf.fit(X_train_log, y_train_log)\n\n    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n    \n    y_pred = clf.predict(X_test_log)\n    \n    f1 = f1_score(y_pred, y_test_log, average = 'weighted')\n    pres = precision_score(y_pred, y_test_log, average = 'weighted')\n    rec = recall_score(y_pred, y_test_log, average = 'weighted')\n    acc = accuracy_score(y_pred, y_test_log)\n    \n    res = res.append({'Preprocessing': preproc, 'Model': f'Logistic Regression', 'Precision': pres, \n                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.3. LightGBM"},{"metadata":{},"cell_type":"markdown","source":"For LightGBM first we need make a pretune for the model to perform close to well"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_lgbm(X_train_lgbm, X_test_lgbm, y_train_lgbm, y_test_lgbm, preproc,\n                 n = 120, depth = 4, child = 2, bag = 0.9, feature = 0.9, l1 = 1, l = 0.01, w2v = False):\n    \n    if w2v:\n        d_train = lgb.Dataset(X_train_lgbm, label=y_train_lgbm)\n    else:\n        d_train = lgb.Dataset(X_train_lgbm.astype(np.float32), label=y_train_lgbm)\n\n    early_stop = 500\n    verbose_eval = False\n    num_rounds = n\n\n    params = {\n        'objective': 'multiclass',\n        'num_class': len(set(y_train_lgbm)),\n        'boosting': 'gbdt', \n        'metric': 'multi_logloss',\n        'max_depth': depth, \n        'max_bin': 22, \n        'bagging_fraction': bag, \n        'feature_fraction': feature, \n        'min_child_samples': child, \n        'min_child_weight': 1, \n        'learning_rate': l,\n        'verbosity': -1, \n        'data_random_seed': 17,\n        'lambda_l1': l1}\n\n    model = lgb.train(params, train_set = d_train, num_boost_round = num_rounds)\n\n    y_pred_proba = model.predict(X_test_lgbm.astype(np.float32), num_iteration=model.best_iteration)\n    y_pred = [np.argmax(x) for x in y_pred_proba]\n    \n    res = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n        \n    f1 = f1_score(y_pred, y_test_lgbm, average = 'weighted')\n    pres = precision_score(y_pred, y_test_lgbm, average = 'weighted')\n    rec = recall_score(y_pred, y_test_lgbm, average = 'weighted')\n    acc = accuracy_score(y_pred, y_test_lgbm)\n    \n    res = res.append({'Preprocessing': preproc, 'Model': 'LightGBM', 'Precision': pres, \n                     'Recall': rec, 'F1-score': f1, 'Accuracy': acc}, ignore_index = True)\n\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune number of estimator at learning rate = 0.01\n\nX, y = BOW(data_100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nns = range(80, 301, 20)\nrecalls = []\nfor n in ns:\n    recalls.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count', n = n, l = 0.01)['Recall'].values[0])\nfig = plt.figure(figsize = (10, 8))\nplt.plot(ns, recalls, label = 'Recall')\nplt.xlabel('Number of estimators')\nplt.ylabel('Recall')\nplt.title('LightGBM recall dependance on number of estimators with learning rate equal to 0.01')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune maximum tree depth and minimum elements in leaf\n\ndepths = range(2, 7, 2)\nchilds = range(2, 7, 2)\nrecalls = []\nfor depth in depths:\n    for child in childs:\n        recalls.append([training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-gram', \n                                     n = 120, l = 0.01, depth = depth, child = child)['Recall'].values[0], depth, child])\nrecalls.sort(reverse = True)\nrecalls[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimal parameters for preliminary tuning are:\n - number of estimators = 120\n - learning rate = 0.01\n - maximum tree depth = 4\n - minimum samples in leaf = 2"},{"metadata":{},"cell_type":"markdown","source":"## 8.4. Calculations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for result evaluation\n\nfull_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\n# Testing Count Vectorizer\n\nX, y = BOW(data_100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nfull_result = full_result.append(training_naive(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nfull_result = full_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nfull_result = full_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\n\n# Testing TF-IDF with 1-gram\n\nX, y = tfidf(data_100, ngrams = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nfull_result = full_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nfull_result = full_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nfull_result = full_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\n\n# Testing TF-IDF with 2-gram\n\nX, y = tfidf(data_100, ngrams = 2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nfull_result = full_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 2-grams'), ignore_index = True)\nfull_result = full_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 2-grams'), ignore_index = True)\nfull_result = full_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 2-grams'), ignore_index = True)\n\n# Testing Word2vec\n\nX, y = w2v(data_100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nfull_result = full_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Word2vec'), ignore_index = True)\n\nfull_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For basic setup all data preprocessing approaches work closely well. The the combination of the exact algorithm and preprocessing method make the difference. F1-score is the highest for Naive Bayes for 1-gram TF-IDF approach. But mostly the advantage is in high precision score. Recall is relatively low. The overall best approach seems to be Word2vec with LogisticRegression.\n\nWe will confirm or reject this hypothesis in later calculations."},{"metadata":{},"cell_type":"markdown","source":"# 9. Best model result analysis\n"},{"metadata":{},"cell_type":"markdown","source":"The scores became remarkably higher. Let us take a look on confusion matrix for the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-setup the algorithm\n\nX, y = w2v(data_100)\nX_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X, y, test_size=0.2, random_state=40)\nfolds = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 40)\nclf_w2v = LogisticRegressionCV(cv = folds, solver = 'saga', multi_class = 'multinomial', n_jobs = -1)\nclf_w2v.fit(X_train_log, y_train_log)\ny_pred = clf_w2v.predict(X_test_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.winter):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=10, rotation = 90)\n    plt.yticks(tick_marks, classes, fontsize=10)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=12)\n    \n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=20)\n    plt.xlabel('Predicted label', fontsize=20)\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = [' Cardiovascular / Pulmonary', ' Consult - History and Phy.', ' Discharge Summary',\n           ' Gastroenterology', ' General Medicine', ' Neurology', ' Obstetrics / Gynecology',\n           ' Orthopedic', ' Radiology', ' SOAP / Chart / Progress Notes', ' Surgery', ' Urology']\ncm = confusion_matrix(y_test_log, y_pred)\nfig = plt.figure(figsize=(16, 16))\nplot = plot_confusion_matrix(cm, classes=classes, normalize=False, \n                             title='Confusion matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to see how the model works is to manually watch at the most valuable and the least valuable words for the class."},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_store = word2vec\ndef word2vec_pipeline(examples):\n    global vector_store\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokenized_list = []\n    for example in examples:\n        example_tokens = tokenizer.tokenize(example)\n        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n        tokenized_list.append(vectorized_example)\n    return clf_w2v.predict_proba(tokenized_list)\n\nc = make_pipeline(CountVectorizer(), clf)\n\nrandom.seed(40)\n\ndata_100_prep = basic_preprocessing(data_100)\nlist_corpus = data_100_prep[\"text\"].tolist()\nlist_labels = data_100_prep[\"class_label\"].tolist()\n\nX_train_data, X_test_data, y_train_data, y_test_data = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=40)\n\ndef get_statistical_explanation(test_set, sample_size, word2vec_pipeline, label_dict):\n    sample_sentences = random.sample(test_set, sample_size)\n    explainer = LimeTextExplainer()\n    \n    labels_to_sentences = defaultdict(list)\n    contributors = defaultdict(dict)\n    \n    # First, find contributing words to each class\n    for sentence in sample_sentences:\n        probabilities = word2vec_pipeline([sentence])\n        curr_label = probabilities[0].argmax()\n        labels_to_sentences[curr_label].append(sentence)\n        exp = explainer.explain_instance(sentence, word2vec_pipeline, num_features=6, labels=[curr_label])\n        listed_explanation = exp.as_list(label=curr_label)\n        \n        for word,contributing_weight in listed_explanation:\n            if word in contributors[curr_label]:\n                contributors[curr_label][word].append(contributing_weight)\n            else:\n                contributors[curr_label][word] = [contributing_weight]    \n    \n    # average each word's contribution to a class, and sort them by impact\n    average_contributions = {}\n    sorted_contributions = {}\n    for label,lexica in contributors.items():\n        curr_label = label\n        curr_lexica = lexica\n        average_contributions[curr_label] = pd.Series(index=curr_lexica.keys())\n        for word,scores in curr_lexica.items():\n            average_contributions[curr_label].loc[word] = np.sum(np.array(scores))/sample_size\n        detractors = average_contributions[curr_label].sort_values()\n        supporters = average_contributions[curr_label].sort_values(ascending=False)\n        sorted_contributions[label_dict[curr_label]] = {\n            'detractors':detractors,\n             'supporters': supporters\n        }\n    return sorted_contributions\n\nlabel_to_text = {\n    0: ' Cardiovascular / Pulmonary',\n    1: ' Consult - History and Phy.',\n    2: ' Discharge Summary',\n    3: ' Gastroenterology',\n    4: ' General Medicine',\n    5: ' Neurology',\n    6: ' Obstetrics / Gynecology',\n    7: ' Orthopedic',\n    8: ' Radiology',\n    9: ' SOAP / Chart / Progress Notes',\n    10: ' Surgery',\n    11: ' Urology'\n}\nsorted_contributions = get_statistical_explanation(X_test_data, 100, word2vec_pipeline, label_to_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n    y_pos = np.arange(len(top_words))\n    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n    \n    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n    \n    top_words = [a[0] for a in top_pairs]\n    top_scores = [a[1] for a in top_pairs]\n    \n    bottom_words = [a[0] for a in bottom_pairs]\n    bottom_scores = [a[1] for a in bottom_pairs]\n    \n    fig = plt.figure(figsize=(10, 10))  \n\n    plt.subplot(121)\n    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n    plt.title('Irrelevant', fontsize=14)\n    plt.yticks(y_pos, bottom_words, fontsize=10)\n    plt.suptitle('Key words', fontsize=12)\n    plt.xlabel('Importance', fontsize=12)\n    \n    plt.subplot(122)\n    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n    plt.title(\"Relevant for \" + name, fontsize=14)\n    plt.yticks(y_pos, top_words, fontsize=12)\n    plt.suptitle(f\"Most important words for {name}\", fontsize=16)\n    plt.xlabel('Importance', fontsize=12)\n    \n    plt.subplots_adjust(wspace=0.8)\n    plt.show()\n\ntest_label = ' Neurology'\ntop_words = sorted_contributions[test_label]['supporters'][:5].index.tolist()\ntop_scores = sorted_contributions[test_label]['supporters'][:5].tolist()\nbottom_words = sorted_contributions[test_label]['detractors'][:5].index.tolist()\nbottom_scores = sorted_contributions[test_label]['detractors'][:5].tolist()\n\nplot_important_words(top_scores, top_words, bottom_scores, bottom_words, test_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_label = ' Cardiovascular / Pulmonary'\ntop_words = sorted_contributions[test_label]['supporters'][:5].index.tolist()\ntop_scores = sorted_contributions[test_label]['supporters'][:5].tolist()\nbottom_words = sorted_contributions[test_label]['detractors'][:5].index.tolist()\nbottom_scores = sorted_contributions[test_label]['detractors'][:5].tolist()\n\nplot_important_words(top_scores, top_words, bottom_scores, bottom_words, test_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words influence on relevancy seems to be truthful. Which can mean that we are going in the right direction.\n\nLet us continue with fixing the class imbalance."},{"metadata":{},"cell_type":"markdown","source":"# 10. Retrieving sample unbalance problem"},{"metadata":{},"cell_type":"markdown","source":"We can see that classes imbalance still prevents us from getting a better result.\n\nThere are several ways to fight imbalance:\n\n1. Undersampling\n2. Oversampling\n3. Use classifiers with class-weight adjustments\n\nThe last one we already did with logistic regression.\n\nUndersampling is cutting off some observations of the major classes to make them more comparable in count with the minor ones. Due to the amount of data we have, such approach might not be profitable, as we make the amount of data even smaller, which might affect model quality. But still we can try.\nSimple random drop of some observations share based on the assumption that dropping some samples will not affect he distribution. Randomness of the samples pick and small amount of data doesn't ensure it. So we need to apply more complex methods.\n\nOversampling, counterwise to undersampling, is adding more copies of observations from minor classes to straighted classes distribution. As we are working with text data, doubling some samples will probably lead to overfitting on these samples. So, just like in the case of undersampling we will need more complex approach."},{"metadata":{},"cell_type":"markdown","source":"## 10.1. Undersampling. Naive approach"},{"metadata":{},"cell_type":"markdown","source":"First we check how many observation the undersampling algorithm cuts off"},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_under_sample = RandomUnderSampler(sampling_strategy = 'majority')\nX, y = BOW(data_100)\nprint(f\"Initial set observations {X.shape[0]}\")\nprint(f\"Initial set target classes {len(set(y))}\")\nX, y = naive_under_sample.fit_resample(X, y)\nprint(f\"Modified set observations {X.shape[0]}\")\nprint(f\"Modified set target classes {len(set(y))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for result evaluation\n\nnaive_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\nnaive_under_sample = RandomUnderSampler(sampling_strategy = 'majority')\n\n# Testing Count Vectorizer\n\nX, y = BOW(data_100)\nX, y = naive_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnaive_result = naive_result.append(training_naive(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nnaive_result = naive_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nnaive_result = naive_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\n\n# Testing TF-IDF with 1-gram\n\nX, y = tfidf(data_100, ngrams = 1)\nX, y = naive_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnaive_result = naive_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nnaive_result = naive_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nnaive_result = naive_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\n\n# Testing Word2vec\n\nX, y = w2v(data_100)\nX, y = naive_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnaive_result = naive_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Word2vec'), ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One can see, that naive approach didn't make any significant gain in metrics. Recall became better in overall, but precision dropped. F1-score therefore slightly decreased.\n\nAs an alternative we will try NearMiss method."},{"metadata":{},"cell_type":"markdown","source":"## 10.2. Undersampling. NearMiss"},{"metadata":{},"cell_type":"markdown","source":"How many observation the undersampling algorithm cuts off."},{"metadata":{"trusted":true},"cell_type":"code","source":"near_under_sample = NearMiss(sampling_strategy='majority')\nX, y = BOW(data_100)\nprint(f\"Initial set observations {X.shape[0]}\")\nprint(f\"Initial set target classes {len(set(y))}\")\nX, y = near_under_sample.fit_resample(X, y)\nprint(f\"Modified set observations {X.shape[0]}\")\nprint(f\"Modified set target classes {len(set(y))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for result evaluation\n\nnear_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\nnear_under_sample = NearMiss(sampling_strategy='majority')\n\n# Testing Count Vectorizer\n\nX, y = BOW(data_100)\nX, y = near_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnear_result = near_result.append(training_naive(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nnear_result = near_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nnear_result = near_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\n\n# Testing TF-IDF with 1-gram\n\nX, y = tfidf(data_100, ngrams = 1)\nX, y = near_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnear_result = near_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nnear_result = near_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nnear_result = near_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\n\n# Testing Word2vec\n\nX, y = w2v(data_100)\nX, y = near_under_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nnear_result = near_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Word2vec'), ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"near_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NearMiss method gave almost the same result as naive approach. Which may treat like naive random undersampling have basically preserved the orginal distribution."},{"metadata":{},"cell_type":"markdown","source":"## 10.3. Oversampling. SMOTE"},{"metadata":{},"cell_type":"markdown","source":"For oversampling algorithm check how many observation the algorithm generates."},{"metadata":{"trusted":true},"cell_type":"code","source":"smote_over_sample = SMOTE(sampling_strategy='minority')\nX, y = BOW(data_100)\nprint(f\"Initial set observations {X.shape[0]}\")\nprint(f\"Initial set target classes {len(set(y))}\")\nX, y = smote_over_sample.fit_resample(X, y)\nprint(f\"Modified set observations {X.shape[0]}\")\nprint(f\"Modified set target classes {len(set(y))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for result evaluation\n\nsmote_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\nsmote_over_sample = SMOTE(sampling_strategy='minority')\n\n# Testing Count Vectorizer\n\nX, y = BOW(data_100)\nX, y = smote_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nsmote_result = smote_result.append(training_naive(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nsmote_result = smote_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nsmote_result = smote_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\n\n# Testing TF-IDF with 1-gram\n\nX, y = tfidf(data_100, ngrams = 1)\nX, y = smote_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nsmote_result = smote_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nsmote_result = smote_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nsmote_result = smote_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\n\n# Testing Word2vec\n\nX, y = w2v(data_100)\nX, y = smote_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nsmote_result = smote_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Word2vec'), ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTE generated oversampling procedure gave a huge leap in quality. More importantly - the quality have got gain in every direction, including both precision and recall."},{"metadata":{},"cell_type":"markdown","source":"## 10.4. Oversampling. ADASYN"},{"metadata":{},"cell_type":"markdown","source":"For oversampling algorithm check how many observation the algorithm generates."},{"metadata":{"trusted":true},"cell_type":"code","source":"adasyn_over_sample = ADASYN(sampling_strategy='minority')\nX, y = BOW(data_100)\nprint(f\"Initial set observations {X.shape[0]}\")\nprint(f\"Initial set target classes {len(set(y))}\")\nX, y = adasyn_over_sample.fit_resample(X, y)\nprint(f\"Modified set observations {X.shape[0]}\")\nprint(f\"Modified set target classes {len(set(y))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for result evaluation\n\nadasyn_result = pd.DataFrame(columns = ['Preprocessing', 'Model', 'Precision', 'Recall', 'F1-score', 'Accuracy'])\n\nadasyn_over_sample = ADASYN(sampling_strategy='minority')\n\n# Testing Count Vectorizer\n\nX, y = BOW(data_100)\nX, y = adasyn_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nadasyn_result = adasyn_result.append(training_naive(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nadasyn_result = adasyn_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\nadasyn_result = adasyn_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'Count Vectorize'), ignore_index = True)\n\n# Testing TF-IDF with 1-gram\n\nX, y = tfidf(data_100, ngrams = 1)\nX, y = adasyn_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nadasyn_result = adasyn_result.append(training_naive(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nadasyn_result = adasyn_result.append(training_logreg(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\nadasyn_result = adasyn_result.append(training_lgbm(X_train, X_test, y_train, y_test, 'TF-IDF 1-grams'), ignore_index = True)\n\n# Testing Word2vec\n\nX, y = w2v(data_100)\nX, y = adasyn_over_sample.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\nadasyn_result = adasyn_result.append(training_logreg(X_train, X_test, y_train, y_test, 'Word2vec'), ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adasyn_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ADASYN algorithm have shown approximately the same results as SMOTE. But still it's slightly worse. So we will keep to SMOTE."},{"metadata":{},"cell_type":"markdown","source":"# 11. Best model result analysis"},{"metadata":{},"cell_type":"markdown","source":"Best model of all we have received is word2vec on SMOTE trained on Logistic Regression.\n\nLet's look at its confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = w2v(data_100)\nX, y = smote_over_sample.fit_resample(X, y)\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X, y, test_size=0.2, random_state=40)\nclf_w2v_smote = LogisticRegressionCV(cv = folds, solver = 'saga', multi_class = 'multinomial', n_jobs = -1)\nclf_w2v_smote.fit(X_train_smote, y_train_smote)\ny_pred = clf_w2v_smote.predict(X_test_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_smote, y_pred)\nfig = plt.figure(figsize=(16, 16))\nplot = plot_confusion_matrix(cm, classes=classes, normalize=False, \n                             title='Confusion matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Result metrics for each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_metrics = pd.DataFrame({'Labels': range(12), \n                              'Recall': recall_score(y_pred, y_test_smote, average = None),\n                             'Precision': precision_score(y_pred, y_test_smote, average = None),\n                             'F1-score': f1_score(y_pred, y_test_smote, average = None)})\nfinal_metrics['Labels'] = final_metrics['Labels'].replace(label_to_text)\nfinal_metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is still not perfect, but yet it is much better, than it was in the beginning. Some classes are recognized pretty well."},{"metadata":{},"cell_type":"markdown","source":"# 12. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this work I made an attempt to build an algorithm, which would predict necessary diagnostics for a patient based on his anamnesis. From basic f1-score of around 0.3 I managed to increase capability of the algorithm up to nearly 0.7.\n\nI barely can be used in real-life diagnostics. but still could be helpful in some adjuscent fields. These could be analysing some popular words-triggers for different diseases in scientific or applied purpose, optimisation of medical facility schedule or a way of preliminary patient analysis in order to desrease a list of potential diagnostics.\n\nThis work can be proceeded with more detailed text analysis.\n\n- Anamnesis text contains some kind of headers for allergies or previous treatment. It may be profitable to try to use it in modelling;\n- More detailed analysis of metrics could be used in order to understand better the specifics of how algorithm works with different classes;\n- Wider range of algorithms could be tried in order to find the best one;\n- For existing algorithms a more precise tuning could have given some more fractions of a percent;\n- Some additional information on minor classes could be used to include them into calculation.\n\nSome of these ideas I will use to improve the quality of the algorithm. But this would be outside the limits of current work."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}