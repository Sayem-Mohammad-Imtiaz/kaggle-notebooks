{"cells":[{"metadata":{"id":"vUpmSFflwzR1"},"cell_type":"markdown","source":"# Highlight searching results for COVID-19 \n"},{"metadata":{"id":"0RtnmBN6A-Oq"},"cell_type":"markdown","source":"# Introduction:\nAbstract is one of the most important sections in any publication. It includes summary for the findings and results in a paper. It is the first thing that researchers view in order to decide whether to go deeper and read the whole publication or skip to another one. The abstract section includes a comprehensive outline of published paper contents, the intended purposes, the publication importance. Hence, we will exploit it to highlight relevant answers for user queries. \nThe solution objective is to ease the search for relevant topics asked about COVID-19. This is done by highlighting related answers from abstract publication extractions. In the following section we will describe the solution flow. \n\n\n# Methodology:\n**Data Preparation**\n\nData set used for searching is provided by Allen Institute for AI. Anserini team have provided already indexing for the data set covering title and abstract.\nReference: https://github.com/castorini/anserini/blob/master/docs/experiments-covid.md\n\nBuilding a customized stop words by compiling all the paper abstracts and computing the term frequency. By displaying the first 150 most frequent terms, we selected manually terms that not necessarily defined as a keyword. For example, a word like “patients” or  “disease” does not add much information as we know beforehand that the dataset is covering medical domain. \n\nBuilding a customized synonyms file for words that we want to expand. This is done by compiling all the queries and sub-quires published on Kaggle competition and computing the term frequency. By discarding the traditional English stop words like “and”, “the”..etc, we selected manually terms that were interesting and added more synonym for it. For example, a word like “animal”, “monkey”, “mice” and “mouse” will be probably used as a reference for clinical experiments on animals, and therefore we clustered them together as synonyms to be used in query expansions. \n\n\n**The solution works as depicted in figure 1:**\nStep 1: A user ask a query in natural language, for example: “what are available vaccine for Covid 19”\nStep 2: A keywords extraction module will process the query. The module aims to expand the query by synonyms, normalize the text, remove stop words. \nStep 3: Now the keywords are used to search in the indexed data set, we retrieve the top ten hits sorted in descending order of the search score. \nStep 4:  Using the abstract and user query we use Bert question and answer model to highlight the answer in the abstract\n![Solution Flow Diagram](../input/diagram/flow2.jpg)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F687442%2F1be903a421119ac5add0beebff9846c1%2Fflow2.jpg?generation=1587076320936373&alt=media) \n# Discussion \n**Solution Pros:**\n•\tSimple and straight forward solution with good results.  \n•\tExploiting Anserini, a toolkit that is built on top of core Lucene libraries. Making it easy to retrieve related documents. [1][2]\n•\tExploiting BERT, a pre-trained model for question and answering task [3]. Bert helped in boosting our result. It facilitated the focus on the search outcome by highlighting the answer of user query within the abstract of retrieved papers. Making it very simply and clear for the user to reach the desired information. \n\n**Solution Cons:**\n•\tBert is unable to extract the answer from publication abstract that are more than 512 tokens. When this case occurs, we are highlighting interesting keywords instead using the extracted keywords.\n•\tWe need to explore other models that are domain specific like scibert, biobert, Bio_ClinicalBERT\n\n\n# Acknowledgments:\nI would like to thank Anserini team for providing demo notebooks and indexed datasets from the Allen Institute for AI [github](https://github.com/castorini/anserini/blob/master/docs/experiments-covid.md) from the Allen Institute for AI.\n\nI would like to thank Chris McCormick for his Bert demos, articles and his notebook Question Answering with a Fine-Tuned BERT\n[here](https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-#scrollTo=W-1zl5XdYInf): \n\n\n# References\nIn this notebook, we'll perform data mining using Covid-19 publications title + abstract.The solution objective is to ease the search for relevant topics asked about COVID-19. This is done by highlighting related answers from abstract publication extractions. \n\n[1] Yang, Peilin, Hui Fang, and Jimmy Lin. \"Anserini: Enabling the use of Lucene for information retrieval research.\" Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2017.\n[2] https://github.com/castorini/anserini\n[3] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n\n"},{"metadata":{"id":"f2vNd7bpJlDZ"},"cell_type":"markdown","source":"First, install Python dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os.path\nfrom pathlib import Path\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess\nversion = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\nprint(version)","execution_count":null,"outputs":[]},{"metadata":{"id":"E_lt0-pXJia0","trusted":true},"cell_type":"code","source":"%%capture\n!pip install pyserini==0.8.1.0\n!pip install transformers\n!pip install nltk\nimport json","execution_count":null,"outputs":[]},{"metadata":{"id":"1fbN2KbZoxrE"},"cell_type":"markdown","source":"Perform the imports and downloads for prerequisites"},{"metadata":{"trusted":true},"cell_type":"code","source":"if(not('11.0.2' in str(version))):\n    print('jdk upgrade required')\n    !curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n    !mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n    !update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1\n    !update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java\nelse:\n    print('jdk level is Ok ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport os\n#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.system(\"ls /usr/lib/jvm\")\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2\"\n!ls '/usr/lib/jvm'","execution_count":null,"outputs":[]},{"metadata":{"id":"q93fI77dY9HU","outputId":"9a3e2285-5f27-4ad8-d7c7-76e67b169922","trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport pandas as pd\nimport numpy as np\nimport string\nimport torch\nimport numpy\nfrom tqdm import tqdm\n#%tensorflow_version 1.x\n!pip install tensorflow==1.15.2\nimport tensorflow\nprint(tensorflow.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"xbXmstbGypef"},"cell_type":"markdown","source":"Download the pre-built index and download synonyms file. The synonym file is a preliminary version that was built manually to help in expanding the search query."},{"metadata":{"id":"JT_OJKftdqGP","trusted":true},"cell_type":"code","source":"%%capture\n\n!wget https://www.dropbox.com/s/j55t617yhvmegy8/lucene-index-covid-2020-04-10.tar.gz\n!tar xvfz lucene-index-covid-2020-04-10.tar.gz\n!wget https://www.dropbox.com/s/szakwmvco88hp3m/synonyms.csv?dl=0\n!mv synonyms.csv?dl=0 synonyms.csv","execution_count":null,"outputs":[]},{"metadata":{"id":"EwVSvHBz9RMM"},"cell_type":"markdown","source":"Sanity check of index size (should be 1.3G):"},{"metadata":{"id":"KVXWA6WS0aqJ","outputId":"86466659-d4b7-43ef-e048-2a146bdc4fb3","trusted":true},"cell_type":"code","source":"!du -h lucene-index-covid-2020-04-10","execution_count":null,"outputs":[]},{"metadata":{"id":"e98LWXwXSxG1"},"cell_type":"markdown","source":"Load BERT from HuggingFace Transformers"},{"metadata":{"id":"E4DXP0LESvhi","trusted":true},"cell_type":"code","source":"from transformers import *\n#let us try different BERT models, so far BERT model had better performance\n\n#dtokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n#dmodel = AutoModelForQuestionAnswering.from_pretrained('allenai/scibert_scivocab_cased')\n#dtokenizer = AutoTokenizer.from_pretrained('monologg/biobert_v1.0_pubmed_pmc', do_lower_case=False)\n#dmodel = AutoModelForQuestionAnswering.from_pretrained('monologg/biobert_v1.0_pubmed_pmc')\n#dtokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n#dmodel = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n\ndtokenizer= BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ndmodel=BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nscoredic={}\n\n\t\t\t","execution_count":null,"outputs":[]},{"metadata":{"id":"wKUMnzjpqoI2"},"cell_type":"markdown","source":"Helper function getsynonym extract synonyms from synonyms.csv"},{"metadata":{"id":"KO5gFkoTqbaP","trusted":true},"cell_type":"code","source":"# retrun synonyms for term \ndef getsynonym(term):\n    \n\n  df = pd.read_csv('../input/synonymscsv/synonyms.csv')\n\n  mylist=[]\n  for col in df.columns:\n    for index, rows in df.iterrows():\n      if rows[col]==term:\n        df2=rows[:]\n        df2=df2.dropna()\n        mylist = df2.values.tolist()\n        break;\n  return mylist","execution_count":null,"outputs":[]},{"metadata":{"id":"kh9MnJ85xbJ6"},"cell_type":"markdown","source":"Helper function expandquery is used to expand the query"},{"metadata":{"id":"1_dpCyWfxawM","trusted":true},"cell_type":"code","source":"#return a string composed of the query and adding more synonynms.\ndef expandquery(query):\n  \n  searchquery=\"\"\n  querylist =query.split(\" \")\n  listofwords=[]\n  for term in querylist:\n    synonymlist = getsynonym(term)\n    if not synonymlist == []:\n      listofwords=listofwords+synonymlist\n    else:\n      searchquery=searchquery+\" \"+term\n  myset = set(listofwords)\n  mylist =list(myset)\n  searchquery2=\" \".join(str(item) for item in mylist)\n  searchquery = searchquery+\" \"+searchquery2\n  \n  return searchquery","execution_count":null,"outputs":[]},{"metadata":{"id":"ol2HCjKkymMc"},"cell_type":"markdown","source":"Helper function unicodedata is used to normalize the text."},{"metadata":{"id":"vgiG-aQfyl2k","trusted":true},"cell_type":"code","source":"import unicodedata\n\ndef normalize_caseless(text):\n    return unicodedata.normalize(\"NFKD\", text.casefold())\n\ndef caseless_equal(left, right):\n    return normalize_caseless(left) == normalize_caseless(right)","execution_count":null,"outputs":[]},{"metadata":{"id":"eejtNwf9yt30"},"cell_type":"markdown","source":"removeCovidStopwords function is used to remove stop words. \nOther than the default stop words extracted from wordnet, We have collected some stop words that are specific to COVID-19 data set. The customized stop words were selected by computing the term frequency of all papers abstract and manually selecting some of the words that are repeated in almost most of the papers.\n"},{"metadata":{"id":"cfUTWPIfytMy","trusted":true},"cell_type":"code","source":"#return a string composed of the query after removing stop words\ndef removeCovidStopwords(query):\n  stop_wordsCovid =set(['what','how',\"which\",\"where\",\"virus\",\"viral\",\"viruses\",\"infection\",\"disease\",\"patients\",\"study\",\",\",\"?\"])\n  stop_words=set(stopwords.words(\"english\"))\n  searchquery=\"\"\n  word_tokens = word_tokenize(query)\n  print(type(stop_wordsCovid))\n  filtered_sentence = [w for w in word_tokens if ((not w in stop_words)and(not w in stop_wordsCovid))]\n  searchquery=\" \".join(str(item) for item in filtered_sentence)\n  return searchquery","execution_count":null,"outputs":[]},{"metadata":{"id":"_cj0w8a30JAD"},"cell_type":"markdown","source":"Function extractquerysearch is used to extract the keywords that we can use to fire search query. The result search query is only used for information retrieval and not with Bert model. In other words, we will use the original query as is with BERT."},{"metadata":{"id":"s_WgJX0K0Irz","trusted":true},"cell_type":"code","source":"# return keywords to be used with pyserini\ndef extractquerysearch(query):\n  searchquery=\"\"\n  searchquery = normalize_caseless(query)\n  searchquery = removeCovidStopwords(searchquery)\n  searchquery=expandquery(searchquery)\n\n  return searchquery","execution_count":null,"outputs":[]},{"metadata":{"id":"C3bF5jq2EffS"},"cell_type":"markdown","source":"Clean some extra text in retrieved paper abstract for a better results display. "},{"metadata":{"id":"3FX3htS8XgVr","trusted":true},"cell_type":"code","source":"# Clean some extra text in paper abstract for a better presentation of results\ndef cleantext(paragraph):\n  if paragraph.startswith('abstract')or paragraph.startswith('Abstract')or paragraph.startswith('ABSTRACT'):\n    paragraph =paragraph[8:]\n  \n  return paragraph","execution_count":null,"outputs":[]},{"metadata":{"id":"NkD0kKxW9mHP"},"cell_type":"markdown","source":"Using the user original query let us extract more keywords."},{"metadata":{"id":"fI2HRj48TTr3","outputId":"6977cf82-71bf-4eeb-90d0-59f9af6fab15","trusted":true},"cell_type":"code","source":"query='What is known about covid-19 transmission, incubation, and environmental stability?'\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted are:\",searchquery)","execution_count":null,"outputs":[]},{"metadata":{"id":"Diwt5kPa268Q"},"cell_type":"markdown","source":"Using the keywords exracted (i.e. searchquery ) Let us use pyserini to search for related publications. We will display the top 10 documents and their score."},{"metadata":{"id":"yFZlcqEX0t1f","outputId":"90e1d9be-9879-488f-bac6-3a4f5bcb721a","trusted":true},"cell_type":"code","source":"from pyserini.search import pysearch\n\nsearcher = pysearch.SimpleSearcher('lucene-index-covid-2020-04-10/')\nhits = searcher.search(searchquery)\n\ndisplay(HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:12px\"><b>Query</b>: '+query+'</div>'))\n\n\n# Prints the first 10 hits\nfor i in range(0, 10):\n  score=hits[i].score\n  scoredic.update({hits[i].lucene_document.get(\"title\") :score })\n  display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               F'{i+1} {hits[i].docid} ({hits[i].score:1.2f}) -- ' +\n               F'{hits[i].lucene_document.get(\"authors\")} et al. --' + \n               F'<a href=\"https://doi.org/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}</a>.'+\n               '<br>' +'<b> Paper Title: </b> '+\n               F'{hits[i].lucene_document.get(\"title\")}. '\n               \n               + '</div>'))","execution_count":null,"outputs":[]},{"metadata":{"id":"-dEsO4m-A6OA"},"cell_type":"markdown","source":"Visualize the scores of relevance for each paper retrieved."},{"metadata":{"id":"U2UGf2agA53a","outputId":"f90c44d1-7d98-4e3a-9c3e-f7b43bc5cdc6","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\ntitles = list(scoredic.keys())\ny_pos = np.arange(len(titles))\nscores = list(scoredic.values())\nerror = np.random.rand(len(titles))\n\nax.barh(y_pos, scores, xerr=error, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(titles)\nax.invert_yaxis()  \nax.set_xlabel('Scores')\nax.set_title(query)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xtZBSEbNP_T9"},"cell_type":"markdown","source":"Using BERT, we will use answer_question function that will extract the answer using the query and absract. In case the answer is not found then we will highlight the keywords instead."},{"metadata":{"id":"rH8NbBlsfxZ_","trusted":true},"cell_type":"code","source":"def answer_question(question, answer_text,dtokenizer,dmodel):\n    \n    answer = \"No highlight detected\"\n    if not question or not answer_text:\n      print(\"Empty question or Empty abstract\")\n      return answer\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = dtokenizer.encode(question, answer_text,max_length=512)\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(dtokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n    \n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n    \n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    \n    start_scores, end_scores = dmodel(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = dtokenizer.convert_ids_to_tokens(input_ids)\n    \n    # Start with the first token.\n    answer = tokens[answer_start]\n    #if bert didn't get the tokens right, then the function retrun and highlight the keywords instead\n    if answer==dtokenizer.cls_token:\n      answer = \"No highlight detected\"\n      return answer\n    # if the first token is [sep] then skip and move forward  \n    if answer==dtokenizer.sep_token:\n      answer=\"\"\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            if tokens[i-1]=='(' or tokens[i-1]  == '-':\n              answer += tokens[i]\n            elif tokens[i] == ')' or tokens[i]  == '-':\n              answer += tokens[i]\n            else:\n              answer += ' ' + tokens[i]\n\n    if answer==dtokenizer.sep_token:\n      answer='No highlight detected'\n    return answer","execution_count":null,"outputs":[]},{"metadata":{"id":"OGM0jSZn4giE"},"cell_type":"markdown","source":"function highlightanswer is used to highlight a string (str) in a paragraph."},{"metadata":{"id":"E6DE31PT5R9X","trusted":true},"cell_type":"code","source":"def highlightanswer(str,paragraph):\n  str_start=\"\"\n  str_end=\"\"\n  flag='none'\n  paragraph=normalize_caseless(paragraph)\n  str=normalize_caseless(str)\n  try:\n    indx = paragraph.index(str)\n  except:\n    return str_start, str, str_end,flag\n\n  if indx==-1:\n    return str_start, str, str_end,flag\n  str_start=paragraph[0:indx]\n  str_end=paragraph[indx+len(str):]\n  flag='done'\n  return str_start, str, str_end, flag","execution_count":null,"outputs":[]},{"metadata":{"id":"Xrxs7HmGySnL"},"cell_type":"markdown","source":"highlight_keywords function will high light the keywords found in the abstract."},{"metadata":{"id":"bRg66j1FhjZS","trusted":true},"cell_type":"code","source":"def highlight_keywords(answer_text):\n\n  abstractwords= word_tokenize(answer_text)\n  searchquery_tokenized=word_tokenize(searchquery)\n  abstractpara=\"\"\n\n  for wrd in abstractwords:\n    if wrd in searchquery_tokenized:\n      abstractpara = abstractpara+\" \"+\"<font color='red'>\"+wrd+\"</font>\"\n    else:\n      abstractpara = abstractpara+\" \"+wrd\n  \n  return abstractpara","execution_count":null,"outputs":[]},{"metadata":{"id":"1sFXpKk-4xtd"},"cell_type":"markdown","source":"display_marker_result function will loop over the search hits sorted by documents score. It will highlight the answer and any kewords that would be interested to the user."},{"metadata":{"id":"gVKIF9qtymO_","trusted":true},"cell_type":"code","source":"def display_marker_result():\n  display(HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:12px; background:#e3e3e3\"><b>Query</b>: '+query+'</div>'))\n  # Prints the first 10 hits\n  for i in range(0, 10):\n    abstract=cleantext(hits[i].lucene_document.get(\"abstract\"))\n    answer =answer_question(query,abstract,dtokenizer,dmodel)\n    strstart, highlighted, strend, myflag= highlightanswer(answer,abstract)\n    if answer=='No highlight detected':\n      display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: </b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: </b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: </b>'+\n               F'<a href=\"https://doi.org/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}</a>.'+\n               '<br> <b>Paper Title: </b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: </b><br>'+\n               F'{highlight_keywords(abstract)}'\n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: </b> highlighting detected keywords </font><br>'+\n                '</div> --------------------------------------------------------------------------------------------------------------------------------------' ))\n    elif myflag=='none':\n      display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: </b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: </b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: </b>'+\n               F'<a href=\"https://doi.org/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}</a>.'+\n               '<br> <b>Paper Title: </b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: </b><br>'+\n               F'{abstract}'\n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: </b>'+F'{highlighted} '+'</font><br>'+\n                '</div> --------------------------------------------------------------------------------------------------------------------------------------' ))\n    else:\n        display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: </b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: </b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: </b>'+\n               F'<a href=\"https://doi.org/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}</a>.'+\n               '<br> <b>Paper Title: </b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: </b><br>'+\n               F'{strstart} ' +'<font color=\"red\">'+F'{highlighted} '+'</font>'+F'{strend}'\n              \n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: </b>'+F'{highlighted} '+'</font><br>'+\n               '</div> ---------------------------------------------------------------------------------------------------------------------------------------' ))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iJFXlcNuFHFM"},"cell_type":"markdown","source":"Let us display the result."},{"metadata":{"id":"bNAuTa3FzWsV","outputId":"b0a45bb5-60a2-4d29-c2ed-44a1794683fe","trusted":true},"cell_type":"code","source":"display_marker_result()","execution_count":null,"outputs":[]},{"metadata":{"id":"8MdsCNtm0aMp"},"cell_type":"markdown","source":"Let us perform a new search now and see results"},{"metadata":{"id":"IYdGhIw40XwO","outputId":"9365ed12-1304-422e-b08a-5fcfabf13616","trusted":true},"cell_type":"code","source":"query ='what are the effectiveness of drugs being developed and tried to treat COVID-19 patients?'\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted is: \",searchquery)\nhits = searcher.search(searchquery)\ndisplay_marker_result()","execution_count":null,"outputs":[]},{"metadata":{"id":"rin6uvI3-Qyw"},"cell_type":"markdown","source":"Again, Let us perform a new search and see results"},{"metadata":{"id":"PzkvpVGl-RxA","outputId":"1164ac47-5932-478d-f049-a84326e72c86","trusted":true},"cell_type":"code","source":"query=\"What do we know about COVID-19 risk factors?\"\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted is: \",searchquery)\nhits = searcher.search(searchquery)\ndisplay_marker_result()","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Highlight_on_COVID_19_publications.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}