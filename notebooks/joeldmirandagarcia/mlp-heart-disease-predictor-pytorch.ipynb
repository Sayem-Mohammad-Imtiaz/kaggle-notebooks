{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries to use","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:01:22.266107Z","iopub.execute_input":"2021-09-09T01:01:22.266562Z","iopub.status.idle":"2021-09-09T01:01:24.57849Z","shell.execute_reply.started":"2021-09-09T01:01:22.266478Z","shell.execute_reply":"2021-09-09T01:01:24.577453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import data to use","metadata":{}},{"cell_type":"code","source":"# Load our data using pandas and explore if we need to clean/feature engineer\ndf = pd.read_csv(r\"../input/heart-disease-uci/heart.csv\")\n#Let's see what we loaded\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:01:27.285689Z","iopub.execute_input":"2021-09-09T01:01:27.286122Z","iopub.status.idle":"2021-09-09T01:01:27.344798Z","shell.execute_reply.started":"2021-09-09T01:01:27.286089Z","shell.execute_reply":"2021-09-09T01:01:27.343617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see if we have any nan's\ndef explore_dataframe(df):\n    # function to print nan values, column types\n    print(\"-\"*25)\n    print(df.info()) #overall info of columns\n    print(\"-\"*25)\n    print(df.isna().sum()) #quantity of nans\n    print(\"-\"*25)\n    print(df.isna().mean()*100) #percentage of nan's\n    print(\"-\"*25)\n    print(df.dtypes) #variable types\n    print(\"-\"*25)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:01:32.79906Z","iopub.execute_input":"2021-09-09T01:01:32.799527Z","iopub.status.idle":"2021-09-09T01:01:32.807033Z","shell.execute_reply.started":"2021-09-09T01:01:32.799482Z","shell.execute_reply":"2021-09-09T01:01:32.805457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explore_dataframe(df)\n#Cool our dataset is complete and don't need to clean","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:01:46.220942Z","iopub.execute_input":"2021-09-09T01:01:46.221378Z","iopub.status.idle":"2021-09-09T01:01:46.272928Z","shell.execute_reply.started":"2021-09-09T01:01:46.221341Z","shell.execute_reply":"2021-09-09T01:01:46.271944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA with plots","metadata":{}},{"cell_type":"code","source":"columns = [col for col in df]\ncolumn_features = columns [ : -1]\ntarget = columns[-1]\nprint(column_features)\nprint(target)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:02:21.890578Z","iopub.execute_input":"2021-09-09T01:02:21.891152Z","iopub.status.idle":"2021-09-09T01:02:21.899747Z","shell.execute_reply.started":"2021-09-09T01:02:21.89109Z","shell.execute_reply":"2021-09-09T01:02:21.898333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns_plot = sns.distplot(df[target])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:02:24.438949Z","iopub.execute_input":"2021-09-09T01:02:24.439334Z","iopub.status.idle":"2021-09-09T01:02:24.895832Z","shell.execute_reply.started":"2021-09-09T01:02:24.4393Z","shell.execute_reply":"2021-09-09T01:02:24.895054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the range and distribution of numerical Variables\ndef univariate_plot(df,columns):\n    fig, ax = plt.subplots(len(columns), 2, figsize = (15, 30))\n    for idx, column in enumerate(columns):\n        sns.boxplot(x= df[column], ax = ax[idx,0])\n        sns.distplot(df[column], ax = ax[idx,1])\n    plt.tight_layout()\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:02:50.512592Z","iopub.execute_input":"2021-09-09T01:02:50.513083Z","iopub.status.idle":"2021-09-09T01:02:50.519666Z","shell.execute_reply.started":"2021-09-09T01:02:50.51305Z","shell.execute_reply":"2021-09-09T01:02:50.518258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"univariate_plot(df, columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:02:54.731031Z","iopub.execute_input":"2021-09-09T01:02:54.731403Z","iopub.status.idle":"2021-09-09T01:02:59.690607Z","shell.execute_reply.started":"2021-09-09T01:02:54.731372Z","shell.execute_reply":"2021-09-09T01:02:59.689492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bivariate_plot(df,columns, target):\n    f,axarr = plt.subplots(len(columns), figsize=(15,40))\n    target_values = df[target].values\n    for idx,column in enumerate(columns):\n        axarr[idx].scatter(df[column].values, target_values)\n        axarr[idx].set_title(column)\n    f.text(-0.01, 0.5, target, va='center', rotation='vertical', fontsize = 12)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:04:13.161986Z","iopub.execute_input":"2021-09-09T01:04:13.162372Z","iopub.status.idle":"2021-09-09T01:04:13.170204Z","shell.execute_reply.started":"2021-09-09T01:04:13.162339Z","shell.execute_reply":"2021-09-09T01:04:13.16873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bivariate_plot(df,column_features,target)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:04:15.054946Z","iopub.execute_input":"2021-09-09T01:04:15.055644Z","iopub.status.idle":"2021-09-09T01:04:17.145207Z","shell.execute_reply.started":"2021-09-09T01:04:15.055589Z","shell.execute_reply":"2021-09-09T01:04:17.14412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def heatmap(df):\n    plt.figure(figsize=(10,6))\n    sns.heatmap(df.corr(),cmap=plt.cm.Reds,annot=True)\n    plt.title('Heatmap displaying the relationship betweennthe features of the data',\n         fontsize=13)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:04:57.915913Z","iopub.execute_input":"2021-09-09T01:04:57.916347Z","iopub.status.idle":"2021-09-09T01:04:57.922567Z","shell.execute_reply.started":"2021-09-09T01:04:57.916309Z","shell.execute_reply":"2021-09-09T01:04:57.921364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heatmap(df)\n#With this we have reached a few insights about the data:\n#-The features that most positively correlate (affect) our target are chest pain (cp), thalach (maximum heart rate achieved)\n#and slope(the slope of the peak exercise ST segment).\n#-The features that most negatively correlate (affect) our target are exang(exercise induced angina), oldpeak(ST depression\n#induced by exercise relative to rest),ca(number of major vessels (0-3) colored by flourosopy) and thal(defects)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:04:59.538232Z","iopub.execute_input":"2021-09-09T01:04:59.538727Z","iopub.status.idle":"2021-09-09T01:05:00.68264Z","shell.execute_reply.started":"2021-09-09T01:04:59.538694Z","shell.execute_reply":"2021-09-09T01:05:00.681546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#these columns are not in the range from 0-1, which might worsen our model\n#normalization funtion\ndef normalize_columns(df,columns):\n    for column in columns:\n        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:09.362729Z","iopub.execute_input":"2021-09-09T01:05:09.363121Z","iopub.status.idle":"2021-09-09T01:05:09.368837Z","shell.execute_reply.started":"2021-09-09T01:05:09.363086Z","shell.execute_reply":"2021-09-09T01:05:09.367896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalize\nnormalize_columns(df,columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:13.304669Z","iopub.execute_input":"2021-09-09T01:05:13.305082Z","iopub.status.idle":"2021-09-09T01:05:13.323544Z","shell.execute_reply.started":"2021-09-09T01:05:13.305046Z","shell.execute_reply":"2021-09-09T01:05:13.322054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataloaders","metadata":{}},{"cell_type":"code","source":"#shuffle rows in the dataframe since they are ordered by target, the first half is all 1's and the second is 0's\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:26.411806Z","iopub.execute_input":"2021-09-09T01:05:26.412221Z","iopub.status.idle":"2021-09-09T01:05:26.421687Z","shell.execute_reply.started":"2021-09-09T01:05:26.412183Z","shell.execute_reply":"2021-09-09T01:05:26.419698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get train-test-split sizes for our dataloader\ntrain_size = int(0.8 * len(df)) + 1\nval_size = math.ceil((len(df) - train_size)/2)\nval_len = train_size + val_size\ntest_size = val_size \nprint(train_size,val_len,test_size)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:27.900854Z","iopub.execute_input":"2021-09-09T01:05:27.901275Z","iopub.status.idle":"2021-09-09T01:05:27.908692Z","shell.execute_reply.started":"2021-09-09T01:05:27.901237Z","shell.execute_reply":"2021-09-09T01:05:27.907395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the different dataframes to use \ndf_train = df.iloc[:train_size]\ndf_val = df[train_size:val_len]\ndf_test = df[val_len:]","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:30.140625Z","iopub.execute_input":"2021-09-09T01:05:30.141162Z","iopub.status.idle":"2021-09-09T01:05:30.147059Z","shell.execute_reply.started":"2021-09-09T01:05:30.141104Z","shell.execute_reply":"2021-09-09T01:05:30.146006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the length of the df's we just created\nprint(len(df_train),len(df_val),len(df_test))\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:34.748935Z","iopub.execute_input":"2021-09-09T01:05:34.749328Z","iopub.status.idle":"2021-09-09T01:05:34.755242Z","shell.execute_reply.started":"2021-09-09T01:05:34.749296Z","shell.execute_reply":"2021-09-09T01:05:34.754249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create our data loaders\ntrain_target = torch.tensor(df_train['target'].values.astype(np.float32))\ntrain = torch.tensor(df_train.drop('target', axis = 1).values.astype(np.float32)) \ntrain_tensor = TensorDataset(train, train_target) \ntrain_loader = DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\nval_target = torch.tensor(df_val['target'].values.astype(np.float32))\nval = torch.tensor(df_val.drop('target', axis = 1).values.astype(np.float32)) \nval_tensor = TensorDataset(val, val_target) \nval_loader = DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\ntest_target = torch.tensor(df_test['target'].values.astype(np.float32))\ntest = torch.tensor(df_test.drop('target', axis = 1).values.astype(np.float32)) \ntest_tensor = TensorDataset(test, test_target) \ntest_loader = DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:41.819748Z","iopub.execute_input":"2021-09-09T01:05:41.820334Z","iopub.status.idle":"2021-09-09T01:05:41.856389Z","shell.execute_reply.started":"2021-09-09T01:05:41.820296Z","shell.execute_reply":"2021-09-09T01:05:41.855313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create model","metadata":{}},{"cell_type":"code","source":"#An MLP(Multi-layer perceptron) should work just fine for our solution\nimport torch.nn.functional as F\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.out = torch.nn.Linear(n_hidden, n_output)# output layer\n        \n        self.dropout = torch.nn.Dropout(0.40)#dropout layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x)) # activation function for hidden layer\n        x = self.dropout(x)\n        x = self.out(x)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:47.945194Z","iopub.execute_input":"2021-09-09T01:05:47.945616Z","iopub.status.idle":"2021-09-09T01:05:47.953682Z","shell.execute_reply.started":"2021-09-09T01:05:47.945576Z","shell.execute_reply":"2021-09-09T01:05:47.952472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init_normal(m):\n    \"\"\"\n    Applies initial weights to certain layers in a model .\n    The weights are taken from a normal distribution \n    with mean = 0, std dev = 0.02.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        y = 0.02\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:05:50.154543Z","iopub.execute_input":"2021-09-09T01:05:50.155083Z","iopub.status.idle":"2021-09-09T01:05:50.160603Z","shell.execute_reply.started":"2021-09-09T01:05:50.155047Z","shell.execute_reply":"2021-09-09T01:05:50.159647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create our neural net\nnet = Net(n_feature=13, n_hidden=128, n_output=2)     # define the network\nprint(net)  # net architecture\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:06:06.998507Z","iopub.execute_input":"2021-09-09T01:06:06.999052Z","iopub.status.idle":"2021-09-09T01:06:07.010574Z","shell.execute_reply.started":"2021-09-09T01:06:06.999015Z","shell.execute_reply":"2021-09-09T01:06:07.009268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Optimizers\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\ncriterion = torch.nn.CrossEntropyLoss()  # Categorical loss","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:06:09.586283Z","iopub.execute_input":"2021-09-09T01:06:09.586731Z","iopub.status.idle":"2021-09-09T01:06:09.592992Z","shell.execute_reply.started":"2021-09-09T01:06:09.586695Z","shell.execute_reply":"2021-09-09T01:06:09.591761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scheduler since we are using SGD, helps us with convergence\nfrom torch.optim.lr_scheduler import StepLR\nscheduler = StepLR(optimizer, step_size=25, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:06:10.960495Z","iopub.execute_input":"2021-09-09T01:06:10.960888Z","iopub.status.idle":"2021-09-09T01:06:10.967163Z","shell.execute_reply.started":"2021-09-09T01:06:10.960854Z","shell.execute_reply":"2021-09-09T01:06:10.965519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train our model\niter = 0\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    net.train()\n    # Decay Learning Rate\n    scheduler.step()\n    # Print Learning Rate\n    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n    for i, (features, labels) in enumerate(train_loader):\n        # Load images\n        data = features\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output/logits\n        outputs = net(data)\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels.type(torch.LongTensor))\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        iter += 1\n\n        if iter % 1 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for features, labels in val_loader:\n                net.eval()\n                # Load images to a Torch Variable\n                data = features\n\n                # Forward pass only to get logits/output\n                outputs = net(data)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct / total\n\n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:06:13.224761Z","iopub.execute_input":"2021-09-09T01:06:13.225127Z","iopub.status.idle":"2021-09-09T01:06:19.284305Z","shell.execute_reply.started":"2021-09-09T01:06:13.225093Z","shell.execute_reply":"2021-09-09T01:06:19.28316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test our model\ntest_loss = 0.\ncorrect = 0.\ntotal = 0.\n\n# set the module to evaluation mode\nnet.eval()\n\n# Iterate through test dataset\nfor batch_idx,(features, labels) in enumerate(test_loader):\n    # Load features\n    data = features\n\n    # Forward pass only to get output\n    outputs = net(data)\n    \n    # calculate the loss\n    loss = criterion(outputs, labels.type(torch.LongTensor))\n    \n    # update average test loss \n    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n\n    # Get predictions from the maximum value\n    _, predicted = torch.max(outputs.data, 1)\n\n    # Total number of labels\n    total += labels.size(0)\n\n    # Total correct predictions\n    correct += (predicted == labels).sum()\n\n    accuracy = 100 * correct / total\n            \n    print('Test Loss: {:.6f}\\n'.format(test_loss))\n\n    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n        accuracy, correct, total))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:06:23.319938Z","iopub.execute_input":"2021-09-09T01:06:23.320513Z","iopub.status.idle":"2021-09-09T01:06:23.331358Z","shell.execute_reply.started":"2021-09-09T01:06:23.320473Z","shell.execute_reply":"2021-09-09T01:06:23.330243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}