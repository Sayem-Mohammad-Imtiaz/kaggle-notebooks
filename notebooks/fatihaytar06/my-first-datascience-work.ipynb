{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first dataScience work"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tmdb_5000_movies.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.drop(['genres','keywords','production_companies','production_countries','homepage','spoken_languages'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most rated and most popular movies on IMDB."},{"metadata":{"trusted":true},"cell_type":"code","source":"x[(x['vote_average']>7.5) & (x['popularity']>100.0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot(kind = 'scatter', x  = 'budget' , y = 'revenue' , color = 'red' , alpha = 0.6)\nplt.xlabel('Budget')\nplt.ylabel('Revenue')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This table shows us that revenue of movies is not proportional with its budget."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot(kind='hist', y = 'vote_average' , bins = 100,figsize=(11,11))\nplt.ylabel('Vote Average')\nplt.title('Movies')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **DICTIONARY**"},{"metadata":{},"cell_type":"markdown","source":"Most known meals in countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"meals = {'Spain' : 'Paella','China':'Peking_Duck','Belgium':'Moules_frites','Brasil':'Feijoada','Denmark':'Frikadeller','Turkey':'Kebab'}\nprint(meals.keys())\nprint(meals.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meals['France'] = 'Crepe' #update dictionary\nprint(meals)\ndel meals['Belgium'] # # remove entry with key 'Belgium'\nprint(meals)\nprint('Turkey' in meals) # check it in dictionary\nmeals.clear() # delete dictionary\nprint(meals)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**While and For Loops**"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nliste = ['Ali','Sinan','Hakan','Elif']\nwhile i != 4:\n    for each in liste:\n        print(each)\n    print('')\n    i+=1\n    \nfor index,value in enumerate(liste):\n    print(index,\" : \",value)\nprint('')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meals = {'Spain' : 'Paella','China':'Peking_Duck','Belgium':'Moules_frites','Brasil':'Feijoada','Denmark':'Frikadeller','Turkey':'Kebab'}\nfor key,value in meals.items():\n    print(key,\" : \",value)\nprint('')\n\nfor index,value in data[['title']][0:2].iterrows():\n    print(index,\" : \",value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SCOPE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 5\ndef f():\n    a = 4\n    return a**2\nprint(a) # a=5\nprint(f()) # a=4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What if there is no local scope\n\nx = 7   #global scope\ndef f():\n    return x*9+15\nprint(f())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import builtins\n#features of built in scope\ndir(builtins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NESTED FUNCTION(INNER FUNCTION)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def circleArea(r):\n    \n    def add(pi = 3.14):\n        return pi\n    return 2*add()*r\nprint(circleArea(4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DEFAULT and FLEXIBLE ARGUMENTS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#default arguments\ndef f(a,b,c=7,d=9):\n    return a*b+(c*d)\nprint(f(6,9))\n#if we want to change default arguments\nprint(f(7,6,9,11))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#flexible arguments\ndef f(*args):\n    m=0\n    for i in args:\n       m += i\n    return m\n\nprint('m = ',f(5,6,8,3,26,76))\ndef f(**kwargs):\n    for key,value in kwargs.items():\n        print(key,\" : \" , value)\nf(country = 'Turkey' , population ='80 million',capital = 'Ankara'  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lambda function\ncircleArea = lambda r : 2*3.14*r\nprint(circleArea(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iterators\n\nlist1=['London','LA','Istanbul','Paris']\niterator = iter(list1)\nprint(next(iterator))\nprint(*iterator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LIST COMPREHENSION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#example of list comprehension\nnum1 = [3,5,8]\nnum2 = [i**2 for i in num1]\nprint(num2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#conditionals on iterable\nnum1=[5,15,25]\nnum2=[i**2 if i==5 else i*9 if i==15 else i*10-15 for i in num1]\nprint(num2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Let's make an object that shows us runtime level of movies\nthresold = (int)(data.runtime.mean()) #average of runtime\ndata['Runtime_level'] = [\"Long\" if i>thresold else \"short\" for i in data.runtime]\ndata.loc[:15,['Runtime_level','runtime']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CLEANING DATA**\n\n        DÄ±ognose Data for Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tmdb_5000_movies.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data Shape :',df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() #that shows us numerical columns\n\n#%25 means first quantile \n#%50 means median and second quantile\n#%75 means third quantile\n#'mean' means average value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br> What is quantile?\n\n* 1,4,5,6,8,9,11,12,13,14,15,16,17\n* The median is the number that is in **middle** of the sequence. In this case it would be 11.\n\n* The lower quartile is the median in between the smallest number and the median i.e. in between 1 and 11, which is 6.\n* The upper quartile, you find the median between the median and the largest number i.e. between 11 and 17, which will be 14 according to the question above."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['runtime'].value_counts(dropna=False)) #print if there are nan values that also be counted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** VISUAL EXPLORATORY DATA ANALYSIS**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plots: visualize basic statistics like outliers, min/max or quantiles\n# For example: compare attack of pokemons that are legendary  or not\n# Black line at top is max\n# Blue line at top is 75%\n# Red line is median (50%)\n# Blue line at bottom is 25%\n# Black line at bottom is min\n# There are no outliers\n#df.boxplot(column='vote_average',by = 'vote_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TIDY DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = data.head()\ndata_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melted = pd.melt(frame=data_new,id_vars='original_title',value_vars=['vote_average','vote_count'])\nmelted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PIVOTING DATA(Reverse of melting)\n# Index is original_title\n# I want to make that columns are variable\n# Finally values in columns are value\nmelted.pivot(index = 'original_title',columns='variable',values='value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCATENATING DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=df.head()\ndata2=df.tail()\nconc_data_row = pd.concat([data1,data2],axis=0,ignore_index=True)\nconc_data_row.drop(['genres','production_companies','production_countries','keywords','spoken_languages'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = df.head()\ndata4 = df.tail()\nconc_data_col = pd.concat([data3,data4],axis = 1)\nconc_data_col.drop(['genres','production_companies','production_countries','keywords','spoken_languages'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a> <br>\n### DATA TYPES\nThere are 5 basic data types: object(string),booleab,  integer, float and categorical.\n<br> We can make conversion data types like from str to categorical or from int to float\n<br> Why is category important: \n* make dataframe smaller in memory \n* can be utilized for anlaysis especially for sklear(we will learn later)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'] = df['title'].astype('category') #convert title from string to category\ndf['vote_count'] = df['vote_count'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MISSING DATA and TESTING WITH ASSERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"tagline\"].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop nan values\ndata1 = df\ndata1['tagline'].dropna(inplace=True)# inplace = True means we do not assign it to new variable. Changes automatically assigned to data\n#so does it work?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Lets check with assert statement\n# Assert statement:\nassert 1==1 # return nothing because it is true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to run all code, we need to make this line comment\n# assert 1==2 # return error because it is false","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert  data1['tagline'].notnull().all() # returns nothing because we drop nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tagline'].fillna('empty',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert data1['tagline'].notnull().all() # returns nothing because we don't have nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With assert statement we can check a lot of thing. For example\n#assert data.columns[1] == 'Name'\n#assert data.Speed.dtypes == np.int","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PANDAS FOUNDATION**\n#Building data frames from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data frames from dictionary\ncountry = ['Turkey','France','UK']\npopulation = ['123','432','543']\nlist_label = ['country','population']\nlist_col =[country,population]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf= pd.DataFrame(data_dict)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add new columns\ndf['capital'] = ['Ankara','Paris','London']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Broadcasting\ndf['income'] = 0 #Broadcasting the entire column(fill the entire column with 0)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VISUAL EXPLORATORY DATA ANALYSIS\n* Plot\n* Subplot\n* Histogram:\n    * bins: number of bins\n    * range(tuble): min and max values of bins\n    * normed(boolean): normalize or not\n    * cumulative(boolean): compute cumulative distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting all data\ndata1=data.loc[:,['vote_average','vote_count','budget','revenue']]\ndata1.plot()\nplt.show()\n#it seems complicated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subplots\ndata1.plot(subplots=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.plot(kind='scatter',x='budget',y='revenue',alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.plot(kind='hist',y='vote_count',bins=50,range=(0,10))\nplt.ylabel('Vote Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind='hist',y='vote_count',bins=30,range=(0,10),normed=True,ax=axes[0])\ndata1.plot(kind='hist',y='vote_count',bins=30,range=(0,10),normed=True,ax=axes[1],cumulative=True)\nplt.savefig('asd.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Indexing pandas time series**"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_list = ['1993-02-14','1993-02-16','1994-01-13','1998-08-06']\nprint(type(time_list[1]))\n#as we can see type of that is string\n#Let's convert it to datetime object\ndatetime_object = pd.to_datetime(time_list)\nprint(type(datetime_object))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#close warning\nimport warnings\nwarnings.filterwarnings('ignore')\n# In order to practice lets take head of movies data and add it a time list\ndata2= data.head(4)\ntime_list2 = ['2009-12-18','2009-05-25','2012-11-06','2012-07-27']\ndatetime_object = pd.to_datetime(time_list2)\ndata2[\"date\"] = datetime_object\n#lets make date as index\ndata2=data2.set_index(\"date\")\ndata2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data2['vote_average'].loc['2012-11-06'])\nprint(data2['vote_average'].loc['2009-05-25':'2012-07-27'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RESAMPLING PANDAS TIME SERIES\n* Resampling: statistical method over different time intervals\n* Needs string to specify frequency like \"M\" = month or \"A\" = year\n* Downsampling: reduce date time rows to slower frequency like from daily to weekly\n* Upsampling: increase date time rows to faster frequency like from daily to hourly\n* Interpolate: Interpolate values according to different methods like âlinearâ, âtimeâ or indexâ"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample('A').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"M\").mean() #resample with month\n# As you can see there are a lot of nan because data2 does not include all months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# We can interpolete from first value\ndata2.resample(\"M\").first().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample('M').mean().interpolate('linear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MANIPULATING DATA FRAMES WITH PANDAS**\n### INDEXING DATA FRAMES\n* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accessor\n* Selecting only some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/tmdb_5000_movies.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['vote_average'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.vote_average[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[2,'budget']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['budget','vote_count']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SLICING DATA FRAME**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(data['budget'])) # series\nprint(type(data[['vote_count']])) #data frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.loc[:5,'vote_average':'vote_count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FILTERING DATA FRAMES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"term1= data['vote_average']> 9.0\ndata[term1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"term2 = data.budget > 10000000\nterm3 = data.vote_count> 10000\ndata[term2 & term3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.budget[data.vote_average>8.0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRANSFORMING DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def div(n):\n    return n/3\ndata.runtime.apply(div)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.runtime.apply(lambda x : x/4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['total_profit'] = data.revenue - data.budget\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### INDEX OBJECTS AND LABELED DAT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# our index name is this:\nprint(data.index.name)\n# lets change it\ndata.index.name = \"index_name\"\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite index\n# if we want to modify index we need to change all of them.\ndata.head()\n# first copy of our data to data3 then change index \ndata3 = data.copy()\n# lets make index start from 100. It is not remarkable change but it is just example\ndata3.index = range(100,4903,1)\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can make one of the column as index\n# It was like this\n# data= data.set_index(\"#\")\n# also we can use \n# data.index = data[\"#\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HIERARCHICAL INDEXING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/tmdb_5000_movies.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data.set_index(['vote_average','runtime'])\ndata1.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PIVOTING DATA FRAMES**\n\n#reshape tool"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {'Name' : ['Fatih','Ali','Zehra','AyÃ§a'],'Gender' : ['M','M','F','F'],'Age' : [21,23,26,27],'Married' : ['Y','N','N','Y'],'Salary' : [3000,2500,2700,2000]}\ndf = pd.DataFrame(dic)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pivoting\ndf.pivot(index='Gender',columns='Married',values = 'Salary')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STACKING and UNSTACKING DATAFRAME\n* deal with multi label indexes\n* level: position of unstacked index\n* swaplevel: change inner and outer level index position"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.set_index(['Gender','Married'])\ndf1\n#let's unstack it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# level determines indexes\ndf1.unstack(level=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.unstack(level=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df1.swaplevel(0,1)\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MELTING DATA FRAMES\n* Reverse of pivoting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.pivot(index='Gender',columns='Married',values = 'Salary')\npd.melt(df,id_vars='Gender',value_vars=['Salary','Married'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CATEGORICALS AND GROUPBY"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# according to Gender take means of other features\ndf.groupby('Gender').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can only choose one of the feature\ndf.groupby('Gender').Salary.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can choose multiple features\ndf.groupby('Gender')[['Salary','Age']].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# as you can see gender is object\n# However if we use groupby, we can convert it categorical data. \n# Because categorical data uses less memory, speed up operations like groupby\n#df[\"Gender\"] = df[\"Gender\"].astype(\"category\")\n#df[\"Married\"] = df[\"Married\"].astype(\"category\")\n#df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}