{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn.utils import shuffle\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data1 = pd.read_csv(\"../input/student-mat.csv\",sep=\",\")\ndata2 = pd.read_csv(\"../input/student-por.csv\",sep=\",\")\ndata = [data1,data2]\ndata=pd.concat(data)\ndata=shuffle(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb177ad49a531ac0ee6d87b7a3aac1942f1eeeb1"},"cell_type":"code","source":"data=data.drop_duplicates([\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd47def8873277b780beef934b68f49f8439e016"},"cell_type":"code","source":"sns.catplot(x=\"school\", hue = \"sex\" , data=data , kind=\"count\",height=6, aspect=.7)\nprint(\"percentage total female : \",(data[\"sex\"] == 'F').value_counts(normalize = True)[1]*100)\nprint(\"percentage total male : \",(data[\"sex\"] == 'M').value_counts(normalize = True)[1]*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa638ad8ac26dbbe131251e9a899ad0fb61d8d7"},"cell_type":"markdown","source":"It is clear that number of female students are more than number of male students in both the  schools, with 'GP' - Gabriel Pereira school having more number of students overall.  "},{"metadata":{"trusted":true,"_uuid":"6b5c83fcc400b860990034a0ce980727986f9afe"},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib.pyplot import figure\nfigure(figsize=(15, 15))\nhmap = sns.heatmap(data.corr(), square=True, annot=True,linewidths=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fe343514a64e63d2e523c93f784fef4ac423379"},"cell_type":"markdown","source":"**Statements based on heatmap values**\nWe can observe few good  correlation values between health and alcohol consumption also alcohol consumption with failures. And also as expected study time has a good correlation value with grades and father edu, mother edu and failures also have good correlation values with grades and first period and second period grades matter a lot to the final grade(high corrleation values). Surprisingly absences and health have low correlation values with the grades."},{"metadata":{"trusted":true,"_uuid":"8280ada4806500d9abf970efa6d7114766fc91ab"},"cell_type":"code","source":"#drop some features that have very less correlation values with grades\ndata = data.drop([\"traveltime\",\"famrel\",\"freetime\",\"goout\",\"health\",\"absences\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2663e58466878a1970f467e9d6bddefc4a70958f"},"cell_type":"code","source":"sns.swarmplot(x=\"internet\",y=\"G3\",hue='address',data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53743d3caed7b775f4664dfd5f75ccf5ab3b437b"},"cell_type":"markdown","source":"It seems most of the urban people are having internet availability and also performing better than rural people."},{"metadata":{"trusted":true,"_uuid":"93e85965a10cd85cbf06b2e5e9c725dc53949865"},"cell_type":"code","source":"sns.catplot(x=\"sex\", hue = \"romantic\", data=data , kind=\"count\",height=6, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7f2afa130e164bd2a5350767de220b65e143b89"},"cell_type":"markdown","source":"It seems females have a higher chance of being in a relationship than male students. "},{"metadata":{"trusted":true,"_uuid":"bdc64da0aa9dcb0d9e68b29fbdded8233e1788ad"},"cell_type":"code","source":"sns.swarmplot(x=\"Dalc\",y=\"G3\",hue=\"sex\",data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f28914c00a193d08c8e1fb1fec01add146a508"},"cell_type":"code","source":"sns.swarmplot(x=\"Walc\",y=\"G3\",hue=\"sex\",data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47ae49c28661c1d1af1e44dd12f87ecbbed1a44e"},"cell_type":"markdown","source":"It seems very large amount of people drink very low alcohol(1) during both weekend and work days. Most of the people who drink high(4,5) are male and also people who drink high during work days are getting bit lower grades than that of others.  "},{"metadata":{"trusted":true,"_uuid":"0855a31a7ac116e90c1cc14676f93f31563b1b51"},"cell_type":"code","source":"#to suppress the warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e433d708afb677ad01e9a4c394b2d452773a179"},"cell_type":"code","source":"#data cleaning(binary and onehot encodings)\ndata.school[data.school == 'GP'] = 1\ndata.school[data.school == 'MS'] = 0\ndata.sex[data.sex == 'M'] = 1\ndata.sex[data.sex == 'F'] = 0\ndata.address[data.address == 'U'] = 1\ndata.address[data.address == 'R'] = 0\ndata.famsize[data.famsize == 'GT3'] = 1\ndata.famsize[data.famsize == 'LE3'] = 0\ndata.Pstatus[data.Pstatus == 'T'] = 1\ndata.Pstatus[data.Pstatus == 'A'] = 0\n#for Medu & Fedu its better in numeric as higher education has higher difference with primary than secondary\n#one hot encoding of necessary features\ncols_to_transform = [ 'Mjob','Fjob','reason','guardian' ]\ndata=pd.get_dummies(data,columns=cols_to_transform) \n#failures, traveltime, studytime does not require onehot  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f13ed3a65bd64884a8a9048a8d839c3db7a6705"},"cell_type":"code","source":"data.schoolsup[data.schoolsup == 'yes'] = 1\ndata.schoolsup[data.schoolsup == 'no'] = 0\ndata.famsup[data.famsup == 'yes'] = 1\ndata.famsup[data.famsup == 'no'] = 0\ndata.paid[data.paid == 'yes'] = 1\ndata.paid[data.paid == 'no'] = 0\ndata.activities[data.activities == 'yes'] = 1\ndata.activities[data.activities == 'no'] = 0\ndata.nursery[data.nursery == 'yes'] = 1\ndata.nursery[data.nursery == 'no'] = 0\ndata.higher[data.higher == 'yes'] = 1\ndata.higher[data.higher == 'no'] = 0\ndata.internet[data.internet == 'yes'] = 1\ndata.internet[data.internet == 'no'] = 0\ndata.romantic[data.romantic == 'yes'] = 1\ndata.romantic[data.romantic == 'no'] = 0\n#remaining doesn't require onehot as distances are not same. one hot is used only when they are equidistant\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52d80f6425f0312c2cd84f472bb53d492e635aaa"},"cell_type":"code","source":"#Final grades\ny =  data[[ 'G3']].mean(axis=1)\ndata = data.drop([\"G3\"], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be874f330fa58b8cfd724a26cdb2fffc63685014"},"cell_type":"code","source":"#spliting data to train and test by 80% and 20% respectievely \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data,y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd55161b43e02562c42458b6484da3d7c90c681a"},"cell_type":"code","source":"# training the model on training set using linear regression\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nregr = linear_model.LinearRegression()\nregr.fit(X_train,y_train)\ny_pred = np.round(regr.predict(X_test))\nmeansqr=[]\nAvgdiff=[]\nr2=[]\nmeansqr.append(mean_squared_error(y_test, y_pred))\nAvgdiff.append(abs(y_test-y_pred).mean())\nr2.append(r2_score(y_test, y_pred))\nprint(\"Mean squared error: %.2f\"% mean_squared_error(y_test, y_pred))\nprint(\"Mean difference: %.2f\"% abs(y_test-y_pred).mean())\nprint(\"r2 score: %.2f\"% r2_score(y_test, y_pred))\n#plotting y_pred and y_test\nt = np.arange(0,len(y_pred) , 1)\nplt.plot(t,y_pred,t,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d15508ee722e155e97e2b2d7e9d23030208e1826"},"cell_type":"code","source":"# RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(X_train, y_train);\ny_pred =np.round(rf.predict(X_test))\nmeansqr.append(mean_squared_error(y_test, y_pred))\nAvgdiff.append(abs(y_test-y_pred).mean())\nr2.append(r2_score(y_test, y_pred))\nprint(\"Mean squared error: %.2f\"% mean_squared_error(y_test, y_pred))\nprint(\"Mean difference: %.2f\"% abs(y_test-y_pred).mean())\nprint(\"r2 score: %.2f\"% r2_score(y_test, y_pred))\n#plotting y_pred and y_test\nt = np.arange(0,len(y_pred) , 1)\nplt.plot(t,y_pred,t,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e85f65b54aba5c52c4e6254c894b5a7177971ca3"},"cell_type":"code","source":"#svm regressor\nfrom sklearn.svm import SVR\nregressor=SVR(kernel=\"linear\",epsilon=1.0,degree=3)\nregressor.fit(X_train,y_train)\ny_pred=regressor.predict(X_test)\nmeansqr.append(mean_squared_error(y_test, y_pred))\nAvgdiff.append(abs(y_test-y_pred).mean())\nr2.append(r2_score(y_test, y_pred))\nprint(\"Mean squared error: %.2f\"% mean_squared_error(y_test, y_pred))\nprint(\"Mean difference: %.2f\"% abs(y_test-y_pred).mean())\nprint(\"r2 score: %.2f\"% r2_score(y_test, y_pred))\n#plotting y_pred and y_test\nt = np.arange(0,len(y_pred) , 1)\nplt.plot(t,y_pred,t,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"983e7a285d7a8f6b541aed86ce3caa36a115b5e2"},"cell_type":"code","source":"#knearest neighbhors\nfrom sklearn import neighbors\nn_neighbors=5\nknn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\nknn.fit(X_train,y_train)\ny_pred=knn.predict(X_test)\nmeansqr.append(mean_squared_error(y_test, y_pred))\nAvgdiff.append(abs(y_test-y_pred).mean())\nr2.append(r2_score(y_test, y_pred))\nprint(\"Mean squared error: %.2f\"% mean_squared_error(y_test, y_pred))\nprint(\"Mean difference: %.2f\"% abs(y_test-y_pred).mean())\nprint(\"r2 score: %.2f\"% r2_score(y_test, y_pred))\n#plotting y_pred and y_test\nt = np.arange(0,len(y_pred) , 1)\nplt.plot(t,y_pred,t,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10bc87d84feb98ee9455c91fcd0cf3abb66d9a21"},"cell_type":"code","source":"objects=('LinearReg','RandForReg','SVMregg','Knn')\nplt.bar(np.arange(len(meansqr)),meansqr)\nplt.xticks(np.arange(len(meansqr)), objects)\nplt.title('Mean Square Error')\nplt.show()\nplt.bar(np.arange(len(Avgdiff)),Avgdiff)\nplt.xticks(np.arange(len(Avgdiff)), objects)\nplt.title('Absolute Mean Error')\nplt.show()\nplt.bar(np.arange(len(r2)),r2)\nplt.xticks(np.arange(len(r2)), objects)\nplt.title('r2_score')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5a8d3a9f65b40b6dfd342343050cfb581eb51e2"},"cell_type":"markdown","source":"R_2 score or R squared coefficient is a statistical measure of how well the data is fit to the regression lines.\nIf the r_2 score is 0 that implies a bad model and r_2 is equal to 1 for an ideal model.\nIn this we got r2_score of around 0.85 which implies it is a good fit considering the data set size."},{"metadata":{"trusted":true,"_uuid":"97aaaca80461af1887562ee8a62eeb528ac27533"},"cell_type":"markdown","source":"* Firstly I used linear regresion to predict but we know that it cannot learn some complex features for better prediction.\n* Randomn forest regressor uses more computation than other techniques and also performs better than other techniques.\n* SVM is known for its performance on smaller datasets and also robust to deviations but svm is used for classification tasks widely.\n* KNN is widely used for classification task. It is known for it's simple and computationally effective algorithm and performs good in case of classification tasks but not in case of regression tasks."},{"metadata":{"_uuid":"646a34a2a0a485aebaadc5be6e0756687bb29bfe"},"cell_type":"markdown","source":"**We got mean squares error of around 2 .2 and absolute mean error around 0.85( less than 1 )and also r2_score of around 0.85 with ranndomnforest regressor doing the best. These models differ by very less margins in the error scores I think the margin would be more if we could get more data and also errors would be less with more data.**"},{"metadata":{"trusted":true,"_uuid":"1f64e11c8c1e784ade077b483b31dabe0e9bf215"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03c1787e20e1504617bf086955d909c3c12cad63"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}