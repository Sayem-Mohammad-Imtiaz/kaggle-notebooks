{"cells":[{"metadata":{},"cell_type":"markdown","source":"A classification problem, to predict whether a loan should be approved or not.\n\nWorkflow:\n    \n    1. Importing libraries\n    2. Loading data\n    3. Summarising data\n    4. Filling missing values if any, for both categorical and numerical\n       * Categorical: Mode\n       * Numerical: Median/Mean/Bfill\n    5. Exploratory data analysis\n       * Data visualisation\n       * Normalisation of data if any outliers found\n    6. Conversion of object data type to numbers\n    7. Correlation between different variables. (Drop any independent variables if not coorelated to the dependent variable.)\n    8. Evaluating different models based on different metrics (Cross validated accuracy, precision, f1 score,\n                                                              recall,AUC curve, confusion matrix)\n       * Random Forest Classifier\n       * Extra Tree Classifier\n       * SVC\n       * Logistic Regression\n       * Kneighbours Classifier\n       * Decision Tree Classifier\n    9. Tuning the hyper parameters of the best models.\n    10.Feature importance"},{"metadata":{},"cell_type":"markdown","source":"##### Importing initial libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Statistical Overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Data summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Let's check if any duplication of rows\n\ndf.duplicated().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Dropping unnecessary columns for now Loan_ID\n\ndf = df.drop(\"Loan_ID\", axis=1)\ndf.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into categorical and numerical.\n\ncat_data = []\nnum_data = []\n\nfor index, type in enumerate(df.dtypes):\n    if type == \"object\":\n        cat_data.append(df.iloc[:, index])\n    else:\n        num_data.append(df.iloc[:,index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data = pd.DataFrame(cat_data).transpose()\ncat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data = pd.DataFrame(num_data).transpose()\nnum_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Getting deep into categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missing values with mode.\n\ncat_data = cat_data.apply(lambda x: x.fillna(x.value_counts().index[0]))\ncat_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Filling missing values in the numerical data.\n#  Firstly, let's check the stats.\nnum_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  For Loan amount since there is major difference between median and mean. So, let's fill the missing values with median.\n\nnum_data.LoanAmount = num_data.LoanAmount.fillna(num_data[\"LoanAmount\"].median())\nnum_data.LoanAmount.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling remaining missing values with previously occuring value in resspective columns.\n\nnum_data.Loan_Amount_Term = num_data.Loan_Amount_Term.fillna(method=\"bfill\")\nnum_data.Credit_History = num_data.Credit_History.fillna(method=\"bfill\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rechecking, if any missing values remaining.\nnum_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"* Distribution plots for numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(num_data.ApplicantIncome);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(num_data.CoapplicantIncome);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(num_data.LoanAmount);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Since above plots are not gaussian. Let's normalise them and get rid of outliers.\n\nnum_data.ApplicantIncome = np.log(num_data.ApplicantIncome)\nnum_data.CoapplicantIncome = np.log(num_data.CoapplicantIncome + 1)  # Since some values are zero to avoid log 0 =infinity.\nnum_data.LoanAmount = np.log(num_data.LoanAmount)\nnum_data.Loan_Amount_Term = np.log(num_data.Loan_Amount_Term)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#   Again checking the plots.\n\nsns.distplot(num_data.ApplicantIncome);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(num_data.CoapplicantIncome);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(num_data.LoanAmount);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Categorical data visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Married v/s Loan status\nsns.countplot(x=\"Married\", hue=\"Loan_Status\", data=cat_data);\n\n#  There is higher chance of loan approval if you are married.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Gender v/s Loan status\nsns.countplot(x=\"Gender\", hue=\"Loan_Status\", data=cat_data);\n\n#  Most of the males have got there loans approved.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Dependents v/s Loan status\nsns.countplot(x=\"Dependents\", hue=\"Loan_Status\", data=cat_data);\n\n#  Having zero dependency increases the probability of loan approval.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Education v/s Loan status\nsns.countplot(x=\"Education\", hue=\"Loan_Status\", data=cat_data);\n\n#  Graduation preffered over non graduates.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Concatenating the updated categorical and numerical data\n\ndata = pd.concat([num_data, cat_data], axis=1)\ndata.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Converting categorical data to numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\n\ncols = ['Gender', 'Married', 'Education', \n        'Self_Employed', 'Property_Area', \n        'Loan_Status', 'Dependents']\nfor col in cols:\n    data[col] = LE.fit_transform(data[col])\n\ndata.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Visualising Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr, annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Interpretation:\n    * The dependent variable Loan_status has less correlation with Loan amount term and Self employed.\n    Shall drop them during modelling.\n    "},{"metadata":{},"cell_type":"markdown","source":"###### Modelling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Splitting into x and y\n\nx = data.drop([\"Loan_Amount_Term\", \"Self_Employed\", \"Loan_Status\"], axis=1)\ny = data[\"Loan_Status\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Splliting into train and test\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Â Put models in a dictionary\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import plot_confusion_matrix\n\n\n\nmodels = {'RandomForestClassifier': RandomForestClassifier(),\n          'ExtraTreesClassifier': ExtraTreesClassifier(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'LogisticRegression': LogisticRegression(),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC()\n         }\n\ndef Fit_Score(model, x_train, y_train, x_test, y_test, x, y):\n    \n    \n    np.random.seed(45)\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        model_scores[name] = {\"Accuracy\": model.score(x_test, y_test),\n                              \"cv_acc\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"accuracy\"))}\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Scores = Fit_Score(model=models, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, x=x, y=y)\npd.DataFrame(Scores.values(), Scores.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Improving the models.\n* Further tunning the hyper parameters."},{"metadata":{},"cell_type":"markdown","source":"* 1. Random Forest Classifier\n\na. Random Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = {\"n_estimators\": np.arange(10, 1000, 50),\n       \"max_depth\": [3, 10],\n       \"min_samples_split\": np.arange(2, 20, 2),\n       \"min_samples_leaf\": np.arange(1, 20, 2)}\n#  Tunning\n\nnp.random.seed(45)\nrs_clf = RandomizedSearchCV(RandomForestClassifier(n_jobs=1), \n                            param_distributions=grid, \n                            cv=5, n_iter=15, verbose=True, refit=True)\n\nrs_clf.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_clf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"b. Grid Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid2 = {\"n_estimators\": [960],\n       \"max_depth\": [3, 10],\n       \"min_samples_split\": [12, 14],\n       \"min_samples_leaf\": [13, 12]}\n#  Tunning\nnp.random.seed(45)\n\ngs_clf = GridSearchCV(RandomForestClassifier(n_jobs=1), \n                            param_grid=grid2, \n                            cv=5, verbose=True, refit=True)\n\ngs_clf.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_clf.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 2. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Let's try with some random params if the score increases\n\ngrid3 = {\"C\": np.logspace(-4, 4, 20),\n        \"solver\": [\"liblinear\"]}\n\n        \n#  Tunning\nnp.random.seed(45)\nrs_lr = RandomizedSearchCV(LogisticRegression(n_jobs=1), \n                            param_distributions=grid3, \n                            cv=5, n_iter=15, verbose=True, refit=True)\n\nrs_lr.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_lr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_lr.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above tunning it seems the best fit would be with accuracy of 81 %."},{"metadata":{},"cell_type":"markdown","source":"#### Further different metrics study. \nLet's take losistic regression as the perfect model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Analytics(model, x_train, y_train, x_test, y_test, x, y):\n    \n\n    models = {'RandomForestClassifier': RandomForestClassifier(max_depth=3, min_samples_leaf=13, \n                                                               min_samples_split=12, n_estimators=960),\n              'ExtraTreesClassifier': ExtraTreesClassifier(),\n              'DecisionTreeClassifier': DecisionTreeClassifier(),\n              'LogisticRegression': LogisticRegression(solver='liblinear', C=545.5594781168514),\n              'KNeighborsClassifier': KNeighborsClassifier(),\n              'SVC': SVC()\n              }\n    model_scores = {}\n\n    np.random.seed(45)\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        y_preds = model.predict(x_test)\n        model_scores[name] ={\"cv_acc\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"accuracy\")),\n                             \"cv_prec\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"precision\")),\n                             \"cv_recall\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"recall\")),\n                             \"cv_f1\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"f1\"))\n                             }\n        \n             \n    return model_scores\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_scores = Analytics(models, x_train, y_train, x_test, y_test, x, y)\nscores = pd.DataFrame(final_scores.values(), final_scores.keys())\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since our sample is imbalanced, Precision and recall also plays an important role in addition to accuracy.\n#### Best models:\n* Logistic Regression/Random Forest Classifier/SVC (good accuracy)\n* Extra Trees Classifier (better precesion)\n\nlet's further explore\n\nLogistic Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression(solver='liblinear', C=545.5594781168514)\n\nLR.fit(x_train, y_train)\nROC_Curve = plot_roc_curve(LR, x_test, y_test);\nConf_Matrix = plot_confusion_matrix(LR, x_test, y_test);\nROC_Curve, Conf_Matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extra Trees Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ET = ExtraTreesClassifier()\nnp.random.seed(45)\nET.fit(x_train, y_train)\nROC_Curve = plot_roc_curve(ET, x_test, y_test);\nConf_Matrix = plot_confusion_matrix(ET, x_test, y_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_dict = dict(zip(x.columns, list(LR.coef_[0])))\nfeatures = pd.DataFrame(feature_dict, index=[0])\nfeatures.plot.bar(figsize=(10, 6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\n##### Models with better scores are:\n\n* RandomForestClassifier/LogisticRegression: \n\n* cv_acc: 0.803\n* cv_prec: 0.792\n* cv_recall: 0.97\n* cv_f1: 0.870\n\nWe can also further standardise the data and even tune other models to get the better scores. \n\n##### Please do suggest some more better models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}