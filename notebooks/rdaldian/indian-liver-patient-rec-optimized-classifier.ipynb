{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"markdown","source":"# Indian Liver Patient Records\n\nHere, we have a prolem to make a model that can classify whether a patient has liver problem or not. We implement a Random Forest Classifier with some hyperparameters tuning using Grid Search method.\n\nIn summary, the algorithm below consists of 4 steps:\n\n**1. Data Preprocessing: **\nFrom the provided data, we have 583 lines of records, 10 dependent variables and an independent variable. We implement Imputer for handling missing data, encoding the categorical data, Feature Scaling and Dimesnionality Reduction using Principle Component Analysis (PCA) method. \n\n\n**2. Make a prediction model: **\n* Here, we implemented Random Forest Classifier to predict the outcome of given independent variables.\n\n\n**3.  Implementing Grid Search: **\nGrid Search is a method to find the best hyperparameters of our model in order to increase the training and testing result\n\n\n**4.  K-Fold Cross Validation: **\nIn the end, we want to make sure that our optimized classifier does not overfit if it is applied to new testing data. \n\n\nSo, let's start our ML journey!"},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# 1. Data Prepropcessing"},{"metadata":{"trusted":true,"_uuid":"3d25a7a0c84c51d42d5250d3eda810b26e182cee"},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the dataset\ndataset = pd.read_csv(\"../input/indian_liver_patient.csv\")\ndataset_desc = dataset.describe(include = 'all')\nprint(dataset_desc)\n\n# Identifying the dependent (x) and independent (y) variables\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 10].values","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"a279099f1ec4d41cde48fd87807087c935f4cf37"},"cell_type":"markdown","source":"From the detail of the data above, we can see that actually there are some different number of data count for \"Albumin_and_Globulin_Ratio\" which is only 583, while the other features have 579. There are possibly some missing values. To make sure, let's check!"},{"metadata":{"trusted":true,"_uuid":"51cd325870f3bac8638c8689be1fe0691952b92c"},"cell_type":"code","source":"# Checking missing data\ndataset_mis = dataset.isnull().sum()\nprint(dataset_mis)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"51b5eb08c3821ad009620e69fb2b4a0c8df8111c"},"cell_type":"markdown","source":"And, it is clearly that \"Albumin_and_Globulin_Ratio\" attribute has 4 missing values. Let's solve it using Imputer."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f9e23db8e80dae3907a38d3cd25523fe6a6a6a9"},"cell_type":"code","source":"# Taking care of missing data!\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nX[:, 2:10] = imputer.fit_transform(X[:, 2:10])","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"675d6157811a715b1edacf81a6fb433f8fab0058"},"cell_type":"markdown","source":"Because we have one categorical attribute \"Gender\", we need to encode this n order our machine learning model can undertand. We implement LabelEncoder to make a unique labels for \"Male\" and \"Female\" differently. Then we implement OneHotEncoder to make a dummy variables."},{"metadata":{"trusted":true,"_uuid":"754b87d0c53623c86179eedb0ad86fecbabc78ea"},"cell_type":"code","source":"# Encoding the categorical values\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 1] = labelencoder_X.fit_transform(X[:, 1])\nonehotencoder_X = OneHotEncoder(categorical_features = [1])\nonehotencoder_X.fit_transform(X).toarray()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"30742b952713b79f6fea674133f8b654e7e46f11"},"cell_type":"markdown","source":"Spliiting the dtaset into 75 % of training data and 25% of testing data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f2746d7ff27d2871309acc25bcd1be7d5d151d02"},"cell_type":"code","source":"# Splitting the dataset into trainig and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"09ccc44ea8e64724ea23eee8b2e75b14d2d3fd39"},"cell_type":"markdown","source":"To make our dataset in a good scale, we need to implement feature scaling. Because if we see the data, the have different scales. For example \"Age\" and \"kaline_Phospota\" have larger scale than other attributes."},{"metadata":{"trusted":true,"_uuid":"9ce0b7a04b0154a17b65de6d6b33d4e2ddf4fc3e"},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"beff3df6910fdd95ce02d544491dc21e82bd93f3"},"cell_type":"markdown","source":"Sometimes, feeding our machine learnig model with all attributes that we have is not the best option. e can implement dimesnionality reduction to reduce features given to the model and decrease the computation cost but with still have the same spirit to obtain the same or even better classifier. In fact, here I only use 6 components of Principle Component Analysis (PCA) resulting a better classifier than using all 10 attributes from the data. The process to find the number of n_components is as below:"},{"metadata":{"trusted":true,"_uuid":"5d4425c35a9107bf7b122795722acf1a0dfe5bd5"},"cell_type":"code","source":"# Dimesnionality Reduction using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX_train  = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Explained Variance:\\n\", explained_variance)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"ee5a4810734d3d23207f35344bb331e8ae4869ac"},"cell_type":"markdown","source":"From the explained variance above, I decide to use only the first 6 features of PCA which already represent more than 80% of total variance. This number is more than enough to feed the machine learning model and has represented all the features. So, let's update our n_compnents = 6."},{"metadata":{"trusted":true,"_uuid":"0947b2ec78c69a0baa9471c71836bbd46702f98a"},"cell_type":"code","source":"pca = PCA(n_components = 6)\nX_train  = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"34cb248d6963d6b13f09ccdc2db4feac1151eb02"},"cell_type":"markdown","source":"# 2. Prediction Model - Random Forest Classifier\n\nWe firstly implemented Random Forest Classifier without tuning the hyperparameter. The idea is we will compare the result with the one that will be tuned based on Grid Search method"},{"metadata":{"trusted":true,"_uuid":"39b3e55ba67f874335d1ed3255380ec9b2af4c82"},"cell_type":"code","source":"# Fitting Random Forest model into the training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train, y_train)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5137c340de545e2b60038b8fc907a01ca63a662"},"cell_type":"code","source":"# Making a prediction\nfrom sklearn.metrics import classification_report\ny_pred = classifier.predict(X_test)\ntest_accuracy = classification_report(y_test, y_pred)\nprint(test_accuracy)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"5dc4bce087ed1f29c125dc8019c0d95a4fe31a05"},"cell_type":"markdown","source":"We only got 67% of testing result from the model without doing any tuning. "},{"metadata":{"_uuid":"c0257acad180075d243ac1240f20e37195403b1f"},"cell_type":"markdown","source":"# 3. Grid Search\nWe then implemented Grid Search to find the best hyperparameters. You can read from sk-learn documentation for Random Forest Classification [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to know more the hyperparameters that can be tuned to get optimum result."},{"metadata":{"trusted":true,"_uuid":"2714c0eb5b87fba1cd60cb5e83bb6c09b0ad3215"},"cell_type":"code","source":"# Grid Search to find the best tuning\n# Params for Random Forest\nparameters = [{'criterion' : ['gini', 'entropy'],\n               'max_depth' : [5, 6, 7, 8, 9, 10, 11, 12],\n               'max_features' : [1, 2, 3],\n               'n_estimators' : [14, 15, 16, 17, 18, 19],\n               'random_state' : [7, 8, 9, 10, 11, 12, 13]}]\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           n_jobs = -1, cv = 10)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nprint(\"Best accuracy of the model for the training set is:\", best_accuracy)\nbest_params = grid_search.best_params_\nprint(\"Best parameters of the model for the training set is:\", best_params)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"40cee4833cbecb60e442c1db59d83b12a1e842a6"},"cell_type":"markdown","source":"After running the Grid Search, we can find that the best accuracy for the training data is up to 75% with best_params: \n - criterion = 'gini'\n - max_depth = 9\n - max_features = 1\n - n_estimators = 16\n - random_state = 10\n \n We then include the best_params in our RandomForestClassifier to update the model. \n \n*Notes: On the code we implemented a distributed computing with \"n_jobs = -1\" in order we can use all core resources of our computer*"},{"metadata":{"trusted":true,"_uuid":"49b0a04165f4a59a44da4141f72ebebe1183b590"},"cell_type":"code","source":"# Tune the hyperparameters of Random Forest Classifier based on best_params resulted from Grid Search method\nclassifier = RandomForestClassifier(criterion = 'gini',\n                                    max_depth = 9,\n                                    max_features = 1,\n                                    n_estimators = 16,\n                                    random_state = 10)\nclassifier.fit(X_train, y_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72661d3214cd4177bb16bcdcee000073593a9b2b"},"cell_type":"code","source":"# Let's check the test accuracy after optimised\ny_pred = classifier.predict(X_test)\ntest_accuracy_optimized = classification_report(y_test, y_pred)\nprint(test_accuracy_optimized)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"92c6aeffe82f9a143da6b0a93d0a65dcde80e686"},"cell_type":"code","source":"# Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", cm)\ntest_accuracy_optimized_cm = (cm[0,0]+cm[1,1])/146 #146 is the total number of testing data\nprint(\"\\nTesting accuracy based on the Confusion Matrix:\\n\", test_accuracy_optimized_cm)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"3fdb51610e147dacde08c9207baa00c2b707a24f"},"cell_type":"markdown","source":"The model indeed increase the accuracy for predicting the test result from 67% up to 71% after doing the hyperparameter tuning."},{"metadata":{"_uuid":"d887c21f3edf8d03282a627c55f917301710133d"},"cell_type":"markdown","source":"# K-Fold Cross Validation\n\nTo make sure that our model is not over fitting when it is implemented to the new set of test data, we then implemented K-Fold Cross Validation to know the mean accuracy and the the standard deviation from given 10 different set of validation."},{"metadata":{"trusted":true,"_uuid":"878eb7c9383f3e0d362df04bdbed6b5d2441770f"},"cell_type":"code","source":"# K-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,\n                            X = X_train, y = y_train,\n                            cv = 10, n_jobs = -1)\nprint(\"Showing all 10 of K-Fold Cross Validation accuracies:\\n\", accuracies)\naccuracies_mean = accuracies.mean()\nprint(\"\\nMean of accuracies:\\n\", accuracies_mean)\naccuracies_std = accuracies.std()\nprint(\"\\nStandard Deviation:\\n\", accuracies_std)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"5370826235c1165d837c0ab66f0148b3e66034e6"},"cell_type":"markdown","source":"From the validation step, we can clearly see that the mean of accuracy for our model is 75% with standard deviation only 3% meaning that our model has very slight variance in the different set of validation. So, we can be confident enought that our model is not likely overfitting. Yeah, we got a good result at the end!\n\nNotes for improvement:\n1. Try to use another model like Neural Networks to get a better result. I have tried to implement Naive Bayes and SVM but they did not beat the Random Forest even after implementing Grid Search.\n2. Use tpot to find the best model with the best hyperparameters. Not forget to mention XGBoost for the same spirit to get the better result. I would love to implement them if I have time in the near future.\n\nEnjoy Machine Learning!\nRoyan Dawud Aldian"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}