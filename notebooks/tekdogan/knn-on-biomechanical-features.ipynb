{"cells":[{"metadata":{"_uuid":"0e470b2dcadf5d24cbd73f3258d61a6afb522dfc"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"0db5b38c674d096afd03136a35975e3c307a0d48"},"cell_type":"markdown","source":"# kNN on Biomechanical Features of Orthopedic Patients\n<br></br>\n![orthopedic-img-rep](https://kaggle2.blob.core.windows.net/datasets-images/2374/3987/4a58a17df89fda0afe579dde6b7f25fa/dataset-cover.jpg)\n\n## Contents\n* [Introduction](#1)\n* [Importing Libraries](#2)\n* [Fetching Dataset](#3)\n* [Data Munging](#4)\n* [Correlations](#5)\n* [K Nearest Neighbors](#6)\n    * [What is kNN?](#7)\n    * [Overview of the Features](#8)\n    * [How it Works?](#9)\n    * [k=3](#10)\n    * [k=7](#11)\n* [Learning with kNN](#12)\n    * [Normalizing](#13)\n    * [Test and Train Variables](#14)\n    * [Initialize and Train the Classifier](#15)\n    * [Best k Values](#16)\n* [Conclusions](#17)\n"},{"metadata":{"_uuid":"b2391c2b35ed60687292fa91516062622fc97209"},"cell_type":"markdown","source":"<a id=1></a>\n## Introduction\nHere in this kernel, firstly I have to explore the [Biomechanical features of orthopedic patients dataset](https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients) by using data science techniques. Then I will be explaining what is k-Nearest Neighbors method (which we will be using in this kernel for learning), and then implement it on the dataset and get the results."},{"metadata":{"_uuid":"658a78ca6c43b6c98f025f1b0f19d7b28c87dcb6"},"cell_type":"markdown","source":"<a id=2></a>\n## Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"322375c5b76e57e61c73f01be3ce52c3f445957b"},"cell_type":"markdown","source":"<a id=3></a>\n## Fetching Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66acaafeefe80791fd5b504082d0fe0bf6686cb1"},"cell_type":"markdown","source":"<a id=4></a>\n## Data Munging"},{"metadata":{"trusted":true,"_uuid":"af8f01e14fcb1ff933a71be26b2e6376fd56f56d"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65bb72c6db1015593ca91c9ed65ce2187653e189"},"cell_type":"markdown","source":"Dataset consists of 310 entries, each indicates an individual patient; and 7 attributes, where six of them are biomechanical numeric features and the last one (`class`) is the target feature.  \n\nEach patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):  \n\n* `pelvic incidence`\n* `pelvic tilt`\n* `lumbar lordosis angle`\n* `sacral slope`\n* `pelvic radius`\n* `grade of spondylolisthesis`"},{"metadata":{"_uuid":"0424d701604f77c065e6ac517182fe145965c0e7"},"cell_type":"markdown","source":"Let's take a glimpse of data by using `head` method."},{"metadata":{"trusted":true,"_uuid":"0154d09c051e4e18b844e4f6806585ddfde2f7bc"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdeb307e0164ecc0ee50e2edae3f07f99da87592"},"cell_type":"markdown","source":"Let's investigate our target feature (`class`) a bit more."},{"metadata":{"trusted":true,"_uuid":"a8cee949bfe61ab7b6749830e0176a5ae4de15ab"},"cell_type":"code","source":"data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6df71e07cc5df52c63acc0b2ca3f5b4d4eb185f"},"cell_type":"markdown","source":"Our label seems like an object-type, which we do not prefer, so let's convert it to binary numeric-type.  \n\nAfter conversion:\n* `Abnormal`  → `1`\n* `Normal` → `0`"},{"metadata":{"trusted":true,"_uuid":"f995273d0a96cc1b40b0f1ce1514a7bf7119e987"},"cell_type":"code","source":"data['class'] = [1 if each == 'Abnormal' else 0 for each in data['class']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9043b58f06fb7b1c2416c2a5fffcf9c421c57461"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6d0b35db26332811ea575434719226c9ce8eec2"},"cell_type":"markdown","source":"<a id=5></a>\n## Correlations\n\nIt's perfect time to examine correlations, since we just converted our target feature to numeric-type."},{"metadata":{"trusted":true,"_uuid":"4dc8ccf50db562e80e4c828eba1e3ab309ffd4d3"},"cell_type":"code","source":"f, ax = plt.subplots(figsize = (10,10))\nsns.heatmap(data.corr(), annot=True, linewidths=.4, fmt= '.2f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6be7127df6d45f4da953102cd9c1de90ffb2bcf"},"cell_type":"markdown","source":"Correlations between `class` feature and other features:  \n\n* `pelvic incidence` : `0.35`\n* `pelvic tilt` : `0.33`\n* `lumbar lordosis angle` : `0.31`\n* `sacral slope` : `0.21`\n* `pelvic radius` : `-0.31`\n* `grade of spondylolisthesis` : `0.44`  \n\nSeems like all the features are *slightly correlated* with label, excepting `pelvic radius`."},{"metadata":{"_uuid":"80e1c7885e5547326077c821a49f985064eb53ca"},"cell_type":"markdown","source":"To visualize the correlations in more details, let's use `scatter_matrix`:"},{"metadata":{"trusted":true,"_uuid":"2d5f2cfdece4bc67f55e9b7d759c8cbdd1a24375"},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(data, alpha = 0.8, figsize = (15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfe819683ce66030c5d770a319ea79bf17fb6d84"},"cell_type":"markdown","source":"<a id=6></a>\n## k Nearest Neighbors\nWe will be using the method named **k Nearest Neighbors** for learning, so let's dive in to see what is under the hood!\n<a id=7></a>\n### What is kNN?\nIn pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.  \n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. [Source](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)"},{"metadata":{"_uuid":"615ab53704acf4230df5c8b8110803d6b4864696"},"cell_type":"markdown","source":"<a id=8></a>\n### How it Works?\nLet's go through a visual example to have the full understanding of it!\n\nWe have two types of data in the dataset (`class 1` and `class 2`), and a test data which is unlabeled yet. These are how they look like:\n![knn_overview](http://i68.tinypic.com/2q2nx1w.jpg)\n<a id=9></a>\n### Overview of the Features\nAnd our data looks like this: (ignore circles for now)\n![knn](http://i67.tinypic.com/64nwib.jpg)\n<a id=10></a>\n### k=3\nIf we pick `k=3`, which means our test data wil be classified by nearest three neighbours, that are 2 red squares (`class 1`) and 1 blue triangle (`class 2`). The majority is obviously `class 1`, so the test data will be classified as `class 1`, which is the red square.  \nHere the schema below visualizes what I mean, where the inner circle is the test space.\n![knn3](http://i63.tinypic.com/2uz9jww.jpg)\n<a id=11></a>\n### k=7\nLet's try `k=7` instead. Now there are 3 red squares and 4 blue triangles, which means blue guys are the winner of voting system.  \nThe outer circle is the new test space as we change `k` to `7`.\n![knn7](http://i64.tinypic.com/2ut26tg.jpg)"},{"metadata":{"_uuid":"536e3c41fa2d54ac4f848f5c5a040ffd05ca14f3"},"cell_type":"markdown","source":"<a id=12></a>\n## Learning with kNN\n\nNow we know what kNN means and how it works. Let's implement in on our dataset with `scikit-learn` library."},{"metadata":{"_uuid":"6850b33f04c5520a889d45ac92a04254f3756485"},"cell_type":"markdown","source":"Firstly create our `x` and `y` variables:  \n* `y` →  target feature (label)\n* `x` →  all the features for training, excepting label"},{"metadata":{"trusted":true,"_uuid":"6b254193b8b037902f0548b2c6965f3dc933f506"},"cell_type":"code","source":"y = data['class']\nx = data.drop(['class'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1f4da5b0ba31d9330831bfd6226363160b87183"},"cell_type":"markdown","source":"<a id=13></a>\n### Normalizing\nNormalizing variables is vital for the sake of healthy learning. To scale all the values between 0 and 1, we have to use the following simple formula:  \n$$\\large x = \\frac{x - min(x)}{max(x) - min(x)} $$  \nSo you can think $\\large53$ as $\\large0.53$ after normalization, if the minimum is $\\large0$ and the maximum is $\\large100$ for a column."},{"metadata":{"trusted":true,"_uuid":"df0732eb4bae821b5b825fc026e25d4c478e3747"},"cell_type":"code","source":"x = (x - np.min(x))/(np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8698c95523737c0d39b0f5fcb734eba33d0b1ea8"},"cell_type":"markdown","source":"<a id=14></a>\n### Test and Train Variables\nWe have to split our data to create train and test variables. To do so, we will be using `sklearn`'s `train_test_split method`.  \n\nWe set the`test_size` parameter to `0.2`, so the train values will be randomly 80% of the data.  Let's briefly describe what all four values correspond to:\n* `x_train` : randomly 80% of data with features of `x` (`pelvic_incidence`, `pelvic_tilt_numeric`, etc.)\n* `x_test` : randomly 20% (the rest) of data with features of `x`\n* `y_train` : randomly 80% of data with feature of `y` (`class`, the target feature)\n* `y_test` : randomly 20% (the rest) of data with feature of `y`  \n\nLet's visualize what I mean:\n\n![train_test](http://i64.tinypic.com/v5agox.jpg)"},{"metadata":{"trusted":true,"_uuid":"9e5d4cfc8920d9e8ff2c451e80f097fd6c346df2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bf0d51050c033ad4ca4637c8bd6cd7ae23b6d00"},"cell_type":"markdown","source":"<a id=15></a>\n### Initialize and Train the Classifier\nAnd initialize the classifier object from `sklearn.neighbors`, then train it by using `fit` method.  \n(`n_neighbors` parameter (k size) is set to `5` by default, if we do not indicate it implicitly)"},{"metadata":{"trusted":true,"_uuid":"b2abb89dd3c2772d99e7b63cedd0e127d7178b6b"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c1342bfeafa176b00a589a45548c0cdaa06a27f"},"cell_type":"markdown","source":"<a id=16></a>\n### Best k Value\nNow let's create a list named `score_list`, and keep all the accuracy values of the algorithms trained in a range of (1,15), which is the number of `k`.  \nThen visualize the accuracy values with a line plot."},{"metadata":{"trusted":true,"_uuid":"16727ed94a2d82a88082e5e9f2beb3fa1e8f579d"},"cell_type":"code","source":"score_list = []\n\nfor each in range (1,15):\n    knn_o = KNeighborsClassifier(n_neighbors = each)\n    knn_o.fit(x_train, y_train)\n    score_list.append(knn_o.score(x_test, y_test))\n\nplt.figure(figsize = (10,10))\nplt.plot(range(1,15), score_list)\nplt.xlabel('k values')\nplt.ylabel('accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a789d7442506b869fc159a199f42a6615645aa1"},"cell_type":"markdown","source":"<a id=17></a>\n## Conclusions\n\n* We are getting the hıghest accuracy score as we choose `k` as `5` or `7`.\n* The highest score we could get is `0.79`, which is not a pretty good score."},{"metadata":{"trusted":true,"_uuid":"a157a13ca40c4ce6012a9823519b484965256533"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}