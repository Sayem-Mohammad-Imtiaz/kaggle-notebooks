{"cells":[{"metadata":{"_uuid":"11a838f7f4b727fa37e8ef251c841a9616b72d88"},"cell_type":"markdown","source":"# INTRODUCTION\n* **In this kernel I ' am going to demonstrate application of some Machine Learning on Heart Disease UCI dataset **\n* **This dataset aims to detect whether there is heart attack on patient or not .**\n* **We are going to use some visualization tool from : **\n* **seaborn**\n* **mathplotlib**\n* **plotly **\n* **and some tools from  scikit  for Machine Learning**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns #seaborn\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d6faa8de641b909fb7fc354ce94a96e56737f19"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4865f0482ccb026194b5cba4071b41a53771c099"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"022b28cea8021eda51b7278898abd22714a49993"},"cell_type":"markdown","source":"**** *Dataframe Columns and Their Meanings**\n* **age**  : age in years\n* **sex** : ( 1 = male; 0 = female)\n* **cp**  : chest pain type\n* **trestbps** : resting blood pressure (in mm Hg on admission to the hospital)\n* **chol** :  serum cholestoral in mg/dl\n* **fbs** : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* **restecg** :  resting electrocardiographic results\n* **thalach**  : maximum heart rate achieved\n* **exang** : exercise induced angina (1 = yes; 0 = no)\n* **oldpeak** : ST depression induced by exercise relative to rest\n* **slope**  : the slope of the peak exercise ST segment\n* **ca** : number of major vessels (0-3) colored by flourosopy\n* **thal** : 3 = normal; 6 = fixed defect; 7 = reversable defect\n* **target**  : 1 or 0**"},{"metadata":{"trusted":true,"_uuid":"2b1cd882d5e8e52edbf093e50eab9099302eaf82"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bb4c404883285903c52f6e31ad8377ec6fbdfb1"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1c6371ecb936913e007508b7d31c8961d5c040e"},"cell_type":"code","source":"df.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0e1819952479a1732166bc5845d922380ec1fd0"},"cell_type":"code","source":"#Correlation Map\nf,ax  = plt.subplots( figsize = (12,10))\nsns.heatmap(df.corr() , annot = True , linewidth = 10 , linecolor = 'black' , fmt = '.1f' , ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97106b2935366b852f4010784055d967ddd3252c"},"cell_type":"markdown","source":"**Since there are no strongly correlated elements its better not using linear regression for ML.**"},{"metadata":{"trusted":true,"_uuid":"e7c5978d0534de2f3b2bf0873af121d5c8043063"},"cell_type":"code","source":"sns.countplot(df.target)\nplt.title(\"Number of Diseases\")\nplt.xlabel(\"NOT DISEASE                               DISEASE\")\nplt.ylabel(\"Count\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f29cb05e18a41dcbeb41a4d62e766c0100e70c4e"},"cell_type":"code","source":"df_disease  = df[df['target'] == 1]\n\nsns.countplot(df_disease.sex,palette=\"Set3\")\nplt.title(\"Hearth disease occurence according to sex\")\nplt.xlabel(\"FEMALE                                      MALE\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94c762fb667c6e284001383197bd9d10c0023b74"},"cell_type":"markdown","source":"**We may infer that males are more likely to have an hearth disease than female**"},{"metadata":{"trusted":true,"_uuid":"6c036e798b87b264ea7d0b7b1daca5829a41d4be"},"cell_type":"code","source":"#%% Box Plot\n # Classifaction  \n #sex M = 1 F = 0\n #choloesterol\n #hue = Target 1 is disease , 0 is not disease\n #outlayer can be seen \n \nsns.boxplot(x =\"sex\", y = \"chol\" , hue = \"target\" , data  = df , palette = \"PRGn\")\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fc75bea2794b9bddc9937d00c358adbdf57d5ec"},"cell_type":"markdown","source":"* **#1 is Male 0 is female. Target is 1 means there is disease . Target is 0 means there is no disease .**\n* **Having a higher chololesterol may not lead to having a hearth disease with respect to our data . **\n* **And females  are more likely to have higher cholesterol than male.**"},{"metadata":{"_uuid":"4d741e657529699419ae4884c49fc9bf30d40330"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"03b363cc5c3a7dcd94ce8f18b004cbab49a5c94e"},"cell_type":"code","source":"#%% Swarmn Plot \n#FOR CLASSIFICATION\nsns.swarmplot (x= \"sex\", y =\"cp\" , hue = \"target\" ,data = df )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb057b31e2021f76cf9126bdd426d371305fec90"},"cell_type":"markdown","source":"* **Males with no chest pain more likely to have no heart disease **\n* **Females may not have  chest pain eventough they have an heart disease**\n* **People with chest pain are more likely to have an heart disease**\n* **Some males may have an chest pain eventough they do not have an heart disease**"},{"metadata":{"trusted":true,"_uuid":"24a09ec0c4174a1cc46a543564779b2d993b7d0c"},"cell_type":"code","source":"#FOR CLASSIFICATION\nsns.swarmplot (x= \"sex\", y =\"chol\" , hue = \"target\" ,data = df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf2c5a8190210f719503d470ef0d3063795bba89"},"cell_type":"markdown","source":"* **Mostly we can infer that if a woman has an high cholesterol , she more likely to have an heart disease**\n* **For males , its hard to classify whether they have a heart disease or not**"},{"metadata":{"trusted":true,"_uuid":"79a357bace4135bafaf3850462d310d58ce48553"},"cell_type":"code","source":"#FOR CLASSIFICATION\nsns.swarmplot (x= \"sex\", y =\"trestbps\" , hue = \"target\" ,data = df )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eec303cf492d82f86c0f672cbae317a06cc10bd0"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"c15f680830ce4a582b366ea451563103cf49758d"},"cell_type":"code","source":"sns.swarmplot(x = \"sex\" , y =\"fbs\", hue =\"target\", data = df )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63005d588379251ccc74f3bdb4179256d54ae261"},"cell_type":"markdown","source":"* **fbs** : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* **For female we can say that fasting blood sugar is not a good property to undestand the hearth disease since female may have an heart disease eventough they do not have blood sugar**\n* **For males who have high fasting blood sugar are more likely to have an heart disease**"},{"metadata":{"trusted":true,"_uuid":"6b5e78ebe11d40e20381e45fabbf242829179fe8"},"cell_type":"code","source":"sns.swarmplot(x = \"sex\" , y =\"thalach\", hue =\"target\", data = df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19248860e8ed13dfcbb045b66e92350e000b6cce"},"cell_type":"code","source":"df_male = df[df[\"sex\"] == 1]\ndf_female =df[df[\"sex\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b996c3ccf59cd464334f02504ea762c29c0f893"},"cell_type":"code","source":"# Pair Plot\nplt.figure(figsize=(10,8), dpi= 80)\nsns.pairplot(df, kind=\"scatter\", hue=\"target\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24ffd57a89bdf3b81d0d5be6e3b282598e3c6a36"},"cell_type":"code","source":"#%% Kde Plot # \n\nsns.kdeplot(df.target ,df.cp , shade =True ,cut = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"142a9a039d7d81e828ccc43ea2dccd98a909cdc0"},"cell_type":"markdown","source":"**It can be concluded from KDE plot that people with heart disease are more likely to have a chest pain in type 2 and \npeople who do not have heart disease are more likely to not have a chest pain**"},{"metadata":{"trusted":true,"_uuid":"efcebfdc90d64ed8a0aed1e2b43739cd51954ae9"},"cell_type":"code","source":"df.head(5)\ndf_FV = df_disease[[\"trestbps\",\"chol\",\"thalach\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbc2005acb6b9210eaad4451722dcf3fa16f5444"},"cell_type":"code","source":"#%% Violin Plot  \n\n#Distribution of trestbps , chol , thalach in patients who HAVE Heart disease\n\n# Show each distribution with both violins and points\npal = sns.cubehelix_palette(2, rot=-.5, dark=.3)\nsns.violinplot(data=df_FV, palette=pal, inner=\"points\") \n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ba14335188f3028c85e7b91de736b4dd5ea1459"},"cell_type":"markdown","source":"* **In this violin plot I have select the people who have a heart disease **\n* **Distribution of trestbps , chol and thalach can be seen in this figure**"},{"metadata":{"trusted":true,"_uuid":"f8679cb8b43f174d4ca40d2d3e381c797b1d1401"},"cell_type":"code","source":"data2 = df_disease[[\"restecg\",\"exang\",\"oldpeak\",\"thal\",\"slope\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2200903a0f7ace3e64a2635826ab2ed1b4ba5d3a"},"cell_type":"code","source":"ax = sns.violinplot(data=data2, palette=\"Set2\", inner=\"points\",scale=\"width\",split = True) \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22a726bffaa31a4d02e805064704c440e89cb1a0"},"cell_type":"markdown","source":"**** *Dataframe Columns and Their Meanings**\n* **restecg** :  resting electrocardiographic results\n* **exang** : exercise induced angina (1 = yes; 0 = no)\n* **oldpeak** : ST depression induced by exercise relative to rest\n* **slope**  : the slope of the peak exercise ST segment\n* **ca** : number of major vessels (0-3) colored by flourosopy\n* **thal** : 3 = normal; 6 = fixed defect; 7 = reversable defect\n"},{"metadata":{"trusted":true,"_uuid":"f1182630416a372168ce14fd43de3b9c983f36f3"},"cell_type":"code","source":"df_2 = df.drop(['age','sex'],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a6a2a3bf9175abb2c2129d16e782a8a8ef82bc7"},"cell_type":"code","source":"color_list = ['red' if i== 1 else 'green' for i in df_2.loc[:,'target']]\npd.plotting.scatter_matrix(df_2.loc[:, df_2.columns != 'target'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b615d49d168b7860b19afa364395abbc9ca35120"},"cell_type":"markdown","source":"**INTRODUCTION TO MACHINE LEARNING**\n"},{"metadata":{"_uuid":"4916ce9bd6f3ee1219a351d051c05a59ca91c563"},"cell_type":"markdown","source":"    * **LOGISTIC REGRESSION\n* **Logistic regression is a widely known machine learning technique which is mostly used for CLASSIFICATION**\n* **Logistic regression gives best solution ,when our dependent variable is binary in this case its 'target' .**\n* **We may have more than 1 independent variable which will used for predicting our dependent variable **\n* **Independent variables may be in form of binary or interval**\n* **Sometimes we may need normalize and scale our inputs for getting better result in terms of predicting**\n"},{"metadata":{"trusted":true,"_uuid":"a52bebaf270f10d70e99db4f68432b29dbbc8308"},"cell_type":"code","source":"x_data = df.drop(['target'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"091c4b7e1f23bde6b6b5b2881110ee64ac5ef475"},"cell_type":"code","source":"y = df.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be3b1cebc39a225cbe3286d538182cfec8163c01"},"cell_type":"code","source":"# %% normalization\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7234e648967aacfca2b6b678fc2aea322a40293e"},"cell_type":"markdown","source":"**We normalized our data in order to make all features balanced . In other words , We do not want 1 feauture dominate all other feautures .****"},{"metadata":{"trusted":true,"_uuid":"3659c14b87e340d7b00578ad0d276a8bc30e151c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09fd2cbe9c83c5bf4d668375f2191ec37bc9d119"},"cell_type":"markdown","source":"* **By using sklearn liblary , we are dividing our data . 20 percent of the data will be used for testing our model. **\n* **%80 percent of the data will be used for training our model**"},{"metadata":{"trusted":true,"_uuid":"473e566be74d1c1e0087c1491e91f3b86379bb83"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy of LogisticRegression Model is  {}\".format(lr.score(x_test,y_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bd69e43be9e0481f4cdbd0563591a9ecf828afe"},"cell_type":"markdown","source":"**KNN ( K Nearest Neighbours)**\n* KNN is a machine learning algorithm which is widely used for classification .\n* Main aims of this algorithm is finding k nearest data to the point which is going to be classified .\n* By looking it's neighbours, algorithm decides to put data into which class .\n![](http://)<br>\n<br>\n<br>\n<br>\n <img src=\"https://dslytics.files.wordpress.com/2017/11/knn.png\" width=\"400px\">\n <br>\n <br>\n*  In this picture k is chosen 3 .\n*  Star is the point to be classified\n*  X1, X2 ara properties of data .\n*  Since 2 of the 3 nearest neighbours are class B , star will be classified as class B."},{"metadata":{"trusted":true,"_uuid":"0bfece7ac78d0f54e53bb5e8b8059d5de415557f"},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\n#x,y = df.loc[:,df.columns != 'target'], df.loc[:,'target']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f610e1a480963616c3359e503538ec975935b86"},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 42)\n\n#%% KNN Classifier \nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=5) accuracy is: ',knn.score(x_test,y_test)) # accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6553a39911ca9a3a293164ed3ab818aad59023a","scrolled":true},"cell_type":"code","source":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76955e4f0851379a1ff096b17f364563d28fba44"},"cell_type":"markdown","source":"**#DECISION TREE**\n* Decision Tree(CART ) is also known as Classification and Regressiion Trees.\n* In this data we are going to use for classification\n* **ADVANTAGES OF USING DECISION TREES**\n* Its easy to use and implement\n* Not needed too much data prepearing\n* It looks like its similar to human thinking form \n* Categorical data can be used \n* **EXAMPLE**\n <img src=\"https://s3-ap-southeast-1.amazonaws.com/he-public-data/Fig%201-18e1a01b.png\" width=\"500px\">"},{"metadata":{"trusted":true,"_uuid":"cf3fef6978f0ea3207e83c55d2a1d1e570ec75f3"},"cell_type":"code","source":"\n# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53842a81779d8a097b96d3328ea155928bc9f64a"},"cell_type":"code","source":"#%% decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"decision tree score: \", dt.score(x_test,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"460d51e5ab4158aef008084786c6708b2cdef417"},"cell_type":"markdown","source":"* **#Random forest**\n<br>\n<br>\n* **Randomis simple , flexible machine learning algorith which is widely used for both classification and regression **\n* **Basically it consists of many decision trees **\n* **The final results depend upon the results of these decision trees**\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*i0o8mjFfCn-uD79-F1Cqkw.png\" width=\"500px\">\n"},{"metadata":{"trusted":true,"_uuid":"796c24bff96ef0bc7e9e702c1dca060aa5c1bf6a"},"cell_type":"code","source":"#%%  random forest\n# %%\n# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algorithm result: \",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e52eca811c8ddc8b8e2cab1a714471092b19669a"},"cell_type":"markdown","source":"**SUPPORT VECTOR MACHINE **\n* SVM is an machine learning algorithm which is used for both classification and regression .\n* Main aim of SVM is finding the hypherplane which is the farthest from support vectors and separates the classes\n* In this picture the middle hyperplane should be chosen \n<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2014/10/xyplot1.png\" width=\"500px\">"},{"metadata":{"trusted":true,"_uuid":"337a8417624d719386506f41e61b67f07ecef9d8"},"cell_type":"code","source":"#%%SVC Code and accuracy \n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=42)\n # %% SVM\n \nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n \n# %% test\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e678c277e6ff3d38bcdb4bd1dcac8c59fe0eeec"},"cell_type":"markdown","source":"* **NAIVE BAYES ALGORITHM**\n<br>\n<br>\n<br>\n<br>\n<br>\n* In this Algorithm , Bayes theorem of probability is used to predict the class of unknown data set.her\n* Bayes Theorem assumes , all properties that we used for deciding are independent of each other\n<img src=\"http://uc-r.github.io/public/images/analytics/naive_bayes/naive_bayes_icon.png\" width=\"500px\">\n<br>\n<br>\n<br>\n<br>\n* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n* P(c) is the prior probability of class.\n* P(x|c) is the likelihood which is the probability of predictor given class.\n* P(x) is the prior probability of predictor.\nFor further Information you can check  the source website<a href = \"https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\">"},{"metadata":{"trusted":true,"_uuid":"1462569df0c91d496858dfd692a23be06657ddcd"},"cell_type":"code","source":"# %%Naive Bayes Application\n# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n \n# %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b54c5d78dac09741d2c8709b9dfd2c2fd1b505d"},"cell_type":"markdown","source":"# Algorithm Performance \n* For understanding algorithm's performance **CONFUSION MATRIX** can be used .\n* Confusion matrix is a matrix which show how did the algorithm classified the data . \n* For example ,  lets think we are making binary classification . \n* There are 4 possibilities. Firstly , \n* 1) We can classifty actual true data as true (True positvie)\n* 2) We can also classify actual true data as false in other words we can classify 1 as 0 (False Positive)\n* 3)We can classify actual false data as True .(False Negative)\n* 4) We can classify actual false data as False(True Negative )\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png\" width=\"500px\">\n"},{"metadata":{"trusted":true,"_uuid":"68394b9088250f09e028f20b33c854aae3eb4abd"},"cell_type":"code","source":"# %%\n# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n\n\n#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cda98828eed0d927f57e3ff58a214d343b492f7"},"cell_type":"code","source":"\n##CONFUSION MATRIX FOR RANDOM FOREST MODEL\ny_pred = rf.predict(x_test)\ny_true = y_test\n#%% confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98380fdce14ec032b1b4375415884b67dfd851a6"},"cell_type":"markdown","source":"# Cross Validation\n* Score of the Machine Learning technique also depends on the how did train and test data are split . \n* Since the data split randomly , we can confront with extraordinary good or bad results than the actual data will give.\n* In order to eliminate this inconsistency , the technique called **'K Fold cross validation is used '**\n<br>\nTrain data is split into K part . \nIn each iteration ,1 part of the splitted dat is used as test data and the other parts are used as train data. Then we find test accuracy for this split.\nAt the end we have ,  K many test accuracy result . \nAt the end , we take avarege of these results and get the final test accuracy.\nBy using these techinque , we are eliminating unexpected result of random distribution .\n\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*me-aJdjnt3ivwAurYkB7PA.png\" width=\"500px\">"},{"metadata":{"trusted":true,"_uuid":"f0ae727062a8b11452b7a277fb9639722774921e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70dc5180a04ccfbd3383537899b5717416b56adc"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8114fdda621f889fb7675a7d04d2f15e4d346908"},"cell_type":"code","source":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn , X = x_train , y = y_train , cv = 10)\nprint(\"avarege accuracy : \" ,np.mean(accuracies))\nprint(\"average std\" ,np.std(accuracies))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5e29172a7f6930066b4513de8219f48798b82a3"},"cell_type":"code","source":"knn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764f3d00d51c91fce9a0a81fff4df8f301abf0f4"},"cell_type":"markdown","source":"# Grid Search Method"},{"metadata":{"trusted":true,"_uuid":"78e02cc78babebf567db826e8cb6e82bcb407d83"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7ee53a3c29b8f82bb4bfd7e08c034fccf8ce437"},"cell_type":"code","source":"# %% grid search cross validation for knn\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x,y)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned best score: \",knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66a880c98ff57761df81b95f73e95ecf0baa4fb9"},"cell_type":"code","source":"# STARTING FROM BEGINNING\nx_data = df.drop(['target'],axis = 1) # We drop target for train and test data\ny = df.target.values # y is our target where there is disease and not \n\n#NORMALIZATION of x \nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n\n#We split our data as test and train\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\n#We fit our data\nknn.fit(x_train,y_train)\nlr.fit(x_train,y_train)\ndt.fit(x_train,y_train)\nrf.fit(x_train,y_train)\nsvm.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b754dbe4a2f3aa958e348a6b5ffb1fa7608a369"},"cell_type":"code","source":"x_axis  = ['logistic regression', 'knn' , 'decision_tree' , 'random_forest' , 'support_vector_machine', ]\ny_results =[lr.score(x_test,y_test),knn.score(x_test,y_test),dt.score(x_test,y_test),rf.score(x_test,y_test),svm.score(x_test,y_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faf6c94f97e1b854a76e1c506ef969b9ab389e33"},"cell_type":"code","source":"plt.figure(figsize=(11,10))\nplt.xlabel(\"Machine Learning Algorithms\")\nplt.ylabel(\"ML SCORES\")\nplt.title(\"ML Algorithms vs Test Scores\")\nplt.plot(x_axis ,y_results)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a901ba6a58aa029dd4b997fcf02643f26866368"},"cell_type":"code","source":"y_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c1a8496c05f362f14e479631c8e25e5ce177cef"},"cell_type":"markdown","source":"# CONCLUSION \n<br>\n* The highest score is found by SVM with 0.868\n* The worst score is foundy by Decision Tree with 0.803\n* As we can see some ML learning algorithm gives better results than other algorithms . \n* But we can not say the best ML algorithm is  x algorithm . \n* Because for another dataset, decision_tree may give higher results than SVM .\n<br>\n<br>\n* ** Sınce I am begiiner , I would appreciate if you give feedbacks in comment section. :)**\n* ** THANKS FOR READING TILL HERE**\n <br>\n <br>\n* For more information you can check the links that I used for preparing these kernels. \n* https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n* Confusion Matrix : https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n* Naive Bayes  : https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained\n* SVM : https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n* Random Forest : https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n* Decision Tree : https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1\n* \n"},{"metadata":{"trusted":true,"_uuid":"5c531da1e414c4b4d09d065bff7603bdbf996655"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}