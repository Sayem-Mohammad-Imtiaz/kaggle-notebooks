{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About the Dataset and notebook:\n\n**A tumor is an abnormal lump or growth of cells. When the cells in the tumor are normal, it is benign. Something just went wrong, and they overgrew and produced a lump. When the cells are abnormal and can grow uncontrollably, they are cancerous cells, and the tumor is malignant.**\n\n**In this note book i have tried to compare the 2 tumors- benign and malignant\n  i have added visualisatiion to show that how are these two tumors different\n  then i have applied various classification models for predictions for given features that whether the tumour is B or M.\n  i have also illustrated the importance of feature scaling.**","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries and Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy\nimport matplotlib as mpl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOADING THE DATA SET USING READ_CSV","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# BASIC PREPROCESSING OF THE DATA","metadata":{}},{"cell_type":"code","source":"df.shape#569 rows and 33 columns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Unnamed: 32',axis = 1,inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.iloc[:,:]\ny = df.iloc[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.drop(['diagnosis','id'],axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VISUALIZATION","metadata":{}},{"cell_type":"code","source":"# VISUALIZATION\nmpl.style.use(['ggplot']) \n# for ggplot-like style\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df.loc[:,'diagnosis':'area_mean'], hue=\"diagnosis\");\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot(kind =\"bar\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot(kind =\"pie\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data_B= df[df.diagnosis !='M']\nnew_data_M= df[df.diagnosis !='B']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FOLLOWNG GRAPHS SHOW THE DIFFERENCE BETWEEN VARIOUS PARAMTERS OF BENIGN AND MALIGNANT TUMORS\n","metadata":{}},{"cell_type":"code","source":"new_data_B.plot(kind = \"density\",x= 'radius_mean', y = 'concavity_mean')\nplt.xlabel(\"mean radius for benigm\")\nplt.ylabel(\"mean concavity for benigm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data_M.plot(kind = \"density\",x= 'radius_mean', y = 'concavity_mean')\nplt.xlabel(\"mean radius for malignant\")\nplt.ylabel(\"mean concavity for malignant\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ABOVE TWO GRAPHS PROVE THE DIFFERENCE BETWEEN 2 TUMORS.","metadata":{}},{"cell_type":"code","source":"new_data_B.plot(kind = \"scatter\",x= 'radius_mean', y = 'area_mean')\nplt.xlabel(\"mean radius for benigm\")\nplt.ylabel(\"mean area for benigm\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data_M.plot(kind = \"scatter\",x= 'radius_mean', y = 'area_mean')\nplt.xlabel(\"mean radius for malignant\")\nplt.ylabel(\"mean area for malignant\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### SIMILARLY , ABOVE TWO GRAPHS PROVE THAT M TUMORS ARE MUCH BIGGER THAN B TUMORS.","metadata":{}},{"cell_type":"code","source":"g = sns.jointplot(x=new_data_M['radius_mean'], y=new_data_M['texture_mean'], data=new_data_M, kind=\"kde\", color=\"m\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$mean radius for malignant$\", \"$mean texture for malignant$\");\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.jointplot(x=new_data_B['radius_mean'], y=new_data_B['texture_mean'], data=new_data_B, kind=\"kde\", color=\"m\")\ng.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$mean radius$\", \"$mean texture$\");\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avgB = {}\nfor i in range(2,new_data_B.shape[1]):\n    m = np.mean(new_data_B.iloc[:,i])\n    avgB.update({new_data_B.columns[i]:m})\n\navgB_df = pd.DataFrame(avgB,index = np.arange(1,31))\navgB_df = avgB_df.transpose()\navgB_df = avgB_df.iloc[:,:1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avgM = {}\nfor i in range(2,new_data_M.shape[1]):\n    m = np.mean(new_data_M.iloc[:,i])\n    avgM.update({new_data_M.columns[i]:m})\n\navgM_df = pd.DataFrame(avgM,index = np.arange(1,31))\navgM_df = avgM_df.transpose()\navgM_df = avgM_df.iloc[:,:1]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#so now, i have 2 data frames and i want to have a combined barplot\n\navgB_df['hue']='B'\navgM_df['hue']='M'\nres=pd.concat([avgB_df,avgM_df])\nres = res.reset_index(level =0)\nsns.barplot(x = res.iloc[:,0],y = res.iloc[:,1],data=res,hue='hue')\nplt.xticks(rotation=90)\nplt.ylabel('average of feature mentioned on X axis')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.PairGrid(new_data_B.loc[:,'radius_mean':'smoothness_mean'])\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot, n_levels=6);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ng = sns.PairGrid(new_data_M.loc[:,'radius_mean':'smoothness_mean'])\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot, n_levels=6);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYZING THE GRAPHS ABOVE, GIVE US A GOOD PICTORIAL IDEA FOR DIFFERENCES BETWEEN THE TUMORS**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(X)\ndata_n_2 = (data - data.mean()) / (data.std())  \ndata = pd.concat([y,data_n_2.iloc[:,10:25]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=45);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the amount of benigan and melignant tissues:\n#lets use countplot for this.\nB,M = y.value_counts()\n\nprint(B,M)\n#we can see that there are 357 B type and 212 M type cells\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### #lets split the data now\n","metadata":{}},{"cell_type":"code","source":"#lets split the data now\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2,random_state = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FITTING & TESTING THE CLASSIFICATION MODELS (without scaling the data)::","metadata":{}},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint(metrics.accuracy_score(prediction,ytest))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\n\nmetrics.accuracy_score(prediction,ytest)\nprint(metrics.accuracy_score(prediction,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#naive bayes\nknn = GaussianNB()\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#decision tree\n\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint(metrics.accuracy_score(ypred,ytest))\nmetrics.confusion_matrix(ytest,prediction)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## comparing accuracy of unscaled data all together","metadata":{}},{"cell_type":"code","source":"#decision tree\n\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint('DECISION TREE CLASSIFIER:: ',metrics.accuracy_score(ypred,ytest))\n\n#random forest\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint('FORSEST TREE CLASSIFICATION:: ',metrics.accuracy_score(prediction,ytest))\n\n\n#SVM\nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nmetrics.accuracy_score(prediction,ytest)\nprint('SUPPORT VECTOR MACHINE:: ',metrics.accuracy_score(prediction,ytest))\n\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint('K NEAREST NEIGHBOURS:: ',metrics.accuracy_score(ypred,ytest))\n\n\n#naive bayes\nNB = GaussianNB()\nNB.fit(xtrain,ytrain)\nypred = NB.predict(xtest)\nprint('NAIVE BAYES ALGORITHM:: ',metrics.accuracy_score(ypred,ytest))\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WE CAN SEE THAT RANDOM FOREST CLASSIFIER GIVES US THE BEST ACCURACY RESULTS.\n\n### SVM BEING THE WORST FOR THIS DATA\n\n###but,scaling the data gives different results","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"#what if i scale the data now::","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets split the data now\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2,random_state = 1)\n\n#decision tree\ndt = DecisionTreeClassifier()\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)\nprint('DECISION TREE CLASSIFIER:: ',metrics.accuracy_score(ypred,ytest))\n\n#random forest\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nprint('FORSEST TREE CLASSIFICATION:: ',metrics.accuracy_score(prediction,ytest))\n\n\n#SVM\nmodel = svm.SVC()\nmodel.fit(xtrain,ytrain)# now fit our model for traiing data\nprediction=model.predict(xtest)# predict for the test data\nmetrics.accuracy_score(prediction,ytest)\nprint('SUPPORT VECTOR MACHINE:: ',metrics.accuracy_score(prediction,ytest))\n\n#knn\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\nprint('K NEAREST NEIGHBOURS:: ',metrics.accuracy_score(ypred,ytest))\n\n\n#naive bayes\nNB = GaussianNB()\nNB.fit(xtrain,ytrain)\nypred = NB.predict(xtest)\nprint('NAIVE BAYES ALGORITHM:: ',metrics.accuracy_score(ypred,ytest))\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AFTER SCALING, SVM PERFORMS THE BEST","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}