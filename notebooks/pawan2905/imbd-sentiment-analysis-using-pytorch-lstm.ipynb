{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in and visualize the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing — convert to lower case","metadata":{}},{"cell_type":"code","source":"df['review'] = df['review'].apply(lambda x:x.lower())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing — remove punctuation","metadata":{}},{"cell_type":"code","source":"from string import punctuation\nprint(punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'] = df['review'].apply(lambda x:''.join([c for c in x if c not in punctuation]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len_review'] = df['clean_text'].apply(lambda x:len(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data Processing — create list of reviews","metadata":{}},{"cell_type":"code","source":"all_text2 = df['clean_text'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize — Create Vocab to Int mapping dictionary","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nall_text2 = ' '.join(all_text2)\n# create a list of words\nwords = all_text2.split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count all the words using Counter Method\ncount_words = Counter(words)\n\ntotal_words = len(words)\nsorted_words = count_words.most_common(total_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_split = df['clean_text'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Tokenize — Encode the words","metadata":{}},{"cell_type":"code","source":"reviews_int = []\nfor review in reviews_split:\n    r = [vocab_to_int[w] for w in review.split()]\n    reviews_int.append(r)\nprint (reviews_int[0:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize — Encode the labels","metadata":{}},{"cell_type":"code","source":"labels_split = df['sentiment'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_labels = [1 if label =='positive' else 0 for label in labels_split]\nencoded_labels = np.array(encoded_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze Reviews Length","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nreviews_len = [len(x) for x in reviews_int]\npd.Series(reviews_len).hist()\nplt.show()\npd.Series(reviews_len).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Removing Outliers — Getting rid of extremely long or short reviews","metadata":{}},{"cell_type":"code","source":"reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\nencoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Padding / Truncating the remaining data","metadata":{}},{"cell_type":"code","source":"def pad_features(reviews_int, seq_length):\n    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n    \n    for i, review in enumerate(reviews_int):\n        review_len = len(review)\n        \n        if review_len <= seq_length:\n            zeroes = list(np.zeros(seq_length-review_len))\n            new = zeroes+review\n        elif review_len > seq_length:\n            new = review[0:seq_length]\n        \n        features[i,:] = np.array(new)\n    \n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pad_features(reviews_int,200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (features[:10,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_feat = len(features)\nsplit_frac = 0.8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training, Validation, Test Dataset Split","metadata":{}},{"cell_type":"code","source":"split_frac = 0.8\ntrain_x = features[0:int(split_frac*len_feat)]\ntrain_y = encoded_labels[0:int(split_frac*len_feat)]\nremaining_x = features[int(split_frac*len_feat):]\nremaining_y = encoded_labels[int(split_frac*len_feat):]\nvalid_x = remaining_x[0:int(len(remaining_x)*0.5)]\nvalid_y = remaining_y[0:int(len(remaining_y)*0.5)]\ntest_x = remaining_x[int(len(remaining_x)*0.5):]\ntest_y = remaining_y[int(len(remaining_y)*0.5):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y = np.array(train_y)\ntest_y = np.array(test_y)\nvalid_y = np.array(valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloaders and Batching","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n# dataloaders\nbatch_size = 50\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the LSTM Network Architecture\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass SentimentLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super().__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Network","metadata":{}},{"cell_type":"markdown","source":"## Instantiate the network","metadata":{}},{"cell_type":"code","source":"# Instantiate the model w/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\nnet = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop\nMost of the code in training loop is pretty standard Deep Learning training code that you might see often in all the implementations that’s using PyTorch framework.","metadata":{}},{"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training params\n\nepochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n        #print(counter)\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing\n- On Test Data","metadata":{}},{"cell_type":"code","source":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output, h = net(inputs, h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### On User-generated Data\nFirst, we will define a tokenize function that will take care of pre-processing steps and then we will create a predict function that will give us the final output after parsing the user provided review.\n","metadata":{}},{"cell_type":"code","source":"# negative test review\ntest_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(review, vocab_to_int):\n    review = review.lower()\n    word_list = review.split()\n    num_list = []\n    #list of reviews\n    #though it contains only one review as of now\n    reviews_int = []\n    for word in word_list:\n        if word in vocab_to_int.keys():\n            num_list.append(vocab_to_int[word])\n    reviews_int.append(num_list)\n    return reviews_int","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(net, test_review, sequence_length=200):\n    ''' Prints out whether a give review is predicted to be \n        positive or negative in sentiment, using a trained model.\n        \n        params:\n        net - A trained net \n        test_review - a review made of normal text and punctuation\n        sequence_length - the padded length of a review\n        '''\n    #change the reviews to sequence of integers\n    int_rev = preprocess(test_review, vocab_to_int)\n    #pad the reviews as per the sequence length of the feature\n    features = pad_features(int_rev, seq_length=seq_length)\n    \n    #changing the features to PyTorch tensor\n    features = torch.from_numpy(features)\n    \n    #pass the features to the model to get prediction\n    net.eval()\n    val_h = net.init_hidden(1)\n    val_h = tuple([each.data for each in val_h])\n\n    if(train_on_gpu):\n        features = features.cuda()\n\n    output, val_h = net(features, val_h)\n    \n    #rounding the output to nearest 0 or 1\n    pred = torch.round(output)\n    \n    #mapping the numeric values to postive or negative\n    output = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n    \n    # print custom response based on whether test_review is pos/neg\n    print(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# positive test review\ntest_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# call function\n# try negative and positive reviews!\nseq_length=200\npredict(net, test_review_pos, seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}