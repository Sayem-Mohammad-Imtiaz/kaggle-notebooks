{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. EDA"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plant1Generation = pd.read_csv(\"/kaggle/input/solar-power-generation-data/Plant_1_Generation_Data.csv\", parse_dates=[\"DATE_TIME\"])\nplant1Generation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plant1GenerationUpdated = plant1Generation.pivot(index=\"DATE_TIME\", columns=\"SOURCE_KEY\", values=[\"DAILY_YIELD\", \"DC_POWER\", \"AC_POWER\",\"TOTAL_YIELD\"])\n# plant1GenerationUpdated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1Generation[\"SOURCE_KEY\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Idea** : We have 3 solutions\n- Create 1 model for summary \"SOURCE_KEY\" that create all 22 summray yield\n- Select only 1 model for specific one \"SOURCE_KEY\" for homework\n- Create 22 models for each \"SOURCE_KEY\"\n\n**In Order to send home work so we selectd option 2 by select**\n**only source \"1BY6WEcLGh8j5v7\" for prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1Generation.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1Weather = pd.read_csv(\"/kaggle/input/solar-power-generation-data/Plant_1_Weather_Sensor_Data.csv\", parse_dates=[\"DATE_TIME\"])\nplant1Weather = plant1Weather.drop([\"PLANT_ID\", \"SOURCE_KEY\"], axis=1)\nplant1Weather.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1Weather.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Feature Engineer\n\n- Create target \"YIELD\" by using TOTAL_YIELD(n) - TOTAL_YIELD(n-1)\n- We will have \"AMBIENT_TEMPERATURE\",\"MODULE_TEMPERATURE\",\"IRRADIATION\" from Plant_1_Generation_Data\n- And 22 \"SOURCE_KEY\" which generate their \"YIELD\"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1WeatherSelect = plant1Generation[plant1Generation[\"SOURCE_KEY\"]==\"1BY6WEcLGh8j5v7\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1WeatherUpdate = pd.DataFrame()\npreviousYield = 0\nfor index, row in plant1WeatherSelect.iterrows():\n    tempValue = row[\"DAILY_YIELD\"] - previousYield\n    if(tempValue < 0):\n        row[\"YIELD\"] = 0\n    else:\n        row[\"YIELD\"] = tempValue\n    previousYield = row[\"DAILY_YIELD\"]\n    plant1WeatherUpdate = plant1WeatherUpdate.append(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1WeatherUpdate.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant1WeatherReady = plant1WeatherUpdate.drop([\"DAILY_YIELD\", \"PLANT_ID\", \"SOURCE_KEY\", \"TOTAL_YIELD\"], axis=1)\nplant1WeatherReady","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Merge Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"mergeData = pd.merge(plant1Weather, plant1WeatherReady, on=\"DATE_TIME\")\nmergeData = mergeData.drop(columns=\"DATE_TIME\")\nmergeData.tail(60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Question 10 fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\ndef resultOfModel(y_test, y_pred):\n    # Reference each value : https://datarockie.com/2019/03/30/top-ten-machine-learning-metrics/\n    print('\\033[1m{:10s}\\033[0m'.format('Confusion Matrix'))\n    print('TN', 'FP', 'FN', 'TP')\n    print(confusion_matrix(y_test, y_pred).ravel())\n\n    print(\"\\n\")\n    print('\\033[1m{:10s}\\033[0m'.format('Classification Report'))\n    print(classification_report(y_test,y_pred))\n\n    print(\"\\n\")\n    print('\\033[1m{:10s}\\033[0m'.format('Matrix'))\n    model_results = pd.DataFrame([['Model result (n=100)', accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)]],\n                   columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n    print(model_results)\n\n    # Compute micro-average ROC curve and ROC area\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\n    auc = metrics.roc_auc_score(y_test, y_pred)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\ntargetColumn = 'YIELD'\n\nX = mergeData.drop(columns=targetColumn).to_numpy()\ny = mergeData[targetColumn].to_numpy()\n\n### Normally use KFold to train test split\n# kf = KFold(n_splits=10, random_state=None, shuffle=False)\n# for train_index, test_index in kf.split(X):\n#     X_train, X_test = X[train_index], X[test_index]\n#     y_train, y_test = y[train_index], y[test_index]\n\n# 0. Create variable\nfold = []\nstep = 10 \nstop = len(mergeData)\n\n# 1. Create index\nfor i in range(0,10):\n    fold.append(np.arange(start=i, stop=stop, step=step))\n    \n# 2. User index to create train test data\n# for i in range(len(fold)):\nfor i in range(1):\n    # Train index derive from other but i \n    # Test index derive from i\n    train_index = []\n    test_index = []\n\n    for j in range(len(fold)):\n        if(i != j):\n            train_index.extend(fold[j])\n        \n    test_index = fold[i]\n    \n    # 3. Assign train test value\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    ### Train model here\n    \n    # 4.1  Linear Regression\n    regr = LinearRegression()\n    \n    # 4.2  Ridge Regression\n    # regr = Ridge(alpha = 0.5)\n\n    # 4.3  SVR Kernel-linear\n    regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n\n    # Train the model using the training sets\n    regr.fit(X_train, y_train)\n\n    # Make predictions using the testing set\n    # y_pred = regr.predict(X_test)\n\n    result = regr.score(X_test, y_test)\n    print(\"Accuracy: %.2f%%\" % (result*100.0))\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X_train)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\ntargets = [0]\ncolors = ['r']\nfor target, color in zip(targets,colors):\n    indicesToKeep = mergeData[targetColumn] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Regression Model for 3 days"},{"metadata":{},"cell_type":"markdown","source":"## 5. Regression Model for 7 days"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}