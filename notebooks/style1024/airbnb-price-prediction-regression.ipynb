{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression Project: AirBNB Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom mpl_toolkits.basemap import Basemap #Plot onto map\nimport matplotlib.pyplot as plt #Plotting\nfrom pandas import Series,DataFrame\nimport seaborn as sns\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (15,10) #Set the default figure size\nplt.style.use('ggplot') #Set the plotting method\n\nfrom sklearn.model_selection import train_test_split #Split the data into train and test\nfrom sklearn.ensemble import RandomForestRegressor #Forest for prediction and regression\nfrom sklearn.linear_model import LinearRegression #Regression for prediction\nfrom sklearn.preprocessing import StandardScaler #Scale the data\nfrom sklearn.metrics import mean_squared_error #Error testing\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"bnb = pd.read_csv(\"../input/us-airbnb-open-data/AB_US_2020.csv\", low_memory=False) #Read the airbnb csv\nbnb.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb_raw = pd.read_csv('../input/us-airbnb-open-data/AB_US_2020.csv', low_memory=False)\nairbnb_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"price\"] = bnb[\"price\"].apply(lambda x: 1 if x < 1 else x) #Make 0's 1 so the log function works","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print some attributes about the prices\nprint(\"Max Price: \", np.max(bnb[\"price\"]))\nprint(\"Min Price: \", np.min(bnb[\"price\"]))\nprint(\"Num Prices Below 20: \", len(bnb.loc[bnb[\"price\"] < 20]))\nprint(\"Num Prices Above 1000: \", len(bnb.loc[bnb[\"price\"] > 1000]))\nprint(\"Num Locations\", len(bnb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.isnull().any()) #Check for null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.loc[bnb[\"reviews_per_month\"].isnull()]) #See where reviews_per_month is null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reviews_Per_Month and Last_Review appear to be null when there are no nulls. The best way to fill these would probably be to make Reviews_Per_Month 0 and a dummy date for last review (01-01-01). This data field will have to be fixed into yyyy-mm-dd anyway, so a non-date value will cause problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.loc[bnb[\"host_name\"].isnull()]) #See where host_name is null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.loc[bnb[\"name\"].isnull()]) #See where name is null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both name and host_name do not inherently have any importance since their ID's are what is important. I can decide what to drop later after more exploration, so I will fill these with generic, obviously filled in names. I am thinking \"AIRBNB HOUSING\" and \"AIRBNB HOST\".\n\nAs for neighborhood_group, it appears this is just here to emphasize certain areas of cities like New York. I will change nulls to \"Other\" in this case, as not all cities have neighborhoods like New York. The neighborhood names (besides NY) also seem inconsistent when looking at it through the other null prints, so that is something to keep in mind."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fix the Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"name\"] = bnb[\"name\"].fillna(\"AIRBNB HOUSING\") #Fill the null name values with \"AIRBNB HOUSING\"\nprint(bnb.loc[bnb[\"name\"] == \"AIRBNB HOUSING\"]) #See where name is fixed to make sure this works","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"host_name\"] = bnb[\"host_name\"].fillna(\"AIRBNB HOST\") #Fill the null host name values with \"AIRBNB HOST\"\nprint(bnb.loc[bnb[\"host_name\"] == \"AIRBNB HOST\"]) #See where host_name is fixed to make sure this works","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"neighbourhood_group\"] = bnb[\"neighbourhood_group\"].fillna(\"Other\") #Fill the null neighbourhood group values with \"Other\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"reviews_per_month\"] = bnb[\"reviews_per_month\"].fillna(0) #Fill the null reviews_per_month values with 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb[\"last_review\"] = bnb[\"last_review\"].fillna(\"01/01/01\") #Fill the null last_review values with 01/01/01\nbnb[\"last_review\"] = pd.to_datetime(bnb[\"last_review\"]) #Convert the last review to datetime\nprint(bnb[\"last_review\"]) #Print the last review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.isnull().any()) #Check for null values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the null values have been fixed."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fix Column Names for My Comfort"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change the column names to ones I prefer\nbnb = bnb.rename(columns = {\"host_id\" : \"hostId\", \"host_name\" : \"hostName\", \"neighbourhood_group\" : \"neighGroup\",\n                            \"neighbourhood\" : \"neigh\", \"room_type\" : \"roomType\", \"minimum_nights\" : \"minNights\",\n                            \"number_of_reviews\" : \"numReviews\", \"last_review\" : \"lastReview\", \"reviews_per_month\" : \"monthlyReviews\",\n                            \"calculated_host_listings_count\" : \"numListings\", \"availability_365\" : \"available\"})\nbnb.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# AirBNB Locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nimport folium.plugins as plugins\n\nlatitude2020 = bnb[\"latitude\"].tolist()\nlongitude2020 = bnb[\"longitude\"].tolist()\nlocations = list(zip(latitude2020, longitude2020))\n\n# Initialize the map:\nusa_map = folium.Map(location = [35, -100], zoom_start = 5)\nplugins.FastMarkerCluster(data = locations).add_to(usa_map)\nusa_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb[\"city\"].unique()) #See all the unique \"cities\" in the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fix City Names into Major Areas"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A list of areas in the dataset that are part of the San Francisco major area\nSF = [\"Oakland\", \"Pacific Grove\", \"San Clara Country\", \"Santa Cruz County\", \"San Mateo County\", \"San Francisco\"]\n\n#Fix the cities into their major areas\n#Input: the city/state/county named state (this column has so many different things)\n#Output: the fixed label\ndef fixState(state):\n    \n    #Fix the labels whose major areas are not as clear\n    if state == \"Broward County\":\n        return \"Miami\"\n    if state == \"Twin Cities MSA\":\n        return \"Minneapolis\"\n    if state == \"Clark County\":\n        return \"Las Vegas\"\n    \n    #Lump labels together if thier major area is the same\n    if state == \"Boston\" or state == \"Cambridge\":\n        return \"Boston\"\n    if state == \"Portland\" or state == \"Salem\":\n        return \"Portland\"\n    if state == \"Jersey City\":\n        return \"New York City\"\n    if state in SF:\n        return \"San Francisco\"\n    \n    return state #Return the label if it does not need to change\n\nbnb[\"city\"] = bnb[\"city\"].apply(fixState) #Fix the city column with its major areas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb[\"city\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"price = bnb[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\nprice = np.log(price) #Take the log of the set for normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bnb.loc[bnb[\"price\"] > 10000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"characteristics = bnb.copy() #Take a copy of the dataframe for usage\ncharacteristics = characteristics.drop(columns = {\"price\"}) #Remove the price, since we cannot predict price if it is already there\ncharacteristics.head() #Take a peek at the data without the price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are several columns that I should not take into account here. The name and hostName columns are all considered categorical data filled with the entirely different values, so there is not nearly enough memory to handle pandas bringing that to dummies. The same goes for lastReview if left as a string. As a datetime (which I set it to), the scaler does not recognize it. Then there is neigh, which I actually tried to use. It turned fitting the model into an hour long endeavor due to it creating a lot of dummy variables and only increased accuracy by about 3%. The trade off for that one is not worth it."},{"metadata":{"trusted":true},"cell_type":"code","source":"charact = characteristics.drop(columns = {\"name\", \"hostName\",  \"neigh\", \"lastReview\", \"id\"}) #Remove the variables discussed above\ncharact.head() #Take a peek at the data after removing the variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"charact = pd.get_dummies(charact) #Get the dummies for easier model training\nscale = StandardScaler() #Add a standard scaler to scale our data for easier use later\nscale.fit(charact) #Fit the scaler with our characteristics\nchara = scale.transform(charact) #Transform the data with our scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(chara[0])) #Print the scaled data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"charaTrain, charaTest, priceTrain, priceTest = train_test_split(chara, price, test_size = 0.1) #Split the data into train and test\nprint(priceTest) #Print one of the splits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit the Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestRegressor(n_estimators = 150) #Build a whole forest of trees\nforest.fit(charaTrain, priceTrain) #Fit the forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = forest.predict(charaTest) #Get the predictions for RMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overallAccuracy = (\"Overall\", forest.score(charaTest, priceTest)) #Get the overall accuracy \nprint(\"Forest Accuracy: \", forest.score(charaTest, priceTest)) #Print the accuracy\nprint(\"Root Mean Square Error: \", np.sqrt(mean_squared_error(priceTest, predict))) #Print the root mean square error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = charact.columns #Get the tested attributes\nattributes = list(zip(attributes, forest.feature_importances_)) #Zip the attributes together with their coefficient\nsortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\nprint(\"According to the Random Forest (most accurate), the most important factors for pricing are: \") #Start printing the most important labels\ni=0 #Counter variable so only the top five are printed\n\n#For each attribute in the sorted attributes\nfor label, coef in sortAtt:\n    if i<5: #If there has not been five printed yet\n        print(label) #Print the label as an important factor\n    i += 1 #Increase i by 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame({\"truePrice\": priceTest.values, \"predPrice\": predict}) #Create a dataframe with the predictions\npredictions.head(100) #Take a peek at the predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error = np.subtract(np.exp(predictions[\"truePrice\"]), np.exp(predictions[\"predPrice\"])) #Get the variance by subtracting the true and prediction\nb = plt.hlines(500, xmin = 0, xmax = 25000, lw = 3) #Print a line to show variance\nc = plt.hlines(-300, xmin = 0, xmax = 25000, lw = 3) #Print a lower line to show variance\na = plt.plot(error, \"b.\") #Plot the error\nplt.show() #Show the plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy I could get was around 60%. This only happened when I added back longitude and latitude (which I removed since I thought the city/major area would cover that). 60% is still not the best, but considering how users can determine their own prices, it is not surprising. There are listings for 24999 and 1 in the price field, so I definitely think the hosts do not use the same criteria when determining price. \n\nThis is for the overall data, however. I wonder if looking at different locations in isolation will provide better accuracies."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Regression by City"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ExtractChara: extracts the desired characteristics like I did step by step for the full dataset\n#Input: the area dataset\n#Output: the extracted characteristics\ndef extractChara(data):\n    characteristics = data.copy() #Take a copy of the dataframe for usage\n    characteristics = characteristics.drop(columns = {\"price\"}) #Remove the price, since we cannot predict price if it is already there\n    \n    charact = characteristics.drop(columns = {\"name\", \"hostName\", \"neigh\", \"lastReview\", \"id\"}) #Remove the variables discussed before\n\n    charact = pd.get_dummies(charact) #Get the dummies for easier model training\n    scale = StandardScaler() #Add a standard scaler to scale our data for easier use later\n    scale.fit(charact) #Fit the scaler with our characteristics\n    chara = scale.transform(charact) #Transform the data with our scaler\n    \n    return chara #Return the extracted characteristics\n\n#RandomForest: build a random forest for the given area\n#Input: the given characteristics, the prices, and the area this is representing\n#Output: the model accuracy with the area\ndef randomForest(chara, price, area):\n    charaATrain, charaATest, priceATrain, priceATest = train_test_split(chara, price, test_size = 0.1) #Split the data into train and test\n    \n    forest = RandomForestRegressor(n_estimators = 150) #Build a whole forest of trees\n    forest.fit(charaATrain, priceATrain) #Fit the forest\n    predictA = forest.predict(charaATest) #Get the predictions for RMSE\n    \n    accuracyA = forest.score(charaATest, priceATest)\n    \n    print(\"{} Accuracy: {}\".format(area, accuracyA)) #Print the accuracy\n    print(\"{} Root Mean Square Error: {}\".format(area, np.sqrt(mean_squared_error(priceATest, predictA)))) #Print the root mean square error\n    \n    return (area, accuracyA) #Return the accuracy with the area for visualization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"areas = bnb[\"city\"].unique() #Get all the unique major areas\naccuracies = [overallAccuracy] #Build a list to build the accuracies\n\nfor area in areas:\n    areaData = bnb.loc[bnb[\"city\"] == area] #Look only at the data for the area\n    \n    priceArea = areaData[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\n    priceArea = np.log(priceArea) #Take the log of the set for normalization\n    \n    areaData = areaData.drop(columns = {\"city\"})\n    charaArea = extractChara(areaData) #Extract the wanted characteristics\n    \n    accuracies.append(randomForest(charaArea, priceArea, area)) #Call the random forest function for the specific area","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accDF = pd.DataFrame(accuracies, columns = [\"Area\", \"Accuracy\"]) #Put the accuracies list into a dataframe\n\naccSort = accDF.sort_values(\"Accuracy\", ascending = False) #Sort the accuracies datafrmame by its accuracies\naccSort.plot.bar(x = \"Area\", y = \"Accuracy\") #Plot the model accuracy for each major area in a bar graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Overall Removing Extraneous Values"},{"metadata":{},"cell_type":"markdown","source":"The max value is 24999 and the min values is 0, so all of these mean errors are going to have issues. So, I will see what happens if I remove unusually low and high values (keep 60 < x < 1000)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bnbTrim = bnb.loc[bnb[\"price\"] > 20] #Trim out values lower than 20\nbnbTrim = bnbTrim.loc[bnbTrim[\"price\"] < 2000] #Trim out values higher than 1500\nprint(\"Max: \",np.max(bnbTrim[\"price\"])) #Print the current max price\nprint(\"Min: \",np.min(bnbTrim[\"price\"])) #Print the current min price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy seems to have gone down upon removing values, which means the mean was not the problem. The root mean square error has remained below 1 every time though."},{"metadata":{"trusted":true},"cell_type":"code","source":"priceTrim = bnbTrim[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\npriceTrim = np.log(priceTrim) #Take the log of the set for normalization\n    \ncharaTrim = extractChara(bnbTrim) #Extract the wanted characteristics\n    \narea, accuracyTrim = randomForest(charaTrim, priceTrim, \"Trimmed\") #Call the random forest function for the specific area","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# RMSE 線性回歸"},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb = airbnb_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb = airbnb.replace(np.nan,0)\nairbnb.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb['neighbourhood_group'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('neighbourhood_group',data=airbnb)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb['last_review'] = pd.to_datetime(airbnb['last_review'])\nairbnb['last_review'] = pd.to_numeric(airbnb['last_review'])\nairbnb['price'].corr(airbnb['last_review']) \n#can drop last_Review date too since it has very little correlation to price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb = airbnb.drop(['id','name','host_id','host_name','latitude','longitude','last_review'],axis=1)\nairbnb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\n\nairbnb['neighbourhood_group'] = airbnb['neighbourhood_group'].replace(0,'null')\nneighbourhood_group = DataFrame({'Neighbourhood_group':airbnb['neighbourhood_group'].unique()})\ncode = encoder.fit_transform(neighbourhood_group['Neighbourhood_group'])\nneighbourhood_group['Code'] = code\nneighbourhood_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbourhood = DataFrame({'Neighbourhood':airbnb['neighbourhood'].unique()})\nneigh_code = encoder.fit_transform(neighbourhood['Neighbourhood'])\nneighbourhood['Code'] = neigh_code               \nneighbourhood","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"room_type = DataFrame({'Room type':airbnb['room_type'].unique()})\nroom_code = encoder.fit_transform(room_type['Room type'])\nroom_type['Code'] = room_code\nroom_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city = DataFrame({'City' : airbnb['city'].unique()})\ncity_code = encoder.fit_transform(city['City'])\ncity['Code'] = city_code\ncity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb['neighbourhood_group'] = encoder.fit_transform(airbnb['neighbourhood_group'])\nairbnb['neighbourhood'] = encoder.fit_transform(airbnb['neighbourhood'])\nairbnb['room_type'] = encoder.fit_transform(airbnb['room_type'])\nairbnb['city'] = encoder.fit_transform(airbnb['city'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airbnb.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalise(feature):\n    nmx = 100\n    nmn = 0\n    \n    mx = feature.max()\n    mn = feature.min()\n    \n    return ((nmx-nmn) / (mx-mn) * (feature-mx) + nmx)\n\nnorairbnb = normalise(airbnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norairbnb.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norairbnb['minimum_nights'] = norairbnb['minimum_nights'].astype(int)\nnorairbnb['reviews_per_month'] = norairbnb['reviews_per_month'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(norairbnb.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = norairbnb['price']\nX = norairbnb.drop('price',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best fit line is {reg.intercept_}')\nprint(f'Number of coeffcients are {len(reg.coef_)}')\ncoef_df = DataFrame({'Variable':X.columns,'Coeff':reg.coef_})\ncoef_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg1 = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg1.fit(x_train,y_train)\ny_pred = reg1.predict(x_test)\nrms = np.mean((y_pred-y_test)*2)\nprint(f'Root mean square error is {rms}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 150, random_state = 42)\nregressor.fit(x_train, y_train)  \nprint(\"Forest Accuracy: \", regressor.score(x_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = DataFrame({'Actual':y_test,'Predict':y_pred})\npred_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this project, the model only came to a 60% accuracy with the random forest, but root mean square errors remained below 1 the whole time. This likely means it cannot quite be fully predicted due the the strangeness that comes with users inputting prices. The accuracy could be higher or lower by major area, so it seems the areas are also inconsistent.\n\nLooking at the characteristics the model determined was most important, these were roomType_Entire home/apt, longitude, latitude, id, and monthlyReviews. Of course higher monthly reviews and getting an entire house would make the price go higher. It means more people are coming in and wanting to not share the AirBNB. This makes perfect sense. The ID being here means that it depends on the property itself, which could have been hidden in the title or similar means. The method I was trying to practice just did not suit NLP. As for Latitude and Longitude, I am quite surprised. I originally cut them out since they were encompassed in the city/major area variable, but that variable never made it into the top spot, even when ignoring items like latitude and longitude. These appear to be more important in grouping major areas together rather than looking at one as a whole, which is pretty interesting."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}