{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Import libraires \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Breif summary about this in Notebook.Below are few topcis we will cover in this notebook.\n* Step by Step EDA\n* Handel imbalanced dataset\n* Model Selection\n* Hyperparameter tuning\n    \n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\") #Load the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at basic statisics of the data like mean,min,max,standard deviation,percentile of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above table we can see that min values for most of the columns are 0. That's highlhy unlikely. Let's look at some of the top and bottom rows in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail(10) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we look at the data.describe method we can see Insulin,Gluscose,Blood Pressure,SkinThickness and BMI are 0 \nBut they can't be zero. They zero values could be be becuase of missing values/data corruption\nBeofer we jump to any conclusion let see the count of missing values in the datset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()\n#There are no missing values in the dataset but there are 0 values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,linewidths=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's replace the 0 values in datasent  with numpy Nan.\nIn later part of the notebook will see how to repalce the incorrect/missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    if col !=\"Pregnancies\" and col !=\"DiabetesPedigreeFunction\" and col !=\"Outcome\":\n        data[col] = data[col].replace(0,np.nan)\n\n#We have replaced the inaccure values with nan values.Now let's look for the missing values        \ntotal_missing_values = pd.DataFrame(data.isnull().sum()) # count the percentage of misisng values\n\npercentage_mising_values = pd.DataFrame(data.isnull().sum() / data.shape[0] * 100) #Compute percentage of missing values\n\nmissing_df = pd.DataFrame(index=data.columns,columns = ['Total missing values','Total missing values %'])\nmissing_df['Total missing values'] = total_missing_values\nmissing_df['Total missing values %'] = percentage_mising_values \nmissing_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a rule of thumb if any feature have more than 30 % of missing data we can drop it becuase imputing the missing values for these columns will not add much value while using mean,median or mode methods. Having said that we can use some advnaced technquies like KNN or Random forest regressor to predict the missing values (Here the column with missing values will become our dependant variable). But in our cases the correlation between SkinThickness,Insulin is very less so we can drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Insulin','SkinThickness'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before imputing the missing values let's the distribtion of the cloumns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(10,10),grid=[3,3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BMI,Glucose and BloodPressure columns have missing values. From above plot we can see that these features follow so what a normal distribution. We can impute the missing values with the mean of the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['BMI'].fillna(data['BMI'].mean(),inplace=True)\ndata['Glucose'].fillna(data['Glucose'].mean(),inplace=True)\ndata['BloodPressure'].fillna(data['BloodPressure'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate analysis\n\nLet's see the distribution of the feautes once again after imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(10,10),grid=[4,4])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age,DiabetesPedigreeFunction and Pregnanices featues seems to be swked. Let anaylse these feautes"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Age'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Outcome',y='Age',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Age'] > 68]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(data[data['Age'] > 68].Age,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='DiabetesPedigreeFunction',x='Outcome',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='Pregnancies',x='Outcome',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having more than 10 pregnancies is higlhy unlikely.This could be beacuse of typo error.Let's remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(data[data['Pregnancies'] >10].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at the dependant variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Outcome'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThis dataset is imblanced . That means there are more rows with 0 label than 1.This will bias the prediction model towards the more common class.\nTake a look at https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18 to under more on imbalanced \ndataset and how it will effect the model performance\nIn this note book we will measure the model performance on Orginal, downsampled and syntheticaly generated sampled datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\nimbalanced = data.copy(deep=True)\ndownsampled = data.copy(deep=True)\nsynthetic_dataset = data.copy(deep=True)\n\n#Before we down sample the data we should split the data based on class labels.\nnot_diabetic = downsampled[downsampled.Outcome==0]\ndiabetic = downsampled[downsampled.Outcome==1]\n\nnot_diabetic_downsampled = resample(not_diabetic,\n                                replace = False, # sample without replacement\n                                n_samples = len(diabetic), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled  = pd.concat([not_diabetic_downsampled, diabetic])\n\n#split the data into x and y\nsynthetic_dataset_X = synthetic_dataset.iloc[:,:-1]\nsynthetic_dataset_y = synthetic_dataset.iloc[:,-1]\nsm = SMOTE(random_state = 10) \n#Fit the SMOTE model to the data\nsynthetic_X, synthetic_y = sm.fit_sample(synthetic_dataset_X, synthetic_dataset_y.ravel()) \ndf_synthetic_dataset_y = pd.DataFrame(columns=['Outcome'])\ndf_synthetic_dataset_y['Outcome'] = synthetic_y\nsynthetic_dataset_X = pd.DataFrame(synthetic_X, columns=synthetic_dataset.columns[:-1])\n#Concat X and Y again into singel dataset\nsynthetic_dataset = pd.concat([synthetic_dataset_X,df_synthetic_dataset_y],axis=1) \n\nprint(\"Class labels of imbalanced dataset has {} 0s and {} 1s.\\n\".format(imbalanced['Outcome'].value_counts()[0],imbalanced['Outcome'].value_counts()[1]))\nprint(\"Class labels of downsampled dataset has {} 0s and {} 1s.\\n\".format(downsampled['Outcome'].value_counts()[0],downsampled['Outcome'].value_counts()[1]))\nprint(\"Class labels of synthetic dataset has {} 0s and {} 1s.\\n\".format(synthetic_dataset['Outcome'].value_counts()[0],synthetic_dataset['Outcome'].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bivariate analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.pairplot(data=data,hue='Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,linewidths=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above heat map we can see Gloucose and BMI are one the feautre which is highly corelated with Outcome"},{"metadata":{},"cell_type":"markdown","source":"All the feautres in the data are in different scale. So we have to scale the values."},{"metadata":{},"cell_type":"markdown","source":"## Modle Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,classification_report\n\ndatasets = [imbalanced,downsampled,synthetic_dataset]\ndataset_names = ['imbalanced','downsampled','synthetic_dataset']\nmodels = [KNeighborsClassifier,LogisticRegression,RandomForestClassifier]\n\ncolumns = ['Dataset','Model','accuray_score','f1_score','TN','FP','FN','TP']\nModel_details = pd.DataFrame(columns=columns)\n\n\n\nfor dataset_name in dataset_names:\n    index_dataset = dataset_names.index(dataset_name)\n    frame = {}\n    for model in models:\n        frame['Dataset'] = dataset_name\n        frame['Model'] = model.__name__\n        dataset = datasets[index_dataset]\n        X= dataset.iloc[:,:-1]\n        y = dataset.iloc[:,-1]\n        \n        #scale the values\n        sc_X = StandardScaler()\n        X = sc_X.fit_transform(X)\n        #split the dataset into train and test\n        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42, stratify=y)\n        clf = model()\n        clf.fit(X_train,y_train)\n        \n        y_pred = clf.predict(X_test)\n        \n        frame['f1_score'] = f1_score(y_test,y_pred)\n        frame['accuray_score'] = accuracy_score(y_test,y_pred)\n        frame['TN'] = confusion_matrix(y_test,y_pred)[0][0]\n        frame['FP'] = confusion_matrix(y_test,y_pred)[0][1]\n        frame['FN'] = confusion_matrix(y_test,y_pred)[1][0]\n        frame['TP'] = confusion_matrix(y_test,y_pred)[1][1]\n        Model_details = Model_details.append(frame,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_details","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper parameter tuning"},{"metadata":{},"cell_type":"markdown","source":"From above result clearly KNeighborsClassifier is the better model.\nObserve that though accuracy score is high for imbalanced dataset but the f1_score is low.\nSynthetic_dataset and downsampled dataset achived high accuray score as well as f1 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\"n_neighbors\": np.arange(1, 25),'weights':['uniform','distance']}\n\n\nX= downsampled.iloc[:,:-1]\ny = downsampled.iloc[:,-1]\n        \n#scale the values\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n#split the dataset into train and test\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42, stratify=y)\n\nclf = KNeighborsClassifier()\nclf_cv= GridSearchCV(clf,param_grid,cv=3,n_jobs=-1)\nclf_cv.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 12 is the optimal value for ‘n_neighbors’ and distance is the weight metric. We can use the ‘best_score_’ function to check the accuracy of our model when ‘n_neighbors’ is 12. ‘best_score_’ outputs the mean accuracy of the scores obtained through cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1}