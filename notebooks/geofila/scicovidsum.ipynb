{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import \nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport os\nimport pickle\nfrom ipywidgets import interact_manual, widgets\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_obj(filename):\n    with open(filename, \"rb\") as fp: # Unpickling\n        b = pickle.load(fp)\n        return b\n\n#dataset creation\ndef load_filenames(path = \"/kaggle/input/CORD-19-research-challenge/\"):\n    filenames = []\n    for root, _, files in os.walk(path):\n        for file in files:\n            if file.split(\".\")[-1] == \"json\":\n                filenames.append(root + \"/\" + file)\n    \n    return filenames\n\ndef format_body(file, colm):\n    if not colm in file.keys():\n        return \"\"\n    body_text = file[colm]\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\n\ndef generate_dataset():\n    #load papers filenames\n    all_files = load_filenames()\n    data = []\n    \n    #load summaries \n    summaries = load_obj(\"/kaggle/input/biobertsum-summaries/summaries.pkl\")\n    for file in tqdm(all_files):\n        file = json.load(open(file, 'rb'))\n        if file['paper_id'] in summaries:\n            summ = summaries[file['paper_id']][0]\n            score = summaries[file['paper_id']][1]\n        else:\n            summ = []\n            score = []\n            \n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_body(file, 'abstract'),\n            format_body(file, 'body_text'),\n            summ, \n            score\n            \n        ]\n        data.append(features)\n    col_names = ['paper_id', 'title', 'abstract', 'text', 'summaries', 'score']\n    dataset = pd.DataFrame(data, columns=col_names)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = generate_dataset()\ndataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport os\npath = \"/kaggle/input/papers-for-biobert/testset/\"\nfiles = os.listdir(path)\npapers = []\nfor f in files:\n    papers += torch.load(path + f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import interact_manual, widgets\nfrom nltk.tokenize import word_tokenize \n\ndef clean_text(text):\n    tokens = word_tokenize(text)\n\n    start = -1\n    for i, token in enumerate(tokens):\n        if start<0:\n            if not(token.isdigit()) and not (token in (string.punctuation)):\n                start = i\n    tokens = tokens[start:]\n    sentence = \" \".join(tokens)\n    return sentence\n\n\n@interact_manual\ndef search_articles(\n    paper_id= \"-\"):\n    \n    if dataset['paper_id'].str.contains(paper_id).sum() == 0:\n        print (\"Searching Summaries for a valid random paper . . .\", end = \"\\n\\n\")\n        loc = np.random.choice(range (len(dataset)))\n        \n        while  dataset.iloc[loc][\"summaries\"] == []:\n            loc = np.random.choice(range (len(dataset)))\n            \n        paper_id = dataset.iloc[loc][\"paper_id\"]\n    \n    row = dataset.loc[dataset['paper_id'] == paper_id]\n    print (\"Paper id: \", np.array(row[\"paper_id\"])[0], end = \"\\n\\n\")\n    print (\"Title: \", np.array(row[\"title\"])[0], end = \"\\n\\n\")\n    \n    abstract = np.array(row['abstract'])[0]\n    if abstract == \"\":\n        print (\"This paper does not incude an abstract!\", end=\"\\n\\n\")\n    else:\n        print (\"Original Abstract of the paper: \")\n        print (abstract, end = \"\\n\\n\")\n\n    print (\"Summaries: \", end = \"\\n\\n\")\n    summaries = np.array(row['summaries'])[0]\n    scores = np.array(row['score'])[0]\n    for i, sent in enumerate(summaries):\n        print (str (i + 1) + \". \" + clean_text(sent))\n        print (\"Sentence importance: \", scores[i], end = '\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Searching Summaries with BioBert</center></h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# BioBERT dependencies\nimport subprocess\n# Tensorflow 2.0 didn't work with the pretrained BioBERT weights\n!pip install tensorflow==1.15\n# Install bert-as-service\n!pip install bert-serving-server==1.10.0\n!pip install bert-serving-client==1.10.0\n\n# We need to rename some files to get them to work with the naming conventions expected by bert-serving-start\n!cp /kaggle/input/biobert-pretrained /kaggle/working -r\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.index /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.index\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.data-00000-of-00001 /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.data-00000-of-00001\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.meta /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#starting BioBert Service\n\nbert_command = 'bert-serving-start -model_dir /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed -max_seq_len=None -max_batch_size=32 -num_worker=2'\nprocess = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)\n\n# Start the BERT client. It takes about 10 seconds for the bert server to start, which delays the client\nfrom bert_serving.client import BertClient\n\nbc = BertClient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\nfrom scipy.spatial.distance import cosine\nimport pickle\nimport numpy as np\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport string\nimport string \n\n\nwith open('/kaggle/input/summary-embeddings/summary_embs_df.pkl','rb') as f:\n    emb_df = pickle.load(f)\n\n\ndef cosine_distance(v1, v2):\n    distance = 1 - cosine(v1, v2)\n    return distance\n\ndef answer_query(query,num_summaries=5):\n    ##Encode the query with biobert\n    qemb = bc.encode([query])\n    \n    \n    \n    relevant_embeddings = emb_df\n\n    ## Compute similarities with relevant embeddings and querry\n    a = np.array([cosine_distance(qemb[0],relevant_embeddings['embedding'][i]) for i in range(relevant_embeddings.shape[0])])\n    asort = np.argsort(a)\n    \n    ## Print everything\n    print('')\n    print('Generated summaries of '+str(num_summaries)+' most relevant papers for query:')\n    print('\"'+query+'\"')\n    for i in range(1,num_summaries):\n        print('--------------------Paper: ', i, \" --------------------\")\n        print(\"From paper with paperId : \"+relevant_embeddings['paper_id'][asort[-i]])\n        print(\"Important sentences : \")\n        soum = relevant_embeddings['summary'][asort[-i]]\n        sents = soum.split('.')      \n        for s in sents:\n            print (clean_text(s))    \n            print('...')\n        print('With average sentence importance score : '+str(relevant_embeddings['sum_score'][asort[-i]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Experimental Results</center></h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#create interact manual\ndef_query = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\"\n\n@interact_manual\ndef search_articles(query=def_query, num_of_papers = \"5\"):\n    answer_query(query, num_summaries= int (num_of_papers))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}