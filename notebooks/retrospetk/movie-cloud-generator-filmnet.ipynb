{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Movie Cloud Generator\n\nThis noteboook uses the tmdb 5000 movie dataset to generate a 2-dimensional projection of movies.\n\nThe goal is to create a projection that puts \"similar\" movies closer to each other, in order to create a map of cinema that is easy and intuitive to navigate. The take-away for the user is a self-discovered movie recommendation.\n\nIt uses multiple t-SNE (t-distributed stochastic neighbor embedding) operations to reduce the dimensionality of this high-dimensional categorical data.\n\nThe final resulting artifact is hosted [here](https://giorgos.fun/filmnet)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport json\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import normalize\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import OneHotEncoder\nfrom matplotlib import pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in files\ncredits = pd.read_csv(\"/kaggle/input/tmdb-movie-metadata/tmdb_5000_credits.csv\")\nmovies = pd.read_csv(\"/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Consolidate everything into a single data frame\n\ndataframe = pd.DataFrame()\ndataframe[\"id\"] = movies[\"id\"] # reference\ndataframe[\"title\"] = movies[\"title\"] # text\ndataframe[\"year\"] = [x[:4] for x in np.array(movies[\"release_date\"].values.tolist())] # z\ndataframe[\"popularity\"] = movies[\"popularity\"] # size\ndataframe[\"genres\"] = [json.loads(x) for x in np.array(movies[\"genres\"].values.tolist())] # color\ndataframe[\"crew\"] = [json.loads(x) for x in np.array(credits[\"crew\"].values.tolist())] # x / y\ndataframe[\"cast\"] = [json.loads(x) for x in np.array(credits[\"cast\"].values.tolist())] # x / y\ndataframe[\"keywords\"] = [json.loads(x) for x in np.array(movies[\"keywords\"].values.tolist())]\ndataframe['rating'] = movies['vote_average']\n\ndataframe = dataframe.iloc[np.where(dataframe['year'] != \"\")[0]]\ndataframe = dataframe.iloc[np.where(dataframe['genres'])[0]]\ndataframe = dataframe.iloc[np.where(dataframe['crew'])[0]]\ndataframe = dataframe.iloc[np.where(dataframe['cast'])[0]]\ndataframe = dataframe.iloc[np.where(dataframe['keywords'])[0]]\n\ndataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using only part of the original data. \n\nThe movie co-ordinates will be generated by considering the following questions:\n1. Who made it?\n2. Who is in it?\n3. What is it about?\n4. How is it categorized?\n\nAnd therefore the four features we need are: \n1. Crew\n2. Cast\n3. Keywords\n4. Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_genres = 2\nnum_keywords = 20\nnum_cast = 50\nnum_crew = 50\n\n# Encode numerical ids into strings\ndef encode(array, length, suffix):\n    encoded = np.zeros(shape=(array.shape[0], length), dtype='<U10')\n    for i, row in enumerate(array):\n        encoded[i,:len(row)] = [str(x['id']) + suffix for x in row][:length]\n    return encoded\n\ncast_encoded = encode(dataframe['cast'], num_cast, 'ca')\ncrew_encoded = encode(dataframe['crew'], num_crew, 'cr')\ngenres_encoded = encode(dataframe['genres'], num_genres, 'ge')\nkeywords_encoded = encode(dataframe['keywords'], num_keywords, 'kw')\n\nkeywords_encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality reduction\n\nAs we can see above, the data has been encoded into lists of unique identifiers. This data cannot be codified into coordinates until we have reduced the dimensions and created a t-SNE projection.\n\nWhat follows is the creation of 2-dimensional projection for each of these features and a preliminary test for a proper clustering split."},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder()\ncast_one_hot = enc.fit_transform(cast_encoded)\n\nsvd = TruncatedSVD(n_components=20)\nsvd_result = svd.fit_transform(cast_one_hot)\n\ncast_tsne = TSNE(random_state=0, learning_rate=200, perplexity=5).fit_transform(svd_result)\nplt.title(\"Cast 2D Projection\")\nplt.scatter(cast_tsne[:,0], cast_tsne[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder()\ncrew_one_hot = enc.fit_transform(crew_encoded)\n\nsvd = TruncatedSVD(n_components=50)\nsvd_result = svd.fit_transform(crew_one_hot)\n\ncrew_tsne = TSNE(random_state=0, learning_rate=50, perplexity=5).fit_transform(svd_result)\nplt.title(\"Crew 2D Projection\")\nplt.scatter(crew_tsne[:,0], crew_tsne[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder()\ngenres_one_hot = enc.fit_transform(genres_encoded)\n\nsvd = TruncatedSVD(n_components=10)\nsvd_result = svd.fit_transform(genres_one_hot)\n\ngenres_tsne = TSNE(random_state=0, learning_rate=50, perplexity=30).fit_transform(svd_result)\nplt.title(\"Genres 2D Projection\")\nplt.scatter(genres_tsne[:,0], genres_tsne[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder()\nkeywords_one_hot = enc.fit_transform(keywords_encoded)\n\nsvd = TruncatedSVD(n_components=100)\nsvd_result = svd.fit_transform(keywords_one_hot)\n\nkeywords_tsne = TSNE(random_state=0, learning_rate=50, perplexity=30).fit_transform(svd_result)\nplt.title(\"Keywords 2D Projection\")\nplt.scatter(keywords_tsne[:,0], keywords_tsne[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combining the features\n\nSo far so good. It seems that we are getting a nice clustering for all of our data.\nHowever, further combining these features leads to mostly noise (or random-seeming orderings of movies).\n\nWhat I am presenting here is what is the mapping used for the live website prototype, which uses the keywords and genres only. It is still not ideal, but it is in a shape that is more easily parsed.\n\nPrevious (failed) configuration types have included:\n* Using all four feature sets in a single t-SNE (genre, keyword, cast, crew)\n* Performing numerical operations with the features; eg: adding cast + crew coordinates\n\nBoth of these included dozens of trials with different hyper-parameters, algorithms and amounts of data at each step.\nThere was no configuration that I tried which seemed to come close to the originally desired outcome.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_matrix = np.concatenate((keywords_tsne, genres_tsne), axis=1)\none_tsne = TSNE(random_state=0, learning_rate=30, perplexity=40, early_exaggeration=60).fit_transform(one_matrix)\nplt.scatter(one_tsne[:, 0], one_tsne[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a better view of the final outcome using the DBSCAN clustering algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"db = DBSCAN(eps=3.5, min_samples=20).fit(one_tsne)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nplt.figure(figsize=(30, 30))\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = one_tsne[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = one_tsne[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the data to a file that is readable by the website's code\ncombined = np.array([one_tsne[:,0], one_tsne[:,1]]).T\npop = (normalize(dataframe['popularity'][:, np.newaxis], axis=0) * 20)\npop_rating = pop.reshape((pop.shape[0],)) * dataframe['rating']\ndf = pd.DataFrame(combined * 50)\n\ndf['color'] = labels\ndf['scale'] = np.clip(pop_rating, 0.1, 2) * 50\ndf['title'] = movies['title'] + \" (\" + dataframe['year'] + \")\"\ndf = df.dropna()\n\ndf.to_csv(\"movie_cloud.csv\", sep=\"@\", index=False, header=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}