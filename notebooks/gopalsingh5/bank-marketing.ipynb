{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Taking insights of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/bank.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's just see how much missing data we have in our dataset."},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = df.isnull().sum().sort_values(ascending=False)\nmissing_per = (missing_data/len(df)) * 100\npd.concat([missing_data, missing_per], keys=[\"Missing Data\", \"Missing%\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\ndf['deposit'].value_counts().plot.pie(explode=[0,0.25],\n                                      autopct='%1.2f%%', \n                                      shadow=True, ax=ax[0], \n                                      fontsize=12, startangle=25)\nax[0].set_ylabel('% of condition loans')\n\nsns.barplot(x='education', y='balance', hue='deposit',data=df,\n           estimator=lambda x: len(x)/len(df) * 100)\nax[1].set_ylabel(\"%\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"month\", hue=\"deposit\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most deposits were made in **May** month and leat were made in **December** month.\nThat's a good information because now we can focus that in which month we need to approach the clients the most."},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_yes_or_no = {\n    \"yes\": 1,\n    \"no\": 0\n}\n\n# Binary deposits 0 or 1.\nbin_deposits = df[\"deposit\"].map(dict_yes_or_no)\nsns.lineplot(x=\"age\", y=bin_deposits, data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the deposits were made by who were 60 years or above."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"loan\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='loan', data=df, hue=\"deposit\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(bins=50, figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['marital'], data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Married people have made most deposits."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['marital'], hue='deposit', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue='deposit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we can't plot a heatmap of a categorical column, we need to convert it into integer values. For which we have sklearn's LabelEncoder class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfig = plt.figure(figsize=(12, 8))\ndf['deposit'] = LabelEncoder().fit_transform(df['deposit'])\n\n\nnumerical_data = df.select_dtypes(exclude='object')\ncorr_matrix = numerical_data.corr()\n\nsns.heatmap(corr_matrix, cbar=True)\nplt.title(\"Correlation Matrix\", fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data"},{"metadata":{},"cell_type":"markdown","source":"Often we split our data randomly with sklearn's `train_test_split` function where we don't know how much random error it is creating, we have a better option for this we can split our dataset with the help of `StratifiedShuffleSplit` with a categorical column.\n\nThis cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. For more [StratifiedShuffleSplit](http://http://https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\n\ndef loan_proportions(data):\n    return data[\"loan\"].value_counts() / len(data)\n\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_set, test_set in stratified.split(df, df[\"loan\"]):\n    stratified_train_set = df.loc[train_set]\n    stratified_test_set = df.loc[test_set]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_train, random_test = train_test_split(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props = pd.DataFrame({\n    \"Overall\": loan_proportions(df),\n    \"Stratified\": loan_proportions(stratified_test_set),\n    \"Random\": loan_proportions(random_test),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there less error compare to random split in straified split. So we will stick to the straified splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = stratified_train_set.drop(\"deposit\", axis=1)\nlabels = stratified_train_set['deposit'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will create a pipeline to preprocess our data in a chain system. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = list(features.select_dtypes(include=['int64']))\n\nnum_pipeline = Pipeline([\n    ('select_data', DataFrameSelector(num_cols)),\n    ('std_scaler', StandardScaler())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_cols = list(features.select_dtypes(include=['object']))\n\ncat_pipeline = Pipeline([\n    ('select_data', DataFrameSelector(cat_cols)),\n    (\"cat_encoder\", OneHotEncoder(sparse=False))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    (\"num_pipeline\", num_pipeline, num_cols),\n    (\"cat_pipeline\", cat_pipeline, cat_cols)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am calling it `prepared_data_for_algos`, because this is the data on our model/models going to train on."},{"metadata":{"trusted":true},"cell_type":"code","source":"prepared_data_for_algos = full_pipeline.fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepared_data_for_algos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nlabels_train = encoder.fit_transform(labels)\nlabels_test = encoder.fit_transform(stratified_test_set['deposit'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing classification models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time for Classification Models\nimport time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=18),\n    \"Neural Net\": MLPClassifier(alpha=1),\n    \"Naive Bayes\": GaussianNB()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Thanks to Ahspinar for the function. \nno_classifiers = len(dict_classifiers.keys())\n\ndef batch_classify(X_train, Y_train, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['classifier', 'train_score', 'training_time'])\n    count = 0\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        df_results.loc[count,'classifier'] = key\n        df_results.loc[count,'train_score'] = train_score\n        df_results.loc[count,'training_time'] = t_diff\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndf_results = batch_classify(prepared_data_for_algos, labels_train)\nprint(df_results.sort_values(by='train_score', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmean_scores = []\nfor model_name, model in dict_classifiers.items():\n    scores = cross_val_score(model, prepared_data_for_algos, labels_train,cv=3)\n    mean_score = scores.mean()\n    mean_scores.append(mean_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = list(dict_classifiers.keys())\npd.DataFrame(mean_scores, columns=[\"Mean Scores\"], index=list(index)).sort_values(by=[\"Mean Scores\"],\n                                                                                 ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\ngradient_clf = GradientBoostingClassifier()\ny_train_pred = cross_val_predict(gradient_clf, prepared_data_for_algos, labels_train, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ngradient_clf.fit(prepared_data_for_algos, labels_train)\nprint(\"{} Accuracy : {}\".format(gradient_clf.__class__.__name__, accuracy_score(labels_train, y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n\nconf_matrix = confusion_matrix(labels_train, y_train_pred)\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", linewidths=0.7, ax=ax)\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels(\"\")\nax.set_yticklabels(['Declined Deposits', 'Accepted Deposits'], fontsize=16, rotation=360)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision vs Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision Score :{}\\nRecall Score :{}\".format(precision_score(labels_train, y_train_pred),\n                                                    recall_score(labels_train, y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(labels_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_clf = MLPClassifier(alpha=1)\nsvm_clf = SVC(gamma='auto', probability=True)\n\ny_scores = cross_val_predict(gradient_clf, prepared_data_for_algos, labels_train, cv=3, method=\"decision_function\")\nneural_y_scores = cross_val_predict(neural_clf, prepared_data_for_algos, labels_train, cv=3, method=\"predict_proba\")\nsvm_y_scores = cross_val_predict(svm_clf, prepared_data_for_algos, labels_train, cv=3, method=\"decision_function\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if y_scores.ndim == 2:\n    y_scores = y_scores[:, 1]\n\nif neural_y_scores.ndim == 2:\n    neural_y_scores = neural_y_scores[:, 1]\n    \nif svm_y_scores.ndim == 2:\n    naives_y_scores = naives_y_scores[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, threshold = precision_recall_curve(labels_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, threshold = precision_recall_curve(labels_train, y_scores)\n\ndef precision_recall_curve(precisions, recalls, thresholds):\n    fig, ax = plt.subplots(figsize=(12,8))\n    plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precisions\")\n    plt.plot(thresholds, recalls[:-1], \"#424242\", label=\"Recalls\")\n    plt.title(\"Precision and Recall \\n Tradeoff\", fontsize=18)\n    plt.ylabel(\"Level of Precision and Recall\", fontsize=16)\n    plt.xlabel(\"Thresholds\", fontsize=16)\n    plt.legend(loc=\"best\", fontsize=14)\n    plt.xlim([-2, 4.7])\n    plt.ylim([0, 1])\n    plt.axvline(x=0.13, linewidth=3, color=\"#0B3861\")\n    plt.annotate('Best Precision and \\n Recall Balance \\n is at 0.13 \\n threshold ', xy=(0.13, 0.83), xytext=(55, -40),\n             textcoords=\"offset points\",\n            arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=12, \n                color='k')\n    \nprecision_recall_curve(precisions, recalls, threshold)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(labels_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr, tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(labels_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Gradient Boost Classifier Score: ', roc_auc_score(labels_train, y_scores))\nprint('Neural Classifier Score: ', roc_auc_score(labels_train, neural_y_scores))\nprint('SVM Classifier: ', roc_auc_score(labels_train, svm_y_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_fpr, nn_tpr, nn_thresholds = roc_curve(labels_train, neural_y_scores)\nsvm_fpr, svm_tpr, svm_thresholds = roc_curve(labels_train, svm_y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"Gradient Boosting Classifier\")\nplot_roc_curve(nn_fpr, nn_tpr, \"Neural Network\")\nplot_roc_curve(svm_fpr, svm_tpr, \"SVC\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfor col in df.select_dtypes(exclude='int64').columns:\n    df[col] = df[col].astype('category').cat.codes\n\n\nX = df.drop('deposit', axis=1)\ny = df['deposit']\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=123)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nrnd_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rnd_clf.feature_importances_\nfeature_names = df.drop('deposit', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\ndef feature_importance_graph(model, indices, importances, feature_names):\n    plt.figure(figsize=(12,6))\n    plt.title(\"Determining Feature importances \\n with {}\".format(model.__class__.__name__), fontsize=18)\n    plt.barh(range(len(indices)), importances[indices], color='#31B173',  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])\n    plt.axhline(y=1.85, xmin=0.21, xmax=0.952, color='k', linewidth=3, linestyle='--')\n    plt.text(0.30, 2.8, '46% Difference between \\n duration and contacts', color='k', fontsize=15)\n    \nfeature_importance_graph(rnd_clf, indices, importances, feature_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators=[('gbc', gradient_clf), ('svm', svm_clf), ('neural_net', neural_clf)],\n    voting='soft'\n)\nvoting_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = prepared_data_for_algos\ny_train = labels_train\n\nX_test = stratified_test_set.drop('deposit', axis=1)\nX_test_prepapred = full_pipeline.transform(X_test)\n\ny_test = labels_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in (gradient_clf, svm_clf, neural_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test_prepapred)\n    print(clf.__class__.__name__, accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}