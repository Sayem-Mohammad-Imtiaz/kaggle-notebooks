{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Basic packages\nimport pandas as pd \nimport numpy as np\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\n# Packages for data preparation\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nimport keras\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))  \ndf = df[['text', 'airline_sentiment']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_mentions(input_text):\n        return re.sub(r'@\\w+', '', input_text)\ndef remove_urls(input_text):\n        return re.sub(r\"http\\S+\", \"\", input_text)\ndef remove_punctuations(input_text):\n        return re.sub(r'[^\\w\\d\\s\\']+', '', input_text)\ndef remove_numbers(input_text):\n        return re.sub(r\"[0-9]\",\" \",input_text)\ndf.text = df.text.apply(remove_mentions).apply(remove_urls).apply(remove_numbers).apply(remove_punctuations)\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])\nassert X_train.shape[0] == y_train.shape[0]\nassert X_test.shape[0] == y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer()\nX_train_tokenizer={}\nfor x in X_train.index:\n    X_train_tokenizer[x]=tknzr.tokenize(X_train[x].lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_tokenizer={}\nfor x in X_test.index:\n    X_test_tokenizer[x]=tknzr.tokenize(X_test[x].lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \n\nfor x in X_train.index:\n    X_train_tokenizer[x] = [w for w in X_train_tokenizer[x] if not w in stop_words] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in X_test.index:\n    X_test_tokenizer[x] = [w for w in X_test_tokenizer[x] if not w in stop_words] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_sentences=[]\npadded_sentences={}\nlabels=[]\nfor x in X_train.index:\n    sentence=''\n    if(y_train[x]==\"positive\"):\n        labels.append([1,0,0])\n    elif(y_train[x]==\"negative\"):\n        labels.append([0,0,1])\n    elif (y_train[x]==\"neutral\"):\n        labels.append([0,1,0])\n    else:\n        print(x)\n    for s in X_train_tokenizer[x]:\n        sentence+=s\n        sentence+=\" \"\n    embedded_sentences.append(one_hot(sentence, 12000))\n    #padded_sentences[x]=pad_sequences(embedded_sentences[x], 21, padding='post')\n    \n    #print(embedded_sentences[x])#,padded_sentences[x] )\n\npadded_sentences=pad_sequences(embedded_sentences,31,padding='post')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_sentences_test=[]\npadded_sentences_test={}\nlabels_test=[]\nfor x in X_test.index:\n    sentence=''\n    if(y_test[x]==\"positive\"):\n        labels_test.append([1,0,0])\n    elif(y_test[x]==\"negative\"):\n        labels_test.append([0,0,1])\n    elif (y_test[x]==\"neutral\"):\n        labels_test.append([0,1,0])\n    else:\n        print(x)\n    for s in X_test_tokenizer[x]:\n        sentence+=s\n        sentence+=\" \"\n    embedded_sentences_test.append(one_hot(sentence, 12000))\n    #padded_sentences[x]=pad_sequences(embedded_sentences[x], 21, padding='post')\n    \n    #print(embedded_sentences[x])#,padded_sentences[x] )\n\npadded_sentences_test=pad_sequences(embedded_sentences_test,31,padding='post')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among the following three cells you can run any of the cell to use that particular model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n    \nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \nembed_dim = 256  # Embedding size for each token\nnum_heads = 64  # Number of attention heads\nff_dim = 64\n\ninputs = layers.Input(shape=(31,))\nembedding_layer = Embedding(12000, 256, trainable=True)(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(embedding_layer)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(16, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(8, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(3, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',f1_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.Input(shape=(31,), dtype=\"int32\")\n# Embed each integer in a 128-dimensional vector\nx = layers.Embedding(12000, 128)(inputs)\n# Add 2 bidirectional LSTMs\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\n# Add a classifier\noutputs = layers.Dense(3, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=12000\nembedding_dim=128\ninputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n\n# Next, we add a layer to map those vocab indices into a space of dimensionality\n# 'embedding_dim'.\nx = layers.Embedding(max_features, embedding_dim)(inputs)\nx = layers.Dropout(0.5)(x)\n\n# Conv1D + global max pooling\nx = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\nx = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\nx = layers.GlobalMaxPooling1D()(x)\n\n# We add a vanilla hidden layer:\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\npredictions = layers.Dense(3, activation=\"sigmoid\", name=\"predictions\")(x)\n\nmodel = tf.keras.Model(inputs, predictions)\n\n# Compile the model with binary crossentropy loss and an adam optimizer.\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",f1_m])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can also change the epochs as per the requirements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=np.array(labels)\nprint(labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_test=np.array(labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(padded_sentences, labels, epochs=5, verbose=1,batch_size=64,validation_data=(padded_sentences_test,labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(padded_sentences_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words=[]\nmax_length=0\nfor x in X_train.index:\n    if max_length<len(X_train_tokenizer[x]):\n        max_length=len(X_train_tokenizer[x])\n    for w in X_train_tokenizer[x]:\n        all_words.append(w)\nunique_words = set(all_words)\nprint(len(unique_words),max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words=[]\nmax_length=0\nfor x in X_t.index:\n    if max_length<len(X_test_tokenizer[x]):\n        max_length=len(X_test_tokenizer[x])\n    for w in X_test_tokenizer[x]:\n        all_words.append(w)\nunique_words = set(all_words)\nprint(len(unique_words),max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len=0\nfor x in X_train.index:\n    if max_len<len(X_train[x].split()):\n        max_len=len(X_train[x].split())\n    #print(x,X_train[x],y_train[x])\nprint(max_len)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}