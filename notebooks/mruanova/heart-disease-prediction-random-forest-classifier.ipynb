{"cells":[{"metadata":{},"cell_type":"markdown","source":">Dedicated to my dad and all those who are dealing with it or know someone that does... <3"},{"metadata":{},"cell_type":"markdown","source":"This [Data Set](https://www.kaggle.com/ronitf/heart-disease-uci) contains 14 attributes used by Machine Learning researchers to predict the presence of heart disease in a patient. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\nimport numpy as np # linear algebra\nprint(\"NumPy version: {}\". format(np.__version__))\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nprint(\"pandas version: {}\". format(pd.__version__))\nimport matplotlib # collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings # ignore warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns description:\n- age: age in years\n- sex: (1 = male; 0 = female)\n- cp: chest pain type\n- trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n- chol: serum cholestoral in mg/dl\n- fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n- restecg: resting electrocardiographic results\n- thalach: maximum heart rate achieved\n- exang: exercise induced angina (1 = yes; 0 = no)\n- oldpeak: ST depression induced by exercise relative to rest\n- slope: the slope of the peak exercise ST segment\n- ca: number of major vessels (0-3) colored by flourosopy\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n- target: refers to the presence of heart disease in the patient (1=yes, 0=no)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.target.value_counts() # df.target.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disease = len(df[df['target'] == 1])\nno_disease = len(df[df['target']== 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcdefaults()\nfig, ax = plt.subplots()\ny = ('Heart Disease', 'No Disease')\ny_pos = np.arange(len(y))\nx = (disease, no_disease)\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(y)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('Count')\nax.set_title('Target')\nfor i, v in enumerate(x):\n    ax.text(v + 10, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ny = ('Heart Disease', 'No Disease')\ny_pos = np.arange(len(y))\nx = (disease, no_disease)\nlabels = 'Heart Disease', 'No Disease'\nsizes = [disease, no_disease]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum() # missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qualitative = []\nquantitative = []\nfor feature in df.columns:\n    if len(df[feature].unique()) <= 8:\n        qualitative.append(feature)\n    else:\n        quantitative.append(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qualitative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantitative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = 15\ncorr = df.corr()\ntop15 = corr.nlargest(top, 'target')['target'].index\ncorr_top15 = df[top15].corr()\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_top15, square=True, ax=ax, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size':12})\nplt.title('Top correlated features of dataset', size=16)\nplt.show()\n\"\"\"\ncorrmat = df.corr()\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\nax.set_title(\"Correlation Matrix\", fontsize=12)\nfilter = df.columns != 'id'\nsns.heatmap(df[df.columns[filter]].corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the variable with the most correlation with target is talach"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(df['thalach']) # histogram distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('target',1)\ny = df['target']\nprint('shape of X and y respectively :', X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint('shape of X and y respectively (train) :', X_train.shape, y_train.shape)\nprint('shape of X and y respectively (test) :', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Logistic Regression')\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred}) # Heart-Disease yes or no? 1/0\nprint(output.head())\npeople = output.loc[output.Predicted == 1][\"Predicted\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)/len(output)\nprint(\"% of people predicted with heart-disease:\", rate_people)\nscore_logreg = score\nout_logreg = output\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,Y_pred)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Greens', fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for logistic regression')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve\nfrom sklearn.metrics import roc_auc_score,roc_curve\ny_probabilities = model.predict_proba(X_test)[:,1]\nfalse_positive_rate_knn, true_positive_rate_knn, threshold_knn = roc_curve(y_test,y_probabilities)\nplt.figure(figsize=(10,6))\nplt.title('ROC for logistic regression')\nplt.plot(false_positive_rate_knn, true_positive_rate_knn, linewidth=5, color='green')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('DecisionTreeClassifier')\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(max_depth=5) \ndecision_tree.fit(X_train, y_train)  \nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred}) # Heart-Disease yes or no? 1/0\nprint(output.head())\npeople = output.loc[output.Predicted == 1][\"Predicted\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)/len(output)\nprint(\"% of people predicted with heart-disease:\", rate_people)\nscore_dtc = score\nout_dtc = output\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,Y_pred)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Greens', fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for decision tree')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve\nfrom sklearn.metrics import roc_auc_score,roc_curve\ny_probabilities = model.predict_proba(X_test)[:,1]\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test,y_probabilities)\nplt.figure(figsize=(10,6))\nplt.title('ROC for decision tree')\nplt.plot(false_positive_rate, true_positive_rate, linewidth=5, color='green')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier')\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100) # , max_depth=5, random_state=1\nmodel.fit(X_train, y_train)\nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred}) # Heart-Disease yes or no? 1/0\nprint(output.head())\npeople = output.loc[output.Predicted == 1][\"Predicted\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)/len(output)\nprint(\"% of people predicted with heart-disease:\", rate_people)\nscore_rfc = score\nout_rfc = output\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,Y_pred)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Greens', fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for random forest')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve\nfrom sklearn.metrics import roc_auc_score,roc_curve\ny_probabilities = model.predict_proba(X_test)[:,1]\nfalse_positive_rate, true_positive_rate, threshold_knn = roc_curve(y_test,y_probabilities)\nplt.figure(figsize=(10,6))\nplt.title('ROC for random forest')\nplt.plot(false_positive_rate, true_positive_rate, linewidth=5, color='green')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNeighborsClassifier')\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score_logreg)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred}) # Heart-Disease yes or no? 1/0\nprint(output.head())\npeople = output.loc[output.Predicted == 1][\"Predicted\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)/len(output)\nprint(\"% of people predicted with heart-disease:\", rate_people)\nscore_knc = score\nout_knc = output\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,Y_pred)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Greens', fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for knc')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC Curve\nfrom sklearn.metrics import roc_auc_score,roc_curve\ny_probabilities = model.predict_proba(X_test)[:,1]\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test,y_probabilities)\nplt.figure(figsize=(10,6))\nplt.title('ROC for knc')\nplt.plot(false_positive_rate, true_positive_rate, linewidth=5, color='green')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcdefaults()\nfig, ax = plt.subplots()\nalgorithms = ('Logistic Regression', 'K Neighbors Classifier', 'Random Forest Classifier', 'Decision Tree Classifier')\ny_pos = np.arange(len(algorithms))\nx = (score_logreg, score_knc, score_rfc, score_dtc) # scores\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(algorithms)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('Performance')\nax.set_title('Which one is the best algorithm?')\nfor i, v in enumerate(x):\n    ax.text(v + 1, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_logreg\noutput = out_logreg\nprint(\"Logistic Regression!\", score)\nif score_dtc > score:\n    score = score_dtc\n    output = out_dtc\n    print(\"Decision Tree Classifier!\", score)\nif score_rfc > score:\n    score = score_rfc\n    output = out_rfc\n    print(\"Random Forest Classifier!\", score)\nif score_knc > score:\n    score = score_knc\n    output = out_knc\n    print(\"K Neighbors Classifier!\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('output.csv', index=False)\nprint(\"Success!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions"},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame(columns=['score'])\nresults.loc['Logistic Regression']=[score_logreg]\nresults.loc['Decision Tree Classifier']=[score_dtc]\nresults.loc['Random Forest Classifier']=[score_rfc]\nresults.loc['K-Neighbors Classifier']=[score_knc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.sort_values('score',ascending=False).style.background_gradient(cmap='Greens',subset=['score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest models are a kind of non parametric models that can be used both for regression and classification. They are one of the most popular ensemble methods, belonging to the specific category of Bagging methods.\n\nEnsemble methods involve using many learners to enhance the performance of any single one of them individually. These methods can be described as techniques that use a group of weak learners (those who on average achieve only slightly better results than a random model) together, in order to create a stronger, aggregated one.\n\nIn our case, Random Forests are an ensemble of many individual Decision Trees, the family of Machine Learning models we saw just before.\n\nRandom Forest introduce randomness and numbers into the equation, fixing many of the problems of individual decision trees, like overfitting and poor prediction power.\n\nIn a Random Forest each tree is built using a subset of the training data, and usually only a subset of the possible features. As more and more trees are built, a wider range of our data is used, and more features come into play, making very strong, aggregated models.\n\nIndividual trees are built independently, using the same procedure as for a normal decision tree but with only a random portion of the data and only considering a random subset of the features at each node. Aside from this, the training procedure is exactly the same as for an individual Decision Tree, repeated N times.\n\nTo make a prediction using a Random Forest each an individual prediction is obtained from each tree. Then, if it is a classification problem, we take the most frequent prediction as the result, and if it is a regression problem we take the average prediction from all the individual trees as the output value."},{"metadata":{},"cell_type":"markdown","source":"Thanks [towards data science](https://towardsdatascience.com/a-summary-of-the-basic-machine-learning-models-e0a65627ecbe)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}