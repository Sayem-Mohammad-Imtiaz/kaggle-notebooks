{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"# Explaining & Visualizing Principal Component Analysis and SVD\nThis Notebook aims to visualize PCA and show its connections to the Singular Value Decomposition. But first, I will briefly explain Quadratic Forms of matricies, constrained optimization problems and derive PCA and its connections to the SVD.\n\n## 1. Quadratic Forms\nLet $A =\n\\begin{bmatrix}\n4 && 1.5 \\\\\n1.5 && 7\n\\end{bmatrix}\n$ (a symmetric matrix). A quadratic form $x^TAx = 4x_1^2 + 7x_2^2 + 3x_1 x_2$\n\nFor **diagonal matricies**, the resulting equation does not include cross-product terms and is thus, much easier to work with. **Example:**\n\nLet $A =\n\\begin{bmatrix}\n4 && 0 \\\\\n0 && 7\n\\end{bmatrix}\n$. A quadratic form $x^TAx = 4x_1^2 + 7x_2^2$. We thus want our matrix to be **diagonal**. We can change it to be that way!\n\n#### Change of Variable\nFirst define a change of variable: $x = Py$ where $P$ is an orthogonal matrix with **orthonormal columns**. We will now see why this is very beneficial and simplifies the problem alot! Now rewrite the equation as:\n\n$x^TAx = (Py)^TA(Py) = y^TP^TA(Py) = y^T(P^TAP)y$\n\nNow it is obvious why a change of variable with $P$ having **orthonormal columns** is nice: We now have $P^TAP = D$ because $A = PDP^T$, where $PDP^T$ is an **eigenvalue decomposition** of $A$.\n\nNow we have $y^TDy$ and the matrix is diagonal! We can get to x again with $x = Py$.\n\n## 2. Constrained Optimization\nI will now describe a very simple constrained optimization problem where we want to maximize the quadratic form $ x^TAx $ subj. to $\\lVert x \\lVert=1$. This problem has a very interesting solution.\n\nSo since $A$ is symmetric, it is **orthogonally diagonizable**. We know from sec. 1: $y^TDy$. So just **create an eigenvalue decomposition**, and let $D$ be the diagonal matrix of eigenvalues, arranged in descending order, with the eigenvectors in $P$ according to $D$ and normalized.\n\nHow is $y^TDy$ maximized?\n\n**Example:**\n\nLet $D =\n\\begin{bmatrix}\n9 && 0 \\\\\n0 && 6\n\\end{bmatrix}\n$. So $y^TDy = 9y_1^2 + 6y_2^2$. The maximum value of the function is reached if $y=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$. It is 9, the largest eigenvalue of $A$, corresponding to its eigenvector $Py$. So the $x$ that maximizes $x^TAx$ is the eigenvector of the largest eigenvalue of $A$.\n\n**Now we are ready to discuss the SVD and PCA!**\n\n## 3. The Singular Value Decomposition\nEvery mxn matrix $A$ can be described in the form $AV=U\\Sigma$. So $A=U\\Sigma V^T$. I will briefly describe the decomposition.\n\n#### Singular Values\nWe want to find vector $v$ of magnitude 1, that maximizes $\\lVert Av \\lVert$. Since the same vector maximizes $\\lVert Av \\lVert^2$, and this problem is much easier, we will maximize $\\lVert Av \\lVert^2$.\n\n**=>** $argmax_v(\\lVert Av \\lVert) = argmax_v(\\lVert Av \\lVert^2)$\n\n$\\lVert Av \\lVert^2 = (Av)^T(Av) = v^T A^T Av$\n\nThis is a **Quadradic Form** since $A^TA$ is **symmetric**. We know that its unit **eigenvector** maximizes $v^T A^T Av$, with its corresponding **eigenvalue** as the maximum. Therefore $max(\\lVert Av \\lVert) = \\sqrt{max(\\lVert Av \\lVert^2)}$, which is called **singular value** $\\sigma_1$ of $A$ with the unit eigenvector $v_1$ from $A^TA$. $A$ has as many singular values as linearly independent columns ($range(A)$).\n\nFinally we have: $\\lVert Av_i \\lVert=\\sigma_i$\n\nNow construct $U\\Sigma$, which must equal to $AV$.\n\n$dim(A)=$mxn\n\n$dim(U)=$mxm, called left singular vectors.\n\n$dim(\\Sigma)=$mxn, diagonal matrix with singular values $\\sigma_i$ of $A$ in descending order.\n\n$dim(V)=$nxn, called right singular vectors (Orthonormal eigenvectors of $A^TA$).\n\nWe define the columns of $U$ [$u_1$,...,$u_m$] as $u_i = \\frac{Av_i}{\\sigma_i}$.\nWe define $\\Sigma$ to be a **diagonal matrix** with the **singular values** $\\sigma$ of A, in descending order, and the eigenvectors in $V$ arranged accordingly.\n\nTherefore $AV = U\\Sigma$ and $A = U\\Sigma V^T$\n\n## 4. Relationship between SVD and PCA\nI will briefly describe how the **covariance matrix** of $X$ ($C_x$) is related its SVD. For simplicity, define $X$ to already be in **mean-deviation form**.\n\n$COV(X) = \\frac{XX^T}{n-1} = C_x$\n\n$COV(X) = (U\\Sigma V^T)(U\\Sigma V^T)^T/(n-1) = U\\Sigma V^T  V\\Sigma^T U^T / (n-1) = U\\frac{\\Sigma^2}{n-1}U^T = C_x$\n\nRecall that the goal of PCA is to find a change of variable $X=PY$ for which the new covariance matrix is diagonal. Because $P^TX=Y$:\n\n$COV(Y) = (P^TX)(P^TX)^T/(n-1) = P^T(C_x(n-1))P/(n-1) = P^TC_xP = D$\n\nSo we have $C_x = PDP^T$\n\nFrom the SVD we have $C_x = U\\frac{\\Sigma^2}{n-1}U^T$\n\nBoth $P$ and $U$ are **orthonormal eigenvectors** of $XX^T$. **Both decompositions differ just by the scaling factor** $n-1$ in the **diagonal matricies** (since the columns of $P$ and $U$ have same length of 1). The eigenvalues $\\lambda_i$ of $D$ are related to the singular values from the **SVD** $\\sigma_i$ via $\\lambda_i = \\frac{\\sigma_i}{n-1}$."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"## Visualizing PCA"},{"metadata":{"_uuid":"a91ca07896026337db8b597592d52839b40c4d91","collapsed":true,"_cell_guid":"04f9f4b0-24e7-4ccd-928a-976a8d1963f0","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"733ce499406578d4a4e312c15fe881f7d1047b55","collapsed":true,"_cell_guid":"fdf40227-22d0-42df-8154-355efcb6889a","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6a6387c9df8743e3a54628143f4277c746adfd9","collapsed":true,"_cell_guid":"a5595c6b-eee5-44a2-94fe-313d2fed329a","trusted":false},"cell_type":"code","source":"df['species'] = df['species'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ndf = df.sample(frac=1.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95acd3ad6d92c1c4336ccaec03c709f3a8cce0fb","_cell_guid":"2ca00403-19d2-4d30-9fdf-54b3b10cb564","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86ab4e22b4d3be10573e720ab49de025629ee870","collapsed":true,"_cell_guid":"13226ed4-154f-4e28-b2f6-c6d92446ee41"},"cell_type":"markdown","source":"### Plot iris-data in 3 dimensions"},{"metadata":{"_uuid":"381f7b2268148e1e8c07255172ac7d3749f17071","collapsed":true,"_cell_guid":"7d08d5c1-c917-478c-b97e-137e1c3c4ce2","trusted":false},"cell_type":"code","source":"X = df.iloc[:, :-1].as_matrix().T\n\nX_mean = np.mean(X, axis=1).reshape(-1, 1)\nX_std = np.std(X, axis=1).reshape(-1, 1)\n\nX -= X_mean\nX /= X_std","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab93a520a04df79a2b8aa9e90fa63eabb4fccf6b","_cell_guid":"6792932e-baa8-4b07-9560-857ce645b99c","trusted":false,"collapsed":true},"cell_type":"code","source":"df1 = df.iloc[:, 1:]\n\nclass0 = df1[df1['species']==0]\nclass1 = df1[df1['species']==1]\nclass2 = df1[df1['species']==2]\n\nclass0_mat = (df[df['species']==0].iloc[:, :-1].as_matrix().T - X_mean) / X_std\nclass1_mat = (df[df['species']==1].iloc[:, :-1].as_matrix().T - X_mean) / X_std\nclass2_mat = (df[df['species']==2].iloc[:, :-1].as_matrix().T - X_mean) / X_std\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(class0.iloc[:, 0], class0.iloc[:, 1], class0.iloc[:, 2], c='blue')\nax.scatter(class1.iloc[:, 0], class1.iloc[:, 1], class1.iloc[:, 2], c='red')\nax.scatter(class2.iloc[:, 0], class2.iloc[:, 1], class2.iloc[:, 2], c='green')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5858a4cb00b6592e6ccdf604ee51e2928b597825","collapsed":true,"_cell_guid":"a8abef80-3210-4b98-89ab-67dcc43f18e6","trusted":false},"cell_type":"code","source":"U, S, Vh = np.linalg.svd(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"089cda8a5f2294dc0b70b777eb86a298f51ed68e","_cell_guid":"2be3d645-0add-4d2f-9d9b-8ff4ca59e72d"},"cell_type":"markdown","source":"### Project data onto first 3 principal components & plot in 3D"},{"metadata":{"_uuid":"5e0eb3020ea687ed72d91214941bf5f1da4e2665","collapsed":true,"_cell_guid":"14442eea-5af3-4a6f-b1e6-68f28b56a01d","trusted":false},"cell_type":"code","source":"U = U[:, :-1]\nY_0 = U.T @ class0_mat\nY_1 = U.T @ class1_mat\nY_2 = U.T @ class2_mat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a20d2d8bd21c3510a4f972ff49f417bdbd3712f2","_cell_guid":"3c0603bb-fe66-44a0-b1c6-8cc717126c24","trusted":false,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(Y_0[0], Y_0[1], Y_0[2], c='blue')\nax.scatter(Y_1[0], Y_1[1], Y_1[2], c='red')\nax.scatter(Y_2[0], Y_2[1], Y_2[2], c='green')\n\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cc9ebd2929b539793a2e31b3e0901d5a882f81c","collapsed":true,"_cell_guid":"7a500dc5-80aa-42ed-99ec-e33efc86df3b","trusted":false},"cell_type":"code","source":"U, S, Vh = np.linalg.svd(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bb592d1ddaa5eb7192eaef548464dc88d498d75","_cell_guid":"5221f7ba-4d60-454e-b967-6c6c83095c61"},"cell_type":"markdown","source":"### Project data onto first and second principal components & plot in 2D"},{"metadata":{"_uuid":"905dc6d6bd754dc72fd11f6c7a6a1d3ddee4b4a7","collapsed":true,"_cell_guid":"28d58f76-7a57-4d03-a764-3217db0f5b42","trusted":false},"cell_type":"code","source":"U = U[:, :-2]\nY_0 = U.T @ class0_mat\nY_1 = U.T @ class1_mat\nY_2 = U.T @ class2_mat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed939f68e7566bc4b4cf1881da0cb21b945fca81","_cell_guid":"844afa34-90fa-48fc-922a-64a7036a480e","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.scatter(Y_0[0], Y_0[1], c='blue')\nplt.scatter(Y_1[0], Y_1[1], c='red')\nplt.scatter(Y_2[0], Y_2[1], c='green')\n\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f676875329f9c39027a8bb41984f9e40685f6a","_cell_guid":"27ddaeca-32d8-4579-a1c9-9f1d1628eace"},"cell_type":"markdown","source":"### Variances of P.C.s\nBy examining how much variance each principal component explains, one can conclude that the first 3 P.C.s suffice to explain most of the data."},{"metadata":{"_uuid":"364cde99a693ef9de8808e1741230ec581fdef18","_cell_guid":"4d2052d8-a79c-45a1-aa7b-3be7da34e575","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Number of P.C.s:', len(S))\nprint('First P.C. explains', np.sum(S[0]) / np.sum(S), 'of the total variance.')\nprint('Second P.C. explains', np.sum(S[1]) / np.sum(S), 'of the total variance.')\nprint('Third P.C. explains', np.sum(S[2]) / np.sum(S), 'of the total variance.')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}