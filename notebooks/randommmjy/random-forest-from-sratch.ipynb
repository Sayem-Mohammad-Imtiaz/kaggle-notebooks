{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I will implement a random forest classifier from scratch in Python and test it using scikit-learn's built-in random forest classifier and the red wine dataset. \n\n\n**Logic of Random Forest:**\n\nThe general idea of random forest is not very sophisticated. We are essentially ensembling a group of decision trees, each trained using a random subset of the data and features, and making a voting-based prediction. The reason we don't want all decision trees to be trained using the same data is because we want random variation between the trees, which may help the model to overcome noises in the data.\n\nSo My procedure in this notebook will be:\n1. Implement the Decision Tree classifier\n2. Implement Bootstrap and Random Subspace(used to randomly select data and features for each decision tree)\n3. Implement the Random Forest Algorithm\n4. Test my implementation with scikit-learn's random forest classifier"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\n\nimport random\nfrom pprint import pprint\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Functions\n\nRandom forest is based on decision tree. So before we begin our implementation let's first bring our decision tree implementation from last time here. Also, random forest algorithm aims to scatter each sub-tree as far as possible. So when training each decision tree, we will add a new parameter of random_subspace, which essentially randomly selects a subset of features that each tree can use to partition the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random\n\n\n# 1. Train-Test-Split\ndef train_test_split(df, test_size):\n    \n    if isinstance(test_size, float):\n        test_size = round(test_size * len(df))\n\n    indices = df.index.tolist()\n    test_indices = random.sample(population=indices, k=test_size)\n\n    test_df = df.loc[test_indices]\n    train_df = df.drop(test_indices)\n    \n    return train_df, test_df\n\n\n# 2. Distinguish categorical and continuous features\ndef determine_type_of_feature(df):\n    \n    feature_types = []\n    n_unique_values_treshold = 15\n    for feature in df.columns:\n        if feature != \"label\":\n            unique_values = df[feature].unique()\n            example_value = unique_values[0]\n\n            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n                feature_types.append(\"categorical\")\n            else:\n                feature_types.append(\"continuous\")\n    \n    return feature_types\n\n\n# 3. Accuracy\ndef calculate_accuracy(predictions, labels):\n    predictions_correct = predictions == labels\n    accuracy = predictions_correct.mean()\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\n\n# 1. Decision Tree helper functions \n# (see \"decision tree algorithm flow chart.png\")\n\n# 1.1 Data pure?\ndef check_purity(data):\n    \n    label_column = data[:, -1]\n    unique_classes = np.unique(label_column)\n\n    if len(unique_classes) == 1:\n        return True\n    else:\n        return False\n\n    \n# 1.2 Classify\ndef classify_data(data):\n    \n    label_column = data[:, -1]\n    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n\n    index = counts_unique_classes.argmax()\n    classification = unique_classes[index]\n    \n    return classification\n\n\n# 1.3 Potential splits?\ndef get_potential_splits(data):\n    \n    potential_splits = {}\n    _, n_columns = data.shape\n    for column_index in range(n_columns - 1):  # excluding the last column which is the label\n        values = data[:, column_index]\n        unique_values = np.unique(values)\n        \n        potential_splits[column_index] = unique_values\n    \n    return potential_splits\n\n\n# 1.4 Lowest Overall Entropy?\ndef calculate_entropy(data):\n    \n    label_column = data[:, -1]\n    _, counts = np.unique(label_column, return_counts=True)\n\n    probabilities = counts / counts.sum()\n    entropy = sum(probabilities * -np.log2(probabilities))\n     \n    return entropy\n\n\ndef calculate_overall_entropy(data_below, data_above):\n    \n    n = len(data_below) + len(data_above)\n    p_data_below = len(data_below) / n\n    p_data_above = len(data_above) / n\n\n    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n                      + p_data_above * calculate_entropy(data_above))\n    \n    return overall_entropy\n\n\ndef determine_best_split(data, potential_splits):\n    \n    overall_entropy = 9999\n    for column_index in potential_splits:\n        for value in potential_splits[column_index]:\n            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n            \n            if current_overall_entropy <= overall_entropy:\n                overall_entropy = current_overall_entropy\n                best_split_column = column_index\n                best_split_value = value\n    \n    return best_split_column, best_split_value\n\n\n# 1.5 Split data\ndef split_data(data, split_column, split_value):\n    \n    split_column_values = data[:, split_column]\n\n    type_of_feature = FEATURE_TYPES[split_column]\n    if type_of_feature == \"continuous\":\n        data_below = data[split_column_values <= split_value]\n        data_above = data[split_column_values >  split_value]\n    \n    # feature is categorical   \n    else:\n        data_below = data[split_column_values == split_value]\n        data_above = data[split_column_values != split_value]\n    \n    return data_below, data_above","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5, random_subspace=None):\n    \n    # data preparations\n    if counter == 0:\n        global COLUMN_HEADERS, FEATURE_TYPES\n        COLUMN_HEADERS = df.columns\n        FEATURE_TYPES = determine_type_of_feature(df)\n        data = df.values\n    else:\n        data = df           \n    \n    \n    # base cases\n    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n        classification = classify_data(data)\n        \n        return classification\n\n    \n    # recursive part\n    else:    \n        counter += 1\n\n        # helper functions \n        potential_splits = get_potential_splits(data, random_subspace)\n        split_column, split_value = determine_best_split(data, potential_splits)\n        data_below, data_above = split_data(data, split_column, split_value)\n        \n        # check for empty data\n        if len(data_below) == 0 or len(data_above) == 0:\n            classification = classify_data(data)\n            return classification\n        \n        # determine question\n        feature_name = COLUMN_HEADERS[split_column]\n        type_of_feature = FEATURE_TYPES[split_column]\n        if type_of_feature == \"continuous\":\n            question = \"{} <= {}\".format(feature_name, split_value)\n            \n        # feature is categorical\n        else:\n            question = \"{} = {}\".format(feature_name, split_value)\n        \n        # instantiate sub-tree\n        sub_tree = {question: []}\n        \n        # find answers (recursion)\n        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth, random_subspace)\n        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth, random_subspace)\n        \n        # If the answers are the same, then there is no point in asking the qestion.\n        # This could happen when the data is classified even though it is not pure\n        # yet (min_samples or max_depth base case).\n        if yes_answer == no_answer:\n            sub_tree = yes_answer\n        else:\n            sub_tree[question].append(yes_answer)\n            sub_tree[question].append(no_answer)\n        \n        return sub_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_example(example, tree):\n    question = list(tree.keys())[0]\n    feature_name, comparison_operator, value = question.split(\" \")\n\n    # ask question\n    if comparison_operator == \"<=\":\n        if example[feature_name] <= float(value):\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n    \n    # feature is categorical\n    else:\n        if str(example[feature_name]) == value:\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n\n    # base case\n    if not isinstance(answer, dict):\n        return answer\n    \n    # recursive part\n    else:\n        residual_tree = answer\n        return predict_example(example, residual_tree)\n\n    \n# 3.2 All examples of the test data\ndef decision_tree_predictions(test_df, tree):\n    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data = pd.read_csv('/kaggle/input/red-wine-dataset/wineQualityReds.csv')\nfull_data['label'] = full_data['quality']\nfull_data = full_data.drop('quality', axis=1)\n\ncolumn_names = []\nfor column in full_data.columns:\n    name = column.replace(\" \", \"_\")\n    column_names.append(name)\nfull_data.columns = column_names\n\nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_label(value):\n    if value <= 5:\n        return \"bad\"\n    else:\n        return \"good\"\n\nfull_data[\"label\"] = full_data.label.apply(transform_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(0)\ntrain_data, test_data = train_test_split(full_data, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bootstrap\n\nBootstrap is a key component of random forest algorithm. Since we want each decision tree to be different, we will not use the entire train set to train each decision tree. In contrast, we will randomly select a subset of data from the train set for each tree. This process of randomly selecting is bootstrapping. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def bootstrap(train_data, n_bootstrap):\n    index = np.random.randint(low=0, high=len(train_data), size=n_bootstrap)\n    boot_data = train_data.iloc[index]\n    \n    return boot_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bootstrap(train_data, n_bootstrap=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Subspace\n\nSo like I previously mentioned, we don't want each decision tree to be trained using same data. So for each tree there will be a parameter to control which features can be used to partition the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_potential_splits(data, random_subspace):\n    \n    potential_splits = {}\n    _, n_columns = data.shape\n    \n    col_index = list(range(n_columns-1))\n    if random_subspace and random_subspace <= len(col_index):\n        col_index = random.sample(population=col_index, k=random_subspace)\n    \n    for column_index in col_index:  # excluding the last column which is the label\n        values = data[:, column_index]\n        unique_values = np.unique(values)\n        \n        potential_splits[column_index] = unique_values\n    \n    return potential_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_potential_splits(train_data.values, random_subspace=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nFinally, we can write our random forest algorithm. The actual algorithm is actually not very sophisticated. For each tree in the forest, we are randomly picking a subset of train data and ask each decision tree to partition the data using random subspace. Then our forest is created!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def  random_forest_algorithm(train_data, n_trees, n_bootstrap, n_features, max_depth):\n    forest = []\n    for i in range(n_trees):\n        boot_data = bootstrap(train_data, n_bootstrap)\n        tree = decision_tree_algorithm(boot_data, max_depth=max_depth, random_subspace=n_features)\n        forest.append(tree)\n        \n    return forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = random_forest_algorithm(train_data, n_trees=4, n_bootstrap=800, n_features=4, max_depth=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(forest[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(forest[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(forest[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(forest[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how all the decision trees are different!"},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nNow I am implementating a classifier, so our prediction will be voting-based, which means I will just use the mode of all results from all decision trees to construct my final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_forest_prediction(test_data, forest):\n    pred_data = {}\n    for i in range(len(forest)):\n        col = \"tree...{}\".format(i)\n        predictions = decision_tree_predictions(test_data, tree=forest[i])\n        pred_data[col] = predictions\n    \n    pred_data = pd.DataFrame(pred_data)\n    \n    return pred_data.mode(axis=1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = random_forest_prediction(test_data, forest)\n\naccuracy = calculate_accuracy(predictions, test_data.label)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=4, criterion='entropy', max_depth=3, max_samples=800, random_state=21)\nrf_model.fit(train_data.drop('label', axis=1), train_data['label'])\n\ny_pred = rf_model.predict(test_data.drop('label', axis=1))\naccuracy = calculate_accuracy(y_pred, test_data.label)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the final accuracy shows that our random forest classifier is pretty close to scikit-learn's random forest classifier, which would show the effectiveness of our implementation."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}