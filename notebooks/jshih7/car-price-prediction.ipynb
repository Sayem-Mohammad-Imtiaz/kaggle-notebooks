{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"19dce43b-4a72-eeec-3cb5-39fc0df6d633"},"source":"## Car Price Prediction ##\n\n*Akshay Navada & Jeffrey Shih*\n\n*ECE464 Statistical Learning*\n\n*Professor Sam Keene*\n\n----------\n\n##Data Collection##\n\n**Car Features**\n\nThe car feature data was collected using the Edmunds car API. A script was written in Node.js using to scrape car details. Edmunds API contains various vehicle specs, consumer and critic ratings, and prices. We obtained 11,915 car samples. Some of the cars were used, and some were new. New cars used their average MSRP as the price, while old cars used their TMV value as the price.\n\n**Popularity**\n\nFirst we tried using the Google Trends API, since we thought if a user wanted to buy a car, they would definitely google it on the web. However this API only gave us relative popularity scores out of a hundred. So comparing several thousand cars to each other was difficult. Furthermore, the Google Trends API had a query limit of 200 queries per day and 5 per hour.\n\nWe decided to switch the way we obtained popularity to using the Twitter streaming API. The streaming API obtains current tweets related to a search query. The streaming API was run for around 4 hours and we obtained 54,000 separate tweets. Our original plan was to use the Car Model and Make as the search queries, but with thousands of different Car Models and makes, the twitter streaming API was very slow to obtain data. We let it run for a day, and it only obtained a few hundred tweets. Hence we used the car brands as the search queries for these tweets.\n\n----------\n\n##Algorithms##\n\nThe classification script is written in Python, making use of the Jupyter Notebook to display graphs. We used scikit-learn’s classification functions.\n\nWe use a random forest classifier with roughly 500 to 1000 estimators, aka the number of trees in the forest. The number of features to consider for the best split is the square root of the number of features, which is the automatic setting.\n\nWe attempted to try a gradient boosting classifier, but this took too long to finish so we omitted it for the time being.\n\n----------\n\n##Results##\n\n**Popularity**\n\nPopularity was scraped and put into JSON file, so it was a trivial matter to load this into Python and plot a bar graph. The top ten most popular cars are as follows, from most to least: Ford, BMW, Audi, Ferrari, Honda, Toyota, Nissan, Dodge, Kia, and Porsche.\n\n**Important Features**\n\nThe most important features in determining the price of cars are found to be the engine horsepower, the engine fuel type, and the engine cylinder, which is sensical since they determine how well a car runs. Model types, the make, and the year come after; these have more to do with the brand and superficial appeal of the car, but are still deciding factors nonetheless.\n\n**Most Overpriced Cars and Brands**\n\nWe hoped to do a more in-depth analysis of car prices and the effects of branding. We have seen that a car’s make and model type may affect its price quite significantly. In order to determine is a car is overpriced or not, we ran the classifier without these features. The most overpriced cars are found to be the ones with the greatest difference between its projected price (by the algorithm) and its actual price (the testing values). In addition to individual models, we took a look at the most overpriced brands.\n\nThe results do not disappoint; indeed it seems that the most overpriced brands are those that sell ridiculously priced cars that everyone wants to buy but don’t have the money for it (also the cars that come up the most in mainstream music)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"559d4431-5db1-5369-c23e-23311c9d3149"},"outputs":[],"source":"%matplotlib inline\n\nimport sklearn\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, make_scorer, mean_squared_error\n\n# If true, include make and model in Random Forest\n# Shows how much make and model come into play, but when when we calculate prices\n# we should omit to see if the models are overpriced\nincludeMakeAndModel = True\n\n# Number of trees in forest\nnEstimators = 500\n\ndef GetDataMatrix():\n    \n    # Data frame with make and model\n    Xmodelmake = pd.read_csv(\"../input/data.csv\",header=0, usecols=(0,1,2,3,4,5,6,7,8,9,10,11,13,14,));\n    \n    # Excluding make and model\n    if not includeMakeAndModel:\n        X = pd.read_csv(\"../input/data.csv\",header=0, usecols=(2,3,4,5,6,7,8,9,10,11,13,14,));\n    else:\n        X = Xmodelmake\n    Y = pd.read_csv(\"../input/data.csv\",header=0, usecols=(15,));\n\n    X, Y, Xmodelmake = shuffle(X, Y, Xmodelmake)\n    Xmake = Xmodelmake['Make']\n    Xmodel = Xmodelmake['Model']\n    \n    # Turns categorical data into binary values across many columns\n    if not includeMakeAndModel:\n        X = pd.get_dummies(X, dummy_na = False, columns=['Engine Fuel Type', 'Transmission Type', 'Driven_Wheels', 'Market Category', 'Vehicle Size', 'Vehicle Style'] );\n    else:    \n        X = pd.get_dummies(X, dummy_na = False, columns=['Make', 'Model', 'Engine Fuel Type', 'Transmission Type', 'Driven_Wheels', 'Market Category', 'Vehicle Size', 'Vehicle Style'] );\n    \n    X.insert(0, 'ModelRef', Xmodel);\n    X.insert(0, 'MakeRef', Xmake);\n    \n    # Fill the null values with zeros\n    X.fillna(0, inplace=True);\n    return (X, Y, Xmodelmake)\n\n##########\n\n(X, Y, Xmodelmake) = GetDataMatrix() #Gets the X,Y\n\n# Turn into a proper one D arrayY = numpy.ravel(Y);\nY_unraveled = np.ravel(Y);\n\n# Split dataset into training and testing\nprint('Splitting into training and testing...')\nX_train, X_test, Y_train, y_test = train_test_split(X, Y_unraveled, test_size=0.10, random_state=32)\nMSE_Scorer = make_scorer(mean_squared_error);\n\n# Model/Make columns are only used later on to relate indices to Model/Makes\nX_train2 = X_train.drop('MakeRef', axis = 1).drop('ModelRef', axis = 1)\nX_test2 = X_test.drop('MakeRef', axis = 1).drop('ModelRef', axis = 1)\n\n# Train using Random Forest\nprint('Training classifier...')\nclf = RandomForestRegressor(n_estimators=nEstimators, max_features=\"sqrt\");\n# The gradient boosting classifier didnt finish running\n# clf = GradientBoostingClassifier(n_estimators=5)\nclf = clf.fit(X_train2, Y_train);\nprint(\"Done training best classifier.\")\n\nprint('Calculating error...')\ny_pred = clf.predict(X_test2);\nscores = cross_val_score(clf,X_test2,y_test, cv = 5)\nprint()\n\nprint(\"Scores:\")\nprint(scores);\nprint(\"Mean absolute error:\");\nmean_error = sum(abs(y_test-y_pred))/len(y_test);\nprint(mean_error);\nprint(\"Mean percent error: \")\nprint(mean_error/np.mean(y_test))\nprint()\n\nprint(\"ypred:\");\nprint(y_test);\nprint(y_pred);\nnp.savetxt(\"ypred_test.csv\",(y_pred,y_test),delimiter=\",\");\nprint()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a66ecbe-e908-5b65-e149-b84fdf8f5706"},"outputs":[],"source":"# If we used JSON file, this would've been easier\n# This code is trying to get the data off Kaggle and making the make and popularities unique\n# since the data can list them multiple times\n\n# Make elements in cars unique and return in same order\ncars = np.asarray(Xmodelmake['Make'])\nuniquecarindices = np.unique(cars, return_index=True)[1]\ncars = np.asarray([cars[index] for index in sorted(uniquecarindices)])\n\n# Make elements in popularities unique and return in same order\npopularities = np.asarray(Xmodelmake['Popularity'])\nuniquepopularityindices = np.unique(popularities, return_index=True)[1]\npopularities = np.asarray([popularities[index] for index in sorted(uniquepopularityindices)])\n\n# Get the indices sorted on popularities from highest to lowest\npopindices = np.argsort(popularities)[::-1]\n\n# Data range\ntotalN = popindices.shape[0]\n\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Car Popularities\")\nplt.bar(range(totalN), popularities[popindices], color=\"b\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(totalN), cars[popindices])\nplt.xlim([-1, totalN])\nplt.xlabel('Car Models')\nplt.ylabel('Popularity')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0635e2b0-bd1a-4ba3-0250-639353ef7827"},"outputs":[],"source":"# Important questions to answer\n\n# 1. What features most predict price?\n\n# Get the importances and calculate standard deviations for each\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Get the feature names\nfeatures = X_test2.columns.values\n\n# Want the top 20 features, so limit the indices and labels\ntopLimit = 20 # limit to show up to, ex. top 10\nindices = indices[0: topLimit] # indices for features\ntopLabels = features[indices[0: topLimit]] # actual feature labels, we want to print these\n\n# Plot the feature importances of the forest (top 20)\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 20 Important Features\")\nax = plt.bar(range(topLimit), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(topLimit), topLabels)\nplt.xlim([-1, topLimit])\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64f57efd-7030-215c-7a4f-c3d801aec224"},"outputs":[],"source":"# 2. What cars are the most over-priced for their feature set?\n\n# Get the errors from the prediction and sort from greatest to least\ny_error = y_test-y_pred\nold_indices = np.argsort(y_error)[::-1] # returns the old indices\n\n# Put top 10 overpriced cars into a list\nmodelmakelist = []\nN = 10 # number of top values to extract\nfor i in range(N):\n    modelmakelist.append(X_test['MakeRef'].iloc[old_indices[i]]\n                         + ' ' + X_test['ModelRef'].iloc[old_indices[i]]\n                         + ' ' + str(X['Year'].iloc[old_indices[i]]))\nmodelmakelist = np.asarray(modelmakelist) # don't index into original\n\n# Plot the top 10 overpriced cars against their price\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 10 Overpriced Cars\")\nplt.bar(range(N), y_error[old_indices[0:N]], color=\"b\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(N), modelmakelist)\nplt.xlim([-1, N])\nplt.xlabel('Car Make and Model')\nplt.ylabel('Price')\nplt.show()\n\n# Put top 10 overpriced brands into a list\n# Scan all entries, if maker already exists, go to next entry, else add maker to list\nexistingmakers = []\npricelist = []\nfor i in range(old_indices.shape[0]):\n    currentmaker = X_test['MakeRef'].iloc[old_indices[i]]\n    if currentmaker not in existingmakers:\n        existingmakers.append(currentmaker)\n        pricelist.append(y_error[old_indices[i]])\n        if len(existingmakers) == N:\n            break\n\nexistingmakers = np.asarray(existingmakers)\npricelist = np.asarray(pricelist)\n    \nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 10 Overpriced Car Brands:\")\nplt.bar(range(N), pricelist, color=\"g\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(N), existingmakers)\nplt.xlim([-1, N])\nplt.xlabel('Car Brand')\nplt.ylabel('Price')\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}