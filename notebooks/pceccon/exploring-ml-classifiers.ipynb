{"cells":[{"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nspam_data = pd.read_csv('../input/spam.csv', encoding='latin-1')\nspam_data.head()","cell_type":"code","metadata":{"_uuid":"c6f1f04d8f9aaf33a5732631373432832dea40da","_kg_hide-input":false,"_kg_hide-output":false,"_cell_guid":"6e172e2f-1c57-40ab-9271-e563acce29ac"},"execution_count":null},{"outputs":[],"source":"spam_data = spam_data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1)\nspam_data = spam_data.rename(columns = {'v1': 'target','v2': 'text'})\n\nspam_data.head()","cell_type":"code","metadata":{"_uuid":"1c9af275eeb789decb4a40015afdf75ce6dcb8ed","_cell_guid":"e4eea5b1-c0f0-4f4d-b9eb-876f127327d6"},"execution_count":null},{"source":"### Pre-processing\nBefore training any model, let's do more data pre-processing. Specifically, I'm goint to:\n\n- Remove stop words\n- Put all phrases in lower case\n- Stem words","cell_type":"markdown","metadata":{"_uuid":"0ceaad663c1e0e04fc78df7097c4e515d740af6b","_cell_guid":"a688a88b-edef-4922-8352-a7bfe2241b84"}},{"outputs":[],"source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer","cell_type":"code","metadata":{"_uuid":"8b340e6ee2c14fdd5d4c450044c1e995403c7b1d","collapsed":true,"_cell_guid":"995da297-1ab3-497b-8baf-aaeaaf3d95dc"},"execution_count":null},{"outputs":[],"source":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer('english')","cell_type":"code","metadata":{"_uuid":"81e184b1ddad6e2788944fe12364ff12e10dfb92","collapsed":true,"_cell_guid":"fcfcf35d-9c64-4995-8688-70ba967dc8d3"},"execution_count":null},{"outputs":[],"source":"spam_data['parsed'] = spam_data['text'].apply(lambda x: x.lower())\nspam_data['parsed'] = spam_data['text'].apply(lambda x: word_tokenize(x))\nspam_data['parsed'] = spam_data['text'].apply(lambda x: [word for word in str(x).split() if word not in stop_words])\nspam_data['parsed'] = spam_data['parsed'].apply(lambda x: [stemmer.stem(word) for word in x])\nspam_data['parsed'] = spam_data['parsed'].apply(lambda x: ' '.join(x))","cell_type":"code","metadata":{"_uuid":"338d63c2a41affa5436ab14f3e701ef52a0f871a","collapsed":true,"_cell_guid":"8076f290-3891-458a-9d9f-11d42d306c0c"},"execution_count":null},{"outputs":[],"source":"spam_data.head()","cell_type":"code","metadata":{"_uuid":"bae6274540c843a2fc47bc460736c1ca57297af5","_cell_guid":"14dfc1c7-8542-4492-b72b-998c0dde748d"},"execution_count":null},{"source":"### Data Exploration","cell_type":"markdown","metadata":{"_uuid":"01aa3778077a39dae55f861ba441b7c25e75d532","_cell_guid":"f015e10c-1451-42b4-bc70-b815c5383e06"}},{"outputs":[],"source":"s = spam_data['target'].value_counts()\nsns.barplot(x=s.values, y=s.index)\nplt.title('Data Distribution')","cell_type":"code","metadata":{"_uuid":"766e0107d9b9d1ed3c82568caa70377ef0859b14","_cell_guid":"24aa5d55-40fe-4f8a-9e78-f114f6b9f274"},"execution_count":null},{"outputs":[],"source":"s1 = spam_data[spam_data['target'] == 'ham']['parsed'].str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['parsed'].str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Lenght Distribution')\nplt.legend()","cell_type":"code","metadata":{"_uuid":"d03942a52d58ec774047285d8dbd170754339144","_cell_guid":"22edc0d7-f0c5-41cd-a23c-8cb20211c4ef"},"execution_count":null},{"source":"We can notice that spams messages are often longer than ham messages.","cell_type":"markdown","metadata":{"_uuid":"efac65ccdf6ca81ef17e0e4187bd7f1f6543b987","_cell_guid":"9239f971-3a4e-4cc6-93ff-70b8a61cb456"}},{"outputs":[],"source":"s1 = spam_data[spam_data['target'] == 'ham']['parsed'].str.replace(r'\\D+', '').str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['parsed'].str.replace(r'\\D+', '').str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Digits Distribution')\nplt.legend()","cell_type":"code","metadata":{"_uuid":"bd966766ec79b5021ea3bd3aaddb20932a95d1d4","_cell_guid":"23c58e6e-7ad8-46a2-9cb4-1da7e72b035b"},"execution_count":null},{"source":"From this plot, it's clear that the digits distribution in ham messages are rigth skewed, presenting lower mean of digits than spam messages.","cell_type":"markdown","metadata":{"_uuid":"7c547592560141991198cc7f1a8a9078e3f1793e","_cell_guid":"665e5677-fe29-480c-a50c-7f9b1ab2f6b1"}},{"outputs":[],"source":"s1 = spam_data[spam_data['target'] == 'ham']['parsed'].str.replace(r'\\w+', '').str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['parsed'].str.replace(r'\\w+', '').str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Non-Digits Distribution')\nplt.legend()","cell_type":"code","metadata":{"_uuid":"5ec01218d727436e27def366d1072dc11568db0d","_cell_guid":"15abba02-732a-4212-be5e-10a9d127623c"},"execution_count":null},{"source":"These distributions resembles the one regarding the text messages length. Here, the values are smaller, though. Hams present less non-digits than spams.","cell_type":"markdown","metadata":{"_uuid":"a52ebe95adfbaca9815470f32bda4dd6027c73ef","_cell_guid":"90b2014a-9234-4b7b-b21a-793f240c09fc"}},{"outputs":[],"source":"spam_data.groupby('target').describe()","cell_type":"code","metadata":{"_uuid":"c5451dd6ede582ae0513b7f77ac03e6b85580e42","_cell_guid":"97198805-290e-41d3-aa17-e4c794ccf6c3"},"execution_count":null},{"source":"### Count Vectorizer vs. Tfidf","cell_type":"markdown","metadata":{"_uuid":"56e2509a46d17a4034781e055f0feb9ac5310a74","_cell_guid":"b1226fe9-901a-4922-8726-9ac41b04ad8e"}},{"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(spam_data['parsed'], \n                                                    spam_data['target'], \n                                                    random_state=0)","cell_type":"code","metadata":{"_uuid":"aa3995dd35bc4ca9d324edde5aeef6331a5947fc","collapsed":true,"_cell_guid":"30c3040a-8c8c-4daf-b0f7-3121199367b3"},"execution_count":null},{"source":"- Count Vectorizer","cell_type":"markdown","metadata":{"_uuid":"91f027c5a2c34251fa71a616368a287508978c77","_cell_guid":"0f4c477b-6b30-4fd1-b60a-f70103e27971"}},{"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer().fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)","cell_type":"code","metadata":{"_uuid":"491f8d7a13cf491b20c2b62b84f3319ab730023d","_cell_guid":"51fa9953-e0e8-460a-8f66-da724153ff57"},"execution_count":null},{"outputs":[],"source":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)","cell_type":"code","metadata":{"_uuid":"a9a5e2c4ea5fe6c4c978211c2968159d6e94f709","_cell_guid":"c701ca45-907a-47bf-bcd7-bb6d259a6145"},"execution_count":null},{"outputs":[],"source":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","cell_type":"code","metadata":{"_uuid":"b4d9fdf9620fd9405318ec32bf8774d29ecc9f3d","_cell_guid":"aa4929bc-0f25-4426-b664-9627bf83f4aa"},"execution_count":null},{"outputs":[],"source":"from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(vect.transform(X_test))\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))","cell_type":"code","metadata":{"_uuid":"79bef5caa97913522cbbdb1966130b9772ecec59","_cell_guid":"2b2dc0d1-0cbb-4429-97ce-7cfc8c0bde11"},"execution_count":null},{"source":"- Tfidf\n\nLet's ignore terms that have a document frequency strictly lower than 3.","cell_type":"markdown","metadata":{"_uuid":"b3c1d239ef6215158342496dcab281bb4c7cc676","_cell_guid":"67392082-4986-4027-88a0-5721ae6d80cd"}},{"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(min_df=3).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)","cell_type":"code","metadata":{"_uuid":"5d49cf682cb5a63a53ef3a8fee193ed73f73b948","_cell_guid":"bb1a6fb2-4c1e-40cf-aafb-99a81f0cad14"},"execution_count":null},{"outputs":[],"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)","cell_type":"code","metadata":{"_uuid":"dcbd97ea7b2f5490516de3c79113e0a2d5334e89","_cell_guid":"e45580e8-b185-4e02-b763-229f8117e7d9"},"execution_count":null},{"outputs":[],"source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","cell_type":"code","metadata":{"_uuid":"93cf07ec996b5d7ae0a04540aafe93d000b6d7a3","_cell_guid":"ae11537e-1df9-491d-a9b3-5dd1e47c0f69"},"execution_count":null},{"outputs":[],"source":"from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(vect.transform(X_test))\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))","cell_type":"code","metadata":{"_uuid":"a0602c49033112c13b4a3e08ad7b16317b132a06","_cell_guid":"dbb21a86-b942-475a-b2f7-095a336f1339"},"execution_count":null},{"source":"### Feature Engineering\n\n\nIn the follwing, let's do some feature engineering to try to improve the performance of our model.","cell_type":"markdown","metadata":{"_uuid":"dfd4fedc7f7b6391edb6f76b0a45a4ff99fa2c1d","_cell_guid":"b275c171-5ef0-4759-be07-db0dedc3b340"}},{"outputs":[],"source":"def add_feature(X, feature_to_add):\n    \"\"\"\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    \"\"\"\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","cell_type":"code","metadata":{"_uuid":"2cb5f8bd78396381677a4a343b5da6368c4eb769","collapsed":true,"_cell_guid":"7f6db999-d38b-4f6a-b10e-d36db77ae70f"},"execution_count":null},{"source":"#### First Model\n\nFirst, let's ignore terms that have a document frequency strictly lower than 3. Using this document-term matrix and an additional feature, the length of document (number of characters), we will test how our Tfidf performs.","cell_type":"markdown","metadata":{"_uuid":"9f0d9ae2b6c57d3d31d84b29eb4bbc524e5b47b0","_cell_guid":"5be64a2c-29ad-4e88-a126-c53683b9f9df"}},{"outputs":[],"source":"vect = TfidfVectorizer(min_df=5).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())","cell_type":"code","metadata":{"_uuid":"229165020dc89f2fd9403a45481988786a79d42a","_cell_guid":"0111edce-e9c9-4685-a6c7-af76f65a4fe7"},"execution_count":null},{"outputs":[],"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)","cell_type":"code","metadata":{"_uuid":"760539d877b187a5f5533f993d24a9abbe115103","_cell_guid":"8f111701-e01d-41a1-b450-0fc4f3894655"},"execution_count":null},{"outputs":[],"source":"index = np.array(vect.get_feature_names() + ['length_of_doc'])\nvalues  = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))","cell_type":"code","metadata":{"_uuid":"d3c95e5e687a796f61c3e0ebe5f54d50f3ea0ae5","_cell_guid":"7b8e8f2b-898a-4131-981a-47e1cb8110ba"},"execution_count":null},{"outputs":[],"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))","cell_type":"code","metadata":{"_uuid":"1f3dd030984b313beb15dbc8255aa1508dd9a3f9","_cell_guid":"9c7a105c-7dde-47e3-ab3d-114754d6eb3d"},"execution_count":null},{"source":"#### Second Model\n\nNow let's use a Tfidf ignoring terms that have a document frequency strictly lower than 5 and using word n-grams from n=1 to n=3 (unigrams, bigrams and trigrams).\n\nWe will also make use of the following additional features:\n- the length of document (number of characters)\n- number of digits per document","cell_type":"markdown","metadata":{"_uuid":"94216e2978f45198cf468fff5c1aecd897e2db8c","_cell_guid":"e9deef49-5d14-4b34-9fb6-5cb44e69f0e8"}},{"outputs":[],"source":"vect = TfidfVectorizer(min_df=5, ngram_range=(1, 3)).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\D+', '').str.len())","cell_type":"code","metadata":{"_uuid":"03ba998706676c9bd531d522cb19862b7f032618","_cell_guid":"08f4b015-e12b-407a-9453-79fbd54c1917"},"execution_count":null},{"outputs":[],"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)","cell_type":"code","metadata":{"_uuid":"e0a4bb02ea48542c14d6f779591bba74047dc0b9","_cell_guid":"fe3ca7f6-3dd4-4298-8aeb-c40981774a68"},"execution_count":null},{"outputs":[],"source":"index = np.array(vect.get_feature_names() + ['length_of_doc', 'digit_count'])\nvalues  = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))","cell_type":"code","metadata":{"_uuid":"2f65c37e837db91ac9f5cc7c8f8cab111eccec41","_cell_guid":"97e97897-472a-4a52-8603-a99dcc64199e"},"execution_count":null},{"outputs":[],"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\D+', '').str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))","cell_type":"code","metadata":{"_uuid":"76b25684504bb18d1044f20ddf5361793174ee37","_cell_guid":"efbf5571-6c31-4554-ba3c-274971a90398"},"execution_count":null},{"source":"#### Third Model\nFinally, let's use a Count Vectorizer ignoring terms that have a document frequency strictly lower than 5 and using character n-grams from n=2 to n=5.\nTo tell Count Vectorizer to use character n-grams we pass in analyzer='char_wb' which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n\nAt this time we are going to use these additional features:\n- the length of document (number of characters)\n- number of digits per document\n- number of non-word characters (anything other than a letter, digit or underscore.)","cell_type":"markdown","metadata":{"_uuid":"375db655da84db1141d85bb3fc89eef6429b50e5","_cell_guid":"8829f43e-6cfd-46b6-929b-49f4f55e8329"}},{"outputs":[],"source":"vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\D+', '').str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\w+', '').str.len())","cell_type":"code","metadata":{"_uuid":"dfa15a1bf18d8616f0ee793abe5d18915b019be7","_cell_guid":"478256e2-a13d-4799-8a53-1e2c0fc10bd6"},"execution_count":null},{"outputs":[],"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)","cell_type":"code","metadata":{"_uuid":"2747f8a6f056fb8d449e61b86a19bbcdc3a26e89","_cell_guid":"b22729b0-77fe-4e45-a7c5-850d34c940a6"},"execution_count":null},{"outputs":[],"source":"index = np.array(vect.get_feature_names() + ['length_of_doc', 'digit_count', 'non_word_char_count'])\nvalues = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))","cell_type":"code","metadata":{"_uuid":"bdb476dccfad3c1c850ff7f4594ce497d23c4933","_cell_guid":"03de3036-e518-4dad-ac1f-c180f18c9d10"},"execution_count":null},{"outputs":[],"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\D+', '').str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\w+', '').str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))","cell_type":"code","metadata":{"_uuid":"72b5bcc7fdac3073cca0a89431978a318a43ea61","_cell_guid":"217b1f07-aaf6-47d9-b302-06ce8802226e"},"execution_count":null}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python"}},"nbformat":4}