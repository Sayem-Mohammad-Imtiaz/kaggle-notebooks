{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression & XGB & LGBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This kernel is based on the my kernel \"[Titanic (0.83253) - Comparison 20 popular models](https://www.kaggle.com/vbmokin/titanic-0-83253-comparison-20-popular-models)\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries ","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nimport pandas_profiling as pp\n\n# models\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Download datasets <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. EDA <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This code is based on my kernel \"[FE & EDA with Pandas Profiling](https://www.kaggle.com/vbmokin/fe-eda-with-pandas-profiling)\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Encoding categorical features <a class=\"anchor\" id=\"4.1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_name = 'class'\ndata_target = data[target_name]\ndata = data.drop([target_name], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I split data on 30% in the test dataset, the remaining 70% - in the training dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, target, target_test = train_test_split(data, data_target, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Creation of training and validation sets <a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Tuning models and test for all features <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- Support Vector Machines and Linear SVC\n- KNN or k-Nearest Neighbors\n- Naive Bayes Classifier or Gaussian Naive Bayes\n- Stochastic Gradient Descent, GradientBoostingClassifier, RidgeClassifier, BaggingClassifier\n- Decision Tree Classifier, Random Forest, XGB Classifier, LGBM Classifier, ExtraTreesClassifier\n- Perceptron, Neural Networks with different archictures (Deep Learning)\n- VotingClassifier (hard or soft voting)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Logistic Regression <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/startupsci/titanic-data-science-solutions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n\nNote the confidence score generated by the model based on our training dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nacc_log = round(logreg.score(train, target) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_test_log = round(logreg.score(test, target_test) * 100, 2)\nacc_test_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 XGB Classifier <a class=\"anchor\" id=\"5.10\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBoost is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will tuning the hyperparameters of the XGBClassifier model using the HyperOpt and 10-fold crossvalidation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = space_eval(space_xgb, best)\nparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nacc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nacc_XGB_Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_test_XGB_Classifier = round(XGB_Classifier.score(test, target_test) * 100, 2)\nacc_test_XGB_Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\nplt.show();\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 LGBM Classifier <a class=\"anchor\" id=\"5.11\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Reference [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will tuning the hyperparameters of the LGBMClassifier model using the HyperOpt and 10-fold crossvalidation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(2, 2**11, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = space_eval(space_lgb, best)\nparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nacc_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nacc_LGB_Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_test_LGB_Classifier = round(LGB_Classifier.score(test, target_test) * 100, 2)\nacc_test_LGB_Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(LGB_Classifier,ax = axes,height = 0.5)\nplt.show();\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Models evaluation <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can now rank our evaluation of all the models to choose the best one for our problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'XGBClassifier', 'LGBMClassifier',],\n    \n    'Score_train': [acc_log, acc_XGB_Classifier, acc_LGB_Classifier],\n    'Score_test': [acc_test_log, acc_test_XGB_Classifier, acc_test_LGB_Classifier]\n                    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.sort_values(by=['Score_train', 'Score_test'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.sort_values(by=['Score_test', 'Score_train'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models['Score_diff'] = abs(models['Score_train'] - models['Score_test'])\nmodels.sort_values(by=['Score_diff'], ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\nplt.figure(figsize=[23,7])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score_train'], label = 'Score_train')\nplt.plot(xx, models['Score_test'], label = 'Score_test')\nplt.legend()\nplt.title('Score of 3 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}