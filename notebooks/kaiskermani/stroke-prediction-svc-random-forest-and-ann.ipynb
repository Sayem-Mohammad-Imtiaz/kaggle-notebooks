{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook we will go through the data of the \"Stroke prediction\" dataset. First we'll explore the data and visualize it, then filling the missing values using KNN. \nLast we'll use ANN to predict the stroke of a given individual.","metadata":{}},{"cell_type":"markdown","source":"Walkthrough the notebook:\n1. <a href=\"#eda\">Exploratory Data Analysis</a>\n2. <a href=\"#preproc\">Preprocessing the data</a>\n    * <a href=\"#format\">Formatting the features</a>\n    * <a href=\"#fillna\">Filling the missing values</a>\n    * <a href=\"#datanorm\">Data Normalization</a>\n    * <a href=\"#train_dev_split\">Train dev splits</a>\n3. <a href=\"#SVC\">Classification using SVC</a>\n4. <a href=\"#randForest\">Classification using Random Forest</a>\n5. <a href=\"#ANN\">Classification using ANN</a>","metadata":{}},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis: <a id=\"eda\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random as rd\n\ndata = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\nprint(\"Shape:\", data.shape)\nprint(\"Features:\", list(data.columns))\nprint()\nprint(\"Data Describe:\")\nprint(data.iloc[:, 0:5].describe())\nprint(data.iloc[:, 5:12].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up graphics and color palette\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 9, 7\n\nsns.set_context('notebook')\nsns.set_style('whitegrid')\npal = sns.color_palette('Set2')\nsns.set_palette(pal)\n\nimport warnings  \nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for null values:","metadata":{}},{"cell_type":"code","source":"print(data.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of the features:","metadata":{}},{"cell_type":"code","source":"sns.histplot(data['stroke'], bins=2)\nplt.show()\nimbalance_ratio = sum(data['stroke']==1)/sum(data['stroke']==0)\nprint(\"Ratio stroke/no_stroke:\")\nprint(imbalance_ratio)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20, 7\nfig, axes = plt.subplots(1, 2)\nsns.histplot(data['age'], bins=8, kde= True, ax=axes[0])\nsns.histplot(data['bmi'], bins=10, kde=True, ax=axes[1])\nplt.suptitle('Continuous data distribution')\nplt.show()\nrcParams['figure.figsize'] = 9, 7\nsns.histplot(data['avg_glucose_level'], kde=True, bins=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = ['gender', 'work_type', 'Residence_type', 'smoking_status']\nrcParams['figure.figsize'] = 20, 15\nfig, axes = plt.subplots(2, 2)\nfor i in range(0, len(cat_features)):\n    sns.histplot(data[cat_features[i]], ax=axes[int(i/2), i%2])\nplt.suptitle('Categorical data distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_features = ['hypertension', 'heart_disease', 'ever_married', 'stroke']\ndata['ever_married'] = [int(b) for b in data['ever_married'] == 'Yes']\nrcParams['figure.figsize'] = 20, 15\nfig, axes = plt.subplots(2, 2)\nfor i in range(0, len(bin_features)):\n    sns.histplot(data[bin_features[i]], bins=2, ax=axes[int(i/2), i%2])\nplt.suptitle('Binary data distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 15,11\ncorr_mat = data.drop(['id'], axis=1).corr()\nsns.heatmap(corr_mat, vmin=-1, vmax=1, cmap=sns.diverging_palette(360, 180, as_cmap=True))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Preprocessing the data: <a id=\"preproc\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Formatting the features: <a id=\"format\"></a>","metadata":{}},{"cell_type":"code","source":"smoke_to_int = {\n    'never smoked': 0,\n    'formerly smoked': 1,\n    'smokes': 2,\n    'Unknown': -1\n}\ndata['smoking_status'] = [smoke_to_int[s] for s in data['smoking_status']]\nprint(data['smoking_status'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"work_to_int = {\n    'Private': 1,\n    'Self-employed': 2,\n    'Govt_job': 2,\n    'children': 4,\n    'Never_worked': 0\n}\ndata['work_cat'] = [work_to_int[s] for s in data['work_type']]\nprint(data['work_cat'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['gender_female'] = [int(m) for m in data['gender'] == 'Female']\ndata['residence_urban'] = [int(m) for m in data['Residence_type'] == 'Urban']\nprint(data['residence_urban'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\n    'id', 'gender_female', 'age', 'hypertension', \n    'heart_disease', 'ever_married', 'work_cat',\n    'residence_urban', 'avg_glucose_level', 'bmi', \n    'smoking_status', 'stroke'\n]\n\ndf = data[features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filing in missing values (KNN algorithm):<a id=\"fillna\"></a>\n\n### 1) BMI","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\ntrain = df[df['bmi'].isna()==False]\npred = df[df['bmi'].isna()]\n\nbmi_regressor = KNeighborsRegressor(n_neighbors=5)\nX = train.drop(['bmi'], axis=1)\ny = train['bmi']\nbmi_regressor.fit(X, y)\ny_hat = bmi_regressor.predict(pred.drop(['bmi'], axis=1))\npred.loc[:, 'bmi'] = y_hat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[:, 'cat'] = 'Train'\npred.loc[:, 'cat'] = 'Pred'\n\nfig, axes= plt.subplots(2,1, sharex=True)\nsns.histplot(train, x='bmi', stat='probability', hue='cat', bins=15, ax=axes[0], kde=True)\nsns.histplot(pred, x='bmi', stat='probability', hue='cat', bins=15, ax=axes[1], kde=True)\nplt.show()\n\ndf = pd.concat([train, pred])\ndf.drop(['cat'], axis=1, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2) Smoking status:","metadata":{}},{"cell_type":"markdown","source":"Since there's an important ratio of people with the smoking status 'Unknown', we'll apply KNN in order to fill their smoking status:","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\ntrain = df[df['smoking_status']!=-1]\npred = df[df['smoking_status']==-1]\n\nsmoker_classifier = KNeighborsClassifier(n_neighbors=5)\nX = train.drop(['smoking_status'], axis=1)\ny = train.loc[:, 'smoking_status']\nsmoker_classifier.fit(X, y)\ny_hat = smoker_classifier.predict(pred.drop(['smoking_status'], axis=1))\npred.loc[:, 'smoking_status'] = y_hat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[:, 'cat'] = 'Train'\npred.loc[:, 'cat'] = 'Pred'\n\nfig, axes= plt.subplots(2,1, sharex=True)\nsns.histplot(train, x='smoking_status', stat='probability', hue='cat', bins=3, ax=axes[0])\nsns.histplot(pred, x='smoking_status', stat='probability', hue='cat', bins=3, ax=axes[1])\nplt.show()\n\ndf = pd.concat([train, pred])\ndf.drop(['cat'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Normalization: <a id=\"datanorm\"></a>","metadata":{}},{"cell_type":"markdown","source":"Very important step!","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = np.array(df.drop(['stroke', 'id'], axis=1))\ny = np.array(df.loc[:, 'stroke'])\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\npd.DataFrame(X).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Dev Splits: <a id=\"train_dev_split\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nprint(\"# of samples: \" + str(y.shape[0]))\n\n# Splitting data into train (80%) CV (10%) test(10%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, stratify = y, random_state = 42)\ny_train = y_train.astype(np.float32).reshape((-1,1))\ny_test = y_test.astype(np.float32).reshape((-1,1))\n\n# #Transposing the data\n# X_train, X_test = [np.array(x).T for x in [X_train, X_test]]\n# y_train, y_test = [np.array(y).reshape(1, -1) for y in [y_train, y_test]]\n\nprint(\"X_train shape: \" + str(X_train.shape) + \"\\t y_train shape:\" + str(y_train.shape))\nprint(\"X_test shape:  \" + str(X_test.shape) + \"\\t y_test shape: \" + str(y_test.shape))\n\nprint(sum(y_train==1))\nprint(sum(y_test==1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Classification using SVC <a id=\"SVC\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\nsvc = SVC(C=1000, class_weight = {0: 1, 1: 20}, kernel='poly', degree=5)\nsvc.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Added Parameter Tuning:\niterations = 30\nC_range = [int(10**rd.uniform(0, 4)) for i in range(iterations)] # Weights for Regularization param\nweight_range = [int(10**rd.uniform(1, 2.5)) for i in range(iterations)] # Weights for imbalanced data\ndegree_range = [int(rd.uniform(2, 7)) for i in range(iterations)]\n\ncombos = list(zip(C_range, weight_range, degree_range))\nbest_score = 0\nbest_combination = (1, 10, 2)\n\nfor C, weight, degree in combos:\n    svc = SVC(C=C, class_weight = {0: 1, 1: weight}, kernel='poly', degree=degree)\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    current_score = f1_score(y_test, y_pred)\n    print(\"C:\", C, \"\\tWeight:\", weight,\"\\tdegree:\", degree, \"\\tScore:\", current_score)\n    if current_score > best_score:\n        best_score = current_score\n        best_combination = (C, weight, degree)\n\nC, weight, degree = best_combination\nsvc = SVC(C=C, class_weight = {0: 1, 1: weight}, kernel='poly', degree=degree)\nsvc.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = svc.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, y_pred))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train, y_pred), end='\\n\\n')\n\ny_pred = svc.predict(X_test)\nprint('Score on the dev set:')\nprint(classification_report(y_test, y_pred))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test,  y_pred))\nprint('f1 score:', f1_score(y_test,  y_pred), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Cassification using Random Forest: <a id=\"randForest\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                                  class_weight = {0: 1, 1: 100}, min_samples_leaf=6,\n                                  max_features = None)\nrf_model.fit(X_train, np.ravel(y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Added Parameter Tuning:\nweight_range = [int(x) for x in np.logspace(1, 2.5, num=5)] # Weights for imbalanced data\nmin_leaf_range = [int(x) for x in np.linspace(8, 20, num=5)] # \nbest_score = 0\nbest_combination = (10, 5)\n\nfor w, leaf_s in [(x, y) for x in weight_range for y in min_leaf_range]:\n    rf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                              class_weight = {0: 1, 1: w}, min_samples_leaf=leaf_s,\n                              max_features = None)\n    rf_model.fit(X_train, np.ravel(y_train))\n    y_pred = rf_model.predict_proba(X_test)\n    current_score = f1_score(y_test,np.around(y_pred[:, 1]))\n    print(\"Weight:\", w,\"\\tMin leaf sample:\", leaf_s, \"\\tF1-Score:\", current_score)\n    if current_score > best_score:\n        best_score = current_score\n        best_combination = (w, leaf_s)\n\nw, leaf_s = best_combination\nrf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                              class_weight = {0: 1, 1: w}, min_samples_leaf=leaf_s,\n                              max_features = None)\nrf_model.fit(X_train, np.ravel(y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf_model.predict_proba(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred[:, 1])))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred[:, 1]))\nprint('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\ny_pred = rf_model.predict_proba(X_test)\nprint('Score on the dev set:')\nprint(classification_report(y_test, np.around(y_pred[:, 1])))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test, y_pred[:, 1]))\nprint('f1 score:', f1_score(y_test,np.around(y_pred[:, 1])), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Cassification using ANN: <a id=\"ANN\"></a>","metadata":{}},{"cell_type":"markdown","source":"First we'll define a function to visualize the history of our model (performance):","metadata":{}},{"cell_type":"code","source":"def dfify(hist):\n\tdf = pd.DataFrame(hist.history)\n\tdf['epoch'] = df.index\n\tval_cols = [x for x in df.columns if x.startswith('val')]\n\tdf_val = df[val_cols+['epoch']]\n\tdf.drop(columns=val_cols, inplace=True)\n\tdf_val.rename(columns={col: col.split('val_')[-1] for col in df_val.columns}, inplace=True)\n\tdf['phase'] = 'train'\n\tdf_val['phase'] = 'val'\n\treturn pd.concat([df, df_val], ignore_index=True)\n\ndef visu_history(hist):\n    rcParams['figure.figsize'] = 14, 10\n    hist_df = dfify(hist)\n    fig, axes = plt.subplots(2, 2)\n    sns.lineplot(data = hist_df, x='epoch', y='loss', hue='phase', ax=axes[0,0])\n    sns.lineplot(data = hist_df, x='epoch', y='auc', hue='phase', ax=axes[0,1])\n    sns.lineplot(data = hist_df, x='epoch', y='precision', hue='phase', ax=axes[1,0])\n    sns.lineplot(data = hist_df, x='epoch', y='recall', hue='phase', ax=axes[1,1])\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import classification_report\n\ntf.keras.backend.clear_session()\n\n\ndef make_model(optimizer, loss_fn, metrics, output_bias='zeros', dropout=0):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(X_train.shape[-1],)),\n        tf.keras.layers.Dense(10, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(6, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n        tf.keras.layers.Dropout(dropout),\n#         tf.keras.layers.Dense(6, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n#         tf.keras.layers.Dropout(dropout),\n#         tf.keras.layers.Dense(3, activation='tanh', kernel_initializer=tf.keras.initializers.HeNormal),\n#         tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.initializers.GlorotUniform, bias_initializer=output_bias)\n    ])\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=metrics\n    )\n    \n    return model\n\n\nloss_fn = tfa.losses.SigmoidFocalCrossEntropy(from_logits=False)\n# loss_fn = tf.losses.BinaryCrossentropy(from_logits=False)\n\nf1_score_tf = tfa.metrics.F1Score(num_classes=1, average='macro')\npres = tf.keras.metrics.Precision()\nrec = tf.keras.metrics.Recall()\nauc = tf.keras.metrics.AUC()\nmetrics = ['accuracy', pres, rec, f1_score_tf, auc]\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=.1, momentum=1)\n\nmodel = make_model(optimizer, loss_fn, metrics)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initializing the output layer bias:","metadata":{}},{"cell_type":"code","source":"# Initializing The final layer:\noutput_initializer = tf.keras.initializers.Constant(np.log(imbalance_ratio)) # sigmoid(ln(x))=x\nmodel = make_model(optimizer, loss_fn, metrics, output_initializer)\nmodel.save_weights('initial_weights')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = model.evaluate(X_train, y_train, batch_size=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[-1].bias.assign([0])\nresult = model.evaluate(X_train, y_train, batch_size=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly the model with the initialized bias has a better initial loss.","metadata":{}},{"cell_type":"markdown","source":"## Overfitting the model on 10 rows of data (test phase):","metadata":{}},{"cell_type":"code","source":"# Overfitting a single batch of 10 rows\nmodel.load_weights('initial_weights')\nhistory = model.fit(X_train[97:107], y_train[97:107], epochs=100, batch_size=256, verbose=2)","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 7, 5\ngrid = sns.lineplot(data = history.history['loss'])\ngrid.set(yscale='log')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.column_stack((np.around(model.predict(X_train[97:107]), 3), y_train[97:107]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overfitting the whole dataset:","metadata":{}},{"cell_type":"code","source":"# defining the class_weights\nclass_weight = {0: 1, 1: 1/imbalance_ratio}\nclass_weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_model(tf.keras.optimizers.Adam(learning_rate=1e-2), loss_fn, metrics, output_initializer)\nmodel.load_weights('initial_weights')\nhistory = model.fit(X_train, y_train, epochs=300, batch_size=1024, verbose=2, class_weight=class_weight, validation_data=(X_test, y_test))","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regularization:","metadata":{}},{"cell_type":"markdown","source":"Applying weight decay, dropout of .2, and early stopping:","metadata":{}},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=400, mode='max', restore_best_weights=True, verbose=1)\nmodel = make_model(tfa.optimizers.AdamW(learning_rate=1e-2, weight_decay=5e-4), loss_fn, metrics, output_initializer, dropout=.2)\nmodel.load_weights('initial_weights')\nhistory = model.fit(\n    X_train, y_train, epochs=400, batch_size=256, class_weight=class_weight, \n    callbacks=[callback], validation_data=(X_test, y_test), verbose=2\n)","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visu_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_train)\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test, y_pred))\nprint('f1 score:', f1_score(y_test,np.around(y_pred)), end='\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:","metadata":{}},{"cell_type":"markdown","source":"In this notebook we looked at the data of of stroke prediction, understood it doing EDA, preprocessed it and applied different machine learning models (polynomial SVM Classifier, Random Forest, ANN) with a special emphasis on the ANN model. The results of the models were pretty much the same (AUC score of .82), arguabely acceptable given that the data is highly imbalanced.","metadata":{}},{"cell_type":"markdown","source":"We can further improve the performance of the models by applying these steps:\n* Tuning the hyperparameters for the ann\n* Data agmentation / Oversampling / Underesampling\n* Ensemble","metadata":{}},{"cell_type":"markdown","source":"Comments and Critics are welcome.\n\nThank you for your time!","metadata":{}}]}