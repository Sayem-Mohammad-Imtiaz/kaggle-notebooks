{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T13:47:59.592567Z","iopub.execute_input":"2021-08-09T13:47:59.593013Z","iopub.status.idle":"2021-08-09T13:47:59.616671Z","shell.execute_reply.started":"2021-08-09T13:47:59.592975Z","shell.execute_reply":"2021-08-09T13:47:59.615688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!\\pip install missingno\n#!pip install pylib","metadata":{"execution":{"iopub.status.busy":"2021-08-08T21:45:18.128245Z","iopub.execute_input":"2021-08-08T21:45:18.128798Z","iopub.status.idle":"2021-08-08T21:45:18.132919Z","shell.execute_reply.started":"2021-08-08T21:45:18.128763Z","shell.execute_reply":"2021-08-08T21:45:18.131714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is to filter any warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:07.029611Z","iopub.execute_input":"2021-08-09T13:48:07.030127Z","iopub.status.idle":"2021-08-09T13:48:07.035464Z","shell.execute_reply.started":"2021-08-09T13:48:07.03009Z","shell.execute_reply":"2021-08-09T13:48:07.034087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the dataset into a dataframe\nimport pandas as pd\n\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:07.833012Z","iopub.execute_input":"2021-08-09T13:48:07.833605Z","iopub.status.idle":"2021-08-09T13:48:07.888177Z","shell.execute_reply.started":"2021-08-09T13:48:07.833553Z","shell.execute_reply":"2021-08-09T13:48:07.887019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the column types (All the datatypes should be numeric)\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:10.05586Z","iopub.execute_input":"2021-08-09T13:48:10.056406Z","iopub.status.idle":"2021-08-09T13:48:10.086387Z","shell.execute_reply.started":"2021-08-09T13:48:10.056362Z","shell.execute_reply":"2021-08-09T13:48:10.083827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find if there are any missing data (There are no missing values)\nimport missingno as msno\nax = msno.bar(data,color='tab:blue')\nax.set_title(\"Missing data report for Pima Indian Diabates dataset\")\nax.set_xlabel(\"Features\")\ntx = ax.set_ylabel(\"Importance\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:10.440474Z","iopub.execute_input":"2021-08-09T13:48:10.440911Z","iopub.status.idle":"2021-08-09T13:48:11.577798Z","shell.execute_reply.started":"2021-08-09T13:48:10.440853Z","shell.execute_reply":"2021-08-09T13:48:11.57674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Draw a correlation heatmap to understand the relationshipo between featrures and target variable outcome\nimport seaborn as sns\nax = sns.heatmap(data.corr(),cmap='Accent',annot=True)\n#Glucose , BMI and age has a slight positive corrlation with the output","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:11.579504Z","iopub.execute_input":"2021-08-09T13:48:11.580135Z","iopub.status.idle":"2021-08-09T13:48:12.467574Z","shell.execute_reply.started":"2021-08-09T13:48:11.580089Z","shell.execute_reply":"2021-08-09T13:48:12.466481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if the data classes are imbalanced\nax = sns.countplot(data[\"Outcome\"],color=\"tab:blue\")\ntx = ax.set_title(\"Distribution of data by classes\")\n# Clases are bit imbalanced and can be used without oversampling or undersampling","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:12.470049Z","iopub.execute_input":"2021-08-09T13:48:12.470413Z","iopub.status.idle":"2021-08-09T13:48:12.615905Z","shell.execute_reply.started":"2021-08-09T13:48:12.470379Z","shell.execute_reply":"2021-08-09T13:48:12.614766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset as X and y variable using train test split\nfrom sklearn.model_selection import train_test_split\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:12.618289Z","iopub.execute_input":"2021-08-09T13:48:12.618776Z","iopub.status.idle":"2021-08-09T13:48:12.629475Z","shell.execute_reply.started":"2021-08-09T13:48:12.618737Z","shell.execute_reply":"2021-08-09T13:48:12.627823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We try to find the feature importance by fitting a random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\nfimp_model = RandomForestClassifier()\nfimp_model = fimp_model.fit(X_train,y_train)\nfeatures = X_train.columns\nimportance= fimp_model.feature_importances_\nfimpdata = pd.DataFrame()\nfimpdata[\"Features\"] = features\nfimpdata[\"Importance\"] = fimp_model.feature_importances_ *100\nfimpdata.sort_values(by='Importance',ascending=False,inplace=True)\nax = sns.barplot(y=\"Features\",x=\"Importance\",data=fimpdata,color=\"tab:blue\")\ntx = ax.set_title(\"Feature Importance of all columnns\")\n\n#It seems the classification is not confined to few columns, \n#so we are not reducing any columns part of this experiment","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:12.631313Z","iopub.execute_input":"2021-08-09T13:48:12.631837Z","iopub.status.idle":"2021-08-09T13:48:13.133072Z","shell.execute_reply.started":"2021-08-09T13:48:12.631771Z","shell.execute_reply":"2021-08-09T13:48:13.13181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We would train three models part of this experiment and compare the accuracy scores\n# 1. Logistic regression --> sklearn.linear_model\n# 2. Decison Tree --> sklearn.tree\n# 3. Random forest classifier --> sklearn.ensemble","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.134943Z","iopub.execute_input":"2021-08-09T13:48:13.135393Z","iopub.status.idle":"2021-08-09T13:48:13.140717Z","shell.execute_reply.started":"2021-08-09T13:48:13.135346Z","shell.execute_reply":"2021-08-09T13:48:13.139299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model using Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report\nmodel1 = LogisticRegression(max_iter=1000)\nmodel1 = model1.fit(X_train,y_train)\ny_pred1 = model1.predict(X_test)\nacc_scores = dict()\nacc_scores[\"Logistic regression\"] = accuracy_score(y_test,y_pred1)\nprint(\"Classification report - Logistic regression \")\nprint(\"============================================\")\nprint(classification_report(y_test,y_pred1))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.144067Z","iopub.execute_input":"2021-08-09T13:48:13.14472Z","iopub.status.idle":"2021-08-09T13:48:13.369817Z","shell.execute_reply.started":"2021-08-09T13:48:13.144669Z","shell.execute_reply":"2021-08-09T13:48:13.368519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training model using Decision tree classifier \nfrom sklearn.tree import DecisionTreeClassifier\nmodel2 = DecisionTreeClassifier()\nmodel2 = model2.fit(X_train,y_train)\ny_pred2 = model2.predict(X_test)\nacc_scores[\"Decision tree\"] = accuracy_score(y_test,y_pred2)\nprint(\"Classification report - Decision tree\")\nprint(\"=====================================\")\nprint(classification_report(y_test,y_pred2))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.372118Z","iopub.execute_input":"2021-08-09T13:48:13.372578Z","iopub.status.idle":"2021-08-09T13:48:13.395906Z","shell.execute_reply.started":"2021-08-09T13:48:13.372531Z","shell.execute_reply":"2021-08-09T13:48:13.394553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training model using Random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\nmodel3 = RandomForestClassifier()\nmodel3 = model3.fit(X_train,y_train)\ny_pred3 = model3.predict(X_test)\nacc_scores[\"Random forest\"] = accuracy_score(y_test,y_pred3)\nprint(\"Classification report - Random Forest \")\nprint(\"======================================\")\nprint(classification_report(y_test,y_pred3))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.397367Z","iopub.execute_input":"2021-08-09T13:48:13.397698Z","iopub.status.idle":"2021-08-09T13:48:13.676054Z","shell.execute_reply.started":"2021-08-09T13:48:13.397665Z","shell.execute_reply":"2021-08-09T13:48:13.675141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training model using Gradient Boosting classifier \nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel4 = GradientBoostingClassifier()\nmodel4 = model4.fit(X_train,y_train)\ny_pred4 = model4.predict(X_test)\nacc_scores[\"xgboost\"] = accuracy_score(y_test,y_pred4)\nprint(\"Classification report - xgboost \")\nprint(\"======================================\")\nprint(classification_report(y_test,y_pred4))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.677302Z","iopub.execute_input":"2021-08-09T13:48:13.677809Z","iopub.status.idle":"2021-08-09T13:48:13.855609Z","shell.execute_reply.started":"2021-08-09T13:48:13.677759Z","shell.execute_reply":"2021-08-09T13:48:13.854723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot to compare accuracies\naccscore_df = pd.DataFrame()\naccscore_df[\"Model\"] = acc_scores.keys()\naccscore_df[\"Accuracy\"] = acc_scores.values()\nax = sns.barplot(x=\"Model\",y=\"Accuracy\",data=accscore_df)\ntx = ax.set_title(\"Comparion of accuracy Logistic regression vs Decision Tree vs Random Forest\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:13.856955Z","iopub.execute_input":"2021-08-09T13:48:13.857433Z","iopub.status.idle":"2021-08-09T13:48:14.060289Z","shell.execute_reply.started":"2021-08-09T13:48:13.857382Z","shell.execute_reply":"2021-08-09T13:48:14.059418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_scores","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:48:14.061468Z","iopub.execute_input":"2021-08-09T13:48:14.061939Z","iopub.status.idle":"2021-08-09T13:48:14.068756Z","shell.execute_reply.started":"2021-08-09T13:48:14.061888Z","shell.execute_reply":"2021-08-09T13:48:14.067183Z"},"trusted":true},"execution_count":null,"outputs":[]}]}