{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"The purpose of this notebook is to create a simple Convolutionnal Neural Network using Tensorflow in order to recognize hand gestures for sign language. We shall then try to use it with the camera using OpenCV."},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D, Flatten, Input\nimport cv2\nimport matplotlib.pyplot as plt\nimport random as rd\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Dataset"},{"metadata":{},"cell_type":"markdown","source":"We import the train dataset and show a random image and its label. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\")\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphabet=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\nn = rd.randrange(df_train.shape[0])\nar = np.array(df_train.loc[n][1:]).reshape((28,28))\nplt.imshow(ar, cmap='gray')\nplt.title(alphabet[df_train.loc[n][0]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"First we separate our labels from our data. We then have to normalize the data of all images, and create a simple one hot encoding of the labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"label\"]\nX = df_train.drop(['label'], axis=1)\n\nX = np.array(X)/255\ny = np.array(y)\n\nY = np.zeros((len(alphabet),df_train.shape[0]))\nfor i in range(len(y)):\n  Y[y[i],i] = 1\nX = X.reshape((-1, 28,28,1))\nY = Y.reshape((26,-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Convolutionnal Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1),padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(556, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(26, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X,y,batch_size=64,epochs=3, validation_split=0.2) #training","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The predictions are remarkably good ever since the first epoch."},{"metadata":{},"cell_type":"markdown","source":"# 4. Validation"},{"metadata":{},"cell_type":"markdown","source":"I decided to use the test dataset as validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\")\n\n#preprocessing\nn = rd.randrange(df_valid.shape[0])\ny = df_valid[\"label\"]\nX = df_valid.drop(['label'], axis=1)\n\nar = np.array(df_valid.loc[n][1:]).reshape((28,28))\n\nX = np.array(X)/255\ny = np.array(y)\n\nY = np.zeros((26,df_valid.shape[0]))\nfor i in range(len(y)):\n  Y[y[i],i] = 1\nX = X.reshape((-1, 28,28,1))\nY = Y.reshape((26,-1))\n\nplt.imshow(ar, cmap='gray')\nplt.title(f\"Prediction :  {alphabet[ np.argmax(model.predict(X[n].reshape(1,28,28,1)))]} | had to predict {alphabet[df_valid.loc[n][0]]}\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Using Webcam with OpenCV "},{"metadata":{},"cell_type":"markdown","source":"*Warning : Doesn't work online, works on user machine*"},{"metadata":{"trusted":true},"cell_type":"code","source":"cap = cv2.VideoCapture(0)\nwhile(True):\n    ret, frame = cap.read()\n    cv2.rectangle(frame, (100, 100), (300, 300), (0, 255, 0), 0)\n    roi = frame[100:300, 100:300]\n    f = cv2.resize(roi, (28, 28))\n    gray = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n    cv2.imshow('frame',frame)\n    if cv2.waitKey(10) & 0xFF == ord('q'):\n        print(\"____\")\n        model.predict(gray.reshape(1,28,28,1))\n        print(alphabet[np.argmax(model.predict(gray.reshape(1,28,28,1)))])\n        \ncap.release()\ncv2.destroyAllWindows()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the webcam shows a problem, the use of a solid gray background in the dataset makes it difficult for the CNN to generalize. A way to solve this issue would be to extract the hand from the background and having a \"binary image\" using OpenCV.  *TO BE CONTINUED...*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}