{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Sample End to End ML Project\n\nNote: made while learning with Chapter 2 of [Hands on ML Book](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) , official code at [Github](https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"housing = pd.read_csv('../input/housing.csv')\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sneak Peek"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- total_bedrooms may have some missing values\n- Most of our attributes are numerical data, apart from categorical ocean_proximity"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.ocean_proximity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- 5 categories in total\n- Need to convert these to oneHotEncoded feature in preprocessing phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins = 50, figsize = (20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- Most of the attributes are heavy tailed. Need to normalize in preprocessing stage.\n- median_house_value which is our target seems to be capped at 5M, so is housing_median_age"},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split\nIf we assume median_income is an important attribute in order to guess out target, we need to ensure that train and test set have representation from all stratas of this attribute. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size = 0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'] = np.ceil(housing.median_income/1.5)\nhousing.income_cat.where(housing.income_cat < 5, 5.0, inplace=True)\nhousing.income_cat.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use scikit learn stratified split\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplitObj = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index, test_index in splitObj.split(housing, housing.income_cat):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(strat_test_set.income_cat.value_counts()/len(strat_test_set)).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop added attribute\n\nfor set in (strat_train_set, strat_test_set):\n    set.drop(['income_cat'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot.scatter(x = 'longitude', y = 'latitude', alpha=0.4,\n                    s = housing.population, label='population',\n                    c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True, figsize = (60,60))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- As expected house costs near the coast are more compared to away from the coast"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot.hexbin(x = 'longitude', y='latitude', gridsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate correlation coefficient\n# This might be important from feature engineering perspective, since two attributes which are heaviliy\n# correlated may not be good as individual features\n\ncorr_matrix = housing.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas Scatter matrix can help plotting multiple scatter plots together\n\nfrom pandas.tools.plotting import scatter_matrix\n\nscatter_matrix(housing.loc[:,['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']].sample(1000),\n              figsize = (12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- Scatter matrix is exceptionally helpful to guess visually the attributes having high correlations. This will be helpful during feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note that our target attribute is median_house_value. Most promising attribute to predict it seems to be\n# median income with correlation of 0.68\n\nhousing.plot.scatter(x = 'median_income', y='median_house_value', alpha = 0.4, figsize = (30,30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- High correlation\n- Cap at the higher price\n- Faint horizontal lines which can be problematic"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try combinations of features\nhousing['rooms_per_household'] = housing.total_rooms/housing.households\nhousing['bedrooms_per_room'] = housing.total_bedrooms / housing.total_rooms\nhousing['population_per_household'] = housing.population/housing.households","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if correlation has improved after attribute combinations\ncorrMat = housing.corr()\ncorrMat.median_house_value.sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observtions\n- Can convert total_bedrooms, total_rooms, population in features which make more sense with respect to predicting our target\n- These derived features seem to have slightly higher correlation with target. Should help."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop labels from training set\nhousing = strat_train_set.drop('median_house_value', axis = 1)\nhousing_labels = strat_train_set['median_house_value'].copy()\nhousing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values\nsample_missing_rows = housing[housing.isnull().any(axis=1)]\nsample_missing_rows.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    from sklearn.impute import SimpleImputer\nexcept:\n    from sklearn.preprocessing import Imputer as SimpleImputer\n\nimputer = SimpleImputer(strategy = 'median')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing text attribute since fit can't be done to text data.\nhousing_num = housing.drop('ocean_proximity', axis = 1)\nhousing_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housing_num)\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above should be same as\nhousing.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the dataset, returns the numpy array\nX = imputer.transform(housing_num)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to dataframe - Housing Truncated\nhousing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing.index)\nhousing_tr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the filled values\nhousing_tr.loc[sample_missing_rows.index.values, :].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Handle Categorical Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat = housing.ocean_proximity\nhousing_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n\nencoder = OrdinalEncoder()\nhousing_encoded = encoder.fit_transform(housing_cat.values.reshape(-1,1))\nencoder.categories_\n\n# This encoding is however problematic as model might learn these categories to be ordered, or more/less \n# important based on the number category is assigned. Model can also assume that two nearby values are more\n# similar than distant values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder()\nhousing_onehot_encoded = one_hot_encoder.fit_transform(housing_cat.values.reshape(-1,1))\n# above method returns a scipy sparse matrix, it can be converted to numpy dense array\nhousing_onehot_encoded.toarray()\n# alternatively call OneHotEncoder(sparse=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custome tranformer can be implemented by adding fit, transform and fit_transform methods to a class.\n# fit_tranform method, get_params and set_params method can be achieved by adding base classes\n# BaseEstimator and TransformerMixin\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Writing a class for combined attributes adder\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3,4,5,6\n\nclass CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, Y=None):\n        return self\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix]/ X[:, household_ix]\n        population_per_household = X[:, population_ix]/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix]/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nattr_adder = CombinedAttributeAdder(add_bedrooms_per_room = False)\nhousing_extra_array = attr_adder.fit_transform(housing.values)\nhousing_extra = pd.DataFrame(housing_extra_array, columns = list(housing.columns)+ \n             ['rooms_per_household', 'population_per_household'], index = housing.index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optionally we can use FunctionTransformer to just convert a function into transformer.\n\n# from sklearn.preprocessing import FunctionTransformer\n\n# def add_extra_features(X, add_bedrooms_per_room=True):\n#     rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n#     population_per_household = X[:, population_ix] / X[:, household_ix]\n#     if add_bedrooms_per_room:\n#         bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n#         return np.c_[X, rooms_per_household, population_per_household,\n#                      bedrooms_per_room]\n#     else:\n#         return np.c_[X, rooms_per_household, population_per_household]\n\n# attr_adder = FunctionTransformer(add_extra_features, validate=False,\n#                                  kw_args={\"add_bedrooms_per_room\": False})\n# housing_extra_attribs = attr_adder.fit_transform(housing.values)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a Preprocessing Pipeline\n\nSo for processing the data\n- You do some operations for numerical part ( There will be many transformations in this, build a pipeline). Pipeline class executes all the fit_transform() methods sequentially.\n- You do some transformations for categorical part.\n\nBoth pipelines are then combined using ColumnTransformer which takes in inputs similar to pipeline and selective columns for which that pipeline should be applied. Note that contrary to Pipeline class though, column transformer calls fit_transform for all constituents parallely and then concatenates the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.ocean_proximity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = ['ocean_proximity']\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('attributeAdder', CombinedAttributeAdder()),\n    ('StandardScaler', StandardScaler())\n])\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n   ('cat', OneHotEncoder(), cat_attribs)\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared.shape\n# Note that one hot encoded 5 categories take 5 columns, 3 for added attributes, 8 original columns.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\nsome_train_data = housing.iloc[:5]\nsome_train_labels = housing_labels.iloc[:5]\nsome_train_data_prepared = full_pipeline.transform(some_train_data)\nsome_train_data_predictions = lin_reg.predict(some_train_data_prepared)\n\n\nresults = pd.DataFrame({'labels': list(some_train_labels),'predictions': list(some_train_data_predictions)})\nresults['differencePercent'] = ((results.predictions - results.labels)*100)/results.labels\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\npredictions = lin_reg.predict(housing_prepared)\nmse = mean_squared_error(predictions, housing_labels)\nrmse = np.sqrt(mse)\nrmse\n\n# Average rmse of $68k is pretty bad.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(predictions, housing_labels)\nmae\n\n# this model is clearly underfitting the data.\n# one option would be feature engineering to add more valuable features. Another option is\n# to try an use a more complex model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\ndecTree = DecisionTreeRegressor()\ndecTree.fit(housing_prepared, housing_labels)\ntreePredictions = decTree.predict(housing_prepared)\nrmse_tree = np.sqrt(mean_squared_error(treePredictions, housing_labels))\nrmse_tree\n\n# 0 error might mean model have overfit the data now.\n\n# To confirm this we need a validation set. We can't use a test set since we might end up overfitting \n# the test set if we iterate our models using test set and then model may not become production ready.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do 10 fold cross validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(decTree, housing_prepared, housing_labels,\n                        scoring = 'neg_mean_squared_error', cv = 10)\ndef display_scores(scores):\n    scores = np.sqrt(-scores)\n    print(f\"Scores: {scores}\")\n    print(f\"Mean: {scores.mean()}\")\n    print(f\"Standard Deviation: {scores.std()}\")\n    \ndisplay_scores(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nSo the mean error is around 71K with standard deviation around +-2503.\nLet's compare the same with linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_scores = scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                        scoring = 'neg_mean_squared_error', cv = 10)\ndisplay_scores(reg_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nDecision tree is actually performing worse than linear regression!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a RandomForestRegressor and check its performance\n# Random forest trains decision trees on random subsets of features and then averages out their predictions\n# called ensemble technique.\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\nforest_predictions = forest_reg.predict(housing_prepared)\nrmse = np.sqrt(mean_squared_error(forest_predictions, housing_labels))\nrmse\n# Random Forest is fitting the training dataset better than linear regression but worse than \n# decision tree. Let's check the validation set error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                               scoring = 'neg_mean_squared_error', cv = 10)\ndisplay_scores(forest_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- Validation set error is for random forest is less than that for single Decision Tree\n- Still the training set error is significantly less than validation set error even for random forest\n\nThus, model is overfitting the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying out the SVM\nfrom sklearn.svm import SVR\nsvm_reg = SVR(kernel='linear')\nsvm_reg.fit(housing_prepared, housing_labels)\nsvm_predictions = svm_reg.predict(housing_prepared)\nrmse = np.sqrt(mean_squared_error(svm_predictions, housing_labels))\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine Tune the model\n\nWe tried certain models above and found out their training and validation errors. In practice, we would do more divergent thinking to train many more models and select 2-3 top performing ones.\nThen we dive deeper into each of those selected models to tune them and get better performance.\n\nFine tuning can be done via hyperparameter search\n- Grid Search\n- Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = [\n    # First try 12 combinations of features\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # Then 6 combinations\n    {'bootstrap': [False],'n_estimators': [3, 10],  'max_features': [2, 3, 4]}\n]\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)\n\n# Total 90 combinations including cross validaiton","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_\n# Gives the best model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the scores during the paramter search\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(grid_search.cv_results_).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get to know feature importances in random forest\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 95% Confidence interval for Test\nhttps://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714\nhttps://machinelearningmastery.com/confidence-intervals-for-machine-learning/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nmean = squared_errors.mean()\nm = len(squared_errors)\n# T Scores\nnp.sqrt(stats.t.interval(confidence, m - 1,\n                         loc=np.mean(squared_errors),\n                         scale=stats.sem(squared_errors)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# T scores manual\ntscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Z Score manually\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Full pipeline Processing and Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pipeline_with_predictor = Pipeline([\n        (\"preparation\", full_pipeline),\n        (\"linear\", LinearRegression())\n    ])\n\n# Transform and fit\nfull_pipeline_with_predictor.fit(housing, housing_labels)\n# Transform and predict\nfull_pipeline_with_predictor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model\nmy_model = full_pipeline_with_predictor\nfrom sklearn.externals import joblib\n\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}