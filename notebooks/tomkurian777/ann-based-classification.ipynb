{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.pandas.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can drop these column, as they are just some indicators\n* EmployeeCount\n* EmployeeNumber\n* StandardHours","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Attrition** is the `target` column\n\nObtain a count plot of the column","metadata":{}},{"cell_type":"code","source":"target = 'Attrition'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=target, data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the countplot it can be observed that the target is imbalanced.\n\nLater in this notebook, we will alter it.","metadata":{}},{"cell_type":"markdown","source":"Now check if any features contains null values","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So from the above series it can be concluded that there are no missing values","metadata":{}},{"cell_type":"markdown","source":"### Now it's time to do some data vizs\nWe will start with category variables","metadata":{}},{"cell_type":"code","source":"# Categorical Variables\n\ncat_vars = [var for var in df.columns if df[var].dtype == 'O' and var != target]\n\n# There are few other categorical features which are not by default\n# We will analyze those variables also\nxtra_vars = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel' ,'JobSatisfaction',\n             'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'NumCompaniesWorked',\n             'StockOptionLevel', 'PercentSalaryHike', 'TrainingTimesLastYear']\n\n\ncat_vars = cat_vars + xtra_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_cat(var, dataframe):\n    plt.figure(figsize=(16, 4))\n    sns.countplot(x=var, hue=target, data=dataframe)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in cat_vars:\n    plot_cat(i, df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's plot a percentage plot to check what percent of employees\n## left at each features","metadata":{}},{"cell_type":"code","source":"def plot_cat_percent(var, dataframe):\n    plt.figure(figsize=(16, 4))\n    ys_df = df[df[target] == 'Yes'].groupby(var).count()[target]\n    no_df = df[df[target] == 'No'].groupby(var).count()[target]\n    rat = ys_df / (ys_df + no_df) * 100\n    rat.plot(kind='bar')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in cat_vars:\n    plot_cat_percent(i, df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These columns are not thatmuch contributing to the attrition rate\n\n**`Gender, Over18, PerformanceRating`**\n\nIn those features ratio of employees leaving are almost the same, So we can drop those columns","metadata":{}},{"cell_type":"markdown","source":"## Numerical Variables","metadata":{}},{"cell_type":"code","source":"num_vars = [var for var in df.columns if var not in cat_vars and var!=target]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[num_vars].hist(bins=30, figsize=(15,15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in num_vars:\n    sns.boxplot(x=target, y=i, data=df)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[num_vars + [target]].groupby(target).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**HourlyRate**, **MonthlyRate**, **YearsSinceLastPromotion** are not significant features here  ","metadata":{}},{"cell_type":"code","source":"num_vars.remove('HourlyRate')\nnum_vars.remove('MonthlyRate')\nnum_vars.remove('YearsSinceLastPromotion')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.heatmap(df[num_vars].corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the heatmap it can be observed that there are some mutually correlated features","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=df['TotalWorkingYears'], y=df['MonthlyIncome'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its predictable, since more the experience more the salary","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(y=df['TotalWorkingYears'], x=df['Age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same inference as above","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=df['YearsAtCompany'], y=df['TotalWorkingYears'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(y=df['YearsInCurrentRole'], x=df['TotalWorkingYears'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(y=df['YearsWithCurrManager'], x=df['TotalWorkingYears'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove `age` from the features set, because age is correlated with working years.\n","metadata":{}},{"cell_type":"code","source":"num_vars.remove('Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_vars.remove('Gender')\ncat_vars.remove('Over18')\ncat_vars.remove('PerformanceRating')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selected features\nfqs = num_vars + cat_vars + [target]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(fqs), len(df.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[fqs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will encode various categories\n\nWe will encode **OverTime**, **Attrition** with OrdinalEncoder, and remaining with One Hot Encoder","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc = OrdinalEncoder()\noc_data = enc.fit_transform(df[['OverTime','Attrition']].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DRop those mentioned columns and replace them with oc_data\nord_cols = ['OverTime','Attrition']\noc_df = pd.DataFrame(oc_data, columns=ord_cols)\ndf = df.drop(columns=ord_cols)\ndf = pd.concat([df, oc_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohc = OneHotEncoder(sparse=False, drop='first')\n\nohc_cols = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\nohc_data = ohc.fit_transform(df[ohc_cols].values)\n\nohc_df = pd.DataFrame(ohc_data, columns=ohc.get_feature_names())\ndf = df.drop(columns=ohc_cols, axis=1)\ndf = pd.concat([df, ohc_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Attrition', axis=1)\ny = df['Attrition'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to use SMOTE method to balance the output classes, This is a simple implementation you can finetune this later","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nXo, yo = oversample.fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yo.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=yo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Xo, yo, test_size=0.2, random_state=41)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_sc.shape, X_test_sc.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Model\n\n## Simple ann model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential()\n\nmodel.add(Dense(38, input_shape=(38,), activation='relu'))\nmodel.add(Dense(19, activation='relu'))\nmodel.add(Dense(9, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=X_train_sc, y=y_train, epochs=50, validation_data=(X_test_sc, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test_sc)\npred = np.where(pred>0.5, 1, 0)\nfrom sklearn.metrics import confusion_matrix, classification_report\nc_m = confusion_matrix(y_test, pred)\nprint(c_m)\nprint(classification_report(y_test, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 2\n\n## For this model, we are adding an earlystopping (it's not required, I am doing it for learning purpose)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_e = Sequential()\n\nmodel_e.add(Dense(38, input_shape=(38,), activation='relu'))\nmodel_e.add(Dense(19, activation='relu'))\nmodel_e.add(Dense(9, activation='relu'))\nmodel_e.add(Dense(1, activation='sigmoid'))\n\nmodel_e.summary()\n\nmodel_e.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_e.fit(x=X_train_sc, y=y_train, \n          epochs=50, validation_data=(X_test_sc, y_test), callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss = pd.DataFrame(model_e.history.history)\nmodel_loss.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model_e.predict(X_test_sc)\npred = np.where(pred>0.5, 1, 0)\nfrom sklearn.metrics import confusion_matrix, classification_report\nc_m = confusion_matrix(y_test, pred)\nprint(c_m)\nprint(classification_report(y_test, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### That's it from my end, This notebook was a part of deep learning journey\n\n### PLzzzzzzz like / star if it's good","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}