{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table Of Content</h1>\n\n\n* [1. Introduction](#1)\n    * [1.1 Aim of the notebook](#1.1)\n    * [1.2 Libraries And Utilities](#1.2)\n    * [1.3 Data Loading](#1.3)\n* [2. Exploratory Data Analysis(EDA)](#2)\n    * [2.1 Continous Features](#2.1)\n    * [2.2 Target Variable](#2.2)\n* [3. Problems with highly imbalanced datasets](#3)\n    * [3.1 Choice of metric](#3.1)\n    * [3.2 Overview of the different methods](#3.2)\n* [4. UnderBagging techniques (Ensemble Undersampling)](#4)\n    * [4.1 EasyEnsemble from scratch](#4.1) \n    * [4.2 BalanceCascade from scratch](#4.2)\n* [5. Conclusion](#5)"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Introduction</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Aim of the notebook</h3>"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to introduce why and how we should use **UnderBagging** techniques to deal with **highly imbalanced, large scale and noisy datasets.**\n\nI decided to do this experiment on the famous credit card fraud detection dataset but this is not the best to do so. Indeed, it is small (only 300 000 samples whereas in real worlds million of samples) and is not as imbalanced as real worlds highly imbalanced datasets (sometimes with 10^6 : 1 ratio). I will try to find another dataset to show how **UnderBagging outperforms many simple undersampling techniques.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities</h3>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,recall_score,confusion_matrix, f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom random import randint\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hr=pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\nstroke=pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/creditcardfraud/creditcard.csv')\ndf=df.rename(columns={'Class':'target'})\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=[f'V{i}' for i in range(1,29)]\nfeatures.append('Amount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Continuous Features</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(15, 10), facecolor='whitesmoke')\n\nfig.suptitle(\"Continous Features Distribution\",x=0.5,y=0.95, fontsize=\"xx-large\",fontweight=\"bold\")\n\nfor plot in range(1,13):\n    \n    locals()[\"ax\"+str(plot)]=fig.add_subplot(4,3,plot)\n    locals()[\"ax\"+str(plot)].set_facecolor(\"whitesmoke\")\n    locals()[\"ax\"+str(plot)].set_yticklabels([])\n    locals()[\"ax\"+str(plot)].tick_params(axis='y', which=u'both',length=0)\n    \n    for direction in [\"top\",\"right\", 'left']:\n        locals()[\"ax\"+str(plot)].spines[direction].set_visible(False)\n\ni = 1\nfor feature in features[:12]:\n   \n    sns.kdeplot(df[feature], ax=locals()[\"ax\"+str(i)], shade=True, color='gold', alpha=0.9, zorder=2)\n    locals()[\"ax\"+str(i)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(i)].set_ylabel(feature, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(i)].set_xlabel('')\n    locals()[\"ax\"+str(i)].set_xlim(-5, 5)\n\n\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''>Exploring the relationship between continuous features and the target variable</span></p>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(15, 10), facecolor='whitesmoke')\n\nfig.suptitle(\"Continous features distribution w.r.t target variable\",x=0.5,y=0.95, fontsize=\"xx-large\",fontweight=\"bold\")\n\nfor plot in range(1,13):\n    \n    locals()[\"ax\"+str(plot)]=fig.add_subplot(3,4,plot)\n    locals()[\"ax\"+str(plot)].set_facecolor(\"whitesmoke\")\n    locals()[\"ax\"+str(plot)].set_yticklabels([])\n    locals()[\"ax\"+str(plot)].tick_params(axis='y', which=u'both',length=0)\n    \n    for direction in [\"top\",\"right\", 'left']:\n        locals()[\"ax\"+str(plot)].spines[direction].set_visible(False)\n\ni = 1\nfor feature in features[:12]:\n   \n    sns.kdeplot(df[df.target==0][feature], ax=locals()[\"ax\"+str(i)], shade=True, color='gold', alpha=0.9, zorder=2)\n    sns.kdeplot(df[df.target==1][feature], ax=locals()[\"ax\"+str(i)], shade=True, color='darkorange', alpha=0.9, zorder=2)\n    locals()[\"ax\"+str(i)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(i)].set_ylabel(feature, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(i)].set_xlabel('')\n    locals()[\"ax\"+str(i)].set_xlim(-10, 10)\n\n\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 20px;'>It seems that there is a clear distinction between the distributions of the variables of each class.</span></p>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Target Variable</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ir=int(len(df[df.target==0])/len(df[df.target==1]))\n\nfig=plt.figure(figsize=(15, 10), facecolor='whitesmoke')\nplt.suptitle(f'Imbalance ratio {ir}:1',x=0.5,y=0.95, fontsize=\"xx-large\",fontweight=\"bold\")\n\nax1=fig.add_subplot(1,1,1)\nax1.set_facecolor(\"whitesmoke\")\n\nfor direction in [\"top\",\"right\", 'left']:\n    ax1.spines[direction].set_visible(False)\n\nsns.countplot(df.target,ax=ax1,color='gold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Problems with highly imbalanced datasets</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Choice of metric</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a simple model to highlight the problem with imbalanced datasets\nX,y=df[features],df.target\n\nkf=StratifiedKFold(n_splits=5)\naccuracy=[]\nrecall=[]\n\nfor train_idx,test_idx in kf.split(X,y):\n    \n    X_train,y_train= X.iloc[train_idx],y.iloc[train_idx]\n    X_test,y_test= X.iloc[test_idx],y.iloc[test_idx]\n\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n\n    predictions=model.predict(X_test)\n    \n    accuracy.append(accuracy_score(y_test,predictions))\n    recall.append(recall_score(y_test,predictions))\n\nprint(f'Model accuracy over the 5 folds: {np.round(np.mean(accuracy),7)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that our model is doing pretty well if we look only at the accuracy. Unfortunately, our model is terrible. To show you why, let's create a simple baseline model. A model totally untalented and unintellectual that is to say a model that always predicts 0 (ie the transaction is legitimate)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating simple predictions\nX,y=df[features],df.target\n\nkf=StratifiedKFold(n_splits=5)\naccuracy_baseline=[]\nrecall_baseline=[]\n\nfor train_idx,test_idx in kf.split(X,y):\n    \n    X_train,y_train= X.iloc[train_idx],y.iloc[train_idx]\n    X_test,y_test= X.iloc[test_idx],y.iloc[test_idx]\n\n    simple_preds=np.zeros(len(X_test))\n    \n    accuracy_baseline.append(accuracy_score(y_test,simple_preds))\n    recall_baseline.append(recall_score(y_test,simple_preds))\n\n\nprint(f'Model accuracy is {np.round(np.mean(accuracy_baseline),7)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that predicting always that the transaction is legitimate is also giving a very good accuracy. In both cases, this accuracy is hiding a harsh reality. Let's look at the percentage of frauds detected by our model. Of course for the baseline, 0% of frauds are detected."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'Percentage of frauds detected over the 5 folds: {np.round(np.mean(recall),7)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This 99% of accuracy is hiding a harsh reality, our model has a fraud detection rate of 60%. This fraud detection rate can be much worse when the dataset is very large scale and noisy. It highlights the fact that in imbalanced datasets the choice of evaluation metric is very important. There are a lot of factors that will influence the choice of evaluation metric. Since the goal of this notebook is to present ensembles of undersampling techniques, we will not cover how to choose the best eveluation metric for your problems, but there are a lot of ressources out there talking about that."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Overview of the different methods</h3>"},{"metadata":{},"cell_type":"markdown","source":"There are two well known different ways of dealing with imbalanced datasets: oversampling and undersampling. In this part I will give a fast explanation of undersampling techniques.The goal is to give you enough context so that everyone can understand what are ensemble undersampling techniques and why it is used for."},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''>Undersampling techniques</span></p>"},{"metadata":{},"cell_type":"markdown","source":"**Undersampling techniques refers to all the techniques that aims at reducing the number of majority samples**. I will show you a simple example of undersampling technique which is called **Random Under-Sampling**. It aims at reducing the number of majority samples by randomly sampling majority samples. There are many others techniques that enable to do a different under-sampling job like Near Miss Undersampling, Condensed Nearest Neighbor, Tomek Links and many others. But we will not cover that in this notebook since its goal is to make an introduction of UnderBagging techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a RandomUnderSampler from scratch. I could have used imblearn library. \n# But just for fun we will reproduce it from scratch.\n\ndef rus(X_train: pd.DataFrame, y_train: pd.DataFrame, sampling_strategy: float):\n    \n    \"\"\" Simple implementation of RandomUnderSampling \"\"\"\n    \n    train=pd.concat([X_train,y_train],axis=1)\n    \n    train_maj=train[train.target==0]\n    train_min=train[train.target==1]\n    \n    train_maj_rus=train_maj.sample(int(1/sampling_strategy*len(train_min)),random_state=randint(1,100000))\n    \n    train_rus=pd.concat([train_maj_rus,train_min])\n    \n    X_train_rus= train_rus.drop('target',axis=1)\n    y_train_rus= train_rus.target\n    \n    return X_train_rus,y_train_rus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's reuse our simple LogisticRegression but this time using random under sampling technique.\nX,y=df[features],df.target\n\nkf=StratifiedKFold(n_splits=5)\n\nrecall=[]\nf1=[]\n\nfor train_idx,test_idx in kf.split(X,y):\n    \n    X_train,y_train= X.iloc[train_idx],y.iloc[train_idx]\n    X_test,y_test= X.iloc[test_idx],y.iloc[test_idx]\n    \n    X_train_rus,y_train_rus= rus(X_train,y_train,sampling_strategy=0.5)\n\n    model=LogisticRegression()\n    model.fit(X_train_rus,y_train_rus)\n\n    predictions=model.predict(X_test)\n    \n    f1.append(f1_score(y_test,predictions))\n    recall.append(recall_score(y_test,predictions))\n\n\nprint(f'Percentage of frauds detected over the 5 folds: {np.round(np.mean(recall),7)}')\nprint(f'F1_score over the 5 folds: {np.round(np.mean(f1),7)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we go, we increase the frauds detection rate using random under sampling strategy. But of course, we increased the number of legitimate transactions classified as frauds by our model."},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''> Problem with simple undersampling techniques </span></p>"},{"metadata":{},"cell_type":"markdown","source":"Even if undersampling techniques tackle the problem of class imbalance by reducing the level of imbalance of the dataset, there are some drawbacks: It only uses a subset of major class samples. Thus, we lose information from the ignored samples. Also, undersampling is very sensitive to noise. The more the training set is large scale and noisy, the more it is important to find a way to tackle the different problems cited above. That's exactly the reason why ensembles of under-sampling techniques has been created. \n        "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">UnderBagging techniques</h1>"},{"metadata":{},"cell_type":"markdown","source":"In this part, I will implement from scratch EasyEnsemble and BalanceCascade algorithms (two underbagging techniques). As I said above, these two algorithms has been introduced to tackle the problem of loss of information and sensitiveness to noisy samples produced by using a simple undersampling technique. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\"> EasyEnsemble from scratch </h3>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''> Simple explanation of the algorithm </span></p>"},{"metadata":{},"cell_type":"markdown","source":"The algorithm consists of repeating the random under-sampling strategy n times. Each time, we create a new subset of data (result of the undersampling) and train a new classifier on it. At the end, our model is composed of n classifiers. To make a prediction, we simply take the average of the different classifiers. This is why these kind of algorithms are called UnderBagging.\n\nBasically, I will:\n\n* Randomly under-sample the training set.\n* create and tune the hyperparameters of an xgboost model using optuna on this under-sampled training set.\n* Save my model.\n* Randomly under-sample the training set another time.\n* create and tune the hyperparameters of a new xgboost model using optuna on this under-sampled training set.\n* Save the model.\n* And so on and so forth for n iterations.\n* At the end, each of my xgboost model will make predictions on the test set.\n* Final prediction is just an average of the predictions of the different xgboost model.\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''> EasyEnsemble algorithm </span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# optuna tuning\ndef objective(trial: pd.DataFrame, X_train: pd.DataFrame, y_train: pd.DataFrame):\n    \n    \"\"\" Simple function to tune hyperparameters of each XGBoost that will constitute to the final model \"\"\"\n    \n    params={'lambda': trial.suggest_loguniform('lambda', 1e-2, 5.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-2, 5.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01,0.012,0.014,0.016,0.018, 0.02,0.05]),\n        'n_estimators': trial.suggest_int('n_estimators',50,500),\n        'max_depth': trial.suggest_categorical('max_depth', [2,3,5,7,9,11]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2021]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        }\n    \n    f1=[]\n    recall=[]\n    \n    kf= StratifiedKFold(n_splits=5)\n     \n    for train_idx,test_idx in kf.split(X_train,y_train):\n        \n        X_train_tuning,y_train_tuning= X_train.iloc[train_idx],y_train.iloc[train_idx]\n\n        X_test_tuning,y_test_tuning= X_train.iloc[test_idx],y_train.iloc[test_idx]\n        \n        model=XGBClassifier(**params,eval_metric='auc',n_jobs=-1)\n        \n        model.fit(X_train_tuning,y_train_tuning)\n        \n        predictions=model.predict(X_test_tuning)\n        \n        f1.append(f1_score(y_test_tuning,predictions))\n        \n        #recall.append(recall_score(y_test_tuning,predictions))\n        \n    return np.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the xgboost model\ndef create_xgb(X_train: pd.DataFrame, y_train: pd.DataFrame):\n    \n    \"\"\" Takes as input the training set composed of X_train and y_train. \n    It returns an xgboost model tuned with the specified training set. \"\"\"\n    \n    study=optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=50)\n    \n    params=study.best_params\n    \n    model=XGBClassifier(**params,eval_metric='auc',n_jobs=-1)\n    \n    model.fit(X_train, y_train, verbose=0)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the underbagging algorithm\ndef easyensemble(X_train: pd.DataFrame, y_train: pd.DataFrame, features: list, n_estimators: int):\n    \n    \"\"\" Simple implementation of easyensemble but with XGBoost as learners instead of AdaBoost.\n    Takes as input a training set, the different features and the number of XGBoost model. \"\"\"\n        \n    models=[]\n    \n    for estimator in range(1,n_estimators):\n        \n        undersampler= RandomUnderSampler(sampling_strategy=0.5,random_state=randint(0,100000))\n        \n        X_train_rus,y_train_rus= undersampler.fit_resample(X_train, y_train)\n        \n        models.append(create_xgb(X_train_rus,y_train_rus))\n        \n         \n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation function\ndef cross_val(df: pd.DataFrame, features: list, n_estimators: int):\n    \n    \"\"\" Cross validation function. \"\"\"\n    X,y=df[features],df.target\n    \n    kf= StratifiedKFold(n_splits=5)\n    recall=[]\n    f1=[]\n    \n    i=1\n    \n    for train_idx,test_idx in kf.split(X,y):\n        \n        X_train_tuning,y_train_tuning= X.iloc[train_idx],y.iloc[train_idx]\n\n        X_test,y_test= X.iloc[test_idx],y.iloc[test_idx]\n    \n        models=easyensemble(X_train_tuning, y_train_tuning, features, n_estimators)\n    \n    \n        y_preds_proba=0\n    \n        for model in models:\n        \n            y_preds_proba+=model.predict_proba(X_test)[:,1]\n        \n        y_preds_proba=y_preds_proba/len(models)\n    \n        \n        predictions=(y_preds_proba>0.5).astype(int)\n    \n        \n        recall.append(recall_score(y_test,predictions))\n        f1.append(f1_score(y_test,predictions))\n        \n        print(f'Cross validation {i} done.')\n        i+=1\n    \n        \n    print(f'Percentage of frauds detected over the 5 folds: {np.round(np.mean(recall),7)}')\n    print(f'F1_score over the 5 folds: {np.round(np.mean(f1),7)}')\n    \n    \n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=cross_val(df, features, n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, Easyensemble improves the performance of my previous logistic regression model (same frauds detection rate but it decreases the number of legitimate transactions predicted as fraudulent by the model (check the f1 score). In large-scale and very noisy datasets, the difference in terms of performance between the easyensemble model and the random under sampler will be much higher. The [conclusion](#5) summarizes why their is not a signficant difference between the performance of the easyensemble algorithm and the simple logistic regression."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<h3 style=\"background-color:yellow;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\"> BalanceCascade from scratch </h3>"},{"metadata":{},"cell_type":"markdown","source":"The algorithm consists of repeating the random under-sampling strategy n times. Each time we create a new subset of data (result of the undersampling), we train a new classifier on it. The only difference between BalanceCascade and EasyEnsemble is that at each iteration, we drop from the training set, a percentage of the correctly classified majority class samples. It enables to reduce the redundant informations in our majority class (ie samples from the majority class that are very easy to classify).At the end, our model is composed of n classifiers. To make a prediction, we simply take the average of the different classifiers. \n\nBasically, I will:\n\n* Randomly undersample the training set.\n* Create and tune the hyperparameters of an xgboost model using optuna on this under-sampled training set.\n* Save my model.\n* Make predictions on all the majority class samples.\n* Drop from the training set the majority class samples that are very easy to predict.\n* Randomly under-sample the new training set another time.\n* create and tune the hyperparameters of a new xgboost model using optuna on this under-sampled training set.\n* Save the model.\n* Make predictions based on the two xgboost models on all my majority class samples.\n* Drop from the training set the majority class samples that are very easy to predict.\n* And so on and so forth while the number of majority class samples in the training set is higher than the number of minority class samples in the training set.\n* At the end, each of my xgboost models will make predictions on the test set.\n* Final prediction is just an average of the predictions of the different xgboost model.\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;font-weight: bold''> BalanceCascade algorithm </span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# balancecascade algorithm\ndef balancecascade(train: pd.DataFrame, features: list, n_estimators: int):\n    \n    \"\"\" Simple implementation of the BalanceCascade algorithm. \n    It takes as input a training set, the differetn features used \n    to train the model, and the number of estimators. The only small \n    trick is that instead of throwing all correctly classified samples \n    of the majority class, it throws a unique percentage of it at each iteration. \n    This process is not random, it throws the easiest samples to predict.\n    Thus, we do not lose all the correctly classified samples.\n    \"\"\"\n    \n    train_maj= train[train.target==0]\n    train_min= train[train.target==1]\n    \n    n_maj= len(train_maj)\n    n_min= len(train_min)\n    \n    ratio=n_min/n_maj\n    \n    keep_rate=np.power(ratio, 1/(n_estimators-1))\n    \n    n_models=0\n    model_list=[]\n    \n    while len(train_maj)>len(train_min):\n        \n        train=pd.concat([train_maj,train_min],axis=0)\n        \n        X_train,y_train=train[features],train.target\n        \n        undersampler=RandomUnderSampler(sampling_strategy=1, random_state=randint(0,10000))\n        X_train_rus,y_train_rus=undersampler.fit_resample(X_train,y_train)\n        \n        model_list.append(create_xgb(X_train_rus, y_train_rus))\n                          \n        y_probs=0\n                          \n        for model in model_list:\n            \n            y_probs+=model.predict_proba(train_maj[features])[:,1]\n        \n        y_probs=y_probs/len(model_list)\n                          \n        train_maj['proba']=y_probs\n                          \n        train_maj=train_maj.sort_values('proba',ascending=False)[:int(keep_rate*len(train_maj)+1)]\n\n        \n        \n    return model_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation function\ndef cross_val(df: pd.DataFrame, features: list, n_estimators: int):\n    \n    \"\"\" Cross validation function. \"\"\"\n    X,y=df[features],df.target\n    \n    kf= StratifiedKFold(n_splits=5)\n    recall=[]\n    f1=[]\n    \n    i=1\n    \n    for train_idx,test_idx in kf.split(X,y):\n        \n        X_train_tuning,y_train_tuning= X.iloc[train_idx],y.iloc[train_idx]\n\n        X_test,y_test= X.iloc[test_idx],y.iloc[test_idx]\n        \n        train=pd.concat([X_train_tuning,y_train_tuning],axis=1)\n    \n        models=balancecascade(train, features, n_estimators)\n    \n\n    \n        y_preds_proba=0\n    \n        for model in models:\n        \n            y_preds_proba+=model.predict_proba(X_test)[:,1]\n        \n        y_preds_proba=y_preds_proba/len(models)\n    \n        \n        predictions=(y_preds_proba>0.5).astype(int)\n    \n        \n        recall.append(recall_score(y_test,predictions))\n        f1.append(f1_score(y_test,predictions))\n        \n        print(f'Cross validation {i} done.')\n        i+=1\n    \n        \n    print(f'Percentage of frauds detected over the 5 folds: {np.round(np.mean(recall),7)}')\n    print(f'F1_score over the 5 folds: {np.round(np.mean(f1),7)}')\n    \n    \n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=cross_val(df, features, n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, **BalanceCascade improves the performance of my previous logistic regression model** (approximately same recall but way better F1-score). It also outperforms Easyensemble in terms of F1-score. When I say outperforms, it is because in real world, we cannot have a model that produces to much false negatives (ie legitimate transactions classified as frauds). In large-scale and very noisy datasets, the difference in terms of performance will be much higher."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style=\"background-color:yellow;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\"> Conclusion </h1>"},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, this dataset is not the best to show the performance of **UnderBagging techniques**. UnderBagging techniques are optimized to tackle highly imbalanced, very noisy, and large-scale datasets. Also, it is very useful when there are overlap between classes. This is not the case in this dataset. **But, there exists many applications of Machine Learning where the dataset corresponds to the caracteristics cited above.**"},{"metadata":{},"cell_type":"markdown","source":"If this kind of notebook is useful for the community, I will make others focusing on more advanced single undersampling techniques and also on oversampling techniques."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}