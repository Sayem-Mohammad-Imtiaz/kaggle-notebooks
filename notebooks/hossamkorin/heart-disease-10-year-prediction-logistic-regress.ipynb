{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  **Importing the Necessary Libraries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/heart-disease-prediction-using-logistic-regression/framingham.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking if we have any missing values\ndf.info()\n#we have 7 columns that contain missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's do a quick pandas profiling report to verify if there are any missing data and help understand what values we\n#should use to fill in any of these missing values\nfrom pandas_profiling import ProfileReport\nrpt=ProfileReport(df)\nrpt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing the missing education values witht the most frequent value\ndf['education'].replace(np.nan, 1.0, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing all other values with either the mean or mode of the coulumn values\ndf['BPMeds'].replace(np.nan, 0.0, inplace= True)\ndf['totChol'].replace(np.nan, df['totChol'].mean(), inplace= True)\ndf['glucose'].replace(np.nan, df['glucose'].mean(), inplace= True)\ndf['cigsPerDay'].replace(np.nan, df['cigsPerDay'].mean(), inplace= True)\ndf['BMI'].replace(np.nan, df['BMI'].mean(), inplace= True)\ndf['heartRate'].replace(np.nan, df['heartRate'].mean(), inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n#the data now contains no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining the predictor and target variables\nx=df.loc[:, df.columns != 'TenYearCHD']\ny=df['TenYearCHD']\nprint(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's take a look at the correlation between the different features\nimport seaborn as sns\ncor=x.corr()\nsns.heatmap(cor, annot=True, cmap='coolwarm')\n#we can tell that there is strong relation between some of our features (cigsPerDay & currentSmoker) and (PrevalentHyp&sysBHP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **There seems to be quite a bit of correlation between some of our features that we may need to decorrelate using Principal Component Analysis:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's try to see if we can reduce the data dimensionality and reduce feature dependencies using PCA\n#standardizaing the features with continuous variables\nscl=StandardScaler()\nx[['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']]=scl.fit_transform(x[['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']])\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's split the data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=1)\n\n#now let's try to perform some PCA to keep only the features that contribute most to our data\npca=PCA(0.95)   #0.95 is the amount of data variance/information that we would like to retain after doing PCA\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(x_train)\npca.n_components_ #we've reduced the number of columns no down to 9 from 15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming our train and test features\nx_train= pca.transform(x_train)\nx_test=pca.transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using the Logistic Regression Model to predict whether a person may get Chronic Heart disease within 10 years","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we're going to try and use Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nLR=LogisticRegression(C=0.001, solver='liblinear')\nLR.fit(x_train, y_train)\nLR.score(x_test, y_test) #good model accuracy with almost 87%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat=LR.predict(x_test)\ny_hat_prob=LR.predict_proba(x_test)\ny_hat_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's also take a look at the confusion matrix to understand the accuracy\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n#this is to define the actual function to plot a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\nprint(confusion_matrix(y_test, y_hat, labels=[1,0]))\ncnf_mtx=confusion_matrix(y_test, y_hat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n#plotting the actual matrix\nplt.figure()\nplot_confusion_matrix(cnf_mtx, classes=['TenYearCHD=1','TenYearCHD=0'],normalize= False,  title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, the built Logistic Regression model provides approximately 87% average accuracy. I am very new to ML, and have taken to the approach of going through some online courses and supplementing what I learn with practice through Kaggle. Therefore, I would appreciate any feedback and constructive criticism on the work I performed.\n\nThanks,\nHossam","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}