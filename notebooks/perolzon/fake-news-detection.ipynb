{"cells":[{"metadata":{"_uuid":"42cb75d0-33cc-4631-bc7c-8af1071ce2a9","_cell_guid":"f337ece3-8d26-4880-9554-230100c7e4c9","trusted":true},"cell_type":"markdown","source":"# Load dataset and label it","execution_count":null},{"metadata":{"_uuid":"c3e5945d-5769-489a-bcb1-57ccc5167cd8","_cell_guid":"199f95b7-a794-429f-ac35-f9a371dbbfd6","trusted":true},"cell_type":"code","source":"import pandas as pd\nfake_data=pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\ntrue_data=pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\n\n#Set labels for the datasets\nfake_data[\"target\"]=0\ntrue_data[\"target\"]=1\n\n#Concat and reindex the data\nall_data=pd.concat([fake_data,true_data],ignore_index=True)\n\n# View the dataset\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d389f3a-dd52-4dfa-9bd9-e472777ac74f","_cell_guid":"9c4baa51-9d7b-432a-9011-d1bdbaa886a1","trusted":true},"cell_type":"markdown","source":"# Study the data","execution_count":null},{"metadata":{"_uuid":"56167d62-c802-4435-9421-e133e6b5cd62","_cell_guid":"de2b41ef-72b2-411f-809e-e44d8dcc38d9","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\ndef preprocess(data):\n    txt = data.text.str.lower().str.cat(sep=' ')\n    words = nltk.tokenize.word_tokenize(txt)\n    words=[word.lower() for word in words if word.isalpha()]\n    \n    alsowordstoremove=[\"t\",\"s\",\"don\",\"wouldn\",\"won\",\"couldn\"]\n    words = [w for w in words if not w in STOP_WORDS] \n    words = [w for w in words if not w in alsowordstoremove]\n    \n    tags=nltk.pos_tag(words)\n    res_chunk = nltk.ne_chunk(tags)\n    return words\n\n\nfiltered_words=preprocess(true_data)\nword_dist = nltk.FreqDist(filtered_words)\ntop_N=30\n\ndf = pd.DataFrame(word_dist.most_common(top_N),\n                        columns=['Word true data', 'Frequency true data'])\n\nfiltered_fake_words=preprocess(fake_data)\nword_dist = nltk.FreqDist(filtered_fake_words)\ndf2=pd.DataFrame(word_dist.most_common(top_N),\n                      columns=['Word fake data','Frequency fake data'])\n\nresult=pd.merge(df,df2,left_index=True, right_index=True)\nprint('All frequencies, not including STOPWORDS:')\nprint('=' * 60)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3116624d-edb1-4a88-bd1b-cd233b12bf52","_cell_guid":"de02f7c2-c41c-4356-9af5-fb34ceb211c0","trusted":true},"cell_type":"markdown","source":"# Create LinearSVC-model to predict fake news","execution_count":null},{"metadata":{"_uuid":"f1c2b9b9-61de-4415-a21b-e53df3fac4ab","_cell_guid":"c8dc8214-0b9a-4e78-b58c-a6e79f6e5569","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\n\n#Using linear SVC to speed up classification\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LinearSVC())])\n\nx_train,x_test,y_train,y_test = train_test_split(all_data['text'], all_data.target, test_size=0.2,\n                                                 random_state=20\n                                                )\n\nmodel = pipe.fit(x_train,y_train)\n\nprediction= model.predict(x_test)\nscore=accuracy_score(y_test,prediction)\nprint(round(score*100,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing if the model predicts wrong if it contains a Reuters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.Series([\"Reuters is a fake news outlet that destroys democratic values.\"])\nprediction = model.predict(test)\nprediction[0]\n# 0 is fake news, while 1 is a true news.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9bfcac-483e-4da4-a7ce-97f3b76e66de","_cell_guid":"98ef4ee6-41af-42dc-8113-1fa2a7b5c1e3","trusted":true},"cell_type":"markdown","source":"It might be that the model are just predicting if \"Reuters\" are a part of the text or not, as seen in the example above. This might be the reason that the model has an accuracy of over 99%","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}