{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Rain in Australia**\n## **Predict next-day rain in Australia**"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing tools\n\nFirstly, lets import all tools we need."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nfrom optuna.samplers import TPESampler\nimport optuna.integration.lightgbm as lightgbm\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory data analysis "},{"metadata":{},"cell_type":"markdown","source":"Before select the models, create some new features or chose them from existing, we need to analysis the data we got. Lets make it consistently."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data reading and primary processing"},{"metadata":{},"cell_type":"markdown","source":"In the follow cells we read the data and subsets them on targets table, which self-explanatory, and on another data, with whom we would treaking.\n\nAlso, before subset we drop all unknown target values and factorize it on 0 if tomorrow none rainy day and 1 otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\ndata.dropna(subset=['RainTomorrow'], inplace=True)\ndata['RainTomorrow'] = pd.factorize(data['RainTomorrow'])[0]\ntarget = data['RainTomorrow']\ndata = data.drop(['RainTomorrow'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create two list with categorical and not features.\n\n***object* - means non digital format**\n\n***float64* - digital**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = ['Location', \n       'WindGustDir',\n       'WindDir9am',\n       'WindDir3pm',\n       'RainToday',\n       'Date']       \ndig = [i for i in list(data) if i not in cat]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further, we will work with *Nan* values."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"percent_nan = data.isna().sum()/data.shape[0]\npercent_nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will change the not a number value in digit columns for their mean, cat - nearest value in the same column."},{"metadata":{"trusted":true},"cell_type":"code","source":"for ind, d in zip(percent_nan.index, percent_nan):\n  if ind in dig and d != 0:\n    data[ind].fillna(data[ind].mean(), inplace=True)\n\ndata.fillna(method='ffill', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our next step is factorize the cat. features list to digit format."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in cat:\n  data['{}_dig'.format(c)] = pd.factorize(data[c])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_dig = ['{}_dig'.format(c) for c in cat]\nfeatures = dig + cat_dig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data visualization"},{"metadata":{},"cell_type":"markdown","source":"To find out insights in our data, lets visualize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[features].hist(figsize=(20, 15), bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, categorical columns mostly had uniform distribution.\n\nDigital columns had nearly bell-shaped distribution, but some of them was heavy-tail."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[features].corrwith(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this matrix we can take information about features with smallest or highest negative/positive correlation with target variable. We may drop som of them, like *'Location_dig', 'Temp9am'* and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[dig])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using this plot, we can see, that some of time based features had a high positive correlation with themselfs. So, we may want drop one of them from each pair."},{"metadata":{},"cell_type":"markdown","source":"I will use cap equal to 0.05. It just heuristic and you may save them in data. Different decision can give different advantages, for example, the fewer features, the higher the models speed and vice versa."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features = []\n\nfor feature, corr in zip(data[features].corrwith(target).index, data[features].corrwith(target).values):\n    if np.abs(corr) < 0.05:\n        drop_features.append(feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Data standartization"},{"metadata":{},"cell_type":"markdown","source":"All machine learning algorithms prefer data with same scale, so lets subtract from each column it mean value and devide by standard deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"for d in dig:\n  data[d] = (data[d] - data[d].mean()) / data[d].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, if we look on new digital columns distribution, we can see, that each mean value is equal to zero and std is equal to unit."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[dig].hist(figsize=(20, 15), bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model selection"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Parameters selection"},{"metadata":{},"cell_type":"markdown","source":"The next step is chose and prepare model for predict target.\n\nFirstly i will take the best algorithms for kaggle competitors - XGBCLassifier()."},{"metadata":{},"cell_type":"markdown","source":"Separate data for train and test subsets. Also i create the new features list without *drop_features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = [feature for feature in features if feature not in drop_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(\n    data[new_features], target, random_state=100, test_size=0.2, shuffle=False) # shuffle False because data is time series.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(seed=123)\nclf.fit(xtrain, ytrain)\nclf.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad for first result. You can compare it with old features list, may be more is better?"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_old, xtest_old, ytrain_old, ytest_old = train_test_split(\n    data[features], target, random_state=100, test_size=0.2, shuffle=False)\n\nclf_old = xgb.XGBClassifier(seed=123)\nclf_old.fit(xtrain_old, ytrain_old)\nclf_old.score(xtest_old, ytest_old)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But i will choose *LGBMClassifier*, cause it more lighter, faster and accurate than *XGBClassifier*."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = LGBMClassifier(random_state=123)\nlgb_model.fit(xtrain, ytrain)\nlgb_model.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also we can go through feature selection procedure. \n\n* Create parameters dictionary\n* Using optuna we choose the better one combination of them"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lightgbm.Dataset(xtrain, ytrain)\nlgb_eval = lightgbm.Dataset(xtest, ytest)\n\ndef create_model(trial):\n    params = {\n            'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'auc',\n            'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 0.5),\n            'max_depth': trial.suggest_int('max_depth', 3, 18),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n            'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n            'random_state': 123\n        }\n    model = LGBMClassifier(**params)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(xtrain, ytrain)\n    preds = model.predict_proba(xtest)[:, 1]\n    score = roc_auc_score(ytest, preds)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampler = TPESampler(seed=123)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=100) # you can increase n_trials number for better result ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = study.best_params \nprint(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_lgb = LGBMClassifier(**params)\nnew_lgb.fit(xtrain, ytrain)\nnew_lgb.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't increase our result by tuning parameters. But difference is really small."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Features selection"},{"metadata":{},"cell_type":"markdown","source":"Later we manually selected the features, which seemed promising for us.\n\nLet's give this job for RFECV algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"select_model = LGBMClassifier(**params)\nselector = RFECV(select_model, step=1, cv=5, verbose=10, min_features_to_select=6)\nselector.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at feature importanse:"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = [new_features[i] for i in range(len(selector.support_)) if selector.support_[i] == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train the model for the last time and see the final result."},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_final, xtest_final, ytrain_final, ytest_final = train_test_split(\n    data[final_features], target, random_state=100, test_size=0.3, shuffle=False)\n\nfinal_lgb = LGBMClassifier(**params)\nfinal_lgb.fit(xtrain_final, ytrain_final)\nfinal_lgb.score(xtest_final, ytest_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, result dose not improve. So we come back to previous features set."},{"metadata":{},"cell_type":"markdown","source":"## 4. Ensemble of models"},{"metadata":{},"cell_type":"markdown","source":"Let's create 3 any different models (of your choice).\n\nAnd list of tupple to them."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = KNeighborsClassifier(n_neighbors=7)\ng = GaussianNB()\nrf = RandomForestClassifier()\n\nestimators = [\n    ('k', k), ('g', g), ('l', final_lgb), ('rf', rf)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check all of these models."},{"metadata":{"trusted":true},"cell_type":"code","source":"k.fit(xtrain, ytrain)\nk.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g.fit(xtrain, ytrain)\ng.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(xtrain, ytrain)\nrf.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further, combine them."},{"metadata":{"trusted":true},"cell_type":"code","source":"vote = VotingClassifier(voting='hard', estimators=estimators)\nvote.fit(xtrain, ytrain)\nvote.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, training ensemble of different models also didnt give improve in this particular problem."},{"metadata":{},"cell_type":"markdown","source":"## 5. Recurrent Neural Network (RNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import models\nfrom keras import layers\nfrom keras.optimizers import Adam\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This in cell for activate check gpu's availability."},{"metadata":{"trusted":true},"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For reccurent nn we need to prepare sequences of data, where k - length of sequence in days."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 30 # for best result, you may increase k value\n\nr_xtrain = [xtrain.iloc[i:i+k] for i in range(xtrain.shape[0]-k-1)]\nr_ytrain = [ytrain.iloc[i+k] for i in range(ytrain.shape[0]-k-1)]\n\nr_xtest = [xtest.iloc[i:i+k] for i in range(xtest.shape[0]-k-1)]\nr_ytest = [ytest.iloc[i+k] for i in range(xtest.shape[0]-k-1)]\n\nr_xtrain = np.array(r_xtrain)\nr_ytrain = np.array(r_ytrain)\n\nr_xtest = np.array(r_xtest)\nr_ytest = np.array(r_ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create a simple 1-layer RNN."},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/cpu:0'):\n    model = models.Sequential()\n\n    model.add(layers.GRU(32, input_shape=(None, r_xtrain.shape[-1]), recurrent_dropout=0.2))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=Adam(amsgrad=True), loss='mse', metrics='accuracy')\n    history = model.fit(x=r_xtrain, \n                        y=r_ytrain,\n                        epochs=15,\n                        validation_data=(r_xtest, r_ytest)\n                        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots the history process."},{"metadata":{"trusted":true},"cell_type":"code","source":"def history_plt(history):\n    loss = history.history['loss']\n    acc = history.history['accuracy']\n    val_loss = history.history['val_loss']\n    val_acc = history.history['val_accuracy']\n\n    epochs = range(1, len(loss) + 1)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].plot(epochs, acc, label='acc', color='b')\n    ax[0].plot(epochs, val_acc, label='val_acc', color='r')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, label='loss', color='b')\n    ax[1].plot(epochs, val_loss, label='val_loss', color='r')\n    ax[1].legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_plt(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(history.history['val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the plots, we can continue train further to achieve better results."},{"metadata":{},"cell_type":"markdown","source":"## 6. Convolutional Neural Network (CNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/cpu:0'):\n    model = models.Sequential()\n\n    model.add(layers.Conv1D(16, 3, input_shape=(None, r_xtrain.shape[-1]), activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.Conv1D(32, 3, activation='relu', kernel_regularizer='l1_l2'))\n\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPool1D())\n    \n    model.add(layers.Conv1D(64, 3, activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.Conv1D(128, 3, activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPool1D())\n    \n    model.add(layers.Dense(1))\n    model.compile(optimizer=Adam(amsgrad=True), loss='mse', metrics='accuracy')\n    history = model.fit(x=r_xtrain, \n                        y=r_ytrain,\n                        epochs=15,\n                        validation_data=(r_xtest, r_ytest)\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_plt(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(history.history['val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In conclusion, I want to say there are many means and opportunities to improve the quality of the model, and each of them is effective in some specific task."},{"metadata":{},"cell_type":"markdown","source":"    * XGBClassifier with all parameters: 0.8624\n    * XGBClassifier without some dropped parameters: 0.8483\n    * LGBMClassifier: 0.8643\n    * LGBMClassifier with parameters tuning: 0.8642\n    * LGBMClassifier with parameters tuning and feature selection: 0.8587\n    * RandomForestClassifier: 0.8607\n    * KNeighborsClassifier: 0.8435\n    * GaussianNB: 0.8399\n    * Ensemble of [RFC, KNC, GNB, LGBM]: 0.8611\n    * Reccurent Neural Network: 0.8127\n    * Convolutional Neural Network: 0.7979"},{"metadata":{},"cell_type":"markdown","source":"### That's all."},{"metadata":{},"cell_type":"markdown","source":"### Good luck to all!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}