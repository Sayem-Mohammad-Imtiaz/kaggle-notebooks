{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predizendo o preço de casas com regressão linear\n\nEste é o primeiro projeto prático utilizando todos os 4 blocos principais do aprendizado de máquina, sendo eles:\n* Dados\n* Modelo\n* Função objetivo\n* Otimização\n\nO objetivo dessa atividade é aplicar esses conceitos em um dataset real de uma competição do Kaggle, onde o desafio é predizer o preço de casas com base nos atributos disponíveis.\n\nNão realizaremos uma análise profunda dos dados, pois o objetivo é aplicar os blocos princiais de aprendizado de máquina.","execution_count":null},{"metadata":{"_uuid":"7528016b50edea21432dc3b56eef8187e135d2d3","_execution_state":"idle","trusted":true},"cell_type":"code","source":"# Importando as bibliotecas necessárias\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom math import sqrt\n\n!pip install -U mxnet-cu101mkl==1.6.0  # updating mxnet to at least v1.6\n!pip install d2l==0.13.2 -f https://d2l.ai/whl.html # installing d2l\n\n\n# Código padrão do kaggle para exibir as bases de dados presentes no ambiente atual\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Carregando o dataset em memória com base no retorno da última célula\ndf_house = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vizualisando o dataset\ndf_house.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Número total de amostras e atributos\ndf_house.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exibindo os tipos de cada atributo\ndf_house.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Amostragem\n\nSeparação dos dados em treino e teste","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_house.drop(['price'],axis =1)\ny = df_house['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removendo atributos com pouca relevância\n\nO atributo Id é um identificador único para cada amostra, logo não possui algum tipo de informação que ajude a predizer o preço da casa.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Número total de amostras: {X_train.shape[0]}\")\nprint(f\"Número total de ids únicos: {X_train['id'].unique().shape[0]}\")\nprint(f\"Existem {X_train.shape[0] - X_train['id'].unique().shape[0]} amostras com ids duplicados\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É possível ver que a primeira amostra duplicada possui os mesmos atributos, porém com preço e data diferente, isso signifca que o preço teve uma variação.\n\nComo apenas uma parcela pequena das casas possuem preços que variam conforme a data, iremos retirar esses dados do conjunto amostral","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated_sample = X_train[ X_train['id'].duplicated() ].iloc[0]\ndf_house[ df_house['id'] == duplicated_sample['id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.DataFrame(y_train).drop(pd.DataFrame(y_train)[pd.DataFrame(X_train)['id'].duplicated()].index)\nX_train = pd.DataFrame(X_train).drop_duplicates(subset='id')\n\nX_train = X_train.drop(['id','date'],axis =1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando se há valores vazios\ndf_house.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_ = min(y_train['price'])\nmax_ = max(y_train['price'])\nx = np.linspace(min_,max_,100)\nmean = np.mean(y_train['price'])\nstd = np.std(y_train['price'])\n\n# For Histogram\nplt.hist(y_train['price'], bins=20, density=True, alpha=0.3, color='b')\ny = norm.pdf(x,mean,std)\n\n# For normal curve\nplt.plot(x,y, color='red')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = df_house.corr()\nprint(correlation_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(correlation_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price & Sqft Living\ndf_house.plot(x='sqft_living',y='price',style = 'o')\nplt.title('Sqft_Living Vs Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nnpx.set_np()\n\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"Construct a Gluon data loader.\"\"\"\n    dataset = gluon.data.ArrayDataset(*data_arrays)\n    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n\n# Normalização do atributo sqft_living\nX_train_simple = X_train['sqft_living'].apply(\n    lambda x: (x - X_train['sqft_living'].mean()) / (X_train['sqft_living'].std()))\n\n# Convertendo para o numpy do mxnet\nX_train_simple = np.array(X_train_simple.values.astype('float32'))\ny_train_simple = np.array(y_train.values.astype('float32'))\n\n# Normalização do atributo sqft_living\nX_test_simple = X_test['sqft_living'].apply(\n    lambda x: (x - X_train['sqft_living'].mean()) / (X_train['sqft_living'].std()))\n\n# Convertendo para o numpy do mxnet\nX_test_simple = X_test_simple.values.reshape(-1,1)\ny_test_simple = y_test.values.reshape(-1,1)\n\nX_test_simple = np.array(X_test_simple.astype('float32'))\ny_test_simple = np.array(y_test_simple.astype('float32'))\n\n# Converter para o numpy do mxnet\n\nbatch_size = 10\ndata_iter = load_array((X_train_simple, y_train_simple), batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo o modelo\nfrom mxnet.gluon import nn\nnet = nn.Sequential()\nnet.add(nn.Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inicializando os pesos\nfrom mxnet import init\nnet.initialize(init.Normal(sigma=0.01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo a função de custo\nfrom mxnet.gluon import loss as gloss\nloss = gloss.L2Loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo o algoritmo de otimização\nfrom mxnet import gluon\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.0003})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Treinamento da rede neural\n\nnum_epochs = 10\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        with autograd.record():\n            l = loss(net(X), y)\n        l.backward()\n        trainer.step(batch_size)\n    l = loss(net(X_train_simple), y_train_simple)\n    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exibir os coeficientes finais\n\nw = net[0].weight.data()\nprint('w', w[0])\nb = net[0].bias.data()\nprint('b', b[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testando o modelo com uma amostra \nx_sample = np.array([X_train_simple[1]])\ny_sample = y_train_simple[1]\nprint(f\"Área livre: {X_train.iloc[1]['sqft_living']}\")\nprint(f\"Valor estimado pelo modelo: {net(x_sample)[0][0]}, valor real: {y_sample[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotar os coeficientes com as features\npred = net(X_train_simple)\nplt.plot(X_train['sqft_living'].values, y_train['price'].values, 'g^', X_train['sqft_living'].values, pred, 'r--')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Próximo desafio\n\nIncluir mais atributos para o treinamento e avaliar o quanto o modelo melhora com mais atributos\n\nRealizar o cálculo de performance (função de custo) no dataset de teste e verificar se o modelo consegue fazer boas predições no dataset de teste.\n\nA regressão linear consegue ajudar solucionar esse problema?\n\nCapítulo 3.4 http://d2l.ai/chapter_linear-networks/softmax-regression.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Testar regressão linear com mais atributos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalização do atributo sqft_living\n\ndef standard_normalization(df, column):\n    return df[column].apply(\n        lambda x: (x - df[column].mean()) / (df[column].std())\n    )\n\ncolumns = ['sqft_living', 'sqft_living15', 'sqft_above', 'bedrooms', 'bathrooms', 'grade']\n\nX_train_simple = X_train[columns]\n\nfor column in columns:\n    X_train_simple[column] = standard_normalization(X_train_simple, column)\n\n# Convertendo para o numpy do mxnet\nX_train_simple = np.array(X_train_simple.values.astype('float32'))\ny_train_simple = np.array(y_train.values.astype('float32'))\n\nX_test_simple = X_test[columns]\n\nfor column in columns:\n    X_test_simple[column] = standard_normalization(X_test_simple, column)\n\nX_test_simple = np.array(X_test_simple.values.astype('float32'))\ny_test_simple = np.array(y_test.values.astype('float32'))\n\n# Converter para o numpy do mxnet\n\nbatch_size = 10\ndata_iter = load_array((X_train_simple, y_train_simple), batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Treinamento da rede neural\n\nnet = nn.Sequential()\nnet.add(nn.Dense(1))\nnet.initialize(init.Normal(sigma=0.01))\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.0003})\n\nnum_epochs = 20\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        with autograd.record():\n            l = loss(net(X), y)\n        l.backward()\n        trainer.step(batch_size)\n    l = loss(net(X_train_simple), y_train_simple)\n    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cálculo do erro total no dataset de teste","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train final loss: {loss(net(X_train_simple), y_train_simple).mean().asnumpy()}\")\nprint(f\"Test final loss: {loss(net(X_test_simple), y_test_simple).mean().asnumpy()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A regressão linear consegue ajudar solucionar esse problema?\n\nSomente com os atributos e abordagens de processamento adotadas, não obtivemos um resultado satisfatório para usar esse modelo em produção.\n\nPara usarmos esse experimento didático em um produto real, teríamos que fazer uma análise exploratória dos dados mais profunda e experimentar diversos modelos para então chegar em uma conclusão final. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}