{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Prediction","metadata":{}},{"cell_type":"markdown","source":"### Goal, Limits and Dataset Content","metadata":{}},{"cell_type":"markdown","source":"**Goal**\n\nThe goal of this notebook is to analyze the heart disease data obtained from [UCI](https://archive.ics.uci.edu/ml/datasets/Heart+Disease), and show which features have the most affect in the occurrence of heart disease.\n\n**Limits**\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date.\n\n**Content**\n\n- age\n- sex\n- chest pain type (4 values)\n- resting blood pressure\n- serum cholestoral in mg/dl\n- fasting blood sugar > 120 mg/dl\n- resting electrocardiographic results (values 0,1,2)\n- maximum heart rate achieved\n- exercise induced angina\n- oldpeak = ST depression induced by exercise relative to rest\n- the slope of the peak exercise ST segment\n- number of major vessels (0-3) colored by flourosopy\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","metadata":{}},{"cell_type":"markdown","source":"## Importing Packages","metadata":{}},{"cell_type":"markdown","source":"We need some packages to read the data which is given as csv file, to visualize and to operate.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overview","metadata":{}},{"cell_type":"code","source":"heart = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndata = heart.copy()\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%']) \\\n    .bar(subset = ['std'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'm creating subsets to make easier to visualize.","metadata":{}},{"cell_type":"code","source":"for col in data.columns:\n    print(\"------------------------------------\")\n    print(\"{}\\n{}\".format(col,data[col].value_counts()))\n    print(\"Unique value counts: \",len(data[col].unique()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\ni = 1\nfor col in data.columns:\n    if len(data[col].unique()) <= 5:\n        plt.subplot(3,3,i)\n        data[col].value_counts().plot.bar()\n        plt.title(col)\n        i = i+1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features and target","metadata":{}},{"cell_type":"code","source":"cols = []\n\nfor col in data.columns:\n    if len(data[col].unique()) >= 5:\n        cols.append(col)\n        \n        \nfig = plt.figure(figsize=(18,12))\ni = 1\nfor col in cols:\n        plt.subplot(2,3,i)\n        sns.histplot(data=data, x=col, hue=\"target\", kde=True)\n        i = i+1\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features, features, target","metadata":{}},{"cell_type":"markdown","source":"Using seaborn's pairplot, not only being able to plot feature/target relations, it's possible to plot all feature relations with each others at once. It's huge but useful. I will use only columns having more than 4 unique values that I created as a list before.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(pd.concat([data[cols], data[\"target\"]], axis=1), hue=\"target\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heart Disease Analysis by Gender","metadata":{}},{"cell_type":"markdown","source":"I suggest [this notebook](https://www.kaggle.com/asimislam/tutorial-python-subplots) if you need help with subplots. The code cell below had gotten from it. Luckily, it's the same dataset.","metadata":{}},{"cell_type":"code","source":"heart_NUM = ['age', 'trestbps', 'thalach', 'oldpeak']\n\n#  plot Numerical Data\na = 4  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(14,22))\n\nfor i in heart_NUM:\n    plt.subplot(a,b,c)\n    plt.xlabel(i)\n    sns.distplot(data[i])\n    c = c+1\n\n    plt.subplot(a,b,c)\n    plt.xlabel(i)\n    plt.boxplot(x=data[i])\n    c = c+1\n\n    plt.subplot(a,b,c)\n    plt.xlabel(i)\n    sns.scatterplot(data=data, x=i, y='chol', hue='sex')\n    c = c+1\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = data[data[\"sex\"] == 0] #female\nm = data[data[\"sex\"] == 1] #male\n\nf_p = f[f[\"target\"] == 1] #female with heart disease\nf_np = f[f[\"target\"] == 0] \n\nm_p = m[m[\"target\"] == 1] #male with heart disease\nm_np = m[m[\"target\"] == 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nplt.subplot(121)\nplt.pie(x=[len(f),len(m)], labels=[\"Female\",\"Male\"], colors=['#009ACD', '#ADD8E6'], autopct='%1.1f%%', startangle=0, pctdistance=1.1,labeldistance=1.25, explode=(0.03,0))\nplt.title(\"Gender distribution of whole dataset\")\nplt.legend(frameon=False, bbox_to_anchor=(1,0.8))\n\nplt.subplot(122)\nplt.pie(x=[len(f_p),len(m_p)], labels=[\"Female\",\"Male\"], colors=['#009ACD', '#ADD8E6'], autopct='%1.1f%%', startangle=0, pctdistance=1.1,labeldistance=1.25, explode=(0.03,0))\nplt.title(\"Gender distribution of patients\")\nplt.legend(frameon=False, bbox_to_anchor=(1,0.8))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"56,4% of people having heart disease is male, but keep in mind that the dataset has more male entries than it has for females. So, how many patients are there in male and female observations?","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nplt.subplot(121)\nplt.pie(x=[len(f_p),len(f_np)], labels=[\"Having Heart Disease\",\"Not having\"], colors=['#b566ff', '#e6ccff'], autopct='%1.1f%%', startangle=0, pctdistance=1.1,labeldistance=1.25, explode=(0.05,0))\nplt.title(\"Female\")\n\nplt.subplot(122)\nplt.pie(x=[len(m_p),len(m_np)], labels=[\"Having Heart Disease\", \"Not having\"], colors=['#80ff80', '#ccffcc'], autopct='%1.1f%%', startangle=0, pctdistance=1.1,labeldistance=1.25, explode=(0.04,0))\nplt.title(\"Male\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix","metadata":{}},{"cell_type":"code","source":"corr = data.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(corr, annot=True, linewidths=.5, cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"I want to use 3 classification methods and compare their scores. The models:\n- SVC\n- Random Forest Classifier\n- Gradient Boosting Classifier\n\nSince, labels are imbalance in dataset StratifiedKFold will be used to get better predictions and reduce overfit/underfit risks.\n\nAlso, feature values have different ranges than each other I will scale data.\n\nTo compare model results accuracy_score and confusion_matrix will help us.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(\"target\", axis=1).values\ny = data[\"target\"].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nmodels = [SVC(), RandomForestClassifier(), GradientBoostingClassifier()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n\nfor train_index, test_index in skf.split(X, y):\n    X_train_fold, X_test_fold = X[train_index], X[test_index]\n    y_train_fold, y_test_fold = y[train_index], y[test_index]\n    acc = []\n    cm = []\n    for model in models:        \n        model.fit(X_train_fold, y_train_fold)\n        pred =  model.predict(X_test_fold)\n        acc.append(accuracy_score(y_test_fold, pred))\n        cm.append(confusion_matrix(y_test_fold, pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = {\"model\": [\"SVC\",\"RandomForestClassifier\",\"GradientBoostingClassifier\"], \"accuracy \": acc}\nresult = pd.DataFrame(score)\nresult","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrices","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/445/1*Z54JgbS4DUwWSknhDCvNTQ.png)","metadata":{}},{"cell_type":"markdown","source":"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix,[9] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa – both variants are found in the literature.[10] The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another).\n\nIt is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table).\n\n1. [Image](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)\n2. [Definiton](https://en.wikipedia.org/wiki/Confusion_matrix)","metadata":{}},{"cell_type":"code","source":"for i,model in enumerate([\"SVC\",\"RandomForestClassifier\",\"GradientBoostingClassifier\"]):\n    sns.heatmap(cm[i], annot=True)\n    plt.title(model)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you find this notebook useful, don't forget to upvote. 👍\nIf you have suggestions, I'm waiting to read them. 🤓","metadata":{}}]}