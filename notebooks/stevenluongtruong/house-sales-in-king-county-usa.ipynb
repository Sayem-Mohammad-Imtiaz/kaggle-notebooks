{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Sales in King County, USA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n\nThis is my first attempt in using simple regression models to predict house prices in King County, USA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### This is a list of features in out dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- <b>id</b> : Unique notation for each house sold (primary key of the dataset)\n\n- <b> date</b>: Date house was sold\n\n\n- <b>price</b>: Price of the house (our prediction target)\n\n\n- <b>bedrooms</b>: Number of bedrooms\n\n\n- <b>bathrooms</b>: Number of bathrooms\n\n- <b>sqft_living</b>: Square footage of the home\n\n- <b>sqft_lot</b>: Square footage of the lot\n\n\n- <b>floors</b> :Total floors (levels) in house\n\n\n- <b>waterfront</b> :House which has a view to a waterfront\n\n\n- <b>view</b>: boolean feature (**True** (1) if the house has been viewed, **False** (0) if the house has not been viewed)\n\n\n- <b>condition</b> :How good the condition is overall\n\n- <b>grade</b>: overall grade given to the housing unit, based on King County grading system\n\n\n- <b>sqft_above</b> : Square footage of house apart from basement\n\n\n- <b>sqft_basement</b>: Square footage of the basement\n\n- <b>yr_built</b> : year the house was built\n\n\n- <b>yr_renovated</b> : Year when the house was renovated\n\n- <b>zipcode</b>: Zip code\n\n\n- <b>lat</b>: Latitude coordinate\n\n- <b>long</b>: Longitude coordinate\n\n- <b>sqft_living15</b> : Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n\n\n- <b>sqft_lot15</b> : LotSize area in 2015(implies-- some renovations)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get an overview of our dataset","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if there is any null value in our dataset\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will add a new feature 'age' to the dataset and remove feature 'yr_built'. We will also drop 'id' and 'date' since these features won't help us predicting the price of houses (our target).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'] = df['yr_built'].max() - df['yr_built']\ndf.drop(df[['yr_built', 'id', 'date']], inplace = True, axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We now look at some basic plots about correlations between the dataset's features and the price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Look at the numbers from the table. It is a real number ranged between -1 to 1. The magnetude of the number close to 1 indicates there is a strong correlation between two variables and the magnetude of the nunber close to 0 indicates there is a weak to none correlation between two variables. Positive sign of the number indicates there is a positive correlation and negative sign indicates negative correlation.\n- We will see it more intuatively with the heatmap below the numeric table.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def corr_heatmap(data):\n    ax = plt.subplots(figsize = (15, 10))\n    sns.heatmap(df.corr(), cmap=\"YlGnBu\")\n    plt.title(\"Correlation between data's features\", fontsize = 20)\ncorr_heatmap(df.corr())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### We now look at few plots between 'price' and other features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (18,14)) # create figure\n\nax0 = fig.add_subplot(221) # add subplot 1 (1 row, 2 columns, first plot) # add_subplot(1, 2, 1)\nax1 = fig.add_subplot(222) # add subplot 2 (1 row, 2 columns, second plot). See tip below** # add_subplot(1, 2, 2)\nax2 = fig.add_subplot(223)\nax3 = fig.add_subplot(224)\n\n# Subplot 1:\nsns.boxplot(x='waterfront',y='price', data=df, ax=ax0)\nax0.set_title('Correlation between waterfront and price')\nax0.set_xlabel('Waterfront')\nax0.set_ylabel('Price')\n\n# Subplot 2:\nsns.boxplot(x='bedrooms',y='price',data=df, ax=ax1)\nax1.set_title('Correlation between number of bedrooms and price')\nax1.set_xlabel('Bedrooms')\nax1.set_ylabel('Price')\n\n#Subplot 3:\nsns.boxplot(x='grade',y='price',data=df, ax=ax2)\nax2.set_title('Correlation between grade and price')\nax2.set_xlabel('Grade')\nax2.set_ylabel('Price')\n\n#Subplot 4:\nsns.boxplot(x='bathrooms',y='price',data=df, ax=ax3)\nax3.set_title('Correlation between bathrooms and price')\nax3.set_xlabel('Bathrooms')\nax3.set_ylabel('Price')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":false}},"cell_type":"markdown","source":"#### I have a few observations based on the boxplots above:\n- It's clearly that houses with waterfront view is more valueable than the ones without.\n- There is a weakly positive correlation between number of bedrooms and the price. The plot shows there are quite many outliers between these two features.\n- There is a positive correlation between grade and price. This is understandable since the grade of the house determines how valueable the house is.\n- Interestingly, the correlation between number of bathrooms and price is a bit stronger than the correlation between number of bedrooms and the price. Even though there are still some outliers, we still see the positive relationship between these two features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Let's look at few more plots to see the relationships between 'price' and other features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (18,14)) # create figure\n\nax0 = fig.add_subplot(221) # add subplot 1 (1 row, 2 columns, first plot) # add_subplot(1, 2, 1)\nax1 = fig.add_subplot(222) # add subplot 2 (1 row, 2 columns, second plot). See tip below** # add_subplot(1, 2, 2)\nax2 = fig.add_subplot(223)\nax3 = fig.add_subplot(224)\n\n# Subplot 1 (regression plot) : \nsns.regplot(x='sqft_living',y='price', data=df, ax=ax0)\nax0.set_title('Correlation between sqft living and price')\nax0.set_xlabel('sqft living')\nax0.set_ylabel('Price')\n\n# Subplot 2 (Scatter plot) :\nsns.scatterplot(x='sqft_lot',y='price',data=df, ax=ax1)\nax1.set_title('Correlation between sqft lot and price')\nax1.set_xlabel('sqft lot')\nax1.set_ylabel('Price')\n\n# Subplot 3 (Scatter plot) :\nsns.scatterplot(x='age',y='price',data=df, ax=ax2)\nax2.set_title('Correlation between age and price')\nax2.set_xlabel('Age')\nax2.set_ylabel('Price')\n\n# Subplot 4 (Regression plot) :\nsns.regplot(x='sqft_above',y='price',data=df, ax=ax3)\nax3.set_title('Correlation between sqft above and price')\nax3.set_xlabel('Sqft above')\nax3.set_ylabel('Price')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I have few more observations based on the plots above:\n- We can see there are positive correlations between sqft living, sft above with price. The regression lines show us that.\n- We have nothing based on sqft lot and age. The plots scatter all over the place.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n> We can use the Pandas method <code>corr()</code>  to find the features other than price that is most correlated with price. Then we plot this table to show it more clearly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new dataframe df_temp with no feature 'long', 'zipcode', 'age', 'lat' : \ndf_temp = df.drop(df[['long','lat', 'zipcode', 'age']], axis = 1)\n\n# Sorting the data and reset the index\ndf_corr = df_temp.corr().iloc[0,:].sort_values(ascending=False).reset_index()\n\ndf_corr = df_corr[df_corr[\"price\"] < 1]  # Eliminate the 'price' in the x-axis\n\nplt.figure(figsize=(15,9))\nsns.barplot(x = \"index\", y = \"price\", data = df_corr);\nplt.xlabel(\"Other features\")\nplt.xticks(rotation = 90)\nplt.title(\"Correlation with price\");\nplt.grid(True)\nsns.set_style(\"dark\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"floor_df = df.floors.value_counts()\nfloor_df.rename_axis('floor value', inplace = True)\nfloor_df_t = floor_df.transpose()\nfloor_df_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncolors_list = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'lightgreen', 'pink']\nexplode_list = [0.12, 0, 0, 0, 0.05, 0.15] # ratio for each continent with which to offset each wedge.\n\nfloor_df.plot(kind='pie',\n                            figsize=(15, 6),\n                            autopct='%1.1f%%', \n                            startangle=90,\n                              \n                            shadow=True,       \n                            labels=None,         # turn off labels on pie chart\n                            pctdistance=1.15,    # the ratio between the center of each pie slice and the start of the text generated by autopct \n                            colors=colors_list,  # add custom colors\n                            explode=explode_list # 'explode' lowest 3 floor values\n                            )\n\n# scale the title up by 12% to match pctdistance\nplt.title('The distribution of floor values', y=1.12) \n\nplt.axis('equal') \n\n# add legend\nplt.legend(labels=floor_df.index, loc='upper left') \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we import folium for the tasks\nimport folium\nfrom folium.plugins import FastMarkerCluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_geo = df[[\"zipcode\" , \"lat\" , \"long\"]]\n\nlat = df_geo[\"lat\"]\nlong = df_geo[\"long\"]\n\ncordinates = list(zip(lat,long))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"king_map = folium.Map(location = [47.5480, -122.257], zoom_start = 10)\n\nFastMarkerCluster(data=cordinates).add_to(king_map)\n\nking_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model developments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  Simple Linear Regression and Multiple Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Let's try simple linear regression model and calculate R^2 between 'price' and other features","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"Y = df['price']\n\nlm1 = LinearRegression()\nX1 = df[['sqft_living']]\nlm1.fit(X1,Y)\nprint('The R^2 score between \\'sqft_living\\' and \\'price\\' is ' + str(lm1.score(X1,Y)))\n\nlm2 = LinearRegression()\nX2 = df[['bathrooms']]\nlm2.fit(X2,Y)\nprint('The R^2 score between \\'bathrooms\\' and \\'price\\' is ' + str(lm2.score(X2,Y)))\n\nlm3 = LinearRegression()\nX3 = df[['grade']]\nlm3.fit(X3,Y)\nprint('The R^2 score between \\'grade\\' and \\'price\\' is ' + str(lm3.score(X2,Y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Let's plot the actual values of price againts the values predicted","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"width = 10\nheight = 8\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(lm1.predict(X1), hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n#We can adjust histogram True or False if we want to include the histogram along with the distribution lines\n\nplt.title('Actual vs Fitted Values for Price based on sqft_living')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of houses')\n\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"width = 10\nheight = 8\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(lm2.predict(X2), hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n#We can adjust histogram True or False if we want to include the histogram along with the distribution lines\n\nplt.title('Actual vs Fitted Values for Price based on bathrooms')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of houses')\n\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"width = 10\nheight = 8\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(lm3.predict(X3), hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n#We can adjust histogram True or False if we want to include the histogram along with the distribution lines\n\nplt.title('Actual vs Fitted Values for Price based on grade')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of houses')\n\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We now use multiple linear regression to predict the 'price' using list of features:","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"features =[\"floors\", \"waterfront\",\"lat\" ,\"bedrooms\" ,\"sqft_basement\" ,\"view\" ,\"bathrooms\",\"sqft_living15\",\"sqft_above\",\"grade\",\"sqft_living\"]","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"lm4 = LinearRegression()\nX4 = df[features]\nX4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm4.fit(X4,Y)\nprint('The R^2 score between these features and \\'price\\' is ' + str(lm4.score(X4,Y)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yhat_mult = lm4.predict(df[features])\nYhat_mult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"><p>Data Pipelines simplify the steps of processing the data. We use the module <b>Pipeline</b> to create a pipeline. We also use <b>StandardScaler</b> as a step in our pipeline.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model', LinearRegression())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe=Pipeline(Input)\npipe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = df[features]\nnew_features\npipe.fit(new_features,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypipe=pipe.predict(new_features)\nypipe[0:4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation and Refinement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import the necessary modules:","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will split the data into training and testing sets:","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"features =[\"floors\", \"waterfront\",\"lat\" ,\"bedrooms\" ,\"sqft_basement\" ,\"view\" ,\"bathrooms\",\"sqft_living15\",\"sqft_above\",\"grade\",\"sqft_living\"]    \nX = df[features]\nY = df['price']\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n\n\nprint(\"number of test samples:\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"RidgeModel = Ridge(alpha = 0.1)\nRidgeModel.fit(x_train,y_train)\nRidgeModel.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n> We now perform a second order polynomial transform on both the training data and testing data. Create and fit a Ridge regression object using the training data, set the regularisation parameter to 0.1, and calculate the R^2 utilising the test data provided.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"pr = PolynomialFeatures(degree=2)\nx_train_pr = pr.fit_transform(x_train[features])\nx_test_pr = pr.fit_transform(x_test[features])\n\nRidgeModel.fit(x_train_pr,y_train)\nRidgeModel.score(x_test_pr,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}