{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Bangaluru Housing Price - Regression Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing other necessary libraries.\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nmatplotlib.rcParams[\"figure.figsize\"] = (22,12)\nimport seaborn as sns\nsns.style=\"darkgrid\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset and storing it into the dataframe df.\ndf = pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')\npd.options.display.max_columns = 30","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About the Dataset\n\nWhat are the things that a potential home buyer considers before purchasing a house? The location, the size of the property, vicinity to offices, schools, parks, restaurants, hospitals or the stereotypical white picket fence? What about the most important factor — the price?\n\nNow with the lingering impact of demonetization, the enforcement of the Real Estate (Regulation and Development) Act (RERA), and the lack of trust in property developers in the city, housing units sold across India in 2017 dropped by 7 percent. In fact, the property prices in Bengaluru fell by almost 5 percent in the second half of 2017, said a study published by property consultancy Knight Frank.\nFor example, for a potential homeowner, over 9,000 apartment projects and flats for sale are available in the range of ₹42-52 lakh, followed by over 7,100 apartments that are in the ₹52-62 lakh budget segment, says a report by property website Makaan. According to the study, there are over 5,000 projects in the ₹15-25 lakh budget segment followed by those in the ₹34-43 lakh budget category.\n\nBuying a home, especially in a city like Bengaluru, is a tricky choice. While the major factors are usually the same for all metros, there are others to be considered for the Silicon Valley of India. With its help millennial crowd, vibrant culture, great climate and a slew of job opportunities, it is difficult to ascertain the price of a house in Bengaluru.\n\n### Reference Kaggle Description of - https://www.kaggle.com/amitabhajoy/bengaluru-house-price-data"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Cleaning the Data.\n\nEDA is the most crucial part. It's the first look at our data with the help of data visualization and descriptive statistics. We need to do it carefully in order to identify and eliminate outliers and missing values. Data Cleaning goes hands in hand with EDA. We'll proceed with our EDA in the upcoming cell blocks."},{"metadata":{},"cell_type":"markdown","source":"The very first steps are getting an overall view of our DataFrame. We do it by calling the .info() and .describe() methods on our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Info of our dataframe.\n# Describing our Dataset\ndf.info()\n# We get to see the column names along with the number of non-null values and their coressponding datatypes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick data description.\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checcking for null values. The society column has a lot of null values. Other one is Balcony.\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the head of the dataframe.\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initial Shape of the DataFrame.\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the column names for the dataframe.\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['area_type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['area_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['availability'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['availability'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['size'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y=\"size\", data=df)\n# Have a quick look at the current dataset and the counts of the various categories listed in the size column.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['society'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total_sqft'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bath'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bath'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"bath\", data=df)\n# Bath is set as float we need to convert it to string. Nevertheless we find the mode is in 2bathrooms.\n# There are some big outliers and need to be taken care of.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['balcony'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['balcony'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"balcony\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are dropping the \nsociety column - too many unique values compared to dataset. No problem if we drop it.\navailability column - too many unique values compared to dataset. No problem if we drop it.\narea_type - No use for our analysis.\nbalcony Not much use for our analysis. Might consider in case of different results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping non-essential columns from the dataframe and storing it in df1. Keeping the original dataframe intact.\ndf1=df.drop(['society','availability','area_type','balcony'],axis = 'columns')\ndf1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Null values in the existing data in the dataframe.\ndf1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since very few NA values we are simply dropping those values from our data.\ndf2=df1.dropna()\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cheking shape after dropping nulls.\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Unique values in the size Column.\ndf2['size'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new column 'BHK' by extracting essential information about BHK in the 'size' column.\n# THis is fr easier analysis of critical information from our dataset.\ndf2['BHK']=df2['size'].apply(lambda x : int(x.split()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Head of the new dataframe.\ndf2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the unique values in the 'BHK' column.\n# With just a look we are able to see big outliers such as 43 bedrooms and 27 bedrooms. Might be anomalous if not also\n# These are outliers.\ndf2['BHK'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for those entries which have more than 10 bedrooms. Very few of them.\ndf2[df2.BHK>10].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just 12 entries have 10 or more bedrooms. Very small proportion of our dataset. Can be easily dropped.\nlen(df2[df2.BHK>10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.total_sqft.unique()\n# The array of unique values in the total_sqft column.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.total_sqft.nunique()\n# Number of unique values in the total_sqft column.\n# Huge number of square foot values. We can replace them with some measure of central tendency like mean of the ranged extremes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functon for identifying proper float values and tackling with improper float values.\ndef is_float(num):\n    try:\n        float(num)\n    except:\n        return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tackling those values that aren't a proper float type number i.e uncleaned data with values as range or different unit\n#data, misentered data etc.\n#Incoming data that is unstructured.\n\ndf2[~df2['total_sqft'].apply(is_float)].head(10)\n\n# We see many range values in the output, so we have to replace these range values with their average in order to make them proper\n# float type and also usable.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see there are 173 such ranged entries. We need to tackle these.\ndf2[~df2['total_sqft'].apply(is_float)]['total_sqft'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#User Defined Function that takes range value and returns the average of the range. This will be used in the total square_feet column in order\n#Convert the range of square feets in the the average of the two.\n\ndef convertion_of_square_feet_to_numerical_values(sqft):\n    tokens = sqft.split('-')   #Splits the characters on '-'.\n    if len(tokens)==2: \n        return (float((float(tokens[0])+float(tokens[1]))/2))\n    try:\n        return float(sqft)\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the Function - 1\nconvertion_of_square_feet_to_numerical_values('1156 - 2278') # We give range as an Input and get float as an output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the Function - 2\nconvertion_of_square_feet_to_numerical_values('1015 - 1510')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the Function - 3\nconvertion_of_square_feet_to_numerical_values('3090 - 5002')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying the function to the copy of the above dataframe denoted by df3. And then displaying it's head.\ndf3=df2.copy()\ndf3['total_sqft'] = df3['total_sqft'].apply(convertion_of_square_feet_to_numerical_values)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Observing a few entries inside the dataframe and displaying it as a sub-dataframe\n# The 30th, 78th and 122nd examples are displayed.\n# .loc() function helps us in accessing row wise examples from our dataframe.\n\ndf3.loc[[30, 378, 1322]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering Begins\nFeature Engineering is a process of using domain knowledge, skills and experiences to extract useful data from raw data in the data mining process. It's very essential for improvement in the process of our machine learning algorithms. It is called applied Machine Learning in some places.\n\nProcess of Feature Engineering - \n\n1. Brainstroming on the testing features.\n2. Deciding what features we should create and what we should omit.\n3. Creating new features.\n4. Checking how the features might work with our model.\n5. Improving the existing features.\n6. Iteratively follow the above process till satisfactory results are reached."},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price per squarefoot is a very important feature in Real Estate business.\n# We have to remove outliers based on invalid Price per square foot data.\n# The Prices are in lakhs so multiplying by 100000(1Lakh).\n\ndf4=df3.copy()\ndf4['Price_Per_sqft'] = df4['price']*100000/df4['total_sqft']\ndf4.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with Dimensionality Curse"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimensionality Curse\nlen(df4.location.unique()) # Unique values of Location","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.location = df4.location.apply(lambda x : x.strip()) #Strips external spaces in the location data\nlocation_stats = df4.groupby('location')['location'].agg('count').sort_values(ascending = False)\nlocation_stats\n# We display in ascending order and find there are many locations in the very end having only single entries.\n# We need to combine them under one location name and deal with sparse data and reduce complexity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Any Location which has less than 10 houses call it other location. Putting them under the same Umbrella.\nlocation_stats_less_than_10 = location_stats[location_stats<=10]\nlocation_stats_less_than_10\n#1052 locations have less than 10 houses among 1293 entries in location. Check the length of the series below.\n# This is a huge amount and needs to be death with to increase efficiency of our model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Current Number of Unique entries without replacement.\na = len(df4.location.unique())\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unique entries after replacing the locations having less than 10 entries as Other Location\ndf4.location = df4.location.apply(lambda location : 'Other Location' if location in location_stats_less_than_10 else location)\nb = len(df4.location.unique())\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of locations combined under 'Other Location' Column\nprint(a - b)\n# 1051 is a big number considering the total number 1293. These 1051 locations had only single entry values and had to be combined.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df4.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outlier Reduction and Removal : Application of Domain Knowledge.\n#### Domain Knowledge = Typical square_foot per bedroom = 300(minimum) : will help in removal of outliers and unusual datapoints. Well omit those values which dosen't satisfy above criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking what all entries have less rhan 300 sqft per bedroom. 300 is our base limit.\ndf4[df4.total_sqft/df4.BHK < 300].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examples of erroneous data - \n\n1. Eg 1. 1000sqft home with 6 bedrooms, 600sqft with 8 bedrooms, very unusual data. So needs to be removed.\n2. Eg 2. 1407sqft home with 6 bedrooms, 500sqft with 3 bedrooms, very unusual data. So needs to be removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Current Shape of our data frame.\ndf4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a new dataframe df5 and removing all the unual data points which give erroneous data for sqft per bedroom.\ndf5 = df4[~(df4.total_sqft/df4.BHK < 300)]\ndf5.shape\n#Shape after removal of certain erroneous datapoints.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total number of unusual datapoints removed\ndf4.shape[0] - df5.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Detecting anomalies based on price per squarefoot\ndf5.Price_Per_sqft.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_df = df5.groupby('location').describe()\ndescribe_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of mean value of Total Sqft grouped by location.\nplt.hist(describe_df['total_sqft']['mean'],bins = 40, color = 'skyblue', density = True )\nplt.xlabel('Square foot', size = 15)\nplt.ylabel('Count', size = 15)\n# Observation - Maximum flats are in the range of 1200 - 1400 sqft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of mean value of BHK grouped by location.\nplt.hist(describe_df['BHK']['mean'],bins = 40, color = 'olivedrab', density = True )\nplt.xlabel('BHK', size = 15)\nplt.ylabel('Count', size = 15)\n# Observation - 2 to 3 BHK flats dominate the distribution.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of mean value of Price Per sqft grouped by location.\nplt.hist(describe_df['Price_Per_sqft']['mean'],bins = 50, color = 'steelblue' )\nplt.xlabel('Price per sqft', size = 15)\nplt.ylabel('Count', size = 15)\n# Observation - Average Price per square foot is in the range of Rs. 5000 to Rs. 6000.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of mean value of baths grouped by location.\nplt.hist(describe_df['bath']['mean'],bins = 40, color = 'yellowgreen' )\nplt.xlabel('Bathrooms', size = 15)\nplt.ylabel('Count', size = 15)\n# Observation - Maximum households have 2 to 3 bathrooms.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering out price per squarefoot data.\n# Keeping only those data that lies within the Mean and one Standard Deviation on either side.\n# These outliers has to be removed.\n\ndef remove_price_per_square_foot_outlier(df):\n    df_out = pd.DataFrame()\n    for key,sub_df in df.groupby('location'):\n        \n        m = np.mean(sub_df.Price_Per_sqft) #Mean of Price Per Square foot column.\n        sd  =np.std(sub_df.Price_Per_sqft) #Standard Deviation of Price Per Square foot column.\n        reduced_df = sub_df[(sub_df.Price_Per_sqft>(m-sd)) & (sub_df.Price_Per_sqft<=(m+sd))] \n        # Keeping those values that are in the range of 1 SD from the mean.\n        df_out = pd.concat([df_out,reduced_df], ignore_index = True)\n        \n    return df_out\ndf6 = remove_price_per_square_foot_outlier(df5)\ndf6.shape #Displaying the Shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outlier data removed, removing data beyond one standard deviation.\ndf5.shape[0] - df6.shape[0]\n# Below we see the number of datapoints removed. We can change and include more datapoints.\n# Say till 1.5 to 2 SD but for my analysis I have considered only 1 SD.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Certain Data Points have same sqft 3BHK flats priced less than 2BHK flats. These are outliers and have to be removed.\n# BHK is an important criterion for determining flat price. \n# For same sqft if number of BHK is more the on with more BHK will have more price.\n# We'll do it location wise and later apply on the entire dataframe locationwise and observe the desired changes.\n\ndef plot_scatter_chart(df,location):\n    bhk2 = df[(df.location==location) & (df.BHK==2)]  #Finding 2BHK flats\n    bhk3 = df[(df.location==location) & (df.BHK==3)]  #Finding 3BHK flats\n    matplotlib.rcParams['figure.figsize'] = (15,10)\n    \n    plt.scatter(bhk2.total_sqft,bhk2.price,color='midnightblue',label='2 BHK', s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='seagreen',label='3 BHK', s=50)\n    \n    plt.xlabel(\"Total Square Feet Area\")\n    plt.ylabel(\"Price (Lakh Indian Rupees)\")\n    plt.title(location)\n    plt.legend()\n    \nplot_scatter_chart(df6,\"Rajaji Nagar\") #Plotting the scatterplot for Rajaji Nagar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Hebbal\nplot_scatter_chart(df6,\"Hebbal\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Yelahanka\nplot_scatter_chart(df6,\"Yelahanka\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df6['location'].value_counts().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Whitefield\nplot_scatter_chart(df6,\"Whitefield\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Electronic City\nplot_scatter_chart(df6,\"Electronic City\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2BHK flat should have price lesser than 3BHK of same sqft.\n#Filtering out all those values of say 2BHK whose price per squarefoot is less than 1BHK.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_bhk_outliers(df):\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('BHK'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.Price_Per_sqft),\n                'std': np.std(bhk_df.Price_Per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('BHK'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.Price_Per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\ndf7 = remove_bhk_outliers(df6)\n#df8 = df7.copy()\ndf7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total number of anomalies removes\ndf6.shape[0] - df7.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter_chart(df7,\"Rajaji Nagar\") #Plotting the scatterplot for Rajaji Nagar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Hebbal\nplot_scatter_chart(df7,\"Hebbal\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Yelahanka\nplot_scatter_chart(df7,\"Yelahanka\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Whitefield\nplot_scatter_chart(df6,\"Whitefield\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the same for Electronic City\nplot_scatter_chart(df6,\"Electronic City\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Observing the cleaned Price Per Square Foot count distribution.\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nplt.hist(df7.Price_Per_sqft,rwidth=0.8)\nplt.xlabel(\"Price Per Square Feet\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing bathroom specific outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df7.bath.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df7[df7['bath']>10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df7.bath,rwidth=0.8)\nplt.xlabel(\"Number of bathrooms\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Domain Knowledge : Any time we have bathrooms greater than bedroom+2 remove those outliers.\ndf7[df7.bath>df7.BHK+2]\n# Eg1. 4Bedrom 7 bathroms -  outlier. unusual\n# Eg1. 6 bedrooms and 9 bathrooms - outlier. unusual.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df8 = df7[df7.bath<df7.BHK+2]\ndf8.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping other unnecessary features for model. Remain only with location, total_sqft, bath, price and BHK.\ndf10 = df8.drop(['size','Price_Per_sqft'],axis='columns')\ndf10.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Location is a categorical column, need to convert it to a numerical using one hot encoding ---> dummies.\n#Using pd.get_dummies() method.\ndummies = pd.get_dummies(df10.location)\ndummies.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rid of the dummy variabe trap. Dropping the categorical column for 'Other Location' type.\ndf11 = pd.concat([df10,dummies.drop('Other Location',axis ='columns')],axis ='columns')\ndf11.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replaced location by one hot encoding. Now we can drop location column now as we have it's encoded columns.\ndf12 = df11.drop('location',axis= 'columns')\ndf12.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking shape of X.\ndf12.shape\n# We have 7251 rows and 245 columns respectively after encoding.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X is our independent variables. Price is what we want to predict based on total_sqft, bath and BHK.\nX = df12.drop(['price'],axis='columns')\nX.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of Independent feature dataframe\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y is our dependent variables. Dependent on location, total_sqft, BHK and bath.\ny = df12.price\ny.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking length of y.\nlen(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Testing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting our dataset in training and testing dataset using sklearn's train_test_split method.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing sklearn's Linear Regression model and fitting and training the model. Testing the same model and displaying the score.\nfrom sklearn.linear_model import LinearRegression\nlr_clf = LinearRegression()\nlr_clf.fit(X_train,y_train)\nlr_clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n\ncross_val_score(LinearRegression(), X, y, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identifying the best Model. Comapring Linear Regression, Lasso and DT Regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(42)\n# Best Model Selection using GridSearch CV\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef find_best_model_using_gridsearchcv(X,y):\n    algos = {\n        'Linear Regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n            }\n        },\n        'Lasso': {\n            'model': Lasso(),\n            'params': {\n                'alpha': [1,2],\n                'selection': ['random', 'cyclic']\n            }\n        },\n        'Decision Tree': {\n            'model': DecisionTreeRegressor(),\n            'params': {\n                'criterion' : ['mse','friedman_mse'],\n                'splitter': ['best','random']\n                \n            }\n        },\n        'KNN Regression' :{\n            'model' : KNeighborsRegressor(),\n            'params' : {\n                'n_neighbors' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n                'weights' : ['uniform', 'distance']\n            }\n        }   \n        }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    for algo_name, config in algos.items():\n        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'Model': algo_name,\n            'Best_score': gs.best_score_,\n            'Best_params': gs.best_params_\n        })\n\n    return pd.DataFrame(scores,columns=['Model','Best_score','Best_params'])\n\nfind_best_model_using_gridsearchcv(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict price function that predicts flat price based on location, sqft, bath and bhk using our model's prediction.\ndef predict_price(location,sqft,bath,bhk):    \n    loc_index = np.where(X.columns==location)[0][0]\n\n    x = np.zeros(len(X.columns))\n    x[0] = sqft\n    x[1] = bath\n    x[2] = bhk\n    if loc_index >= 0:\n        x[loc_index] = 1\n\n    return lr_clf.predict([x])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Dataframe\ndf12.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting some flat values - based on Location, Sqft, BHK and Bathrooms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('1st Phase JP Nagar',1000, 2, 2),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('1st Phase JP Nagar',2000, 3, 3),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('Indira Nagar',2000, 3, 3),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('Indira Nagar',2500, 3, 3),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('5th Block Hbr Layout',8000, 6, 4),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('5th Block Hbr Layout',3500, 4, 3),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('Varthur',1600, 3, 2),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('Vishveshwarya Layout',2000, 3, 3),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('Whitefield',1500, 2, 2),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(predict_price('6th Phase JP Nagar',1250, 3, 2),2),'Lakhs')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nAs always said in the world of data science. Data Preparation is the most vital, crucial and tedious step for any project. Identifying the type of problem and later applying suitable models might be bit easier with a plethora of functionalities available with standard libraries. Yet domain knowledge plays a very vital role in scaling up these tasks. I had a wonderful data preparation experience with this real world dataset and will use the knowledge gained in my upcoming projects as well. Data preparation and EDA is the major part. Then comes understanding the models and getting the most optimum model out of them all.\nThank You,\nRegards\nAbhilash Jash"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}