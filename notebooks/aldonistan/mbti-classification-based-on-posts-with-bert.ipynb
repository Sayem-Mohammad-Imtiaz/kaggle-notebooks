{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **INTRODUCTIONS**\nMBTI is a personality  Myers Briggs Type Indicator is a personality type system that divides personalities into 16 distinct  types across 4 axis:\n![](http://www.allkpop.com/upload/2020/03/content/051754/1583448868-chart.png)","metadata":{}},{"cell_type":"markdown","source":"Where each person will have a personality abbreviated from the combination of all 4 axis i.e. someone who is introverted, relies more on intuitions, feeling, and perceive rather than judge will be be labelled as an INFP.\nThe combinations of all these axis creates a more complex traits, strengths, and weaknesses.\n\n![](https://yassinetounsi.com/storage/2021/05/MBTI-Types.jpg)\n\nIt is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. Itâ€™s safe to say that this test is still very relevant in the world in terms of its use.","metadata":{}},{"cell_type":"markdown","source":"**==================================================================================================================================================**\n**==================================================================================================================================================**","metadata":{}},{"cell_type":"markdown","source":"# **Data Pipeline**","metadata":{}},{"cell_type":"code","source":"#Import the necessary libraries for data prep and viz\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nimport regex as re\nimport transformers\nfrom keras import backend as K\nimport plotly.express as px\nimport imageio\nfrom wordcloud import WordCloud, STOPWORDS\n\ndata=pd.read_csv('../input/mbti-type/mbti_1.csv')\ndata.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T05:47:41.709923Z","iopub.execute_input":"2021-08-20T05:47:41.710299Z","iopub.status.idle":"2021-08-20T05:47:42.398806Z","shell.execute_reply.started":"2021-08-20T05:47:41.710265Z","shell.execute_reply":"2021-08-20T05:47:42.397697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if TPU is available\nuse_tpu = True\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T05:47:42.400125Z","iopub.execute_input":"2021-08-20T05:47:42.400377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.pie(data,names='type',title='Distribution of personality types',hole=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data is extremely imbalanced.** This might cause overfitting to happen since the total data only amounts to 8675.\nBut first, let's see the most significant words in each personality type!","metadata":{}},{"cell_type":"code","source":"\nfig, ax = plt.subplots(len(data['type'].unique()), sharex=True, figsize=(15,10*len(data['type'].unique())))\n\nk = 0\nfor i in data['type'].unique():\n    df_4 = data[data['type'] == i]\n    wordcloud = WordCloud().generate(df_4['posts'].to_string())\n    ax[k].imshow(wordcloud)\n    ax[k].set_title(i)\n    ax[k].axis(\"off\")\n    k+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['type'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n**This dataset contains quite a lot of URLs and symbols, so let's go ahead and remove those.**","metadata":{}},{"cell_type":"code","source":"def clean_text(data):\n    data_length=[]\n    lemmatizer=WordNetLemmatizer()\n    cleaned_text=[]\n    for sentence in tqdm(data.posts):\n        sentence=sentence.lower()\n        \n        #removing links from text data\n        sentence=re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n    \n        #removing other symbols\n        sentence=re.sub('[^0-9a-z]',' ',sentence)\n    \n        \n        data_length.append(len(sentence.split()))\n        cleaned_text.append(sentence)\n    return cleaned_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.posts = clean_text(data)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initialize BERT Tokenizer and attention masks******","metadata":{}},{"cell_type":"code","source":"#Split dataset\nfrom sklearn.model_selection import train_test_split\n\nposts = data['posts'].values\nlabels =  data['type'].values\ntrain_data, test_data = train_test_split(data, random_state=0, test_size=0.2)\n\ntrain_size = len(train_data)\ntest_size = len(test_data)\ntrain_size, test_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize Bert tokenizer and masks\nfrom transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 1800\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 1800):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n        \n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ntrain_input_ids = tokenize_sentences(train_data['posts'], tokenizer, MAX_LEN)\ntrain_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntrain_attention_masks = create_attention_masks(train_input_ids)\n\ntest_input_ids = tokenize_sentences(test_data['posts'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_masks,test_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create train and test datasets\nBATCH_SIZE=32 \nNR_EPOCHS=20\n#def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n#    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n#    if train:\n#        dataset = dataset.shuffle(buffer_size=buffer_size)\n#    dataset = dataset.repeat(epochs)\n#    dataset = dataset.batch(batch_size)\n#    if train:\n#        dataset = dataset.prefetch(1)\n    \n #   return dataset\n\n#train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n#test_dataset = create_dataset((test_inputs, test_masks, test_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Model\n* Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 16 neurons and softmax activation","metadata":{}},{"cell_type":"code","source":"\n#from transformers import TFBertModel\n\n#from tensorflow.keras.layers import Dense, Flatten\n\n#class BertClassifier(tf.keras.Model):    \n#        def __init__(self, bert: TFBertModel, num_classes: int):\n#            super().__init__()\n#            self.bert = bert\n#            self.classifier = Dense(16, activation='softmax')\n\n#        @tf.function\n#        def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n#            outputs = self.bert(input_ids,\n#                                   attention_mask=attention_mask,\n#                                   token_type_ids=token_type_ids,\n#                                   position_ids=position_ids,\n#                                   head_mask=head_mask)\n#            cls_output = outputs[1]\n#            cls_output = self.classifier(cls_output)\n\n#            return cls_output\n        \n        \n#with strategy.scope():        \n#    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define f1 functions for evaluation\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(): \n    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32,\n                                           name=\"input_word_ids\")\n    bert_layer = transformers.TFBertModel.from_pretrained('bert-large-uncased')\n    bert_outputs = bert_layer(input_word_ids)[0]\n    pred = tf.keras.layers.Dense(16, activation='softmax')(bert_outputs[:,0,:])\n    \n    model = tf.keras.models.Model(inputs=input_word_ids, outputs=pred)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.00002), metrics=['accuracy', f1_m, precision_m, recall_m])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n    \nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"types = np.unique(data.type.values)\n\ndef get_type_index(string):\n    return list(types).index(string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['type_index'] = data['type'].apply(get_type_index)\ntrain_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_labels = tf.keras.utils.to_categorical(train_data.type_index.values, num_classes=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.fit(np.array(train_input_ids), one_hot_labels, verbose = 1, epochs = NR_EPOCHS, batch_size = BATCH_SIZE,  callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run test and evaluate accuracy**","metadata":{}},{"cell_type":"code","source":"test_data['type_index'] = data['type'].apply(get_type_index)\ntest_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels = tf.keras.utils.to_categorical(test_data.type_index.values, num_classes=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(np.array(test_input_ids), test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The accuracy is not great, this is because the data inside the dataset is very imbalanced, which causes the huge disparation between training score and testing score (overfitting).**","metadata":{}},{"cell_type":"code","source":"cols = data['type'].unique()\ncols = cols.tolist()\n\ncolnames = ['sentence']\ncolnames = colnames+cols\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_predict = pd.DataFrame(columns = colnames)\nsentence = \"Time to debate on it. Strike at the weakest point and make others cry with facts\"\n\ndf_predict.loc[0, 'sentence'] = sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_inputs = tokenize_sentences(df_predict['sentence'], tokenizer, MAX_LEN)\nsentence_inputs = pad_sequences(sentence_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nprediction = model.predict(np.array(sentence_inputs))\ndf_predict.loc[0, cols] = prediction\n\ndf_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test the model to predict a single sentence. ","metadata":{}}]}