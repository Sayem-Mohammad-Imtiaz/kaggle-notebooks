{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n* In this notebook we examine using HuggingFace transformers to fine-tune a pre-trained model for sentiment analysis.\n* The data consist of shoppers' reviews (free text) and ratings (score of 1 to 5) of women's apparels from an ecommerce shop.\n* The task is to predict the ratings based on a given free text review.","metadata":{}},{"cell_type":"markdown","source":"# Install Packages","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install fast_ml==3.68\n!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:08:37.2674Z","iopub.execute_input":"2021-08-08T04:08:37.267807Z","iopub.status.idle":"2021-08-08T04:08:59.651865Z","shell.execute_reply.started":"2021-08-08T04:08:37.267724Z","shell.execute_reply":"2021-08-08T04:08:59.650817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom fast_ml.model_development import train_valid_test_split\nfrom transformers import Trainer, TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom torch import nn\nfrom torch.nn.functional import softmax\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T04:08:59.655672Z","iopub.execute_input":"2021-08-08T04:08:59.655981Z","iopub.status.idle":"2021-08-08T04:09:06.861701Z","shell.execute_reply.started":"2021-08-08T04:08:59.655948Z","shell.execute_reply":"2021-08-08T04:09:06.860848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure that Accelerator is set to \"GPU\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (f'Device Availble: {DEVICE}')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.210677Z","iopub.status.idle":"2021-08-08T04:05:57.211149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\ndf.drop(columns = ['Unnamed: 0'], inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.212306Z","iopub.status.idle":"2021-08-08T04:05:57.212954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* only the `Review Text` and `Rating` columns as these are the only columns that we are interested in for this task.\n* The `Review Text` column serves as input variable to the model and the `Rating` column is our target variable.\n* The `Rating` is label encoded.","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:38:30.292507Z","iopub.execute_input":"2021-07-27T13:38:30.292841Z","iopub.status.idle":"2021-07-27T13:38:30.297592Z","shell.execute_reply.started":"2021-07-27T13:38:30.292809Z","shell.execute_reply":"2021-07-27T13:38:30.296409Z"}}},{"cell_type":"code","source":"df_reviews = df.loc[:, ['Review Text', 'Rating']].dropna()\ndf_reviews['Rating'] = df_reviews['Rating'].apply(lambda x: f'{x} Stars' if x != 1 else f'{x} Star')\ndf_reviews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.214367Z","iopub.status.idle":"2021-08-08T04:05:57.21502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ndf_reviews['Rating'] = le.fit_transform(df_reviews['Rating'])\ndf_reviews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.216254Z","iopub.status.idle":"2021-08-08T04:05:57.216886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (le.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.218166Z","iopub.status.idle":"2021-08-08T04:05:57.218794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We split the data into train, validation and test set in the ratio of 80%, 10% and 10% respectively\n* The review/rating are converted into a list format, where each item in the list corresponds to one review/rating","metadata":{}},{"cell_type":"code","source":"(train_texts, train_labels,\n val_texts, val_labels,\n test_texts, test_labels) = train_valid_test_split(df_reviews, target = 'Rating', train_size=0.8, valid_size=0.1, test_size=0.1)\n\ntrain_texts = train_texts['Review Text'].to_list()\ntrain_labels = train_labels.to_list()\nval_texts = val_texts['Review Text'].to_list()\nval_labels = val_labels.to_list()\ntest_texts = test_texts['Review Text'].to_list()\ntest_labels = test_labels.to_list()\n\nprint ('Sample training data')\nprint ('')\nprint (train_texts[0:5])\nprint ('')\nprint ('')\nprint ('Sample target variable')\nprint ('')\nprint (train_labels[0:5])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.219913Z","iopub.status.idle":"2021-08-08T04:05:57.220572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We initialize the `bert-base-uncased` tokenizer and encode the train, valdiation and test data ","metadata":{}},{"cell_type":"code","source":"class DataLoader(torch.utils.data.Dataset):\n    def __init__(self, sentences=None, labels=None):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n        \n        if bool(sentences):\n            self.encodings = self.tokenizer(self.sentences,\n                                            truncation = True,\n                                            padding = True)\n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        \n        if self.labels == None:\n            item['labels'] = None\n        else:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def encode(self, x):\n        return self.tokenizer(x, return_tensors = 'pt').to(DEVICE) \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.221752Z","iopub.status.idle":"2021-08-08T04:05:57.222414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataLoader(train_texts, train_labels)\nval_dataset = DataLoader(val_texts, val_labels)\ntest_dataset = DataLoader(test_texts, test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.223552Z","iopub.status.idle":"2021-08-08T04:05:57.224207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This is how the data looks like after going through the `DataLoader`. This is the format of data accepted by HuggingFace transformer that we will be using.\n* The output data is a dictionary consisting of 3 keys-value pairs\n- `input_ids`: this contains a tensor of integers where each integer represents words from the original sentence. The `tokenizer` steps has transformed the individuals words into tokens represented by the integers. The first token `101` is the start of sentence token and the`102` token is the end of sentence token. Notice that there are many trailing zeros, this is due to padding that was applied to the sentences at the `tokenizer` step.\n- `attention_mask`: this is an array of binary values. Each position of the `attention_mask` corresponds to a token in the same position in the `input_ids`. `1` indicates that the token at the given position should be attended to and `0` indicates that the token at the given position is a padded value.\n- `labels`: this is the target label","metadata":{}},{"cell_type":"code","source":"train_dataset.__getitem__(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.225593Z","iopub.status.idle":"2021-08-08T04:05:57.22626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the original text before tokenizing","metadata":{}},{"cell_type":"code","source":"train_texts[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.227502Z","iopub.status.idle":"2021-08-08T04:05:57.22813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's set up the evaluation metrics.  ","metadata":{}},{"cell_type":"code","source":"f1 = datasets.load_metric('f1')\naccuracy = datasets.load_metric('accuracy')\nprecision = datasets.load_metric('precision')\nrecall = datasets.load_metric('recall')\n\ndef compute_metrics(eval_pred):\n    metrics_dict = {}\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    metrics_dict.update(f1.compute(predictions = predictions, references = labels, average = 'macro'))\n    metrics_dict.update(accuracy.compute(predictions = predictions, references = labels))\n    metrics_dict.update(precision.compute(predictions = predictions, references = labels, average = 'macro'))\n    metrics_dict.update(recall.compute(predictions = predictions, references = labels, average = 'macro'))\n\n    return metrics_dict","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.229102Z","iopub.status.idle":"2021-08-08T04:05:57.229508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {idx:label for idx, label in enumerate(le.classes_)}\nlabel2id = {label:idx for idx, label in enumerate(le.classes_)}\n\nconfig = AutoConfig.from_pretrained('distilbert-base-uncased',\n                                    num_labels = 5,\n                                    id2label = id2label,\n                                    label2id = label2id)\nmodel = AutoModelForSequenceClassification.from_config(config)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.23033Z","iopub.status.idle":"2021-08-08T04:05:57.230716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (f'id2label: {id2label}')\nprint (f'label2id: {label2id}')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.231549Z","iopub.status.idle":"2021-08-08T04:05:57.231969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.232779Z","iopub.status.idle":"2021-08-08T04:05:57.233214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.234085Z","iopub.status.idle":"2021-08-08T04:05:57.234479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='/kaggle/working/results',\n    num_train_epochs=1,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.05,\n    report_to='none',\n    evaluation_strategy='steps',\n    logging_dir='/kagge/working/logs',\n    logging_steps=50)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.235273Z","iopub.status.idle":"2021-08-08T04:05:57.235684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate the trained model on test data. The output contains the unnormalized score, groudtruth label ids and evaluation metrics","metadata":{}},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset)\n\nprint ('Predictions, unnormalized score')\nprint (test_results.predictions)\nprint ('')\nprint ('Ground Truth Labels')\nprint (test_results.label_ids)\nprint ('')\nprint ('Metrics')\nprint (test_results.metrics)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.236735Z","iopub.status.idle":"2021-08-08T04:05:57.237146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label2id_mapper = model.config.id2label\nproba = softmax(torch.from_numpy(test_results.predictions))\npred = [label2id_mapper[i] for i in torch.argmax(proba, dim = -1).numpy()]\nactual = [label2id_mapper[i] for i in test_results.label_ids]\nclass_report = classification_report(actual, pred, output_dict = True)\npd.DataFrame(class_report)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T04:05:57.238Z","iopub.status.idle":"2021-08-08T04:05:57.238411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/sentiment_model')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T09:11:44.96191Z","iopub.execute_input":"2021-08-07T09:11:44.962366Z","iopub.status.idle":"2021-08-07T09:11:45.67512Z","shell.execute_reply.started":"2021-08-07T09:11:44.962304Z","shell.execute_reply":"2021-08-07T09:11:45.674161Z"},"trusted":true},"execution_count":null,"outputs":[]}]}