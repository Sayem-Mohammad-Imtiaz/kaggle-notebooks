{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import silhouette_score\nInteractiveShell.ast_node_interactivity = \"all\"\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/top-spotify-songs-from-20102019-by-year/top10s.csv', encoding='latin1')\ndf.head()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns = {'Unnamed: 0': 'id'})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['id'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['top genre'].value_counts().head()\ndf['artist'].value_counts().head()\ndf['title'].value_counts().head()\ndf['year'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.title == 'Company']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop year, title, and deduplicate rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"yearless_df = df.drop(['year', 'title', 'pop'], axis=1)\nyearless_df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['top genre'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Super categorization of top genre's feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in yearless_df['top genre']:\n    if 'pop' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n        \n    elif 'hip hop' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'hip hop')\n\n    elif 'edm' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n\n    elif 'r&b' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n\n    elif 'latin' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'latin')\n\n    elif 'room' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'room')\n\n    elif 'electro' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n        \nyearless_df['top genre'] = yearless_df['top genre'].replace('chicago rap', 'hip hop')\n        \nyearless_df[\"top genre\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearless_df['top genre'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearless_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# genre_df = pd.DataFrame(yearless_df['top genre'].value_counts()).reset_index()\n# genre_df.columns = ['top genre','count']\n# genre_df['top_genre_modeling'] = genre_df['top genre'] \n# genre_df.loc[genre_df['count']< 4,'top_genre_modeling'] = 'other'\n# genre_df = genre_df.drop(['top genre'], axis=1)\n# genre_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = yearless_df\nvalue_counts = temp_df.stack().value_counts() # Entire DataFrame \nto_remove = value_counts[value_counts <= 3].index\ntemp_df.replace(to_remove, 'other', inplace=True)\ntemp_df['top genre'].value_counts()\ntemp_df.head()\ntemp_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearless_df['top genre'] = temp_df['top genre']\nyearless_df[['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'artist']] = df[['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'artist']]\nyearless_df['top genre'].value_counts()\nyearless_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = yearless_df\nnew_df.artist.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.isnull().any()\nnew_df = new_df.drop_duplicates()\nnew_df = new_df.reset_index(drop=True)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(new_df.dB)\nplt.show()\nplt.hist(new_df.bpm)\nplt.show()\nplt.hist(new_df.nrgy)\nplt.show()\nplt.hist(new_df.live)\nplt.show()\nplt.hist(new_df.val)\nplt.show()\nplt.hist(new_df.dur)\nplt.show()\nplt.hist(new_df.acous)\nplt.show()\nplt.hist(new_df.spch)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.bpm.unique()\nnew_df.dB.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.bpm = new_df.bpm.replace(0, new_df.bpm.mean())\nnew_df.bpm.unique()\nnew_df.dB = new_df.dB.replace(-60, new_df.dB.mean())\nnew_df.dB.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.get_dummies(new_df[['artist', 'top genre']])\nnew_df = new_df.join(temp_df, how='left')\nnew_df = new_df.drop(columns = ['artist', 'top genre'], axis=1)\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We have to check for collinearity and reduce dimensionality"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std = StandardScaler().fit_transform(new_df)\npca = PCA(n_components=.95)\nprincipalComponents = pca.fit_transform(X_std) # Plot the explained variances\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Pulsar Dataset Explained Variance')\nplt.show()\n\n# plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n# plt.xlabel('PCA 1')\n# plt.ylabel('PCA 2')\n# PCA_components = pd.DataFrame(principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=20)\nprincipalComponents = pca.fit_transform(X_std)\npca_df = pd.DataFrame(principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_of_squared_distances = []\nK = range(1,20)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(pca_df)\n    sum_of_squared_distances.append(km.inertia_)\n    \nax = sns.lineplot(x=K, y = sum_of_squared_distances)\nax.set(xlabel='K', ylabel='sum of squared distances', title='Elbow graph')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=13)    \nkmeans.fit(pca_df)\ny_kmeans = kmeans.predict(pca_df)\ny_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pca_df.iloc[:, 0], pca_df.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.title(\"Spotify Dendograms\")\ndendogram = dendrogram(linkage(pca_df, method='ward'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac = AgglomerativeClustering(n_clusters=13, affinity='euclidean', linkage='ward')\nac.fit_predict(pca_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan = DBSCAN(eps = 9, min_samples = 3)\ndbscan.fit(pca_df)\nlabels = dbscan.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nprint('Estimated number of clusters: %d' % n_clusters_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_labels = pd.DataFrame(kmeans.labels_)\nac_labels = pd.DataFrame(ac.labels_)\ndbscan_labels = pd.DataFrame(dbscan.labels_)\n\nsilhouette_score(pca_df, kmeans_labels, metric='euclidean')\nsilhouette_score(pca_df, ac_labels, metric='euclidean')\nsilhouette_score(pca_df, dbscan_labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_df = new_df.join(dbscan_labels, how='left')\ndbscan_df = dbscan_df.rename(columns = {0: 'labels'})\ndbscan_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled = pd.DataFrame(new_df)\ndf_scaled['dbscan'] = dbscan.labels_\ndf_mean = (df_scaled.loc[df_scaled.dbscan!=-1, :].groupby('dbscan').mean())\nresults = pd.DataFrame(columns=['Variable', 'Var'])\nfor column in df_mean.columns[1:]:\n    results.loc[len(results), :] = [column, np.var(df_mean[column])]\n    selected_columns = list(results.sort_values('Var', ascending=False,).head(7).Variable.values) + ['dbscan']\n    tidy = df_scaled[selected_columns].melt(id_vars='dbscan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12 is the number of clusters in DBScan\nfor i in range(12):\n    sns.catplot(x='dbscan', y='value', hue='variable', data=tidy[tidy['dbscan']==i], height=5, aspect=.7, kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# You can see all of the clusters above"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}