{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pandas_profiling import ProfileReport as pp\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom collections import Counter\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"ticks\", context=\"talk\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar(var):\n    cnt = data[var].value_counts().reset_index()\n# cnt.reset_index()\n    cnt.plot.bar(x='index', y=var, grid=True, legend=False, title=var)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = (data.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\n# print(\"Categorical variables:\")\n# print(categorical_list)\nsns.set_style('darkgrid')\n\nfor c in categorical_list:\n    bar(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('id',inplace=True, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ht_cnt = data['hypertension'].value_counts().reset_index() \n# print(ht_cnt)\n# plt.pie(x=ht_cnt['hypertension'], labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(data['age'], kde=True).set_title('Age Intervals Count')\n\nplt.subplot(2,3,2)\nsns.histplot(data['avg_glucose_level'], kde=True).set_title('Glucose Level Intervals Count')\n\nplt.subplot(2,3,3)\nsns.histplot(data['bmi'], kde=True).set_title('BMI Intervals Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=data['stroke'], y=data['age'],\n              color=\"b\", \n              scale=\"linear\", data=data)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=data['stroke'], y=data['bmi'],\n              color=\"b\", \n              scale=\"linear\", data=data)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=data['stroke'], y=data['avg_glucose_level'],\n              color=\"b\", \n              scale=\"linear\", data=data)\n# plt.subplot(2,3,2)\n# sns.boxenplot(x=data['stroke'], y=data['age'],\n#               color=\"b\", \n#               scale=\"linear\", data=data)\n\n# plt.subplot(2,3,3)\n# sns.swarmplot(x=\"stroke\", y=\"age\",hue=\"smoking_status\", data=data, palette=\"PRGn\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,8)) \nsns.heatmap(data.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\ndata.dropna(inplace=True)\nprint(data.shape)\n# data.isnull().sum()\n\ndata.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndata['gender'] = le.fit_transform(data['gender'])\ndata['ever_married'] = le.fit_transform(data['ever_married'])\ndata['work_type'] = le.fit_transform(data['work_type'])\ndata['Residence_type'] = le.fit_transform(data['Residence_type'])\ndata['smoking_status'] = le.fit_transform(data['smoking_status'])\ndata.head()\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers\ndata = data.drop(detect_outliers(data, ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease', 'stroke']), axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# creating instance of one-hot-encoder\nenc = OneHotEncoder(handle_unknown='ignore')\n# passing bridge-types-cat column (label encoded values of bridge_types)\nenc_df = pd.DataFrame(enc.fit_transform(data[['hypertension']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'hypertension_n', 1:'hypertension_y' })\ndata = data.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(data[['smoking_status']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'smoking_status_0', 1:'smoking_status_1', 2:'smoking_status_2', 3:'smoking_status_3' })\ndata = data.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(data[['Residence_type']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'Residence_type_n', 1:'Residence_type_y' })\ndata = data.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(data[['work_type']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'work_type_n', 1:'work_type_y', 2:'work_type_2', 3:'work_type_3', 4:'work_type_4' })\ndata = data.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(data[['ever_married']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'ever_married_n', 1:'ever_married_y' })\ndata = data.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(data[['heart_disease']]).toarray())\nenc_df=enc_df.rename(columns={0 : 'heart_disease_n', 1:'heart_disease_y' })\ndata = data.join(enc_df)\n\ndata.drop(['hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], axis=1, inplace=True)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = data['stroke']\nX = data.drop(['stroke'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2)\nX_sm, Y_sm = sm.fit_resample(X, Y)\nY_sm.value_counts()\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()\n\nX_sm = sc.fit_transform(X_sm)\nX_train, X_test, Y_train, Y_test = train_test_split(X_sm, Y_sm, test_size=0.6, random_state=101) \n\nX_test, X_valid, Y_test, Y_valid = train_test_split(X_test, Y_test, test_size=0.5, random_state=101) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n#     'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = models.keys()\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, Y_train)\n  score = model.score(X_valid, Y_valid)\n  print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, Y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, Y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, Y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, Y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, Y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) / (tp + fp + tn + fn)\n  precision = tp / (tp + fp)\n  recall    = tp / (tp + fn)\n  f1score  = 2 * precision * recall / (precision + recall)\n  specificity = tn / (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, Y_test)}\\n')\n  print(\"\")\n   \n#   modelNames.remove(modelNames[0])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(Y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential ([\n#     layers.Dense(64, activation='relu', input_shape=(X_sm.shape[-1],)),\n#     layers.Dense(128, activation='relu'),\n#     layers.Dense(256, activation='relu'),\n#     layers.Dropout(0.3),\n#     layers.Dense(256, activation='relu'),\n#     layers.Dropout(0.3),\n#     layers.Dense(1, activation='sigmoid'),\n        keras.layers.Dense(64,input_shape=(X_sm.shape[-1],),activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n    \n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# X_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = [\n    keras.metrics.FalseNegatives(name=\"fn\"),\n    keras.metrics.FalsePositives(name=\"fp\"),\n    keras.metrics.TrueNegatives(name=\"tn\"),\n    keras.metrics.TruePositives(name=\"tp\"),\n    keras.metrics.Precision(name=\"precision\"),\n    keras.metrics.Recall(name=\"recall\"),\n]\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-2), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy']\n)\n\ncallbacks = [keras.callbacks.ModelCheckpoint(\"stroke_model_at_epoch_{epoch}.h5\")]\n\nmodel.fit(\n    X_sm,\n    Y_sm,\n#     batch_size=2048,\n    epochs=30,\n#     verbose=2,\n    validation_split=0.25,\n    callbacks=callbacks,\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}