{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DATASET_PATH = \"../input/imdb-review-dataset/imdb_master.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\nimport itertools\nfrom typing import List","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Рассмотрим преобразование текстов в удобоваримый для нейронной сети вид.<br>\nА именно:\n\n- Текст разбивается на слова (токенизация, знаки препинания считаются словами)\n- Слова подсчитываются для формирования ограниченного словаря. Каждому слову сопоставляется определеннный номер (индекс, айди) в словаре. Редким словам назначается специальный номер (эквивалентно замене редких слов на спец. слово **\\<UNK\\>** (неизвестное слово)). \n- Последовательности слов преобразуются в последовательности номеров слов.\n- Полученные последовательности выравниваются по заданной максимальной длине через обрезание или дополнение номером спец.символа **\\<PAD\\>**"},{"metadata":{},"cell_type":"markdown","source":"Класс для хранения текста в виде последовательности индексов слов и его закодированной метки (класса)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids: List[int], label_id: int):\n        self.input_ids = input_ids\n        self.label_id = label_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Класс словаря. Метод word2id возвращает номер слова, id2word - наоборот, восстанавливает слово.\nunk_index - номер слова, которым будут обозначены все неизвестные слова"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocab:\n    def __init__(self, itos: List[str], unk_index: int):\n        self._itos = itos \n        # строим обратный индекс - слово - номер\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Интерфейс объекта, преобразующего тексты в последовательности номеров. \n\n**fit_transform** выучивает новый словарь из текста и преобразует текст с его помощью. Используется на обучающей выборке текстов.\n\n**transform** выполняет преобразование при помощи уже выученного словаря. Использует для тестовых текстов"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Простая реализация данного интерфейса. Разбиение на слова производится с помощью библиотеки NLTK. В словаре содержатся несколько спец. слов. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size # \n        self._tokenizer = nltk.tokenize.TweetTokenizer()\n        \n    def tokenize(self, text):\n        return self._tokenizer.tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        # в словаре будут max_vocab_size - 2 самых частых слов\n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform_single_text(self, text):\n        tokens =  self.tokenize(text)\n        ids = [self.vocab.word2id(token) for token in tokens]\n        return ids\n        \n    def transform(self, texts):\n        result = []\n        for text in tqdm(texts):\n            result.append(self.transform_single_text(text))\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in tqdm(texts)]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tqdm(tokenized_texts):\n            tokens = tokens\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Строим экземпляр входных данных. Обеспечиваем длину последовательности номеров равной max_seq_len. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Делим на выборки и преобразуем тексты"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_df = pd.read_csv(INPUT_DATASET_PATH, encoding='latin-1')\ntrain_val_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(train_val_df, test_size=0.05, stratify=train_val_df.label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2id = SimpleTextTransformer(max_vocab_size=10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])\nprint([text2id.vocab.id2word(x) for x in train_ids[0][:30]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features[3].__dict__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tensor.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(text2id.vocab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Попробуем простую Bag-of-words модель (с собственным словарём)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(max_features=10_000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получим sparse-матрицы из scipy"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = count_vectorizer.fit_transform(train_df['review'])\nX_val = count_vectorizer.transform(val_df['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.array([classes[c] for c in train_df['label']])\ny_val = np.array([classes[c] for c in val_df['label']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in X_train[0].indices:\n    print(count_vectorizer.get_feature_names()[i], X_train[0,i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(max_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg.fit(X_train,y_train,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred = log_reg.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_val, lr_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(log_reg.coef_.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_weights_indices = log_reg.coef_.argsort()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_weights_indices[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Поскольку каждый вес соответствует конкретному слову, мы можем понять, какие слова линейная модель считает наиболее весомыми при решении задачи"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_feature_names = count_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in sorted_weights_indices[:20]:\n    print(cv_feature_names[index], log_reg.coef_[0,index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in sorted_weights_indices[-20:]:\n    print(cv_feature_names[index], log_reg.coef_[0,index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Построим нейронную сеть"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(train_tensor, train_labels.type(torch.float32))\nval_dataset = TensorDataset(val_tensor, val_labels.type(torch.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BILSTM_Network(nn.Module):\n    def __init__(self, vocab_size, pad_index, embedding_size=300, hidden_size=512):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embedding_size, pad_index)\n        self.drop1 = nn.Dropout(p=0.5)\n        self.rnn = nn.LSTM(300, hidden_size, bidirectional=True, num_layers=1, batch_first=True)\n        self.drop2 = nn.Dropout(p=0.5)\n        self.fc = nn.Linear(2 * hidden_size,1)\n        self.pad_index = 0\n        \n    def forward(self, x):\n        pad_mask = (x == 0).type(torch.float32).view(x.size(0), x.size(1), 1)\n        # x: B x N\n        batch_size = x.size(0)\n        x = self.emb(x)\n        x = self.drop1(x)\n        # x: B, N, C\n        \n        # x: B x N x 2h\n        seq,_ = self.rnn(x)\n        \n        x = self.drop2(seq)\n        x = pad_mask * -1e9 + x * (1 - pad_mask)\n        \n        #x: B x 2h\n        x,_ = torch.max(x, dim=1)\n        x = self.fc(x)\n        #x: B x 1\n        x = torch.sigmoid(x)\n        return x.view(-1)\n    \n    def to_prediction(self, output):\n        return output > 0.5\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BestModel:\n    def __init__(self, path, initial_criterion):\n        self.path = path\n        self.criterion = initial_criterion\n        \n    def update(self, model, optimizer, criterion):\n        self.criterion = criterion\n        torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'criterion': criterion}, self.path)\n        \n    def load_model_data(self):\n        return torch.load(self.path)\n    \n    def restore(self, model, optimizer):\n        model_data = self.load_model_data()\n        model.load_state_dict(model_data['model_state'])\n        optimizer.load_state_dict(model_data['optimizer_state'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(epochs, model, optimizer, criterion, loaders, device, best_model, lr_scheduler=None, n_prints=1, clip=None):\n    print_every = len(loaders['train']) // n_prints\n    for epoch in range(epochs):\n        model.train()\n        running_train_loss = 0.0\n        \n        for iteration, (xx, yy) in enumerate(loaders['train']):\n            optimizer.zero_grad()\n            xx, yy = xx.to(device), yy.to(device)\n            out = model(xx)\n            loss = criterion(out, yy)\n            running_train_loss += loss.item()\n            loss.backward()\n            \n            if clip is not None:\n                nn.utils.clip_grad_norm_(model.parameters(),clip)\n            \n            optimizer.step()\n            \n            if(iteration % print_every == print_every - 1):\n                running_train_loss /= print_every\n                print(f\"Epoch {epoch}, iteration {iteration} training_loss {running_train_loss} lr={np.round(get_lr(optimizer),6)}\")\n                running_train_loss = 0.0\n                \n        if lr_scheduler:\n            lr_scheduler.step()\n            \n        with torch.no_grad():\n            model.eval()\n            running_corrects = 0\n            running_total = 0\n            running_loss = 0.0\n            for xx, yy in loaders['validation']:\n                batch_size = xx.size(0)\n                xx, yy = xx.to(device), yy.to(device)\n\n                out = model(xx)\n                \n                loss = criterion(out, yy)\n                running_loss += loss.item()\n                \n                predictions = model.to_prediction(out).type(torch.float32)\n                running_corrects += (predictions == yy).sum().item()\n                running_total += batch_size\n            \n            mean_val_loss = running_loss / len(loaders['validation'])\n            accuracy = running_corrects / running_total\n            \n            if accuracy > best_model.criterion:\n                best_model.update(model, optimizer, accuracy)\n            \n            print(f\"Epoch {epoch}, val_loss {mean_val_loss}, accuracy = {accuracy}, lr={get_lr(optimizer)}\")\n    best_model.restore(model, optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available else 'cpu'\nnetwork = BILSTM_Network(len(text2id.vocab), pad_index=0, embedding_size=300, hidden_size=1024//2).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(network.parameters(),lr=3e-4)\ncriterion = nn.BCELoss()\nbest_model = BestModel(\"best_model.md\", 0)\nscheduler= None\n# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,0.9,-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_dataset,32,shuffle=True)\nval_loader = DataLoader(val_dataset,64, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(20, network,\n            optimizer,\n            criterion,\n            {'train': train_loader, 'validation': val_loader},\n            device,\n            best_model, \n            n_prints=5,\n            lr_scheduler=scheduler,\n            clip=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model.restore(network, optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, loader, device):\n    all_preds = []\n    correct_preds = []\n    with torch.no_grad():\n        model.eval()\n        for xx, yy in loader:\n            xx = xx.to(device)\n            output = model(xx)\n            all_preds.extend((output > 0.5).tolist())\n            correct_preds.extend(yy.type(torch.int8).tolist())\n            \n    return all_preds, correct_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_preds, correct_preds = evaluate(network, val_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(correct_preds, model_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Воспользуемся моделью BERT"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Выполните, чтобы очистить память видеокарты"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel network\ndel optimizer\ngc.collect()\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_model = \"bert-base-multilingual-cased\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained(selected_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = AutoModel.from_pretrained(selected_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bert_tokenizer.tokenize(train_df['review'].iloc[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_max_length = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_train = [ bert_tokenizer.encode(t, return_tensors='pt',\n                                     max_length=bert_max_length,\n                                     truncation=True,\n                                     padding='max_length').view(-1) for t in tqdm(train_df['review'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_val = [ bert_tokenizer.encode(t, return_tensors='pt',\n                                     max_length=bert_max_length,\n                                     truncation=True,\n                                     padding='max_length').view(-1) for t in tqdm(val_df['review'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_train = torch.stack(bert_train)\nbert_val  = torch.stack(bert_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model.eval()\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    res = bert_model.forward(bert_train[0:1],output_hidden_states=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l, h in enumerate(res.hidden_states):\n    print(l)\n    print(h.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbert_model = bert_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(DataLoader(bert_train,batch_size=16))).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_layers(bert: BertModel, inputs_ids_batch, selected_layers: List[int]):\n    output = bert_model(inputs_ids_batch,output_hidden_states=True)\n    if len(selected_layers) > 1:\n        concatenated = torch.cat([output.hidden_states[l] for l in selected_layers], dim=-1)\n    else:\n        concatenated = output.hidden_states[selected_layers[0]]\n    \n    return concatenated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pool_bert(input_ids, selected_layers, batch_size=16):\n    \n    pooled = np.zeros((len(input_ids), 768 * len(selected_layers)), dtype=np.float32)\n    offset = 0\n    for batch in tqdm(DataLoader(input_ids,batch_size=batch_size, shuffle=False)):\n        batch = batch.to(device)\n        with torch.no_grad():\n            concatenated = get_bert_layers(bert_model, batch, selected_layers)\n\n        non_padding_mask = (batch > 0.5).view(batch.size(0), batch.size(1), 1).type(torch.float32)\n        summed = (non_padding_mask * concatenated).sum(dim=-2)\n        \n        lengths = non_padding_mask.squeeze(-1).sum(dim=-1,keepdim=True)\n        mean_pooled = summed / lengths\n\n        pooled[offset:offset + batch.size(0)] = mean_pooled.cpu().numpy()\n        offset += batch.size(0)\n    return pooled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_train_pooled = pool_bert(input_ids=bert_train, selected_layers=[10,11,12],batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_val_pooled = pool_bert(input_ids=bert_val, selected_layers=[10,11,12],batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_log_reg = LogisticRegression(max_iter=500)\nbert_log_reg.fit(bert_train_pooled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_log_pred = bert_log_reg.predict(bert_val_pooled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_val, bert_log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Построим и натренируем собственную модель LSTM поверх BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMOverBERT(nn.Module):\n    def __init__(self, bert: BertModel, lstm_hidden_size=512):\n        super().__init__()\n        self.bert = bert\n        \n    def bert_encode(self, bert_input):\n        # ?\n        pass\n    \n    def lstm_encode(self, bert_output):\n        # ?\n        pass\n    \n    def classify(self, lstm_encoded, padding_mask):\n        pass\n    \n    \n    def forward(self,bert_input):\n        padding_mask = bert_input == 0\n        \n        # BATCH_SIZE x TEXT_LENGTH x BERT_HIDDEN\n        encoded = self.bert_encode(bert_input)\n        \n        # BATCH_SIZE x TEXT_LENGTH x LSTM_HIDDEN\n        lstm_encoded = self.lstm_encode(encoded)\n        \n        logits = self.classify(lstm_encoded, padding_mask)\n        \n        return torch.sigmoid(logits).view(-1)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network = LSTMOverBERT(bert_model, lstm_hidden_size=384)\n# optimizer = torch.optim.Adam(network.parameters(),lr=3e-4)\n# criterion = nn.BCELoss()\n# best_model = BestModel(\"best_model_over_bert.md\", 0)\n# scheduler= None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_train_dataset = TensorDataset(bert_train, train_labels.type(torch.float32))\n# bert_val_dataset = TensorDataset(bert_val, val_labels.type(torch.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_loader = DataLoader(bert_train_dataset,24,shuffle=True)\n# val_loader = DataLoader(bert_val_dataset,32, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_model(...)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}