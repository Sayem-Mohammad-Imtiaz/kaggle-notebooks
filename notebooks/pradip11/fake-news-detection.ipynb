{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the datasets\nimport pandas as pd\n\ndf_real = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ndf_fake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\n\ndisplay(df_real)\ndisplay(df_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about the datasets\n\ndisplay(df_real.info())\ndisplay(df_fake.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping publishers\n\ndef remove_publisher(text):\n    if \" - \" in text:\n        return text.split(\" - \", 1)[1]\n    return text\n\ndf_real[\"text\"] = df_real[\"text\"].apply(remove_publisher)\ndf_fake[\"text\"] = df_fake[\"text\"].apply(remove_publisher)\n\ndisplay(df_real)\ndisplay(df_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove html\nfrom bs4 import BeautifulSoup\n\ndef remove_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    html_free = soup.get_text()\n    return html_free","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove punctuation\nimport string\n\ndef remove_punctuation(text):\n    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n    return no_punct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r\"\\w+\")\n\ndef word_tokenizer(text):\n    tokenized_text = tokenizer.tokenize(text.lower())\n    return tokenized_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words(\"english\")]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatizing\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]\n    return lemmatized_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning\n\ndef clean_data(text):\n    text = remove_html(text)\n    text = remove_punctuation(text)\n    text = word_tokenizer(text)\n    text = remove_stopwords(text)\n    text = word_lemmatizer(text)\n    return text\n\ndf_real[\"title\"] = df_real[\"title\"].apply(clean_data)\ndf_real[\"text\"] = df_real[\"text\"].apply(clean_data)\n\ndf_fake[\"title\"] = df_fake[\"title\"].apply(clean_data)\ndf_fake[\"text\"] = df_fake[\"text\"].apply(clean_data)\n\ndisplay(df_real)\ndisplay(df_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding label and merging data\n\ndf_real[\"label\"] = 1\ndf_fake[\"label\"] = 0\n\ndata = pd.concat([df_real, df_fake])\nprint(data.shape)\n\ndisplay(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building corpus\n\ncorpus = []\nfor lst in data[\"title\"] + data[\"text\"]:\n    for item in lst:\n        corpus.append(item)\n\nprint(len(corpus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building vocabulary\n\nvocab = set(corpus)\nprint(\"%d unique words\" % (len(vocab)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate tf-idf\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport numpy as np\n\ncount_vect = CountVectorizer(stop_words=\"english\")\nsf = count_vect.fit_transform(corpus)\n\ntfidf_trans = TfidfTransformer()\ntransformed_weights = tfidf_trans.fit_transform(sf)\n\nweights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\ndf_weights = pd.DataFrame({\"term\": count_vect.get_feature_names(), \"weight\": weights})\ndf_weights.sort_values(by=\"weight\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build word-tfidf dictionary\n\ntfidf_score = {}\nfor _, item in df_weights.iterrows():\n    tfidf_score[item[\"term\"]] = item[\"weight\"]\n\nprint(len(tfidf_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set mean tfidf score\n\nmean_tfidf = df_weights[\"weight\"].mean()\nprint(\"%.10f\" % mean_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to drop words based on tfidf score\n\ndef drop_words(lst):\n    text = []\n    for item in lst:\n        if item in tfidf_score and tfidf_score[item] >= mean_tfidf:\n            text.append(item)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter data\n\ndata[\"title\"] = data[\"title\"].apply(drop_words)\ndata[\"text\"] = data[\"text\"].apply(drop_words)\n\ndisplay(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many words present in each sample \n\nlength = []\n[length.append(len(str(text))) for text in data[\"title\"] + data[\"text\"]]\ndata[\"length\"] = length\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get minimum, maximum, average length\n\nprint(\"Minimum sentence length = \", min(data[\"length\"]))\nprint(\"Maximum sentence length = \", max(data[\"length\"]))\navg_sent_len = round(sum(data[\"length\"])/len(data[\"length\"]))\n\n\nprint(\"Average sentence length = \", avg_sent_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many samples have less than 500 words\n\nprint(len(data[data[\"length\"] < 500]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the outliers\n\ndata = data.drop(data[\"text\"][data[\"length\"] < 500].index, axis=0)\nprint(\"Minimum sentence length = \", min(data[\"length\"]))\nprint(\"Maximum sentence length = \", max(data[\"length\"]))\navg_sent_len = round(sum(data[\"length\"])/len(data[\"length\"]))\nprint(\"Average sentence length = \", avg_sent_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nsns.distplot(data[\"length\"], ax=ax)\nax.set_xlim(1, 5000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unnecessary features\n\ndata[\"text\"] = data[\"title\"] + data[\"text\"]\ndata.drop(columns=[\"title\", \"subject\", \"date\", \"length\"], axis=1, inplace=True)\n\ndisplay(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corpus size and vocabulary size\n\ncorpus = []\nfor lst in data[\"text\"]:\n    for item in lst:\n        corpus.append(item)\n\nprint(\"Corpus size: \", len(corpus))\nvocab = set(corpus)\nprint(\"Vocabulary size: \", len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries and setting parameters\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nembedding_dim = 100\nmax_length = avg_sent_len\ntrunc_type = \"post\"\npadding_type = \"post\"\noov_tok = \"<OOV>\"\nvocab_size = len(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test split\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(data.text,data.label,test_size=0.3, random_state=42, shuffle=True, stratify=data.label)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\nvocab_size=len(word_index)\n\ntokenixed_train = tokenizer.texts_to_sequences(x_train)\nx_train = pad_sequences(tokenixed_train, maxlen=max_length, truncating=trunc_type)\n\ntokenized_test = tokenizer.texts_to_sequences(x_test)\nx_test = pad_sequences(tokenized_test, maxlen=max_length, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the Glove vector\n\nembeddings_index = {}\n\nwith open(\"../input/glove6b100d/glove.6B.100d.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype=\"float32\")\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make numpy arrays\n\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model\nimport tensorflow as tf\n \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout = 0.2)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout = 0.2)),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n# YOUR CODE HERE\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\ncallback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-3, patience=5, verbose=1, mode=\"auto\", restore_best_weights=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\n\nhistory = model.fit(x_train, y_train, batch_size=64, validation_data=(x_test, y_test), callbacks=[callback], epochs=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history[\"val_\" + string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, \"val_\" + string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}