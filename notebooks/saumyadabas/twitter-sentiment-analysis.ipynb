{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib notebook","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport re  #for regular expression\nimport nltk #for text manipulation\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\n\npd.set_option(\"display.max_colwidth\",200)\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ntest = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/test.csv\")\ntrain[train['label']== 0].head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['label']== 1].head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"label\"].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combi = train.append(test, ignore_index=True)\ncombi.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer \nimport gensim\nbow_vectorizer = CountVectorizer(max_df=0.90 ,min_df=2,max_features=1000,stop_words='english')\nbow = bow_vectorizer.fit_transform(combi['tweet'])\nbow.shape\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfdif_vectorizer = TfidfVectorizer(max_df=0.90 ,min_df=2,max_features=1000,stop_words='english')\ntfidf = tfdif_vectorizer.fit_transform(combi['tweet'])\ntfidf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_tweet = combi['tweet'].apply(lambda x: x.split()) #tokenizing\nmodel_w2v = gensim.models.Word2Vec(tokenized_tweet,vector_size = 200 ,window=5,min_count=2,sg=1,hs=0,negative=10,workers= 2,seed = 34)\nmodel_w2v.train(tokenized_tweet, total_examples=len(combi['tweet']),epochs=20)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_w2v.wv.most_similar(positive='dinner')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_w2v.wv.most_similar(negative='trump')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nvector = model_w2v.wv['food']\nprint(vector)\nlen(model_w2v.wv['food'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model_w2v.wv[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary\n                         \n            continue\n    if count != 0:\n        vec /= count\n    return vec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordvec_arrays = np.zeros((len(tokenized_tweet),200))\nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i],200)\n    wordvec_df = pd.DataFrame(wordvec_arrays)\n    wordvec_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordvec_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_bow = bow[:31962,:]\ntest_bow = bow[31962:,:]\nxtrain_bow,xvalid_bow,ytrain,yvalid = train_test_split(train_bow,train['label'],random_state = 411,test_size=0.3)\n\nlreg = LogisticRegression()\nlreg.fit(xtrain_bow, ytrain)\nprediction = lreg.predict_proba(xvalid_bow)\npredict_int = prediction[:,1] >= 0.3 \npredict_int = predict_int.astype(np.int)\nf1_score(yvalid,predict_int)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = lreg.predict_proba(test_bow)\ntest_pred_int = test_pred[:,1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['label'] = test_pred_int \nsubmission = test[['id','label']]\nsubmission.to_csv('sub_lreg_bow.csv',index=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w2v = wordvec_df.iloc[:31962,:]\ntest_w2v = wordvec_df.iloc[31962:,:]\nxtrain_w2v = train_w2v.iloc[ytrain.index,:]\nxvalid_w2v = train_w2v.iloc[yvalid.index,:]\n\nlreg.fit(xtrain_w2v, ytrain)\nprediction = lreg.predict_proba(xvalid_w2v)\npredict_int = prediction[:,1] >= 0.3 \npredict_int = predict_int.astype(np.int)\nf1_score(yvalid,predict_int)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidf = tfidf[:31962,:]\ntest_tfidf = tfidf[31962:,:]\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\nprediction = lreg.predict_proba(xvalid_tfidf)\npredict_int = prediction[:,1] >= 0.3 \npredict_int = predict_int.astype(np.int)\nf1_score(yvalid,predict_int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ","metadata":{},"execution_count":null,"outputs":[]}]}