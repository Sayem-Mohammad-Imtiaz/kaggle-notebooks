{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Continuing exploring restaurant customers\nIn the notebook <a href='https://www.kaggle.com/erelin6613/customer-is-always-right'>Customer is always right</a> we did some basic analysis based on a sample of data. It is usually the case we have wast amount of data, often even sparse data when dealing with recommendation problem. Luckily for us, we have a few tools whcih come to resque.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install pyspark --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 list | grep pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pyspark as spark\nimport pyspark.sql.functions as F\nimport pyspark.ml as ml\nimport pyspark.mllib as mllib\nfrom pyspark.sql.types import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = spark.SparkContext()\nsql = spark.sql.SQLContext(sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_dir = '../input/restaurant-recommendation-challenge'\nfiles = dict()\nfor f in os.listdir(root_dir):\n    if f.endswith('.csv') and f != 'SampleSubmission (1).csv':\n        files[f] = sql.read.format('csv').options(header='true').load(os.path.join(root_dir, f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files['train_full.csv'].groupBy('discount_percentage').count().orderBy('count').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files['train_full.csv'].groupby('target').count().orderBy('count').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_s = files['train_full.csv'].select('prepration_time').toPandas()\nprep_s['prepration_time'].astype('float').hist(color='gold')\ndel prep_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_method = files['orders.csv'].select('payment_mode').toPandas()\np_method['payment_mode'].astype('float').hist()\ndel p_method","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = files['orders.csv'].select('grand_total').toPandas()\nsns.distplot(total['grand_total'].astype('float'), color='purple')\ndel total","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope this will be enough to ensure our sampled distributions are roughly the same on larger scale too.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing\n\nWe will need to dive into data once again to extract features we need but now we will handle it with pyspark. We will begin by dropping columns where there are only two values present: one value or NaN. Could be there is some value either in NaN or another value but for now let's treat them as not informative. Also for now we will drop columns such as 'wednesday_to_time1' as they appear to have not much of a variation (we might reconsider those later).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"I defined a function to automate dropping columns iterrating trough \ncolumns of each dataframe but it is painfully slow. Feel free to use \nit if you have plenty of spare time\"\"\"\n\ndef remove_cols(frame):\n    for col in tqdm(frame.columns):\n        nans = frame.rdd.map(lambda row: (\n            row[col], sum([c == None for c in row]))).collect()\n        #print(nans)\n        if len(nans) > 0:\n            distincts = frame.select(col).distinct().collect()\n            #print(distincts)\n            if len(distincts) == 2:\n                frame = frame.drop(col)\n    return frame\n                \n#for k, v in files.items():\n#    files[k] = remove_cols(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', \n            'friday', 'saturday', 'sunday']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = {'train_full.csv': ['commission', 'display_orders', \n                              'country_id', 'CID X LOC_NUM X VENDOR',\n                             'city_id', 'vendor_category_en', 'latitude_x', \n                              'latitude_y', 'longitude_x', 'longitude_y'],\n          'test_full.csv': ['commission', 'display_orders', \n                            'country_id', 'CID X LOC_NUM X VENDOR',\n                           'city_id', 'vendor_category_en', 'latitude_x', \n                              'latitude_y', 'longitude_x', 'longitude_y'],\n          'orders.csv': ['akeed_order_id', 'CID X LOC_NUM X VENDOR'],\n          'train_customers.csv': ['language'],\n          'train_customers.csv': ['language']}\n\nfor k, v in to_drop.items():\n    for col in v:\n        files[k] = files[k].drop(col)\n\nfor each in ['train_full.csv', 'test_full.csv']:\n    for col in weekdays:\n        for column in files[each].columns:\n            if col in column:\n                files[each] = files[each].drop(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need numeric features to be present as numeric values, not as strings. Then we need to encode categories, here we actually do not need to cast a numeric type but first things first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols = ['delivery_charge', 'serving_distance', 'vendor_rating', \n                'prepration_time', 'discount_percentage', 'verified_x', \n                'is_open', 'status_y', 'verified_y', 'rank', \n                'open_close_flags', 'location_number_obj']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in numeric_cols:\n    files['train_full.csv'] = files['train_full.csv'].withColumn(\n        col, files['train_full.csv'][col].cast(DoubleType()))\n    files['test_full.csv'] = files['test_full.csv'].withColumn(\n        col, files['test_full.csv'][col].cast(DoubleType()))\n\nfiles['train_full.csv'] = files['train_full.csv'].withColumn(\n        'target', files['train_full.csv']['target'].cast(DoubleType()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another step will be is to exctract a numeric feature from 'primary_tag' column. It is a categorical one feature but still let's make a sure we do not have too much unnecessary data at hand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files['train_full.csv'] = files['train_full.csv'].withColumn(\n    'primary_tags', F.regexp_extract(\n        files['train_full.csv']['primary_tags'], r\"[0-9]+\", 0))\nfiles['test_full.csv'] = files['test_full.csv'].withColumn(\n    'primary_tags', F.regexp_extract(\n        files['test_full.csv']['primary_tags'], r\"[0-9]+\", 0))\n\nfiles['train_full.csv'] = files['train_full.csv'].withColumn(\n    'primary_tags', files['train_full.csv'][col].cast(DoubleType()))\nfiles['test_full.csv'] = files['test_full.csv'].withColumn(\n    'primary_tags', files['test_full.csv'][col].cast(DoubleType()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next let's join full frames with customers so we are not missing anything out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files['train_customers.csv'] = files['train_customers.csv'].withColumnRenamed(\n    'akeed_customer_id', 'customer_id')\nfiles['test_customers.csv'] = files['test_customers.csv'].withColumnRenamed(\n    'akeed_customer_id', 'customer_id')\ntrain_df = files['train_full.csv'].join(files['train_customers.csv'], on=['customer_id'])\ntest_df = files['test_full.csv'].join(files['test_customers.csv'], on=['customer_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop('gender').drop('language')\ntest_df = test_df.drop('gender').drop('language')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember funny column with date of birth? We still need preprocess that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.withColumn('dob', train_df.dob.cast(\n    DoubleType())).na.fill(2020.0)\ntest_df = test_df.withColumn('dob', test_df.dob.cast(\n    DoubleType())).na.fill(2020.0)\n\ntrain_df = train_df.fillna({'location_type': 'unknown'})\ntest_df = test_df.fillna({'location_type': 'unknown'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.withColumn('age', (2020-train_df.dob)).drop('dob')\ntest_df = test_df.withColumn('age', (2020-test_df.dob)).drop('dob')\nmedian_age = np.array(train_df.select('age').collect())\nmedian_age = np.median(median_age[median_age != 0.0])\n\ntrain_df = train_df.withColumn('age', F.when(\n    (train_df.age<100) & (train_df.age>12), train_df.age).otherwise(median_age))\ntest_df = test_df.withColumn('age', F.when(\n    (test_df.age<100) & (test_df.age>12), test_df.age).otherwise(median_age))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.select('age').distinct().show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I could be wrong about my impression of columns `created_at_x` and `updated_at_x` with their pairs for y, range from 2018 till 2020 but it seems to me we should engineer some feature that will tell us who is the lolyal customer. Let's try to do that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.withColumn('created_at_x', F.to_date(train_df.created_at_x))\ntrain_df = train_df.withColumn('created_at_y', F.to_date(train_df.created_at_y))\ntrain_df = train_df.withColumn('updated_at_x', F.to_date(train_df.updated_at_x))\ntrain_df = train_df.withColumn('updated_at_y', F.to_date(train_df.updated_at_y))\n\ntest_df = test_df.withColumn('created_at_x', F.to_date(test_df.created_at_x))\ntest_df = test_df.withColumn('created_at_x', F.to_date(test_df.created_at_x))\ntest_df = test_df.withColumn('updated_at_x', F.to_date(test_df.updated_at_x))\ntest_df = test_df.withColumn('updated_at_y', F.to_date(test_df.updated_at_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    train_df = train_df.withColumn('x_loyal', F.datediff(\n        train_df.updated_at_x, train_df.created_at_x))\n    train_df = train_df.withColumn('y_loayl', F.datediff(\n        train_df.updated_at_y, train_df.created_at_y))\n\n    test_df = test_df.withColumn('x_loyal', F.datediff(\n        test_df.updated_at_x, test_df.created_at_x))\n    test_df = test_df.withColumn('y_loayl', F.datediff(\n        test_df.updated_at_y, test_df.created_at_y))\nexcept Exception:\n    pass\n\ntrain_df = train_df.drop('created_at_x').drop(\n    'created_at_y').drop('updated_at_x').drop('updated_at_y')\ntest_df = test_df.drop('created_at_x').drop(\n    'created_at_y').drop('updated_at_x').drop('updated_at_y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.select('x_loyal').distinct().show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am tourturing you a lot with preprocessing and feature crafting. Let's drop the rest and see what we do with what we have. But first a small step we neglected at first: categories endcoding. That is what we will start with next time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = ['OpeningTime', 'OpeningTime2', 'language', \n           'customer_id', 'vendor_tag', 'vendor_tag_name', \n           'created_at', 'updated_at', 'id', 'authentication_id', \n           'id_obj', 'is_akeed_delivering', 'one_click_vendor']\ntarget = 'target'\n\nfor col in to_drop:\n    train_df = train_df.drop(col)\n    test_df = test_df.drop(col)\ntrain_df.show(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = ['location_number', 'location_type', 'status_x',\n               'vendor_category_id', 'device_type', 'status', \n               'verified']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df.select('one_click_vendor').distinct().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical:\n    stringIndexer = ml.feature.StringIndexer(inputCol=col, outputCol=col + \"_ind\")\n    indexer = stringIndexer.fit(train_df)\n    train_df = indexer.transform(train_df)\n    test_df = indexer.transform(test_df)\n    encoder = ml.feature.OneHotEncoder(\n        inputCols=[stringIndexer.getOutputCol()], outputCols=[col + \"_ohe\"])\n    ohe_encoder = encoder.fit(train_df)\n    train_df = ohe_encoder.transform(train_df)\n    test_df = ohe_encoder.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nnumeric_cols = ['delivery_charge', 'serving_distance', 'vendor_rating', \n                'prepration_time', 'discount_percentage', 'verified_x', \n                'is_open', 'status_y', 'verified_y', 'rank', \n                'open_close_flags', 'location_number_obj']\n\n\"\"\"\ntrain_df.show(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = numeric_cols + [col+'_ohe' for col in categorical]\nassembler = ml.feature.VectorAssembler(\n    inputCols=columns, \n    outputCol=\"features\")\n\ntrain = assembler.transform(train_df)\ntest = assembler.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fit, train_eval = train.randomSplit([0.75, 0.25], seed=13)\n\nl_reg = ml.classification.LogisticRegression(labelCol='target', featuresCol='features', maxIter=20)\nl_reg=l_reg.fit(train_fit)\n\npredict_train=l_reg.transform(train_fit)\npredict_test=l_reg.transform(train_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test.select('prediction').distinct().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Well we need not even to evaluate there is something wrong with our results. What exactly we do not account for?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1) Class imbalance\nRemember the distribution of target column? From a sample of 1000 only 128 were targets of 1 which is only 12.8%. It is reasonable to assume this percentage will not vary a lot in training set. Should we check?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fit.groupBy('target').count().orderBy('count').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_eval.groupBy('target').count().orderBy('count').show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Huh, that is even more severe imbalance that our sample of 1000 showed before. Will we do better handling it?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bal_train = train.filter(train.target==1.0)\ntarget_count = bal_train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_df = train.filter(train.target==0.0).distinct()\ntarget_df = target_df.sample(False, fraction=target_count/target_df.count())\ntarget_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bal_train = bal_train.unionByName(target_df)\nbal_train.sample(False, 0.1).show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fit, train_eval = bal_train.randomSplit([0.75, 0.25], seed=13)\n\nl_reg = ml.classification.LogisticRegression(labelCol='target', featuresCol='features', maxIter=20)\nl_reg=l_reg.fit(train_fit)\n\npredict_train=l_reg.transform(train_fit)\npredict_test=l_reg.transform(train_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test.select('prediction').distinct().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That already looks much better, doesn't it? But really how accurate are results? That is going to be our next step.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}