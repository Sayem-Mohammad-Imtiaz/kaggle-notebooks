{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Drop unwanted columns and rename remaining columns\nif len(data.columns) > 3: \n    data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\nif data.columns[0] != 'label':\n    data = data.rename(columns={'v1': 'label', 'v2': 'text'})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"930ba9fae25d64593e87ad58a044433c2a4c9c17"},"cell_type":"code","source":"# Count observations in each label\ndata.label.value_counts()\n#data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"722953269aab80901340aece0c9ee82535e8f291"},"cell_type":"code","source":"# Convert label to numerical variable\ndata['label'] = data.label.map({'ham': 0, 'spam': 1})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac5e59cd8f5a753a643fff92e06b459ef5ac3398"},"cell_type":"code","source":"# Extract every text \ntexts = []\nfor index, row in data.iterrows():\n    texts.append((row['text'], row['label']))\ntexts[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5b5b53c87fadb0ba551c1dd04d3ae1e6538c62d"},"cell_type":"code","source":"# * * * PREPROCESSING * * * \n# Remove whitespace and punctutation \ntokenized = []\nfor t in texts:\n    m = t[0]\n    text = re.sub('[' + string.punctuation + ']', ' ', m)\n    text = re.sub('[\\n\\t\\r]', '', text)\n    words = text.split()\n    tokenized.append((words, t[1]))\ntokenized[0] # First element","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fcea6859e3b53e53cba6b2e436f3ad749e33574"},"cell_type":"code","source":"# Remove stopwords\nstopwords = []\ntry:\n    f = open('../input/stopword-lists-for-19-languages/englishST.txt', 'r')\n    stopwords = f.read().split('\\n')\nexcept IOError:\n    print('Problem opening file')\nfinally:\n    f.close()\nprint('Sentence before stopwrods removed: \\n', tokenized[51])\nfiltered = []\nfor t in tokenized:\n    text = t[0]\n    f_text = []\n    for word in text:\n        if word not in stopwords and len(word) > 2:\n            f_text.append(word)\n    filtered.append((f_text, t[1]))\n\nprint('\\nSentence after stopwords removed: \\n', filtered[51])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"216c8e2fb348ecd67ec64487adb8c1ec68ab2711"},"cell_type":"code","source":"# Stem the words\nstemmer = PorterStemmer()\nstemmed = []\nfor t in filtered:\n    text = t[0]\n    stemmed_text = []\n    for word in text:\n        stemmed_word = stemmer.stem(word.lower())\n        stemmed_text.append(stemmed_word)\n    stemmed.append((stemmed_text, t[1]))\n\nstemmed[51]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b5113b3596323ff5f313cd81185085c771002dc"},"cell_type":"code","source":"# Counting number of texts each word occurs\nword_count = {}\nfor t in stemmed:\n    text = t[0]\n    already_counted = []\n    for word in text:\n        if word not in word_count:\n            word_count[word] = 1\n        elif word not in already_counted:\n            word_count[word] += 1\n            already_counted.append(word)\n\n#  Removing the words that only occurs once\nfor i in range(len(stemmed)):\n    stemmed[i] = (list(filter(lambda x: word_count[x] > 4, stemmed[i][0])), stemmed[i][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdc5e719ccce59c7abf1b7580084d60db5fbb6a"},"cell_type":"code","source":"# Splitting data in trainingdata and testdata (80-20 ratio)\ntotaltexts = data.label.value_counts()\ntotal = totaltexts[0] + totaltexts[1] # Total number of texts\ntest_number = int(0.20 * total) # Number of testing mails\n# Picking randomly\ntest_set = []\ntaken = {}\nwhile len(test_set) < test_number:\n    #print(len(train_texts))\n    num = random.randint(0, test_number - 1)\n    if num not in taken.keys():\n        test_set.append(stemmed.pop(num))\n        taken[num] = 1\n\ntrain_set = stemmed # Trainset is the remaining texts\n        \n# Total number of hams and spams\nnumber_of_hams = data.label.value_counts()[0]\nnumber_of_spams = data.label.value_counts()[1]\n\nlen(train_set)/total, len(test_set)/total","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fdb9ca97b76cae9e82986fa1ff03cb6c857f724"},"cell_type":"markdown","source":"###### Probability that a text containing a given word is spam (Bayes' theorem):\n$P(Spam|Word) =  \\frac{P(Word|Spam)P(Spam)}{P(Word|Spam)P(Spam) + P(Word|Ham)P(Ham)} $"},{"metadata":{"trusted":true,"_uuid":"32fda77aae69fe5a9d07d5bba050c266ae09172c"},"cell_type":"code","source":"# * * * TRAINING THE MODEL * * * \n\n# meaning: Computing probabilities needed for P(Spam|Word)\n\n# Need to train these 4 possibilities:\n# 1) Probability that a word appears in spam messages\n# 2) Probability that a word appears in ham messages\n# 3) Overall probability that any given message is spam\n# 4) Overall probability that any given message is not spam (is ham)\n\ndef p_appears_in_spam(word):\n    count = 0\n    total_spams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 1:\n            total_spams += 1\n            if word in text:\n                count += 1\n    return count/total_spams\n             \n\ndef p_appears_in_ham(word):\n    count = 0\n    total_hams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 0:\n            total_hams += 1\n            if word in text:\n                count += 1\n    return count/total_hams\n\ndef total_spams_and_hams(tset):\n    spams = 0\n    hams = 0\n    for t in tset:\n        spams += 1 if t[1] == 1 else 0\n        hams += 1 if t[1] == 0 else 0\n    return spams, hams\n\n\np_spam = total_spams_and_hams(train_set)[0]/len(train_set) # Probability that a message is spam\np_ham = total_spams_and_hams(train_set)[1]/len(train_set) # Probability that a message is ham\n\n# Finally we can compute P(Spam | Word)\ndef p_is_spam_given_word(word):\n    return (p_appears_in_spam(word)*p_spam)/((p_appears_in_spam(word)*p_spam + p_appears_in_ham(word)*p_ham))\n\nword = 'free'\nprint('Probability that a message is spam given the word \"{}\" is: {}'.format(word, p_is_spam_given_word(word)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"269ade28228e79ab2368f486985c5a6ed6a9fabf"},"cell_type":"code","source":"# Collecting the probabilities in a dictionary\nprobabilities = {}\nfor t in train_set:\n    text = t[0]\n    for word in text:\n        if word not in probabilities:\n            p = p_is_spam_given_word(word)\n            if p == 0:\n                probabilities[word] = 0.2 # To deal with the zero probability problem. Tweaking this value\n            elif p == 1:\n                probabilities[word] = 0.98 # Tweaking this value\n            else:\n                probabilities[word] = p","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"594d5eb0b650154b6fdc7d9e6255c5523fd0fabc"},"cell_type":"markdown","source":"### Combining Individual Probabilities\nDetermining whether a message is spam or ham based only on the presence of one word is error-prone, must try to consider all the words (or the most interesting) in the message\n###### Probability that a text is spam: $P(Spam) =  \\frac{p_1p_2...p_n}{p_1p_2...p_n + (1-p_1)(1-p_2)...(1-p_n)} $\n\n$p_1$: The probability $P(S|W_1)$, that it is spam knowing it contains a first word (for example \"free\")"},{"metadata":{"trusted":true,"_uuid":"d699adb3ca123001676b31e8f3e3b44fec17032d"},"cell_type":"code","source":"# * * * TESTING THE MODEL * * * \n# Training is done\n# This function will be used to classify new messages, using the trained probabilities \n\nfrom functools import reduce\ndef p_is_spam(words):\n    probs = []\n    for word in words:\n        if word in probabilities:\n            probs.append(probabilities[word])\n        # 'else' is for unseen word, a value to tweak\n        # Assumes it is somewhat higher probability that an unseen word belongs to a ham message than a spam message\n        # as \n        else:\n            probs.append(0.4) \n    probs_not = list(map(lambda prob: 1-prob, probs))\n    product = reduce(lambda x, y: x * y, probs, 1) \n    product_not = reduce(lambda x, y: x * y, probs_not, 1)\n    return product/(product + product_not)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14c7346162ce8a5894f62483ba6c7b9711a740f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9517ace1b443297300e41ca138ec77441730ae49"},"cell_type":"code","source":"total_correct = 0\ntrue_spam_as_spam = 0\ntrue_spam_as_ham = 0\ntrue_ham_as_ham = 0\ntrue_ham_as_spam = 0\n\n# Care most about minimizing false positives, that is: labeling non-spam messages as spam\nfalse_positives = []\n\nfor t in test_set:\n    guess = -1\n    words = t[0]\n    answer = t[1]\n    p_spam = p_is_spam(words)\n    # If p > 0.95, predict 'yes' (is spam)\n    guess = 1 if p_spam > 0.95 else 0\n    if guess == answer:\n        total_correct += 1\n        if answer == 0: # true negative\n            true_ham_as_ham += 1\n        else: # true positive\n            true_spam_as_spam += 1 \n    else:\n        if answer == 0: # false positive\n            true_ham_as_spam += 1\n            false_positives.append((words, p_spam))\n        else: # true negative\n            true_spam_as_ham += 1\n\n            \ntrue_spams = total_spams_and_hams(test_set)[0]\ntrue_hams = total_spams_and_hams(test_set)[1]\n\nprint('Total test texts: ', len(test_set))\nprint('Number of correct: ', total_correct)\nprint('Accuracy: ', total_correct*100/(true_spams+true_hams))\nprint('-------------------------------')\nprint('Ham precision: ', true_ham_as_ham/(true_ham_as_ham + true_spam_as_ham))\nprint('Ham recall: ', true_ham_as_ham/(true_ham_as_ham + true_ham_as_spam))\nprint('Spam precision: ', true_spam_as_spam/(true_spam_as_spam + true_ham_as_spam)) # Most important \nprint('Spam recall: ', true_spam_as_spam/(true_spam_as_spam + true_spam_as_ham))\nprint('-------------------------------')\nprint('False Positives (hams that got labeled as spam):')\nfor i, (text, p) in enumerate(false_positives):\n    print('{}: Words in text: {} | Degree of certainty: {}'.format(i+1, text, p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ec4fa251aa91f28d11f83aa9e6b8290b40864c6"},"cell_type":"code","source":"# * * * VISUALISATIONS * * * \nfrom wordcloud import WordCloud\n\nspam_words = \"\"\nham_words = \"\"\n\nall = train_set + test_set\n\nfor t in all:\n    text = t[0]\n    s = \"\"\n    for word in text:\n        s += word + ' '\n    if t[1] == 0:\n        ham_words += s\n    else:\n        spam_words += s + ' '\n\n# # Generate a word cloud image\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"995e6df9c2da12211164674ba154416f260863cb"},"cell_type":"code","source":"#Spam Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9e4e2fb58c38ef62d01d54794fb16ff4950be16"},"cell_type":"code","source":"# Ham Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10c7dc42157abd7ef074ec0197e2b33cd0e68fe6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}