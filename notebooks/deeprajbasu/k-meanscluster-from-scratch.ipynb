{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\n\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X = dataset.iloc[:, [3, 4]].values\n\n# for visualization purpose we will look only at annual income and spending score"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, [3, 4]].values\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nsns.set(style='darkgrid')\n\nfig, ax = plt.subplots(figsize=(20,7.75))\nsns.scatterplot(x=\"Annual Income (k$)\", y=\"Spending Score (1-100)\", data=df,size=\"Age\",sizes=(55,700))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can see how our data is clustered, visually there seems to be 7 -9 different clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#variables for number of samples, and number of feature for each sample\n\nm=X.shape[0] #number of training examples\nn=X.shape[1] #number of features. Here n=2\nn_iter=100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of clusters that we would like to divide the data into\n\nK=5 # number of clusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1 : initialize random centroids"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as rd\n##array that will contain all the centroids\n\nCentroids=np.array([]).reshape(n,0) \nCentroids\n\n#initialize k number of centroids by randomly selecting datapoints\nfor i in range(K):\n    rand=rd.randint(0,m-1)#generate random index positing between 0 and last index of data\n    \n    Centroids=np.c_[Centroids,X[rand]]# add this random data point to our centroid array\nCentroids\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can acess the points like so \n#if we want to retrieve the first centroid\nCentroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Centroids[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output of our algorithm should be a dictionary with cluster number as Keys and the data points which belong to that cluster as values. So letâ€™s initialize the dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"Output={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## we will find the eucledian distance of each data point from all the generated centroids\n\n## So every row in EuclidianDistance matrix will have distances of that particular data point from all the centroids.\n> ### therefore, the position of the smallest value will point to which cluster that particular row, or datapoint belongs to\n> ### for example in my matrix, if the 5th element has the smallest value in 2 column,\n> ### we can understand that the 5th element from X belongs to 2nd cluster for this particular iteration\n## we will then take the smallest of these distances and store the index of that in matric C\n\n## as everything is ordered\n\n## if the nimimum distance is with centroid 3, we will store this position, the index value, this then points back to the cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"EuclidianDistance=np.array([]).reshape(m,0) #m, again is the number of data samples\n\nfor k in range(K):# iterating for k number of times, k is the number of clusters we want \n       tempDist=np.sum((X-Centroids[:,k])**2,axis=1)#generating eucledian distance for all centroid  with current data pooit \n       EuclidianDistance=np.c_[EuclidianDistance,tempDist]#adding this list of distances to our matrix\n\n\n####IMPORTANT####\nC=np.argmin(EuclidianDistance,axis=1)+1 #Storing the index of the minimum distance in this data point\n## therefore, for the datapoint X[i] the group it belongs to is C[i]\n\n###we can then take the mean of this group and update the centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EuclidianDistance[:5] # ditance of each data point from the centroids\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C[:15] #position of the closest cluster for each datapoint\n\n#example, for the first data point, its closest to the third cluster, \n#based on this we shall re group the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# moving the Centroids\n## in the next Iterations new centroid value will be assigned by taking the mean distance. Calcified data points./ for every cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y={}# temporary dictionary to hold our solution for this iteration\nfor k in range(K):\n    Y[k+1]=np.array([]).reshape(2,0) # populate our dictionary with k elements(representing our clusters), each having a default key and a 2,0 array\n    \n####IMPORTANT######\n\nfor i in range(m):\n     Y[C[i]]=np.c_[Y[C[i]],X[i]]# addding all the datapoints to chosen clusters in Y, based on their stored indexes in C\n     \nfor k in range(K):\n    Y[k+1]=Y[k+1].T #reshaping all the points to look like 2,0 arrays\n    \nfor k in range(K):\n     Centroids[:,k]=np.mean(Y[k+1],axis=0) # assiginng the mean of each group as new centroids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we just need to run the above in a loop so that we can perfect the clustering "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(n_iter):\n    \n    EuclidianDistance=np.array([]).reshape(m,0) #m, again is the number of data samples\n\n    for k in range(K):# iterating for k number of times, k is the number of clusters we want \n       tempDist=np.sum((X-Centroids[:,k])**2,axis=1)#distance between current datapoint and each centroid\n       EuclidianDistance=np.c_[EuclidianDistance,tempDist]#add distance to our list\n    #the result is for each datapoint, the distances  to each centroid\n\n\n    ####IMPORTANT####\n    #!!!!!!!!!!!!!!!#\n    \n    C=np.argmin(EuclidianDistance,axis=1)+1 #storing the index of the minimum distance fron the clusters in C : the index position denotes which centroid, or cluster\n    #this list contains the value of the nearest centroid for each datapoint, \n    #you can say it contains the info for which datapoint belongs to which cluster\n    \n    \n    #the position here is which datapoint for X, and the Value is the cluster it belongs too   \n    \n    #***********#\n    ## therefore, for the datapoint X[i] the group it belongs to is C[i]\n    \n    \n    \n    Y={}# temporary dictionary to hold our solution for this iteration\n    for k in range(K):\n        Y[k+1]=np.array([]).reshape(2,0) # populate our dictionary with k elements(representing our clusters), each having a default key and a 2,0 array\n    \n    ####IMPORTANT######\n    #!!!!!!!!!!!!!!!#\n    for i in range(m):\n         Y[C[i]]=np.c_[Y[C[i]],X[i]]# addding all the datapoints to chosen clusters in Y, based on their stored indexes in C\n\n    for k in range(K):\n        Y[k+1]=Y[k+1].T #reshaping all the points to look like 2,0 arrays\n\n    for k in range(K):\n         Centroids[:,k]=np.mean(Y[k+1],axis=0) # assiginng the mean of each group as new centroids\n\n    Output=Y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color=['red','blue','green','cyan','magenta']\nlabels=['cluster1','cluster2','cluster3','cluster4','cluster5']\n\nfor k in range(K):\n    plt.scatter(Output[k+1][:,0],Output[k+1][:,1],c=color[k],label=labels[k])\nplt.scatter(Centroids[0,:],Centroids[1,:],s=300,c='yellow',label='Centroids')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}