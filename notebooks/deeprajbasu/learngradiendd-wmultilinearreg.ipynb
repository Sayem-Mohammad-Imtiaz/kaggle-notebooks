{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/szeged-weather/weatherHistory.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/graduate-admissions/Admission_Predict.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n#x = df[['GRE Score','CGPA']]\ny = df['Chance of Admit ']\n# showcasing the relationship between all the columns and the label\n#loop throught our feature columns\nfor col in df.columns:\n    #we want every feature column except for chance of admit itself \n    #remember, the variable col contains a list with column name as string it it \n    if(col != ['Chance of Admit ']):\n        \n        #create a scatter plot with current feature column, \n        #chance of admit(y), always remains on the y axis \n        plt.scatter(df[col],y)\n        \n        #set the x, y axis lables as respective column names, \n        #again admission chance of admission always remains in the y axis\n        plt.xlabel(col)\n        plt.ylabel('Chance of Admit')\n        \n        #display our graphs \n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nx = np.array(df[['GRE Score','CGPA']])\ny = np.array(df['Chance of Admit '])\n\n#x=preprocessing.scale(x)\n#y=preprocessing.scale(y)\nx[:5],' ' ,y[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ax = sns.scatterplot(x=x, y=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## minimise *sum of squared errors*\n\n### sum (original - predicted)**2"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for any given x, this mothod uses linear regression to predict a value\n\ndef predict(x,m,b):\n    r =  0 \n    for i in range(0,len(x)):\n        r = r +(m[i]*x[i]) \n    return r+b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using our trained m and b, the coeffiecients, this method tells us how accurate our model is by returning the sum of all errors\ndef error(b, m, X,Y):\n    totalError = 0\n    for i in range(0, len(X)):\n        x = X[i]\n        y = Y[i]\n        for n in range(0,len(m)):\n            totalError += (y - (m[n] * x[n] + b)) ** 2\n    return totalError / float(len(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is the gradient decent method, \n#this method will be used over thousands of iterations to train our coefficents\n#this takes our two coefficents, calculates the gradient, \n## it then uses the gradient to update the coefficients \n## these updated coeffiencents are exported\n\n#by interating many times this method, will keep adjusting the coeffienects, based on their respecive gradients\n## by doing so after a certain amoint of trainin we can make accurate predictions \n\ndef gradient_dec(b_current, m_current, X,Y, lr):\n    #initialise the variables that will store gradients\n    b_gradient = 0\n    m_gradient = np.zeros(m_current.shape)\n    \n    N = float(len(X))\n    \n    #loop through all entries in the dataset\n    for i in range(0, len(X)):\n        x = X[i]#data feature\n        y = Y[i]#data target\n        \n        for n in range(0, len(x)):\n            #calculate the GRADIENT for our two coefficents respectively \n            b_gradient += -(2/N) * (y - ((m_current[n] * x[n]) + b_current))\n        \n            m_gradient[n] += -(2/N) * x[n] * (y - ((m_current[n] * x[n]) + b_current))\n    \n    #new variables to strore the adjusted coeffcients\n    new_b = b_current - (lr * b_gradient)\n    new_m = m_current - (lr * m_gradient)\n    #print(new_b,new_m)\n    #return the adjusted coeffecients \n    return [new_b, new_m]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.array(X.loc[1]),initial_m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is the training method, \n#this method runs the GD method multiple times to get the trained coeffecients\n\n#this method accepts our features, target, initial coeffecients = usually 0 , learning rate and number of training interations\n\ndef train(X,Y, initial_b, initial_m, lr, num_iter):\n    #variables to strore the coeffecients, initiazed here,\n    b = initial_b\n    m = initial_m\n    \n    #run the training loopas many times as specified in number of iterations\n    for i in range(num_iter):\n        #use the gradient_dec method get our slightly more accurate coefficents\n        bx, mx = gradient_dec(b, m, X,Y, lr)\n        \n        #update coeff with new coeff\n        b = bx\n        m = mx\n        \n    #after training is done for the number of loops return coeffecients     \n    return [b, m]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =x\nY =y\nlr = 0.000001 #learning rate \ninitial_b = 0 # initial y-intercept guess\ninitial_m = np.zeros(X.shape[1]) # initial slope guess\nnum_iter = 500# number of 'epochs'\n\n\n# #calculate y-intercept, and slope by training our model \n[b, m] = train(X,Y, initial_b, initial_m, lr, num_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[b, m]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y[56]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error(b, m, X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[56]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(x[56],m,b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# pred_for = [i for i in range(0,22,1)]\n# predictions =[]\n# for yhat in pred_for:\n#     predictions.append(predict(yhat,m,b))\n\n\n# ax = sns.scatterplot(x=x, y=y)\n\n# plt.scatter(x=pred_for, y=predictions, color='r')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}