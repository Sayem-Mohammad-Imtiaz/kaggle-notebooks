{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Please do Vote up if you liked my work\n\nMy [Linkedin](https://www.linkedin.com/in/letian-dai-phd-physics-nanomaterial-nanoscience-nanotechnology-datascience-bigdata/) <br>\nMy [Git](https://github.com/daiwofei)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load datasets\ntrain_df = pd.read_csv('/kaggle/input/human-activity-recognition-with-smartphones/train.csv')\ntest_df = pd.read_csv('/kaggle/input/human-activity-recognition-with-smartphones/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Subject represents the number of participant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#check if the dataset has the missing data\nsns.heatmap(train_df.isnull(),yticklabels='auto',cbar=False,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if the dataset has the missing data\nsns.heatmap(test_df.isnull(),yticklabels='auto',cbar=False,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not missing data in train and test dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a new column named 'ID'\ntrain_df['Id'] = np.arange(1,7353)\ntest_df['Id'] = np.arange(7353,10300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# rename the columns of dataframe otherwise the GCS cannot recognize them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns = np.arange(1,565).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.rename(columns={'564': 'Id','563':'Activity','562':'subject'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.columns = np.arange(1,565).astype(str)\ntest_df = test_df.rename(columns={'564': 'Id','563':'Activity','562':'subject'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save data into a temporary file\ntrain_df.to_csv(path_or_buf = 'train.csv', index = False)\ntest_df.to_csv(path_or_buf = 'test.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialize the clients and move your data to GCS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#REPLACE THIS WITH YOUR OWN GOOGLE PROJECT ID\nPROJECT_ID = 'optimal-chimera-279914'\n#REPLACE THIS WITH A NEW BUCKET NAME. NOTE: BUCKET NAMES MUST BE GLOBALLY UNIQUE\nBUCKET_NAME = 'optimal-chimera-279914'\n#Note: the bucket_region must be us-central1.\nBUCKET_REGION = 'us-central1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the list of AutoML models in my account \nfrom google.cloud import automl\n\n# TODO(developer): Uncomment and set the following variables\n# project_id = \"YOUR_PROJECT_ID\"\n\nclient = automl.AutoMlClient()\n# A resource that represents Google Cloud Platform location.\nproject_location = client.location_path(PROJECT_ID, \"us-central1\")\nresponse = client.list_models(project_location, \"\")\n\nprint(\"List of models:\")\nfor model in response:\n    # Display the model information.\n    if (\n        model.deployment_state\n        == automl.enums.Model.DeploymentState.DEPLOYED\n    ):\n        deployment_state = \"deployed\"\n    else:\n        deployment_state = \"undeployed\"\n\n    print(\"Model name: {}\".format(model.name))\n    print(\"Model id: {}\".format(model.name.split(\"/\")[-1]))\n    print(\"Model display name: {}\".format(model.display_name))\n    print(\"Model create time:\")\n    print(\"\\tseconds: {}\".format(model.create_time.seconds))\n    print(\"\\tnanos: {}\".format(model.create_time.nanos))\n    print(\"Model deployment state: {}\".format(deployment_state))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the information of one model\nfrom google.cloud import automl\n\n# TODO(developer): Uncomment and set the following variables\n# project_id = \"YOUR_PROJECT_ID\"\nmodel_id = \"TBL6336883534082342912\"\n\nclient = automl.AutoMlClient()\n# Get the full path of the model.\nmodel_full_id = client.model_path(PROJECT_ID, \"us-central1\", model_id)\nmodel = client.get_model(model_full_id)\n\n# Retrieve deployment state.\nif model.deployment_state == automl.enums.Model.DeploymentState.DEPLOYED:\n    deployment_state = \"deployed\"\nelse:\n    deployment_state = \"undeployed\"\n\n# Display the model information.\nprint(\"Model name: {}\".format(model.name))\nprint(\"Model id: {}\".format(model.name.split(\"/\")[-1]))\nprint(\"Model display name: {}\".format(model.display_name))\nprint(\"Model create time:\")\nprint(\"\\tseconds: {}\".format(model.create_time.seconds))\nprint(\"\\tnanos: {}\".format(model.create_time.nanos))\nprint(\"Model deployment state: {}\".format(deployment_state))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From there, we'll use our account with the AutoML and GCS libraries to initialize the clients we can use to do the rest of our work. The code below is boilerplate you can use directly, assuming you've entered your own PROJECT_ID and BUCKET_NAME in the previous step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is very important to run this before running the model\nfrom google.cloud import storage, automl_v1beta1 as automl\n\nstorage_client = storage.Client(project=PROJECT_ID)\ntables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=BUCKET_NAME)\nautoml_client = automl.AutoMlClient()\n# Note: AutoML Tables currently is only eligible for region us-central1. \nprediction_client = automl.PredictionServiceClient()\n# Note: This line runs unsuccessfully without each one of these parameters\ntables_client = automl.TablesClient(project=PROJECT_ID, region=BUCKET_REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create your GCS Bucket with your specified name and region (if it doesn't already exist)\nbucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to actually move my local files to GCS, I've copied over a few helper functions from another helpful tutorial Notebook on moving Kaggle data to GCS.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# upload the train and test csv files\n\nupload_blob(BUCKET_NAME, 'train.csv', 'train.csv')\nupload_blob(BUCKET_NAME, 'test.csv', 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train an AUTOML Model\n\nI'll break down the training step for AutoML into three operations:\n\n* Importing the data from your GCS bucket to your autoML client\n* Specifying the target you want to predict on your dataset\n* Creating your model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing from GCS to AutoML\nThe first step is to create a dataset within AutoML tables that references your saved data in GCS. This is relatively straight forward, first just simply choose a name for your dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# each time you should choose a new dataset_display_name (very important) \ndataset_display_name = 'act_recog'\nnew_dataset = False\ntry:\n    dataset = tables_client.get_dataset(dataset_display_name=dataset_display_name)\nexcept:\n    new_dataset = True\n    dataset = tables_client.create_dataset(dataset_display_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And next, give it the path to where the relevant data is in GCS (GCS file paths follow the format gs://BUCKET_NAME/file_path) and import your data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# gcs_input_uris have the familiar path of gs://BUCKETNAME//file\n\nif new_dataset:\n    gcs_input_uris = ['gs://' + BUCKET_NAME + '/train.csv']\n\n    import_data_operation = tables_client.import_data(\n        dataset=dataset,\n        gcs_input_uris=gcs_input_uris\n    )\n    print('Dataset import operation: {}'.format(import_data_operation))\n\n    # Synchronous check of operation status. Wait until import is done.\n    import_data_operation.result()\n#print(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select the Target in your dataset\n\nNow specify which column in your dataset is the target, and which is an ID column (if any). In our case, the target_column is *Activity*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_display_name = 'activity_model'\nTARGET_COLUMN = 'Activity'\nID_COLUMN = 'Id'\n\n# TODO: File bug: if you run this right after the last step, when data import isn't complete, you get a list index out of range\n# There might be a more general issue, if you provide invalid display names, etc.\n\ntables_client.set_target_column(\n    dataset=dataset,\n    column_spec_display_name=TARGET_COLUMN\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make all columns nullable (except the Target and ID Column)\nfor col in tables_client.list_column_specs(PROJECT_ID,BUCKET_REGION,dataset.name):\n    if TARGET_COLUMN in col.display_name or ID_COLUMN in col.display_name:\n        continue\n    tables_client.update_column_spec(PROJECT_ID,\n                                     BUCKET_REGION,\n                                     dataset.name,\n                                     column_spec_display_name=col.display_name,\n                                     type_code=col.data_type.type_code,\n                                     nullable=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create your model\nNow the moment you've all been waiting for, the actual training step! Run the following code below to actually train an AutoML model using the setup we've described. In TRAIN_BUDGET, you can set the maximum amount of time AutoML is allowed to run for, which helps manage both time and cost. Generally, the longer you allow AutoML to run, the better results you can expect and it will automatically stop early if it finds the optimal solution sooner than your allocated budget.\n\nNote: TRAIN_BUDGET is specified in milli-hours, so 1000 refers to 1 hour of wall clock time. Spending more time training (setting a higher TRAIN_BUDGET) is expected to result in more accurate models.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model. This will take hours (up to your budget). AutoML will early stop if it finds an optimal solution before your budget.\n# On this dataset, AutoML usually stops around 2000 milli-hours (2 hours)\n\nTRAIN_BUDGET = 1200 # (specified in milli-hours, from 1000-72000)\nmodel = None\ntry:\n    model = tables_client.get_model(model_display_name=model_display_name)\nexcept:\n    response = tables_client.create_model(\n        model_display_name,\n        dataset=dataset,\n        train_budget_milli_node_hours=TRAIN_BUDGET,\n        exclude_column_spec_names=[ID_COLUMN, TARGET_COLUMN]\n    )\n    print('Create model operation: {}'.format(response.operation))\n    # Wait until model training is done.\n    model = response.result()\n# print(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 4: Batch Predict on your Test Dataset\nNow we're ready to see what AutoML can do! We'll use some code that should look familiar to point our newly created autoML model to our test file and spit out some new predictions.\n\nGo ahead and select the gcs_input_uris based on the location of where your test data is within GCS, and choose a gcs_output_uri_prefix that relates to the path where you'd like your predictions written out to once done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I can go directly to this step if I have already created the model\ngcs_input_uris = 'gs://' + BUCKET_NAME + '/test.csv'\ngcs_output_uri_prefix = 'gs://' + BUCKET_NAME + '/predictions'\n\nbatch_predict_response = tables_client.batch_predict(\n    model=model, \n    gcs_input_uris=gcs_input_uris,\n    gcs_output_uri_prefix=gcs_output_uri_prefix,\n)\nprint('Batch prediction operation: {}'.format(batch_predict_response.operation))\n# Wait until batch prediction is done.\nbatch_predict_result = batch_predict_response.result()\nbatch_predict_response.metadata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 5: Download your predictions\nCongratulations on successfully running batch prediction! Your results can be found within your GCS bucket, and we can use our helper functions from before to download them from GCS into an environment we can work with more easily within our Notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The output directory for the prediction results exists under the response metadata for the batch_predict operation\n# Specifically, under metadata --> batch_predict_details --> output_info --> gcs_output_directory\n# Then, you can remove the first part of the output path that contains the GCS Bucket information to get your desired directory\ngcs_output_folder = batch_predict_response.metadata.batch_predict_details.output_info.gcs_output_directory.replace('gs://' + BUCKET_NAME + '/','')\ndownload_to_kaggle(BUCKET_NAME,'/kaggle/working','tables_1.csv', prefix=gcs_output_folder)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, you're pretty much done! In the last piece of code below, we'll simply generate a csv file in the format that we can use to submit to the competition: namely, with two columns only for ID and the predicted *Activity*.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = pd.read_csv(\"tables_1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_activity = []\nfor i in np.arange(len(preds_df)):\n    ind = np.argmax(preds_df.loc[i][-6:])\n    if ind == 0:\n        predicted_activity.append('LAYING')\n    elif ind == 1:\n        predicted_activity.append('STANDING')\n    elif ind == 2:\n        predicted_activity.append('SITTING')\n    elif ind == 3:\n        predicted_activity.append('WALKING')\n    elif ind == 4:\n        predicted_activity.append('WALKING_UPSTAIRS')\n    else:\n        predicted_activity.append('WALKING_DOWNSTAIRS')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df['Predicted_Activity'] = predicted_activity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = preds_df[['Id', 'Predicted_Activity']]\nsubmission_df.columns = ['Id', 'Activity']\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.sort_values(by = 'Id',axis=0, ascending=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classfication report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (classification_report(test_df['Activity'], submission_df['Activity']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Comparing the standard model offered by scikit-learn module in Python, how much difference of the precision of prediction using AutoML to build the model? ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prepare the Train and Test dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train contains the columes except the 'subject' and 'Activity'\nX_train = train_df.drop(['subject', 'Activity'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# because we want to predict the activities so y_train only contains the 'Activity'\ny_train = train_df['Activity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to make the ML model works, it is necessary to replace the String to Integer of 'Activity'\ninteger_activity = []\nfor i in np.arange(len(y_train)):\n    ind = y_train.loc[i]\n    if ind == 'LAYING':\n        integer_activity.append(0)\n    elif ind == 'STANDING':\n        integer_activity.append(1)\n    elif ind == 'SITTING':\n        integer_activity.append(2)\n    elif ind == 'WALKING':\n        integer_activity.append(3)\n    elif ind == 'WALKING_UPSTAIRS':\n        integer_activity.append(4)\n    else:\n        integer_activity.append(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = integer_activity\ny_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the type of y_train\ntype(y_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train contains the columes except the 'subject' and 'Activity'\nX_test = test_df.drop(['subject', 'Activity'], axis=1)\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# because we want to predict the activities so y_train only contains the 'Activity'\ny_test = test_df['Activity']\ny_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the String to corresponding Integer in y_test\ninteger_activity2 = []\nfor i in np.arange(len(y_test)):\n    ind = y_test.loc[i]\n    if ind == 'LAYING':\n        integer_activity2.append(0)\n    elif ind == 'STANDING':\n        integer_activity2.append(1)\n    elif ind == 'SITTING':\n        integer_activity2.append(2)\n    elif ind == 'WALKING':\n        integer_activity2.append(3)\n    elif ind == 'WALKING_UPSTAIRS':\n        integer_activity2.append(4)\n    else:\n        integer_activity2.append(5)\n\ny_test =integer_activity2\ny_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2.1 - Linear Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nlm = LinearRegression()\nlm.fit(X_train,y_train)\nlm.score(X_test,y_test)\n# get the nearest integer using function .round()\nprediction = lm.predict(X_test).round()\nprint ('Linear regression test accuracy', lm.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is necessary to see the distibution of the predicted values in the range of -2 to 6.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,8])\nplt.hist(prediction, bins = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is interesting to separately get the accuracy for each activity.So we need to separate the X_test to separated groups of activities. \nlabel = test_df['Activity']\nlabel_counts = label.value_counts()\nlabel_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate over each activity\n\nfor activity in label_counts.index:\n    #create dataset\n    act_data = test_df[label==activity].copy()\n    act_X_test = act_data.drop(['subject', 'Activity'], axis=1)\n    act_y_test = act_data['Activity']\n    y_temp = []\n    for x in act_y_test:\n        if x == 'LAYING':\n            y_temp.append(0)\n        elif x == 'STANDING':\n            y_temp.append(1)\n        elif x == 'SITTING':\n            y_temp.append(2)\n        elif x == 'WALKING':\n            y_temp.append(3)\n        elif x == 'WALKING_UPSTAIRS':\n            y_temp.append(4)\n        else:\n            y_temp.append(5)\n\n    act_y_test = y_temp\n\n    y_predict = lm.predict(act_X_test).round()\n    \n    print ('Activity : {}'.format(activity))\n    print ('Classification report : ',classification_report(act_y_test, y_predict))\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 - KNN (K nearest neighbors) Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nerror_rate =[]\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate, color ='red',linestyle='dashed',marker='v',\n        markerfacecolor = 'blue', markersize=10)\nplt.title('Error Rate vs. K value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best value of K is close to 9","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=9) # why 9 is because of Elbow method\nknn.fit(X_train,y_train)\nprint('test accuracy:', knn.score(X_test,y_test))\n# get the nearest integer using function .round()\nprediction = knn.predict(X_test).round()\nprint ('KNN Model test accuracy', knn.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The precision of KNN model is better than linear regression model. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.3 - Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\n# get the nearest integer using function .round()\nprediction = logmodel.predict(X_test).round()\nprint ('Logistic regression test accuracy', logmodel.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The precision of Logistical regression model is better than KNN model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.4 - Decision Tree Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\n# get the nearest integer using function .round()\nprediction = dtree.predict(X_test).round()\nprint ('Decision Tree Classifier test accuracy', dtree.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing the Linear Regression Model, KNN Model and Logistic Regression Model, the precision of Decision Tree Model is the worst.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.5 - Random Forest Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 20)\nrfc.fit(X_train, y_train)\n# get the nearest integer using function .round()\nprediction = rfc.predict(X_test).round()\nprint ('Random Forest Classifier test accuracy', rfc.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## 2.6 - Support Machine Vector Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC(random_state=101)\nsvm.fit(X_train, y_train)\n# get the nearest integer using function .round()\nprediction = svm.predict(X_test).round()\nprint ('Support Machine Vector test accuracy', svm.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.7 LGBM Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nX_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom lightgbm import LGBMClassifier\nlgbm = LGBMClassifier(n_estimators = 500, random_state = 3)\nlgbm = lgbm.fit(X_train, y_train)\n\n# get the nearest integer using function .round()\nprediction = lgbm.predict(X_test).round()\nprint ('LGBM Classifier test accuracy', lgbm.score(X_test, y_test))\nprint (classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [['Cloud AutoML', 0.92], ['Linear Regression Model', 0.85], ['KNN Classifier Model', 0.91], ['Logistic Regression Model', 0.96],\n       ['Decission Tree Classifier Model', 0.86],['Random Forest Classifier Model', 0.92], ['Support Vector Machine Classifier Model', 0.95],\n       ['LGBM Classifier Model', 0.94]] \naccuracy_df = pd.DataFrame(data, columns = ['Models','Accuracy'])\naccuracy_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Conclusion, in this Kaggle, I have used 8 different Machine Learning methods, which contains the \"Cloud AutoML\", \"Linear Regression Model\", \"KNN Classifier Model\", \"Logistic Regression Model\", \"Decission Tree Classifier Model\", \"Random Forest Classifier Model\", \"Support Machine Vector Classifier Model\" and \"LGBM Classifier Model\". I defined the best model of treating this prediction problem is the one who get the highest accuracy. It is the \"Logistic Regression Model\", whose accuracy is 0.96.    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}