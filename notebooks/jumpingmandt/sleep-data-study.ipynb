{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Please do Vote up if you liked my work\n\nMy [Linkedin](https://www.linkedin.com/in/letian-dai-phd-physics-nanomaterial-nanoscience-nanotechnology-datascience-bigdata/) <br>\nMy [Git](https://github.com/daiwofei)\n\nSleep Cycle (SC; NorthCube,Gothenburg,Sweden) is a mobile-phone application that is avaiable on android-based as well as iOS-based devices. SC is a smart alarm-clock that tracks your sleep patterns and wakes you up dring light sleep. SC tracks sleep throughout the night and use a 30-minute window that ends up with the desired alarm time during which the alarm goes off at the lightest possible stage (i.e., light sleep). SC scores sleep through motion detection via one of two motion-detection modes: (i) microphone, which used the built-in microphone to analyze movements, or (ii) accelerometer, which uses the phone's built-in accelerometer. SC tracks movements through the night and uses them to detect and score sleep as well as to plot a graph (hypnogram). By selecting the microphone option to monitor sleep, the SC application uses sound analysis to identify sleep phrases by tracking movements in bed. The SC application uses the smartphone's built-in microphone to pick up sounds from the sleeper. After receiving the sound input, the application then filters the sound using a series of high and low cut-off filters to identify specific noises that correlate with movement. When there is no motion, the application registers deep sleep; when there is little motion, it registers light sleep; when there is a lot of motion, it registers wakefulness. The algorithm SC uses for sleep scoring is not available to the public. Through this method, it is possible to extract 30-s-epoch information about sleep scoring (wake/light sleep/deep sleep) which was also used for measuring sleep parameters. Time in bed (TiB) was calculated based on the \"went to sleep\" and waking times reported by the application. Total sleep time (TST) was calculated by subtracting all the \"awake\" epochs from the TiB. Sleep onset latency (SOL) was calculated by summing up all the awake 30-s-epochs before the first light-sleep epoch. Wake after sleep onset (WASO) was calculated by summing up all the awake epochs that lie between the first light-sleep epoch and the \"woke up\" time. Sleep efficiency SE = (TST/TiB) x 100. (([source from site](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6806072/))","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 - Input the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This CSV is separated by the \"delimiter=;\"\ndf = pd.read_csv('/kaggle/input/sleep-data/sleepdata.csv',delimiter=\";\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that there are many missing data (NaN) in the CSV file. We need to find how many percentage of missing data there are.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#check the null part in the whole data set, red part is missing data\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are missing data in the features \"Wake up\", \"Sleep Notes\" and \"Heart rate\". And we can notice that the feature of \"Time in bed\" is counted from the features \"Start\" and \"End\". As we know, the quality of sleep not only depends on the duration of sleep \"Time in bed\", but also depends on the moment you sleep \"Start\". <br>\n\nThe next step is to convert the \"Start\" and \"End\" to timestamp. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport datetime\n\ndf['Start'] = pd.to_datetime(df['Start'])\ndf['End'] = pd.to_datetime(df['End'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The feature \"Time in bed\" is counted from df['End'] - df['Start']. We can convert it with unit of seconds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Time in bed'] = df['End'] - df['Start']\ndf['Time in bed'] = df['Time in bed'].astype('timedelta64[s]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The sleep quality need to be converted from *string* to *float* type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sleep quality'] = df['Sleep quality'].apply(lambda x: np.nan if x in ['-'] else x[:-1]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am intereted to the moment of falling down in the bed. For example,\"2014-12-29 22:57:49\" is \"22:57:49\", which is ***82669 second*** in a day (***95.68 %*** of a day - percentage in a day)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Start time'] = pd.Series([val.time() for val in df['Start']])\ndf['End time'] = pd.Series([val.time() for val in df['End']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Start time in second'] = df['Start time'].apply(lambda x: (x.hour*60+x.minute)*60 + x.second)\ndf['End time in second'] = df['End time'].apply(lambda x: (x.hour*60+x.minute)*60 + x.second)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try to find the correlation between the features non-null","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# visualisation of this correlation\nfig = plt.figure(figsize = (12,10))\nr = sns.heatmap(df.corr(),cmap='Oranges')\n# set title\nr.set_title('Correlation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's check the correlations of features to the \"sleep quality\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['Sleep quality'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It is clear to see that the \"Time in bed\" is the most related to the \"Sleep quality\" except itself. The \"start time\" of sleep time is more related to the \"end time\". \n\n## The next steps are dealing with the missing data of features \"Wake time\", which could affect the quality of sleep.\nBecause of the heat, the urgent urination or the thirsty during the night, we would wake up to deal with them. Frequent waking up will inevitably lead to a decline in the quality of sleep at night. The source of data only give two cases \":)\", \":|\" and \":(\". I can't be 100% sure about the meaning of these symbols. So I just convert these two symbols to 0 for \":)\" ,1 for \":|\" and 2 for \":(\". \n\nSo, I am looking for the reference about this device provided by ***Northcobe***. I found an article about this topic from the [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6806072/). They compared different methods including commercially available sleep trackers, namely an activity tracker: Mi Band (Xiaomi,Beijing,China); a scientific actigraph: Motionwatch 8 (CamNTech,Cambridge,UK) and a much-used mobile phone application: Sleep Cycle (***Northcube***,Gothenburg,Sweden). \n\nFrom the Discussion part in [Kaggle site](https://www.kaggle.com/danagerous/sleep-data/discussion/80305), Jovita Tamulyte provided some useful information about these symbols. In her opinion, it could be your subjective feeling mood in the morning: <br>\n:) - you feeling great after night (good) <br>\n:| - not so good (average)<br>\n:( - terrible night (bad)<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can replace these symbols with positive and negative number \ndf['Wake up'] = df['Wake up'].replace({':)':2, ':|':1, ':(':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[[\"Sleep quality\", \"Wake up\", \"Time in bed\", \"Start time in second\", \"End time in second\",\"Activity (steps)\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the NaN elements\ndf2 = df2.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the type from object to interger\ndf2['Wake up'] = df2['Wake up'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlations of features to the \"sleep quality\"\ndf2.corr()['Sleep quality'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The next step is to analyze the feature \"Heart rate\", which has the most missing data. \n## Unfortunately, I have no idea what does this \"heart rate\" feature mean? Does it the average heart rate during the sleep or the heart rate at the moment of waking up.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df[[\"Sleep quality\", \"Wake up\",\"Heart rate\",\"Time in bed\", \"Start time in second\", \"End time in second\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the NaN elements\ndf3 = df3.dropna()\ndf3['Wake up'] = df3['Wake up'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlations of features to the \"sleep quality\"\ndf3.corr()['Sleep quality'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualisation of this correlation\nfig = plt.figure(figsize = (12,10))\nr = sns.heatmap(df3.corr(),cmap='Oranges')\n# set title\nr.set_title('Correlation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The \"Sleep quality\" is most affected by \"Time in bed\". ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2 - Explore Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot\nsns.pairplot(df2, hue='Wake up')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joint plot of features \"Sleep quality\" and \"Time in bed\" with unit second.\nsns.jointplot(x='Sleep quality',y='Time in bed',data=df,color='blue',kind = 'kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The average of \"Time in bed\"\n\nprint ('The average time in bed of these users is :', df['Time in bed'].mean(), 'second')\nprint ('The average time in bed of these users is :', df['Time in bed'].mean()/3600, 'hour')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Histogram of Start time and End time\nplt.figure(figsize=(10,6))\ndf['Start in hour'] = df['Start time in second'].apply(lambda x: x/3600)\ndf['End in hour'] = df['End time in second'].apply(lambda x: x/3600)\ndf['Start in hour'].hist(alpha=0.5,color='blue',label='Start Time',bins=50)\ndf['End in hour'].hist(alpha=0.5,color='red',label='End Time',bins=50)\nplt.legend()\nplt.xlim((0, 24)) \nplt.xticks(np.arange(0, 25, 1))\nplt.xlabel('Hour in a day')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Histogram of Steps\nplt.figure(figsize=(10,6))\ndf['Activity (steps)'].hist(alpha=0.5,color='green',label='Steps',bins=50)\nplt.legend()\n\nplt.xlabel('Steps')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From the histogram above, the \"steps\" is not the \"steps\" during the sleep, but the \"steps\" during the day, which represent the activity during the day. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joint plot of features \"Sleep quality\" and \"Activity\" with unit second.\nsns.jointplot(x='Sleep quality',y='Activity (steps)',data=df,color='red',kind = 'kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the non-meaning value of steps (0)\n\ndf_new = df[df['Activity (steps)'] != 0]\ndf_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlations of features to the \"sleep quality\"\ndf_new.corr()['Sleep quality'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot\nplt.figure(figsize=(10,6))\nplt.scatter(df_new['Sleep quality'],df_new['Activity (steps)'], c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',\n            label=\"Sleep quality vs. Steps\")\nplt.xlabel(\"Sleep quality\")\nplt.ylabel(\"Steps during the day\")\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we only look at the correlation between the \"Sleep quality\" and \"Activity (steps)\", they are not that related. According to experience, it is generally believed that a large amount of activity during the day will result in improved sleep quality. However, we didn't see that relation here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3 - Machine Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Train and test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use features of \"Time in bed\",\"Start time in second\", \"End time in second\" and \"Activity (steps)\" to predict the feature \"Sleep quality\"\n# We choose to use df\nX = df[['Time in bed', 'Start time in second','End time in second','Activity (steps)']].values\ny = df['Sleep quality'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to normalize the features, it is better to use MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 - LinearRegression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nlm.score(X_test,y_test)\n\nprint('test accuracy:', lm.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 - KNN (K nearest neighbors) model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nerror_rate =[]\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate, color ='red',linestyle='dashed',marker='v',\n        markerfacecolor = 'blue', markersize=10)\nplt.title('Error Rate vs. K value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=14) # why 5 is because of Elbow method\nknn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test accuracy:', knn.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 - Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\nprint('test accuracy:', logmodel.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 - Decision Tree Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\nprint('test accuracy:', dtree.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 - Random Tree Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=20)\nrfc.fit(X_train,y_train)\nprint('test accuracy:', rfc.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.6 - Support Machine Vector (SVM) Algorithm\n\nSupport vector machines (SVMs) are a set of supervised machine learning methods used for classification, regression and outlier detection.\n\nThe advantages of support vector machines are :\n\n• Effective in high dimentional spaces.\n• Still effective in cases where number of dimensions is greater than the number of samples.\n• Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n• Versatile: different kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines are : \n\n• If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial. \n• SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross validation. \n\n[source from site](https://scikit-learn.org/stable/modules/svm.html)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First SVM model\nfrom sklearn.svm import SVC\nsvm=SVC(random_state=101)\nsvm.fit(X_train, y_train)\nprint('train accuracy:', svm.score(X_train,y_train))\nprint('test accuracy:', svm.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reduce the unnecessary features to improve estimators' accuracy scores then apply gridsearch method\n\nSelectKBest: removes all but the highest scoring features\n\nFor classification generally these methods are used: chi2, f_classif, mutual_info_classif\n\n$\\textbf{chi2}$: Computes chi-squared stats between each non-negative feature and class. This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.\n\n$\\textbf{f_classif}$: Compute the ANOVA F-value for the provided sample.\n\n$\\textbf{mutual_info_classif}$: Estimates mutual information for a discrete target variable. Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n[source from site](https://scikit-learn.org/stable/modules/feature_selection.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\naccuracy_list_train=[]\nk=np.arange(1,5,1)\nfor each in k:\n    x_new = SelectKBest(f_classif, k = each).fit_transform(X_train,y_train)\n    svm.fit(x_new, y_train)\n    accuracy_list_train.append(svm.score(x_new,y_train))\n    \nplt.plot(k, accuracy_list_train, color='green', label='train')\nplt.xlabel('k values')\nplt.ylabel('train accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'best features number': k, 'train_score': accuracy_list_train}\ndf3 = pd.DataFrame(data=d)\nprint ('max accuracy:', df3['train_score'].max())\nprint ('max accuracy id:', df3['train_score'].idxmax())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To sum up,\nprint ('Using the normalisation preprocessing: \\n'\n    'Linear Regresion Model precision:',lm.score(X_test,y_test),'\\n',\n    'KNN Model precision:', knn.score(X_test,y_test),'\\n',\n      'Logistic Regression Model precision:',logmodel.score(X_test,y_test),'\\n',\n      'Decision Tree Model precision:', dtree.score(X_test,y_test),'\\n',\n      'Random Tree Model precision:', rfc.score(X_test,y_test),'\\n',\n      'Support Machine Vector precision:', svm.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best machine learning model for this case is Linear Regression Model with the accuracy about 41%. \n\nBecause Kaggle supports the Machine Learning Engine (AutoML) from Google Cloud Platform. We can try if we can improve the accuracy via the Google ***AutoML***. There is a tutorial about how to use Google AutoML in kaggle from the [link](https://www.kaggle.com/devvret/automl-tables-tutorial-notebook).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Initialize the clients and move your data to GCS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#REPLACE THIS WITH YOUR OWN GOOGLE PROJECT ID\nPROJECT_ID = 'optimal-chimera-279914'\n#REPLACE THIS WITH A NEW BUCKET NAME. NOTE: BUCKET NAMES MUST BE GLOBALLY UNIQUE\nBUCKET_NAME = 'optimal-chimera-279914'\n#Note: the bucket_region must be us-central1.\nBUCKET_REGION = 'us-central1'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From there, we'll use our account with the AutoML and GCS libraries to initialize the clients we can use to do the rest of our work. The code below is boilerplate you can use directly, assuming you've entered your own PROJECT_ID and BUCKET_NAME in the previous step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import storage, automl_v1beta1 as automl\n\nstorage_client = storage.Client(project=PROJECT_ID)\ntables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=BUCKET_NAME)\nautoml_client = automl.AutoMlClient()\n# Note: AutoML Tables currently is only eligible for region us-central1. \nprediction_client = automl.PredictionServiceClient()\n# Note: This line runs unsuccessfully without each one of these parameters\ntables_client = automl.TablesClient(project=PROJECT_ID, region=BUCKET_REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create your GCS Bucket with your specified name and region (if it doesn't already exist)\nbucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to actually move my local files to GCS, I've copied over a few helper functions from another helpful tutorial Notebook on moving Kaggle data to GCS.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the clean data without NaN\ndf_clean = df[['Time in bed', 'Start time in second','End time in second','Activity (steps)','Sleep quality']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename the columns by removing the space ' '\ndf_clean.columns = ['Timeinbed','Starttimeinsecond','Endtimeinsecond','Activity','Sleepquality']\ndf_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomly split the data into train and test set including the features and prediction\ntrain_set = df_clean.sample(frac=0.75, random_state=0)\ntest_set = df_clean.drop(train_set.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a new column named 'ID'\ntrain_set['ID'] = np.arange(1,666)\ntest_set['ID'] = np.arange(666,888)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_set.set_index(np.arange(1,666))\ntrain_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = test_set.set_index(np.arange(666,888))\ntest_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\n# Write the dataframes back out to a csv file, which we can more easily upload to GCS. \ntrain_set.to_csv(path_or_buf='train.csv', index=False)\ntest_set.to_csv(path_or_buf='test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I just run those functions on my train.csv and test.csv files saved locally and all my data is in the right place within Google Cloud Storage.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"upload_blob(BUCKET_NAME, 'train.csv', 'train.csv')\nupload_blob(BUCKET_NAME, 'test.csv', 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Train an AutoML Model\n\nI'll break down the training step for AutoML into three operations:\n\n+ Importing the data from your GCS bucket to your autoML client\n+ Specifying the target you want to predict on your dataset\n+ Creating your model\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing from GCS to AutoML\n\nThe first step is to create a dataset within AutoML tables that references your saved data in GCS. This is relatively straight forward, first just simply choose a name for your dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_display_name = 'sleep_quality'\nnew_dataset = False\ntry:\n    dataset = tables_client.get_dataset(dataset_display_name=dataset_display_name)\nexcept:\n    new_dataset = True\n    dataset = tables_client.create_dataset(dataset_display_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And next, give it the path to where the relevant data is in GCS (GCS file paths follow the format gs://BUCKET_NAME/file_path) and import your data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# gcs_input_uris have the familiar path of gs://BUCKETNAME//file\n\nif new_dataset:\n    gcs_input_uris = ['gs://' + BUCKET_NAME + '/train.csv']\n\n    import_data_operation = tables_client.import_data(\n        dataset=dataset,\n        gcs_input_uris=gcs_input_uris\n    )\n    print('Dataset import operation: {}'.format(import_data_operation))\n\n    # Synchronous check of operation status. Wait until import is done.\n    import_data_operation.result()\nprint(dataset)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This dataset is too small that it has only 665 elements (minimum number is 1000 for Google Cloud AutoML)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}