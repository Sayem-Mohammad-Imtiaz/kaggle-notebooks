{"cells":[{"cell_type":"markdown","source":"This is a work in progress :)","metadata":{"_cell_guid":"c04eb52f-702d-4766-96d6-e08ec0c2e6a7","_uuid":"7a8c9e5d07876a1f259b04e16566774cbba179e7"}},{"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport matplotlib\nmatplotlib.use('Agg')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"2e071134-9229-4696-8bdf-0547808a6dc2","_uuid":"14349886827ad2146a50b4bbf15ef494bbd1e25e"},"execution_count":28},{"outputs":[],"cell_type":"code","source":"df = pd.read_csv('../input/Tweets.csv')","metadata":{"_cell_guid":"8315b8c1-7f52-430f-92f5-d7b546015e92","_uuid":"28bf380ad139d0e1596b4fdfefc25e396b50634d","collapsed":true},"execution_count":29},{"outputs":[],"cell_type":"code","source":"df.head()","metadata":{"_cell_guid":"f618839c-75d6-43d1-9da2-cb85377f1c19","_uuid":"15ea124acf703f6eefc73a87cef5bd36e1a2e053"},"execution_count":30},{"outputs":[],"cell_type":"code","source":"positive = df[df['airline_sentiment']=='positive']\nnegative = df[df['airline_sentiment']=='negative']\nnr_group = negative.groupby(['negativereason',\"airline\"],as_index=False).count()\nnegative['negativereason'].unique()","metadata":{"_cell_guid":"1f3dd29a-2e1a-4d53-bc72-a3767783cbdd","_uuid":"cf33abd25c90606dfc29b8260575f4c549a552dc"},"execution_count":31},{"outputs":[],"cell_type":"code","source":"negative['airline'].unique()","metadata":{"_cell_guid":"74aa7759-6d19-4125-acae-02fb5d51d511","_uuid":"930ef6383a8c48ce084de83d70c8ff73c1a57133"},"execution_count":32},{"outputs":[],"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nf,axarray = plt.subplots(1,2,figsize=(15,10))\ng = sns.barplot(x= nr_group['negativereason'],y=nr_group['tweet_id'],ax=axarray[0])\nfor tick in axarray[0].get_xticklabels():\n        tick.set_rotation(90)\nsns.barplot(x= nr_group['airline'],y=nr_group['tweet_id'],ax=axarray[1])\nfor tick in axarray[1].get_xticklabels():\n        tick.set_rotation(90)\naxarray[0].xaxis.set_label_position('top') \naxarray[1].xaxis.set_label_position('top') ","metadata":{"_cell_guid":"0902fcf6-3e95-4b96-94b0-6e4e3f2ef47d","_uuid":"e19e33ffc05b2f2bc1f7fb616a5bbc9fba43cfa5"},"execution_count":33},{"cell_type":"markdown","source":"Removing @ which mostly refers to the airline and does not add information.","metadata":{"_cell_guid":"f09b53ae-d31b-4215-a296-2cca9b762804","_uuid":"515a5d5121f3a9e176572791e08832eb2006ab19"}},{"outputs":[],"cell_type":"code","source":"import re \ntext = [re.sub(r\"@(\\S*)\",r\"\",text) for text in df['text'].astype('str')]","metadata":{"_cell_guid":"e8bda2a1-1f70-4b9f-b9cb-65b9b8ff1c11","_uuid":"6ddbc31168a4196e9e5cbc28cf99f8ca30cbef8a","collapsed":true},"execution_count":34},{"outputs":[],"cell_type":"code","source":"text[0:5]","metadata":{"_cell_guid":"8500ef13-28a0-4ae4-ac77-1c19a59281b4","_uuid":"f45648b03dd7edd7276c131ed516919a05d7e53d"},"execution_count":35},{"cell_type":"markdown","source":"Removing URLs","metadata":{"_cell_guid":"3ba4f996-f0e9-4e37-8846-8f455866c278","_uuid":"d280e698372f2c45da0d0f18782ced99dd687f92"}},{"outputs":[],"cell_type":"code","source":"text = [re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",t) for t in text]","metadata":{"_cell_guid":"babe2ca4-cf8d-49eb-aa0d-e14f3711baee","_uuid":"b5e72caa4788db29752a3038d67426174faf30f7","collapsed":true},"execution_count":36},{"cell_type":"markdown","source":"Recognize Emoticons and transform them into text. For inscante turn :) into 'Happy'.","metadata":{"_cell_guid":"88661aea-be88-4411-a6a0-3ab1cc605fed","_uuid":"88c191b42425b489c219db553c15b4d20a463d06"}},{"outputs":[],"cell_type":"code","source":"#happyness\ntext = [re.sub(r':\\)|:-\\)|:D|;\\)|;-\\)|:-D',\"HAPPY\",t) for t in text]\n#sadness\ntext = [re.sub(r':\\(|:-\\(|;\\(|;-\\(',\"SAD\",t) for t in text]\n","metadata":{"_cell_guid":"573814f7-d75f-4feb-b580-da25aa6a4c29","_uuid":"e889103518651fb2ccefb2b3bfbb15c04bcca3bc","collapsed":true},"execution_count":37},{"outputs":[],"cell_type":"code","source":"text[0:5]","metadata":{"_cell_guid":"74e2f4e8-9ce0-4127-9a72-bce7c8e04f4b","_uuid":"9a9f653013c946795ea1126abd085ed34ed74ede"},"execution_count":38},{"outputs":[],"cell_type":"code","source":"import unicodedata\n#text = [re.sub('u[\\U0001F602-\\U0001F64F]', lambda m: unicodedata.name(m.group()), t,flags=re.UNICODE) for t in text]","metadata":{"_cell_guid":"c386e1d2-895c-43ba-a32f-aec287dd3a45","_uuid":"098c2bed55b4927fa3dd7e4e857940d31d1c6909","collapsed":true},"execution_count":39},{"outputs":[],"cell_type":"code","source":"text = [re.sub(\"[^a-zA-Z]\",\" \",t).lower() for t in text]\n","metadata":{"_cell_guid":"16c292e2-0926-4806-a6bd-121ac80e3754","_uuid":"4e9435a8422ce79b01420e0314be62205c80dcf0","collapsed":true},"execution_count":40},{"outputs":[],"cell_type":"code","source":"text = pd.Series(text)","metadata":{"_cell_guid":"054c29ea-760c-4738-b61d-273b9fe44135","_uuid":"122f9f4d38ab18ba5ee0b4629890ad3ac05a8e82","collapsed":true},"execution_count":41},{"cell_type":"markdown","source":"## Bag of Words","metadata":{"_cell_guid":"3a7eebd2-4317-4ef8-a69c-217fd855ed4c","_uuid":"76b5766660eca864c28a0680f7081ca6fd78334a"}},{"outputs":[],"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,   \n                             max_features = 5000) \ntdf = vectorizer.fit_transform(text)\n","metadata":{"_cell_guid":"911a4472-9fce-4d1c-9020-69dd026558c5","_uuid":"cb42c7c0c32c8fb6ae8ce2a16e25be1fabf244d0","collapsed":true},"execution_count":15},{"outputs":[],"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators = 20) \nscores = cross_val_score(clf, tdf.toarray(), df[\"airline_sentiment\"], cv=5)\nscores.mean()","metadata":{"_cell_guid":"1b345295-60db-4fc9-a347-f56fec36f1cb","_uuid":"27e186cb0c460288a5d85d36f0169447a49f36bb"},"execution_count":16},{"cell_type":"markdown","source":"","metadata":{"_cell_guid":"a71227c7-6d23-4d9c-9da5-4684bfdb1677","_uuid":"c8cb2ffa9f36e24798ece53928f93cac6a3dfd9e"}},{"cell_type":"markdown","source":"## Tri-Grams","metadata":{"_cell_guid":"7e0e0679-39ca-4f3e-97d4-35cf25360a68","_uuid":"bf556111e0ec8dcb2a42126e0083e6ea867fd13f"}},{"cell_type":"markdown","source":"Let's now try using 3-grams which ar sequences of 3 words. ","metadata":{}},{"outputs":[],"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,\n                             ngram_range=(3,3),   \n                             max_features = 5000) \ntdf = vectorizer.fit_transform(text) ","metadata":{"_cell_guid":"f293eeb0-18dc-44a9-9de9-c8acb8c67c3e","_uuid":"64913e5f48ae66c4765e5354d3035c4ee7f01ce3","collapsed":true},"execution_count":17},{"outputs":[],"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators = 20) \nscores = cross_val_score(clf, tdf.toarray(), df[\"airline_sentiment\"], cv=5)\nscores.mean()","metadata":{"_cell_guid":"dbed7944-2458-4545-8c88-53ecb41a28f4","_uuid":"1f6743d53016bb73fa371d715a6f200a8ce2cebf","collapsed":true},"execution_count":18},{"cell_type":"markdown","source":"## NLTK","metadata":{"_cell_guid":"38be329d-c69c-4cc2-bc9f-30acf2fbf572","_uuid":"22afc50c0f125bcef66d2978c378d8cfb6fe7eca","collapsed":true}},{"cell_type":"markdown","source":"I am now going to exploit a Natural Language processing library, called NLTK ( Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. Oâ€™Reilly Media Inc.) to extract some valuable information from tweet I hope to use to improve my models.","metadata":{"_cell_guid":"ee1332ea-a996-4d41-92fb-b472ddf270fc","_uuid":"77164146b4600e861e88ef4c5fee8cf80441d2a0"}},{"cell_type":"markdown","source":"So far our tweets are still strings, without weird symbols and punctuations but strings. The first thing we are going to do is to tokenize them  which means we are going to create lists of words.","metadata":{"_cell_guid":"991b7026-1339-4207-90cb-1f9ba5e7218a","_uuid":"d76417871be93afc05be65b3ae27c52ca9218601"}},{"outputs":[],"cell_type":"code","source":"text.head()","metadata":{"_cell_guid":"5445dd73-55d1-4132-8dad-5f86adf7c405","_uuid":"4d9da0b06ab23dd3618150329b40a590224e5879"},"execution_count":42},{"outputs":[],"cell_type":"code","source":"import nltk\ntokens = [nltk.word_tokenize(t) for t in text]","metadata":{"_cell_guid":"1e5ae561-d161-44f4-9bc6-a519c89c96b5","_uuid":"16f579acf2390a50727f71c37aad72d454f6707f","collapsed":true},"execution_count":43},{"outputs":[],"cell_type":"code","source":"tokens[0:2]","metadata":{"_cell_guid":"9fc93921-2d12-4cd6-b15a-1cf95e830bfb","_uuid":"e47946b1300dffb6dc111c03ca26b943179c5a13"},"execution_count":44},{"cell_type":"markdown","source":"Nowe we are going to pos tag each tweet. Pos Tagging, which stands for Part Of Speech Tagging is the process through which each word of a sentence is paired with its role (Noun, Verb, Adjective, etc.) in that sentence. https://en.wikipedia.org/wiki/Part-of-speech_tagging","metadata":{"_cell_guid":"6c7eadf6-8e59-4638-adf1-097a179030fe","_uuid":"9331727ccdbaa9a46751993064b91c0a70badd6b"}},{"outputs":[],"cell_type":"code","source":"tags = [nltk.pos_tag(t) for t in tokens]","metadata":{"_cell_guid":"36facd22-8fa7-4734-b340-6cdd1512d03a","_uuid":"674255d8518c4c374c0d04a588a24c1a77ad8fd0","collapsed":true},"execution_count":45},{"outputs":[],"cell_type":"code","source":"tags[4]","metadata":{"_cell_guid":"84b7bd1c-e4f7-4250-b3bb-a0e8665c6f39","_uuid":"ca49d98319cfac0e836d96f9627225f1638bc791"},"execution_count":46},{"outputs":[],"cell_type":"code","source":"adjectives = []\nfor i in range(len(tags)):\n    tmp = []\n    for j in range(len(tags[i])):\n        if tags[i][j][1]=='JJ' or tags[i][j][1]== 'JJS' or tags[i][j][1]=='JJR' or tags[i][j][1]=='VBG' or tags[i][j][1]== 'VBG' or tags[i][j][1]=='VBD':\n            tmp.append(tags[i][j][0])\n    adjectives.append(tmp)","metadata":{},"execution_count":129},{"outputs":[],"cell_type":"code","source":"adjectives[0:5]","metadata":{},"execution_count":130},{"outputs":[],"cell_type":"code","source":"adjectives = [\" \".join(adj) for adj in adjectives]","metadata":{},"execution_count":131},{"outputs":[],"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nadj_vec = CountVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,\n                             ngram_range=(1,1),   \n                             max_features = 5000) \nadf = adj_vec.fit_transform(adjectives) ","metadata":{},"execution_count":132},{"outputs":[],"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators = 10) \nscores = cross_val_score(clf, adf.toarray(), df[\"airline_sentiment\"], cv=5)\nscores.mean()","metadata":{},"execution_count":133},{"outputs":[],"cell_type":"code","source":"","metadata":{"collapsed":true},"execution_count":null}],"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.1","name":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}