{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport random\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.layers import Activation, Dense, LSTM\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nyoutube_data = pd.read_csv('/kaggle/input/youtube-new/USvideos.csv')\ncategories = pd.read_json('/kaggle/input/youtube-new/US_category_id.json')\n\nadditional_data = pd.read_csv('/kaggle/input/youtube-trending-video-dataset/US_youtube_trending_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Combine datasets & remove all categories except music (id=10) and sport (id=17) \ntitles1 = pd.DataFrame({\"title\": youtube_data.title, \"category_id\": youtube_data.category_id })\ntitles2 = pd.DataFrame({\"title\": additional_data.title, \"category_id\": additional_data.categoryId})\nall_data = titles1.append(titles2).drop_duplicates(subset='title', keep=\"last\")\n\nall_data.groupby(\"category_id\").apply(len) # Number of titles in each category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create category dict\ncategory_dict = {}\nfor i in categories[\"items\"]:\n    category_dict[int(i['id'])] = i['snippet']['title']\n\nyoutube_data['category'] = youtube_data['category_id'].map(category_dict)\n\ncategory_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifier functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data_with_categories(data, id1, id2):\n    data = data[data[\"category_id\"].isin([id1,id2])]\n    data[\"label\"] = (data[\"category_id\"] == id2).astype(int) # id1: label=0, id2: label=1\n    data[\"generated\"] = 0\n    return data\n\ndef train_and_run_classifier(data):\n    # Split train x, y and test x, y\n    titles = data.title[data.generated == 0].values\n    y = data.label[data.generated == 0].values\n\n    titles_train, titles_test, y_train, y_test = train_test_split(titles, y, test_size=0.25, random_state=123)\n    \n    # Generated data will only be used for train\n    titles_train = np.append(titles_train, data.title[data.generated == 1].values)\n    y_train = np.append(y_train, data.label[data.generated == 1].values)\n    \n    # Vectorize train\n    vectorizer = CountVectorizer()\n    vectorizer.fit(titles_train)\n\n    X_train = vectorizer.transform(titles_train)\n    X_test  = vectorizer.transform(titles_test)\n    \n    # Do logistic regression and report test accuracy\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    return classifier.score(X_test, y_test)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word-level generator functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\nimport itertools\nimport nltk\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\nfrom keras.layers import LSTM\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n\nclass KerasBatchGenerator(object):\n    def __init__(self, data, num_steps, batch_size, vocabulary, word_to_index, skip_step=5):\n        self.data = data\n        self.num_steps = num_steps\n        self.batch_size = batch_size\n        self.vocabulary = vocabulary\n        # this will track the progress of the batches sequentially through the\n        # data set - once the data reaches the end of the data set it will reset\n        # back to zero\n        self.current_idx = 0\n        # skip_step is the number of words which will be skipped before the next\n        # batch is skimmed from the data set\n        self.skip_step = skip_step\n        \n        self.word_to_index = word_to_index\n\n    def generate(self):\n        x = np.zeros((self.batch_size, self.num_steps))\n        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n        while True:\n            i = 0\n            while i < self.batch_size:\n                # I don't want to see in x a title end token to predict y \n                if self.current_idx < len(self.data) and self.data[self.current_idx] == self.word_to_index[title_end_token]:\n                    self.current_idx += self.skip_step\n                if self.current_idx + self.num_steps >= len(self.data):\n                    # reset the index back to the start of the data set\n                    self.current_idx = 0\n                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n                # convert all of temp_y into a one hot representation\n                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n                self.current_idx += self.skip_step\n                i += 1\n            yield x, y\n            \n            \ntitle_start_token = \"TITLE_START\"\ntitle_end_token = \"TITLE_END\"\nunknown_token = \"UNKNOWN_TOKEN\"\n\ndef create_and_train_generator(data, category_id, vocabulary_size=250):\n    #Tokenize\n    titles = data.title[data[\"category_id\"] == category_id].to_numpy()\n\n    sentences = itertools.chain(*[nltk.sent_tokenize(x.lower()) for x in titles])\n    tokenized_titles = [\"%s %s %s\" % (title_start_token, x, title_end_token) for x in sentences]\n    tokenized_titles = [nltk.word_tokenize(title) for title in tokenized_titles]\n    final_title = []\n    word_freq = {}\n    for title in tokenized_titles:\n        final_title.append([token for token in title if token.isalpha() or token == title_start_token or token == title_end_token])\n        for word in title:\n            if word in word_freq.keys():\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n    tokenized_titles = final_title\n    \n    # Create vocabulary\n    sorted_words = sorted(word_freq, key = word_freq.get, reverse = True)\n    index_to_word = sorted_words[:vocabulary_size-1]\n    index_to_word.append(unknown_token)\n    word_to_index = dict([(word, index) for index, word in enumerate(index_to_word)])\n\n    # Replace non common words with unknown token\n    for i, sent in enumerate(tokenized_titles):\n        tokenized_titles[i] = [w if w in word_to_index else unknown_token for w in sent]\n        \n    # Create training data\n    num_steps = 1\n    skip_step = 1\n    batch_size = 10\n    train_data_proportion = 0.90\n\n    # Create the training data\n    # A concatenation of all tokens as integers (indices)\n    X = list(itertools.chain(*np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_titles])))\n    \n    split_index = round(len(X) * train_data_proportion)\n    \n    # Create 2 batch generators out of the concatenation\n    train_data_generator = KerasBatchGenerator(X[:split_index], num_steps, batch_size, vocabulary_size, word_to_index, skip_step)\n    valid_data_generator = KerasBatchGenerator(X[split_index + 1:], num_steps, batch_size, vocabulary_size, word_to_index, skip_step)    \n        \n    # Create model    \n    hidden_size = 250\n\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, hidden_size, input_length=num_steps))\n    model.add(LSTM(hidden_size, return_sequences=True))\n    model.add(LSTM(hidden_size, return_sequences=True))\n    model.add(Dropout(rate=0.5))\n    model.add(TimeDistributed(Dense(vocabulary_size)))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n    \n    # Train model\n    num_epochs = 7\n    model.fit_generator(train_data_generator.generate(), len(X[:split_index])//(batch_size*num_steps), num_epochs, validation_data=valid_data_generator.generate(), validation_steps=len(X[split_index+1:])//(batch_size*num_steps))\n    return model, word_to_index, index_to_word\n\n\ndef generate_one_title(model, word_to_index, index_to_word):\n    # We start the sentence with the start token\n    new_title = [word_to_index[title_start_token]]\n    # Repeat until we get an end token\n    while not new_title[-1] == word_to_index[title_end_token]:\n        x = np.zeros((1,1))\n        x[0, :] = new_title[-1]\n        next_word_probs = model.predict(x)[0][0]\n        sampled_word = word_to_index[unknown_token]\n        # We don't want to sample unknown words\n        while sampled_word == word_to_index[unknown_token]:\n            next_word_probs = next_word_probs.astype(float)\n            next_word_probs /= next_word_probs.sum()\n            #next_word_probs = np.asarray(next_word_probs).astype('float64') # To avoid rounding errors\n            samples = np.random.multinomial(1, next_word_probs)\n            sampled_word = np.argmax(samples)\n        new_title.append(sampled_word)\n    title_str = [index_to_word[x] for x in new_title[1:-1]]\n    return title_str\n\n    \ndef generate_titles(model, word_to_index, index_to_word, num_titles):\n    senten_min_length = 2\n    senten_max_length = 8\n\n    new_titles = []\n    for i in range(num_titles):\n        sent = []\n        # We want long sentences, not sentences with one or two words\n        while len(sent) < senten_min_length or len(sent) > senten_max_length:\n            sent = generate_one_title(model, word_to_index, index_to_word)\n        title = \" \".join(sent)\n        #print(title)\n        new_titles.append(title)\n        if i % 100 == 0:\n            print(\"Generated\", i, \"titles\")\n    \n    return new_titles\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main program"},{"metadata":{},"cell_type":"markdown","source":"Run cells below to train a classifier for two categories, generate new titles in those categories and train a new classifier with the added generated titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose which categories to classify\ncategory_id1 = 1\ncategory_id2 = 20\n\n\ndata = get_data_with_categories(all_data, category_id1, category_id2)\n\nscore = train_and_run_classifier(data)\n\n# Create models\nvocabulary_size = 5\nmodel1, word_to_index1, index_to_word1 = create_and_train_generator(data, category_id1, vocabulary_size)\nmodel2, word_to_index2, index_to_word2 = create_and_train_generator(data, category_id2, vocabulary_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generated_titles2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_titles = 300\nprint(\"Generating titles of category\", category_id1)\ngenerated_titles1 = generate_titles(model1, word_to_index1, index_to_word1, num_titles)\nprint(\"Generating titles of category\", category_id2)\ngenerated_titles2 = generate_titles(model2, word_to_index2, index_to_word2, num_titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"append_titles = list(range(0,num_titles+1,100))\naug_scores = []\nfor index in append_titles:\n    aug_data = data.append(pd.DataFrame({\"title\": generated_titles1[:index], \"category_id\": category_id1, \"label\": 0, \"generated\": 1}))\n    aug_data = aug_data.append(pd.DataFrame({\"title\": generated_titles2[:index], \"category_id\": category_id2, \"label\": 1, \"generated\": 1}))\n    aug_score = train_and_run_classifier(aug_data)\n    aug_scores.append(aug_score)\n    \nprint(\"Accuracy before augmentation:\", score)\nbest_score_index = np.argmax(aug_scores)\nprint(\"Best accuracy after augmenting\", append_titles[best_score_index], \"titles:\", aug_scores[best_score_index])\nfig = plt.figure(figsize=(15,10))\nax = fig.add_axes([0,0,1,1])\nax.bar([str(elem) for elem in append_titles], aug_scores)\n\nax.set_ylabel('Test accuracy')\nax.set_title('Accuracy vs. # of augmented titles (Vocabulary size =' + str(vocabulary_size) + ')')\nplt.ylim(0.8,0.9)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rick and Morty: Why Morty Matters\n#['TITLE_START','rick','and','morty','why','morty','UNKNOWN_TOKEN','TITLE_END']\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}