{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Sales in King County, USA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The purpose of this notebook is to gain a deeper understanding of linear regression by applying it to the \"House Sales in King County\" dataset. I will aim to archieve the highest possible prediction score by checking the underlying assumptions of a linear regression model and taking appropiate actions if needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.) Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1.) Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't hava any missing values in our dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.2.) Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['id', 'date'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All our variables are numercial except for 'date'. Even though being numerical, variables like view, condition, etc. can be considered as categorical since they are not continuous. For those variables we could use an encoding method (e.g. dummy variables). However, I've decided to not consider those variables as categorical. Since I decided to not include 'date' in the regression, I will drop it along with 'id'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.) Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns=['price'])\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)\ny_pred = model_lr.predict(X_test)\nprint(\"Training set score: {:.7f}\".format(model_lr.score(X_train, y_train)))\nprint(\"Test set score: {:.7f}\".format(model_lr.score(X_test, y_test)))\nprint(\"RMSE: {:.7f}\".format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With just a simple multiple regression we can already archive a r2-score of ~0.7. This is not optimal but not too bad for now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.) Check Model Adequacy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To improve our score we should take a look at the assumptions our model is making. We are using linear regression so we should check our data regarding:\n\n- Linearity\n- Outliers\n- Homoscedasticity\n- Normality\n- Multicollinearity\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.1.) Linearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Actual vs. Predicted Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_residuals(model, features, label):\n    predictions = model_lr.predict(features)\n    df_result = pd.DataFrame({'Actual':label, 'Predicted':predictions})\n    df_result['Residuals'] = abs(df_result['Actual']) - abs(df_result['Predicted'])\n    return df_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_assumption(model, features, label):\n    df_result = calculate_residuals(model, features, label)\n    fig1, ax1 = plt.subplots(figsize=(12,8))\n    ax1 = sns.regplot(x='Actual', y='Predicted', data=df_result, color='steelblue')\n    line_coords = np.arange(df_result.min().min(), df_result.max().max())\n    ax1 = plt.plot(line_coords, line_coords,  # X and y points\n              color='indianred')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_assumption(model_lr, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = calculate_residuals(model_lr, X_test, y_test)\nfig2, ax2 = plt.subplots(figsize=(12,8))\nax2.scatter(x=df_result['Predicted'], y=df_result['Residuals'], color='steelblue')\nplt.axhline(y=0, color='indianred')\nax2.set_ylabel('Residuals', fontsize=12)\nax2.set_xlabel('Predicted', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check the linearity of our model by looking at the actual vs. predicted plot or the predicted vs. residuals plot. For linearity in case of the former, the data points should be symmetrically distributed around the diagonal line. This is not the case here as our predictions are biased especially with higher values. Similarly, for linearity in case of the latter, the data points should be symmetrically distributed around the horizontal line. As observed though, the residual variance increases with higher values. This indicates the violation of the underlying assumptions and are dealt with in a later section. \n(I found the functions on this blog: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we get back to the linearity assumption, we will focus on a different problem that is appearent in the plots: Outliers. Our model failed to predict certain values by a significant amount, thus it is worth it to have a look at the price distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nfig3, ax3 = plt.subplots(figsize=(15,4))\nax3 = sns.boxplot(x=df['price'], color='steelblue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[~(df['price']>4000000)]\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the boxplot that there are just a few data points where the price exceeds 4,000,000. After examining the data points I reached the conclusion that these \"outliers\" are not due to a mistake (false entry, etc.) because the high prices seem plausible to some degree given the underlying attributes (sqft_living, grade, etc.). However, I will omit all the data where the price exceeds 4,000,000 as there are only 11 entries and the regression is affected (as shown in the residual and the actual vs. predicted plots). This implies that our model only applies to a certain price range (price < 4,000,000).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Regression without outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = df1.drop(columns=['price'])\ny1 = df1['price']\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr1 = LinearRegression()\nmodel_lr1.fit(X1_train, y1_train)\ny1_pred = model_lr1.predict(X1_test)\nprint(\"Training set score: {:.7f}\".format(model_lr1.score(X1_train, y1_train)))\nprint(\"Test set score: {:.7f}\".format(model_lr1.score(X1_test, y1_test)))\nprint(\"RMSE: {:.7f}\".format(np.sqrt(metrics.mean_squared_error(y1_test, y1_pred))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_assumption(model_lr1, X1_test, y1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = calculate_residuals(model_lr1, X1_test, y1_test)\nfig4, ax4 = plt.subplots(figsize=(12,8))\nax4.scatter(x=df_result['Predicted'], y=df_result['Residuals'], color='steelblue')\nax4.set_ylabel('Residuals', fontsize=12)\nax4.set_xlabel('Predicted', fontsize=12)\nplt.axhline(y=0, color='indianred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Omitting the \"outliers\" in the regression model led to a slight improvement of the score. We also got rid of the outliers from our actual vs. predicted and residual plots. However, the problem of non-linearity still remains. Let's dive deeper into that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nsns.pairplot(df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade']],  \n             y_vars=['price'], x_vars=['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade'], \n             height=5, plot_kws={'color':'steelblue'}) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nsns.pairplot(df[['price','sqft_above', 'sqft_basement', 'lat', 'sqft_living15']],  \n             y_vars=['price'], x_vars=['sqft_above', 'sqft_basement', 'lat', 'sqft_living15'], height=5,\n             plot_kws={'color':'steelblue'}) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the correlation plot I chose to plot the relationship of the variable price and the above seen variables. None of the independent variables show perfect linearity. The variables sqft_living, sqft_above and sqft_living15 show certain degrees of linearity. However, this is not optimal for our regression model. To deal with the non-linearity we will try to perform a polynomial regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2)\nX_train_poly = poly.fit_transform(X1_train)\nX_test_poly = poly.fit_transform(X1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr_poly = LinearRegression()\nmodel_lr_poly.fit(X_train_poly, y1_train)\ny_pred_poly = model_lr_poly.predict(X_test_poly)\nprint(\"Training set score: {:.7f}\".format(model_lr_poly.score(X_train_poly, y1_train)))\nprint(\"Test set score: {:.7f}\".format(model_lr_poly.score(X_test_poly, y1_test)))\nprint(np.sqrt(metrics.mean_squared_error(y1_test, y_pred_poly)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig10, ax10 = plt.subplots(figsize=(12,8))\nax10 = sns.regplot(x=y1_test, y=y_pred_poly, color='steelblue')\nline_coords = np.arange(df_result.min().min(), df_result.max().max())\nplt.plot(line_coords, line_coords, color='indianred')\nax10.set_ylabel('Predicted', fontsize=12)\nax10.set_xlabel('Actual', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = calculate_residuals(model_lr1, X1_test, y1_test)\nfig11, ax11 = plt.subplots(figsize=(12,8))\nax11.scatter(x=y_pred_poly, y=y1_test-y_pred_poly, color='steelblue')\nax11.set_ylabel('Residuals', fontsize=12)\nax11.set_xlabel('Predicted', fontsize=12)\nplt.axhline(y=0, color='indianred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A polynomial regression (degree: 2), often considered a special kind of linear regression, significantly improves our r2 score! The distributions of the data points in our actual vs. predicted and residual plots looks better too. However, we can still observe what seems to be outliers and inaccurities. As mentioned earlier, especially in the residual plot we can still see increased variances with higher values. This is an indicator for heteroscedasticity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.2.) Homoscedasticity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Homoscedasticity describes constant variance in the residuals as can be observed in a residual plot. Ideally, the residuals should be distributed evenly around the horizontal line without any increasing or decreasing trend. This is not the case in our residual plot so we will try to log-transform the dependent variable in order to tackle heteroscedasticity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1.drop(columns=['price'])\nprice_trans = np.log1p(df1['price'])\nX2_train, X2_test, y2_train, y2_test = train_test_split(X, price_trans, test_size=0.3, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2)\nX2_train_poly = poly.fit_transform(X2_train)\nX2_test_poly = poly.fit_transform(X2_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr_poly2 = LinearRegression()\nmodel_lr_poly2.fit(X2_train_poly, y2_train)\ny2_pred_poly = model_lr_poly2.predict(X2_test_poly)\nprint(\"Training set score: {:.7f}\".format(model_lr_poly2.score(X2_train_poly, y2_train)))\nprint(\"Test set score: {:.7f}\".format(model_lr_poly2.score(X2_test_poly, y2_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig13, ax13 = plt.subplots(figsize=(12,8))\nax13.scatter(x=y2_pred_poly, y=y2_test-y2_pred_poly, color='steelblue')\nax13.set_ylabel('Residuals', fontsize=12)\nax13.set_xlabel('Predicted', fontsize=12)\nplt.axhline(y=0, color='indianred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming our dependent variable increased our r2 score slightly. More importantly, the residuals seem to be more evenly distributed around the horizontal line than before. However, we can still observe many outliers. \n(Any suggestions on how i should proceed with the outlier problem is welcomed)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.3.) Normality","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The normality assumption can easily be observed by plotting the residual histogram or the QQ-plot of the residuals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig15, ax15 = plt.subplots(figsize=(12,8))\nsns.distplot(y2_test-y2_pred_poly, color='steelblue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfig16, ax16 = plt.subplots(figsize=(8,5))\nstats.probplot(y2_test-y2_pred_poly, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both plots show that the normality assumption is met. We archieved this by log-transforming our dependent variable in the previos step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.4.) Multicollinearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## VIF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"One method to identify which variables are affected by multilinearity is the Variation Inflation Factor (VIF). A value of >10 indicates multicollinearity. Let's check the VIF's for our X_test from the very first regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n# from statsmodels.tools.tools import add_constant\n\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X_test.values, i) for i in range(X_test.shape[1])]\nvif[\"features\"] = X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif['VIF'] = vif['VIF'].apply(lambda x: \"{:.2f}\".format(x))\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, most of our variables are affected by multicollinearity. This is a problem especially when interpreting the coefficients and the individual effects the independent variables have on the dependent variable. However, our goal here is primarily prediction precision, so we don't have to worry about collinearity too much.\n(this blog amongst others provide an overview on when multicollinearity needs to be tackled and when not: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.) Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After checking the underlying assumptions of a linear regression model and taking the appropiate actions, we archieved an r2 score of ~0.82. There is definitely room for improvement and we should also consider different regression models aswell. Any suggestions on how I could improve or make changes in the model would be highly appreciated.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}