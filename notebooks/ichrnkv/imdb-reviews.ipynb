{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport collections\nimport numpy as np \nimport pandas as pd \nimport copy\nimport re\nimport traceback\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# чтение данных\n\ndef read_data(path_to_data):\n    df = pd.read_csv(path_to_data)\n    df['sentiment'] = df['sentiment'].map({'positive':0, 'negative':1})\n    return df['review'], df['sentiment']\n\nX, y = read_data('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# разделение данных на трейн и тест\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   test_size=0.3,\n                                                   stratify=y)\n\nprint('Number of 1/0 classes elements in train: {}'.format(np.bincount(y_train)))\nprint('Number of 1/0 classes elements in test: {}'.format(np.bincount(y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\n\n# токенизируем\ntrain_tokenized = tokenize_corpus(X_train.values)\ntest_tokenized = tokenize_corpus(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_fake_token(word2id, token='<PAD>'):\n    word2id_new = {token: i + 1 for token, i in word2id.items()}\n    word2id_new[token] = 0\n    return word2id_new\n\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n# строим словарь\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\nprint('\\n'.join(' '.join(str(t) for t in sent)\n                for sent in train_token_ids[:1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\n\nplt.hist([len(s) for s in train_token_ids], bins=100);\nplt.xticks(np.arange(0, 6000, 500))\nplt.title('Гистограмма длин предложений');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensure_length(txt, out_len, pad_value):\n    if len(txt) < out_len:\n        txt = list(txt) + [pad_value] * (out_len - len(txt))\n    else:\n        txt = txt[:out_len]\n    return txt\n\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, texts, targets, out_len=100, pad_value=0):\n        self.texts = texts\n        self.targets = targets\n        self.out_len = out_len\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n    \n    \nMAX_SENTENCE_LEN = 500\ntrain_dataset = PaddedSequenceDataset(train_token_ids,\n                                      y_train.values,\n                                      out_len=MAX_SENTENCE_LEN)\ntest_dataset = PaddedSequenceDataset(test_token_ids,\n                                     y_test.values,\n                                     out_len=MAX_SENTENCE_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# архитектура нейросети\n\nclass Net(torch.nn.Module):\n    def __init__(self, vocabulary_size, embedding_size, n_hidden_neurons, n_classes):\n        super(Net, self).__init__()\n        self.emb_layer = torch.nn.Embedding(vocabulary_size, embedding_size, padding_idx=0)\n        \n        self.conv1 = torch.nn.Conv1d(in_channels=embedding_size,\n                                     out_channels=200,\n                                     kernel_size=5,\n                                     padding=2)\n        \n        self.conv2 = torch.nn.Conv1d(in_channels=100,\n                                     out_channels=200,\n                                     kernel_size=5,\n                                     padding=2)\n        \n        self.conv3 = torch.nn.Conv1d(in_channels=100,\n                                     out_channels=50,\n                                     kernel_size=5,\n                                     padding=2)\n        \n        \n        self.fc1 = torch.nn.Linear(62*25, n_hidden_neurons)\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        self.fc3 = torch.nn.Linear(n_hidden_neurons, n_classes)\n        \n        self.max_pool = torch.nn.MaxPool2d(kernel_size=2,\n                                        stride=2)\n        self.act = torch.nn.ReLU()\n        self.dropout50 =  torch.nn.Dropout(p=0.5)\n        self.sm = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.emb_layer(x)\n        x = x.permute(0, 2, 1)\n        \n        x = self.conv1(x) # 200 x 500\n        x = self.max_pool(x) # 100 x 250\n        x = self.dropout50(x)\n        x = self.act(x)\n        \n        x = self.conv2(x) # 200 x 250\n        x = self.max_pool(x) # 100 x 125\n        x = self.dropout50(x)\n        x = self.act(x)\n        \n        \n        x = self.conv3(x) # 50 x 125\n        x = self.max_pool(x) # 25 x 62\n        x = self.dropout50(x)\n        x = self.act(x)\n        \n        x = x.view(x.size(0), x.size(1)*x.size(2))\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.act(x)\n        x = self.fc3(x)\n        return self.sm(x)\n    \n    def inference(self,x):\n        x = self.forward(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 100\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = Net(vocabulary_size=len(vocabulary), embedding_size=500,\n           n_hidden_neurons=100, n_classes=2)\nmodel.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss() \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_eval_nn(model,\n                  train_dataloader, test_dataloader,\n                  loss_function, optimizer,\n                  n_epochs, early_stopping_patience,\n                  lr=1e-3, l2_reg_alpha=0,\n                  scheduler=None, device=None):\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n    \n    if scheduler:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n        lr_scheduler = scheduler(optimizer)\n    else:\n        lr_scheduler = None\n\n    \n    best_val_loss = float('inf')\n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n\n    for epoch_i in range(n_epochs):\n        try:\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n\n\n            for idx, (batch_x, batch_y) in enumerate(train_dataloader):\n                if idx > 10000:\n                    break\n                optimizer.zero_grad()\n\n                x_batch = batch_x.to(device)\n                y_batch = batch_y.to(device)\n                preds = model(x_batch)\n                loss_val = loss_function(preds, y_batch)\n                # optimizer.zero_grad()\n\n                loss_val.backward()\n                optimizer.step()\n\n                mean_train_loss += float(loss_val)\n                train_batches_n += 1\n\n            mean_train_loss /= train_batches_n\n            print('Среднее значение функции потерь на обучении', mean_train_loss)\n\n\n            model.eval()               \n            mean_val_loss = 0\n            val_batches_n = 0\n\n            with torch.no_grad():\n                for idx, (batch_x, batch_y) in enumerate(test_dataloader):\n                    if idx > 1000:\n                        break\n\n                    x_batch = batch_x.to(device)\n                    y_batch = batch_y.to(device)\n\n                    preds = model.forward(x_batch)\n                    loss_val = loss_function(preds, y_batch)\n\n                    mean_val_loss += float(loss_val)\n                    val_batches_n += 1\n\n            mean_val_loss /= val_batches_n\n            print('Среднее значение функции потерь на валидации', mean_val_loss)\n            \n            if mean_val_loss < best_val_loss:\n                best_epoch_i = epoch_i\n                best_val_loss = mean_val_loss\n                best_model = copy.deepcopy(model)\n                print('Новая лучшая модель!')\n                \n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                early_stopping_patience))\n                break\n            \n            if lr_scheduler:\n                lr_scheduler.step(mean_val_loss)\n                \n            print()\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n                \n        except Exception as ex:\n            print('Ошибка при обучении {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    return best_val_loss, best_model\n\n\n\nbest_val_loss, best_model = train_eval_nn(model=model, device=device,\n              train_dataloader=train_dataloader,\n              test_dataloader=test_dataloader,\n              loss_function=criterion, optimizer=optimizer,\n              n_epochs=100, early_stopping_patience=10,\n               lr=1e-3, l2_reg_alpha=0,\n              scheduler=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))\n                                          # torch.optim.lr_scheduler.StepLR(optim, step_size=2, gamma=0.3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = batch_x.to(device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)\n    \n    \ntest_pred = predict_with_model(best_model, test_dataset)\nprint('Доля верных ответов', accuracy_score(y_test.values, test_pred.argmax(-1)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}