{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#reading the data\nimport pandas as pd\ndf=pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of data frame\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Canacer detected', round(df['diagnosis'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Malignant ', round(df['diagnosis'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data set is  a little bit unbalanced but We can consider this ratio for model creation","metadata":{}},{"cell_type":"code","source":"#drop Unnamed: 32\ndf1=df.drop(columns='Unnamed: 32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#there is no null values\nprint(df.info())\ndf1.diagnosis.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null values","metadata":{}},{"cell_type":"code","source":"#Label Encoding to change the tha char value of diagnosis to 0s and 1\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf1['diagnosis']= label_encoder.fit_transform(df1['diagnosis'])\ndf1.diagnosis.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 or B : 212 \n\n1 or M : 357","metadata":{}},{"cell_type":"code","source":"\n#checking duplicates\nprint(df1.duplicated().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining x and y\nx=df1.iloc[:,2:]\ny=df1.diagnosis\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality reduction with PCA","metadata":{}},{"cell_type":"code","source":"#getting the n_component for the PCA\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n#PCA is largely affected by scales and different features might have different scales.\n#so we will standerise the data to get the components\nstdSC=StandardScaler()\nx_train_std=stdSC.fit_transform(x_train)\npcaModel=PCA()\npcaModel.fit(x_train_std)\npcaModel.explained_variance_\npcaModel.explained_variance_ratio_\n#pcaModel.explained_variance_ratio_*100\nnp.cumsum(pcaModel.explained_variance_ratio_*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that 24 componets define almost 99.9% of data .","metadata":{}},{"cell_type":"code","source":"# create pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import  LogisticRegression\n#from sklearn.linear_model import Ridge\n\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\nsteps_4_model=[('std',StandardScaler()),('pca',PCA(n_components=24)),('log',LogisticRegression())]\n#parameters = [ {'model__alpha': np.arange(0, 0.2, 0.01) } ]\n\n\nfinalModel=Pipeline(steps=steps_4_model)\nfinalModel.fit(x_train,y_train)\nfinalModel.predict(x_test)\nprint(\"Train Score (Linear):\",finalModel.score(x_train,y_train))\nprint(\"Test Score (Linear):\",finalModel.score(x_test,y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.crosstab(y_train,finalModel.predict(x_train)))\nprint(\"**********************************************\")\nprint(pd.crosstab(y_test,finalModel.predict(x_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,finalModel.predict(x_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to visulaise the new dimension\n\nstdSc=StandardScaler()\nstdSc.fit(x)\nx_train_std=stdSc.transform(x_train)\n\npcaModel_24c=PCA(n_components=24)\npcaModel_24c.fit(x_train_std)\npcaModel_24c.explained_variance_\n(pcaModel_24c.explained_variance_ratio_*100)\nx_train_comp=pcaModel_24c.transform(x_train_std)\npd.DataFrame(x_train_comp).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the visualisation with 2 components of the new dimension\nfrom matplotlib import pyplot as plt,style\nA=pd.DataFrame(x_train_comp[:,0:2],index=x_train.index)\n\n#pd.DataFrame[(A,columns=[\"c1\",\"c2\"])\nA.columns=[\"c1\",\"c2\"]\n\nimport seaborn as sns\n\n\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nsns.scatterplot(data=x_train,x=x_train.radius_mean,y=x_train.texture_mean,hue=y_train).set_title('orig_datset')\nplt.subplot(1,2,2)\nsns.scatterplot(data=A,x=A.c1,y=A.c2,hue=df1.diagnosis).set_title('with PCA')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality reduction with LDA ","metadata":{}},{"cell_type":"markdown","source":"### The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the transform method.","metadata":{}},{"cell_type":"code","source":"# create pipeline\nfrom sklearn.discriminant_analysis import  LinearDiscriminantAnalysis\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state= 0)\n\n\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\nsteps_4_model=[('std',StandardScaler()),('lda', LinearDiscriminantAnalysis(n_components=1)),('log',LogisticRegression())]\n#parameters = [ {'model__alpha': np.arange(0, 0.2, 0.01) } ]\n\n\nfinalModel=Pipeline(steps=steps_4_model)\nfinalModel.fit(x_train,y_train)\nfinalModel.predict(x_test)\nprint(\"Train Score (Linear):\",finalModel.score(x_train,y_train))\nprint(\"Test Score (Linear):\",finalModel.score(x_test,y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.crosstab(y_train,finalModel.predict(x_train)))\nprint(pd.crosstab(y_test,finalModel.predict(x_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visulaisation for LDA\n\n\n\nimport seaborn as sns\nstyle.use('ggplot')\n\nstdSc=StandardScaler()\nstdSc.fit(x)\nx_train_std=stdSc.transform(x_train)\n\nLDAModel_24c=LinearDiscriminantAnalysis(n_components=1)\nLDAModel_24c.fit(x_train_std,y_train)\n\nx_train_lda=LDAModel_24c.transform(x_train_std)\npd.DataFrame(x_train_lda).head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLDA=pd.DataFrame(x_train_lda,columns=[\"c1\"],index=x_train.index)\ntrainLDA['diagnosis']=y_train\ntrainLDA.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nsns.scatterplot(data=x_train,x=x_train.radius_mean,y=x_train.texture_mean,hue=y_train).set_title('orig_datset')\nplt.subplot(1,2,2)\nsns.scatterplot(data=trainLDA,x=trainLDA.c1,y=0,hue='diagnosis').set_title('with LDA')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}