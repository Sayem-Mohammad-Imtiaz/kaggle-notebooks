{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, seaborn as sns, pandas as pd, matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(r\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ntest = pd.read_csv(r\"../input/twitter-sentiment-analysis-hatred-speech/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check non racist/sexist tweets\n\ntrain[train['label']==0].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check racist/sexist tweets\n\ntrain[train['label']==1].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['tweet_length'] = [len(x) for x in train['tweet']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the average length of tweet per category\n\ntrain.groupby('label')['tweet_length'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['label'],train['tweet_length'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['tweet_length'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['tweet_length'][train['label']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['tweet_length'][train['label']==1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data cleaning and feature extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining the two datsets for cleaning\n\n#combi = train.drop('tweet_length',axis=1).append(test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define function to remove unwanted patterns in tweets\n\ndef rmv_pat(text,pattern):\n    r = re.findall(pattern, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    return text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['tidy_tweet'] = np.vectorize(rmv_pat)(train['tweet'],\"@[\\w]*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing everything except characters and hashtags with spaces\n\ntrain['tidy_tweet'] = train['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing short words\n\ntrain['tidy_tweet'] = train['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define overall function to clean tweet\n\ndef clean_tweet(tweet,data):\n    data['tidy_tweet'] = np.vectorize(rmv_pat)(tweet,\"@[\\w]*\")\n    data['tidy_tweet'] = data['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n    data['tidy_tweet'] = data['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n    return data['tidy_tweet']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['tidy_tweet'] = clean_tweet(test['tweet'],test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['tidy_tweet'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle train set\n\nshuffle = np.random.permutation(31962)\nX_train = train['tidy_tweet'][shuffle]\ny_train = train['label'][shuffle]\n\n#shuffle test set\nshuffle2 = np.random.permutation(17197)\nX_test = test['tidy_tweet'][shuffle2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting word features using TF-IDF Vectorizer\n\ntfidf1 = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english').fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting word features using bag of words\n\nbow = CountVectorizer(stop_words='english',analyzer='word').fit_transform(X_train)\n\n#extracting word features using TF-IDF transformer with bag of words\n\ntfidf2 = TfidfTransformer().fit_transform(bow)\n\n#using word2vec\n\ntokenized_tweet = X_train.apply(lambda x: x.split()) # tokenizing \nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features/independent variables\n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34) \n\nmodel_w2v.train(tokenized_tweet, total_examples= len(X_train), epochs=20)\n\ndef word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary                                     \n            continue\n    if count != 0:\n        vec /= count\n    return vec\n\nwordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n    wordvec_df = pd.DataFrame(wordvec_arrays)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.neural_network import MLPClassifier\nmlp_scores = cross_val_score(MLPClassifier(),X=tfidf1,y=y_train,cv=3,scoring='recall').mean()\nprint('MLP score with tfidf1: ',mlp_scores)\nprint('\\n')\n\nfrom sklearn.neural_network import MLPClassifier\nmlp_scores = cross_val_score(MLPClassifier(),X=bow,y=y_train,cv=3,scoring='recall').mean()\nprint('MLP score with bow: ',mlp_scores)\nprint('\\n')\n\nfrom sklearn.neural_network import MLPClassifier\nmlp_scores = cross_val_score(MLPClassifier(),X=tfidf2,y=y_train,cv=3,scoring='recall').mean()\nprint('MLP score with tfidf2: ',mlp_scores)\nprint('\\n')\n\nfrom sklearn.neural_network import MLPClassifier\nmlp_scores = cross_val_score(MLPClassifier(),X=wordvec_df,y=y_train,cv=3,scoring='recall').mean()\nprint('MLP score with word2vec: ',mlp_scores)\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Pipeline to pass data through feature extractors and classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe =  Pipeline([('bow', CountVectorizer(stop_words='english',analyzer='word')),\n                 ('estimator', MLPClassifier())\n                ])\n\npipe.fit(X_train,y_train)\n\nprediction = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}