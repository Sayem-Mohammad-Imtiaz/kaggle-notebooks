{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd, numpy as np, seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-22T23:56:28.289112Z","iopub.execute_input":"2021-08-22T23:56:28.289872Z","iopub.status.idle":"2021-08-22T23:56:29.296715Z","shell.execute_reply.started":"2021-08-22T23:56:28.289741Z","shell.execute_reply":"2021-08-22T23:56:29.295949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/diabetes-data-set/diabetes.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-22T23:56:29.298197Z","iopub.execute_input":"2021-08-22T23:56:29.2988Z","iopub.status.idle":"2021-08-22T23:56:29.316629Z","shell.execute_reply.started":"2021-08-22T23:56:29.298744Z","shell.execute_reply":"2021-08-22T23:56:29.315713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T23:56:29.31802Z","iopub.execute_input":"2021-08-22T23:56:29.318338Z","iopub.status.idle":"2021-08-22T23:56:29.347064Z","shell.execute_reply.started":"2021-08-22T23:56:29.31829Z","shell.execute_reply":"2021-08-22T23:56:29.346158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"feature-selection-techniques","metadata":{}},{"cell_type":"code","source":"corrmat = data.corr()\ntop_corr_feat = corrmat.index\nplt.figure(figsize=(7,7))\n#plot heat map\ng = sns.heatmap(data[top_corr_feat].corr(),annot=True,\n               cmap='gist_rainbow')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select from model technique for feature importance\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier \n\nfeat = data.drop(\"Outcome\",axis=1)\ntarget = data[\"Outcome\"]\n\nfeature_names = np.array(feat.columns)\nRFC = RandomForestClassifier().fit(feat,target)\nimportance = np.abs(RFC.feature_importances_)\nsns.barplot(x=importance, y=feature_names)\nplt.title(\"Feature importances\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature importance\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(feat,target)\nmodel.feature_importances_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.Series(model.feature_importances_, index=feat.columns)\nfeat_importance.plot(kind='barh')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#univariate selection\n#apply selctkbest to selct top 5 features\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nbestfeatures = SelectKBest(score_func = chi2, k=5)\nfit = bestfeatures.fit(feat,target)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(feat.columns)\n#concat the two dataframes for better viz\nfeat_scores = pd.concat([dfcolumns,dfscores],axis=1)\nfeat_scores.columns =['Feature', 'Score']\nfeat_scores.nlargest(5, 'Score') #top 5 features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = feat_scores.nlargest(5, 'Score')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use top features\noptimum_features = report['Feature']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = data.loc[0:,list(optimum_features)].join(data[\"Outcome\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=1)\nX_pca = pca.fit_transform(new_data.drop('Outcome',axis=1))\nPCA_df = pd.DataFrame(data = X_pca, columns = ['PC1'])\nPCA_df = pd.concat([PCA_df, new_data['Outcome']], axis = 1)\nPCA_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(x=PCA_df['PC1'],\n            y = PCA_df['Outcome'], color = 'red',\n           marker = '+', fit_reg = True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split dataset into training and test set\nfrom sklearn.model_selection import train_test_split\nX = new_data.drop(\"Outcome\",axis=1).values\ny = new_data[\"Outcome\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 45, shuffle = True, stratify = y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC,ExtraTreesClassifier as XTC\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import *\nmcc= make_scorer(matthews_corrcoef)\n\ndef evaluate_model(cv):\n    model = RFC()\n    # evaluate the model\n    scores = cross_val_score(model, X_train, y_train,\n                             scoring= mcc,\n                             cv=cv, n_jobs=-1)\n    # return scores\n    return scores.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#iterate over a range of folds to get best K value:\n\nfolds = range(5,11)\n\n# record mean and min/max of each set of results\nmeans = list()\n# evaluate each k value\n\nfor k in folds:\n    # define the test condition\n    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    # evaluate k value\n    k_mean = evaluate_model(cv)\n    # report performance\n    print('> folds=%d, rfc mean score = %.3f ' % (k, k_mean))\n    # store mean accuracy\n    means.append(k_mean)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save randomforestclassif\nmodel = RFC()\nmodel.fit(X_train,y_train)\nimport pickle\nmodel1 = pickle.dumps(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#evaluate extratreesclassif\n\ndef evaluate_model(cv):\n    model = XTC()\n    # evaluate the model\n    scores = cross_val_score(model, X_train, y_train,\n                             scoring= mcc,\n                             cv=cv, n_jobs=-1)\n    # return scores\n    return scores.mean()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = range(5,11)\n\n# record mean and min/max of each set of results\nmeans = list()\n# evaluate each k value\n\nfor k in folds:\n    # define the test condition\n    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    # evaluate k value\n    k_mean = evaluate_model(cv)\n    # report performance\n    print('> folds=%d, xtc mean score = %.3f ' % (k, k_mean))\n    # store mean accuracy\n    means.append(k_mean)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save xtratreesclassif\nmodel = XTC()\nmodel.fit(X_train,y_train)\nmodel2 = pickle.dumps(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = pickle.loads(model1)\nxtc = pickle.loads(model2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stack classifier with extratrees and randomforest as base estimators\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression as LR\nbase_model, end_model = [('random_forest',rfc),('xtra_trees',xtc)], LR()\nfinal_model = StackingClassifier(base_model,end_model, cv=10)\nscores = cross_val_score(final_model, X_train, y_train,\n                              scoring= mcc,\n                              cv=10, n_jobs=-1)\nscores.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X_train, y_train)\n# #save the final_model\nmodel3 = pickle.dumps(final_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run predictions with the 3 models\nrfc_pred = rfc.predict(X_test)\nxtc_pred = xtc.predict(X_test)\nfinal_model_pred = final_model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report as report, confusion_matrix as cm\nprint(\"report on random forest classifier : \\n\", report(y_pred=rfc_pred,y_true=y_test))\nprint('\\n')\nprint(\"report on extra trees classifier : \\n\", report(y_pred=xtc_pred,y_true=y_test))\nprint('\\n')\nprint(\"report on stacked classifier : \\n\", report(y_pred=final_model_pred,y_true=y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"matrix of random forest classifier : \\n\", cm(y_pred=rfc_pred,y_true=y_test,labels=[0,1]))\nprint('\\n')\nprint(\"matrix of extra trees classifier : \\n\", cm(y_pred=xtc_pred,y_true=y_test,labels=[0,1]))\nprint('\\n')\nprint(\"matrix of stacked classifier : \\n\", cm(y_pred=final_model_pred,y_true=y_test,labels=[0,1]))","metadata":{},"execution_count":null,"outputs":[]}]}