{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nimport spacy\nimport pickle\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\ntrue['label'] = 0\n\ncleansed_data = []\nfor data in true.text:\n    if \"@realDonaldTrump : - \" in data:\n        cleansed_data.append(data.split(\"@realDonaldTrump : - \")[1])\n    elif \"(Reuters) -\" in data:\n        cleansed_data.append(data.split(\"(Reuters) - \")[1])\n    else:\n        cleansed_data.append(data)\n\ntrue[\"text\"] = cleansed_data\ntrue.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nfake['label'] = 1\n\ndataset = pd.concat([true, fake])\ndataset = dataset.sample(frac = 1, random_state = 0).reset_index(drop = True)\ndataset = dataset.iloc[:7500]\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\nPARAM_PATH = '../input/pickled-param/pickled-param.pickle'\nMAX_LEN = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\ntext_list = dataset.text\nprint(\"Spacy NLP ...\")\nnlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\nword_dict = {}\nword_index = 1\nlemma_dict = {}\ndocs = nlp.pipe(text_list, n_threads = 2)\nword_sequences = []\nfor doc in tqdm(docs):\n    word_seq = []\n    for token in doc:\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n            word_dict[token.text] = word_index\n            word_index += 1\n            lemma_dict[token.text] = token.lemma_\n        if token.pos_ is not \"PUNCT\":\n            word_seq.append(token.text)\n    word_sequences.append(word_seq)\ndel docs\ngc.collect()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    if '.pkl' in path or '.pickle' in path:\n        with open(path,'rb') as f:\n            return pickle.load(f)\n    else:\n        with open(path, encoding=\"utf8\", errors='ignore') as f:\n            return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\n    \ndef P_glove(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - word_dict.get(word, 0)\ndef correction_glove(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates_glove(word), key=P_glove)\ndef candidates_glove(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known_glove([word]) or known_glove(edits1_glove(word)) or [word])\ndef known_glove(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in word_dict)\ndef edits1_glove(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = load_embeddings(GLOVE_EMBEDDING_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = []\nfor sequence in word_sequences:  \n    if len(sequence) > MAX_LEN:\n        sequence = sequence[:MAX_LEN]\n    else:\n        padding = MAX_LEN - len(sequence)\n        sequence.extend(['unknown']*padding)\n    matrix = []\n    for word in sequence:\n        try:\n            seq = embeddings_index[word]\n        except:\n            seq = embeddings_index['unknown']\n        matrix.append(list(seq))\n    x_train.append(np.array(matrix))\n    \ndel word_sequences, sequence, matrix, seq, embeddings_index, word_dict\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(x_train).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# this is the size of our encoded representations\nencoding_dim = 150\n\n# this is our input placeholder\ninput_ = tf.keras.layers.Input(shape=(None,300))\n# \"encoded\" is the encoded representation of the input\nencoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = tf.keras.layers.Dense(300, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = tf.keras.models.Model(input_, decoded)\n\n# intermediate result\n# this model maps an input to its encoded representation\nencoder = tf.keras.models.Model(input_, encoded)\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.array(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5, \n                                                          verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n\nautoencoder.fit(x_train, x_train,\n                epochs=30,\n                batch_size=128,\n                shuffle=True,\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reconst_test = autoencoder.predict(X_train)\nencode_test = encoder.predict(x_train)\nencode_test = encode_test.reshape(dataset.shape[0],200*encoding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import cluster\n\n# Training for 2 clusters (Fake and Real)\nkmeans = cluster.KMeans(n_clusters=2, verbose=1)\n\n# Fit predict will return labels\nclustered = kmeans.fit_predict(encode_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\nincorrect = 0\nfor index, row in enumerate(dataset['label'].values):\n    if row == clustered[index]:\n        correct += 1\n    else:\n        incorrect += 1\n        \nprint(\"Correctly clustered news: \" + str((correct*100)/(correct+incorrect)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maxlen = 512\n# X = []\n# for text in dataset.text[:100]:\n#     enc_di = tokenizer.encode(text).ids\n#     len_ = len(enc_di)\n    \n#     if len_ > maxlen:\n#         enc_di = enc_di[:500] + enc_di[len_-(maxlen-500):]\n#     else:\n#         padding_len = maxlen-len_\n#         enc_di += [1] * (padding_len)\n    \n#     input_ids = tf.constant(enc_di)[None, :]  # Batch size 1\n#     outputs = model(input_ids)\n#     last_hidden_states = outputs[0].numpy().squeeze(0)\n#     X.append(last_hidden_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.array(X).shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}