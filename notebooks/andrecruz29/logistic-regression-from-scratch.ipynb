{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hey guys!!** \nThis is my first attempt to make a logistic regression. \nFeel free to comment if you know better ways of doing this or if I am wrong. \nThanks for choosing my notebook!** :D**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this section, N will be the size of our train set.\n#I do not know the best value for this. \n#So I chose the one that got me best results for less Entropy. \n\nN=100\nD=12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I am trying to separate data from the targets.\n#I am also slicing these arrays into train and test.\n#Finally, I have arbitrarily decided that 7 or more \n#in quality is considered a 'Good Wine'.\n \nX = np.asarray(df.drop('quality', axis=1))\nX = (X-X.mean())/ np.std(X)\nones = np.ones((len(df['quality']),1))\nXb = np.concatenate((ones, X), axis=1)\nXb_t = Xb[N:,:] \nXb = Xb[:N,:]\n\nT = np.asarray(df['quality'])\nfor i in range(1599):\n    if T[i] >= 7:\n        T[i] = 1\n    else:\n        T[i] = 0\n\nT_t = T[N:]\nT = T[:N]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = np.random.randn(D)/np.sqrt(D)\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\nY = sigmoid(Xb.dot(w))\n\ndef cross_entropy(T,Y):\n    E=0\n    for i in range(N):\n        if T[i] == 1:\n            E -= np.log(Y[i])\n        else:\n            E -= np.log(1-Y[i])\n    return E\n\nlearn_rate=0.001\nfor i in range(10000):    \n    w += learn_rate*(Xb.T.dot(T-Y))\n    Y = sigmoid(Xb.dot(w))\n    \ndef classification_rate(T,Y):\n    return np.mean(T==Y)\n\nprint('Cross Entropy:', cross_entropy(T,Y))\n\nY=np.round(Y)\n\nprint('Classification Rate Train:',classification_rate(T,Y))\n\n#Predictions\n\nYhat = sigmoid(Xb_t.dot(w))\nYhat = np.round(Yhat)\n\nprint('Classification Rate Test:',classification_rate(T_t,Yhat))\n\n#This graphic shows the weights for each aspect of the wine. \n#I have noted that my Cross Entropy is still high.\n#The predictions seem to be ok, though. \n#Raising N also increases the test accuracy up to 91%. \n\nlabels = df.columns\nplt.barh(labels[:-1], w[1:], color='g')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}