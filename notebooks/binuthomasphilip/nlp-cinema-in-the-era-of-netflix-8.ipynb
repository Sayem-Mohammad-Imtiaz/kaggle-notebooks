{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https://lnkd.in/gj7bMQA\n\n### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{},"cell_type":"markdown","source":"I last had a Television way back in 2009.I cater to my needs through internet and youtube.Netflix has ccome into India recently.Through this dataset we will try to Learn more about NetFlix.In this Kernel I will be covering following topics\n\n1.Data Import and Preprocessing\n\n2.Exploratory Data Analysis\n\n3.Feature Engineering\n\n4.Conclusion\n\n### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Data Import and Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Importing Python Modules"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install plotly\n!pip install cufflinks \n!pip install textblob","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n#plt.style.use('ggplot')\nimport plotly as py\nimport cufflinks as cf\nfrom plotly.offline import iplot\npy.offline.init_notebook_mode(connected=True)\ncf.go_offline()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the Dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/netflix-shows/netflix_titles.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of Dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Rows     :',df.shape[0])\nprint('Columns  :',df.shape[1])\nprint('\\nFeatures :\\n     :',df.columns.tolist())\nprint('\\nMissing values    :',df.isnull().values.sum())\nprint('\\nUnique values :  \\n',df.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping Missing Values"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df=df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Content Update"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_shows=df[df['type']=='TV Show']\ndf_movies=df[df['type']=='Movie']\ndf_date = df_shows[['date_added']].dropna()\ndf_date['year'] = df_date['date_added'].apply(lambda x : x.split(', ')[-1])\ndf_date['month'] = df_date['date_added'].apply(lambda x : x.lstrip().split(' ')[0])\n\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'][::-1]\ndfu = df_date.groupby('year')['month'].value_counts().unstack().fillna(0)[month_order].T\nplt.figure(figsize=(10, 7), dpi=200)\nplt.pcolor(dfu, cmap='afmhot_r', edgecolors='white', linewidths=2) # heatmap\nplt.xticks(np.arange(0.5, len(dfu.columns), 1), dfu.columns, fontsize=7, fontfamily='serif')\nplt.yticks(np.arange(0.5, len(dfu.index), 1), dfu.index, fontsize=7, fontfamily='serif')\n\nplt.title('Netflix Contents Update', fontsize=12, fontfamily='calibri', fontweight='bold', position=(0.20, 1.0+0.02))\ncbar = plt.colorbar()\n\ncbar.ax.tick_params(labelsize=8) \ncbar.ax.minorticks_on()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for the year 2019 more content update happened in the month of August and May."},{"metadata":{},"cell_type":"markdown","source":"### Datetime Conversion"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df[\"date_added\"] = pd.to_datetime(df['date_added'])\ndf['day_added'] = df['date_added'].dt.day\ndf['year_added'] = df['date_added'].dt.year\ndf['month_added']=df['date_added'].dt.month\ndf['year_added'].astype(int);\ndf['day_added'].astype(int);\n#df.year_added = df.year_added.astype(float)\n#df.style.set_precision(0)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Type"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['type'].value_counts();\nf,ax=plt.subplots(1,2,figsize=(18,8))\ndf['type'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Type of Movie')\nax[0].set_ylabel('Count')\nsns.countplot('type',data=df,ax=ax[1],order=df['type'].value_counts().index)\nax[1].set_title('Count of Source')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So 98 % items in the dataset are movies and remaining small percentage is TV show"},{"metadata":{},"cell_type":"markdown","source":"### Movie Rating"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['rating'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Movie Rating')\nax[0].set_ylabel('Count')\nsns.countplot('rating',data=df,ax=ax[1],order=df['rating'].value_counts().index)\nax[1].set_title('Count of Rating')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"33% Fall in catogery TV-MA (\"TV-MA\" is a rating assigned by the TV Parental Guidelines to a television program that was designed for mature audiences only.)\n\n23% fall in catigery TV-14 (Programs rated TV-14 contains material that parents or adult guardians may find unsuitable for children under the age of 14.\n\n12.5 % fall in category TV-PG (TV-PG: Parental guidance suggested. This program contains material that parents may find unsuitable for younger children)"},{"metadata":{},"cell_type":"markdown","source":"### Country"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"group_country_movies=df.groupby('country')['show_id'].count().sort_values(ascending=False).head(10);\nplt.subplots(figsize=(15,8));\ngroup_country_movies.plot(kind='bar',fontsize=12,color='blue');\n#group_country_movies.plot('bar',fontsize=12,color='blue');\nplt.xlabel('Number of Movies',fontsize=12)\nplt.ylabel('Country',fontsize=12)\nplt.title('Movie count by Country',fontsize=12)\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can say most movies are from Hollywood,Bollywood and British Film industry."},{"metadata":{},"cell_type":"markdown","source":"## Date"},{"metadata":{},"cell_type":"markdown","source":"### How many Movies Per Year?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"group_country_movies=df.groupby('year_added')['show_id'].count().sort_values(ascending=False).head(10);\nplt.subplots(figsize=(15,8));\ngroup_country_movies.plot(kind='bar',fontsize=12,color='blue');\nplt.xlabel('Number of Movies',fontsize=12)\nplt.ylabel('Year',fontsize=12)\nplt.title('Movie Count By Year',fontsize=12)\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every Year the movie Count is increasing indicating that popularity of Netfilx is increasing every years.But we can see a dip in 2020 possibily the affect of Coronavirus."},{"metadata":{},"cell_type":"markdown","source":"## Which are most popular words for Title?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nfrom PIL import Image\nplt.style.use('seaborn')\nwrds1 = df[\"title\"].str.split(\"(\").str[0].value_counts().keys()\n\nwc1 = WordCloud(stopwords=STOPWORDS,scale=5,max_words=1000,colormap=\"rainbow\",background_color=\"black\").generate(\" \".join(wrds1))\nplt.figure(figsize=(20,14))\nplt.imshow(wc1,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Key Words in Movie Titles\",color='black',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Love,Man,Christmas etc are some of the most prominent words for movie Title"},{"metadata":{},"cell_type":"markdown","source":"## Rating"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['rating'].value_counts().iplot(kind='bar',xTitle='Rating',yTitle='Count',title='Bar Chart of Rating')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most movies fall in the category\n\n**1.TV-MA:** Mature audience only. This program is specifically designed to be viewed by adults and therefore may be unsuitable for children under 17.\n\n**2.TV-14:** Parents strongly cautioned. This program contains some material that many parents would find unsuitable for children under 14 years of age.\n\n**3.R:** Restricted, Children Under 17 Require Accompanying Parent or Adult Guardian. This rating means the film contains adult material such as adult activity, harsh language, intense graphic violence, drug abuse and nudity.\n\n**4.TV-PG:** Programs rated TV-PG may contain some material that parents or guardians may find inappropriate for younger children. Programs assigned a TV-PG rating may include some inappropriate language, very little sexual content, suggestive dialogue, and/or mild violence.\n\n**PG-13:** Parents Strongly Cautioned, Some Material May Be Inappropriate for Children Under 13. This rating is a stronger caution for parents that content included may not be appropriate for children under 13 (pre-teen ages). This may include stronger language, extended violence or sexual situations and drug-use"},{"metadata":{},"cell_type":"markdown","source":"## Description?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#df['description'][1]\n#df['length']=df['description'].str.len()\ndf.dropna();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Contractions "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"contractions = {\n    \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def cont_to_exp(x):\n    if type(x) is str:\n        x =x.replace('\\\\','')\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = \"I don't know what date is today, I am 5'8\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(cont_to_exp(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have written a code by which we can expand the contractions used in the text.We can apply this cleaning technique to our description column."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time \ndf['description'] = df['description'].apply(lambda x: cont_to_exp(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So with the above code we have managed to convert the contractions and expanded them to full form."},{"metadata":{},"cell_type":"markdown","source":"# 3.Feature Engineering"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\ndf['polarity'] = df['description'].apply(lambda x: TextBlob(x).sentiment.polarity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descripion Length"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['desc_len'] = df['description'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Count"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['word_count'] = df['description'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average Word Length"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_avg_word_len(x):\n    words = x.split()\n    word_len = 0 \n    for word in words:\n        word_len = word_len + len(word)\n        \n    return word_len/len(words)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['avg_word_len'] = df['description'].apply(lambda x: get_avg_word_len(x))\n\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Description Distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\ng = sns.distplot(df['desc_len'])\ng.set_title(\"Length Of Description Distribution\", fontsize=20)\ng.set_xlabel(\"Length\", fontsize=15)\ng.set_ylabel(\"Frequency\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the many reviews are in te ranged 140 to 150 words."},{"metadata":{},"cell_type":"markdown","source":"## Length of Description Vs Rating"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"rating\", y=\"desc_len\", data=df,width=0.8,linewidth=3)\nax.set_xlabel('Rating',fontsize=30)\nax.set_ylabel('Length of Description',fontsize=30)\nplt.title('Length of Description Vs Rating',fontsize=40)\nax.tick_params(axis='x',labelsize=20,rotation=90)\nax.tick_params(axis='y',labelsize=20,rotation=0)\nplt.grid()\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suprisingly the mean length of the description across all the ratings remain same."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Sentiment Polarity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['polarity'].iplot(kind='hist',colors='red',bins=50,xTitle='Polarity',yTitle='Count',linecolor='black',title='Sentiment Polarity Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So looking at the distribution of polarity we can say that the sentiment expressed about the movies in the description is slightly towards positive.We can almost conclude that people are not happy with at least 40% of the movies."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of description text lenght"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['desc_len'].iplot(kind='hist',colors='red',bins=50,xTitle='Desc_Lenght',yTitle='Count',linecolor='black',title='Description Text Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We had earlier ploted the distplot which showed us similar result.Most of the text in the description column has 140-150 words.Very few description has more than 155 words."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of description text word length"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['word_count'].iplot(kind='hist',colors='red',bins=50,xTitle='Word_Count',yTitle='Count',linecolor='black',title='Description Word Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the movies have description which is i between 20-30 words."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of description text average word length"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['avg_word_len'].iplot(kind='hist',colors='red',bins=50,xTitle='Avg_Word_Len',yTitle='Count',linecolor='black',title='Description Average Word Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from the distribution we can clearly see that for the description of the movie the average word length used is generally between 4 to 6.5."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Unigram"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = ['this is a test example']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_words(x,n):\n    vec = CountVectorizer(ngram_range=(1,1),stop_words='english').fit(x)\n    bow = vec.transform(x)\n    sum_words = bow.sum(axis=0)\n    words_freq = [(word,sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x : x[1], reverse = True)\n    return words_freq[:n]\n    \n    #vec.vocabulary_.items()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(get_top_n_words(x,3))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"words = get_top_n_words(df['description'],20)\nwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 20 Unigram Words"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(words,columns =['Unigram','Frequency'])\ndf1 = df1.set_index('Unigram')\ndf1.iplot(kind ='bar',xTitle = 'Unigram',yTitle='Count',title = 'Top 20 Unigram Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have got a list of top 20 Unigram words and they are life,young and man."},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Bigram"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_top_n_words_2(x,n):\n    vec = CountVectorizer(ngram_range=(2,2),stop_words='english').fit(x)\n    bow = vec.transform(x)\n    sum_words = bow.sum(axis=0)\n    words_freq = [(word,sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x : x[1], reverse = True)\n    return words_freq[:n]\n    \n    #vec.vocabulary_.items()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(get_top_n_words_2(x,3))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"words_2 = get_top_n_words_2(df['description'],20)\nwords_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 20 Bigram Words"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(words_2,columns =['Bigram','Frequency'])\ndf2 = df2.set_index('Bigram')\ndf2.iplot(kind ='bar',xTitle = 'Bigram',yTitle='Count',title = 'Top 20 Bigram Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly all the top three bigram words start with y ie year old,young man,young woman."},{"metadata":{},"cell_type":"markdown","source":"## Listed in?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom collections import Counter\ncol = \"listed_in\"\ncategories = \", \".join(df['listed_in']).split(\", \")\ncounter_list = Counter(categories).most_common(50)\nlabels = [_[0] for _ in counter_list][::-1]\nvalues = [_[1] for _ in counter_list][::-1]\nplt.figure(figsize=(12,5))\nsns.barplot(values[0:20],labels[0:20]);\nplt.xlabel('Count',fontsize=10)\n#plt.ylabel('',fontsize=20)\nplt.title('Movie Listing',fontsize=20)\n#ax.tick_params(labelsize=20)\nplt.grid()\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More of the listings are in the Catogery International TV Shows,TV Drama and Movies"},{"metadata":{},"cell_type":"markdown","source":"# 4.Conclusion\n\n1.This data set had text data so we tried to get insights from the text data.We saw from the Content chart that for the year 2019 more content update happened in the month of August and May.So 98 % items in the dataset are movies and remaining small percentage is TV show.\n\n2.Exploring the movie ratings we could conclude 33% Fall in catogery TV-MA (\"TV-MA\" is a rating assigned by the TV Parental Guidelines to a television program that was designed for mature audiences only).23% fall in catigery TV-14 (Programs rated TV-14 contains material that parents or adult guardians may find unsuitable for children under the age of 14).12.5 % fall in category TV-PG (TV-PG: Parental guidance suggested. This program contains material that parents may find unsuitable for younger children)\n\n3.Based on the country we could conclude most movies are from Hollywood (USA),Bollywood (India) and British Film industry.\n\n4.From the date data we could conclude that every year the movie count is increasing indicating that popularity of Netfilx is increasing every year.We see more movies are added in the month of September followed by October and March.Love,Man,Christmas etc are some of the most prominent words for movie Title.\n\n5.From the reviews we can conclude that the many reviews are in te ranged 140 to 150 words.The length of the reviews remains almost same for different ratinngs.Very few description has more than 155 words.\n\n6.From the sentiment analysis we can conclude we can say that the sentiment expressed about the movies in the description is slightly towards positive.We can almost conclude that people are not happy with at least 40% of the movies.\n\n7.We plotted unigrams and bigrams and could identify most used words.Life,Young and Man are the top three words in the unigram plot.Years Old,Young Man,Young Woman are the most represented words in the bigrams.The word young shows may be the movies are more about young people.More of the listings are in the Catogery International TV Shows,TV Drama and Movies"},{"metadata":{},"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}