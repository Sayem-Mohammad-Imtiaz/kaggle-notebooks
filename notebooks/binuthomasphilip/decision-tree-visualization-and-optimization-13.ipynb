{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https://lnkd.in/gj7bMQA\n\n### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{},"cell_type":"markdown","source":"In this kernel we will be building a decision tree model.After that we will pruning the decision tree to avoid overfittig.In this notebook we will be covering following things\n\n1.Data Import and Preprocesing\n\n2.Feature Engineering\n\n3.Decision Tree Model Build\n\n4.Model Evaluation\n\n5.Decision Tree Prunning\n\n6.Conclusion"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Importing Data and Pre Processing"},{"metadata":{},"cell_type":"markdown","source":"### Importing Python Modules"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('../input/social-network-ads/Social_Network_Ads.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The datset we have the User ID, Gender,Age,Salary and the data if Purchase made by a user."},{"metadata":{},"cell_type":"markdown","source":"# 2.Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Creating Matrix of Features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X=dataset.iloc[:,[2,3]].values\ny=dataset.iloc[:,4].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting Data into Train and Test"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have scaled the data because we have considered Age and Estimated salary to make the purchase prediction.There is a big difference in the reange of these features."},{"metadata":{},"cell_type":"markdown","source":"# 3.Model Built"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier(criterion='entropy',random_state=0)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"### Ploting Decision Tree"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(clf,filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our tree has many nodes.So Splitting has taken place many times.The problem with this is that we may have good accuracy on our training Dataset because the model learns by heart the values.We we try to predict the result for unseen data (test) data there is possibility of our accuracy reducing."},{"metadata":{},"cell_type":"markdown","source":"### Accuracy Score"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ny_pred = clf.predict(X_test)\ny_train = clf.predict(X_train)\nfrom sklearn.metrics import accuracy_score \nprint(\"Train Accuracy is:\",accuracy_score(y_train,y_train))\nprint(\"Test Accuracy is:\",accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have 100% accuracy for our training test and around 92.5% accuracy for our test set.This shows that our training model set is overfitting."},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cr =classification_report(y_test,y_pred)\nprint(\"Classification Report\")\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have quite good value of Accuracy and F1 Score."},{"metadata":{},"cell_type":"markdown","source":"### Visualising the Training Set"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,clf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Decision Tree (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising the Test Set"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,clf.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('Decision Tree (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above training set results we can see that our model is overfitting on training set.We can reduce the overfitting by pruning our decision tree."},{"metadata":{},"cell_type":"markdown","source":"# 5.Decision Tree Pruning\n\nIn the desision tree algorithm there are parameters like min_samples_leaf and max_depth to prevent the tree from overfitting.We can use another method cost complexity pruning to control the size of our trees.The complexity of tree is controlled by a parameter ccp_alpha.Greater value of ccp_alpha meanse more number of nodes in the tree are pruned."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"path = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nccp_alphas","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of nodes in the last tree is: 1 with ccp_alpha: 0.2789340883820818"},{"metadata":{},"cell_type":"markdown","source":"### Training and Test Accurac Vs Alpha"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above cure we can see that we can get train and test set accuracy when we select the alpha value between 0.02 to 0.27.So I am selecting a value of 0.12 for alpha while creating the decision tree model."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.12)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pred=clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pruned Decision Tree"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(clf,filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So by using alpha value as 0.12 we have been able to prune our decision tree to a large extent.This help in time reduction for training and also prevents overfitting of our models"},{"metadata":{},"cell_type":"markdown","source":"# 6.Conclusion\n\n1.In this data set we had Age,Salary,Sex and purchase decision.We have made use of Age,Salary to predict whether a customer will make a purchase decision.\n\n2.We have used feature scaling and then built a decision tree model to make purchase prediction.\n\n3.Based on the model evaluation we could conclude that our model had over fitting on the training set.\n\n4.We used Decision Tree Pruning technique to optimise over model.This reduced the size of our decision tree by considering an optimum value of alpha."},{"metadata":{},"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}