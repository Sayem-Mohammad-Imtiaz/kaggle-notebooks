{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Check Data\nLet's upload our data set and make small reviews. We will do a detailed review later."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Veriable Description \n1. Pregnancies: <br>\n   Number of times pregnant\n2. Glucose: <br>\n   Plasma glucose concentration over 2 hours in an oral glucose tolerance test.\n3. BloodPressure: <br>\n   Diastolic blood pressure (mm Hg)\n4. Skin Thickness: <br>\n   Triceps skin fold thickness (mm)\n5. Insulin: <br>\n   2-Hour serum insulin (mu U/ml)\n6. BMI: <br>\n   This is a numerical value of your weight in relation to your height. A BMI between 18.5 and 25 kg/m² indicates a normal weight. A BMI of less than 18.5 kg/m² is considered underweight. A BMI between 25 kg/m² and 29.9 kg/m² is considered overweight.\n7. DiabetesPedigreeFunction: <br>\n   Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n8. Age: <br>\n   Age (years)\n9. Outcome: <br>\n   Class variable (0 if non-diabetic, 1 if diabetic)  "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Variable Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(data[variable],bins=50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ncolumns = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n       'Insulin','BMI', 'DiabetesPedigreeFunction','Age','Outcome']\n\nfor col in columns:\n    plot_hist(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation & Outlier Detection <a id=\"2\"></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](://media3.giphy.com/media/MVDPX3gaKFPuo/giphy.gif?cid=ecf05e47486a445c592a49fb8fca7f525cdd691411c8d66a&rid=giphy.gif)\nYesss. All data is full. We should celebrate this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier Detection \nfor col in columns:\n    sns.boxplot(x = data[col])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Glucose'].fillna(data['Glucose'].mean(), inplace = True)\ndata['BloodPressure'].fillna(data['BloodPressure'].mean(), inplace = True)\ndata['SkinThickness'].fillna(data['SkinThickness'].median(), inplace = True)\ndata['Insulin'].fillna(data['Insulin'].median(), inplace = True)\ndata['BMI'].fillna(data['BMI'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\np=msno.bar(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=data.Outcome,data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"BMI\",data=data,kind=\"bar\")\ng.set_ylabels(\"BMI\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pregnancies"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"Pregnancies\",data=data,kind=\"bar\")\ng.set_ylabels(\"Pregnancies\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Glucose"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"Glucose\",data=data,kind=\"bar\")\ng.set_ylabels(\"Glucose\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BloodPressure"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"BloodPressure\",data=data,kind=\"bar\")\ng.set_ylabels(\"BloodPressure\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SkinThickness"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"SkinThickness\",data=data,kind=\"bar\")\ng.set_ylabels(\"SkinThickness\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insulin"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"Insulin\",data=data,kind=\"bar\")\ng.set_ylabels(\"Insulin\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BMI"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"BMI\",data=data,kind=\"bar\")\ng.set_ylabels(\"BMI\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DiabetesPedigreeFunction"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"DiabetesPedigreeFunction\",data=data,kind=\"bar\")\ng.set_ylabels(\"DiabetesPedigreeFunction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Outcome\",y=\"Age\",data=data,kind=\"bar\")\ng.set_ylabels(\"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,8))\ncorr = data.corr()\nsns.heatmap(\n    corr,\n    mask= np.zeros_like(corr, dtype=np.bool),\n    cmap= sns.diverging_palette(240,10,as_cmap=True),\n    square= True,\n    ax=ax\n    )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import neighbors\nfrom sklearn import metrics\nfrom sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Nearest Neighbors\n* Estimation is made on the similarities of the observations.\n* In order to estimate the \"Y\", which is the dependent variable value of the observation unit whose independent variable values ​​are given, the decoration of the relevant observation units with the other observation units in the table will be calculated.\n<br><br>\n- Determine the number of neighbors (K).\n- Calculate distances from unknown point to all other points.\n- Select the closest k-observation, with distances in order and according to the specified k number.\n- Classification is the most common class, and regression is the average value as the estimated value."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nprint(\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Score: \", knn_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find k value\nscore_list = []\n\nfor each in range(1,100):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(X_train,y_train)\n    score_list.append(knn2.score(X_test,y_test))\n    \nplt.plot(range(1,100),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=43).fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", knn_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5,n_jobs=-1,verbose=2).fit(X_train,y_train)\ny_pred = knn_cv.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", knn_cv.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classification\n* It is one of the strong and flexible modeling techniques. It can be used for classification and regression. Robust is a regression modeling technique.\n* But it is to determine a line or curve to a margin range so that it can get the maximum point with the smallest error."},{"metadata":{"trusted":true},"cell_type":"code","source":"zero = data[data[\"Outcome\"] == 0]\none = data[data[\"Outcome\"] == 1]\n# scatter plot\nplt.scatter(zero.BMI,zero.DiabetesPedigreeFunction,color=\"red\",label=\"zero\",alpha= 0.3)\nplt.scatter(one.BMI,one.DiabetesPedigreeFunction,color=\"green\",label=\"one\",alpha= 0.3)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nsvr_model = SVR(\"linear\").fit(X_train,y_train)\nsvr_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svr_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", svr_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRID SEARCH METHOD\nsvr_model = SVR('linear')\nsvr_params = {\n    \"C\": [0.1, 0.5, 1, 3]  # penalty coefficient values\n}\nsvr_cv_model = GridSearchCV(svr_model, svr_params, cv=5, verbose=2, n_jobs=-1).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \",svr_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Artificial Neural Network\n* It is one of the powerful machine learning algorithms that can be used for classification and regression problems that refer to the way the human brain processes information.\n* The aim is to reach the coefficients that can make estimates with the smallest error.\n![](https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)\n\n\n## Artificial Neural Network - Model & Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nmlp_model = MLPClassifier().fit(X_train,y_train)\nmlp_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = mlp_model.predict(X_test)\nprint(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \", mlp_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way\nmlp_params = {\n    \"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001],\n    \"hidden_layer_sizes\": [(10,20),(5,5),(100,100)]\n}\n\nmlp_cv_model = GridSearchCV(mlp_model,mlp_params,cv=10,verbose=2,n_jobs=-1).fit(X_train,y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", mlp_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Tree (CART) \n* The aim is to transform the complex structures in the dataset into simple decision structures.\n* Heterogeneous data sets are divided into homogeneous subgroups according to a specified target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\ncard_model = DecisionTreeClassifier(random_state=42)\ncard_model.fit(X_train,y_train)\ny_pred = card_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean of error squares: \", np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"Accuracy: \",card_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way\ncard_params = {\n    \"max_depth\": [2,3,4,5,10,20],\n    \"min_samples_split\": [2,10,5,30,50,10]\n}\ncard_model = DecisionTreeClassifier()\ncard_cv_model = GridSearchCV(card_model,card_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \",card_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forests\n* The basis is based on gathering and evaluating the estimates produced by more than one decision tree created by the bootstrap method.\n- Lowers the mean square root value of the error squares.\n- Increases the correct classification rate.\n- It reduces variance and is resistant to memorization.\n- Combining the predictions produced by multiple decision trees, the basis of\n- Observations for trees are selected using the random sample selection method of bootstrap, and random subspace method.\n- In each node of the decision tree, the best branching variable is chosen among the less number of variables randomly selected from all variables.\n- 2/3 of the data set is used in tree building. Outside data is used for performance evaluation of trees and determination of variable significance.\n- Random variables are selected at each joint."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nrf_model = RandomForestClassifier(random_state=42).fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nprint(\"Accuracy: \", rf_model.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way\nrf_params = {\n    \"max_depth\": [5,8,10],\n    \"max_features\": [2,5,10],\n    \"n_estimators\": [200,500,1000,2000],\n    \"min_samples_split\": [2,10,80,100]\n}\n\nrf_cv_model = GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", rf_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Machines (GBM)\n* AdaBoost is a generalized version that can be easily adapted to classification and regression problems.\n* A series of models in the form of a single predictive model is established on the residuals.\n* Finding coefficients or decision rules to minimize the mean of error squares.\n* Gradient Boosting creates a series of models in the form of a single predictive model.\n* A model in the series is created by overwriting the prediction residues / errors of the previous model in the series (feet).\n* GBM can use many basic learner types.\n* COST functions and link functions can be modified.\n* Boosting + Gradient Descent\n\n### Introduction to Boosting Methods\n* Weak students are based on the idea of ​​coming together and revealing a strong student.\n* Bad estimation is the big value that results from squaring the difference between the real and the estimated values. Trees that make bad predictions are also weak estimators.\n\n### AdaBoost (Adaptive Boosting)\n* It is the algorithm that brings the idea of ​​weak classifiers to come together and form a strong classifier.\n\n![](https://miro.medium.com/proxy/1*m2UHkzWWJ0kfQyL5tBFNsQ.png)\n\n* Classification process has been made in the 1st picture\n* + Symbols in the box represent blue classification, - symbols represent red classification. In this case, some symbols are in the wrong box.\n* In the second picture, a classification was made again considering the situations in the first picture.\n* In the third picture, a new classification was made by taking the second picture into consideration.\n* The 4th image classification process has been provided successfully."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\ngbm_model = GradientBoostingClassifier().fit(X_train,y_train)\ny_pred = gbm_model.predict(X_test)\nprint(\"Accuracy: \", gbm_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way\ngbm_model = GradientBoostingClassifier().fit(X_train,y_train)\ngbm_params = {\n    \"learning_rate\": [0.001,0.1,0.001],\n    \"max_depth\": [3,5,8],\n    \"n_estimators\": [100,200,500],\n    \"min_samples_split\": [1,0.5,0.8],\n}\n\ngbm_cv_tuned = GridSearchCV(gbm_model,gbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", gbm_cv_tuned.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\n* XGBoost is the scalable and scalable version of GBM optimized to increase speed and forecast performance.\n* It is scalable.\n* It is fast.\n* Prediction success is high.\n* It proved its success in many kaggle competitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nfrom xgboost import XGBClassifier\n\ny = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nxgb = XGBClassifier().fit(X_train,y_train)\nprint(\"Accuracy: \", xgb.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\nxgb_params = {\n    \"learning_rate\": [0.01,0.01,0.5],\n    \"max_depth\": [2,3,4,5,8],\n    \"n_estimators\": [100,200,500,1000],\n    \"colsample_bytree\": [0.4,0.7,1]\n}\n\nxgb_cv_model = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=1).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", xgb_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM\n* Light GBM is another type of GBM developed to increase XGBoost's training time performance.\n* More performance.\n* Leaf-wise growth strategy instead of level-wise growth strategy.\n* Depth-first search (DFS) instead of Breadth-first search (BFS)."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nfrom lightgbm import LGBMClassifier\n\nlgb_model = LGBMClassifier().fit(X_train,y_train)\nprint(\"Accuracy: \", lgb_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\n    \"learning_rate\": [0.01,0.1,0.5,1],\n    \"n_estimators\": [20,40,100,200,500,1000],\n    \"max_depth\": [1,2,3,4,5,6,7,8,9,10]\n}\n\nlgbm_cv_model = GridSearchCV(lgb_model,lgbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", lgbm_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost (Category Boosting)\n* Another fast, successful GBM derivative that can automatically combat categorical variables.\n* Categorical variable support\n* Fast and scalable GPU support\n* More successful predictions\n* Fast train and fast prediction\n* Russia's first open source, successful ML study"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Outcome\"].values\nx_data = data.drop([\"Outcome\"],axis=1)\nX = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\nfrom catboost import CatBoostClassifier\ncatb_model = CatBoostClassifier(random_state=42).fit(X_train,y_train)\nprint(\"Accuracy: \", catb_model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another way\ncatb_params = {\n    \"iterations\": [200,500,100],\n    \"learning_rate\": [0.01,0.1],\n    \"depth\": [3,6,8]\n}\ncatb_model = CatBoostClassifier()\ncatb_cv_model = GridSearchCV(catb_model,catb_params,cv=5,n_jobs=-1,verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: \", catb_cv_model.best_score_*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Automation of Machine Learning Tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compML(df,alg):\n    y = data[\"Outcome\"].values\n    x_data = data.drop([\"Outcome\"],axis=1)\n    X = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n\n    model = alg().fit(X_train,y_train)\n    model_name = alg.__name__\n    print(model_name,\": \", model.score(X_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    LGBMClassifier,\n    XGBClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    DecisionTreeClassifier,\n    MLPClassifier,\n    KNeighborsClassifier,\n    SVR\n]\n\nfor i in models:\n    compML(data,i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are better results found with GridSearch above. I do not repeat because the procedures are long."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}