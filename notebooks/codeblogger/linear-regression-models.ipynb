{"cells":[{"metadata":{},"cell_type":"markdown","source":"- [Load and Check Data](#-1)\n- [Data Preparation](#0)\n- [Simple Linear Regressions](#1)\n- [Multiple Linear Regression](#2)\n- [Ridge Regression](#4)\n- [Lasso Regression](#5)\n- [ElasticNet Regression](#6)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nimport warnings            \nwarnings.filterwarnings(\"ignore\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Check Data <a id=\"-1\"></a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"y_2018 = pd.read_csv(\"/kaggle/input/world-happiness/2018.csv\");\ny_2019 = pd.read_csv(\"/kaggle/input/world-happiness/2019.csv\");\n\ndata = pd.concat([y_2018,y_2019],sort=False)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Veriable Description\n\n1. Overall rank: Ranking of countries by happiness level\n2. Country or region: Country or region names\n3. Score: Happiness scores\n4. GDP per capita: Value representing the country's income and expense levels\n5. Social support\n6. Healthy life expectancy\n7. Freedom to make life choices\n8. Generosity\n9. Perceptions of corruption"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change the column names for convenience."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename(columns={\n    \"Overall rank\": \"rank\",\n    \"Country or region\": \"country\",\n    \"Score\": \"score\",\n    \"GDP per capita\": \"gdp\",\n    \"Social support\": \"social\",\n    \"Healthy life expectancy\": \"healthy\",\n    \"Freedom to make life choices\": \"freedom\",\n    \"Generosity\": \"generosity\",\n    \"Perceptions of corruption\": \"corruption\"\n},inplace=True)\ndel data[\"rank\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Value "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns[data.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are empty elements in only one column. Let's look at how many."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"corruption\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_data_corruption = data[data[\"score\"] > 6.774].mean().corruption\ndata.loc[data[\"corruption\"].isnull(),[\"corruption\"]] = avg_data_corruption\ndata[data[\"corruption\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation <a id=\"0\"></a>\n## Inconsistent Observation\n* 95% of a machine learning model is said to be preprocessing and 5% is model selection. For this we need to teach the data to the model correctly. In order to prepare the available data for machine learning, we must apply certain pre-processing methods. One of these methods is the analysis of outliers. The outlier is any data point that is substantially different from the rest of the observations in a data set. In other words, it is the observation that goes far beyond the general trend.\n\n![](https://miro.medium.com/max/854/1*RW-vfIbKZh-UGsLfTAWpyw.png)"},{"metadata":{},"cell_type":"markdown","source":"Outlier values behave differently from other data models and they increase the error with overfitting, so the outlier model must be detected and some operations must be performed on it.\n### 1.Using Box Graph\nWe can see contradictory observations with many visualization techniques. One of them is the box chart. If there is an outlier, this is drawn as the point, but the other population is grouped together and displayed in boxes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()\ndf = df.select_dtypes(include=[\"float64\",\"int64\"])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]\nfor col in column_list:\n    sns.boxplot(x = df[col])\n    plt.xlabel(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have observed that there are outliers in the \"social\" and \"corruption\" column. This may cause us to negatively affect us while training our data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for corruption\ndf_table = df[\"corruption\"]\n\nQ1 = df_table.quantile(0.25)\nQ3 = df_table.quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is \" + str(lower_bound))\nprint(\"upper bound is \" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_vector = (df_table < (lower_bound)) | (df_table > (upper_bound))\noutliers_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_vector = df_table[outliers_vector]\noutliers_vector.index.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deleting data is not suitable for this data set. That's why we will fill out the outliers with the average."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_table = data.copy()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values] = df_table[\"corruption\"].mean()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Linear Regressions <a id=\"1\"></a>\nSimple linear regression is a statistical method that allows us to summarize and analyze the relationships between two continuous (quantitative) variables:\n\n## score - gdp\nFirstly let's observe the relationship between gdp and score with the help of graphics.\n* independent variable : x\n* dependent variable : y"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=\"gdp\",y=\"score\",data=df_table,kind=\"reg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nX = data[[\"gdp\"]]\nX.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[[\"score\"]]\ny.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression()\nmodel = reg.fit(X,y)\nprint(\"intercept: \", model.intercept_)\nprint(\"coef: \", model.coef_)\nprint(\"rscore. \", model.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"rscore meaning:\n* For example, the gdp argument used here describes 63% of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\nplt.figure(figsize=(12,6))\ng = sns.regplot(x=data[\"gdp\"],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_ylabel(\"score\")\ng.set_xlabel(\"gdb\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we want to do here is; For example, to answer the question of what is the happiness level of a country with a gdp value of 1. In other words, to estimate the desired value with the existing data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.intercep_ + model.coef_ * 1\nmodel.predict([[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gdb_list = [[0.25],[0.50],[0.75],[1.00],[1.25],[1.50]]\nmodel.predict(gdb_list)\nfor g in gdb_list:\n    print(\"The happiness value of the country with a gdp value of \",g,\": \",model.predict([g]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a class and make the job easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_reg(col,text,prdctn):\n    \n    sns.jointplot(x=col,y=\"score\",data=df_table,kind=\"reg\")\n    plt.show()\n    \n    X = data[[col]]\n    y = data[[\"score\"]]\n    reg = LinearRegression()\n    model = reg.fit(X,y)\n    \n    # prediction\n    plt.figure(figsize=(12,6))\n    g = sns.regplot(x=data[col],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\n    g.set_title(\"Model Equation\")\n    g.set_ylabel(\"score\")\n    g.set_xlabel(col)\n    plt.show()\n    \n    print(text,\": \", model.predict([[prdctn]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## score - social"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_reg(\"social\",\"The happiness value of the country whose sociability value is 2:\",2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## score - healthy"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_reg(\"healthy\",\"The happiness value of the country whose healthiest value is 1.20:\",1.20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## score - freedom"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_reg(\"freedom\",\"The happiness value of the country whose freedom value is 0.89:\",0.89)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiple Linear Regression <a id=\"2\"></a>\nThe main purpose is to find the linear function that expresses the relationship between dependent and independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sms\n\nX = df.drop(\"score\",axis=1)\ny = df[\"score\"]\n\n# OLS(dependent,independent)\nlm = sms.OLS(y,X)\nmodel = lm.fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R-squared: Percentages of independent variables that explain the change in dependent variables. <br>\nF-statistic: Expresses the significance of the model. <br>\ncoef: refers to coefficients. <br>\nstd err: standard errors. <br><br>\n\nHere we can make the following comments.\n- When the gdp value is increased by 1, the score increases by 0.8114.\n- When there is an increase of 1 unit from the social value, the score increases by 1.9740.\n..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create model with sckit learn\n\nlm = LinearRegression()\nmodel = lm.fit(X,y)\nprint(\"constant: \",model.intercept_)\nprint(\"coefficient: \",model.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREDICTION\n# Score = 0.929921*gdp + 1.06504217*social + 0.94321492*healthy + 1.40426054*freedom + 0.52070628*generosity + 0.88114008*corruption\n\nnew_data = [[1],[2],[1.25],[1.75],[1.50],[0.75]]\nnew_data = pd.DataFrame(new_data).T\nnew_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(new_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the amount of error\n\nfrom sklearn.metrics import mean_squared_error\n\nMSE = mean_squared_error(y,model.predict(X))\nRMSE = np.sqrt(MSE)\n\nprint(\"MSE: \", MSE)\nprint(\"RMSE: \", RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Linear & Multiple Linear Regression - Model Tuning <a id=\"3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop(\"score\",axis=1)\ny = df[\"score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(\"Training error\",np.sqrt(mean_squared_error(y_train,model.predict(X_train))))\nprint(\"Test error\",np.sqrt(mean_squared_error(y_test,model.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every time we change the random_state value we defined at first, a different result is returned. We need to find out which of these returns the best result. For this we need to do the following."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, y_train, cv=10, scoring=\"neg_mean_squared_error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvs_avg_mse = np.mean(-cross_val_score(model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\"))\ncvs_avg_rmse = np.sqrt(cvs_avg_mse)\n\nprint(\"Cross Val Score MSE = \",cvs_avg_mse)\nprint(\"Cross Val Score RMSE = \",cvs_avg_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge Regression <a id=\"4\"></a>\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n* It is resistant to over learning.\n* It is biased but its variance is low.\n* It is better than OLS when there are too many parameters.\n* Builds a model with all variables. It does not exclude the unrelated variables from the model, it approximates its coefficients to zero.\n\n![](https://i.ibb.co/2SJtqyB/Ek-A-klama-2020-04-21-202339.jpg)\n\n* The delta parameter that gives the smallest \"cross validation\" value is selected.\n* With this delta selected, the model is fit for observations again."},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression - Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nridge_model = Ridge(alpha=0.1).fit(X_train, y_train)\nridge_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An alpha value will be assigned with each coefficient. Error coefficients will be examined according to these values."},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In contrast to the different beta values, the changes in the coefficients of the variables in our data set appear in the graph above. As can be seen, as the coefficients increase, it approaches zero."},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression - Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"predict: \", y_pred[0:10])\nprint(\"real: \", y_train[0:10].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = np.mean(mean_squared_error(y_train,y_pred)) # rmse = square root of the mean of error squares\nprint(\"train error: \", RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two values ​​above. One of them is unverified, the other is the values ​​that represent the square root of the sum of the verified error squares. As you can see, the unverified value is almost half of the verified value. This result shows us that it is more correct to use the second method, not the first method, while taking the square root of the mean of the error squares."},{"metadata":{"trusted":true},"cell_type":"code","source":"# test error\ny_pred = ridge_model.predict(X_test)\nRMSE = np.mean(mean_squared_error(y_test,y_pred))\nprint(\"test error: \", RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Model - Model Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(10).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(30).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(90).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly."},{"metadata":{"trusted":true},"cell_type":"code","source":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,1000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use alpha_ feature to attract the most appropriate value."},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeCV.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso Regression <a id=\"5\"></a>\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n* Lasso regression = L1\n* Ridge regression = L2\n\n* It has been proposed to eliminate the disadvantage of leaving the related-unrelated variables in the model of the Ridge regression.\n* Coefficients near zero in Lasso.\n* But when the L1 norm is big enough in lambda, some coefficients make it zero. Thus, it makes the selection of the variable.\n* It is very important to choose Lambda correctly, CV is used here too.\n* Ridge and Lasso methods are not superior to each other.\n\n## Lasso Regression - Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nlasso_model = Lasso().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"intercept: \", lasso_model.intercept_)\nprint(\"coef: \", lasso_model.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coefficients for different lambda values\n\nalphas = np.random.randint(0,10000,10)\nlasso = Lasso()\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train,y_train)\n    coefs.append(lasso.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.gca()\nax.plot(alphas,coefs)\nax.set_xscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Regression - Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_model.predict(X_train)[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_model.predict(X_test)[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lasso_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Regression - Model Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_cv_model = LassoCV(cv=10,max_iter=100000).fit(X_train,y_train)\nlasso_cv_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_cv_model.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_tuned = Lasso().set_params(alpha= lasso_cv_model.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ElasticNet Regression <a id=\"6\"></a>\n* The aim is to find the coefficients that minimize the sum of error squares by applying a penalty.\n* ElasticNet combines L1 and L2 approaches.The aim is to find the coefficients that minimize the sum of error squares by applying a penalty.\n\n## ElasticNet Regression - Model & Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nenet_model = ElasticNet().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enet_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enet_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\nenet_model.predict(X_train)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enet_model.predict(X_test)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = enet_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}