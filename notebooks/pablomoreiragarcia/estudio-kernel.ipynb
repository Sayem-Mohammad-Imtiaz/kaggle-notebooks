{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Este es el kernel que vamos a estudiar\n\n### Alumnos:\n\n* Pablo Moreira Garcia\n* Ruben Martinez Sotoca"},{"metadata":{"papermill":{"duration":0.018699,"end_time":"2020-12-14T14:18:36.472052","exception":false,"start_time":"2020-12-14T14:18:36.453353","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Heart Faliure Prediction using Descision Tree"},{"metadata":{"papermill":{"duration":0.014602,"end_time":"2020-12-14T14:18:36.502439","exception":false,"start_time":"2020-12-14T14:18:36.487837","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Athul Mathew Konoor - 20016  M-Tech AI and DS 19AI613 Machine Learning Project- Hyper Parameter Tuning."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:36.536788Z","iopub.status.busy":"2020-12-14T14:18:36.536067Z","iopub.status.idle":"2020-12-14T14:18:36.586042Z","shell.execute_reply":"2020-12-14T14:18:36.586602Z"},"papermill":{"duration":0.068907,"end_time":"2020-12-14T14:18:36.586785","exception":false,"start_time":"2020-12-14T14:18:36.517878","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Read csv file into dataframe\ndf = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Primero importa la libreta de pandas y carga la base de datos. Luego llama a la función head para ver las primeras 5 instancias."},{"metadata":{"papermill":{"duration":0.015698,"end_time":"2020-12-14T14:18:36.61854","exception":false,"start_time":"2020-12-14T14:18:36.602842","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:36.668201Z","iopub.status.busy":"2020-12-14T14:18:36.667478Z","iopub.status.idle":"2020-12-14T14:18:36.670965Z","shell.execute_reply":"2020-12-14T14:18:36.671498Z"},"papermill":{"duration":0.037406,"end_time":"2020-12-14T14:18:36.671653","exception":false,"start_time":"2020-12-14T14:18:36.634247","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"Y = df['DEATH_EVENT']\nX = df[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']]\n\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, distingue entre las variables predictoras y la variable clase (DEATH_EVENT)."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:36.707572Z","iopub.status.busy":"2020-12-14T14:18:36.706908Z","iopub.status.idle":"2020-12-14T14:18:38.489935Z","shell.execute_reply":"2020-12-14T14:18:38.489375Z"},"papermill":{"duration":1.801979,"end_time":"2020-12-14T14:18:38.490056","exception":false,"start_time":"2020-12-14T14:18:36.688077","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Heatmap to Invertigate Correlation in Data\nsns.set()\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(df.corr(), linewidths=.5, ax=ax, cmap='Blues')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Luego, añade las librerías necesarias para realizar el mapa de calor de correlación entre las variables de toda la base de datos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:38.534486Z","iopub.status.busy":"2020-12-14T14:18:38.533617Z","iopub.status.idle":"2020-12-14T14:18:38.677242Z","shell.execute_reply":"2020-12-14T14:18:38.67635Z"},"papermill":{"duration":0.168441,"end_time":"2020-12-14T14:18:38.677417","exception":false,"start_time":"2020-12-14T14:18:38.508976","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = Y, test_size=0.2, random_state=52)\n\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of Y_train:', Y_train.shape)\nprint('Shape of Y_test:', Y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separa la base de datos en conjunto de entrenamiento y test. Y luego, utilizando la función shape, muestra la cantidad de filas y columnas de cada conjunto."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:38.724313Z","iopub.status.busy":"2020-12-14T14:18:38.723351Z","iopub.status.idle":"2020-12-14T14:18:38.928669Z","shell.execute_reply":"2020-12-14T14:18:38.928111Z"},"papermill":{"duration":0.231543,"end_time":"2020-12-14T14:18:38.92879","exception":false,"start_time":"2020-12-14T14:18:38.697247","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=None,max_features=None, max_leaf_nodes=None,\n                       min_impurity_split=1e-07, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       presort=False, random_state=None, splitter=\"best\")\n\n\nY_pred = dt.predict(X_test)\nY_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este paso importa la librería de árboles de decisión y entrena el modelo con el conjunto de entrenamiento. Luego, define los hiperparámetros del árbol de decisión y genera las predicciones para el conjunto de test.\n\nComete un fallo al declarar un modelo con los valores por defecto de los hiperparámetros y entrenarlo con el conjunto train y luego predice con este modelo. Pero entre medias hace un cambio en los hiperparámentros que no almacena en ningún sitio por lo que no influye al que realmente se utiliza. Si quisiera predecir con los hiperparámetros personalizados, debería asignarlo a la variable dt y entranarlo antes de predecir."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:38.974426Z","iopub.status.busy":"2020-12-14T14:18:38.972591Z","iopub.status.idle":"2020-12-14T14:18:38.979381Z","shell.execute_reply":"2020-12-14T14:18:38.978595Z"},"papermill":{"duration":0.03184,"end_time":"2020-12-14T14:18:38.979501","exception":false,"start_time":"2020-12-14T14:18:38.947661","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test,Y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizando librerías importadas, calcula la tasa de verdaderos y falsos positivos en su predicción obteniendo la curva ROC y calculando el área bajo la curva (AUC)."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:39.033628Z","iopub.status.busy":"2020-12-14T14:18:39.032895Z","iopub.status.idle":"2020-12-14T14:18:39.607918Z","shell.execute_reply":"2020-12-14T14:18:39.607373Z"},"papermill":{"duration":0.609132,"end_time":"2020-12-14T14:18:39.608089","exception":false,"start_time":"2020-12-14T14:18:38.998957","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\n\nmax_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor max_depth in max_depths:\n   dt = DecisionTreeClassifier(max_depth=max_depth)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n\n   # Add auc score to previous train results\n   train_results.append(roc_auc)\n\n   Y_pred = dt.predict(X_test)\n    \n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n    \n   # Add auc score to previous test results\n   test_results.append(roc_auc)\n    \nline1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"Tree depth\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:39.665126Z","iopub.status.busy":"2020-12-14T14:18:39.664008Z","iopub.status.idle":"2020-12-14T14:18:40.13028Z","shell.execute_reply":"2020-12-14T14:18:40.129491Z"},"papermill":{"duration":0.501533,"end_time":"2020-12-14T14:18:40.1304","exception":false,"start_time":"2020-12-14T14:18:39.628867","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\n\nmin_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor min_samples_split in min_samples_splits:\n   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds =    roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n    \n\nline1, = plt.plot(min_samples_splits, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(min_samples_splits, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"min samples split\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:40.189125Z","iopub.status.busy":"2020-12-14T14:18:40.187861Z","iopub.status.idle":"2020-12-14T14:18:40.607944Z","shell.execute_reply":"2020-12-14T14:18:40.607255Z"},"papermill":{"duration":0.455752,"end_time":"2020-12-14T14:18:40.608059","exception":false,"start_time":"2020-12-14T14:18:40.152307","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\n\nmin_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\n\nfor min_samples_leaf in min_samples_leafs:\n   dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"min samples leaf\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:40.669348Z","iopub.status.busy":"2020-12-14T14:18:40.668189Z","iopub.status.idle":"2020-12-14T14:18:41.067767Z","shell.execute_reply":"2020-12-14T14:18:41.067064Z"},"papermill":{"duration":0.437065,"end_time":"2020-12-14T14:18:41.06792","exception":false,"start_time":"2020-12-14T14:18:40.630855","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\n\nmax_features = list(range(1, 12))\ntrain_results = []\ntest_results = []\n\nfor max_feature in max_features:\n   dt = DecisionTreeClassifier(max_features=max_feature)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   Y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nline1, = plt.plot(max_features, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(max_features, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"AUC score\")\nplt.xlabel(\"max features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con estas funciones, hace una comparación entre la predicción realizada al conjunto de entrenamiento y la realizada sobre el conjunto de test. En todos los casos compara la tasa de verdaderos y falsos positivos a través de la curva ROC, pero en cada apartado utiliza un hiperparámetro distinto para mostrar como evoluciona el modelo con el valor de dicha variable."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.123345Z","iopub.status.busy":"2020-12-14T14:18:41.122566Z","iopub.status.idle":"2020-12-14T14:18:41.224287Z","shell.execute_reply":"2020-12-14T14:18:41.223571Z"},"papermill":{"duration":0.131318,"end_time":"2020-12-14T14:18:41.224401","exception":false,"start_time":"2020-12-14T14:18:41.093083","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn import decomposition, datasets\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.279024Z","iopub.status.busy":"2020-12-14T14:18:41.278276Z","iopub.status.idle":"2020-12-14T14:18:41.281623Z","shell.execute_reply":"2020-12-14T14:18:41.281089Z"},"papermill":{"duration":0.032853,"end_time":"2020-12-14T14:18:41.28174","exception":false,"start_time":"2020-12-14T14:18:41.248887","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"std_slc = StandardScaler()\n\npca = decomposition.PCA()\n\ndec_tree = tree.DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.336887Z","iopub.status.busy":"2020-12-14T14:18:41.335901Z","iopub.status.idle":"2020-12-14T14:18:41.338569Z","shell.execute_reply":"2020-12-14T14:18:41.339137Z"},"papermill":{"duration":0.032754,"end_time":"2020-12-14T14:18:41.339293","exception":false,"start_time":"2020-12-14T14:18:41.306539","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pipe = Pipeline(steps=[('std_slc', std_slc),\n                           ('pca', pca),\n                           ('dec_tree', dec_tree)])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.394036Z","iopub.status.busy":"2020-12-14T14:18:41.393354Z","iopub.status.idle":"2020-12-14T14:18:41.397205Z","shell.execute_reply":"2020-12-14T14:18:41.396459Z"},"papermill":{"duration":0.033526,"end_time":"2020-12-14T14:18:41.397331","exception":false,"start_time":"2020-12-14T14:18:41.363805","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"n_components = list(range(1,X.shape[1]+1,1))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.45465Z","iopub.status.busy":"2020-12-14T14:18:41.453796Z","iopub.status.idle":"2020-12-14T14:18:41.45656Z","shell.execute_reply":"2020-12-14T14:18:41.457037Z"},"papermill":{"duration":0.035067,"end_time":"2020-12-14T14:18:41.457187","exception":false,"start_time":"2020-12-14T14:18:41.42212","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"criterion = ['gini', 'entropy']\nmax_depth = [2,4,6,8,10,12]\nmax_features = [6,7,8,9,10,11,12]\n\nmin_samples_leaf = [0.10, 0.2, 0.3, 0.4, 0.5]\nmin_samples_split = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.515295Z","iopub.status.busy":"2020-12-14T14:18:41.51435Z","iopub.status.idle":"2020-12-14T14:18:41.518228Z","shell.execute_reply":"2020-12-14T14:18:41.517637Z"},"papermill":{"duration":0.035417,"end_time":"2020-12-14T14:18:41.518342","exception":false,"start_time":"2020-12-14T14:18:41.482925","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"parameters = dict(pca__n_components=n_components,\n                dec_tree__criterion=criterion,\n                dec_tree__max_depth=max_depth,\n                dec_tree__min_samples_leaf = min_samples_leaf,\n                dec_tree__min_samples_split = min_samples_split)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:18:41.573911Z","iopub.status.busy":"2020-12-14T14:18:41.573182Z","iopub.status.idle":"2020-12-14T14:24:49.728557Z","shell.execute_reply":"2020-12-14T14:24:49.729145Z"},"papermill":{"duration":368.185971,"end_time":"2020-12-14T14:24:49.729307","exception":false,"start_time":"2020-12-14T14:18:41.543336","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"clf_GS = GridSearchCV(pipe, parameters)\nclf_GS.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:24:49.786381Z","iopub.status.busy":"2020-12-14T14:24:49.785664Z","iopub.status.idle":"2020-12-14T14:24:49.802084Z","shell.execute_reply":"2020-12-14T14:24:49.802717Z"},"papermill":{"duration":0.047049,"end_time":"2020-12-14T14:24:49.802895","exception":false,"start_time":"2020-12-14T14:24:49.755846","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\nprint('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\nprint('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n# print('Max_features:', clf_GS.best_estimator_.get_params()['dec_tree__max_features'])\nprint('min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\nprint('min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\nprint()\nprint(clf_GS.best_estimator_.get_params()['dec_tree'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui encuentra los mejores hiperparametros para el modelo utilizado, usando la busqueda GRID."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:24:49.861292Z","iopub.status.busy":"2020-12-14T14:24:49.860241Z","iopub.status.idle":"2020-12-14T14:24:50.142108Z","shell.execute_reply":"2020-12-14T14:24:50.14272Z"},"papermill":{"duration":0.312719,"end_time":"2020-12-14T14:24:50.14289","exception":false,"start_time":"2020-12-14T14:24:49.830171","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=4, min_samples_leaf=0.1,\n                       min_samples_split=0.1)\n\nY_predict = dt.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T14:24:50.203257Z","iopub.status.busy":"2020-12-14T14:24:50.202179Z","iopub.status.idle":"2020-12-14T14:24:50.463046Z","shell.execute_reply":"2020-12-14T14:24:50.462166Z"},"papermill":{"duration":0.292047,"end_time":"2020-12-14T14:24:50.463174","exception":false,"start_time":"2020-12-14T14:24:50.171127","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \n\ndt = DecisionTreeClassifier()\nmodel = dt.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos comprobar, al repetir la ejecución de las matrices de confusión varias veces, vemos que cambian las proporciones de una ejecución a otra. Esto se debe a que no se ha fijado un random_state que asegure que los datos que se utilizan son siempre los mismos. A continuación añadiremos un random_state en ambas matrices y podremos comprobar que los resultados ya no varían:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state = 1)\ndt.fit(X_train, Y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=4, min_samples_leaf=0.1,\n                       min_samples_split=0.1)\n\nY_predict = dt.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vuelve a cometer el mismo error de antes, entrenando un modelo estándar y luego cambiando los parámetros. Estos parámetros nos se almacenan en ninguna parte y el modelo que predice es el que se ha creado al principio. Por tanto, esa acción se puede obviar o debería hacerse cuando se crea el modelo."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \n\ndt = DecisionTreeClassifier(random_state = 1)\nmodel = dt.fit(X_train, Y_train)\nY_predict = model.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora podemos comprobar tambien que como dijimos antes las especificaciones de hiperparametros que realiza no se almacenan ni se utilizan, ya que ejecutamos el mismo codigo con estas especificaciones y sin ellas y los resultados no cambian."},{"metadata":{},"cell_type":"markdown","source":"Como detalle, podemos mostrar como se podría mejorar el análisis del modelo si establecieran los parámetros antes de entrenar y predecir."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report \nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=4, min_samples_leaf=0.1,\n                       min_samples_split=0.1, random_state = 1)\n\ndt.fit(X_train, Y_train)\n\n\nY_predict = dt.predict(X_test)\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_predict, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=True,\n                                    cmap='Blues')\n\nplt.show()\n\nprint(classification_report(Y_test, Y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A modo de conclusión, podemos decir que se trata de un estudio bastante correcto de modelos, pero que comete varios errores a la hora de generarlos (aunque algunos de esos errores son acciones que no tienen ninguna repercusión en el proceso). \nA parte de esto, no se producen fugas de datos ni otro tipo de errores que pueda afectar al estudio."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}