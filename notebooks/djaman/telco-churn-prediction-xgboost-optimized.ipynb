{"cells":[{"metadata":{"_uuid":"c008f5e6d0137ff39b3e9acf1b4e7043e20051a1"},"cell_type":"markdown","source":"# Telcom Customer Churn\nBuilt a model using Xgboost and using Grid Search Optimisation technique to tweak various hyperparameters.\nPlease upvote if you find this kernel useful "},{"metadata":{"trusted":true,"_uuid":"3717b04989b4048fdf842261a81c26584d4c203a"},"cell_type":"code","source":"#installing dependencies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01f6df1a4f097068bb45e1e4781087ae363be705"},"cell_type":"code","source":"\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import cross_validation, metrics \nfrom sklearn.grid_search import GridSearchCV   #Perforing grid search\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ec39e4aec1b75130e01d53e670832eadfd005c"},"cell_type":"code","source":"#reading the csv file \ndf = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"054ebe04b2a5c4ec4d9c745390b753f6d94e28a1"},"cell_type":"code","source":"#snippet of dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b616332d3f6966fba4427f3c7c997512636c62a1"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0afe7c5b92df57bde78157f1e2b21b92f0ad7e"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2813b62d83603ebc9d875cde16a5e0b197c5c137"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6aac784dfcc03d36ce6eca497cbf417b833324d"},"cell_type":"code","source":"#density plot of churn column\ndf.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a35977bae292b025ecabec2ea4511f5009a1d2b"},"cell_type":"code","source":"#dropping the column customerID as its not needed\ndf = df.drop(columns = ['customerID'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50da4ba2fdee0da099a7daabc731c5ed519119a0"},"cell_type":"code","source":"#http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n#One hot encoding all the columns with values Yes No.\n\nfrom sklearn.preprocessing import LabelEncoder\nencoded_df = df.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)\nencoded_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d98426ba8ac6d839654e989702b6879c48094854"},"cell_type":"code","source":"plt.hist(encoded_df['Churn'] )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2e265a671cb9914b5d14a72b6369ba2be4b3752c"},"cell_type":"code","source":"# Correlation matrix\ncorr = encoded_df.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddfb23109eabc52cff2848c8473f24094140fd45"},"cell_type":"code","source":"\nencoded_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d28fd867aa218ba47028172ae23d25dd56aa38b6"},"cell_type":"code","source":"features = encoded_df [['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n       'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n       'MonthlyCharges', 'TotalCharges']]\nlabel = encoded_df['Churn']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bf5af67802983c378b4c3248bb9c06b77aa1a06"},"cell_type":"code","source":"#train test split\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.33, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"383b0172961d3a3e14aeca69cafee11206b18f7c"},"cell_type":"markdown","source":"\n# TRAIN THE MODEL"},{"metadata":{"_uuid":"e8c3942e7d1b647bd321220a5e3ac2fd3d625292"},"cell_type":"markdown","source":"lets define a function which will help us create XGBoost models and perform cross-validation. The best part is that you can take this function as it is and use it later for your own models."},{"metadata":{"trusted":true,"_uuid":"15ebb216938a0cd6aa1c2b482407dda580c9a0fb"},"cell_type":"code","source":"\n#function to train the model\n\ndef modelfit(alg,features_train,label_train):\n    \n    X_train, X_test, y_train, y_test = train_test_split(features_train, label_train, test_size=0.33, random_state=7)\n    #Fit the algorithm on the data\n    alg.fit(X_train, y_train)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X_train)\n    dtrain_predprob = alg.predict_proba(X_train)[:,1]\n    \n    #Predict Test set:\n    dtest_predictions = alg.predict(X_test)\n    dtest_predprob = alg.predict_proba(X_test)[:,1]\n        \n    #Print model report:\n    print (\"Model Report\")\n    #print (\"Training set Accuracy : %.4g\" % alg.score(y_train.values, dtrain_predictions))\n    print (\"Training set Accuracy : %.4g\" % metrics.accuracy_score(y_train.values, dtrain_predictions))\n    #print (\"Test set Accuracy : %.4g\" % alg.score(y_test.values, dtest_predictions))\n    print (\"Test set Accuracy : %.4g\" % metrics.accuracy_score(y_test.values, dtest_predictions))\n    print (\"Training set AUC Score (Train) : %f\" % metrics.roc_auc_score(y_train, dtrain_predprob))\n    print (\"AUC Score (Test) : %f\" % metrics.roc_auc_score(y_test, dtest_predprob))               \n                                                               \n    #the feat imp result will be in np array, convert it to Series so we can plot it later\n    feat_imp = pd.Series(alg.feature_importances_)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33743a19a343cbf08e741024df3624abc505c33d"},"cell_type":"code","source":"#Choose all predictors except target & IDcols\nxgb1 = XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb1, features, label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d7a9a89a83e9e2db62dc7a9f9fa489a2e991ceb"},"cell_type":"markdown","source":"# Hyperparameter Optimization"},{"metadata":{"_uuid":"8df5c234bec00efd85852078bbc59a15540723a9"},"cell_type":"markdown","source":"We will be tweaking parameters to increase the accuracy  \n\n- Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.  \n- Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.  \n- Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.  \n- Lower the learning rate and decide the optimal parameters  "},{"metadata":{"trusted":true,"_uuid":"eb97f3c18d15cb6185ab083f102b2bfcaa65d9cc"},"cell_type":"code","source":"#Base Model\n\nxgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb2, features, label)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e410196b0e44c9a4e1b013b7093b29a8edc56500"},"cell_type":"markdown","source":"# Grid Search\nThe grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter."},{"metadata":{"_uuid":"3c192ed73550d9c9d77d798b630839b930ead656"},"cell_type":"markdown","source":"## Tune max_depth and min_child_weight"},{"metadata":{"trusted":true,"_uuid":"dd6976af468c867429e76426ca76bdc35b3d23f7"},"cell_type":"code","source":"#parameters are passed as a Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored.\nparam_test2 = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6] }\ngsearch2 = GridSearchCV(estimator = xgb2, \n     param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n    \ngsearch2.fit(X_train,y_train)\ngsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b852973cca99e8542c24c1c472f38362baa9aae"},"cell_type":"markdown","source":"The best values are max_depth': 4, 'min_child_weight': 6"},{"metadata":{"_uuid":"694e5a7c722c845a20cde488455c8337b82e87a7"},"cell_type":"markdown","source":"\n## Tune Gamma "},{"metadata":{"trusted":false,"_uuid":"f8e81e02b94d7728a13d37b417c483ad23d80d3d"},"cell_type":"code","source":"#Update the estimator using the parameters already tunes before that is, max_Depth and min_child_weight\n\nxgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\n#parameters are passed as a Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored.\nparam_test3 = {\n 'gamma':[0.1,0.2,0.3,0.4,0.5] }\ngsearch3 = GridSearchCV(estimator = xgb3, \n     param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n    \ngsearch3.fit(X_train,y_train)\ngsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e092d3cdb9d118e2ab846096161c98289c15212"},"cell_type":"markdown","source":"## Tune Learning Rate "},{"metadata":{"trusted":false,"_uuid":"acc6a445ed1e16e8608c150d317e4bd15c4ede7c"},"cell_type":"code","source":"#Update the estimator using the parameters already tunes before that is, gamma\n\nxgb4 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=4,\n min_child_weight=6,\n gamma=0.2,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nparam_test4 = {\n 'learning_rate':[0.1,0.2,0.3,0.4,0.5] }\ngsearch4 = GridSearchCV(estimator = xgb4, \n     param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n    \ngsearch4.fit(X_train,y_train)\ngsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a5e917d7ae0d5317f6753f4c3362fc21e20c215f"},"cell_type":"code","source":"#Re- train the model with tweaked paramters \nxgb5 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=4,\n min_child_weight=6,\n gamma=0.2,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb5, features, label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"723367697dd1d86c994144c49ee1d7eaa45a72cc"},"cell_type":"markdown","source":"Test Acuracy increased to 78.45%"},{"metadata":{"trusted":false,"_uuid":"abdff05a79d903438654762fe956fc0d04e180cb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}