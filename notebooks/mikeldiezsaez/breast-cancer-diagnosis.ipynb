{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Breast cancer diagnosis\n\nThe aim is to find a good classifier capable of defining if a breast cancer is benign or malignant depending on some given parameters.\n\nAs a reference, the data set has been taken from https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\nThe data set is mainly explained in that site and the source of the data is provided."},{"metadata":{},"cell_type":"markdown","source":"## First look to the data set\n\nWe need to take a look to the csv columns delimiter and other issues that could be problematic at the time of the data import."},{"metadata":{"trusted":true},"cell_type":"code","source":"check_data = !head -n 2 '../input/breast-cancer-wisconsin-data/data.csv'\ncheck_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import libraries\n\nAll the necessary libraries are included herein. \nIn case of any new library is needed, it is added here."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import plot_roc_curve, roc_curve, roc_auc_score, auc\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization\n\n### Import data\nThe CSV file is imported taking the delimiter into consideration and then the first five rows are visualized in order to confirm the import process has been carried out as expected. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import csv file\nbreast_cancer = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv', sep = ',')\nbreast_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data overview\n\nCount of values per row, mean, standard deviation, percentiles and max/min values are checked to understand the data distribution and try to capture any error in the data. There are some columns such as concavity_mean, concave points_mean, concavity_worst and concavity points_worst have a minimum value equal to zero, but it could be possible in this case. \nColumns, the count of non-null objects and the type of the values per column are displayed.\n\nAs well, the unique values at diagnosis column are checked to confirm there are only two types of diagnosis: benign or malignant.\n\nOnce all this data has been reviewed, it is concluded that there is not missing or mistaken data in the data set, so it can be used as is."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data cleaning (last col missing values, the rest is clean)\nbreast_cancer = breast_cancer.loc[:, 'diagnosis':'fractal_dimension_worst']\nprint(breast_cancer.describe())\nprint(breast_cancer.info())\nbreast_cancer['diagnosis'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Labels distribution\n\nThere are enough labels for each diagnosis type to make good predictions, as can be seen in the histogram herein below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a histogram to verify there is an enough number of each label \nplt.hist(breast_cancer['diagnosis'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preparation for visual comparison of mean values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remap diagnosis labels\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'B':0, 'M':1})\n\n# split df into benign and malignant to compare data distribution for both labels\nbreast_cancer_B = breast_cancer[breast_cancer['diagnosis'] == 0]\nbreast_cancer_M = breast_cancer[breast_cancer['diagnosis'] == 1]\n\n# extract features\nfeatures = list(breast_cancer.columns[1:11])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create subplots to compare values distribution\n\nIn the figures below can be seen some important features when comparing their mean values distribution, for instance: radius_mean, perimeter_mean, area_mean, concavity_mean and concavity mean_points seem to be the most important features since the distributions of benign and malignant labels are not so coupled.\nNevertheless, let us confirm the suspect by mean of random forest features importances."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create subplots for data mean values\nfig, axes = plt.subplots(nrows=len(features)//2, ncols=2, figsize=(8, 10))\naxes = axes.ravel()\nnum_bins = 50\n\n# create each subplot\nfor idx, ax in enumerate(axes):\n    ax.figure\n    width_per_bin = (max(breast_cancer[features[idx]]) - min(breast_cancer[features[idx]])) / num_bins\n    ax.hist([breast_cancer_B[features[idx]], breast_cancer_M[features[idx]]], \n            bins=np.arange(min(breast_cancer[features[idx]]) - width_per_bin, \n                           max(breast_cancer[features[idx]]) + width_per_bin, \n                           width_per_bin), \n            histtype='step', alpha = 0.5, color=['g', 'r'], label=['B', 'M'])\n    ax.legend()\n    ax.set_title(features[idx])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features importance\n\nWe are going to verify which are the most important features for the definition of benign and malignant labels and the weight of each one.\nAnyway, all the features are going to be used in the models, which maybe can be finally improved by using these features."},{"metadata":{"raw_mimetype":null,"trusted":true},"cell_type":"code","source":"# prepare data for train and test\nX = breast_cancer.iloc[:, 1:11].values\ny = breast_cancer['diagnosis'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n\n# run a quick random forest to see if features importances are in line with above distributions\nrnd_forest = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrnd_forest.fit(X, y)\nfeatures_imp = pd.Series(rnd_forest.feature_importances_, index=features).sort_values(ascending=False)\nprint(features_imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning models\n\nFor this type of problem, the following classifiers could be used:\n- Random Forest\n- SGD\n- K-Neighbors\n- Voting classifier including the previous classifiers.\n\nThey are analyzed, compared and the best one is going to be adapted into an Adaboost classifier.\n\nA GridSearchCV has been run for all the models, except the Voting Classifier, to use the best parameters. This step has been removed from here for running speed purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest classifier model\nmodel_forest = RandomForestClassifier(n_estimators=10,  criterion='entropy',\n                                      max_depth=3, min_samples_leaf=2, \n                                      max_features='auto', random_state=21)\nmodel_forest.fit(X_train, y_train)\n\n# stochastic gradient descent classifier model\nmodel_sgd = SGDClassifier(random_state=21, loss='log', penalty='l1', \n                          alpha=0.0001, max_iter=100, learning_rate='optimal', \n                          n_iter_no_change=15, eta0=0.1)\nmodel_sgd.fit(X_train, y_train)\n\n# KNeighbors classifier model\nmodel_kneighbors = KNeighborsClassifier(n_neighbors=9, weights='distance', \n                                        algorithm='auto', leaf_size=2)\nmodel_kneighbors.fit(X_train, y_train)\n\n# VotingClassifier\nmodel_voting = VotingClassifier(\n    estimators=[('forest', model_forest), ('sgd', model_sgd), ('kneighbors', model_kneighbors)],\n    voting='hard')\nmodel_voting.fit(X_train, y_train)\n\nmodels_list = [model_forest, model_sgd, model_kneighbors, model_voting]\nmodels_name = ['Random Forest', 'SGD', 'K-Neighbors', 'Voting']\n\n# print the scores of the four models\nprint('Random forest score: {:.2f}'.format(model_forest.score(X_test, y_test)))\nprint('SGD score: {:.2f}'.format(model_sgd.score(X_test, y_test)))\nprint('K-Neighbors score: {:.2f}'.format(model_kneighbors.score(X_test, y_test)))\nprint('Voting Classifier score: {:.2f}'.format(model_voting.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-validation scores\n\nThe previous scores show that Random Forest performs even better than the Voting Classifier. Nonetheless, a cross-validation score is calculated with 5 different splits. More or less splits do not change the result of this comparison.\n\nThe results confirm that Random Forest is the best one of these four models through all the splits.\nA dataframe including all the croos-validation scores is returned as well as a plot to ease the visualization of the performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare cross validated scores\ncross_val_score_forest = cross_val_score(model_forest, X_test, y_test, cv=5)\ncross_val_score_sgd = cross_val_score(model_sgd, X_test, y_test, cv=5)\ncross_val_score_kneighbors = cross_val_score(model_kneighbors, X_test, y_test, cv=5)\ncross_val_score_voting = cross_val_score(model_voting, X_test, y_test, cv=5)\n\ncross_val_scores = [cross_val_score_forest, cross_val_score_sgd, \n                    cross_val_score_kneighbors, cross_val_score_voting]\ncross_val_names = models_name.copy()\n\ndef print_scores(scores, c=0):\n    \n    \"\"\"\n    Create a list to link all the cross-validated scores\n    to each model and create a dictionary for a pandas \n    dataframe generation.\n    \"\"\"\n    \n    scores_to_df = []\n    for index, i in enumerate(scores[c]):\n        scores_to_df.append(i)\n    return scores_to_df\n\n# create the pandas dataframe\ndict_scores = {}\nfor i in range(len(cross_val_scores)):\n    dict_scores[cross_val_names[i]] = print_scores(cross_val_scores, c=i)\ndf_scores = pd.DataFrame(dict_scores)\n\n# plot the cross-validated scores stored in the pandas dataframe\nplt.plot(df_scores)\nplt.xlabel('Cross validation')\nplt.xticks(np.arange(0, len(df_scores)+ 1, 1))\nplt.ylabel('Score')\nplt.legend(df_scores.columns)\nplt.title('Cross-validation scores comparison')\nplt.show()\n\npd.options.display.float_format = '{:.2f}'.format\ndf_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC curves\n\nAlthough Random Forest classifier seems to be the one performing best, we can compare the models by plotting the ROC curves and check their area ratio under the curve.\n\nIt is clear that Random Forest can be a good candidate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discard Voting Classifier due to no decision_function nor predict_proba\nmodels_no_voting = models_list.copy()\ndel(models_no_voting[-1])\nname_no_voting = models_name.copy()\ndel(name_no_voting[-1])\n\n# plot roc curves\nfor m, n in zip(models_no_voting, name_no_voting):\n    ax = plt.gca()\n    plot_roc_curve(m, X_test, y_test, name=n, ax=ax)\nax.set_title('ROC curves comparison')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest ROC curves cross-validation\n\nIt is important to ensure that Random Forest is consistent through different data splits and check how high is the standard deviation of all the splits. This way, it is possible to have a better idea of the performance variability of this model.\n\nThese ROC curves have been defined with 5 and 10 splits, but since no relevant changes have been detected between both numbers of splits, finally 5 splits are used. \n\nConclusions:\n- The standard deviation, represented by a light blue shaped area, is not high and almost all the splits are inside the standard deviation thresholds.\n- The lowest AUC is 0.95 for fold 2.\n- The mean AUC for all the splits is equal to 0.98 with a standard deviation of 0.02.\n- Random Forest estimator could be good enough to pass to a Adaboost classifier as base estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check AUC variation per rnd_forest classifier k-fold\nn_splits = 5\ncv = StratifiedKFold(n_splits=n_splits)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    model_forest.fit(X[train], y[train])\n    viz = plot_roc_curve(model_forest, X[test], y[test],\n                         name='ROC fold {}'.format(i+1),\n                         alpha=0.3, lw=1, ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0\n    tprs.append(interp_tpr)\n    aucs.append(auc(viz.fpr, viz.tpr))\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=0.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nstd_tpr = np.std(tprs, axis=0) \ntpr_lower = np.maximum(mean_tpr - std_tpr, 0)\ntpr_upper = np.minimum(mean_tpr + std_tpr, 1)\n\nax.plot(mean_fpr, mean_tpr, linestyle='-', lw=2, color='b', \n        label='Mean ROC (AUC = {:.2f} $\\pm$ {:.2f})'.format(mean_auc, std_auc), alpha=0.8)\nax.fill_between(mean_fpr, tpr_lower, tpr_upper, label='$\\pm$ 1 st. dev.', alpha=0.2)\nax.set_title('ROC curves for Random Forest Classifier and {} folds'.format(n_splits))\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adaboost model definition and comparison\n\nAn Adaboost is built using the previously defined Random Forest model as the base estimator. A GridSearchCV has been run to use the best parameters of the Adaboost model. This step has been removed from here for running speed purposes.\n\nThe adaboost model is then compared by means of the ROC curve to all the models, except the Voting Classifier. As can be seen, Random Forest is slightly improved with an AUC value very near to 1.\n\nThe score of Adaboost is higher than 0.97, even better than the score of 0.94 provided by the Random Forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboost classifier model\nmodel_adaboost = AdaBoostClassifier(base_estimator=model_forest, learning_rate=0.1, \n                                   n_estimators=500, random_state=0)\nmodel_adaboost.fit(X_train, y_train)\n\n# Discard Voting Classifier due to no decision_function nor predict_proba\nmodels_no_voting_adaboost = models_no_voting.copy()\nmodels_no_voting_adaboost.append(model_adaboost)\nname_no_voting_adaboost = name_no_voting.copy()\nname_no_voting_adaboost.append('Adaboost')\n\n# plot roc curves\nfor m, n in zip(models_no_voting_adaboost, name_no_voting_adaboost):\n    ax = plt.gca()\n    plot_roc_curve(m, X_test, y_test, name=n, ax=ax)\nax.set_title('ROC curves comparison')\nplt.show()\nprint('Adaboost score: {:.3f}'.format(model_adaboost.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adaboost ROC curves\n\nAs done previously for the Random Forest model, a k-fold verification is done for the Adaboost model with 5 and 10 different splits. Results are slightly worse with 10 folds, so this is the baseline to conclude the performance of the Adaboost. The following results are obtained:\n- Worst AUC is equal to 0.96 for the fold 2.\n- Mean AUC is equal to 0.99 with a standard deviation of 0.01.\n- The standard deviation of the folds ROC curves is even smaller than for Random Forest.\n- The performance is really good and it works better than the Random Forest alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check AUC variation per Adaboost classifier k-fold\nn_splits = 10\ncv = StratifiedKFold(n_splits=n_splits)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\nfig, ax = plt.subplots(figsize=[8, 8])\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    model_adaboost.fit(X[train], y[train])\n    viz = plot_roc_curve(model_adaboost, X[test], y[test],\n                         name='ROC fold {}'.format(i+1),\n                         alpha=0.3, lw=1, ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0\n    tprs.append(interp_tpr)\n    aucs.append(auc(viz.fpr, viz.tpr))\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=0.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nstd_tpr = np.std(tprs, axis=0) \ntpr_lower = np.maximum(mean_tpr - std_tpr, 0)\ntpr_upper = np.minimum(mean_tpr + std_tpr, 1)\n\nax.plot(mean_fpr, mean_tpr, linestyle='-', lw=2, color='b', \n        label='Mean ROC (AUC = {:.2f} $\\pm$ {:.2f})'.format(mean_auc, std_auc), alpha=0.8)\nax.fill_between(mean_fpr, tpr_lower, tpr_upper, label='$\\pm$ 1 st. dev.', alpha=0.2)\nax.set_title('ROC curves for Adaboost Classifier and {} folds'.format(n_splits))\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nAn Adaboost model using Random Forest as the base estimator performs benign/malignant classifications with high TP rates and low FP rates. \nThere could be some cases where the classification performs worse, as happens in ROC fold 2 of the above ROC curves, but nevertheless, the AUC in this case is 0.96 even being the worst fold.\nBesides of that, the score of this method is 0.974, so there is room for further improvements.\n\n\n### Future Works\n\nFuture works shall be focused on trying other classifiers which could increase the current score of Adaboost model.\nFor instance, the following algorithms are serious candidates, apart from others not included in the list:\n\n- Extra Trees\n- Bagging\n- Ridge\n- Feel free to try others!\n\n\n### Suggestions\n\nDo not hesitate to suggest any correction, modification or enhancement of the analysis herein described!\nLots of thanks.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}