{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. 讀取 spam.csv\n\n* 定義 `readData_rawSMS` function\n    * header\n        * `0`: 第一列(橫)為欄位名稱\n            > 即 v1, v2\n        * `1`: 第二列(橫)為欄位名稱\n            > 即 ham, Go until jurong point, crazy.. Available only ...\n        * `None`: 本資料(spam.csv)沒有欄位名稱\n    * usecols\n        * `[0,1]`: 僅使用第一行(直)和第二行(直)的資料，其他行(直)略過不讀取且不使用。\n    * data_rawSMS.columns = ['label', 'content']\n        > 重新命名欄位名稱：由 `v1, v2` 改為 `label, content`"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def readData_rawSMS(filepath):\n    data_rawSMS = pd.read_csv(filepath, header=0, usecols=[0,1], encoding='latin-1') #\n    data_rawSMS.columns = ['label', 'content']\n    return data_rawSMS\n\ndata_rawSMS = readData_rawSMS(os.path.join(dirname, filename))\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. 資料集拆成 Train 和 Test\n## **(偷懶版) 直接使用 random 切割**\n```\n* 首先，讓每筆資料隨機產生介於 0 ~ 1 的數字。\n* 接著，數字 >= 0.5 當作 Training Data；其他 (數字 < 0.5 ) 則為 Testing Data。\n    > 此方法將導致 Training Data 和 Testing Data 的比例 1:1\n```\n* 如何知道 dataframe 的大小？使用 `shape`\n    * `data_rawSMS.shape` 顯示 (rows,columns) -> (5572,2)\n* `np.random.rand(n)`\n    * Random values in a given shape.\n        > Ex: [0.60025928 0.18572491 0.90311005 ... 0.57453736 0.37580751 0.57922529]\n        > https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html\n* `np.where(tmp_train == True)`\n    * Return elements chosen from x or y depending on condition. (回傳符合條件的 index)\n        > https://numpy.org/doc/stable/reference/generated/numpy.where.html\n* `data_rawSMS.iloc[index]`\n    * Purely integer-location based indexing for selection by position. (根據 index 取 rows)\n        > https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\n\n## **(正規版) Cross-validation**\n* 考慮「Spam」和「Ham」的分佈"},{"metadata":{"trusted":true},"cell_type":"code","source":"### 偷懶版 ###\n\ndef Separate_TrainAndTest(data_rawSMS):\n    n = data_rawSMS.shape[0]\n    tmp_train = (np.random.rand(n) >= 0.5)\n    # print(np.random.rand(n), tmp_train)\n    return data_rawSMS.iloc[np.where(tmp_train == True)[0]], data_rawSMS.iloc[np.where(tmp_train == False)[0]]\n\ndata_rawTrain, data_rawTest = Separate_TrainAndTest(data_rawSMS)\nprint(data_rawTrain)\nprint(data_rawTrain[data_rawTrain['label']=='ham'].shape[0])\nprint(data_rawTest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. 手工取特徵 (特徵將作為後續分類使用)\n* 從 Training Data 計算哪些「詞」重要。\n* generate_key_list function\n    * size_table\n        * 要選多少個重要的「詞」出來，等於決定特徵向量的維度。\n        * Default: 200 words\n    * ignore\n        * 英文字，字長少於幾個以下就不要算。\n        * `I`: 1個字\n        * `no`: 2個字\n        * Default: 3個字"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom collections import defaultdict\n# 垃圾訊息(spam)\n# 有效訊息(ham)\n\ndef generate_key_list(data_rawTrain, size_table=200, word_len_ignored=3):\n    dict_spam_raw = defaultdict(lambda:0)\n    dict_ham_raw = defaultdict(lambda:0)\n    dict_IDF = defaultdict(lambda:0)\n\n    for i in range(data_rawTrain.shape[0]):                               #data_rawSMS\n        # Separate sentences with spaces\n        finds = re.findall('[A-Za-z]+', data_rawTrain.iloc[i].content)    #data_rawSMS\n        \n        if data_rawTrain.iloc[i].label == 'spam':                         #data_rawSMS\n            for find in finds:\n                if len(find) < word_len_ignored:  # word length < 3\n                    continue\n                else:\n                    find = find.lower() # 英文轉成小寫\n                    dict_spam_raw[find] += 1\n                    dict_ham_raw[find] += 0\n        else:\n            for find in finds:\n                if len(find) < word_len_ignored:\n                    continue\n                else:\n                    find = find.lower()\n                    dict_ham_raw[find] += 1\n                    dict_spam_raw[find] += 0\n        \n        word_set = set()\n        for find in finds:\n            if len(find) < word_len_ignored:\n                continue\n            else:\n                find = find.lower()\n                dict_IDF[find] += 1\n                word_set.add(find)\n        # print(dict_IDF, word_set)\n        \n    word_df = pd.DataFrame(list(zip(dict_ham_raw.keys(), dict_ham_raw.values(), dict_spam_raw.values(), dict_IDF.values())))\n    word_df.columns = ['keyword', 'ham', 'spam', 'IDF']\n    print(word_df)\n    \n    print('### Training Data ###')\n    print('TF(word) =', word_df['ham'][0], '/', data_rawTrain[data_rawTrain['label']=='ham'].shape[0], '= one_word_frequency / Label_ham_DocumentCounts')\n    word_df['ham'] = word_df['ham'].astype('float')/data_rawTrain[data_rawTrain['label']=='ham'].shape[0]\n    print('TF(word) =', word_df['spam'][0], '/', data_rawTrain[data_rawTrain['label']=='spam'].shape[0], '= one_word_frequency / Label_spam_DocumentCounts')\n    word_df['spam'] = word_df['spam'].astype('float')/data_rawTrain[data_rawTrain['label']=='spam'].shape[0]\n    \n    print('\\nTrainingData_word_count:', word_df.shape[0])\n    TrainingData_word_count = word_df.shape[0]\n    print('IDF(word) = log(', TrainingData_word_count, '/', word_df['IDF'][0], ') = log( TrainingData_total_WordCount / one_word_count(ham+spam) )')\n    word_df['IDF'] = np.log10(TrainingData_word_count/word_df['IDF'].astype('float'))\n    \n    \n    print('\\n### score = TF * IDF ###')\n    word_df['ham_score'] = word_df['ham'] * word_df['IDF']\n    word_df['spam_score'] = word_df['spam'] * word_df['IDF']\n    word_df['diff'] = word_df['spam_score'] - word_df['ham_score'] # spam - ham 值越大，代表該字越常出現在 spam(垃圾訊息) 裡。\n    print(word_df)\n    \n    selected_spam_key = word_df.sort_values('diff', ascending=False)\n    # print(selected_spam_key)\n    \n    keyword_dict = dict()\n    i = 0\n    for word in selected_spam_key.head(size_table).keyword:\n        keyword_dict.update({word.strip():i})\n        i+=1\n    print(keyword_dict, len(keyword_dict))\n    return keyword_dict\n\n# build a tabu list based on the training data\n# size_table: how many features are used to classify spam\n# word_len_ignored: ignore those words shorter than this variable\nkeyword_dict = generate_key_list(data_rawTrain, size_table=300, word_len_ignored=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Train 和 Test 轉為特徵向量"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_Content(content, keyword_dict):\n\tm = len(keyword_dict)\n\tres = np.int_(np.zeros(m))\n\tfinds = re.findall('[A-Za-z]+', content)\n\tfor find in finds:\n\t\tfind = find.lower()\n\t\ttry:\n\t\t\ti = keyword_dict[find]\n\t\t\tres[i] = 1\n\t\texcept:\n\t\t\tcontinue\n\treturn res\n\ndef raw2feature(data_rawTrain, data_rawTest, keyword_dict):\n    n_train = data_rawTrain.shape[0]\n    n_test = data_rawTest.shape[0]\n    m = len(keyword_dict)\n    \n    print(n_train, n_test, m)\n    X_train = np.zeros((n_train, m))\n    X_test = np.zeros((n_test, m))\n    \n    Y_train = np.int_(data_rawTrain.label=='spam')\n    print(Y_train)\n    Y_test = np.int_(data_rawTest.label=='spam')\n    \n    for i in range(n_train):\n        X_train[i,:] = convert_Content(data_rawTrain.iloc[i].content, keyword_dict)\n    for i in range(n_test):\n        X_test[i,:] = convert_Content(data_rawTest.iloc[i].content, keyword_dict)\n        \n    return [X_train, Y_train], [X_test, Y_test]\n     \nTrain, Test = raw2feature(data_rawTrain, data_rawTest, keyword_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. 依據特徵資料訓練分類器\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nRandom_forest = RandomForestClassifier(n_estimators=50)\nX_train = Train[0]\ny_train = Train[1]\nRandom_forest.fit(X_train, y_train)\n\n\nX_test = Test[0]\ny_test = Test[1]\nrandomForest_predict = Random_forest.predict(X_test)\nrandomForest_score = metrics.accuracy_score(y_test, randomForest_predict)\nprint(\"(Testing) Random Forest Score :\", randomForest_score)\n\nY_hat = Random_forest.predict(X_test)\nn = np.size(y_test)\nprint('Testing Accuarcy: {:.6f}％ ({})'.format(sum(np.int_(Y_hat==y_test))*100./n, Random_forest.__module__))\n\n\nn=np.size(Train[1])\nY_hat_RF = Random_forest.predict(X_train)\nprint('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==Train[1]))*100./n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictSMS(SMS,model,keyword_dict):\n    X = convert_Content(SMS, keyword_dict)\n    Y_hat = model.predict(X.reshape(1,-1))\n    if int(Y_hat) == 1:\n        print ('SPAM: {}'.format(SMS))\n    else:\n        print ('ham: {}'.format(SMS))    \n\ninputstr='go to visit www.yahoo.com.tw, Buy one get one free, Hurry!'\npredictSMS(inputstr, Random_forest, keyword_dict)\n\ninputstr=('Call back for anytime.')\npredictSMS(inputstr, Random_forest, keyword_dict)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}