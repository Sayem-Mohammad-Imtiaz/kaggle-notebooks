{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Keras Tutorial - Text Generation using RNN (Part 7)\n\n**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Network without getting into intrinsic details of the Network. It can use Tensorflow or Theano as backend. This tutorial series will cover Keras from beginner to intermediate level.\n\nYOU CAN CHECK OUT REST OF THE TUTORIALS OF THIS SERIES.\n\n[PART 1](https://www.kaggle.com/akashkr/tf-keras-tutorial-neural-network-part-1)<br>\n[PART 2](https://www.kaggle.com/akashkr/tf-keras-tutorial-cnn-part-2)<br>\n[PART 3](https://www.kaggle.com/akashkr/tf-keras-tutorial-binary-classification-part-3)<br>\n[PART 4](https://www.kaggle.com/akashkr/tf-keras-tutorial-pretrained-models-part-4)<br>\n\n<font color=red>IF YOU HAVEN'T GONE THROUGH THE PREVIOUS PART OF THIS TUTORIAL, IT'S RECOMMENDED FOR YOU TO GO THROUGH THAT FIRST.</font><br>\n[PART 5](https://www.kaggle.com/akashkr/tf-keras-tutorial-basics-of-nlp-part-5)<br>\n[PART 6](https://www.kaggle.com/akashkr/tf-keras-tutorial-bi-lstm-conv1d-gru-part-6)\n\nIn the previous notebooks we worked on text classification. Let's see how to generate text data.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_path = '../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview of Dataset\n\nSince the generation of text data takes a lot of time, we will use smaller sample to test our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(data_path).head(500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will be using only **Review Text** column to generate the text. Rest of the columns are resundant as of now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of data: {df.shape}')\n# Find the number of missing values\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\nThe preprocssing in this type of model will be quite different. Let's see how we are going to make the data.\nLet's take a sentence-\n    `Wow, this dress is quite wonderful.`\n    \n1. Tokenize this sentence\n\n    `[23, 5 ,6, 34, 11, 21]` - suppose this is the tokenized sentence. Lets assume that this was the longest sentence in our corpus\n2. slash into n-gram and append - We convert one sentence like this\n\n    `[23, 5]\n    [23, 5, 6]\n    [23, 5 ,6, 34]\n    [23, 5 ,6, 34, 11]\n    [23, 5 ,6, 34, 11, 21]`\n    \n3. Pad sequence - we pre pad all the sequence with zeros making the length of each sequence equal to the longest senquence in the corpus (Which we have assumed to be this sentence)\n\n    `[0, 0, 0, 0, 23, 5]\n    [0, 0, 0, 23, 5, 6]\n    [0, 0, 23, 5 ,6, 34]\n    [0, 23, 5 ,6, 34, 11]\n    [23, 5 ,6, 34, 11, 21]`\n    \n4. Split into X and Y - We split the data to make the last column as target variable and rest as feature.\n    \n    X<br>\n    `[0, 0, 0, 0, 23]\n    [0, 0, 0, 23, 5]\n    [0, 0, 23, 5 ,6]\n    [0, 23, 5 ,6, 34]\n    [23, 5 ,6, 34, 11]`\n    \n    Y<br>\n    `[5]\n    [6]\n    [34]\n    [11]\n    [21]`\n    \n5. Convert Y to one hot encoding\n\nY<br>\n    `[0, 0, 0, 0, 1, 0 ...]\n    [0, 0, 0, 1, 0, 0 ...]\n    [1, 0, 0, 0, 0, 0 ...]\n    [0, 1, 0, 0, 0, 0 ...]\n    [0, 0, 0, 0, 0, 1 ...]`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Review Text'].astype(str).str.lower())\n\ntotal_words = len(tokenizer.word_index)+1\ntokenized_sentences = tokenizer.texts_to_sequences(df['Review Text'].astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slash sequences into n gram sequence\ninput_sequences = list()\nfor i in tokenized_sentences:\n    for t in range(1, len(i)):\n        n_gram_sequence = i[:t+1]\n        input_sequences.append(n_gram_sequence)\n        \n# Pre pad with max sequence length\nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create predictors and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nEnable GPU for faster computation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(250)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nearlystop = EarlyStopping(monitor='loss', min_delta=0, patience=4, verbose=0, mode='auto')\nhistory = model.fit(xs, ys, epochs=50, verbose=1, callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()\n    \nplot_graphs(history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\nNow we will take a seed text, encode it and predict the next word. Then take the next word into the seed and predict the next ... for some number of words.. say 40.\nNote that gradually the accuracy will start decreasing since we already take those words into seed which is predicted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def complete_this_paragraph(seed_text, next_words):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return seed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_this_paragraph(\"this is a good\", 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_this_paragraph(\"i loved that dress\", 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_this_paragraph(\"This shirt is so\", 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voila! Here it is. It does make some sense here. You can increase the training data size to see better results.\n### Like and upvoteüëç","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}