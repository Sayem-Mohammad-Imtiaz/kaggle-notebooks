{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Discussion 8 Logistic Regression\n\n#### by Luke Wiebolt"},{"metadata":{},"cell_type":"markdown","source":"One of my biggest weaknesses is that I have food allergies. To find out what I'm allergic to I have been on a food elimination diet for the past 6 months. This means that I can't eat food that contains dairy, seafood, wheat, soy, nuts, and eggs. This cuts out a large portion of foods, however I manage. One thing I miss is chocolate and candy. I'm able to eat some candy like Sour Patch Kids, and can't eat others like chocolate, from the milk and dairy. Let's say I wanted to build a model to predict the likelihood of a candy being chocolate based on a variety of input variables. Previously, I have guessed if the piece of candy is chocolate or not, so we will benchmark the model against a 50% accuracy."},{"metadata":{},"cell_type":"markdown","source":"How would you know if the output of your logistic regression model is valid?\n\nWhat are the key statistics to determine the validity of your regression model?"},{"metadata":{},"cell_type":"markdown","source":"### Instantiate Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/the-ultimate-halloween-candy-power-ranking/candy-data.csv')\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Data', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's drop the competitorname\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[['chocolate', 'fruity', 'caramel', 'peanutyalmondy', 'nougat', 'crispedricewafer',\n               'hard', 'bar', 'pluribus', 'sugarpercent', 'pricepercent', 'winpercent']]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.values[:, 1:12]\nY = df.values[:, 0]\n\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split (X, Y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(11,11));\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=.5, cmap = \"YlGnBu\");\nplt.xlabel('');\nplt.ylabel('');\nplt.title('Pearson Correlation matrix heatmap');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that our groups are similar\ndf['chocolate'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression Assumptions\n\n1. Assumes there is a linear relationship between any continuous predictors and the logit of the outcome variable, or target variable (Field, 2013). This can be checked with a scatter plot of the target variables and the predictor variables.\n2. Independence of Errors - this produces overdispersion (Field, 2013). The distribution of errors is random and not correlated or influenced by the errors in previous observations. The opposite of this would be autocorrelation. Checking for this is done by plotting the residuals sequentially as well as running a Durbin - Watson Test (Eliezer, 2008). "},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nlogit_model=sm.Logit(y_train, x_train)\nresult=logit_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After running the first time I removed the variables Hard, CrispyRiceWafer, and Bar from my model as these were making it so that my model was failing to converge. Looking at the variable x4 (nougat) the coefficient is extremely high and the p-value is nearly 1.0 this is clearly an issue so we will remove this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_slim = data[['chocolate', 'fruity', 'caramel', 'peanutyalmondy',\n                'hard','pluribus', 'sugarpercent', 'pricepercent', 'winpercent']]\n\nX_s = df_slim.values[:, 1:9]\nY_s = df_slim.values[:, 0]\n\nx_train_1, x_test_1, y_train_1, y_test_1 = train_test_split (X_s, Y_s, test_size = 0.2, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nlogit_model=sm.Logit(y_train_1, x_train_1)\nresult=logit_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = data[['chocolate', 'fruity', 'pluribus', 'winpercent']]\n\nX_f = df_final.values[:, 1:4]\nY_f = df_final.values[:, 0]\n\nx_train_2, x_test_2, y_train_2, y_test_2 = train_test_split (X_f, Y_f, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nlogit_model=sm.Logit(y_train_2, x_train_2)\nresult=logit_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Below is a way we can test to see if our model valid based on the output and various key statistics "},{"metadata":{},"cell_type":"markdown","source":"The output above gives us a wide range of statistics and some that are similar when creating a linear regression like the coeficient table, even though the overall equation itself is different. The Log-likelihood is based on summing the probabilities associated with the predicted and actual outomes. This indicates how much unezplated information there is in the model after fitting, the larger the value the more un explained observations. \n\nPseudo R-Squared is similar to what is used in linear regression however it is not the same. The approach of goodness of fit does not apply in this case as we are maximizing our likelihood of each predictor to our target variable. The pseudo r squres have been created as a way to measure on a similar scale of 0 to 1. More research should be done on these before using or reporting\nhttps://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train_2, y_train_2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We let our logistic regression model predict what it thinks the value of the x_test training set will be (y_pred) and we measure the accuracy. These are the 17 records that we seperated when creating a train and test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(x_test_2)\nprint('Accuracy of logistic regression  classifier on test set: {:.2f}'.format(logreg.score(x_test_2, y_test_2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix')\nprint(confusion_matrix)\nprint('This means we have 8 + 7 = 15 correct predictions')\nprint('and')\nprint('This means we have 1 + 1 = 2 incorrect predictions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From our confusion matrix we conclude that:\n* True positive: 8(We predicted a positive result and it was positive)\n* True negative: 7(We predicted a negative result and it was negative)\n* False positive: 1(We predicted a positive result and it was negative)\n* False negative: 1(We predicted a negative result and it was positive)\n* Accuracy = (TP+TN)/total\n* Accuracy = (8+7)/17 ~ 88%\n* Error Rate = (FP+FN)/total\n* Error rate = (1+1)/17 ~12%"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\nprint('read more from the documentation on each of these metrics')\nprint('https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test_2, logreg.predict(x_test_2))\nfpr, tpr, thresholds = roc_curve(y_test_2, logreg.predict_proba(x_test_2)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Our ROC curve is showing that our model is performing well. The dotted line is a completely random classifier and our blue line is our logistic regression, it is a good thing when our line is to the far left of the dotted line. However, remember that this is so accurate due to a small sample size and may have a problem of overfitting (Li ,2017)."},{"metadata":{},"cell_type":"markdown","source":"Our model implies that fruity, pluribus, and winpercent are variables that would be more indicitive in selecting a chocolate or non-chocolate candy. The model shows that if it is fruity it is more likely to not be chocolate, if it is pluribus or if it comes in multiple packs it less likely to be chocolate, and if it has a higher win percent it is more likely to be chocolate.\n\nOne of the worries here is that our dataset is too small. If this model were ever to be used further it would need to be tested against a larger and more robust dataset. There are ways this can be expanded on by simply comparing the two models side by side and assessing both of their accuracy. In the future I will touch this up and see what else I can add. Thanks for following along!"},{"metadata":{},"cell_type":"markdown","source":"### References\n\nEliezer (2008) Lecture 8 - Residual Analysis - Checking Independence of Errors. Mar 4th, 2008. GSB420 Business Statistics. Retrieved From http://gsb420.blogspot.com/2008/03/lecture-8-residual-analysis-checking_04.html\n\nField, A (2013) Discovering Statistics using IBM SPSS Statistics. Sage Publications Ltd. 4th Edition.\nISBN 978-1-4462-4917-8\n\nLi, S (2017) Building A Logistic Regression in Python, Step by Step. Towards Data Science. Sep 28th, 2017. Retrieved From https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n \nWalia, A (2019) Logistic Regression in Python. Medium.  Mar 9th, 2019. Retrieved From\nhttps://medium.com/@anishsingh20/logistic-regression-in-python-423c8d32838b\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}