{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"05b2d269-c7ac-24a3-5e86-c098dfc64782"},"source":"# Glass type classification with machine learning"},{"cell_type":"markdown","metadata":{"_cell_guid":"4eb01345-ed8a-3bfb-5de4-8e0cbc1c9f43"},"source":"I'm a data science newbie and this is my first Kaggle notebook. Here's my plan of attack for the glass classification problem.\n\n# Contents\n\n## 1) Prepare Problem\n\n * Load libraries\n\n * Load and explore the shape of the dataset\n\n## 2) Summarize Data\n\n* Descriptive statistics\n\n* Data visualization\n\n## 3) Prepare Data\n\n* Data Cleaning\n\n* Split-out validation dataset\n\n*  Data transformation  \n\n## 4) Evaluate Algorithms\n\n* Dimensionality reduction\n\n* Compare Algorithms\n\n## 5) Improve Accuracy\n\n* Algorithm Tuning\n\n## 6) Diagnose the performance of the best algorithms\n\n* Diagnose overfitting by plotting the learning and validation curves\n* Further tuning\n\n## 7) Finalize Model\n\n* Create standalone model on entire training dataset\n\n* Predictions on test dataset"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae75a7ff-7be3-301b-fc17-c0feaf2a2690"},"source":"## 1. Prepare Problem"},{"cell_type":"markdown","metadata":{"_cell_guid":"219b832e-5873-b374-3b13-e9024940cc2e"},"source":"### Loading the libraries "},{"cell_type":"markdown","metadata":{"_cell_guid":"6c7ec43b-0101-fb94-0e45-0a1a15f48782"},"source":"Let us first begin by loading the libraries that we'll use in the notebook"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5108bded-bfae-ea87-1fc2-f87051462b79"},"outputs":[],"source":"import numpy as np  # linear algebra\nimport pandas as pd  # read dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.base import TransformerMixin # To create new classes for transformations\nfrom sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.stats import boxcox # data transform\nfrom sklearn.model_selection import (train_test_split, KFold , StratifiedKFold,\n                                     cross_val_score, GridSearchCV ) # model selection modules\nfrom sklearn.pipeline import Pipeline # streaming pipelines\nfrom sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\nfrom collections import Counter\nimport warnings\n# load models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import (XGBClassifier, plot_importance)\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom time import time\n%matplotlib inline "},{"cell_type":"markdown","metadata":{"_cell_guid":"ace42ae9-4dcb-4246-df8b-61d272b774e2"},"source":"### Loading and exploring the shape of the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78b7ead5-7457-2862-d531-e8a714f659d5"},"outputs":[],"source":"warnings.filterwarnings('ignore')\ndf = pd.read_csv('../input/glass.csv')\nfeatures = df.columns[:-1].tolist()\nprint(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c26f2bcd-7712-dc4a-4aff-84b927494bee"},"source":"The dataset consists of 214 observations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2efc020c-5616-328f-a9e4-1a146caa4aa3"},"outputs":[],"source":"df.head(15)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55802dc9-e552-4b6b-b697-d6aaaba3e4b9"},"outputs":[],"source":"df.dtypes"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ac1e4a7-60a8-3641-57f9-98ffe8938907"},"source":"## 2. Summarize data"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e5400e7-da03-71ce-1ecb-e7c914251522"},"source":"### Descriptive statistics"},{"cell_type":"markdown","metadata":{"_cell_guid":"646eac5f-e512-0a07-de93-c9fcccb41082"},"source":"Let's first summarize the distribution of the numerical variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b27e57c-0779-c34d-777f-b2b067edcfcf"},"outputs":[],"source":"df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa036d73-7f87-697f-7c96-1dc1f58befb7"},"source":"The features are not on the same scale. For example Si has a mean of 72.65 while Fe has a mean value of 0.057. Features should be on the same scale for an algorithm such as logistic regression (gradient descent) to converge fast. Let's go ahead and check the distribution of the glass types."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"515037ee-2c4d-4d40-ca6d-aca001cfc8f9"},"outputs":[],"source":"df['Type'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f90a223e-3d2d-197d-6dc9-7b347489d87a"},"source":"The dataset is pretty unbalanced. The instances of types 1 and 2 constitute more than 67 % of the glass types."},{"cell_type":"markdown","metadata":{"_cell_guid":"9d171ec0-f8e1-ec93-f298-4c4c1233d3fc"},"source":"###  Data Visualization"},{"cell_type":"markdown","metadata":{"_cell_guid":"69e7d5f0-eabe-0233-5a9b-88edc025e3d4"},"source":"* **Univariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5dcff8b-73f9-7800-a8a2-7582674ef980"},"source":"Let's go ahead an look at the distribution of the different features of this dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7713d993-98e5-73ae-93c2-205625882995"},"outputs":[],"source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], label='Skew = %.3f' %(skew))\n    plt.legend(loc='best')\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd5a38f8-d1d7-fc87-2c54-67638dd5bc2a"},"source":"None of the features is normally distributed. The features Fe, Ba, Ca and K exhibit the highest skew coefficients. Moreover, the distribution of potassium (K) and Barium (Ba) seem to contain many outliers.\nLet's identify the indices of the observations containing outliers using [Turkey's method](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/).\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6da6e550-1c86-d299-6161-b3645b3dc30c"},"outputs":[],"source":"# Detect observations with more than one outlier\n\ndef outlier_hunt(df):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than 1 outlier. \n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in df.columns.tolist():\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        \n        # Interquartile rrange (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 1 )\n    \n    return multiple_outliers   \n\nprint('The dataset contains %d observations with multiple outliers' %(len(outlier_hunt(df[features]))))   "},{"cell_type":"markdown","metadata":{"_cell_guid":"571464cd-c861-714d-c014-1fd31a6dbdb1"},"source":"Aha! there exists some 35 observations with multiple outliers.  These  could harm the efficiency of our learning algorithms. We'll make sure to get rid of these in the next sections.\n\nLet's examine the boxplots for the several distributions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f41504ec-e040-7e5c-4fb4-4ffbcf28c3b7"},"outputs":[],"source":"sns.boxplot(df[features])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3aa1de24-14d5-4d06-f6f6-cfca73071592"},"source":"Unsurprisingly, Silicon has a mean that is much superior to the other constituents as we already saw in the previous section. Well, that is normal since glass is mainly based on silica."},{"cell_type":"markdown","metadata":{"_cell_guid":"9a802551-24cc-1d1d-9451-47f57a81947e"},"source":"* **Multivariate plots**"},{"cell_type":"markdown","metadata":{"_cell_guid":"193f3d80-ab7c-bd10-2e01-1694a4d5b3a3"},"source":"Let's now proceed by drawing a pairplot to visually examine the correlation between the features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de8796c8-76b9-581f-1d1e-9e8a8ce8646a"},"outputs":[],"source":"plt.figure(figsize=(8,8))\nsns.pairplot(df[features],palette='coolwarm')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"efd5c18c-c764-5914-5cb8-b42ca725c80e"},"source":"Let's go ahead and examine a heatmap of the correlations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6356d9c-0287-b7d4-a59b-2c510633a9ed"},"outputs":[],"source":"corr = df[features].corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features,\n           cmap= 'coolwarm')\nplt.show()\nprint(corr)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e380fa58-3e3e-5bda-0833-9309d0da6520"},"source":"There seems to be a strong positive correlation between RI and Ca. This could be a hint to perform Principal component analysis in order to decorrelate some of the input features."},{"cell_type":"markdown","metadata":{"_cell_guid":"d6c716ee-31a3-00f1-fd33-1d4562024318"},"source":"## 3. Prepare data"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c0820df-fe56-d78b-f21d-658608d6810a"},"source":"### - Data cleaning "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0b463a3-9997-212a-277c-1383a3aaa6e1"},"outputs":[],"source":"df.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"72563242-d055-8451-f33d-89fe57d64df0"},"source":"This dataset is clean; there aren't any missing values in it."},{"cell_type":"markdown","metadata":{"_cell_guid":"4a0ee8da-20d9-fead-b65c-e2fd3139d281"},"source":"### - Hunting and removing multiple outliers\n\nLet's remove the observations containing multiple outliers with the function we created in the previous section."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa7077cd-b938-c4e2-2f9b-41055ddae9b8"},"outputs":[],"source":"outlier_indices = outlier_hunt(df[features])\ndf = df.drop(outlier_indices).reset_index(drop=True)\nprint(df.shape)\nprint(df.tail())"},{"cell_type":"markdown","metadata":{"_cell_guid":"d65f3c11-12f0-2528-a8ae-976ccf22a856"},"source":"Removing observations with multiple outliers leaves us with 179 observations to learn from. Not that much! Let's now see how our distributions look like."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"618331c3-804a-aa64-151e-b57897cc6c91"},"outputs":[],"source":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], label='Skew = %.3f' %(skew))\n    plt.legend(loc='best')\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64355be4-9cae-735f-3e19-b1720c4364e3"},"outputs":[],"source":"df['Type'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9ba8bc01-836e-4abb-7d30-1ad38ebaa561"},"source":"### - Split-out validation dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1633837b-e058-8a40-22db-22a79d870245"},"outputs":[],"source":"# Define X as features and y as lablels\nX = df[features] \ny = df['Type'] \n# set a seed and a test size for splitting the dataset \nseed = 7\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f13a10d9-e30a-4940-cf2d-276c2f40274d"},"source":"### - Data transformation  "},{"cell_type":"markdown","metadata":{"_cell_guid":"7d9cd04d-a413-0a90-86a7-4219be1e5b7f"},"source":"Let's examine if a Box-Cox transform can contribute to the normalization of some features. It should be emphasized that all transformations should only be done on the training set to avoid data snooping. Otherwise the test error estimation will be biased."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0eb56c0-ed10-415f-4da4-8c9017bacdd1"},"outputs":[],"source":"features_boxcox = []\n\nfor feature in features:\n    bc_transformed, _ = boxcox(df[feature]+1)  # shift by 1 to avoid computing log of negative values\n    features_boxcox.append(bc_transformed)\n\nfeatures_boxcox = np.column_stack(features_boxcox)\ndf_bc = pd.DataFrame(data=features_boxcox, columns=features)\ndf_bc['Type'] = df['Type']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61be6a71-8da4-0d4b-a776-a7d9bd058124"},"outputs":[],"source":"df_bc.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d6583ae-a6e3-bba8-1b07-d61d55b5d5a3"},"outputs":[],"source":"for feature in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(df[feature], color='blue', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df[feature].skew(),3))) )\n    ax[0].set_title(str(feature))   \n    ax[0].legend(loc=0)\n    ax[1].hist(df_bc[feature], color='red', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df_bc[feature].skew(),3))) )\n    ax[1].set_title(str(feature)+' after a Box-Cox transformation')\n    ax[1].legend(loc=0)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58dfa4e3-f842-66ce-a98e-8c218060a935"},"outputs":[],"source":"# check if skew is closer to zero after a box-cox transform\nfor feature in features:\n    delta = np.abs( df_bc[feature].skew() / df[feature].skew() )\n    if delta < 1.0 :\n        print('Feature %s is less skewed after a Box-Cox transform' %(feature))\n    else:\n        print('Feature %s is more skewed after a Box-Cox transform'  %(feature))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b91d19e-75c6-1853-4561-0edebfa9ead4"},"source":"The Box-Cox transform seems to do a good job in reducing the skews of the different distributions of features.  However, it does not lead to the normalization of the feature distributions.  Next, let's explore dimensionality reduction techniques."},{"cell_type":"markdown","metadata":{"_cell_guid":"1ecd4339-f5b3-b5e3-2b65-78c3d35b0dee"},"source":"## 4. Evaluate Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"0765edc5-5eaa-a03f-5dd8-5f9471a0c3c7"},"source":"### - Dimensionality reduction"},{"cell_type":"markdown","metadata":{"_cell_guid":"99b123a3-2068-3639-7547-8326fcbd5d5c"},"source":"* **XGBoost**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24bad968-7341-4fea-0d3e-b8f851fb7f21"},"outputs":[],"source":"model_importances = XGBClassifier()\nstart = time()\nmodel_importances.fit(X_train, y_train)\nprint('Elapsed time to train Random Forests %.3f seconds' %(time()-start))\nplot_importance(model_importances)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6829d607-159d-8509-69c5-41cd6589355b"},"source":"It appears that no main features dominate the importance in the XGBoost modeling of the problem. Also, XGBoost seems to take a lot of time to train in the Kaggle kernel; it might be badly configured. For the rest of this notebook, we will only make use of scikit-learn models."},{"cell_type":"markdown","metadata":{"_cell_guid":"b3f0438c-534e-9271-d8eb-fa0e21a6d883"},"source":"* **PCA**"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd92f517-7e97-ac27-737d-3149299db9dc"},"source":"Let's go ahead and perform a PCA on the features to decorrelate the ones that are linearly dependent and then let's plot the cumulative explained variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edc50398-249b-cd5e-b743-71384cae5e40"},"outputs":[],"source":"pca = PCA(random_state = seed)\npca.fit(X_train)\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\nplt.figure(figsize=(8,6))\nplt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n       alpha = 0.7)\nplt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n        color= 'red')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xticks(np.arange(1,len(var_exp)+1,1))\nplt.legend(loc='center right')\nplt.show()\n\n# Cumulative variance explained\nfor i, sum in enumerate(cum_var_exp):\n    print(\"PC\" + str(i+1), \"Cumulative variance: %.3f% %\" %(cum_var_exp[i]*100))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f1c2d6a-96f0-0670-b0d6-9aaf14ad59c7"},"source":"It appears that about 99 % of the variance can be explained with the first 5 principal components. However feeding the PCA features to the learning algorithms did not contribute to a better performance. This might be because PCA is a linear method."},{"cell_type":"markdown","metadata":{"_cell_guid":"5daf9b7e-abac-6a02-8bfb-972dcf70ca29"},"source":"### - Compare Algorithms"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f8f3969-57b0-fa78-1ea2-2e3677d64eba"},"source":"Now it's time to compare the performance of different machine learning algorithms. We'll use 10-folds cross-validation to assess the performance of each model with the metric being the classification accuracy. Pipelines encompassing a Box-Cox transformation, Standarization  are used in order to avoid data leakage."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80407180-6aa2-727f-5ec8-9d8613ca1d60"},"outputs":[],"source":"class Box_Cox(BaseEstimator,TransformerMixin):\n    \"\"\"\n    Box-Cox transformation estimator:\n    Takes a feature vector X of numerical distributions\n    and performs a box-cox transformation to each feature.\n    \"\"\"\n    def __init__(self):\n        self.lbds = {}\n    \n    def fit(self,X, *args):        \n        features = X.columns.tolist()\n        for feature in features:\n            # Skip Silicon\n            #if feature == 'Si':   continue\n            feat_transf, lmbda = boxcox(X[feature]+1)\n            self.lbds[feature] = lmbda\n        return self\n    \n    def transform(self,X, *args):\n        features = X.columns.tolist()       \n        for feature in features:\n            # Skip Silicon\n            #if feature == 'Si':   continue\n            if feature != 'Si':\n                X[feature] = X[feature].apply(\\\n                lambda x: ((x+1)**(self.lbds[feature]) -1.0)/self.lbds[feature] if self.lbds[feature] != 0 \\\n                                              else np.log(x+1) )\n        return X"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b22729c-2fc9-ef2a-7d1f-d069edec987d"},"outputs":[],"source":"n_components = 5\npipelines = []\n\n#print(df.shape)\npipelines.append( ('SVC',\n                   Pipeline([ \n                               ('BC', Box_Cox()),\n                              ('sc', StandardScaler()),\n                             #('pca', PCA(n_components = n_components, random_state=seed ) ),\n                             ('SVC', SVC(random_state=seed))]) ) )\n\n\npipelines.append(('KNN',\n                  Pipeline([ ('BC', Box_Cox()),\n                              ('sc', StandardScaler()),\n                          #  ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                            ('KNN', KNeighborsClassifier()) ])))\npipelines.append( ('RF',\n                   Pipeline([('BC', Box_Cox()),\n                            #   ('sc', StandardScaler()),\n                            # ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('RF', RandomForestClassifier(random_state=seed)) ]) ))\n\npipelines.append( ('GNB',\n                   Pipeline([ ('BC', Box_Cox()),\n                              ('sc', StandardScaler()),\n                            # ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('GNB', GaussianNB()) ]) ))\n\ntree = DecisionTreeClassifier(max_depth=3)\npipelines.append( ('Ada',\n                   Pipeline([ ('BC', Box_Cox()),\n                             # ('sc', StandardScaler()),\n                            # ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                    ('Ada', AdaBoostClassifier(base_estimator=tree,random_state=seed)) ]) ))\n\npipelines.append( ('ET',\n                   Pipeline([('BC', Box_Cox()),\n                              #('sc', StandardScaler()),\n                            # ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('ET', ExtraTreesClassifier(random_state=seed)) ]) ))\npipelines.append( ('GB',\n                   Pipeline([ ('BC', Box_Cox()),\n                           #   ('sc', StandardScaler()),\n                           #  ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n\npipelines.append( ('LR',\n                   Pipeline([ ('BC', Box_Cox()),\n                              ('sc', StandardScaler()),\n                         #    ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n\nresults, names, times  = [], [] , []\nnum_folds = 10\nscoring = 'accuracy'\n\nfor name, model in pipelines:\n    start = time()\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n                                n_jobs=-1) \n    t_elapsed = time() - start\n    results.append(cv_results)\n    names.append(name)\n    times.append(t_elapsed)\n    msg = \"%s: %f (+/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n                                                       100*cv_results.std(), t_elapsed)\n    print(msg)\n\n   \nfig = plt.figure(figsize=(12,8))    \nfig.suptitle(\"Algorithms comparison\")\nax = fig.add_subplot(1,1,1)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a44e7822-1a6b-1aeb-919c-e26e7ed24cda"},"source":"\n**Observation:** The best performances are achieved by SVC, RF, KNN, ET and GB .However, these algorithms also yield a wide distribution. It is worthy to continue our study by tuning these  algorithms. \n\n Gaussian Naive Bayes and Logistic Regression perform badly. This might be due to the fact that the data is not normally distributed as these algorithms perform well when data that is normally distributed."},{"cell_type":"markdown","metadata":{"_cell_guid":"5af8c34c-cefb-734e-2b6a-e13ab043a93c"},"source":"## 5. Algorithm tuning"},{"cell_type":"markdown","metadata":{"_cell_guid":"c30058df-e5f6-c3aa-c700-441550aba44b"},"source":"### Tuning the Support vector classifier\n\nLet's start by tuning the hyperparameters of the SVC Classifier. We will use three kernels for this purpose: \n\n* A Radial Basis Function Kernel\n\n* A Polynomial kernel"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e841efd8-faa5-514f-af2e-a152c2c65e81"},"outputs":[],"source":"# Create a pipeline with SVC\npipe_svc = Pipeline([ ('BC', Box_Cox()),\n                       ('scl', StandardScaler()), \n                     #('pca', PCA(n_components=n_components, random_state=seed)),\n                    ('svc', SVC(random_state=seed) )])\n\n# Set the parameter ranges\ngamma_param_range = [0.001, 0.01, 0.1, 1.0, 10.0]\nparam_range = [1.0,3.0,5.0,7.0,8.0]\ndegree_range = [4, 6, 8]\n\n# Set the grid parameters\nparam_grid_svc =  [\n    #{'svc__C': param_range,'svc__kernel': ['linear']},   # Linear kernel\n    {'svc__C': param_range,'svc__gamma': gamma_param_range,'svc__kernel': ['rbf']}, # radial basis function\n    {'svc__C': param_range, 'svc__degree': degree_range, 'svc__kernel':['poly']} #polynomial\n    ]\n# Use 10 fold CV\nkfold = StratifiedKFold(n_splits=num_folds, random_state= seed)\ngrid_svc = GridSearchCV(pipe_svc, param_grid= param_grid_svc, cv=kfold, scoring=scoring, \n                        n_jobs=-1)\n\n#Fit the pipeline\nstart = time()\ngrid_svc = grid_svc.fit(X_train, y_train)\nend = time()\n\nprint(\"SVC grid search took %.3f seconds\" %(end-start))\n\n# Best score and best parameters\nprint('-------Best score----------')\nprint(grid_svc.best_score_ * 100.0)\nprint('-------Best params----------')\nprint(grid_svc.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"26024d71-6455-0c8f-340b-2be082fe6006"},"source":"The best support vector estimator achieves a score of 74 %."},{"cell_type":"markdown","metadata":{"_cell_guid":"cc56bc26-0dc1-ff23-1f4d-1f4ab01d793f"},"source":"### Tuning Random Forests\n\nFor random forest, we can tune the number of grown trees (n_estimators), the trees' depth (max_depth), the criterion of splitting (gini or entropy) and so on.... Let's start tuning these."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f2ebd38-9e26-957f-9790-589ef0988dec"},"outputs":[],"source":"# Create a pipeline with a Random forest classifier\npipe_rfc = Pipeline([ ('BC', Box_Cox()),\n                   #   ('scl', StandardScaler()), \n                  #   ('pca', PCA(n_components=n_components, random_state=seed)),\n                    ('rfc', RandomForestClassifier(random_state=seed, n_jobs=-1) )])\n\n# Set the grid parameters\nparam_grid_rfc =  [ {\n    'rfc__n_estimators': [100,200,300], # number of estimators\n    #'rfc__criterion': ['gini', 'entropy'],   # Splitting criterion\n    'rfc__max_features':[0.05 , 0.1], # maximum features used at each split\n    'rfc__max_depth': [None, 5], # Max depth of the trees\n    'rfc__min_samples_split': [0.005, 0.01, 0.02 ], # mininal samples in leafs\n    }]\n# Use 10 fold CV\nkfold = StratifiedKFold(n_splits=num_folds, random_state= seed)\ngrid_rfc = GridSearchCV(pipe_rfc, param_grid= param_grid_rfc, cv=kfold, scoring=scoring, \n                      n_jobs=-1)\n\n#Fit the pipeline\nstart = time()\ngrid_rfc = grid_rfc.fit(X_train, y_train)\nend = time()\n\nprint(\"RFC grid search took %.3f seconds\" %(end-start))\n\n# Best score and best parameters\nprint('-------Best score----------')\nprint(grid_rfc.best_score_ * 100.0)\nprint('-------Best params----------')\nprint(grid_rfc.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a863efc-d65d-3212-2fd2-18d461b26ae9"},"source":"# TO BE CONTINUED ...."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9997cfd-b4d0-7cc5-f3b9-e897f3e6e7fc"},"outputs":[],"source":"#coding=utf-8\n###\n###this is my first kaggle code here ,please tell me if I am wrong\n###"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}