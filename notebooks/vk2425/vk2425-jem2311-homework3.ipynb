{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task 1: Identify Features"},{"metadata":{},"cell_type":"markdown","source":"Assemble a dataset consisting of features and target (for example in a dataframe or in two arrays X and y). What features are relevant for the prediction task?\nAre there any features that should be excluded because they leak the target information? Show visualizations or statistics to support your selection.\nYou are not required to use the description column, but you can try to come up with relevant features using it. Please don’t use bag-of-word approaches for now as we’ll discuss these later in the class."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_orig = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_orig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get subsample of data\ndf = df_orig.sample(frac=0.5,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Paint color correlation\nimport seaborn as sb\nplt.xticks(rotation=90)\nax = sb.scatterplot(x=\"paint_color\", y=\"price\", data=df_orig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Latitude correlation\nax = sb.scatterplot(x=\"lat\", y=\"price\", data=df_orig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Longitude correlation\nax = sb.scatterplot(x=\"long\", y=\"price\", data=df_orig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# State correlation\nplt.xticks(rotation=90)\nax = sb.scatterplot(x=\"state\", y=\"price\", data=df_orig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Region correlation\nax = sb.scatterplot(x=\"region\", y=\"price\", data=df_orig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We decided to cut several columns:\n* id/VIN: These columns may have an extremely strong correlation to price and the model may just learn to associate id/VIN and price. New values will throw off the model.\n* url: image_url, region_url: Text url's are not releveant for the prediciton of the car price.\n* model: There are too many unique values such that when preprocessed, there would not be any meaningful information.\n* size: There are too many null values.\n* title_status: Almost all (95%) of the cars had 'clean' title statuses.\n* paint_color : As seen in the plots above, there did not appear to be a meaningful correlation with price.\n* region, lat, long, county, state: We avoided all location data. As seen in the plots above, there is essentially no correlation for location and price.\n* description: This column leaks the target information, because often times price is included in description."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = df.drop(columns = ['id',\n                           'url', \n                           'region', \n                           'region_url',\n                           'title_status', \n                           'size', \n                           'description', \n                           'vin', \n                           'lat', \n                           'long', \n                           'image_url',\n                           'county',\n                           'state',\n                           'model',\n                           'paint_color'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Task 2 Preprocessing and Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"Create a simple minimum viable model by doing an initial selection of features, doing appropriate preprocessing and cross-validating a linear model. Feel free to exclude features or do simplified preprocessing for this task. As mentioned before, you don’t need to validate the model on the whole dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop where nan is present\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cut price outliers\ndf = df[df['price'] > 1000]\ndf = df[df['price'] < 40000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature names\ncontinuous_features = ['year', 'odometer']\ncategorical_features = ['drive', 'type', 'fuel', 'transmission', 'manufacturer', 'condition', 'cylinders']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X and y\ny = df['price']\nX = df.drop(columns=['price'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Minimum Preprocessing\ndef train_and_eval_classifier(classifier, cname):\n    categorical_transformer = Pipeline(steps=[\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', categorical_transformer, categorical_features)])\n    \n    pipe = make_pipeline(preprocessor, classifier)\n    scores = cross_val_score(pipe, X_train, y_train, scoring=\"r2\")\n    print(\"CV Score for {}: {}\".format(cname, np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Minimum viable models\n# Try a couple of basic classifiers:\ntrain_and_eval_classifier(LinearRegression(), \"LinearRegression\")\ntrain_and_eval_classifier(Lasso(max_iter=6000), \"Lasso\")\ntrain_and_eval_classifier(ElasticNet(), \"ElasticNet\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 3 Additional Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_orig.sample(frac=0.5,axis=0)\ndf = df.drop(columns = ['id',\n                   'url', \n                   'region', \n                   'region_url',\n                   'title_status', \n                   'size', \n                   'description', \n                   'vin', \n                   'lat', \n                   'long', \n                   'image_url',\n                   'county',\n                   'state',\n                   'model',\n                   'paint_color'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop only null rows instead of entirely dropping null values\n# We will fix null with imputation later\ndf = df.dropna(thresh=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cut continuous outliers on quantiles instead\n\npl = df.price.quantile(0.1)\npu = df.price.quantile(0.99)\n\nol = df.odometer.quantile(0.1)\nou = df.odometer.quantile(0.99)\n\nyl = df.year.quantile(0.1)\nyu = df.year.quantile(0.99)\n\ndf = df[df.price > pl]\ndf = df[df.price < pu]\n\ndf = df[df.odometer > ol]\ndf = df[df.odometer < ou]\n\ndf = df[df.year > yl]\ndf = df[df.year < yu]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature names\ncontinuous_features = ['year', 'odometer']\ncategorical_features = ['drive', 'type', 'fuel', 'transmission', 'manufacturer', 'condition', 'cylinders']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract X and y\ny = df['price']\nX = df.drop(columns=['price'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\n# Added Polynomial features\n# Added StandardScaler\n# Added imputation for continuous and categorical\n\ndef train_and_eval_classifier(classifier, cname):\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('polyfeatures', PolynomialFeatures()),\n        ('scaler', StandardScaler())])\n\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='UNK')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, continuous_features),\n            ('cat', categorical_transformer, categorical_features)])\n    \n    pipe = make_pipeline(preprocessor, classifier)\n    scores = cross_val_score(pipe, X_train, y_train, scoring=\"r2\")\n    print(\"CV Score for {}: {}\".format(cname, np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try a couple of classifiers to see if we actually improved\ntrain_and_eval_classifier(LinearRegression(), \"LinearRegression\")\ntrain_and_eval_classifier(ElasticNet(), \"ElasticNet\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After creating derived features adding standard scaling, and adding imputation, as well as performing more in-depth preprocessing and data cleaning,\nour models did improve by a significant amount! In fact our Linear Regression model increased in score from 0.35 to 0.76, and our ElasticNet improved from 0.17 to 0.61! "},{"metadata":{},"cell_type":"markdown","source":"# Task 4 Any Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost\ndef train_and_eval_xgb():\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('polyfeatures', PolynomialFeatures()),\n        ('scaler', StandardScaler())])\n\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='UNK')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, continuous_features),\n            ('cat', categorical_transformer, categorical_features)])\n    \n    \n    pipe = make_pipeline(preprocessor, xgb.XGBRegressor(objective='reg:squarederror')) \n    \n    # Parameter tuning\n    param_grid = {'xgbregressor__n_estimators': [100, 120, 140],\n                  'xgbregressor__learning_rate': [0.01, 0.1],\n                  'xgbregressor__max_depth': [5, 7]}\n    \n    xgb_grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, return_train_score=True, n_jobs=-1)\n    xgb_grid.fit(X_train, y_train) \n    print(\"Best score: %0.3f\" % xgb_grid.best_score_) \n    print(\"Best parameters set:\", xgb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_and_eval_xgb()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 5 Feature Selectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_influential_features():\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())])\n\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='UNK')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, continuous_features),\n            ('cat', categorical_transformer, categorical_features)])\n    \n    x = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=7, n_estimators=140)\n    pipe = make_pipeline(preprocessor, x)\n    \n    scores = cross_val_score(pipe, X_train, y_train, scoring=\"r2\")\n    print(\"CV Score for {}: {}\".format(\"XGBoost\", np.mean(scores)))\n    \n    feature_sel = SelectFromModel(pipe, 1e-5)\n    feature_sel.fit(X_train, y_train)\n    \n    return feature_sel, pipe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inf features, baseline score for XGBoost\ninf_features, pipe = find_influential_features()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature importances\nimportant_features = inf_features.estimator_.named_steps['xgbregressor'].feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get caregorical feature names\ncat_feature_names = pipe.named_steps['columntransformer'].transformers[1][1].steps[1][1].fit(X_train[categorical_features], y_train).get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add continuous feature names\nfeature_names = np.concatenate((continuous_features, cat_feature_names))\nfeature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Twenty most important features\ntop_twenty = important_features.argsort()[-20:][::-1]\ntt_fv = [important_features[i] for i in top_twenty]\ntt_fn = [feature_names[i] for i in top_twenty]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display 20 most important features\nimport matplotlib.pyplot as plt\nplt.scatter(tt_fn, tt_fv)\nplt.xticks(tt_fn, tt_fn, rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 4_cylinders, diesel, fwd, year, 4wd, gas, 8_cylinders are some of the most significant features, whereas others can most likely be removed without huge decreases to score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Preprocessor\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='UNK')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, continuous_features),\n        ('cat', categorical_transformer, categorical_features)])\n\npreprocessor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can select only the 50 most important features, and see what happens to our performance!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run preprocessing, and only select dataset of 50 top features.\ntop_50 = important_features.argsort()[-50:][::-1]\nX_train_trans = pd.DataFrame(preprocessor.transform(X_train).toarray())\nX_train_trans = X_train_trans[top_50]\nX_train_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrain and evaluate model on new dataset.\nx = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=7, n_estimators=140)\nnp.mean(cross_val_score(x, X_train_trans, y_train, scoring=\"r2\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the performance very slightly decreased from 0.861 to 0.860. In this case, cutting the least important features (selecting the most important features) does not really improve the score, however, it does help us create smaller more explainable models: we were able to cut 20 features out with only a very tiny decrease in r2 score! For a very explainable model, we can probably cut many more features with only small losses to score. This is explored next."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best score, from part 4\n# Compute the marginal score of features\nbest_score = 0.862\n\ntotal_features = continuous_features + categorical_features\nfor i, feature in enumerate(total_features):\n    total_features_n = total_features.copy()\n    total_features_n.remove(feature)\n    \n    if i < 2:\n        cont_n = [total_features_n[0]]\n        cat_n = total_features_n[1:]\n    else:\n        cont_n = total_features_n[:2]\n        cat_n = total_features_n[2:]\n    \n    X_train_cut_col = X_train[total_features_n]\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, cont_n),\n            ('cat', categorical_transformer, cat_n)])\n\n    preprocessor.fit(X_train_cut_col, y_train)\n    X_train_cut_col_trans = pd.DataFrame(preprocessor.transform(X_train_cut_col).toarray())\n\n    x = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=7, n_estimators=140)\n    score = np.mean(cross_val_score(x, X_train_cut_col_trans, y_train, scoring=\"r2\"))\n    \n    marginal_score = best_score - score\n    print(\"Marginal score of column {}: {}\".format(feature, marginal_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that cutting the columns year, cylinder, odometer, fuel, and manufacturer create the largest drops in score (i.e. the largest marginal scores). Hence, we can reduce our feature set to the ones derived from these columns. These will be our selected features."},{"metadata":{},"cell_type":"markdown","source":"# Task 6 Explainable Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From previous exploration, we can see that certain columns are less relevant\nX_train_cut_col = X_train[[\"cylinders\", \"fuel\", \"year\", \"odometer\"]]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, ['year', 'odometer']),\n        ('cat', categorical_transformer, [\"cylinders\", \"fuel\"])])\n\npreprocessor.fit(X_train_cut_col, y_train)\nX_train_cut_col_trans = pd.DataFrame(preprocessor.transform(X_train_cut_col).toarray())\n\nx = xgb.XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=7, n_estimators=140)\nnp.mean(cross_val_score(x, X_train_cut_col_trans, y_train, scoring=\"r2\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minimum_model = X_train_cut_col_trans.shape[1]\nprint(\"MINIMUM IMPORTANT FEATURES/COEFFICIENTS NEEDED: {}\".format(minimum_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attempt a random forests regressor with only 15 leaves.\ndef evaluate_minimum_random_forests():\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())])\n\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='UNK')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, continuous_features),\n            ('cat', categorical_transformer, categorical_features)])\n\n\n    pipe = make_pipeline(preprocessor, RandomForestRegressor(max_leaf_nodes=minimum_model))    \n    \n    scores = cross_val_score(pipe, X_train, y_train, scoring=\"r2\")\n    print(\"CV Score for {}: {}\".format(\"Minimum Random Forests Regressor\", np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_minimum_random_forests()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the calculations above, we discovered that our performance is contingent upon really 15 very important features. We reduced a model of 78 features to only 15 leaves/important features, which is a huge increase in explainablitiy, while still maintaining a very high score of 0.69. Our best models are operating at ~0.86, but these are more sophisticated models (XGBoost). Compared to some of the simpler regression models used earlier, we are able to achieve similar accuracies with very few leaves."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}