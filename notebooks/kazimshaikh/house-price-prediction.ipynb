{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndata_df = pd.read_csv('/kaggle/input/boston-house-prices/housing.csv', header=None, delim_whitespace=True, names=names) #, delimiter=r\"\\s+\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.duplicated().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.hist(bins=12, figsize=(12,10), grid=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* CRIM per capita: crime rate by town\n\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n* INDUS: proportion of non-retail business acres per town\n\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n* NOX: nitric oxides concentration (parts per 10 million)\n\n* RM: average number of rooms per dwelling\n\n* AGE: proportion of owner-occupied units built prior to 1940\n\n* DIS: weighted distances to five Boston employment centres\n\n* RAD: index of accessibility to radial highways\n\n* TAX: full-value property-tax rate per 10,000usd\n\n* PTRATIO: pupil-teacher ratio by town\n\n* B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\n* LSTAT: % lower status of the population\n\n* MEDV--> our resident value target"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(25, 12))\nsns.heatmap(data_df.corr(), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_df.drop('MEDV',axis = 1)\ny = data_df['MEDV']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr_all = LinearRegression()  \nlr_all.fit(X_train, y_train) \n\ny_pred1=lr_all.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coefficient of intercept\nlr_all.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the coefficient values to a dataframe\nlr_all_coeffcients = pd.DataFrame([X_train.columns,lr_all.coef_]).T\nlr_all_coeffcients = lr_all_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlr_all_coeffcients #print out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy score \nlr_all.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 𝑅^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\n* Adjusted 𝑅^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\n* MAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y.\n\n* MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value.\n\n* RMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# other evaluation metrics\nfrom sklearn import metrics\nprint('R^2:',metrics.r2_score(y_test, y_pred1))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred1))*(len(y_test)-1)/(len(y_test)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_pred1))\nprint('MSE:',metrics.mean_squared_error(y_test, y_pred1))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ridge Regression**\n* Ridge regression is one of the simple techniques to reduce model complexity and prevent over-fitting which may result from linear regression\n* The loss function is altered by adding a penalty equivalent to square of the magnitude of the coefficients\n* One parameter: Alpha (also called 'lambda')\n* higher the alpha value --> more restriction on the coeffs\n* lower alpha --> more generalization\n* Normal pratice: alpha>1 (e.g. 150;230)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=100)\nridge.fit(X_train, y_train)\ny_pred2 = ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**compare Linear regression vs Ridge(alpha=0.1) vs Ridge(alpha=100)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nrr1 = Ridge(alpha=0.01)\nrr1.fit(X_train, y_train)\n\nrr2 = Ridge(alpha=100)\nrr2.fit(X_train, y_train)\n\nprint('Linear regression test score:',lr_all.score(X_test, y_test))\nprint('Ridge regression test score with low alpha(0.1):', rr1.score(X_test, y_test))\nprint('Ridge regression test score with high alpha(100):', rr2.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In terms of magnitude of coefficients: Rigde regression with high alpha penalizes the coefficients on CHAS, NOX, and RM a lot"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(names[0:13], lr_all.coef_, alpha=0.4, linestyle='none', marker='o', markersize=7, color='green', label='Linear Regression')\nplt.plot(names[0:13], rr1.coef_, alpha=0.4, linestyle='none', marker='*', markersize=7, color='red', label=r'Ridge;$\\alpha=0.01$')\nplt.plot(names[0:13], rr2.coef_,alpha=0.4, linestyle='none', marker='d', markersize=7, color='blue', label=r'Ridge;$\\alpha=100$')\nplt.xlabel('Coefficient Index', fontsize=16)\nplt.ylabel('Coefficient Magnitude', fontsize=16)\nplt.legend(fontsize=13, loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lasso Regression**\n* Lasso regression is another simple technique to reduce model complexity and prevent over-fitting which result from lienar regression\n* Lasso regression not only helps in reducing over-fitting but it can help us in feature selection\n* Normal practice: alpha<1 (e.g. 0.1, 0.03)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.8)\nlasso.fit(X_train, y_train)\ny_pred3 = lasso.predict(X_test)\n\nlasso.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Removing the predictors with zero coefficients: CHAS and NOX"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the coefficient values to a dataframe\nlasso_coeffcients = pd.DataFrame([X_train.columns,lasso.coef_]).T\nlasso_coeffcients = lasso_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlasso_coeffcients #print out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Viewing by comparing linear and lasso regression coefficient plots \nimport matplotlib.pyplot as plt\nplt.plot(names[0:13], lasso.coef_, alpha=0.4, linestyle='none', marker='o', markersize=7, color='green', label='Lasso Regression')\nplt.plot(names[0:13], lr_all.coef_, alpha=0.4, linestyle='none', marker='d', markersize=7, color='blue', label='Linear Regression')\nplt.xlabel('Coefficient Index', fontsize=16)\nplt.ylabel('Coefficient Magnitude', fontsize=16)\nplt.legend(fontsize=13, loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameter tunning**\n* Ridge and Lasso regression: Choosing alpha\n* Hyperparameters cannot be learned by fitting he model\n* Solution: GridSearch/RandomizedSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"#find best alpha for Ridge Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'alpha':np.arange(1, 10, 500)} #range from 1-500 with equal interval of 10 \nridge = Ridge() \nridge_best_alpha = GridSearchCV(ridge, param_grid)\nridge_best_alpha.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best alpha for Ridge Regression:\", ridge_best_alpha.best_params_)\nprint(\"Best score for Ridge Regression with best alpha:\", ridge_best_alpha.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find best alpha for Lasso Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'alpha':np.arange(0, 0.1, 1)} #range from 0-1 with equal interval of 0.1 \nlasso = Lasso() \nlasso_best_alpha = GridSearchCV(lasso, param_grid) \nlasso_best_alpha.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best alpha for Lasso Regression:\", lasso_best_alpha.best_params_)\nprint(\"Best score for Lasso Regression with best alpha:\", lasso_best_alpha.best_score_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}