{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier from scratch\n\nToday, we will build the **K Nearest Neighbors algorithm** just with numpy and pandas library. There will be a lot of things to optimize, by the end of the notebook you'll feel like you have invented the algorithm by yourself. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What is KNN\nKNN stands for K-Nearest Neighbors. It's basically a classification algorithm that will make a prediction of a class of a target variable based on it's nearest neighbors. It will calculate the distance from the given point you want to classify your instance based on the majority classes of k-nearest points.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Some libraries for visualization purposes\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom pandas.plotting import scatter_matrix\nfrom collections import Counter\n\n# Increase the default plot size and set the color scheme\nplt.rcParams['figure.figsize'] = 8, 5\nplt.style.use(\"fivethirtyeight\")\n\n# Disable warnings \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summarize the data\n- Load the dataset\n- Get the high level overview of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data\ndf = pd.read_csv('/kaggle/input/palmer-archipelago-antarctica-penguin-data/penguins_size.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Info and Describe of the data\nprint(df.shape)\nprint(df.describe())\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The amount of missing values are very less, so we can drop those rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis + Feature Engineering\n- Data Visualisation\n- Cleaning the data\n- Feature Engineering\n- Feature Extraction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Count Plots to see the balance of data in categorical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 7))\nplt.subplot(1, 3, 1)\nsns.countplot(data=df, x='species')\nplt.subplot(1, 3, 2)\nsns.countplot(data=df, x='sex')\nplt.subplot(1, 3, 3)\nsns.countplot(data=df, x='island')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have one more outlier data in 'sex' column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf = df.loc[df.sex != '.', :]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dist plot to check the variation of the data in the continous variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nplt.subplot(2, 2, 1)\nsns.distplot(df['culmen_length_mm'])\nplt.subplot(2, 2, 2)\nsns.distplot(df['culmen_depth_mm'])\nplt.subplot(2, 2, 3)\nsns.distplot(df['flipper_length_mm'])\nplt.subplot(2, 2, 4)\nsns.distplot(df['body_mass_g'])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the the penguins belonging to the island 'Torgersen' is very less.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Box Plot to check the outliers in the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nplt.subplot(2, 2, 1)\nsns.boxplot(df['culmen_length_mm'])\nplt.subplot(2, 2, 2)\nsns.boxplot(df['culmen_depth_mm'])\nplt.subplot(2, 2, 3)\nsns.boxplot(df['flipper_length_mm'])\nplt.subplot(2, 2, 4)\nsns.boxplot(df['body_mass_g'])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no outliers in our data, so we can make KNN without any issues","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Multivariate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Checking for coorelations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the columns are corelated as much as 0.95 so we will not be dropping any columns, though we can see that flipper length and body mass is highly corelated. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=df, hue='species');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n### Categorical values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Building - KNN\n- Split the data\n- Define the distance functions \n- Calculate the distance of test point from all the points in the dataset\n- Sort the distance\n- Take the majority vote \n- Predict the class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\ntrain = df[df['is_train'] == True]\ntest = df[df['is_train'] == False]\n\ntrain_x = train[train.columns[:len(train.columns) - 1]]\ntrain_x = train_x.drop('species', axis=1) # Dropping the label\ntrain_y = train['species']\n\n\ntest_x = test[test.columns[:len(test.columns) - 1]]\ntest_x = test_x.drop('species', axis=1) # Dropping the label\ntest_y = test['species']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the categorical values in train dataset to one hot encoded","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = pd.get_dummies(train_x)\ntest_x = pd.get_dummies(test_x);\n\n# One hot encoding the label data to fit in the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x.shape)\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distance between 2 vectors\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Euclidean Distance\ndef euclidean_distance(point1, point2):\n    distance = 0\n    for i in range(point1.shape[0]):\n        distance += np.square(point1[i] - point2[i])\n    return np.sqrt(distance)\n\n# Manhattan Distance\ndef manhattan_distance(point1, point2):\n    distance = 0\n    for i in range(point1.shape[0]):\n        distance += abs(point1[i] - point2[i])\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def knn(train_x, train_y, dis_func, sample, k):\n    \"\"\"\n    Parameters:\n    train_x: training samples\n    train_y: corresponding labels\n    dis_func: calculates distance\n    sample: one test sample\n    k: number of nearest neighbors\n    \n    Returns:\n    cl: class of the sample\n    \"\"\"\n    \n    distances = {}\n    for i in range(len(train_x)):\n        d = dis_func(sample, train_x.iloc[i])\n        distances[i] = d\n    sorted_dist = sorted(distances.items(), key = lambda x : (x[1], x[0]))\n    \n    # take k nearest neighbors\n    neighbors = []\n    for i in range(k):\n        neighbors.append(sorted_dist[i][0])\n    \n    #convert indices into classes\n    classes = [train_y.iloc[c] for c in neighbors]\n    \n    #count each classes in top k\n    counts = Counter(classes)\n    \n    #take vote of max number of samples of a class\n    list_values = list(counts.values())\n    list_keys = list(counts.keys())\n    cl = list_keys[list_values.index(max(list_values))]\n    \n    return cl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = knn(train_x, train_y, euclidean_distance, test_x.iloc[3], k=5)\nprint(model)\nprint(test_y.iloc[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that we are getting the correct results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the accuracy\ndef get_accuracy(test_x, test_y, train_x, train_y, k):\n    correct = 0\n    for i in range(len(test_x)):\n        sample = test_x.iloc[i]\n        true_label = test_y.iloc[i]\n        predicted_label_euclidean = knn(train_x, train_y, euclidean_distance, sample, k)\n        if predicted_label_euclidean == true_label:\n            correct += 1\n    \n    accuracy_euclidean = (correct / len(test_x)) * 100\n    \n    correct = 0\n    for i in range(len(test_x)):\n        sample = test_x.iloc[i]\n        true_label = test_y.iloc[i]\n        predicted_label_euclidean = knn(train_x, train_y, manhattan_distance, sample, k)\n        if predicted_label_euclidean == true_label:\n            correct += 1\n    \n    accuracy_manhattan = (correct / len(test_x)) * 100\n    \n    print(\"Model accuracy with Euclidean Distance is %.2f\" %(accuracy_euclidean))\n    print(\"Model accuracy with Manhattan Distance is %.2f\" %(accuracy_manhattan))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(test_x, test_y, train_x, train_y, k=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Testing the same dataset with the KNN algo from SKlearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = neighbors.KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\nclassifier.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test_x)\naccuracy = accuracy_score(test_y, y_pred)\nprint('Accuracy with sklearn KNN with the same hyperparameters: {:.4f}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting the exact same results as Sklearn implementation of KNN proving that our algorithm works.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}