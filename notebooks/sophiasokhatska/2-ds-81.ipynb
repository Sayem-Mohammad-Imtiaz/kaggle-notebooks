{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Системи Data Science. Лабораторна робота №2. \n## Назва: Прогнозування рейтингу проектів японської мультиплікації\n## Виконала: Сохацька Софія Максимівна, група КМ-81.\n## 1.Методи розв'язання задачі\n### 1.1 Перелік  існуючих  потенційних  методів  розв’язання  поставленої задачі\nПоставлена задача прогнозування, в нашому випадку прогнозування рейтингу, э однію з задач регресії. Розглянемо деякі методи розв'язання регресійних задач.\n#### 1. Лінійна регресія\nЛінійна регресія відноситься до виду регресійній моделі, яка складається з взаємопов'язаних чисел. Парна (проста) лінійна регресія - це модель, що дозволяє моделювати взаємозв'язок між значеннями однієї незалежної та однієї залежної змінними за допомогою лінійної моделі - прямої. Більш поширеною моделлю є множинна лінійна регресія, яка передбачає встановлення лінійної залежності між безліччю вхідних незалежних і однієї залежної змінної. Ми можемо побудувати модель множинної лінійної регресії в такий спосіб:\n\n$$Y = a_1 * X_1 + a_2 * X_2 + a_3 * X_3 ....... a_n * X_n + b$$\nДе $a_n$ - це коефіцієнти, $X_n$ - змінні і $b$ - зсув.\n\nПереваги та недоліки:\n* Вона легко моделюється і є особливо корисною при створенні не надто складних залежностей, а також при невеликій кількості даних.\n* Чутлива до викидів\n\n#### 2. Поліноміальна регресія\nДля створення такої моделі, яка підійде для нелінійно поділюваних даних, можна використовувати поліноміальних регресію. В даному методі проводиться крива лінія, залежна від точок площини. У полиномиальной регресії ступінь деяких незалежних змінних перевищує 1. Наприклад, вийде щось подібне:\n\n$$Y = a_1*X_1 + (a_2)²*X_2 + (a_3)⁴*X_3 ....... a_n*X_n + b$$\n\nПереваги та недоліки:\n* Моделює нелінійно розділені дані (чого не може лінійна регресія)\n* Більш гнучка і може моделювати складні взаємозв'язки.\n* Повний контроль над моделюванням змінних об'єкта (вибір ступеня)\n* Необхідно мати деякі знання про дані, для вибору найбільш підходящої ступеня\n* При неправильному виборі ступеня, дана модель може бути перенасичена\n\n#### 3. Гребінна (Ridge) регресія (L2 регуляризація)\nГребінна регресія є лінійною моделлю регресії, направленою на регуляризацію моделі. У порівнянні з лінійною регрессією, вона значно обмежує коефіцієнти при змінних, тим самим зменшуючи її вплив на результат.\n\nПереваги та недоліки: \n* Запобігає перенавчанню\n* Значне спрощення моделі\n* Нижча точність прогнозування при недостатній кількості даних\n* Відсутність відбору признаків\n\n#### 4. Регресія по методу «ласо» (L1 регуляризація)\nАльтернативою гребінного методу як методу регуляризації лінійної регресії є метод ласо. Відрізняється від гребінної регресії тим, що якщо в першій всі коефіцієнти хоч і наближались до нуля, проте залишались ненульовими, то в регресії по методу \"ласо\" деякі з них стають рівними нулю, в результаті чого деякі ознаки ніяк не впливають на модель.\n\nПереваги та недоліки:\n\n* Значно спрощує інтерпретацію моделі \n* Виявляє найбільш важливі ознаки моделі\n* Запобігає перенавчанню\n* Запобігає перенавчанню настільки, що можливе недонавчання\n* Нижча точність прогнозування при недостатній кількості даних\n\n\n#### 5. Регресія \"еластична сітка\"\n\nЕластична сітка - це гібрид методів регресії ласо і гребінної регресії. Вона використовує як L1, так і L2 регуляризації, враховуючи ефективність обох методів.\n\n#### 6. Метод k найближчих сусідів\n\nМетод заснований на знаходженні k найближчих сусідів для нової точки, для якої робиться прогноз, та встановлення прогнозованого параметру як середнього значення даних, які вже були визначені для найближчих сусідів. \n\nПереваги та недоліки:\n* Легкий у моделюванні та інтерпритації\n* Стійкий до аномальних викидів\n* Вимагає репрезентативного набору даних\n* Сильна прив'язка до існуючого набору даних\n\n#### 7. Метод дерева рішень\nОснова методу дерева рішень полягає в побудові послідовності так званих правил «якщо ... то ...», яка приводить нас до істинної відповіді максимально коротким шляхом. \n\nПереваги та недоліки:\n* Легкий у моделюванні та інтерпритації\n* Не потребує нормалізації чи стандартизації признаків\n* Стійкість до аномальних викидів та відсутніх даних\n* Вища гнучкість за рахунок наявності багатьох гіперпараметрів\n* Схильний до перенавчання","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Опис  обраного  методу  моделювання та його математичного забезпечення\nВ якості основного методу моделювання для даної задачі було обрано метод дерева рішень. Проте, зважаючи на те, що у минулій лабораторній було виявлено високу кореляцію значень, така модель буде дуже схильна до перенавчання. Тому було вирішено додати також метод регуляризації, щоб запобігти перенавчанню остаточної моделі. Остаточна модель буде враховувати прогнози двох складових моделей у пропорції 6:4\n$$Y = 0.6 *Y_t + 0.4 * Y_r$$\nде $Y_t$ - модель дерева рішень, $Y_r$ - гребінна регресія.\n\nЯк вже було зазначено у попередньому розділі, гребінна регресія також є лінійною моделлю регресії, тому її формула аналогічна тій, що використовується в звичайній лінійній регресії. Проте гребньовій регресії коефіцієнти (а) вибираються не тільки з точки зору того, наскільки добре вони дозволяють прогнозувати результат на навчальних даних, вони ще підганяються відповідно до додаткових обмежень. Нам потрібно, щоб величина коефіцієнтів була якомога менше. Це означає, що кожна ознака повинна мати якомога менший вплив на результат (тобто кожна ознака повинна мати невеликий регресійний коефіцієнт) і в той же час вона повинен як і раніше мати гарну прогнозну силу. Це обмеження є прикладом регуляризації (Regularization). Регуляризація означає явне обмеження моделі для запобігання перенавчання. Така регрессія застосовується у разі високої колінеарності змінних, при яких стандартна лінійна і поліноміальна регресії стають неефективними. \n\nРозлянемо більш детально також метод дерева рішень. Дерево рішень - різновид схеми, де показані можливі наслідки прийняття серії пов'язаних між собою рішень. Такий підхід дозволяє зважити різні варіанти дій, беручи до уваги будь-який набір параметрів. Дерево рішень, як правило, починається з одного вузла, від якого через ребра (гілки) відгалужуються можливі варіанти (листи). Гілки містять  атрибути, від яких залежить цільова функція, а листи - значення цільової функції, зміненої в ході руху від кореня по листа. Проміжні вузли містять атребути, за якими розрізняється подальший \"шлях\". Кожен внутрішній вузол відповідає одній з вхідних змінних. Дерево може бути також «вивчено» поділом вихідних наборів змінних на підмножини, що засновані на тестуванні значень атрибутів. Це процес, який повторюється на кожному з отриманих підмножин. Рекурсія завершується тоді, коли підмножина в вузлі має ті ж значення цільової змінної, таким чином, воно не додає цінності для пророкувань. При використанні дерева рішень для задачі регресії дуже важдиво обрізати дерево рішень, що б уникнути ситуації, коли кожному листу відповідає одне значення вибірки. \n\nДля побудови моделі будемо використовувати бібліотеку `````sklearn`````. Також в даній лабораторній для роботи з даними та побудовою графіків використовуються такі бібліотери, як `````numpy`````, `````pandas`````, `````seaborn````` та `````matplotlib`````.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import  Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:02.990906Z","iopub.execute_input":"2021-05-31T23:28:02.991381Z","iopub.status.idle":"2021-05-31T23:28:04.438047Z","shell.execute_reply.started":"2021-05-31T23:28:02.991246Z","shell.execute_reply":"2021-05-31T23:28:04.436564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Формалізація  цільової  функції  оптимізації. Визначення  метрик оцінки ефективності моделі\n### 2.1 Формалізація  цільової  функції  оптимізації (функції витрат)\nФункція втрат - це міра кількості помилок, які наша лінійна регресія робить на наборі даних. Функції витрат обчислюють відстань між передбаченим значенням y (х) і його фактичним значенням. Так як поставлена задача відноситься до классу регресійних задач, будемо використовувати средню квадратичну помилку (MSE), що є найросповсюдженішою функцію витрат для регресійних методів.Щоб обчислити MSE, ми просто беремо всі значення помилок, рахуємо їх квадрати довжин (відстань між передбаченим і фактичним значенням) і усереднюємо його. Формула функціїї MSE: \n$$ MSE = {1 \\over N}\\sum\\limits_{i=1}^{N} {(y_i - \\widehat y)^2} $$\nде $y_i$ - фактичне значення, а $\\widehat y$ - передбачене\n\nПроте така цільова функція досить чутлива до аномальних відхилень, адже в ній присутнє піднесення до квадрату. Тож введемо також додаткову фунуцію MAE - середню абсолютну похибку\n$$ MAE = {1 \\over N}\\sum\\limits_{i=1}^{N} {|(y_i - \\widehat y)|} $$\nБезсумнівною перевагою MAE є те, що модулі не збільшують в рази відхилення, що вважаються викидами.\n\nВ даній роботі будемо використовувати обидві метрики, адже MAE більш стійка викидів, а MSE краще демонструє невеликі відхилення.\n### 2.2 Визначення  метрик оцінки ефективності моделі\n\nДля визначення ефективності регресійної моделі зручно використовувати коефіцієнт детермінації -  $R^2$. Він приймає значення від від 0 до 1. Вважається, що чим ближче коефіцієнт до 1, тим кращою є модель. Формула для визначення $R^2$ : \n$$ R^2 = 1 - {{\\sum\\limits_{i=1}^{N} {(y_i - \\widehat y)^2}} \\over {\\sum\\limits_{i=1}^{N} {(y_i - \\overline y)^2}}} $$\nде $\\overline y$ - середнє значення y\n\nДля зручності заздалегідь напишемо функцію, що буде підраховувати ці три значення.","metadata":{}},{"cell_type":"code","source":"def metrics (y, y_pred):\n    mse = mean_squared_error(y, y_pred)\n    mae = mean_absolute_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    print(f\"MSE: {mse}\")\n    print(f\"MAE: {mae}\")\n    print(f\"R2: {r2}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:04.440728Z","iopub.execute_input":"2021-05-31T23:28:04.441044Z","iopub.status.idle":"2021-05-31T23:28:04.449632Z","shell.execute_reply.started":"2021-05-31T23:28:04.441005Z","shell.execute_reply":"2021-05-31T23:28:04.448117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Імплементація обраного методу моделювання\n### 3.1 Cтворення моделі\n\nПеред тим, як починати створювати модель, спершу підргузимо дані, що були отримані після виконання минулої лабораторної роботи, та підготуємло їх до роботи.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/csv-from-lab-1/df.csv\")\nx = df.copy()\nx.drop([\"Score\",\"Name\"], axis=1, inplace=True)\ny = df[\"Score\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(f\"Розмірність тренувальних даних: {x_train.shape}\\nРозмірність тестових даних: {x_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:04.451954Z","iopub.execute_input":"2021-05-31T23:28:04.452519Z","iopub.status.idle":"2021-05-31T23:28:06.234801Z","shell.execute_reply.started":"2021-05-31T23:28:04.452471Z","shell.execute_reply":"2021-05-31T23:28:06.233402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тепер підготуємо наші моделі.","metadata":{}},{"cell_type":"code","source":"ridge = Ridge(random_state=42)\ntree = DecisionTreeRegressor(random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:06.236489Z","iopub.execute_input":"2021-05-31T23:28:06.236865Z","iopub.status.idle":"2021-05-31T23:28:06.241983Z","shell.execute_reply.started":"2021-05-31T23:28:06.236833Z","shell.execute_reply":"2021-05-31T23:28:06.240573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Навчання моделі\nПеред тим,як об'єднати прогнози обидвох моделей, подивимося на їх у дії незалежно один від одного. Для початку виконаємо прогноз, використовуючи гребінний метод, та оцінимо ефективність моделі на тренувальному та тестовому наборах даних. Також порівняємо спрогнозовані дані з реальними, вивівши графік.","metadata":{}},{"cell_type":"code","source":"ridge.fit(x_train, y_train)\ny_pred_ridge = ridge.predict(x_test)\ny_pred_ridge_train = ridge.predict(x_train)\nprint(\"Тренувальний набір:\")\nmetrics(y_train, y_pred_ridge_train)\nprint(\"Тестовий набір\")\nmetrics(y_test, y_pred_ridge)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:06.246811Z","iopub.execute_input":"2021-05-31T23:28:06.24719Z","iopub.status.idle":"2021-05-31T23:28:06.938727Z","shell.execute_reply.started":"2021-05-31T23:28:06.247156Z","shell.execute_reply":"2021-05-31T23:28:06.937429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_graf(y_pred, y):\n    rng = np.arange(len(y_pred))\n    plt.figure(figsize=(14, 7))\n    plt.ylabel(\"Рейтинг\")\n    plt.scatter(rng, y, label=\"Реальні дані\", alpha=0.6, s = 10)\n    plt.scatter(rng, y_pred, label=\"Передбачені дані\", alpha=0.6, s = 10)  \nmodel_graf(y_pred_ridge, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:06.940988Z","iopub.execute_input":"2021-05-31T23:28:06.94145Z","iopub.status.idle":"2021-05-31T23:28:07.23021Z","shell.execute_reply.started":"2021-05-31T23:28:06.9414Z","shell.execute_reply":"2021-05-31T23:28:07.228868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Результати досить втішні. Навіть без особливого настроювання гіперпараметрів, коефіцієнт детермінації моделі перевищуе 0.5, отже модель є прийнятною. Проте, як ми можемо бачити, ця модель схильна прогнозувати параметри більш усереднено, як ми можемо побачити з підвищеного скупчення точок у центрі.\n\nТепер спрогнозуємо рейтинг використовуючи метод дерева рішень та знову виведемо графік порівняння розподілів.","metadata":{}},{"cell_type":"code","source":"tree.fit(x_train, y_train)\ny_pred_tree = tree.predict(x_test)\ny_pred_tree_train = tree.predict(x_train)\nprint(\"Тренувальний набір:\")\nmetrics(y_train, y_pred_tree_train)\nprint(\"Тестовий набір\")\nmetrics(y_test, y_pred_tree)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:07.231743Z","iopub.execute_input":"2021-05-31T23:28:07.232052Z","iopub.status.idle":"2021-05-31T23:28:07.904769Z","shell.execute_reply.started":"2021-05-31T23:28:07.232022Z","shell.execute_reply":"2021-05-31T23:28:07.903325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_graf(y_pred_tree, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:07.90623Z","iopub.execute_input":"2021-05-31T23:28:07.906547Z","iopub.status.idle":"2021-05-31T23:28:08.135864Z","shell.execute_reply.started":"2021-05-31T23:28:07.906516Z","shell.execute_reply":"2021-05-31T23:28:08.13431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Коефіцієнт детермінації цієї моделі також перевищуе 0.5 без оптимізації гіперпараметрів. Також вона має кращий розподіл - вже немає такого скупчення по центру, як у минулої моделі. Проте помітні більш малі скупчнення по всій шкалі рейтингу. Також, порівнюючи метрики тренувального та тестового набору, бачимо що наша модель є дуже перенавчена.\n\nТепер поєднаємо наші моделі та подивимося на результат.","metadata":{}},{"cell_type":"code","source":"p = 0.6 * y_pred_tree + 0.4 * y_pred_ridge\np_train = 0.6 * y_pred_tree_train + 0.4 * y_pred_ridge_train\nprint(\"Тренувальний набір:\")\nmetrics(y_train, p_train)\nprint(\"Тестовий набір\")\nmetrics(y_test, p)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:08.137672Z","iopub.execute_input":"2021-05-31T23:28:08.138051Z","iopub.status.idle":"2021-05-31T23:28:08.15109Z","shell.execute_reply.started":"2021-05-31T23:28:08.138014Z","shell.execute_reply":"2021-05-31T23:28:08.150018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_graf(p, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:08.152873Z","iopub.execute_input":"2021-05-31T23:28:08.153262Z","iopub.status.idle":"2021-05-31T23:28:08.373504Z","shell.execute_reply.started":"2021-05-31T23:28:08.153228Z","shell.execute_reply":"2021-05-31T23:28:08.372436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Як ми бачимо, рішення поєднати два методи було вдалим. Остаточна модель поєднала в собі переваги обох методів. Гребінний метод зменшив перенасичення моделі, а метод дерев підвищив точність та покращив розподіл результатів. Результуючий коефіцієнт детермінації кращий, ніж для кожного методу окремо.","metadata":{}},{"cell_type":"markdown","source":"### 3.3 Оптимізація   гіперпараметрів","metadata":{}},{"cell_type":"markdown","source":"Незважаючи на те, що наша модель навіть без особливої настройки гіперпараметрів видає досить непогані результати, спробуємо покращити їх. Для цього спочатку розберемо які гіперпараметри існують в цих методах та що вони визначають.\n\nДля методу гребінної регресії основним впливовим гіперпараметром є параметр `````alpha````` - міцність регуляризації. Чим більше значення - тим більша регуляризація. Збільшення alpha змушує коефіцієнти стискатися\nдо близьких до нуля значень, що знижує якість роботи моделі на навчальному наборі, але може поліпшити її узагальнюючу здатність.\n\nДля методу дерева рішень основними є наступні параметри:\n* `````splitter````` - стратегія, яка використовується для вибору поділу на кожному вузлі. Підтримувані стратегії є «кращими» для вибору кращого поділу і «випадковими» для вибору кращого випадкового поділу.\n* `````max_depth````` - максимальна глибина дерева. Теоретична максимальна глибина, яку може досягти дерево рішень, на одиницю менше кількості навчальних вибірок, але жоден алгоритм не дозволить вам досягти цієї точки з очевидних причин, однією з яких є переоснащення. Тож зменшення цього параметру може допомогти нам завадити перенасиченню нашої моделі.\n* `````min_samples_split````` - мінімальна кількість вибірок, необхідна для поділу внутрішнього вузла. Більш високі значення цього параметру не дозволяють моделі вивчати відносини, які можуть бути дуже специфічними для конкретної вибірки, обраної для дерева. Занадто високі значення можуть привести до недостатньої підгонки.\n* `````min_samples_leaf````` - мінімальна кількість вибірок, що повина бути у листовому (без дочірніх елементів) вузлі. Збільшуючи це значення, ми гарантуємо, що дерево не зможе відповідати початковому набору даних, створюючи групу невеликих гілок виключно для однієї вибірки кожна.\n\nПерейдемо до підбору оптимальних значень гіперпараметрів за допомогою вбудованої функції sklearn GridSearchCV().\n\nОптимальні значення для першої моделі:","metadata":{}},{"cell_type":"code","source":"alpha = np.arange(0.5, 10, 0.5)\ngrid_r = {'alpha': alpha}\nr = Ridge()\nparam_r = GridSearchCV(estimator = r, param_grid = grid_r,verbose=1, cv = 3)\nparam_r.fit(x_train, y_train)\nparam_r.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:08.374951Z","iopub.execute_input":"2021-05-31T23:28:08.375544Z","iopub.status.idle":"2021-05-31T23:28:08.380714Z","shell.execute_reply.started":"2021-05-31T23:28:08.375495Z","shell.execute_reply":"2021-05-31T23:28:08.379721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Оптимальні значення для другої моделі:","metadata":{}},{"cell_type":"code","source":"splitter = ['best', 'random']\nmax_depth = np.arange(5, 30, 5)\nmin_samples_leaf = np.arange(1, 15, 2)\nmin_samples_split = np.arange(2, 30, 2)\ngrid_t = {'splitter': splitter,\n            'max_depth': max_depth,\n            'min_samples_leaf': min_samples_leaf,\n            'min_samples_split': min_samples_split}\nt = DecisionTreeRegressor()\nparam_t = GridSearchCV(estimator = t, param_grid = grid_t,verbose=1, cv = 3)\nparam_t.fit(x_train, y_train)\nparam_t.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:08.382129Z","iopub.execute_input":"2021-05-31T23:28:08.382718Z","iopub.status.idle":"2021-05-31T23:28:08.393391Z","shell.execute_reply.started":"2021-05-31T23:28:08.382676Z","shell.execute_reply":"2021-05-31T23:28:08.392635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Оновимо наші моделі.","metadata":{}},{"cell_type":"code","source":"ridge = Ridge(alpha = 2.5)\nridge.fit(x_train, y_train)\ny_pred_ridge = ridge.predict(x_test)\n\ntree = DecisionTreeRegressor(max_depth=10, min_samples_leaf=13, min_samples_split=28, random_state=42)\ntree.fit(x_train, y_train)\ny_pred_tree = tree.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:08.394566Z","iopub.execute_input":"2021-05-31T23:28:08.395138Z","iopub.status.idle":"2021-05-31T23:28:09.169501Z","shell.execute_reply.started":"2021-05-31T23:28:08.395098Z","shell.execute_reply":"2021-05-31T23:28:09.168185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Також перевіримо коефіцієнти, що визначають вплив обох моделей на остаточну модель даних. Перевіримо різні комбінації коефіцієнти та подивимося, які з них видають найкращий результат.","metadata":{}},{"cell_type":"code","source":"k1 = np.arange(0.1, 0.9, 0.1)\nk2 = np.arange(0.9, 0.1, -0.1)\nr2_list = []\nfor i in range (8):\n    p = k1[i] * y_pred_ridge + k2[i] * y_pred_tree\n    r2 = r2_score(y_test, p)\n    r2_list.append(r2)\nplt.figure(figsize=(14, 7))\nplt.plot(k1, r2_list)\nplt.ylabel(\"Коефіцієнт детермінації\")\nplt.xlabel(\"Коефіцієнт при гребінному методі\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:09.170864Z","iopub.execute_input":"2021-05-31T23:28:09.17119Z","iopub.status.idle":"2021-05-31T23:28:09.356476Z","shell.execute_reply.started":"2021-05-31T23:28:09.17116Z","shell.execute_reply":"2021-05-31T23:28:09.355047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отже, оптимальна формула має вигляд \n$$Y = 0.7 *Y_t + 0.3 * Y_r$$","metadata":{}},{"cell_type":"markdown","source":"### 3.4 Остаточне тестування моделі","metadata":{}},{"cell_type":"code","source":"ridge.fit(x_train, y_train)\ny_pred_ridge = ridge.predict(x_test)\ny_pred_ridge_train = ridge.predict(x_train)\n\ntree.fit(x_train, y_train)\ny_pred_tree = tree.predict(x_test)\ny_pred_tree_train = tree.predict(x_train)\n\np = 0.7 * y_pred_tree + 0.3 * y_pred_ridge\np_train = 0.7 * y_pred_tree_train + 0.3 * y_pred_ridge_train\nprint(\"Тренувальний набір:\")\nmetrics(y_train, p_train)\nprint(\"Тестовий набір\")\nmetrics(y_test, p)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:09.357968Z","iopub.execute_input":"2021-05-31T23:28:09.358277Z","iopub.status.idle":"2021-05-31T23:28:10.319041Z","shell.execute_reply.started":"2021-05-31T23:28:09.358247Z","shell.execute_reply":"2021-05-31T23:28:10.317539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_graf(p, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:10.320952Z","iopub.execute_input":"2021-05-31T23:28:10.321247Z","iopub.status.idle":"2021-05-31T23:28:10.537522Z","shell.execute_reply.started":"2021-05-31T23:28:10.321219Z","shell.execute_reply":"2021-05-31T23:28:10.536289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### 3.5 Інтерпритація результату\n\nРозроблена модель показала гарні результати. Рішення щодо поєднання двох моделей, що мають різні переваги та недоліки, значно покращила резкультат. Модель дерева рішень має дуже гарний розподіл, проте дуже схильна до перенавчання. Гребінна модель навпаки, має тенденцію до групування результатів у певному проміжку, проте запобігає перенавантаженню. При поєднанні цих двох моделей вони перекрили недоліки одна одної, та показали кращий результат, ніж кожна модель окремо.\n\nНа результати, що були отримані, також вплинули результати минулої лабораторної роботи. По-перше, була проведена очистка даних, перевірена відсутність дублікатів, `````NaN````` та `````Null````` значень. Також було очищено більшість полів від записів зі значенням `````Unknown`````, що значно підвищило чистоту даних та вплинуло на точність прогнозів. Також, всі категоріальні характеристики було закодовано за допомогою `````One Hot Encoding`````, що дуже сильно збільшила загальну кількість параметрів, проте також посприяло вирішенню задачі регресійного аналізу.\n\nНа вибір  методів моделювання також мав вплив проведений у минулій лабораторній роботі розвідувальний аналіз даних, що визначив високу ступінь кореляції параметрів. В результаті цього при виборі методу моделювання були враховані ризики перенавчання обраної регресійної моделі.\n\nТакож фінальний результат покращило проведення оптимізаіцї гіперпараметрів обох складових моделей. При переборі можливих значень деяких гіперпараметрів було обрано оптимальні, що покращило ефективність моделі на 3.5%. Проте через велику ресурсоємкість такої перевірки, були перевірені далеко не всі гіперпараметри, а в тих, що все ж оптимізовувалися, перевірено лише певну вибірку з неповного діапазону значень. В результаті, не зважаючи на все ж наявне покращення ефективності моделі після проведення оптимізації, різниця ефективностей відносно невелика. Проте її можна збільшити при більш детальнії перевірці гіперпараметрів при наявності більших обчислювальних ресурсів.\n\n### 3.6 Документування  класів бізнес-профіля Еріксона-Пенкера","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input//photos-lab-2/ericsson-penker_2.png') ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T23:28:10.539313Z","iopub.execute_input":"2021-05-31T23:28:10.539757Z","iopub.status.idle":"2021-05-31T23:28:10.568161Z","shell.execute_reply.started":"2021-05-31T23:28:10.539706Z","shell.execute_reply":"2021-05-31T23:28:10.566934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Верифікація та валідація\n### 4.1 Верифікація\n\nУмовою лабораторної роботи було створення моделі, що вирішувала би поставлену бізнес-задачу DS, а саме задачу прогнозування рейтингу проектів, на основі даних, підготовлених в ході минулої лабораторної роботи.\n\nВ рамках виконання цієї лабораторної роботи було наведено та описано існуючі  потенційні  методи розв’язання  поставленої задачі, їх переваги та недоліки, та на основі цього вибрано модель для виконання поставленої задачі. Вибраною моделлю виявилася комбінація моделі гребінної регресії та моделі дерева рішень. Також було описано більш детальний принцип роботи обох вибраних моделей. Такий вибір був зумовлений високою кореляцією параметрів та великою їх кількістю.\n\nДля обраної задачі як для задачі регресії було формалізовано дві функції витрат MSE та MAE, а також було обрано коефіцієнт детермінації $R^2$ Для визначення ефективності регресійної моделі.\n\nБула виконана імплементація обраної моделі, що включала в себе наступні етапи\n\n* створення моделі \n* навчання моделі\n* візуалізація моделі\n* оптимізація гіперпараметрів\n* остаточне тестування моделі\n* інтерпретація  отриманих  результатів\n* документування класів бізнес-профіля Еріксона-Пенкера\n\nУсі умови лабораторної роботи було виконано, тож верифікація пройшла успішно.\n### 4.2 Валідація\n\nЦільові функції побудованої моделі становлять:\n* MSE: 0.2466890535019699\n* MAE: 0.3594311481692238\nТакі результати є не найкращими, проте прийнятними. Коефіцієнт детермінації остаточної моделі становить\n$R^2 \\approx 0.684$. Це доволі непоганий результат, що каже про те, що модель є прийнятною та може бути використана для вирішення поставленої задачі. Проте модель все ще схильна до перенавантажень, хоч і не таких значних. Ефективність моделі може бути покращена більш точним підбором гіперпараметрів використовуваних методів регресії, проте це потребує використання більшої кількості обчислювальних ресурсів. Загалом, враховуючи прийнятність моделі для вирішення задачі регресії, можемо казати що вона пройшла кінцеву валідацію\n\n## 5. Висновки\nЗа результатами верифікації можемо прийти до висновку, що робота відповідає заданим вимогам. Також в процесі виконання лабораторної роботи було отримано наступні навички у сфері моделювання даних:\n\n* знання типових задачі Data Science\n* розуміння методів розв'язання регресійних задач, їх переваги та недоліки\n* формалізація цільових функції оптимізації задач моделювання та визначення метрик оцінки ефективності моделі \n* навички роботи з бібліотекою `````sklearn````` для машинного навчання\n* імплементування моделі, що прогнозує рейтинги проектів японської анімації\n* проведення оптимізація гіперпараметрів моделей\n* інтерпретування результатів створення моделі \n\nБули успішно пройдені етапи верифікації та валідації, метрики показали гарні результати, отже модель є прийнятною для вирішення поставленої задачі прогнозування рейтингу проектів японської мультиплікації.","metadata":{}}]}