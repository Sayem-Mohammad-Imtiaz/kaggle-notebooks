{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <h1 align=\"center\">Introduction </h1>Mall Analytics measure the quality of relationships between the mall and the store. By tracking customers we analyize their shopping behaviour and spending index.\n![mall](https://www.dw.com/image/17955220_303.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Note:- This Kernel is subject to get updated as soon as i find something which can be revelant to the context. Please Upvote if you like the Kernel"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Loading the datatset<b>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/Mall_Customers.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>The First Gaze<b>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shape of the data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic Information "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Checking for the Null values </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata['Gender']=le.fit_transform(data['Gender'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Loading dependencies for Visualization</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\", palette=\"PuBuGn_d\", color_codes=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Gender Distribution </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Gender',data=data,palette='winter')\nsize=data['Gender'].value_counts()\nprint('Female :',size[0]/(size[0]+size[1])*100)\nprint('Male :',size[1]/(size[0]+size[1])*100)\nplt.title(\"Gender distirbution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A great insight, why female contribute more to the shopping \n> The real reason is sobering.  In virtually every society in the world, women have primary care-giving responsibilities for both children and the elderly (and often, just about everybody else in-between). In this primary caregiving role, women find themselves buying on behalf of everyone else in their lives. More here \n> https://www.forbes.com/sites/bridgetbrennan/2013/03/06/the-real-reason-women-shop-more-than-men/#1a0c65d174b9"},{"metadata":{},"cell_type":"markdown","source":"<b> Age ,Annual Income and Spending Score Distribution </b>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 ,6))\nn = 0 \ncolor=['red','green','blue']\ncount=0\nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(data[x] , color=color[count])\n    plt.title('Distplot of {}'.format(x))\n    count+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <b>Understanding the distributionn and relation between the attributes<b>"},{"metadata":{},"cell_type":"markdown","source":"We will be using pairs plot which allows us to see both distribution of single variables and relationships between two variables. Pair plots are a great method to identify trends for follow-up analysis and, fortunately and here in this example we will identify the pattern "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(data)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pair plot , we figure out that the <b>Age</b> between <b>20-40</b> having high spending index and following it, the spending score doesn't show any frequent rise in the score. \nWe also conclude that <b>age between 20-40</b> have dense and higher Annual Income and the trend decreases down the age. We also see that <b>Spending score</b> is releatively less with higher Annual income (50-75)K compare to 25-50K Annual income. Spending Index (45-60) becomes constant for indiviudal with <b> Annual income between 50-75K dollar </b> and then the spending index increases for higher and lower Annual income. This is weird!\n"},{"metadata":{},"cell_type":"markdown","source":"<b> Checking for the correleation<b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 8)\ncorr=data.corr()\nsns.heatmap(corr)\nplt.title(\"Data correleation\", fontsize=14)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The one with the least inference with each other can be analysized by seeing the color saturity. We see that Age is highly uncorreleated with the spending index. The maximum correleation is represnted by the bright skin colour and least with the black colour. We analyized from the heatmap, that the data is not well correleated\n"},{"metadata":{},"cell_type":"markdown","source":"## <h1> Determing Relationship with the attributes </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Gender'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Gender vs Spending Score', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We conclude that spending score is more distributed in female"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Age'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Age vs Spending Score', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Annual Income (k$)'], data['Spending Score (1-100)'], palette = 'pastel')\nplt.title('Gender vs Spending Score', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Violin plot vs the Box plot\n> a violin plot is more informative than a plain box plot. While a box plot only shows summary statistics such as mean/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot shows the presence of different peaks, their position and relative amplitude."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Gender'], data['Annual Income (k$)'], palette = 'pastel')\nplt.title('Gender vs Annual Income (k$)', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18, 6)\nsns.violinplot(data['Age'], data['Annual Income (k$)'], palette = 'pastel')\nplt.title('Gender vs Annual Income (k$)', fontsize = 14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X=data.iloc[:,:-1]\ny=data.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, n_estimators=300)\nclf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap_values = shap.TreeExplainer(clf).shap_values(X)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values[0], X)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"Age\", shap_values[0], X)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"Gender\", shap_values[0], X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 1 represents Male\n* 0 represents Female"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.dependence_plot('Annual Income (k$)', shap_values[0], X)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <h1> CLUSTERING </h1>"},{"metadata":{},"cell_type":"markdown","source":"K Means Clustering \n> k-means is one of the simplest unsupervised learning algorithms that solve the clustering problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). The main idea is to define k centers, one for each cluster.\n\n> To start with k-means algorithm, you first have to randomly initialize points called the cluster centroids (K). K-means is an iterative algorithm and it does two steps: 1. Cluster assignment 2. Move centroid step.\n\n> 1. Cluster assignment\n\n> the algorithm goes through each of the data points and depending on which cluster is closer, It assigns the data points to one of the three cluster centroids.\n\n> 2. Move centroid\n\n> Here, K-means moves the centroids to the average of the points in a cluster. In other words, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.\n\n> This process is repeated until there is no change in the clusters (or possibly until some other stopping condition is met). K is chosen randomly or by giving specific initial starting points by the user."},{"metadata":{},"cell_type":"markdown","source":"![K means clustering](https://cdn-images-1.medium.com/max/800/0*rrzG3LyOnAvOepbJ.png)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(18,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data['Age'], data[\"Annual Income (k$)\"], data[\"Spending Score (1-100)\"], c='red', s=60)\nax.view_init(30, 185)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending Score (1-100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Implicit objective function in k-Means measures sum of distances of observations from their cluster centroids, called Within-Cluster-Sum-of-Squares (WCSS). This is computed as\n![](https://content.edupristine.com/images/blogs/Beyond_the_k-Means_5.png)\n"},{"metadata":{},"cell_type":"markdown","source":"> where Yi is centroid for observation Xi. By definition, this is geared towards maximizing number of clusters, and in limiting case each data point becomes its own cluster centroid. This is, naturally, neither practical nor desirable. Fig. 2 plots WCSS for k=1.20 and we can see that it continuously drops, indicating more clusters the better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nwcss = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k, init=\"k-means++\")\n    kmeans.fit(data.iloc[:,1:])\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize=(12,6))    \nplt.grid()\nplt.plot(range(1,11),wcss, linewidth=2, color=\"blue\", marker =\"8\")\nplt.xlabel(\"K Value\")\nplt.xticks(np.arange(1,11,1))\nplt.ylabel(\"WCSS\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>K=5</b>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"km = KMeans(n_clusters=5)\nclusters = km.fit_predict(data.iloc[:,1:])\ndata[\"label\"] = clusters\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data.Age[data.label == 0], data[\"Annual Income (k$)\"][data.label == 0], data[\"Spending Score (1-100)\"][data.label == 0], c='blue', s=60)\nax.scatter(data.Age[data.label == 1], data[\"Annual Income (k$)\"][data.label == 1], data[\"Spending Score (1-100)\"][data.label == 1], c='red', s=60)\nax.scatter(data.Age[data.label == 2], data[\"Annual Income (k$)\"][data.label == 2], data[\"Spending Score (1-100)\"][data.label == 2], c='green', s=60)\nax.scatter(data.Age[data.label == 3], data[\"Annual Income (k$)\"][data.label == 3], data[\"Spending Score (1-100)\"][data.label == 3], c='orange', s=60)\nax.scatter(data.Age[data.label == 4], data[\"Annual Income (k$)\"][data.label == 4], data[\"Spending Score (1-100)\"][data.label == 4], c='purple', s=60)\nax.view_init(30, 185)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending Score (1-100)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <h2>Hierarchical CLustering </h2>\n"},{"metadata":{},"cell_type":"markdown","source":"> In hierarchical clustering, we assign each object (data point) to a separate cluster. Then compute the distance (similarity) between each of the clusters and join the two most similar clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Spending Score (1-100)']=data['Spending Score (1-100)'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking every attribute in account"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data,method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When taking Annual Income and Spending Score in account"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,3:5],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking gender and Spending Score in account"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,[1,4]],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking age and spending score in account"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(data.iloc[:,[2,4]],method='ward'))\nplt.title('Dendogram', fontsize=20)\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> End Of Kernel </h1>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}