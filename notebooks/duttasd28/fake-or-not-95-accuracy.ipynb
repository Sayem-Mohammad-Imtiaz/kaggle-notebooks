{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake or not?\nIn this notebook, let us predict if we have a fake news article in our hands or not. \nFake news is a menace, let us create some ways to battle it.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Information about the dataset\nThe data is in two parts.\n\n1. True.csv - true articles\n2. Fake.csv - Fake articles","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dfTrue = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ndfFake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfTrue.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfFake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\n\nstpwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Real News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# generate real word list\nrealNewsWords = [str(i) for i in dfTrue['title']]\nrealWordsString = (\" \".join(realNewsWords)).lower()\nrealWordsString = re.sub(r'[^\\w\\s]', '', realWordsString)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Word cloud\nwc = WordCloud(width = 800, height = 800,\n               stopwords = stpwords,\n              background_color = 'white').generate(realWordsString)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wc); \nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Fake","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Fake word list\nfakeNewsWords = [str(i) for i in dfFake['title']]\nfakeWordsString = (\" \".join(fakeNewsWords)).lower()\nfakeWordsString = re.sub(r'[^\\w\\s]', '', fakeWordsString)\n\n# generate Word cloud\nwc = WordCloud(width = 800, height = 800,\n               stopwords = stpwords,\n              background_color = 'white').generate(fakeWordsString)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wc); \nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the column names\ndfFake.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the  Fake dataset\nfakeData = dfFake.drop(['text', 'subject', 'date'], axis = 1)\nfakeData['Prediction'] = pd.Series([0]*len(fakeData))\nfakeData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the real dataset\nrealData = dfTrue.drop(['text', 'subject', 'date'], axis = 1)\nrealData['Prediction'] = pd.Series([1]*len(realData))\nrealData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate the data\ndata = pd.concat([realData, fakeData], axis = 0, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top of data has real news\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bottom has fake news\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### cleans the text\ndef clean_text(text):\n    ''' Cleans the text data, then returns a list of words'''\n    ps = PorterStemmer()  # Stemmer\n    clean_text = text.lower()  # make all into lower case\n    clean_text = re.sub('[^A-Za-z\\s]+', ' ', clean_text) # remove punctuations and numbers\n    clean_text = clean_text.split() # list of words\n    clean_text = [ps.stem(word) for word in clean_text if not word in stopwords.words('english')] # Stopword removal\n    clean_text = ' '.join(clean_text)\n    \n    return clean_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(\"this is 43 ? i though DONAld troops is missing values sunshine!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[ : , :-1]\ny = data.iloc[ : , -1]\ny.head()  # Separating into dependant and independent features\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newsCorpus = [clean_text(text) for text in X['title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(newsCorpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defined vocabulary size\nvocabSize = 6000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot representation of all the news articles\none_hot_news = [one_hot(text, vocabSize) for text in newsCorpus]\none_hot_news[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get max length\nmax([len(vec) for vec in one_hot_news])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As maximum length is 35, so a vector size of 40 would be enough\n\nPadding ensures that all news articles are of the same length. This ensures that input to the neural network are all of the same length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pad the sentences, make fixed length\nmax_length = 40\nembedded_news = pad_sequences(one_hot_news, padding = 'pre', maxlen = max_length)\nembedded_news[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling Start","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_features_length = 40\nfrom tensorflow.keras.layers import Dropout\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, embedding_features_length, input_length = max_length))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(120))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile the model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test Split\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(embedded_news)\ny = np.array(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\n# Now we fit the model\nmodel.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 5, batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(data = mat, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}