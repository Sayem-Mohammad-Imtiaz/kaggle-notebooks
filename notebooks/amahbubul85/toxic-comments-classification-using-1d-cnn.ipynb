{"cells":[{"metadata":{"id":"MemFGW7ksHC4"},"cell_type":"markdown","source":"<h2 align=center> Toxic Comments Classification using 1D CNN with Keras</h2>","execution_count":null},{"metadata":{"id":"x_XiyvkqfYS0"},"cell_type":"markdown","source":"## Task 1: Import Packages and Functions","execution_count":null},{"metadata":{"id":"PvVbtuzBsHDO","outputId":"5eb3a632-26a2-4d37-e2e4-6e0f32202ad3","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D,GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"_yPEK8PqgKQt"},"cell_type":"markdown","source":"## Task 2: Load and Explore Data","execution_count":null},{"metadata":{"id":"UkOBp_TDsHDj","outputId":"3780a302-1f23-4115-bf02-f06f10da8067","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/cleaned-toxic-comments/train_preprocessed.csv')\ntrain_df.sample(10, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"id":"KLqhFuE2sHDp","outputId":"673cb57d-e7b4-4c31-83e4-c29ecd071200","trusted":true},"cell_type":"code","source":"x = train_df['comment_text'].values\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"dLEAz9Mx7apX","outputId":"eadfbb06-40f3-4fbd-b300-ef5f7f0d94d4","trusted":true},"cell_type":"code","source":"# View few toxic comments\ntrain_df.loc[train_df['toxic']==1].sample(10, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"0hGVtLwo2VlV","outputId":"5ddfdee6-3668-403e-a601-a0de2b786ec9","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train_df['comment_text'].loc[train_df['toxic']==1].values\nwordcloud = WordCloud(\n    width = 640,\n    height = 640,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (12, 8),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qmfnwRYksHDu","outputId":"11b3e928-ae41-4530-eb22-63f5fd8f809d","trusted":true},"cell_type":"code","source":"y = train_df['toxic'].values\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"id":"3-pxZLFQw3VR","outputId":"ea865536-9643-455f-899d-a4ae290af3a1","trusted":true},"cell_type":"code","source":"# Plot frequency of toxic comments\ntrain_df['toxic'].plot(kind='hist', title='Distribution of Toxic Comments');","execution_count":null,"outputs":[]},{"metadata":{"id":"8v5D7BG5w_nV","outputId":"0213470a-f0d3-4077-e784-2e3a2ee09039","trusted":true},"cell_type":"code","source":"train_df['toxic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"IZs1wuFjhxss"},"cell_type":"markdown","source":"## Task 3: Data Prep â€” Tokenize and Pad Text Data","execution_count":null},{"metadata":{"id":"51i_F9iRsHDV","trusted":true},"cell_type":"code","source":"max_features = 20000\nmax_text_length = 400","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\nprint(x_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"id":"wRuNEC_DsHDy","trusted":true},"cell_type":"code","source":"x_tokenizer = tf.keras.preprocessing.text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))\nx_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \nx_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)","execution_count":null,"outputs":[]},{"metadata":{"id":"Wki7nrWgiRTp"},"cell_type":"markdown","source":"## Task 4: Prepare Embedding Matrix with Pre-trained GloVe Embeddings","execution_count":null},{"metadata":{"id":"08VYEZ8DqwRH","trusted":true},"cell_type":"code","source":"embedding_dims = 100\nembeddings_index = dict()\nf = open('../input/glove6b/glove.6B.100d.txt')\nfor line in f:\n  values = line.split()\n  word = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.zeros((max_features, embedding_dims))\nfor word, index in x_tokenizer.word_index.items():\n  if index > max_features -1:\n    break\n  else:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"id":"9ikSokUmeptj"},"cell_type":"markdown","source":"## Task 5: Create Embedding Layer","execution_count":null},{"metadata":{"id":"wp8GJRoesHD9","trusted":true},"cell_type":"code","source":"print('Build model...')\nmodel = Sequential()\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\n#load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\n#(we don't want to update them during training).\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                    trainable=False))\nmodel.add(Dropout(0.2))","execution_count":null,"outputs":[]},{"metadata":{"id":"8_ttnzcroOex"},"cell_type":"markdown","source":"### Task 6: Build the Model","execution_count":null},{"metadata":{"id":"mHxD6duxf5r5","trusted":true},"cell_type":"code","source":"filters = 250\nkernel_size = 3\nhidden_dims = 250","execution_count":null,"outputs":[]},{"metadata":{"id":"vhDOsCOXoUjB","trusted":true},"cell_type":"code","source":"# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters,\n                 5,\n                 padding='valid',\n                 activation='relu'))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# We project onto 6 output layers, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"TwESlTgLsLP2","trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"yds9-uJbeygs"},"cell_type":"markdown","source":"## Task 6: Train Model","execution_count":null},{"metadata":{"id":"jBAmWQbLsHD4","trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.15, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"edW2v5B9UipJ","trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 3","execution_count":null,"outputs":[]},{"metadata":{"id":"ZDQkgmhOsHED","outputId":"bc527adf-9f6d-4c3f-f7a0-91f5f0790a2f","trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"NFFWvo5nfGAP"},"cell_type":"markdown","source":"## Task 7: Evaluate Model","execution_count":null},{"metadata":{"id":"9J54v6q0UEVM","outputId":"7e6fdd2a-6a7f-422a-e1bf-aac493c32b9e","trusted":true},"cell_type":"code","source":"model.evaluate(x_val, y_val, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"id":"QluBE6zqvUao"},"cell_type":"markdown","source":"## (Optional) Task 8: Visualize Embeddings","execution_count":null},{"metadata":{"id":"D0h9uKhIJQkV","trusted":true},"cell_type":"code","source":"word_index = x_tokenizer.word_index\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","execution_count":null,"outputs":[]},{"metadata":{"id":"ZPDbc7odLWpi","outputId":"0bb6d90b-e1a1-4a6a-b560-663d023027a7","trusted":true},"cell_type":"code","source":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"gsVgBaKjJ3QN","trusted":true},"cell_type":"code","source":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\n\nfor word_num in range(max_features):\n  word = reverse_word_index[word_num+1]\n  embeddings = weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}