{"cells":[{"metadata":{"_uuid":"006bb929-603c-4aea-b4b9-00f7ac16deba","_cell_guid":"2a47bf22-5d9f-4508-8f69-576d517b6477","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport PIL\nimport tensorflow as tf\nimport pathlib\nimport ipywidgets as widgets\nimport io\nfrom PIL import Image\n\nfrom PIL import ImageShow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33e461cd-a3c9-4344-8996-5a427bb15f7f","_cell_guid":"c8a77c57-4a3f-467f-ac61-1a14345a107f","trusted":true},"cell_type":"markdown","source":"# Download Dataset"},{"metadata":{"_uuid":"1c3b4451-756b-44eb-981d-11b150a2bb5c","_cell_guid":"830830c6-39d4-485f-911f-3227b880821a","trusted":true},"cell_type":"code","source":"my_data = pd.read_csv(\"../input/honey-bee-annotated-images/bee_data.csv\")\ndataset_path = \"../input/bee-images-separated/bee_imgs\"\ndata_dir = pathlib.Path(dataset_path)\n\nhealthy = list(data_dir.glob('healthy/*'))\nPIL.Image.open(str(healthy[0]))\n\n#Use to print out how many photos are in all the photos in the bee_imgs directory\n#image_count = len(list(data_dir.glob('*/*.png')))\n#print(image_count)\n\n#use to print out how many photos per category in the CSV\n#print(my_data['health'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ca48b3a-0a99-41fa-a76c-31878a0f1220","_cell_guid":"44d6ce7f-2b4a-4ff2-9afc-0d3bbe401850","trusted":true},"cell_type":"markdown","source":"# Create Dataset\nValidation split splits the images into 80% for training and 20% for validation"},{"metadata":{"_uuid":"b9f1d054-bcdd-42d5-b258-cd42da8d8e29","_cell_guid":"9eaf68a5-656e-4d4b-bdcd-dbb25627648b","trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_height = 180\nimg_width = 180\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)\n\nclass_names = train_ds.class_names\n#Prints how many classes are in the new dataset\n#print(class_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b82314ae-4814-4a3b-89b5-f8e042735b8c","_cell_guid":"0c1b7ab0-a225-41ae-9ce0-32f63ee186b5","trusted":true},"cell_type":"markdown","source":"# Visualize Dataset\nUncomment to see a sampling of 9 random images that exist in the training data"},{"metadata":{"_uuid":"402366f7-0e1a-4045-9f5e-6ed7a7f10eca","_cell_guid":"7b5075f1-3667-4ac7-8ad0-bb8b2b720f4c","trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(10, 10))\n# for images, labels in train_ds.take(1):\n#     for i in range(9):\n#         ax = plt.subplot(3, 3, i + 1)\n#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n#         plt.title(class_names[labels[i]])\n#         plt.axis(\"off\")\n#         plt.show() #NEEDED. Will not display images unless this is added.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c1e64bb-4ec5-42a2-a126-f124d910c077","_cell_guid":"9ac55771-2d37-45cd-b86e-e7391f79b013","trusted":true},"cell_type":"markdown","source":"# Configure Dataset\ndataset.cache(): keeps images in memory after they're loaded off the disk for the first epoch\n<br>dataset.prefetch(): overlaps data preprocessing and model execution while training"},{"metadata":{"_uuid":"d91d04ed-f8fe-45f9-b1ec-d874b2fbd0d3","_cell_guid":"2c6a30a8-9444-4f76-8c1a-3574d6a58e0c","trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e2d8d33-58ec-4f01-a8c6-f3c417b11a53","_cell_guid":"093b3ee5-8727-4c3c-8e77-286dca56b930","trusted":true},"cell_type":"markdown","source":"# Standardize Dataset"},{"metadata":{"_uuid":"8fa5f9df-ccbf-4937-90c8-8d44e7cd0bc2","_cell_guid":"94340875-76ba-4014-a6bf-bd5c0ee7cba8","trusted":true},"cell_type":"code","source":"normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n\nnormalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nimage_batch, labels_batch = next(iter(normalized_ds))\nfirst_image = image_batch[0]\nprint(np.min(first_image), np.max(first_image))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d258c59-58f3-4445-9853-c844ca06196b","_cell_guid":"e67f50a6-9d99-4f76-8dac-97b0affb2b20","trusted":true},"cell_type":"markdown","source":"# Create Model\nConsists of three convolutional blocks with each having a max pooling layers\n<br>The fully connected layer is activated by the ReLU function"},{"metadata":{"_uuid":"d5b40474-b260-4e39-bfd3-5f4bf8335c51","_cell_guid":"f4ca4acc-cc96-43f4-abd7-30b05ca57a8a","trusted":true},"cell_type":"code","source":"num_classes = 6\n\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8d4c61-d578-4d52-9098-0006b35ba13c","_cell_guid":"c0339343-5eee-47ef-897f-c5e7ad2d43c5","trusted":true},"cell_type":"markdown","source":"# Compile Data\nUse Adam optimizer and the Sparse Categorical Croessentropy loss function"},{"metadata":{"_uuid":"a83d4151-f894-48c2-a1d3-6a594447de51","_cell_guid":"3927cc7f-dc0c-4627-add8-2e910220a7e8","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b1043e9-0e2a-420e-87be-2f94e273a750","_cell_guid":"544e7fba-f655-471e-a6a4-7970ef3f6fb0","trusted":true},"cell_type":"markdown","source":"# Model Summary"},{"metadata":{"_uuid":"6bd47d34-56ea-4fd7-b092-cfe4cbce5a06","_cell_guid":"5a68b18f-a2b0-4592-b81e-751bc376d6ef","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90aa9e0-ec0a-4f9f-9d48-175972a23f78","_cell_guid":"9a9cf734-e971-42d2-96b9-286924c2f668","trusted":true},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"_uuid":"880a97ac-dfd0-4cf2-ac73-0a378ba90c99","_cell_guid":"b4a91fd9-55d4-4276-ae83-6be9d5d6f0e3","trusted":true},"cell_type":"code","source":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b6120a5-e0c5-4fb6-8b1b-830610543926","_cell_guid":"7053db15-77bb-4ee8-9a0f-42839fe3f088","trusted":true},"cell_type":"markdown","source":"# Visualize Training Results"},{"metadata":{"_uuid":"fd5e5ee3-8b29-45f8-b7ef-85a8065a91b1","_cell_guid":"4575c8e7-cbe4-4f0b-9fed-6dae29028a87","trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24a993c8-5c8c-40f1-a2fb-d8a7ca14f2d8","_cell_guid":"c6da38f5-d546-453f-8afb-f5f43e638d3f","trusted":true},"cell_type":"markdown","source":"# Predict on New data"},{"metadata":{"_uuid":"d015d49f-895e-45cc-a328-e2a2086b4c1c","_cell_guid":"2362058c-8936-42a2-b721-1b3bcdd349d9","trusted":true},"cell_type":"markdown","source":"Use the commented paths below to check different types of bee png's from the predictions folder. The prediction folder contains different images to how well the data predicts the following:\n* Bee drawing\n![bee_drawing.jpg](attachment:bee_drawing.jpg)\n* Yellow and Black stripes\n![bee_pattern.jpg](attachment:bee_pattern.jpg)\n* Healthy Bees\n![001_054.png](attachment:001_054.png)\n![healthy_bee_side_profile.jpg](attachment:healthy_bee_side_profile.jpg)\n![healthy_bee_top_view.jpg](attachment:healthy_bee_top_view.jpg)\n* Bees with pollen (not being robbed)\n![honey_bee_not_robbed_2.jpg](attachment:honey_bee_not_robbed_2.jpg)\n![honey_bee_not_robbed_1.jpg](attachment:honey_bee_not_robbed_1.jpg)\n* Bees with deformed wings\n![unhealthy_bee_2.jpg](attachment:unhealthy_bee_2.jpg)\n![unhealthy_bee_1.jpg](attachment:unhealthy_bee_1.jpg)\n* Bees visibly infected with varroa mites\n![varroa_infected_3.jpg](attachment:varroa_infected_3.jpg)\n![varroa_infected_2.jpg](attachment:varroa_infected_2.jpg)\n![varroa_infected_1.jpg](attachment:varroa_infected_1.jpg)"},{"metadata":{"_uuid":"686b43a0-d8f1-49b0-8e54-9d7f940fea3d","_cell_guid":"6038cf69-993f-4c9e-94a8-9d02b27a6851","trusted":true},"cell_type":"code","source":"#Bee Drawing\nbee_drawing = \"../input/new-data-for-predictions/Predictions/bee_drawing.png\"\n\n#Yellow and Black Stripes\nbee_pattern = \"../input/new-data-for-predictions/Predictions/bee_pattern.png\"\n\n#Healthy Bees\nhealthy_bee_from_data  = \"../input/bee-images-separated/bee_imgs/healthy/001_054.png\"\nhealthy_bee_side = \"../input/new-data-for-predictions/Predictions/healthy_bee_side_profile.png\"\nhealthy_bee_top = \"../input/new-data-for-predictions/Predictions/healthy_bee_top_view.png\"\n\n#Bees with Pollen\nbee_with_pollen_1 = \"../input/new-data-for-predictions/Predictions/honey_bee_not_robbed_1.png\"\nbee_with_pollen_2 = \"../input/new-data-for-predictions/Predictions/honey_bee_not_robbed_2.png\"\n\n#Bees with Deformed Wings\nunhealthy_bee_1 = \"../input/new-data-for-predictions/Predictions/unhealthy_bee_1.png\"\nunhealthy_bee_2 = \"../input/new-data-for-predictions/Predictions/unhealthy_bee_2.png\"\n\n#Bees Infected with Varroa Mites\nvarroa_infected_1 = \"../input/new-data-for-predictions/Predictions/varroa_infected_1.png\"\nvarroa_infected_2 = \"../input/new-data-for-predictions/Predictions/varroa_infected_2.png\"\nvarroa_infected_3 = \"../input/new-data-for-predictions/Predictions/varroa_infected_3.png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace the file path with any from the previous section and run the section below to see the results\nbee_img = \"../input/new-data-for-predictions/Predictions/honey_bee_not_robbed_1.png\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"961bf34a-6af7-48dd-8274-0d76d498fd47","_cell_guid":"e0d394fb-5f8f-4a29-8ecb-7828e7bd69e7","trusted":true},"cell_type":"code","source":"#For pictures off the internet\n#_url = \"https://\"\n#_path = tf.keras.utils.get_file('File_name', origin=_url)\n\n\nimg = keras.preprocessing.image.load_img(\n    bee_img, target_size=(img_height, img_width)\n)\n\nimg_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nscore = tf.nn.softmax(predictions[0])\n\nprint(\n    \"This image most likely belongs to the category {} with a {:.2f} percent confidence.\"\n    .format(class_names[np.argmax(score)], 100 * np.max(score))\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b65e9cd7-82a5-4f01-9ae7-a047943ae4f7","_cell_guid":"000602d7-7821-4038-b7fa-e24c6cfd0474","trusted":true},"cell_type":"markdown","source":"# Predictions Results:\n <br>bee_drawing = **ants** with a **99.83** percent confidence.\n <br><br>bee_pattern = **robbed** with a **100.00** percent confidence.\n <br><br>healthy_bee_from_data = **healthy** with a **99.95** percent confidence.\n <br><br>healthy_bee_side = **ants** with a **90.64** percent confidence.\n <br><br>healthy_bee_top = **healthy** with a **99.84** percent confidence.\n <br><br>bee_with_pollen_1 = **ants** with a **99.08** percent confidence.\n <br><br>bee_with_pollen_2 = **few_var** with a **88.72** percent confidence.\n <br><br>unhealthy_bee_1 = **ants** with a **100.00** percent confidence.\n <br><br>unhealthy_bee_2= **robbed** with a **99.99** percent confidence.\n <br><br>varroa_infected_1 = **healthy** with a **100.00** percent confidence.\n <br><br>varroa_infected_2 = **healthy** with a **100.00** percent confidence.\n <br><br>varroa_infected_3 =**ants** with a **100.00** percent confidence."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}