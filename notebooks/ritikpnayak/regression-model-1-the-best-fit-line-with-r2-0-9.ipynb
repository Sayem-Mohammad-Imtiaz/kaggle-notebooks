{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Disclaimer"},{"metadata":{},"cell_type":"markdown","source":"***It is the first part of a series of notebooks meant to highlight my work on making a regression model and testing it on the data. I would like to point out that many of the concepts that I use in this notebook is already discussed and explained in my previous notebooks. Therefore, if the audience want to learn more about the concepts I use, they can visit my previous notebooks, the links of which are also given below. Nonetheless, I would say that the code in my notebook is self explanatory that would not require much reading.***"},{"metadata":{},"cell_type":"markdown","source":"# Previous notebooks on the same dataset:"},{"metadata":{},"cell_type":"markdown","source":"1. [Elemental approach to finding correlation](https://www.kaggle.com/ritikpnayak/elemental-approach-to-finding-correlation)\n\n2. [Computing the magnitude of skewness in Maths score](https://www.kaggle.com/ritikpnayak/computing-the-magnitude-of-skewness-in-maths-score)"},{"metadata":{},"cell_type":"markdown","source":"# Previous notebooks on the same subject:"},{"metadata":{},"cell_type":"markdown","source":"1. [Introduction to Hypothesis Testing and Estimation](https://www.kaggle.com/ritikpnayak/introduction-to-hypothesis-testing-and-estimation)"},{"metadata":{},"cell_type":"markdown","source":"# Here we go!"},{"metadata":{},"cell_type":"markdown","source":"# ***Throughout the series of notebooks, I'll perform regression analysis to predict the reading scores using the writing scores***"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Is correlation \"by chance\"?"},{"metadata":{},"cell_type":"markdown","source":"1. In my previous notebook, I found that there is a very strong correlation between the reading scores and the writing scores\n2. The correlation is linear and its magnitude (Pearson's coefficient) is greater that 0.9 .\n3. In this section, we'll analyse if the correlation is true for the population (whole of the data) or is it merely by chance.\n4. I'll perform Hypothesis Testing for that purpose.\n\nPlease refer to my notebook; [Elemental approach to finding correlation](https://www.kaggle.com/ritikpnayak/elemental-approach-to-finding-correlation) to see the computation of correlation\n\nAlso refer to my notebook; [Introduction to Hypothesis Testing and Estimation](https://www.kaggle.com/ritikpnayak/introduction-to-hypothesis-testing-and-estimation) to learn more about Hypothesis Testing"},{"metadata":{},"cell_type":"markdown","source":"***What is the Null Hypothesis?***\n\n1. ***Our Null Hypothesis:*** The correlation of magnitude as strong as 0.9 has occured ***by chance***.\n2. If the p value for the null hypothesis turns out to be very small, the null hypothesis would be rejected.\n3. I wish the null hypothesis stands false for I really want that the correlation of such a strong magnitude to be statistically significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"class HypothesisTest(object):\n    \n    def __init__(self, data):\n        self.data = data\n        self.MakeModel()\n        self.actual = self.TestStatistic(data)\n        \n    def PValue(self, iters = 1000):\n        self.test_stats = [self.TestStatistic(self.RunModel()) for _ in range(iters)]\n        count = sum(1 for x in self.test_stats if x >= self.actual)\n        return count / iters\n    \n    def TestStatistic(self, data):\n        raise UnimplementedMethodException()\n        \n    def MakeModel(self):\n        pass\n    \n    def RunModel(self):\n        raise UnimplementedMethodException()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CorrPermute(HypothesisTest):\n    \n    def TestStatistic(self, data):\n        xs, ys = data\n        test_stat = abs(correlation(xs, ys))\n        return test_stat\n    \n    def RunModel(self):\n        xs, ys = self.data\n        xs = np.random.permutation(xs)\n        return xs, ys\n    \ndef correlation(x, y):\n    std_x = np.std(x)\n    std_y = np.std(y)\n    if std_x and std_y > 0:\n        return covariance(x, y) / std_x / std_y\n    else:\n        return 0\n    \ndef de_mean(x):\n    x_bar = np.mean(x)\n    return [x_i - x_bar for x_i in x]\n\ndef covariance(x, y):\n    n = len(x)\n    return np.dot(de_mean(x), de_mean(y)) / (n - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = df['reading score']\nws = df['writing score']\n\nct = CorrPermute((rs, ws))\npvalue = ct.PValue()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The P-Value is: ', pvalue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What is the conclusion?***\n\n1. The P value for the null hypothesis is 0\n2. This means that the null hypothesis is FALSE and that the magnitude as strong as 0.9 of the correlation between the 2 variables (reading scores and writing scores) is unlikely to have occured by chance.\n3. In other words, we can say that the correlation is statistically significant."},{"metadata":{},"cell_type":"markdown","source":"# 2. Using Linear Least Squares "},{"metadata":{},"cell_type":"markdown","source":"***What are linear least squares?***\n\n1. The correlation measures the strength and sign of a relationship.\n2. It doesn't measure the slope.\n3. There are several ways to measure the slope.\n4. The most common method is a \"linear least squares fit\".\n5. It is the one that minimizes the \"mean squared error (MSE)\".\n6. We'll use the concept of the general equation of a line; ***y = mx + c***\n\nTo know more about it, please refer to this link: [Linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares#:~:text=Linear%20least%20squares%20(LLS)%20is,and%20generalized%20(correlated)%20residuals.)"},{"metadata":{},"cell_type":"markdown","source":"***We'll get the intercept and slope of the line (using, of course, linear least squares) that we'll use to predict the reading scores***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LeastSquares(xs, ys):\n    meanx, varx = np.mean(xs), np.var(xs)\n    meany = np.mean(ys)\n    \n    slope = covariance(xs, ys) / varx\n    inter = meany - slope * meanx\n    \n    return inter, slope\n\ndef FitLine(xs, inter, slope):\n    fit_xs = np.sort(xs)\n    fit_ys = inter + slope * fit_xs\n    \n    return fit_xs, fit_ys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter, slope = LeastSquares(ws, rs)\nfit_xs, fit_ys = FitLine(ws, inter, slope)\n\nprint('intercept is: {} and slope is: {}'.format(inter, slope))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Residuals"},{"metadata":{},"cell_type":"markdown","source":"***What are residuals?***\n\n1. It is the deviation of the fitted values of y from the actual values of y.\n2. In our example, the actual values of y is in ys whereas the fitted values of y is in fit_ys.\n3. The actual equation of the line that we use in regression is; ***ys = intercept + slope * xs + residuals***\n\nTo know more about it, please refer to this link: [Errors and residuals\n](https://en.wikipedia.org/wiki/Errors_and_residuals)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Residuals(xs, ys, inter, slope):\n    xs = np.asarray(xs)\n    ys = np.asarray(ys)\n    res = ys - (inter + slope * xs)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = Residuals(ws, rs, inter, slope)\n\ndf['residuals'] = res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Is it good to predict the reading scores with the writing scores or without it?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSE if we predict the reading scores using the writing scores: ', np.std(res))\nprint('RMSE if we predict the reading scores without using the writing scores: ', np.std(rs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***How to interpret the above values?***\n\n1. The \"Root Mean Square Error (RMSE)\" is more than 4 ***if we use the writing scores*** to predict the values of the reading scores.\n2. The \"Root Mean Square Error (RMSE)\" is more than 14 ***if we do not use the writing scores*** to predict the values of the reading scores.\n\n***Therefore, as the RMSE is less in the former situation, it would be better off to use the writing scores to predict the reading scores***"},{"metadata":{},"cell_type":"markdown","source":"# 4. Coefficient of Determination"},{"metadata":{},"cell_type":"markdown","source":"***What is it?***\n\n1. Denoted by R^2 or \"R squared\".\n2. It is a metric used to determine how good our model is."},{"metadata":{"trusted":true},"cell_type":"code","source":"r_squared = 1 - (np.var(res) / np.var(rs))\n\nprint('The Coefficient of Determination or r^2 is: ', r_squared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***The value of R^2 is more than 0.9, which means that our line is a perfect fit for the data. We can use this line to predict the reading scores.***"},{"metadata":{},"cell_type":"markdown","source":"# 5. Plotting the line"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\nplt.xlabel('Writing scores')\nplt.ylabel('Reading scores')\n\nplt.plot(fit_xs, fit_ys, color = 'black')\nplt.scatter(ws, rs, color = 'green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***As expected, our line fits the data well.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}