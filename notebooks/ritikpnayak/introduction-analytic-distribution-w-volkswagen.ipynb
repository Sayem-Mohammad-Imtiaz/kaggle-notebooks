{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"![Volkswagen](http://www.investopedia.com/thmb/5zbLZHbrLNwpLLbZjbteyvgDdY4=/1024x683/filters:fill(auto,1)/GettyImages-1135311347-4aabd8ab95354b9f9eae9e1d8b61ee33.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# 1.1. What are analytic distributions? "},{"metadata":{},"cell_type":"markdown","source":"To employ the dictionary meaning of distribution, it is; \"the way in which something is shared out among a group or spread over an area\". Histograms, mean, variance, effect size are a few examples of distribution. One thing that is common in these kind of distributions is that they are based on empirical observations. These along with Probability mass functions and Cumulative distribution functions are known as empirical distributions.\n\nIn contast to empirical distributions, the analytic distributions are the ones that are charcterized by a CDF or a PDF. The analytic distribution can be used to model the empirical distributions. Some examples of analytic distribution are normal distribution, exponential distribution, lognormal distribution, etc. In this notebook, I'm going to model certain variables from the dataset and check if that model fits in any of the analytic distribution. That said, I may wave hands at many of the concepts and use the concept quite implicitly, for that is the purpose of the notebook."},{"metadata":{},"cell_type":"markdown","source":"# 1.2. Knowing my data"},{"metadata":{},"cell_type":"markdown","source":"I have have the necessary information, sufficient enough to carry out an anlysis with a few variables (columns) of the dataset, still for the benefit of the audience, I'm affixing the link to the dataset\n\n[100,000 UK Used Car Data set](http://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes)\n\nThe dataset has 13 sub datasets. For this notebook, one such dataset is required. I'll therefore use the data for the Volkswagen car.\n\nThere are 9 variables in the dataset but I'll use 3 of them, namely;\n1. year: the year in which the model (of the car) is registered.\n2. price: price of the vehicle in Euro.\n3. fuelType: the type of engine fuel.\n\nFor my purpose, I'll only use data from one year and for one fuel type, namely; 2019 and Petrol, respectively."},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing the libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\nfrom scipy import stats\nfrom scipy.stats import norm\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"vw = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/vw.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vw.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vw.fuelType.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol = vw[(vw['year'] == 2019) & (vw['fuelType'] == 'Petrol')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol_price = t19_petrol.price.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Transforming the data"},{"metadata":{},"cell_type":"markdown","source":"I'm transforming (modifying) one variable of the dataset; \"price\". I'm dividing the prices by 1000 and sorting. I'm doing this because, the price in its usual form produce a cdf value of 0.0 each. After the transformation, a value, say 32000 euros become 3.2K euros."},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol_price = sorted(t19_petrol_price / 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Brief introduction to CDF"},{"metadata":{},"cell_type":"markdown","source":"Let's say we have a sample of 10 values and we want to find the percentile rank of a number 'x' that may appear in the sample, then we say that the percentile is the percentage of values in the sample that are less than or equal to that of 'x'. \n\nA cumulative distribution function is a normalized percentile rank. That is, we define the CDF on a scale of 0-1, whereas the percentile rank is measured on a 0-100 scale.\n\nWe can use the following function to define calculate the cdf of a sample. In this scenario, the sample would be the price of various models (petrol) of the Volkswagen car that are launched in the year 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"def EvalCdf(sample, x):\n    count = 0\n    for i in sample:\n        if i <= x:\n            count += 1\n    prob = count / len(sample)\n    \n    return prob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.1. Applying the cdf on the sample"},{"metadata":{},"cell_type":"markdown","source":"Now that we have with us both the sample and the function needed to convert that sample into cumulative values (normalized percentile ranks), we will straight away apply that function on each value of that sample and plot the distribution using matplolib library's line plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol_price_cdf = []\n\nfor i in t19_petrol_price:\n    t19_petrol_price_cdf.append(EvalCdf(t19_petrol_price, i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\n\nplt.xlabel('Price')\nplt.ylabel('CDF')\nplt.plot(t19_petrol_price, t19_petrol_price_cdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After plotting the price v/s cdf (price with their corresponding cumulative distribution values), we can say that the shape of the plot is nearly sigmoid.\n\nNow our remaining discourse would be upon the plot that surfaced. Many of you might wonder, why would that happen? We have the plot, now infer the obvious and move on. \nWhilst it is true that I will infer some conclusions out of the plot and move on, I will do that through a different process. \n\nThe sigmoid shape of the plot must have disguised a more useful insight that needs to be acknowledged. "},{"metadata":{},"cell_type":"markdown","source":"# 5. Fitting into a different model"},{"metadata":{},"cell_type":"markdown","source":"Plotting the graph of one variable must be an example of empirical distribution for we haven't done anything hypothetical as of now, however the study of the plot would be done using the concepts of analytic distribution.\n\n**What does the shape indicate?**\n\nThe shape of the plot is approximately sigmoid which is also the shape of the cdf of the normal and lognormal distribution. But how do we know? The shape may resemble to that of a *normal* or *lognormal* distribution but is there any way to cross check or double check? Yes is the answer."},{"metadata":{},"cell_type":"markdown","source":"# 6. Normal Probability Plot"},{"metadata":{},"cell_type":"markdown","source":"# 6.1. For normal distribution"},{"metadata":{},"cell_type":"markdown","source":"To check whether the distribution is approximately normal, we can use the normal probability plot. In a normal probability plot, we plot ordered values (the values in our sample, that is price) and the theoretical values (the values from a sample standard normal distribution, that is mean = 0 and standard deviation = 1). \n\nIf the distribution of the sample is approximately normal, the result is a straight line with intercept mu and slope sigma.\n\nIn Python, ***scipy.stats provides us with an inbuilt method called \"probplot\"*** which not only does the calculations and ransformations but also plots the values."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\n#plt.plot(stats.probplot(t19_petrol_price)[0][0], stats.probplot(t19_petrol_price)[0][1])\nstats.probplot(t19_petrol_price, plot = plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What does the plot tell us?**\n\nAs we can see, the plot is not exactly how it should look like (the red straight line). Albeit, most of the figure aligns with the straight line, both of its tail deviate substantially from the straight line, which indicates that the curve matches near the mean and deviates in the tails."},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(stats.probplot(t19_petrol_price)[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t19_petrol_price[-5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.probplot(t19_petrol_price)[0][1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.2. For lognormal distribution"},{"metadata":{},"cell_type":"markdown","source":"Normal probability plots are also used to check whether a distribution is lognormal. It is wise to ponder that the distribution may fit into the lognormal distribution because the shape of the cdf of a lognormal distribution is also sigmoid.\n\nThe concept is same, but the only difference is that the we employ the base 10 logarithm values for the ordered values (price in our case, which is the y-axis)."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_ordered_values = [math.log(x) for x in stats.probplot(t19_petrol_price)[0][1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\nstats.probplot(log_ordered_values, plot = plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What does the plot tell us?**\n\nThe plot is slightly different from that of the previous one. The curve matches near the mean and \"slightly\" deviates in the tails. This portrays that the distribution, more precisely, fit the lognormal distribution. "},{"metadata":{},"cell_type":"markdown","source":"# 7. Which model is the best?"},{"metadata":{},"cell_type":"markdown","source":"Frankly speaking, the answer to the question; \"which model is best\" depends on what do we want to infer? In that case, I leave it for my generous audience to find out the rather ambiguous applications of each of the normal and lognormal distributions. This will end up in a lot of brainstorming but that will be fun. I liked investing time in this exercise.  "},{"metadata":{},"cell_type":"markdown","source":"# 8. Epilogue"},{"metadata":{},"cell_type":"markdown","source":"Analytic models leave out details that are unneeded and unnecessary for our purposes. That said, it is important to recall that many real world phenomena can be modelled with analytic distributions. These models smooth out measurement errors from the observed distribution. When an analytic model fits a dataset, a large amount of data can be summarized using a small set of parameters. \n\nAt the same time, it is important to be aware of the fact that no model is perfect. \n\"Models are useful if they capture the relevant aspects of the real world and\nleave out unneeded details. But what is “relevant” or “unneeded” depends\non what you are planning to use the model for.\" - Allen B. Downey in ThinkStats book\n\nWhat is the purpose of this notebook?\n\nI was exploring these concepts and was eager to apply them on datasets. I think this is a good way to learn. A lot of things can be done on this kind of dataset, but as someone rightly pointed out; \"many a little makes a mickle\", understanding concepts in little portions and applying them in real world is the best way to learn. That sais, I wish to come up with more notebooks in tandem with my work on these datasets.\n\nPlease give your valuable feedback :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}