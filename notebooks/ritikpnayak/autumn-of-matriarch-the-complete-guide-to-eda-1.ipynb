{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Disclaimer"},{"metadata":{},"cell_type":"markdown","source":"***In the series; \"Autumn of Matriarch\" based on the \"Women Entrepreneurship and Labor Force\" dataset, I will guide and present my work for fellow Kagglers to enact an effective Exploratory Data Analysis. My approach, throughout the series would be, as many may point out, \"a statistical analysis\". I hope the notebooks fing the appropriate audience.***"},{"metadata":{},"cell_type":"markdown","source":"# Importing the data and libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom collections import Counter\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/women-entrepreneurship-and-labor-force/Dataset3.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Frame"},{"metadata":{},"cell_type":"markdown","source":"I need not acknowledge the very meaning of a dataframe. I would rather import and perform a couple of abatements in the dataframe which are as follows:\n\n1. Cleaning and\n2. Transformation"},{"metadata":{},"cell_type":"markdown","source":"***How to clean this specific set of data?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Is the data clean?***\n\nNo, becuase the names of the columns and their values are not apparent.\n\n***How do we clean that?***\n\n1. In this specific dataset, a minor change while importing would do the job. \n2. It looks like the dataframe is ***not separated with commas (,) but with semi colons (;)***.\n3. To do away with that, we can use the ***delimiter attribute*** of the ***pandas.read_csv function***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/women-entrepreneurship-and-labor-force/Dataset3.csv', delimiter = ';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe now looks good"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.1. Transformation"},{"metadata":{},"cell_type":"markdown","source":"***What transformations do we require to do on this data?***\n\nI'll specifically change the name of the columns to less words in small alphabets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns = {'Level of development':'lod',\n                    'European Union Membership':'eum',\n                    'Currency':'currency',\n                    'Women Entrepreneurship Index':'wei',\n                    'Entrepreneurship Index':'ei',\n                    'Inflation rate':'ir',\n                    'Female Labor Force Participation Rate':'flfp',\n                    'Country':'country',\n                    'No':'no'},\n         inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe now looks like this."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.eum.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.lod.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.currency.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Distributions"},{"metadata":{},"cell_type":"markdown","source":"***What is distributions?***\n\n1. It is the share of the data that every group of the dataframe has.\n2. Put differently, it is the visualization of the footprint of each value of a variable/column of the dataframe.\n\n***How do we report it?***\n\nI'll use 3 methods:\n\n1. Histograms\n2. Probability mass function\n3. Cumulative distribution function"},{"metadata":{},"cell_type":"markdown","source":"# 2.1. Histograms"},{"metadata":{},"cell_type":"markdown","source":"***Why Histograms?***\n\n1. One of the best ways to describe a variable.\n2. It reports the number of times each value of a variable appear in the dataset. "},{"metadata":{},"cell_type":"markdown","source":"I'll plot the histograms in pairs to see the relationship between variables. That is to say, I'll check if the same values have same or exact opposite distributions in the 2 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df.wei, label = 'wei', alpha = 0.5)\nplt.hist(df.ei, label = 'ei', alpha = 0.8)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df.flfp, label = 'flfp', alpha = 0.5)\nplt.hist(df.ei, label = 'ei', alpha = 0.8)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"developed = df[df.lod == 'Developed']\ndeveloping = df[df.lod == 'Developing']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(developed.flfp, label = 'flfp', alpha = 0.5)\nplt.hist(developed.ei, label = 'ei', alpha = 0.8)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(developing.flfp, label = 'flfp', alpha = 0.5)\nplt.hist(developing.ei, label = 'ei', alpha = 0.8)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What is the conclusion?***\n\nIn all three of the histograms, I see no apparent relation between variables. Nevertheless, we'll find the correlation further in the series as well."},{"metadata":{},"cell_type":"markdown","source":"# 2.2. Outliers and skewness"},{"metadata":{},"cell_type":"markdown","source":"***What are Outliers?***\n\n1. The values in the data that are either too large or too small.\n2. The outliers are values that are far off from the mean and median of the data.\n3. They can directly affect the mean, because it is takes into account the sum of all values in the data, but not the median.\n4. Recognizing the outliers is very important because in its presence, the mean of the data may be very misleading."},{"metadata":{},"cell_type":"markdown","source":"***How to recognize the outliers?***\n\n1. Recognizing the outliers is premised upon the general notion that most of the values of a distribution lie in the range of (mean - standard dev) and (mean + standard dev).\n2. Therefore, the values below (mean - standard dev) and the values above (mean + standard dev) are all outliers. \n\nAn example is shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Big outliers:')\nfor i in df.ei:\n    if i > np.mean(df.ei) + np.std(df.ei):\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Small outliers:')\nfor i in df.ei:\n    if i < np.mean(df.ei) - np.std(df.ei):\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What is the range of outliers in the variables; 'wei', 'ei', and 'flfp'?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Outliers in the column wei are the values below; {}, and above; {}: '.format(np.mean(df.wei) - np.std(df.wei), np.mean(df.wei) + np.std(df.wei)))\nprint('Outliers in the column ei are the values below; {}, and above; {}: '.format(np.mean(df.ei) - np.std(df.ei), np.mean(df.ei) + np.std(df.ei)))\nprint('Outliers in the column flfp are the values below; {}, and above; {}: '.format(np.mean(df.flfp) - np.std(df.flfp), np.mean(df.flfp) + np.std(df.flfp)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What is skewness?***\n\n1. It is the measure of the assymetry in our distribution.\n2. It can be used to detect outliers. \n3. Positive skewness means that the tail extends more to the right.\n4. Negative skewness means that the tail extends more to the left"},{"metadata":{},"cell_type":"markdown","source":"***How to compute skewness?***\n\nI'll employ ***\"Pearson's median coefficient\"*** to compute the strength of the skewness, because;\n\n1. It's more efficient.\n2. Based on the difference between the sample mean and the median.\n\n***What are the other methods?***\n\nThe other method includes computing it using moments. I'll not ply this method for my notebook, however one can check my notebook that is strictly made for this purpose: [Computing the magnitude of skewness in Maths score](https://www.kaggle.com/ritikpnayak/computing-the-magnitude-of-skewness-in-maths-score)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def PearsonMedianCoeff(sample, xbar, median):\n    gp = 3 * (xbar - median) / len(sample)\n    return gp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skewness in wei: ', PearsonMedianCoeff(df.wei, np.mean(df.wei), np.median(df.wei)))\nprint('Skewness in ei: ', PearsonMedianCoeff(df.ei, np.mean(df.ei), np.median(df.ei)))\nprint('Skewness in flfp: ', PearsonMedianCoeff(df.flfp, np.mean(df.flfp), np.median(df.flfp)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What are the countries that account for the outiers in the dataset?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['wei'] <= 33.7073934294814) | (df['wei'] >= 61.96319480581269)]['country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['ei'] <= 31.207569895884333) | (df['ei'] >= 63.2747830452921)]['country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['flfp'] <= 44.753797301977436) | (df['flfp'] >= 72.20973210978727)]['country']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Are the countries with highest wei rate, members of EU?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['wei'] >= 61.96319480581269]['eum']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Are the countries with highest wei rate, developed?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['wei'] >= 61.96319480581269]['lod']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***While it is evident that most of the countries that have the highest rate of wei, are the members of EU, it is rather stringent that all the outperforming countries are developed.***"},{"metadata":{},"cell_type":"markdown","source":"***The same analysis can be done for flfp rate***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['flfp'] >= 72.20973210978727]['eum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['flfp'] >= 72.20973210978727]['lod']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***EU members are not in the fore in leading the flfp rates, however, the developed countries outperform their developing counterparts, again.***"},{"metadata":{},"cell_type":"markdown","source":"# 2.3. Effect Size"},{"metadata":{},"cell_type":"markdown","source":"***What is effect size?***\n\n1. It is quite evident from the name itself that it is the measure of the size of an effect.\n2. In our data, the effect size is not quite helpful.\n3. However, we can find if there is an apparent difference between the range of wei, ei and flfp, taken two at a time.\n\nFor this purpose, we'll use \"Cohen's d\"; which is defined as; \n\nd = [mean(x1) - mean(x2)] / s"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CohenEffectSize(group1, group2):\n    \n    diff = np.mean(group1) - np.mean(group2)\n    \n    var1 = np.var(group1)\n    var2 = np.var(group2)\n    n1, n2 = len(group1), len(group2)\n    \n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    cohens_d = diff / math.sqrt(pooled_var)\n    \n    return cohens_d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Effect size between wei and ei: ', CohenEffectSize(df.wei, df.ei))\nprint('Effect size between flfp and ei: ', CohenEffectSize(df.flfp, df.ei))\nprint('Effect size between flfp and wei: ', CohenEffectSize(df.flfp, df.wei))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***There is a lil difference between the range of wei and ei but the difference between the range of flfp and ei, and flfp and wei is rich.***"},{"metadata":{},"cell_type":"markdown","source":"# 2.4. Probability mass function"},{"metadata":{},"cell_type":"markdown","source":"***What is Probability mass function (pmf)?***\n\n1. Another way to represent a distribution.\n2. It is the normalized frequency.\n3. Put differently, it is the measure of the frequency (the no. of times a value occurs in a variable/column) divided by n.\n4. We divide the frequencies by n to find the probabilities of the frequencies. Division by the sum of all frequencies is called \"normalization\"."},{"metadata":{},"cell_type":"markdown","source":"***For this purpose, I'll convert the values of flfp, wei and ei into ranges. I'll make 3 variables/columns that will tell us the range to which the values of flfp, wei and ei belong.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rng_flfp'] = df.flfp.apply(lambda x: 1 if x<=10\n                            else 2 if x<=20\n                            else 3 if x<=30\n                            else 4 if x<=40\n                            else 5 if x<=50\n                            else 6 if x<=60\n                            else 7 if x<=70\n                            else 8 if x<=80\n                            else 9)\n\ndf['rng_wei'] = df.wei.apply(lambda x: 1 if x<=10\n                            else 2 if x<=20\n                            else 3 if x<=30\n                            else 4 if x<=40\n                            else 5 if x<=50\n                            else 6 if x<=60\n                            else 7 if x<=70\n                            else 8 if x<=80\n                            else 9)\n\ndf['rng_ei'] = df.ei.apply(lambda x: 1 if x<=10\n                            else 2 if x<=20\n                            else 3 if x<=30\n                            else 4 if x<=40\n                            else 5 if x<=50\n                            else 6 if x<=60\n                            else 7 if x<=70\n                            else 8 if x<=80\n                            else 9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***PMF for what purpose?***\n\n1. To see if there is any difference in the range of wei values between the developed and non developed countries.\n2. That is, if there is a difference in the no. of times the range of wei occurs in developed and developing countries, then by what percent is the lead.\n\nThe idea would be more evident from the code. ***Oftentimes, the code that speaks volumes.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"developed = df[df.lod == 'Developed']\ndeveloping = df[df.lod == 'Developing']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def defaultval():\n    return 0\n\nd1 = defaultdict(defaultval)\nfor key, value in Counter(developed.rng_wei.value_counts()).items():\n    d1[key] = value / sum(developed.rng_wei.value_counts())\n    \nd2 = defaultdict(defaultval)\nfor key, value in Counter(developing.rng_wei.value_counts()).items():\n    d2[key] = value / sum(developing.rng_wei.value_counts())\n\ndiffs = []\nfor i in range(1, 10):\n    diff = d1[i] - d2[i]\n    diffs.append(100 * diff)\n    \nplt.bar(range(1, 10), diffs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = defaultdict(defaultval)\nfor key, value in Counter(developed.rng_ei.value_counts()).items():\n    d1[key] = value / sum(developed.rng_ei.value_counts())\n    \nd2 = defaultdict(defaultval)\nfor key, value in Counter(developing.rng_ei.value_counts()).items():\n    d2[key] = value / sum(developing.rng_ei.value_counts())\n\ndiffs = []\nfor i in range(1, 10):\n    diff = d1[i] - d2[i]\n    diffs.append(100 * diff)\n    \nplt.bar(range(1, 10), diffs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = defaultdict(defaultval)\nfor key, value in Counter(developed.rng_flfp.value_counts()).items():\n    d1[key] = value / sum(developed.rng_flfp.value_counts())\n    \nd2 = defaultdict(defaultval)\nfor key, value in Counter(developing.rng_flfp.value_counts()).items():\n    d2[key] = value / sum(developing.rng_flfp.value_counts())\n\ndiffs = []\nfor i in range(1, 10):\n    diff = d1[i] - d2[i]\n    diffs.append(100 * diff)\n    \nplt.bar(range(1, 10), diffs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***The graphs manifest some apparent conclusions that I leave for the readers to draw.***"},{"metadata":{},"cell_type":"markdown","source":"# 2.5. Cumulative distribution function"},{"metadata":{},"cell_type":"markdown","source":"***What is Cumulative distribution function (cdf)?***\n\nI'll not dwell on the definition, because I have used it in most of my notebooks. To know more about it, please refer to my notebook; [Introduction: Analytic distribution w/ Volkswagen](https://www.kaggle.com/ritikpnayak/introduction-analytic-distribution-w-volkswagen#4.-Brief-introduction-to-CDF).\n\nFollowing is the application of cdfs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def EvalCdf(sample, x):\n    count = 0\n    for i in sample:\n        if i <= x:\n            count += 1\n    prob = count / len(sample)\n    return prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\nc1 = [EvalCdf(sorted(developed.ei), x) for x in sorted(developed.ei)]\nc2 = [EvalCdf(sorted(developing.ei), x) for x in sorted(developing.ei)]\n\nplt.plot(sorted(developed.ei), c1, label = 'CDF of ei of developed countries')\nplt.plot(sorted(developing.ei), c2, label = 'CDF of ei of developing countries')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\nc1 = [EvalCdf(sorted(developed.wei), x) for x in sorted(developed.wei)]\nc2 = [EvalCdf(sorted(developing.wei), x) for x in sorted(developing.wei)]\n\nplt.plot(sorted(developed.wei), c1, label = 'CDF of wei of developed countries')\nplt.plot(sorted(developing.wei), c2, label = 'CDF of wei of developing countries')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\n\nc1 = [EvalCdf(sorted(developed.flfp), x) for x in sorted(developed.flfp)]\nc2 = [EvalCdf(sorted(developing.flfp), x) for x in sorted(developing.flfp)]\n\nplt.plot(sorted(developed.flfp), c1, label = 'CDF of flfp of developed countries')\nplt.plot(sorted(developing.flfp), c2, label = 'CDF of flfp of developing countries')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***What is the conclusion?***\n\n1. ***Graph 1:*** 80% of the values of ei rate in developing countries are less than 40 whereas, almost 45% of the values of ei rate in developed countries is more than 60.\n2. ***Graph 2:*** 80% of the values of wei rate in developing countries are less than 40 whereas, almost 55% of the values of wei rate in developed countries is more than 60.\n3. ***Graph 3:*** 80% of the values of flfp rate in developing countries are less than 70 whereas, almost 80% of the values of flfp rate in developed countries is more than 60.\n\nFrom all of the graphs it is evident that the developing countries are trailing behind the developed countries in more than one index. Nevertheless, making that kind of inference solely on this analysis not possible and unethical. A lot more things can be done that we'll see in the notebooks that are to follow. "},{"metadata":{},"cell_type":"markdown","source":"# Epilogue:"},{"metadata":{},"cell_type":"markdown","source":"Dear reader,\n\nAfter spending a lot of time teaching data science and making notebooks on Kaggle I ave realized that what is better than to set about doing what one really wants to do. Whenever I wanted to do something, I initially used to think unnecessarily and all that used to culminate in the wilting of my very conviction. Bow, I have started to apply the concepts that I have learnt by making notebooks, many not one, on Kaggle. This gives not only confidence, but eternal joy of godly grace. \n\nIn that sense, I introduced to an yet another \"application notebook\" of some of the basic concepts of statistical analysis that I want everyone to know and learn about. It is thourough but all. Some more notebooks would be required to complete the analysis and to end up answering certain questions. As they say, many a little makes a mickle, I assume this little notebook is complete in its own way.\n\nYours making more notebooks\n\nRitik Prakash Nayak"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}