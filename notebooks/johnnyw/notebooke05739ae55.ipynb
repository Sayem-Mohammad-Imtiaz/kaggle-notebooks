{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00b28801-a862-0f07-fc48-e43d7da10a7d"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport keras\nfrom keras.models import Sequential\nimport nltk\nimport re\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nfrom collections import defaultdict\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2856c83d-4d34-a4a1-a43f-1e3ea399b6bc"},"outputs":[],"source":"with open(\"../input/Tweets.csv\", \"r\") as f:\n    df = pd.read_csv(f)\ndf = df.drop(['tweet_id', 'negativereason', 'airline', 'airline_sentiment_gold', 'name',\n             'negativereason_gold', 'negativereason_confidence', 'retweet_count', 'tweet_coord',\n             'tweet_created', 'tweet_location', 'user_timezone'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2b999fc-c4d4-f6e3-736a-d9c529b757e1"},"outputs":[],"source":"df = df[df['airline_sentiment_confidence'] > 0.75]\nX, y = df.iloc[:, 1:], df.iloc[:, :1]\nprint(\"number of samples: {}, number of targets: {}\".format(len(X), len(y)))\nle = LabelEncoder()\ny = le.fit_transform(np.array(y).ravel())\nprint(X[:10])\nprint(y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0cc52d13-099f-045f-1193-ed3907a2e6cd"},"outputs":[],"source":"print(X['text'][:140:7])\n\ndef text_cleaner(text):\n    text = re.sub(r'@\\w+', '_TN', text)\n    text = re.sub(r'[\\w\\-][\\w\\-\\.]+@[\\w\\-][\\w\\-\\.]+[a-zA-Z]{1,4}', '_EM', text)\n    text = re.sub(r'\\w+:\\/\\/\\S+', r'_U', text)\n    text = text.replace('\"', ' ')\n    text = text.replace('\\'', ' ')\n    text = text.replace('_', ' ')\n    text = text.replace('-', ' ')\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\\\n', ' ')\n    text = text.replace('\\'', ' ')\n    text = re.sub(' +', ' ', text)\n    text = text.replace('\\'', ' ')\n    text = re.sub(r'([^!\\?])(\\?{2,})(\\Z|[^!\\?])', r'\\1 _BQ\\n\\3', text)\n    text = re.sub(r'([^\\.])(\\.{2,})', r'\\1 _SS\\n', text)\n    text = re.sub(r'([^!\\?])(\\?|!{2,})(\\Z|[^!\\?])', r'\\1 _BX\\n\\3', text)\n    text = re.sub(r'([^!\\?])\\?(\\Z|[^!\\?])', r'\\1 _Q\\n\\2', text)\n    text = re.sub(r'([^!\\?])!(\\Z|[^!\\?])', r'\\1 _X\\n\\2', text)\n    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2 _EL', text)\n    text = re.sub(r'(\\w+)\\.(\\w+)', r'\\1\\2', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = re.sub(r'([#%&\\*\\$]{2,})(\\w*)', r'\\1\\2 _SW', text)\n    text = re.sub(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}', r' _BS', text)\n    text = re.sub(r' (?:[;:=]-?[\\)\\}\\]d>])|(?:<3)', r' _S', text)\n    text = re.sub(r' [x:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}', r' _BF', text)\n    text = re.sub(r' [x:=]-?[\\(\\[\\|\\\\/\\{<]', r' _F', text)\n    text = re.sub(r'\\s\\s', r' ', text)\n    return text\n\nX['text'] = X['text'].apply(text_cleaner)\n\nprint(X['text'][:140:7])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"071d21f7-08d8-9e5a-40cd-c799fde51250"},"outputs":[],"source":"X['text'] = X['text'].apply(lambda x: x.split())\nprint(X['text'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d6df670-bea7-7994-4261-8745d6b86285"},"outputs":[],"source":"X['text'].apply(lambda x: LineSentence(x))\nprint(X['text'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9ec472a-a4c9-538a-01fc-b71fe51c6eb6"},"outputs":[],"source":"# this part not currently used\n\ndef chunker(sentence):\n    tokens = nltk.word_tokenize(' '.join(sentence))\n    tagged = nltk.pos_tag(tokens)\n    entities = nltk.chunk.ne_chunk(tagged)\n    return entities\n\ndef get_content_words(chunks):\n    content_words = ['NN', 'NNS', 'JJ']\n    exclude = ['i', 'you', 'he', 'she', 'it', 'them', 'us', 'retweet', 'rt']\n    prefixes = ('@', '\\\\', '?', 'http', '/', '#', 'rt')\n    content = [x for x in chunks if type(x[-1]) == str and x[-1] in content_words]\n    content = [x[0] for x in content if x[0].lower() not in exclude]\n    content = [x for x in content if not x.lower().startswith(prefixes)]\n    return content"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e1853d9-eae1-9f58-0d2c-b7b290a15a14"},"outputs":[],"source":"sentences = X['text'].values\nmodel = Word2Vec(sentences, min_count=10)\nprint(\"vocabulary size: {} words\".format(len(model.vocab)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55c5300d-2dc9-7f06-df8b-7de241069e5c"},"outputs":[],"source":"print(model.similar_by_word('disappointed'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd671e19-7543-d2b1-2cf9-9d49319fdb41"},"outputs":[],"source":"# see http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\nclass Word_Vectoriser(object):\n    \n    def __init__(self, w2v):\n        self.w2v = w2v\n        self.dim = len(list(w2v.values())[0])\n        self.word2weight = None\n    \n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n        return self\n    \n    def get_params(self, *args, **kwargs):\n        return self\n    \n    def transform(self, X):\n        return np.array([\n            np.mean([self.w2v[w] * self.word2weight[w] for w in words if w in self.w2v]\n                 or [np.zeros(self.dim)], axis=0)\n            for words in X])\n\nX_train, X_test, y_train, y_test = train_test_split(X['text'], y, test_size=0.20, random_state=1)\nw2v = dict(zip(model.index2word, model.syn0))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"495b1a90-aea6-25ce-f8da-5da8b39d5c59"},"outputs":[],"source":"print(X_train.shape)\nprint(y_train.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6de6f007-fff4-cdd7-73a0-31fb8edb5387"},"outputs":[],"source":"clf = SVC()\nparameters = {'C': [10, 100, 1000, 10000], 'gamma': [0.01, 0.001, 0.0001]}\ngrid_search = GridSearchCV(clf, parameters, n_jobs=8, verbose=1)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_, grid_search.best_score_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"623df36d-1fb7-ab63-8143-4bba420ee17c"},"outputs":[],"source":"pipeline = Pipeline([('w2v', Word_Vectoriser(w2v)),\n                     ('svm', SVC(C=10000, gamma=0.0001))])\n\npipeline.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3329e74-fce3-be44-e73f-6f2460b67ab5"},"outputs":[],"source":"# make pipeline for vectorisation and classification steps\npipeline = Pipeline([('w2v', Word_Vectoriser(w2v)),\n                     ('svm', SVC(C=10000, gamma=0.01))])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"290ddacb-d259-63ec-b33f-fc1cbcfadadf"},"outputs":[],"source":"pipeline.score(X_test, y_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"810b63f6-14d9-1b75-a08a-85c8f5893fe8"},"outputs":[],"source":"print(le.inverse_transform([0, 1, 2]))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a9b80d5-5116-9785-3207-5e9c74a8f130"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e79b9743-17db-e8ea-c03a-ebc2fdf3e0a9"},"outputs":[],"source":"# test positives\nindices = np.argwhere(y_test == 2)\nX_pos = [X_test[i] for i in indices]\nprint(X_pos)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce49e0d9-55fd-e9a7-a4c6-68eb0a97ce5a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}