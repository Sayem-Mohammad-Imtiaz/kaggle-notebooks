{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# <center>Exploratory Data Analysis for beginners"},{"metadata":{},"cell_type":"markdown","source":"<center>\nThis notebook is a basic introduction to <strong>Exploratory Data Analysis (EDA)</strong>, the foundation of any Data Science project. Because it's meant of beginners, as I myself was when writing it, I will ask some very basic questions to understand why they do things the way they do in this industry.<br/><br/>   \nAlthough modelling is the most highlighted part of the job, experienced Data Scientists say that preparing the data before they can start training models for it takes most of their time. And when they speak about it, many times, you hear them saying \"this is not pretty\" or \"not glamarous\". I actually find this detective work pretty beautiful. I hope you will like it too.<br/><br/>  \nData preparation contains <strong>multiple steps</strong>. When I first started training myself in this field, I began reading about EDA in <a href=\"https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html\">7 Steps to Mastering Data Preparation for Machine Learning with Python</a> on KDnuggets.<br/>\nIn this tutorial I will only discuss <strong>Exploratory Data Analysis.</strong><br/><br/>\nI will work on the data from the World Happiness Report from 2020.<br/><br/>\n<i>I like to sprinkle my writings with fun facts from different domains</i>"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n1. [Why EDA ?](#1.-Why-EDA-?)\n2. [Pandas, Numpy, Matplotlib, Seaborn](#2.-Pandas,-Numpy,-Matplotlib,-Seaborn)\n3. [Data types](#3.-Data-types)\n4. [Exploring categorical features](#4.-Exploring-categorical-features)\n5. [Exploring numerical features](#5.-Exploring-numerical-features)\n6. [Bivariate analysis](#6.-Bivariate-analysis)\n7. [Outliers](#7.-Outliers)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Why EDA ?\n\nBecause in order to start working with our data, we need to know what kind of data we are dealing with. And this detective work got itself the dry name of exploratory data analysis (which I don't think does justice to it). \n\nThese are only some of the questions that we ask ourselves. Depending on the answer, we have to proceed with different processing steps before we can use any algorithms on our data:\n- Do we have 1000 or 1 million entries in our data ?\n- Are we dealing with text or numbers ?\n- Do we have dates ? What format to these dates have ?\n- Do we have outliers ? (Data points that are extremely different than all the other ones)\n- Do we have missing data ? That is, is any of the cells in our dataset empty ?"},{"metadata":{},"cell_type":"markdown","source":"If I just open my data, the csv file, in a spreadsheet application and look at it with the naked eye, I won't be able to tell much.<br/>\n<img src=\"https://mihaelagrigore.info/wp-content/uploads/2020/10/Happiness-CSV.png\"></img>"},{"metadata":{},"cell_type":"markdown","source":"I will open the csv file and read all my data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/world-happiness-report/2020.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Pandas, Numpy, Matplotlib, Seaborn\n\n![Red pandas](https://mihaelagrigore.info/wp-content/uploads/2020/10/red-panda-970798_640.jpg)\nThese are red pandas. We are mostly used to the black & white ones. This image by <a href=\"https://pixabay.com/users/1443435-1443435/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=970798\">1443435</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=970798\">Pixabay</a> is a tribute to diversity.\n\n### 2.1 Pandas\nIn the code right above (before the cute furry animals), I just imported pandas library and used **read_csv** to read my csv data in a **Pandas DataFrame.**  \n\n\nPandas is a software library created for data manipulation and analysis. Using pandas we can read various file formats easily into data structures specifically created for data manipulation procedures.  \n\nThe most commonly used data structures in pandas are [Series](https://pandas.pydata.org/pandas-docs/stable/reference/series.html) and [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html). **Series** stores one-dimensional data (like a table with only one column) and **DataFrame** stores 2-dimensional data (tables with multiple columns).\n\nThe best place to learn pandas is the official documentation. If during or after reading this you feel like you need a more thorough  work session with pandas, have a look at this [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) tutorial.  # Mind that it only takes 10 min if you're some species from another planet. For humans, most likely, it takes way more than that.\n\n### 2.2 Numpy\nNumpy is a library mainly used for the Mathematical functions it implements. This way we don't have to write the functions ourselves all the time.\n\n### 2.3 Matplotlib\nMatplotlib brings us data visualizations.\n\n### 2.4 Seaborn\nSeaborn takes visualisations to the next level: more powerful and more beautiful. You'll see.."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's set the precision to 2 decimal places\npd.set_option(\"display.precision\", 2)\n\n#the first 3 rows of our pandas DataFrame object\n#if we run df.head(), it will display the first 5 rows by default\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas makes it very easy to handle tabular data.   \n\nTabular data means that our data fits or belongs in a table. Other types of data can be visual (that is, images, for which it doesn't really make sense to be stored as csv files).\n\nThe standard way to store tabular data is that:  \n- **each row** represents a different **observation**. Observation is a fancy Statistics term, but it just means a new data point, a new measurement we did. \nIf our data is about happiness in various countries, each row contains data for a new country.  \n- **each column** is a different **feature** (or attribute) of our observations. For the World Happiness Report dataset, examples of features can be the Country name, the Regional indicator or the Social Support score.  "},{"metadata":{},"cell_type":"markdown","source":"Let's use the numpy library to see the maximum value of the **feature** *Ladder score* across **all observations** in our dataset (all countries). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's import the numpy library\nimport numpy as np\n\n#and use a numpy function to see what's the maximum value for our Ladder score feature\nnp.max(df[\"Ladder score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And since we're here, I'll do a quick demo of how convenient it is to use pandas DataFrame structure.  \nWe found the maximum values for \"Ladder score\" feature. What is the row number of the entry with the max Ladder score ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Ladder score'].argmax()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It only took one line of code to find the row number. Let's see this observation's features, to convince ourselves we got the right entry. # Mind that when displaying one single entry from the DataFrame, the feature values won't appear o a row anymore, but will be displayed as a column (I find this switch a bit confusing)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[df['Ladder score'].argmax()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data types\n\nWe have some idea about or features types just by looking a the CSV file. But a better method is the one below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#DataFrame has this very handy method.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What I see in the output above:\n- my data is a DataFrame, with 153 entries (from 0 to 152)\n- I have 20 columns (from 0 to 19)\n- all my columns have 153 non-null values (I don't have \"missing\" data in any of these columns)\n- my column types are: object (2 of them) and float64* (18 of them)\n\n*float64 means they can store fractional numbers and each number takes 64 bits\n\nThe 'object' type I see above most likely refers to a string. I'll use [DataFrame indexing / selection](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) to look at one particular value to verify my assumption."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Country name'][0])\nprint(df['Regional indicator'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so, in this case, 'object' means String. "},{"metadata":{},"cell_type":"markdown","source":"## 4. Exploring categorical features\n\nWe have 2 features which contain text:\n- Contry\n- Region\n\n### Country\n\nOur intuition is that each country is unique in our dataset (one country per row). This is what we would expect from a study of happiness levels in different countries across the worls. We can verify this assumption, to make sure we don't have errors in our data. For example, the social scientist running this study could have accidentally entered the same observation twice because she was working late to finish her data analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many entries we have for each country\n#shown in descending order (highest value first)\ndf[\"Country name\"].value_counts().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment the line below to see what data type we used. This is a nice way to explore the functioning of pandas.\n#print(\"\\nThe code above returns a date of type: \", type(df['Country name'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Region\n\nLet's have a look at the **regions** now. It would be interesting to see what different regions we have. This would open the door for questions like: 'Are people happier in Western Europen than in Eastern Europe ?'. We don't know yet what question we can ask and exploring our data informs our next steps.\n\nBy the way, since we are dealing with long column names, it's worth mentioning that I don't have to type the whole column name. I just input the first 3 letters and press Tab for autocomplete.\n\nWe see in the output below that:\n- Europe is split into 2: 'Western Europe' and 'Central and Eastern Europe'\n- The Americas are divided into 2: 'Latin America and Caribbean' and 'North America and ANZ' (which is North America, Australia and New Zealand)\n- Africa is split into 2: 'Sub-Saharan Africa' and 'Middle East and North Africa'\n- Asia is divided into 3: 'Southeast Asia', 'South Asia' and 'East Asia'\n- There is a group of post-Soviet republics in Eurasia making up the 'Commonwealth of Independent States'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#here's each individual region and its corresponding frequency (the statistical term \n#for the number of times this region appears in our dataset)\ndf['Regional indicator'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we have 10 regions and pandas DataFrame has a method to find this out\nprint(f\"The number of regions in our dataset is: {df['Regional indicator'].nunique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just used Python's fancy formatting in the line of code above. If you like it and want to read more, know that it's called Literal String Interpolation (but the popular name is f-string). You can read more [here](https://www.programiz.com/python-programming/string-interpolation)."},{"metadata":{},"cell_type":"markdown","source":"### Visualisation for categorical features\n\nSince the frequencies (the number of times they appear in our dataset) of our regions is greater than one, it invites us to look at them in a more intuitive way rather than the text displayed above.  \n\nIt is generally much better for the audience to present any data in visual form, whenever possible. For countries, nothing else made sense since each country appeared once in our data. But for regions, we can use a **bar chart**.\n\nThe bar chart below shows the same information as the table we've seen earlier.   \nBut in visual form it's so much easier to gain insights like \"Sub-Saharan Africa is present in our dataset approximately twice as much as the next region in line, Western Europe\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Regional indicator'].value_counts().plot(kind='bar', title='Absolute frequency distribution of Regional indicator')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code above I've used **Pandas built-in capabilities for data visualization**. I didn't feel a need to turn to matplotlit or seaborn for basic visualisation that can be provided by pandas.  \nIf you feel like you want to read more abour Pandas visualisation, see the [official documentation.](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)"},{"metadata":{},"cell_type":"markdown","source":"Another obsvervation for the plot above is that those numbers are absolute frequencies. That is, the bar chart shows the number of times each region is present in our dataset. Sometimes it's enough to know that we have 39 countries from Sub-Saharan Africa. But there are times when we're wondering how much this represents in terms of percentage.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['Regional indicator'].value_counts()/df.shape[0]).plot(kind='bar', title='Relative frequency of Regional indicators')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know that Sub-Saharan Africa represents 25% of our data. For this dataset this is not unusual. But imagine you're trying to see how happy people are in a single country, you broadcast a digital survey that people can take and during data analysis you realize that 25% of the people who filled in the survey are from the same city in this country.  "},{"metadata":{},"cell_type":"markdown","source":"## 5. Exploring numerical features\n\nPandas has a nice built-in method that performs descriptive statistics on a DataFrame.  \nIt shows us: \n- the number of values for each feature (again, an opportunity to see if we have missing values for any feature)\n- the mean value\n- the standard error\n- the min and max value\n- the median of our data (50%)\n- the lower and upper quartile (25% and 75%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights from the descriptive statistics above:\n- Ladder score actually goes from 2.5 to 7.8. There's no 0 or 10. \n- Healthy life expectancy has a minimum of 45 and a maximum of 76. This is a large range. There are countries in our dataset where life expenctancy is 45 years !\n- Generosity can be negative. It's the only feature that has negative values.  \n- Other features are more difficult to interpret from the descriptive stats above."},{"metadata":{},"cell_type":"markdown","source":"Numerical data is best viewed as histograms. We will use both matplotlit and seaborn for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolumns = ['Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\\\n           'Perceptions of corruption']\n\n\nscols = int(len(columns)/2)\nsrows = 2\nfig, axes = plt.subplots(scols, srows, figsize=(10,6))\n\nfor i, col in enumerate(columns):\n    ax_col = int(i%scols)\n    ax_row = int(i/scols)\n    \n    sns.distplot(df[col], hist=True, ax=axes[ax_col, ax_row])\n    axes[ax_col, ax_row].set_title('Frequency distribution '+ col, fontsize=12)\n    axes[ax_col, ax_row].set_xlabel(col, fontsize=8)\n    axes[ax_col, ax_row].set_ylabel('Count', fontsize=8)\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights from the visual exploration of our numerical data:\n- the distributions of GDP, social support, healthy life expectancy, freedom and corruption are all [left skewed](http://www.cvgs.k12.va.us/DIGSTATS/main/descriptv/d_skewd.html) (or negative skew). That is to say, most of our values do not happen to be in the middle of the min-max range, but are pushed towards the upper end of our range. For all but Perception of corruption this is good news. \n- generosity, though, is right skewed. The majority of the countries are in the bottom half of the generosity scale (unfortunately)\n\nIf you feel the need to read more about why we might want to look at the distribution of our data, [here is a very quick overview](http://www.cvgs.k12.va.us/DIGSTATS/main/descriptv/)."},{"metadata":{},"cell_type":"markdown","source":"## 6. Bivariate analysis\n\nAll the explorations above belong to univariate analysis (that is, we looked at each variable individually).\nWe can also perform bivariate analysis - we can look at pairs of two variables to explore a possible relation between them.\n\nWhen Data Scientists perform a bivariate analysis, they look at scatterplots like the ones below and they search for clouds of dots that arrange themselves into straight diagonal lines. This is a visual representation of two variables that correlate.  \n\nHere's how to read the plots below:  \nLet's look at the **second plot on the first row**. On the **far left** of the image we see \"Logged GDP per capita\". All plots on the first row have on the y axis (the vertical axis) the Logged GDP per capita as the label of the Y axis. Now look at the bottom of the plots, all the way down, under the second column we have \"Social support\" as the name of the X axis. All plots on the second columns have the Social support on the x axis (the horizontal axis).  \n\nArmed with this information, let's look at the contents of the second plot, first row. As the 'Social support' increases, so does 'Logged GDP per capita'. What does this mean ? Nothing more than the fact that the two feature seems to be correlated (correlation, not causation). Most likely (intuition dictates) as the country gets riches it can afford to offer more social support to its inhabitants.\n\nNow look at the fourth subplot on the same row. The datapoints are all over place and there seems to be no correlation between 'GDP per capita' and 'Freedom to make life choices'.   \n\nCorrelation is not assessed only by looking at a scatterplot, but this is a good start.\n\nTake a few moments to explore the plots below. Look on the diagonal, from upper left to lower right. Do you recognize them from the univariate analysis section ? These are the histograms we've seen earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This will take slightly longer than other plots, don't worry if the plots don't show up immediately.\ncolumns = ['Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\\\n           'Perceptions of corruption']\nsns.pairplot(df[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seaborn allows us to add a 'hue' to our plots. \nWe will set our scatterplots to assign **different colors to datapoints that belong to different global regions.**\n\nYou can read about [Seaborn pairplot here](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n\nThis helps us gain insight like: Sub-Saharan African countries (the purple dots, according to the legend on the right) have the lowest GDP and the lowest Healthy life expectancy, but they are not less generous than more fortunate countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This will take slightly longer than other plots, don't worry if the plots don't show up immediately.\ncolumns = ['Regional indicator','Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\\\n           'Perceptions of corruption']\nsns.pairplot(df[columns], hue=\"Regional indicator\", palette=\"Paired\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation is not assessed only by looking at a scatterplot, but the mono-coloured pairplot above was a good start.  \nAnother useful tool in the EDA toolset is the **correlation matrix.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"meaningful_columns = ['Ladder score','Logged GDP per capita', 'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Ladder score in Dystopia']\n\nplt.figure(figsize=(8,6))\n#sns.heatmap(df.corr(), annot = True, fmt='.1g', cmap= 'coolwarm')\nsns.heatmap(df[columns].corr(), annot = True, fmt='.1g', cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Outliers\n\nA nice way to spot outliers is a Box and Whiskers plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"small = ['Social support', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\nmedium = ['Ladder score', 'Logged GDP per capita']\nlarge = ['Healthy life expectancy']\n\nf, axs = plt.subplots(1,3,figsize=(15,5))\n\n# equivalent but more general\nax1=plt.subplot(1, 3, 1)\ndf.boxplot(column=small, ax = ax1)\nplt.xticks(rotation=90)\n\nax2=plt.subplot(1, 3, 2)\ndf.boxplot(column=medium, ax = ax2)\n\nax3=plt.subplot(1, 3, 3)\ndf.boxplot(column=large, ax = ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classical interpretation in Statistics is that whatever falls outside the 'whiskers' represents an outlier.  \n\nIf you'd like to read more about box plots and what the box, the line that splits the box and the whiskers represent, [this resource](https://publiclab.org/notes/mimiss/06-18-2019/creating-a-boxplot-to-identify-outliers-using-codap) seemed to have nice visuals. \n\nIn practice, deciding what to do with outliers depends on many factory (whether you think they can be a mistake in data collection, for example). "},{"metadata":{},"cell_type":"markdown","source":"Let's examine the case of Perceptions of corruption."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axs = plt.subplots(1,2,figsize=(12,4))\n\n# equivalent but more general\nax1=plt.subplot(1, 2, 1)\nsns.distplot(df['Perceptions of corruption'], hist=True, ax=ax1)\n\nax2=plt.subplot(1, 2, 2)\ndf.boxplot(column=['Perceptions of corruption'], ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because 'Perceptions of corruption' feature is left skewed, countries with lowest perception of corruption are automatically categorized as outliers in the boxplot. \n\nBut just because they are technically outliers does not necessarily mean we should do something about them. The next question is: is the data correct ? Let's see who these outliers are."},{"metadata":{"trusted":true},"cell_type":"code","source":"(df[df['Perceptions of corruption'] < 0.4])[['Country name', 'Perceptions of corruption']].sort_values(by = 'Perceptions of corruption', axis=0, ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's no surprise to find almost all these countries in the bottom of the Perceptions of Corruption top. I admit I did not know about the low corruption in Rwanda !  "},{"metadata":{},"cell_type":"markdown","source":"*If you find the line of code above confusing, I did too, in the begining. When I found lines like this in someone else's code, I used to dissect them to examine the output and the data types. Maybe this tip helps.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment the code below, line by line, if you want to dissect the previous line of code.\n#I find it useful to first make a hypothesis about what I expect the line of code does before running it. \n\n#print(f'df has {len(df)} entries')\n\n#df['Perceptions of corruption'] < 0.4\n\n#df[df['Perceptions of corruption'] < 0.4]\n\n#print(f\"our selection has {len(df[df['Perceptions of corruption'] < 0.4])} entries\")\n\n#(df[df['Perceptions of corruption'] < 0.4])[['Country name', 'Perceptions of corruption']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it for EDA for this rather simple tabular dataset.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}