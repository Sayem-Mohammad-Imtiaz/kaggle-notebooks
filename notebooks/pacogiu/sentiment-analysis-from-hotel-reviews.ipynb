{"cells":[{"metadata":{"_uuid":"a9f45f9a7c858b859a4b61954f43ff663e1706ea"},"cell_type":"markdown","source":"**Sentiment Analysis**:\nHere I go through some data exploration, and manipulation to build up a database that contains some paramenters for Sentiment Analysis. Using a quick NaiveBayesClassifier I build a score for each word that can be used to rank results (indipendently form their review scores)\n\nThis is a WIP, so comments and suggestions are more than welcome!\n\nMany bits of this Kernel come from info and tutorials sparse around the internet, I will try to add the links to the major resources once I am done with the analysis, please bear with me!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"655cd1ad4c96adbafb68d2c1a8446891880b30f4"},"cell_type":"code","source":"# Standard batch import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport time\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58365a85c07b9dd8c7c55759cfef7715bdba79ff"},"cell_type":"code","source":"df=pd.read_csv('../input/Hotel_Reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc8738a6c2dba9da26f989c83c105d60b39a6a7d"},"cell_type":"code","source":"# Lets look at some stats shall we?\ndisplay(df.shape)\ndisplay(df.describe())\ndisplay(df.describe(include=['O']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"992bd3b05c90a35da770fd85fcb00a890346c8b8"},"cell_type":"code","source":"# Keys handy for getting columns later on\ndf.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"432b599d5adcc3429eb0422e55ff3e07566e977c"},"cell_type":"code","source":"# Just for fun, create a database with only hotels info\nhotels=df[['Hotel_Name','Average_Score','Total_Number_of_Reviews',\\\n           'Hotel_Address', 'Additional_Number_of_Scoring','lat', 'lng']].drop_duplicates().reset_index()\nprint(hotels.shape)\nhotels.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd4b361d1ad41f636a59945dc2da522d87961c80"},"cell_type":"markdown","source":"**Add columns, Clean columns and feel the data**\nI play a bit with the data to get confortable with it and get some new columns and lists that could be useful for better understanding of the dataset."},{"metadata":{"trusted":true,"_uuid":"7e7c79ded0bc32fe16243ef45166895bf892b36e"},"cell_type":"code","source":"# Now lets get some new columns running\n# Hotel address ---> Country ('Netherlands', 'UK', 'France', 'Spain', 'Italy', 'Austria')\nhotels['Country']=hotels['Hotel_Address'].apply(lambda x: x.split()[-1]).replace('Kingdom','UK')\nhotels.Country.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c322850f7745c9ef51cbdb3e4fb67723adb9763"},"cell_type":"code","source":"# I would be curious to see which country gives more reviews (UK most of the time)\n# So lets get the most common nationality of reviews for each hotel\nmost_national=df[['Hotel_Name','Hotel_Address','Reviewer_Nationality']].groupby(['Hotel_Name','Hotel_Address']).agg(lambda x:x.value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82faddeedcbcf84ae78fc57ccc840cce209d6a8f"},"cell_type":"code","source":"# In case we want to use the Tags, here I clean up the format \n# and create list from Tags ranked them by occurence\ntags_rank=pd.Series(re.findall(r'[\\']\\s([\\w\\s]+)\\s[\\']',''.join(df.Tags))).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e031190f889a6379e7e0e2cde1e30668a6baa85"},"cell_type":"code","source":"# For some reason days where not numbers but string so..\n# Correct day of the review into integer\ndf['days_since_review']=pd.to_numeric(df['days_since_review'].str.replace(r'[a-z]+', \"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"87f6cb7de2860544be11928ebe21fb5426d80341"},"cell_type":"code","source":"# Lets isolate the reviews that have some information into negative/positive\n# For simplicity I start getting the badly scored and highly scored reviews \n# to feed later on to the classifier.\n# This was he/she/it will understand how a bad or good review looks like\n# DataFrames with Negative reviews and positive reviews given\nneg_rev=df[df.Negative_Review!='No Negative'].reset_index().drop('index',1)\npos_rev=df[df.Positive_Review!='No Positive'].reset_index().drop('index',1)\nneg_rev = neg_rev[neg_rev['Reviewer_Score']<5].reset_index().drop('index',1)\npos_rev = pos_rev[pos_rev['Reviewer_Score']>8].reset_index().drop('index',1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3edf6d197cf0675596a1ff34029437fafc0e6813"},"cell_type":"markdown","source":"**Create some useful function to play with words**\nHere we define some handy functions, some choices have been made for making them a bit faster (like using Toktok) as we will have to run this over the whole database (515K! my laptop is old...)"},{"metadata":{"trusted":true,"_uuid":"6a9ed4aa971621bfa31c115cffc6338b5a31b60e"},"cell_type":"code","source":"# Takes review and gives back the clean list of words\n\n#TokTok faster than word_tokenize\nfrom nltk.tokenize import ToktokTokenizer\ntoktok = ToktokTokenizer()\n\n# Stopwords, numbers and punctuation to remove\nremove_punct_and_digits = dict([(ord(punct), ' ') for punct in string.punctuation + string.digits])\nstopWords = set(stopwords.words('english'))\n\n\ndef word_cleaner(data):\n    cleaned_word = data.lower().translate(remove_punct_and_digits)\n    words = word_tokenize(cleaned_word)\n    words = [toktok.tokenize(sent) for sent in sent_tokenize(cleaned_word)]\n    wordsFiltered = []\n    if not words:\n        pass\n    else:\n        for w in words[0]:\n            if w not in stopWords:\n                wordsFiltered.append(w)\n                end=time.time()\n    return wordsFiltered\n\n\n# Example\nwordsFiltered = word_cleaner(neg_rev.Negative_Review[1])\nprint(neg_rev.Negative_Review[1])\nprint(wordsFiltered)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b9420d1d8f84de78f7d5dc080f6cc6616ebef9c2"},"cell_type":"code","source":"# We take a small sample within our database to speed up Learning\n# with a decent machine and some time to spare we can easily skip this step\nneg_red=neg_rev[:50000].copy()\npos_red=pos_rev[:50000].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c9f173b3fbedbc35a2f46eb09b84b012c2da8e7"},"cell_type":"code","source":"# Create set related to positive and negative review\ndef word_feats(words):\n    return dict([(word, True) for word in words])\nneg_set=[(word_feats(word_feats(word_cleaner(neg_red.loc[i,'Negative_Review']))), 0) for i in range(len(neg_red))]\npos_set=[(word_feats(word_feats(word_cleaner(pos_red.loc[i,'Positive_Review']))), 1) for i in range(len(pos_red))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"737951fc42b94f3b355e3904601c4727fa6547fd"},"cell_type":"code","source":"# Finally some Machine is Learning!\n# Train the model and use CV to test accuracy\n\nnegcutoff = int(len(neg_set)*3/4)\nposcutoff = int(len(pos_set)*3/4)\n \ntrainfeats = neg_set[:negcutoff] + pos_set[:poscutoff]\ntestfeats = neg_set[negcutoff:] + pos_set[poscutoff:]\nprint(len(trainfeats), len(testfeats))\n \nclassifier = NaiveBayesClassifier.train(trainfeats)\nprint( 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"53837200d952062c603def5e29a5d99e8ee8cc47"},"cell_type":"code","source":"# Now we want to have a clear overview of the most hated/loved words so...\n# Builds the dataframe of words with respective sentiment and score\n\ncpdist = classifier._feature_probdist\nword=[]\nscore=[]\nsentiment=[]\nfor (fname, fval) in classifier.most_informative_features(100):\n            def labelprob(l):\n                return cpdist[l, fname].prob(fval)\n\n            labels = sorted([l for l in classifier._labels\n                             if fval in cpdist[l, fname].samples()],\n                            key=labelprob)\n            if len(labels) == 1:\n                continue\n            l0 = labels[0]\n            l1 = labels[-1]\n            if cpdist[l0, fname].prob(fval) == 0:\n                ratio = 'INF'\n            else:\n                ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n                                   cpdist[l0, fname].prob(fval))\n            sentiment.append(int(l1))\n            word.append(fname)\n            score.append(float(ratio))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96efdb9fcea3ef8921c9fc5be43c60bce7145d9d"},"cell_type":"code","source":"# Divides scores into negative and positive\nword_scores=pd.DataFrame({'word':word,'sentiment':sentiment,'score':score})\nneg_word_scores=word_scores[word_scores.sentiment==0]\npos_word_scores=word_scores[word_scores.sentiment==1]\ndisplay(word_scores[word_scores['sentiment']==1].head())\ndisplay(word_scores[word_scores['sentiment']==0].head())\nneg_given=df[df.Negative_Review!='No Negative'].reset_index().drop('index',1)\npos_given=df[df.Positive_Review!='No Positive'].reset_index().drop('index',1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77cf7319f4e9ce47de981e8c5aaedc867982c5f4"},"cell_type":"code","source":"# I want to create two new columns, one that will give a positive and one a negative score\n# Sums positive and negative scores for a given review\ndef pos_sentiment_sum(review):\n    pos=0\n    asd=word_cleaner(review)\n    set_w=set(pos_word_scores.word)-set(['no','negative','positive'])\n    \n    for word in asd:\n        if word in set_w:\n            pos+=pos_word_scores[pos_word_scores['word']==word].score.iloc[0]\n    \n    return pos\n\ndef neg_sentiment_sum(review):\n    neg=0\n    asd=word_cleaner(review)\n    set_w=set(neg_word_scores.word)-set(['no','negative','positive'])\n    \n    for word in asd:\n        if word in set_w:\n            neg+=neg_word_scores[neg_word_scores['word']==word].score.iloc[0]\n    \n    return neg\n\n# TEST\nprint('negative score:',neg_sentiment_sum(df.Negative_Review[6584]),\\\n       'positive score:', pos_sentiment_sum(df.Positive_Review[6584]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a2b433d80be015df97f25d7ed9850410cbddd8cd"},"cell_type":"code","source":"# VERY SLOW! AROUND 45min for full database\n# I comment it out so it does not need to run when I submit\n\n# This is the final step where we get the additional columns \n# Produce the pos and neg colums in database\n\n#=================\n#pos_col=[]\n#for i in range(len(df)):\n#    if df.Positive_Review[i]=='No Positive':\n#        pos_col.append(int(0))\n#    else:\n#        pos_col.append(pos_sentiment_sum(df.Positive_Review[i]))\n#df['pos_score']=pos_col\n#\n#neg_col=[]\n#for i in range(len(df)):\n#    if df.Negative_Review[i]=='No Negative':\n#        neg_col.append(int(0))\n#    else:\n#        neg_col.append(neg_sentiment_sum(df.Negative_Review[i]))\n#df['neg_score']=neg_col\n#=================","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0a6d8a4bbb38a573941b8877cc739052e9307728"},"cell_type":"code","source":"# The analysis can go on, but this is the last step for now\n# Reviews grouped by rate band\nscore_9=df[df.Reviewer_Score>9].copy()\nscore_4=df[df.Reviewer_Score<4].copy()\nscore_6=df[df.Reviewer_Score<7].copy()\nscore_7=df[(df.Reviewer_Score>7)&(df.Reviewer_Score<8)].copy()\nscore_8=df[(df.Reviewer_Score>8)&(df.Reviewer_Score<9)].copy()\nprint(score_6.shape)\nprint(score_7.shape)\nprint(score_8.shape)\nprint(score_9.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"08602a8f596b9e902c54070164ec1de677ca28d4"},"cell_type":"code","source":"# Some plots\n\nfrom matplotlib import rcParams\nrcParams.update({'figure.autolayout': True})\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(7,7))\nplt.hist(df['Reviewer_Score'],bins=20)\nplt.ylabel('Number_Reviewers',fontsize=16)\nplt.xlabel('Rating',fontsize=16)\nplt.title('Ratings accross Users',fontsize=16)\nplt.axvline(df['Reviewer_Score'].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_user.png')\n\nplt.figure(figsize=(7,7))\nplt.hist(df['Average_Score'],bins=20)\nplt.ylabel('Number Hotels',fontsize=16)\nplt.xlabel('Rating',fontsize=16)\nplt.title('Ratings accross Hotels',fontsize=16)\nplt.axvline(df['Average_Score'].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_hotel.png')\n\nplt.figure(figsize=(7,4))\nweek_bins=int(np.floor((max(df['days_since_review'])-min(df['days_since_review']))/7))\nvals = plt.hist(df['days_since_review'],bins=week_bins);\nplt.ylabel('Number ratings',fontsize=16)\nplt.xlabel('Days passed',fontsize=16)\nplt.title('Ratings per week',fontsize=16)\nplt.axhline(vals[0].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_week.png')\n\nplt.figure(figsize=(7,4))\nplt.plot(vals[0]);\nplt.ylabel('Number ratings',fontsize=16)\nplt.xlabel('Weeks passed',fontsize=16)\nplt.title('Ratings per week',fontsize=16)\nplt.axhline(vals[0].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_week_plot.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}