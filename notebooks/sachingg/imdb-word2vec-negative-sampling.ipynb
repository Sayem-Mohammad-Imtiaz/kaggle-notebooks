{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"review = pd.read_csv(\"/kaggle/input/IMDB Dataset.csv\")\nreview.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below code will create word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_comments = review['review'].tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nSTOPWORDS = '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\ncleanwords = re.compile(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 1000\nall_comments = []\nall_comments_set = set()\nfor comments in raw_comments:\n    #Limit the vocab size to vovab_size\n    if(len(all_comments_set) > vocab_size):\n        break\n    comments = re.sub(cleanwords,' ',comments)\n    temp = comments.split()\n    all_comments.append(temp)\n    all_comments_set.update(temp) #for adding a list to set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(all_comments_set)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(all_comments_set))\nword_to_int_dict = {}\nint_to_word_dict = {}\n\nfor i,word in enumerate(all_comments_set):\n    word_to_int_dict[word] = i\n    int_to_word_dict[i] = word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_to_int_dict['One'])\nprint(int_to_word_dict[word_to_int_dict['One']])\nvocab_size = len(int_to_word_dict)\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The below function will create a oneHot Representation of all words in dictonary\nlen_dict = len(int_to_word_dict)\noneHot = np.zeros((len_dict,len_dict))\nfor key in int_to_word_dict:\n    oneHot[key][key] = 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oneHot[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's use a window size of last 3 words to create the context/target pairs\ncontext_target_pair = []\nfor raw_comments in all_comments:\n    n = len(raw_comments)\n    for i in range(0,n-2):\n        print(\"adding pair :\",raw_comments[i],raw_comments[i+1])\n        pair=(raw_comments[i],raw_comments[i+1])\n        context_target_pair.append(pair)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_list = []\n\nfor i in range(0,len(context_target_pair)):\n    word1 = context_target_pair[i][0]\n    value1 = word_to_int_dict[word1]\n    word2 = context_target_pair[i][1]\n    value2 = word_to_int_dict[word2]\n    #print(word1,word2)\n    pair_list.append((value1,value2,1))\n\nprint(len(pair_list))\nprint(pair_list[0:2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's use Negative Sampling approach to create word embeddings. This means that for every X,Y pair we need to add K invalid X,Y pairs\nK=4\nimport random\n\ndatalen = len(pair_list)\nfor i in range(0,datalen):\n    for j in range(0,K):\n        rand_int = random.randint(0,vocab_size)\n        pair_list.append((value1,value2,0))\n\nprint(len(pair_list))\nm = len(pair_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.shuffle(pair_list)\npair_list[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temp = []\nY_temp = []\nX_target_temp = []\nfor i in range(0,m):\n    X_temp.append(oneHot[pair_list[i][0]])\n    Y_temp.append(pair_list[i][2])\n    X_target_temp.append(oneHot[pair_list[i][1]])\n\n#X = pd.DataFrame(X_train,columns=['x1','x2','valid'],dtype='float32')\nX_temp[1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(X_temp,dtype='float32')\nX_target_train = pd.DataFrame(X_target_temp,dtype='float32')\n\nY_train = pd.DataFrame(Y_temp,columns=['target'],dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_placeholders(n_x):\n    X1 = tf.placeholder(tf.float32,name='X',shape=(None,n_x))\n    Y1 = tf.placeholder(tf.float32,name='Y',shape=(None))\n    X1_target = tf.placeholder(tf.float32,name='X_target',shape=(None,n_x))\n\n    return X1,Y1,X1_target\n\nEMBEDDING_DIM = 300 \n\ndef get_minibatch(X,Y,X_target,batch_size):\n    m,n = X.shape\n    total_batch = (np.int32)(m/batch_size)\n    left_over = (np.int32)(m%batch_size)\n\n    minibatches = []\n    start=0\n    for i in range(0,total_batch):\n        end=start+batch_size\n        X_temp = X[start:end]\n        X_target_temp = X_target[start:end]\n        Y_temp = Y[start:end]\n        minibatches.append((X_temp,X_target_temp,Y_temp))\n        start = end\n    \n    X_temp = X[start:m]\n    X_target_temp = X_target[start:m]\n    Y_temp = Y[start:m]\n    minibatches.append((X_temp,X_target_temp,Y_temp))\n    \n    return minibatches\n\ndef init_params(n_x):\n\n    Xinitializer = tf.contrib.layers.xavier_initializer(dtype=tf.dtypes.float32)\n    #xavier_initializer_conv2d is designed to keep the scale of the gradients roughly the same in all layers\n    W1 = tf.Variable(Xinitializer(shape=(EMBEDDING_DIM,vocab_size)))\n    W2 = tf.Variable(Xinitializer(shape=(vocab_size,EMBEDDING_DIM)))\n    W3 = tf.Variable(Xinitializer(shape=(vocab_size,1)))\n\n    b1 = tf.Variable(Xinitializer(shape=(EMBEDDING_DIM,1)))\n    b2 = tf.Variable(Xinitializer(shape=(vocab_size,1)))\n    b3 = tf.Variable(Xinitializer(shape=(1,1)))\n\n    parameters = {\n        'W1' : W1,\n        'W2' : W2,\n        'W3' : W3,\n        'b1' : b1,\n        'b2' : b2,\n        'b3' : b3\n    }\n    \n    return parameters\n\ndef fwd_move(X,Y,X_target,parameters):\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n\n\n    print(\"W1 shape is :\",W1.shape)\n    print(\"X shape is :\",X.shape)\n    z1 = tf.matmul(W1,tf.transpose(X)) + b1\n    print(\"z1 shape is :\",z1.shape)\n\n    #We also need to make sure that input at is normalized. For Now we are leaving it\n    a1 = tf.nn.relu(z1)\n    print(\"a1 shape is :\",a1.shape)\n    print(\"W2 shape is :\",W2.shape)\n    print(\"b2 shape is :\",b2.shape)\n\n    z2 = tf.matmul(W2,a1) + b2\n    print(\"z2 shape is :\",z2.shape)\n    a2 = tf.nn.relu(z2)\n\n    #We have a2 here which represents a vector of vocab size binary clasification problems\n    z3 = tf.matmul(X_target,W3) + b3\n    print(\"z3 shape is :\",z3.shape)\n\n    return z3,parameters\n\ndef sigmoid_cost(z,Y):\n\n    logit = z\n    label=Y_train\n    print(\"Logit Shape :\",logit.shape)\n    print(\"Label Shape :\",label.shape)\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=label))\n    print(\"Inside Cost function Cost is: \",cost)\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    return (1/(1+np.exp(-z)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_target_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess = tf.Session()\ndef model(X_train,X_tatget_train,Y_train,num_epochs=1500,minibatch_size=32,learning_rate=0.001):\n    m,n_x = X_train.shape\n    parameters = init_params(n_x)\n    X1,Y1,X_target1 = create_placeholders(n_x)\n    z,params = fwd_move(X_train,Y_train,X_tatget_train,parameters)\n    cost = sigmoid_cost(z,Y_train)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n    total_batch = (np.int16)(m/minibatch_size)\n    init_op = tf.initialize_all_variables()\n    init = tf.global_variables_initializer()\n    #Graph for tensorflow\n    writer = tf.summary.FileWriter('C:/Users/sacgupt5/Documents/AI/graphs', sess.graph)\n    sess.run(init_op)\n    sess.run(init)\n    sess.run(parameters)\n    #sess.run(print(\"Shape of W1 is :\",W1.shape))\n    minibatches = get_minibatch(X_train,Y_train,X_target_train,minibatch_size)\n    for epoch in range (0,num_epochs):\n        #print(\"Epoch is :\", epoch)\n        minibatch_cost = 0\n        epoch_minibatch_cost = 0\n\n        for i in range (0,len(minibatches)):\n            X_mini,X_target_mini,Y_mini = minibatches[i]\n            #print(\"Minibatch Shape is \",X1.shape,Y1.shape)\n            #z1,params = sess.run(fwd_move(X1,X_target1,Y1,parameters,feed_dict={X: X_mini, Y: Y_mini,X_target: X_target_mini,parameters:parameters}))\n            _ , epoch_minibatch_cost = sess.run([optimizer, cost], feed_dict={X1: X_mini, Y1: Y_mini,X_target1: X_target_mini})\n            #_ , epoch_minibatch_cost = sess.run([optimizer, cost], feed_dict={z:z1,Y: Y_mini})\n            minibatch_cost = epoch_minibatch_cost + minibatch_cost\n        epoch_cost = minibatch_cost/total_batch\n\n        if(epoch % 100 == 0):\n            print(\"Epoch Error is \",epoch_cost)\n\n    print(\"Final Error is :\",epoch_cost)\n\n    #z_test,params = sess.run(fwd_move(X_test,Y_test,parameters))\n    #predicted_cost = (sigmoid(z_test))\n    #accuracy = tf.metrics.accuracy((predicted_cost),Y_test)\n    #print(\"Accuracy is : \", accuracy)\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = model(X_train,X_target_train,Y_train,num_epochs=400,minibatch_size=32,learning_rate=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W1 = parameters['W1']\nb1 = parameters['b1']\nprint(sess.run(W1))\nprint(sess.run(b1))\nvector = sess.run(W1+b1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = np.array(vector)\nEmbed = np.transpose(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Embed.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the code****\n**CosineSimilarity(u, v)=(u.v)/(||u||.||v||)=cos(θ)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity(u, v):\n    \"\"\"\n    Cosine similarity reflects the degree of similariy between u and v\n        \n    Arguments:\n        u -- a word vector of shape (n,)          \n        v -- a word vector of shape (n,)\n\n    Returns:\n        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n    \"\"\"\n\n    distance = 0.0\n    \n    dot = np.dot(u,v)\n    norm_u = np.linalg.norm(u)\n    \n    norm_v = np.linalg.norm(v)\n    cosine_similarity = ((dot)/(norm_u * norm_v))\n\n    return cosine_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = Embed[word_to_int_dict['out']]\nJake  = Embed[word_to_int_dict['Jake']]\nwonderful = Embed[word_to_int_dict['wonderful']]\nlittle = Embed[word_to_int_dict['little']]\nprint(\"cosine_similarity(out,Jake) = \", cosine_similarity(out, Jake))\nprint(\"cosine_similarity(wonderful,little) = \", cosine_similarity(wonderful, little))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"watching = Embed[word_to_int_dict['watching']]\nprint(\"cosine_similarity(wonderful,with) = \", cosine_similarity(watching,wonderful))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def euclidean_dist(vec1, vec2):\n    return np.sqrt(np.sum((vec1-vec2)**2))\n\ndef find_closest(word_index, vectors):\n    min_dist = 10000 # to act like positive infinity\n    min_index = -1\n    query_vector = vectors[word_index]\n    for index, vector in enumerate(vectors):\n        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n            min_dist = euclidean_dist(vector, query_vector)\n            min_index = index\n    return min_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(int_to_word_dict[find_closest(word_to_int_dict['with'],Embed)])\nprint(int_to_word_dict[find_closest(word_to_int_dict['out'],Embed)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(int_to_word_dict['out'])\nprint(int_to_word_dict[find_closest(word_to_int_dict['wonderful'],Embed)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}