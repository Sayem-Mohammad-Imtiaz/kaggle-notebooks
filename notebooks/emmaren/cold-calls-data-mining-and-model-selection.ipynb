{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","name":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":"# Cold Calls: Data Mining and Model Selection\n### Emma Ren\n### July 2017\n<br> This kernel aims to predict car insurance cold call success. It shows data exploration and visualization, along with feature engineering and model selection. Any comments/suggestions are welcome.","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2968d91615580d7b49a58cacb222a5394ef532a8","_cell_guid":"089cd104-5306-4bb6-aa5c-3b5245c1ed4d","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.stats import mode\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"75653dacc83277bfcda1127742eb526b741c4557","_cell_guid":"7cbab62e-4f14-446b-89dc-1f1c13edd60d","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Read-in train and test datasets\ntrain = pd.read_csv('../input/carInsurance_train.csv')\ntest = pd.read_csv('../input/carInsurance_test.csv')","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"7c795ea36e88a21b9c14679724480d4852738234","_cell_guid":"3cacdd33-734e-454b-8b60-8bdd47291eeb","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"print('The train dataset has %d observations and %d features' % (train.shape[0], train.shape[1]))\nprint('The test dataset has %d observations and %d features' % (test.shape[0], test.shape[1]))","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"a54704564e2de4d4b48809fcbd89f9fdc323091b","_cell_guid":"41bd97e9-ea68-485d-b1ce-223f4eae4669","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Data Exploration & Visualization","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"6f655935f2bf695e0c856b5df4e1dbbba6603bc6","_cell_guid":"f5f8ea66-05fb-4dbe-8bdb-a6085a8482ed","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Take a peak at the data\ntrain.describe()","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"e8ab81a6fa5115959ff9c5182ab64b40185183b3","_cell_guid":"5ca326fa-9449-4783-b99b-1339a0f204f6","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"train.describe(include=['O'])","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"ef98244e21401af511ce92e2d3863397d7ce61dc","_cell_guid":"0a181e2f-2897-48c5-bf89-2e846375e7b0","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"train.head()","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"7f5c54fff9240abbeeb52a5ca1ecf84fd34efc0f","_cell_guid":"7f73973f-358e-4843-b0eb-f28f138db248","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# First check out correlations among numeric features\n# Heatmap is a useful tool to get a quick understanding of which variables are important\ncolormap = plt.cm.viridis\ncor = train.corr()\ncor = cor.drop(['Id'],axis=1).drop(['Id'],axis=0)\nplt.figure(figsize=(12,12))\nsns.heatmap(cor,vmax=0.8,cmap=colormap,annot=True,fmt='.2f',square=True,annot_kws={'size':10},linecolor='white',linewidths=0.1)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"15fb0bd9400ff1cefff43e311e410947db6eef2b","_cell_guid":"eb89f342-b029-4364-bc68-990a79ba6445","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"Features are fairly independent, except DaysPassed and PreAttempts. Cold call success is positively correlated with PreAttemps,DaysPassed,Age and Balance, and negatively correlated with default, HHInsurance, CarLoan, LastContactDay and NoOfContacts.","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8998f25a078c0a9057262c5cd19bfa6387ebe63f","_cell_guid":"4fc33c1c-dfed-4f6f-98b7-f89ba13ffda9","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Next, pair plot some important features\nimp_feats = ['CarInsurance','Age','Balance','HHInsurance', 'CarLoan','NoOfContacts','DaysPassed','PrevAttempts']\nsns.pairplot(train[imp_feats],hue='CarInsurance',palette='viridis',size=2.5)\nplt.show()","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"00ac05acc3f7e716c8a2f5145186ce4c2dd631f6","_cell_guid":"c46b891a-8865-4013-8493-1c2899eb94d1","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"<br><b>Age</b>: It's interesting to see that seniors are more likely to buy car insurance. \n<br><b>Balance</b>: For balance, the data point at the upper right corner might be an outlier \n<br><b>HHInsurance</b>: Households insured are less likely to buy car insurance \n<br><b>CarLoan</b>: People with car loan are less likely to buy \n<br><b>NoOfContacts</b>: Too many contacts causes customer attrition\n<br><b>DaysPassed</b>: It looks like the more day passed since the last contact, the better\n<br><b>PrevAttempts</b>: Also, more previous attempts, less likely to buy. There is a potential outlier here ","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"1ac0b02c32b989345c9ca34be952c77f9828b21c","_cell_guid":"6951e259-9775-4b5b-8c27-e2c9a58f38dd","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Take a further look at Age\nfacet = sns.FacetGrid(train, hue='CarInsurance',size=5,aspect=3,palette='seismic')\nfacet.map(plt.hist,'Age',bins=30,alpha=0.5,normed=True)\nfacet.set(xlim=(0,train.Age.max()+10))\nfacet.add_legend()","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"0eb9f48690cbf781429423f95c4de85d1bef5124","_cell_guid":"5bc46738-3bea-45ad-9886-95fdd220b1ec","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"It looks like young people(<=30 years) and seniors are more likely to buy car insurance from this bank","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"40630e43655264a682222d5638c183cf026a1143","_cell_guid":"06bd3639-67b0-486b-9175-3c24fd69ba10","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Next check out categorical features\ncat_feats = train.select_dtypes(include=['object']).columns\nplt_feats = cat_feats[(cat_feats!= 'CallStart') & (cat_feats!='CallEnd')]\n\nfor feature in plt_feats:\n    plt.figure(figsize=(10,6))\n    sns.barplot(feature,'CarInsurance', data=train,palette='Set2') ","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"f43a9937235d6caf2548debc98f0af326b709039","_cell_guid":"8dc23ab1-999b-4472-88f7-b73e73ed59df","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"<br> <b>Job</b>: Student are most likely to buy insurance, followed by retired and unemployed folks.This is aligned with the age distribution. There might be some promotion targeting students?\n<br> <b>Marital status</b>: Married people are least likely to buy car insurance. Opportunities for developing family insurance business\n<br> <b>Education</b>: People with higher education are more likely to buy\n<br> <b>Communication</b>: No big difference between cellular and telephone\n<br> <b>Outcome in previous campaign</b>: Success in previous marketing campaign is largely associated with success in this campaign\n<br> <b>Contact Month</b>: Mar, Sep, Oct, and Dec are the hot months. It might be associated with school season? ","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7dc50980591f9eb9293f3c4a4b0fd61de7fd954f","_cell_guid":"9c501485-dd50-4145-93f8-6d9d6a5f2e1e","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Check outliers\n# From the pairplot, we can see there is an outlier with extreme high balance. Drop that obs here.\ntrain[train['Balance']>80000]\ntrain = train.drop(train[train.index==1742].index)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"86cd04d50621a71ffd4ba0053853333700227085","_cell_guid":"f616db7a-4de4-44d0-9760-46bc18bb874a","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Handling Miss Data","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"a9e186970536986838e0472d471d8c113ff20105","_cell_guid":"a6b676a1-61f2-44da-9e20-c2a321008b46","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# merge train and test data here in order to impute missing values all at once\nall=pd.concat([train,test],keys=('train','test'))\nall.drop(['CarInsurance','Id'],axis=1,inplace=True)\nprint(all.shape)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"5f7c8ea12c8317fe01a24ae2e2d0f4ddb65b214f","_cell_guid":"37497361-8178-4a51-83a8-bd42f1e963f8","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"total = all.isnull().sum()\npct = total/all.isnull().count()\nNAs = pd.concat([total,pct],axis=1,keys=('Total','Pct'))\nNAs[NAs.Total>0].sort_values(by='Total',ascending=False)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"4a0df30de185777901e9a235937112f9f0fd9c37","_cell_guid":"11571776-4cda-4af0-b266-098ea9b4c768","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"all_df = all.copy()\n\n# Fill missing outcome as not in previous campaign\nall_df[all_df['DaysPassed']==-1].count()\nall_df.loc[all_df['DaysPassed']==-1,'Outcome']='NoPrev'\n\n# Fill missing communication with none \nall_df['Communication'].value_counts()\nall_df['Communication'].fillna('None',inplace=True)\n\n# Fill missing education with the most common education level by job type\nall_df['Education'].value_counts()\n\n# Create job-education level mode mapping\nedu_mode=[]\njob_types = all_df.Job.value_counts().index\nfor job in job_types:\n    mode = all_df[all_df.Job==job]['Education'].value_counts().nlargest(1).index\n    edu_mode = np.append(edu_mode,mode)\nedu_map=pd.Series(edu_mode,index=all_df.Job.value_counts().index)\n\n# Apply the mapping to missing eductaion obs\nfor j in job_types:\n    all_df.loc[(all_df['Education'].isnull()) & (all_df['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\nall_df['Education'].fillna('None',inplace=True)\n\n# Fill missing job with none\nall_df['Job'].fillna('None',inplace=True)\n\n# Double check if there is still any missing value\nall_df.isnull().sum().sum()","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"e357cdcf053cf4dae535c88f64a5503ea88bd0d6","_cell_guid":"bef393f5-535e-4569-8156-e4bdbfe337e5","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Feature Engineering","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7615afec1f229a1c59e22ea0473901d58823982c","_cell_guid":"56788663-03b4-4c26-8586-677af2156a1d","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"There are three types of features:\n<br><b>Client features</b>: Age, Job, Marital, Education, Default, Balance, HHInsurance, CarLoan\n<br><b>Communication features</b>: LastContactDay, LastContactMonth, CallStart, CallEnd, Communication, NoOfContacts, DaysPassed\n<br><b>Previous campaign features</b>: PrevAttempts, Outcome\n","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"eb7a33c6185ce9cf64710d9e9d9c0847f407d9ce","_cell_guid":"fce403c7-598d-465d-8775-e76493f00e2d","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# First simplify some client features\n\n# Create age group based on age bands\nall_df['AgeBand']=pd.cut(all_df['Age'],5)\nprint(all_df['AgeBand'].value_counts())\n\nall_df.loc[(all_df['Age']>=17) & (all_df['Age']<34),'AgeBin'] = 1\nall_df.loc[(all_df['Age']>=34) & (all_df['Age']<49),'AgeBin'] = 2\nall_df.loc[(all_df['Age']>=49) & (all_df['Age']<65),'AgeBin'] = 3\nall_df.loc[(all_df['Age']>=65) & (all_df['Age']<80),'AgeBin'] = 4\nall_df.loc[(all_df['Age']>=80) & (all_df['Age']<96),'AgeBin'] = 5\nall_df['AgeBin'] = all_df['AgeBin'].astype(int)\n\n# Create balance groups\nall_df['BalanceBand']=pd.cut(all_df['Balance'],5)\nprint(all_df['BalanceBand'].value_counts())\nall_df.loc[(all_df['Balance']>=-3200) & (all_df['Balance']<17237),'BalanceBin'] = 1\nall_df.loc[(all_df['Balance']>=17237) & (all_df['Balance']<37532),'BalanceBin'] = 2\nall_df.loc[(all_df['Balance']>=37532) & (all_df['Balance']<57827),'BalanceBin'] = 3\nall_df.loc[(all_df['Balance']>=57827) & (all_df['Balance']<78122),'BalanceBin'] = 4\nall_df.loc[(all_df['Balance']>=78122) & (all_df['Balance']<98418),'BalanceBin'] = 5\nall_df['BalanceBin'] = all_df['BalanceBin'].astype(int)\n\nall_df = all_df.drop(['AgeBand','BalanceBand','Age','Balance'],axis=1)\n\n# Convert education level to numeric \nall_df['Education'] = all_df['Education'].replace({'None':0,'primary':1,'secondary':2,'tertiary':3})\n","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"ccd0bb08c51e4a0493d8d1ac732e0fdb216e1420","_cell_guid":"37157cbb-6405-4b60-8908-f6db8c215cc7","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Next create some new communication Features. This is the place feature engineering coming into play\n\n# Get call length\nall_df['CallEnd'] = pd.to_datetime(all_df['CallEnd'])\nall_df['CallStart'] = pd.to_datetime(all_df['CallStart'])\nall_df['CallLength'] = ((all_df['CallEnd'] - all_df['CallStart'])/np.timedelta64(1,'m')).astype(float)\nall_df['CallLenBand']=pd.cut(all_df['CallLength'],5)\nprint(all_df['CallLenBand'].value_counts())\n\n# Create call length bins\nall_df.loc[(all_df['CallLength']>= 0) & (all_df['CallLength']<11),'CallLengthBin'] = 1\nall_df.loc[(all_df['CallLength']>=11) & (all_df['CallLength']<22),'CallLengthBin'] = 2\nall_df.loc[(all_df['CallLength']>=22) & (all_df['CallLength']<33),'CallLengthBin'] = 3\nall_df.loc[(all_df['CallLength']>=33) & (all_df['CallLength']<44),'CallLengthBin'] = 4\nall_df.loc[(all_df['CallLength']>=44) & (all_df['CallLength']<55),'CallLengthBin'] = 5\nall_df['CallLengthBin'] = all_df['CallLengthBin'].astype(int)\nall_df = all_df.drop('CallLenBand',axis=1)\n\n# Get call start hour\nall_df['CallStartHour'] = all_df['CallStart'].dt.hour\nprint(all_df[['CallStart','CallEnd','CallLength','CallStartHour']].head())\n\n# Get workday of last contact based on call day and month, assuming the year is 2016\nall_df['LastContactDate'] = all_df.apply(lambda x:datetime.datetime.strptime(\"%s %s %s\" %(2016,x['LastContactMonth'],x['LastContactDay']),\"%Y %b %d\"),axis=1)\nall_df['LastContactWkd'] = all_df['LastContactDate'].dt.weekday\nall_df['LastContactWkd'].value_counts()\nall_df['LastContactMon'] = all_df['LastContactDate'].dt.month\nall_df = all_df.drop('LastContactMonth',axis=1)\n\n# Get week of last contact\nall_df['LastContactWk'] = all_df['LastContactDate'].dt.week\n\n# Get num of week in a month. There might be easier ways to do this, I will keep exploring. \nMonWk = all_df.groupby(['LastContactWk','LastContactMon'])['Education'].count().reset_index()\nMonWk = MonWk.drop('Education',axis=1)\nMonWk['LastContactWkNum']=0\nfor m in range(1,13):\n    k=0\n    for i,row in MonWk.iterrows():\n        if row['LastContactMon']== m:\n            k=k+1\n            row['LastContactWkNum']=k\n            \ndef get_num_of_week(df):\n    for i,row in MonWk.iterrows():\n        if (df['LastContactWk']== row['LastContactWk']) & (df['LastContactMon']== row['LastContactMon']):\n            return row['LastContactWkNum']\n\nall_df['LastContactWkNum'] = all_df.apply(lambda x: get_num_of_week(x),axis=1)\nprint(all_df[['LastContactWkNum','LastContactWk','LastContactMon']].head(10))","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"121d8a8284ddb32c226652359efacfef281acf6a","_cell_guid":"2b8cfd5e-72c6-4850-bcb1-f3912bf0c926","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"The two previous campaign features are good to go, no cleaning needed. I also tried to add some interactions and polynomial features, but none of them seems helpful. I am planning to explore more on this.  ","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"17bc927f5a46b25cb6be3946569ce567ba1d0ac4","_cell_guid":"fa3926bb-ba9b-4e43-9f17-4bf30181d08b","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"## Assembling Final Datasets","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"eb94c36879517557d3fd74821a694fd0a887bd80","_cell_guid":"0fabc27d-78f1-4030-850d-4bd8cfbec58f","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Spilt numeric and categorical features\ncat_feats = all_df.select_dtypes(include=['object']).columns\nnum_feats = all_df.select_dtypes(include=['float64','int64']).columns\nnum_df = all_df[num_feats]\ncat_df = all_df[cat_feats]\nprint('There are %d numeric features and %d categorical features\\n' %(len(num_feats),len(cat_feats)))\nprint('Numeric features:\\n',num_feats.values)\nprint('Categorical features:\\n',cat_feats.values)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"f1f6f815ce9aea6a5cb866ff4f0b85cc072682cb","_cell_guid":"55237fba-7f94-46c5-99ce-38c0336b8889","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# One hot encoding\ncat_df = pd.get_dummies(cat_df)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"e9ef860486f31340e953407e66871b0509daef04","_cell_guid":"67f0fd52-c36a-47d1-af72-d2f76e404edf","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Merge all features\nall_data = pd.concat([num_df,cat_df],axis=1)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"5837f4c1c64b09bffcf4656b4c3dd0169ad5c564","_cell_guid":"8bdbb212-8115-49ef-9d1e-cc3771c8727c","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Split train and test\nidx=pd.IndexSlice\ntrain_df=all_data.loc[idx[['train',],:]]\ntest_df=all_data.loc[idx[['test',],:]]\ntrain_label=train['CarInsurance']\nprint(train_df.shape)\nprint(len(train_label))\nprint(test_df.shape)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"533f032da408560dff835eb4205e3925f26f2c3f","_cell_guid":"0e234482-ebd1-4b55-8060-6c7c606f94ea","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Train test split\nx_train, x_test, y_train, y_test = train_test_split(train_df,train_label,test_size = 0.3,random_state=3)","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"16e6d17c844ea27551080213ee4a7bf02e605949","_cell_guid":"23665bef-ddf5-48e4-bc3d-4cdb005322ac","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Modeling","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"aec1275adbe763f9b5f571061701104c5bf15ee8","_cell_guid":"2f253542-4531-4770-96cc-cf2a72b5c5a7","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Create a cross validation function \ndef get_best_model(estimator, params_grid={}):\n    \n    model = GridSearchCV(estimator = estimator,param_grid = params_grid,cv=3, scoring=\"accuracy\", n_jobs= -1)\n    model.fit(x_train,y_train)\n    print('\\n--- Best Parameters -----------------------------')\n    print(model.best_params_)\n    print('\\n--- Best Model -----------------------------')\n    best_model = model.best_estimator_\n    print(best_model)\n    return best_model","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"49faa7224aae65364031d1ec461b88182428de98","_cell_guid":"1e4b6e2f-e665-446a-af8c-805734a83fca","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# Create a model fitting function\ndef model_fit(model,feature_imp=True,cv=5):\n\n    # model fit   \n    clf = model.fit(x_train,y_train)\n    \n    # model prediction     \n    y_pred = clf.predict(x_test)\n    \n    # model report     \n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')\n\n    print('\\n--- Train Set -----------------------------')\n    print('Accuracy: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv)),np.std(cross_val_score(clf,x_train,y_train,cv=cv))))\n    print('AUC: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc'))))\n    print('\\n--- Validation Set -----------------------------')    \n    print('Accuracy: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv)),np.std(cross_val_score(clf,x_test,y_test,cv=cv))))\n    print('AUC: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc'))))\n    print('-----------------------------------------------') \n\n    # feature importance \n    if feature_imp:\n        feat_imp = pd.Series(clf.feature_importances_,index=all_data.columns)\n        feat_imp = feat_imp.nlargest(15).sort_values()\n        plt.figure()\n        feat_imp.plot(kind=\"barh\",figsize=(6,8),title=\"Most Important Features\")\n","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"b4965250a8c8befb6b0ef3ad10f3883275327432","_cell_guid":"b5b6df2e-0508-4940-9ede-a1c4a71df0a5","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"# The confusion matrix plotting function is from the sklearn documentation below:\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = ['Success','Failure']\n","outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"ef339ddf21d1307d6d926838191703281728f766","_cell_guid":"ef61c6bd-7214-4eb2-bfb9-b9840402c57d","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### k-Nearest Neighbors (KNN)","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"3dc1375697a4378f294c8b9357bc9835a8cbda04","_cell_guid":"386085dc-c3a6-4d7c-a335-0e4feee887c5","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Let's start with KNN. An accuracy of 0.76 is not very impressive. I will just take this as the model benchmark. \nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':[5,6,7], \n              'p':[1,2],\n              'weights':['uniform','distance']}\nclf_knn = get_best_model(knn,parameters)\nmodel_fit(model=clf_knn, feature_imp=False)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"b87fc1a4fdc799bcbd62eaef70579702b4cc13c7","_cell_guid":"08be9081-39d2-42f7-9399-2c91b96c6528","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### Naive Bayes Classifier","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e696182ca6168fda03219b7a66a190a17a26cfe9","_cell_guid":"aab7a6b0-99ae-4fd1-a7f8-cd0fdb2adee8","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# As expected, Naive Bayes classifier doesn't perform well here. \n# There are multiple reasons. Some of the numeric features are not normally distributed, which is a strong assemption hold by Naive Bayes. \n# Also, features are definitely not independent.  \nclf_nb = GaussianNB()\nmodel_fit(model=clf_nb,feature_imp=False)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"512a8ea8ae8c4ef4fd66888ecdeca16a60557986","_cell_guid":"c82686bb-23d8-414f-8fb3-31efe21bb79b","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### Logistic Regression","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"db0c0d3bd2554ac0dbe54f8e0b09a55f5674ed6b","_cell_guid":"979bc524-df0c-48b6-8fb0-c7bb0d4b8064","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# We're making progress here. Logistic regression performs better than KNN. \nlg = LogisticRegression(random_state=3)\nparameters = {'C':[0.8,0.9,1], \n              'penalty':['l1','l2']}\nclf_lg = get_best_model(lg,parameters)\nmodel_fit(model=clf_lg, feature_imp=False)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"ab5eb1c9fcc98687904df2da36a1ae27ad67364b","_cell_guid":"ca70a253-7f1d-46c2-9ade-44b4ecf66d22","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### Random Forest","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5691996bddd0aa9f44fffc2d3b6aeb88794faffa","_cell_guid":"1da78838-948d-4381-bffb-6d3625798569","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# I did some manual parameter tuning here. This is the best model so far. \n# Based on the feature importance report, call length, last contact week, and previous success are strong predictors of cold call success\nrf = RandomForestClassifier(random_state=3)\nparameters={'n_estimators':[100],\n            'max_depth':[10],\n            'max_features':[13,14],\n            'min_samples_split':[11]}\nclf_rf= get_best_model(rf,parameters)\nmodel_fit(model=clf_rf, feature_imp=True)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"f9107003bee3eb19361fa5f2730064848d2b44c6","_cell_guid":"601ab481-cc64-49cc-9ed6-d5e42c0a6bc6","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### Support Vector Machines","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"83e69f6027125464c6b93e916263015d413aa4bf","_cell_guid":"9853e102-f2a5-4cc6-b1ef-b6219a647a16","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# try a SVM RBF model \nsvc = svm.SVC(kernel='rbf', probability=True, random_state=3)\nparameters = {'gamma': [0.005,0.01,0.02],\n              'C': [0.5,1,5]}\nclf_svc = get_best_model(svc, parameters)\nmodel_fit(model=clf_svc,feature_imp=False)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"33ebcdfbe8be0bb36fd69d9a5f2e23101e5c888e","_cell_guid":"b205e829-9c07-48e1-aa7f-60f51da65265","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"### XGBoost","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8dcb32466f1ac188cfee40de13b6b6df2c138459","_cell_guid":"428fdcbc-14f3-4446-8def-3ecb699ff251","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Finally let's try out XBGoost. As expected, it outperforms all other algorithms. \n# Also, based on feature importances, some of the newly created features such as call start hour, last contact week and weekday \n# have been picked as top features. \n\nimport xgboost as xgb\nxgb = xgb.XGBClassifier()\nparameters={'n_estimators':[900,1000,1100],\n            'learning_rate':[0.01],\n            'max_depth':[8],\n            'min_child_weight':[1],\n            'subsample':[0.8],\n            'colsample_bytree':[0.3,0.4,0.5]}\nclf_xgb= get_best_model(xgb,parameters)\nmodel_fit(model=clf_xgb, feature_imp=True)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"1895649fafcb7afd30e83abe3da7a1bc24029823","_cell_guid":"388142ed-3628-402d-bb49-9913718171c2","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Model Evaluation","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"71379f610c9d053a58b42e9b13ed2901633e6c9e","_cell_guid":"87a0c1fe-1922-40f3-9e7d-80187f2450ab","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# Compare model performance\nclfs= [clf_knn, clf_nb, clf_lg, clf_rf, clf_svc, clf_xgb]\nindex =['K-Nearest Neighbors','Naive Bayes','Logistic Regression','Random Forest','Support Vector Machines','XGBoost']\nscores=[]\nfor clf in clfs:\n    score = np.mean(cross_val_score(clf,x_test,y_test,cv=5,scoring = 'accuracy'))\n    scores = np.append(scores,score)\nmodels = pd.Series(scores,index=index)\nmodels.sort_values(ascending=False)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"b6162a566ae1501084db2054a9536f13f8a34b28","_cell_guid":"3f1adc20-4df3-4e2e-a21c-a020ff14840c","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Ensemble Voting","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"88ac9338e0305c1f621667a96f21d4bf98035b67","_cell_guid":"ef460171-8da1-462f-a108-5115522fc17d","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"# XGBoost and Random Forest show different important features, implying that those models are capturing different aspects of the data\n# To get the final model, I ensembled different classifiers based on majority voting.\n# XGBoost and Random Forest are given larger weights due to their better performance. \n\nclf_vc = VotingClassifier(estimators=[('xgb', clf_xgb),                                       \n                                      ('rf', clf_rf),\n                                      ('lg', clf_lg), \n                                      ('svc', clf_svc)], \n                          voting='hard',\n                          weights=[4,4,1,1])\nclf_vc = clf_vc.fit(x_train, y_train)","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"5a345aa1ba39ffd45c61f19cdb9975a6f40555e9","_cell_guid":"1267b1e2-3bf7-4459-9945-b03bee978c2f","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"print('Final Model Accuracy: %.5f'%(accuracy_score(y_test, clf_vc.predict(x_test))))","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"97578512f3560afd6fadfcdf587c619c37a06a99","_cell_guid":"489cb158-5143-4abf-85d6-9112a1e3e838","collapsed":false},"execution_count":null,"cell_type":"code"},{"source":"## Next Steps","outputs":[],"metadata":{"_execution_state":"busy","_uuid":"c14b4090e323ac6978759237c86c0eb183a5f58f","_cell_guid":"98805d17-0e22-42e7-b4e2-ee863e77671a","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"Here are some thoughts on steps for further improvements:\n<br> 1) Do more feature engineering, including exploring interaction and polynomial terms\n<br> 2) Visualize decision boundaries for some classifiers\n<br> 3) Introduce more base models for learning\n<br> 4) Try different ensembling approaches","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e7f1d6438cf9b26192f4016b34f7fa0793589803","_cell_guid":"33750d34-a5da-4a38-8677-9898ee5d98fd","collapsed":false},"execution_count":null,"cell_type":"markdown"},{"source":"","outputs":[],"metadata":{"_execution_state":"busy","trusted":false,"_uuid":"b78eea9b66de0fbc3ba85cbda9b7e8835a4ea31c","_cell_guid":"48d77e61-d975-4386-a291-997d3d294ec4","collapsed":false},"execution_count":null,"cell_type":"code"}]}