{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting some insight about our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(15,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset contains few categorical features and few continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True,cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are few features that have negative correlation with target and few with positive correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(df.target.unique(),df.target.value_counts(),color=['red','green'])\nplt.xticks([0,1])\nprint('No disease:{}%\\nDisease:{}%'.format(round(df.target.value_counts(normalize=True)[0],2)*100,\n                                           round(df.target.value_counts(normalize=True)[1],2)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\ncolumn_trans = make_column_transformer(\n                (OneHotEncoder(),['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']),\n                (StandardScaler(),['age', 'trestbps', 'chol', 'thalach', 'oldpeak']),\n                remainder = 'passthrough')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying OneHotEncoder on the categorical features and StandardScalar on continuous features"},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['target'], axis = 1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_trans.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Algorithms"},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nlogreg = LogisticRegression(solver='lbfgs')\npipe = make_pipeline(column_trans,logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_scores = []\nfor k in range(1,31):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    pipe = make_pipeline(column_trans,knn_classifier)\n    knn_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.plot([k for k in range(1, 31)], knn_scores, color = 'red')\nfor i in range(1,31):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1]*100,2)))\nplt.xticks([i for i in range(1, 31)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_scores[25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Support Vector Classifier (SVC)"},{"metadata":{},"cell_type":"markdown","source":"There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    pipe = make_pipeline(column_trans,svc_classifier)\n    svc_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.cm import rainbow\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.figure(figsize=(10,10))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_scores[0] #linear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    pipe = make_pipeline(column_trans,dt_classifier)\n    dt_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_scores[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    pipe = make_pipeline(column_trans,rf_classifier)\n    rf_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncolors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],5))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_scores[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Gaussian NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\npipe = make_pipeline(column_trans,nb)\ncross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is worth to test our model on 'testing dataset' using two classification models that are having high accuracy score than other classification models. They are (i) Logistic Regression and (ii) SVC using linear kernel."},{"metadata":{},"cell_type":"markdown","source":"## Make predictions on \"unseen\" data"},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = make_pipeline(column_trans,logreg)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. SVC with linear kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_classifier = SVC(kernel = 'linear')\npipe = make_pipeline(column_trans,svc_classifier)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suggestions are Welcome. Thank you!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}