{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center; background-color:#4DBDE3; font-weight:bold; color:white;border-radius: 50px 15px\">Coronavirus tweets</h1>","metadata":{}},{"cell_type":"markdown","source":"**To review different concepts in NLP, I looked at different notebooks and I really liked this one :**\n* https://www.kaggle.com/andreshg/commonlit-a-complete-analysis\n\n**Go take a look if you have time !**","metadata":{}},{"cell_type":"markdown","source":"<a id='table_contents'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   ðŸ“‹&ensp; Table of Contents &ensp;ðŸ“‹ \n</h1>\n\n* **[Import libraries](#libraries)**\n* **[Load data](#load_data)**\n* **[EDA](#eda)**\n    * [Missing values](#missing_val)\n    * [Target visualization](#target_vis)\n    * [Variables visualization](#var_vis)\n* **[Data Preprocessing](#data_preprocessing)**\n    * [Pretreatment / Corpus cleaning](#pretreatment)\n    * [Target processing](#target_process)\n* **[Tokens visualization](#tokens_vis)**\n    * [Top Words](#top_words)\n    * [WordCloud](#wordcloud)\n* **[Clustering](#clustering)**\n    * [KMeans](#kmeans)\n    * [Latent semantic analysis (LSA)](#lsa)\n    * [Latent Dirichlet Allocation (LDA)](#lda)\n* **[Vectorization](#vectorization)**\n    * [Count Vectorizer](#countvect)\n    * [TF-IDF Vectorizer](#tfidfvect)\n    * [Continuous Bag of Word (CBOW)](#cbow)\n    * [Skip-Gram (SG)](#sg)\n    * [Test learnt embeddings](#learn_embeddings)\n* **[Modeling](#modeling)**\n    * [Class rebalancing (Re-sampling)](#class_rebalancing)\n    * [XGBoost](#xgboost)\n    * [Linear Support Vector Classification](#linsvc)\n    * [LSTM](#lstm)","metadata":{}},{"cell_type":"markdown","source":"<a id='libraries'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n    Import libraries\n</h1>","metadata":{}},{"cell_type":"code","source":"# Visualizations\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Text manipulations\nimport re\nimport string\nimport unicodedata\n\n# Natural Language Toolkit\nfrom nltk import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\n# WordCloud\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n# Scikit-learn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split, learning_curve\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, f1_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n\n#XGBoost model\nimport xgboost as xgb\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\n# Imbalanced-Learn Library\nfrom imblearn.under_sampling import RandomUnderSampler, AllKNN, NearMiss\nfrom imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE , SMOTE, ADASYN\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import SparseCategoricalCrossentropy\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import (LSTM, Embedding, BatchNormalization, Dense, \n                        Dropout, Bidirectional, GlobalMaxPool1D)\n\n# Warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Others\nimport time\nfrom pprint import pprint\nfrom collections import Counter\n\n# Some colors for prints\nclass print_color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T16:36:33.860797Z","iopub.execute_input":"2021-09-04T16:36:33.861207Z","iopub.status.idle":"2021-09-04T16:36:33.881651Z","shell.execute_reply.started":"2021-09-04T16:36:33.86117Z","shell.execute_reply":"2021-09-04T16:36:33.880649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='load_data'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n    Load data\n</h1>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\")\ntest = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\")\nprint(f\"Train shape : {train.shape}\\nTest shape  : {test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:18.484329Z","iopub.execute_input":"2021-09-04T15:48:18.484789Z","iopub.status.idle":"2021-09-04T15:48:18.935033Z","shell.execute_reply.started":"2021-09-04T15:48:18.484731Z","shell.execute_reply":"2021-09-04T15:48:18.933647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:18.937711Z","iopub.execute_input":"2021-09-04T15:48:18.938185Z","iopub.status.idle":"2021-09-04T15:48:18.968231Z","shell.execute_reply.started":"2021-09-04T15:48:18.938137Z","shell.execute_reply":"2021-09-04T15:48:18.966888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:18.970166Z","iopub.execute_input":"2021-09-04T15:48:18.970602Z","iopub.status.idle":"2021-09-04T15:48:18.98499Z","shell.execute_reply.started":"2021-09-04T15:48:18.97056Z","shell.execute_reply":"2021-09-04T15:48:18.983233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='eda'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   EDA\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='missing_val'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Missing Values\n</h2>","metadata":{}},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"print(train.isna().sum())\nmsno.bar(train, color=px.colors.qualitative.D3[0], sort=\"ascending\", figsize=(10,5), fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:18.986926Z","iopub.execute_input":"2021-09-04T15:48:18.987596Z","iopub.status.idle":"2021-09-04T15:48:19.668249Z","shell.execute_reply.started":"2021-09-04T15:48:18.987543Z","shell.execute_reply":"2021-09-04T15:48:19.667306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"print(test.isna().sum())\nmsno.bar(test, color=px.colors.qualitative.D3[2], sort=\"ascending\", figsize=(10,5), fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:19.669743Z","iopub.execute_input":"2021-09-04T15:48:19.670391Z","iopub.status.idle":"2021-09-04T15:48:20.187375Z","shell.execute_reply.started":"2021-09-04T15:48:19.670338Z","shell.execute_reply":"2021-09-04T15:48:20.186211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some values are missing in the **Location** columns, but these columns will not be important to us.","metadata":{}},{"cell_type":"markdown","source":"<a id='target_vis'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Target visualization\n</h2>","metadata":{}},{"cell_type":"code","source":"val_counts = train[\"Sentiment\"].value_counts()\n\nfig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n        subplot_titles=(\"Pie\", \"Bar\")\n    )\n\nfig.add_trace(\n     px.pie(val_counts, \n            names=val_counts.index, \n            values=val_counts.values,\n            hole=0.4,\n            opacity=0.9\n    ).data[0],\n     row=1, col=1\n    \n).update_traces(\n    textposition='inside', textinfo='percent+label'\n).update_layout(\n    title=dict(text='<b>Sentiment distribution<b>', x=0.48), \n    legend=dict(x=0, y=-0.05, orientation='h')\n)\n\nfig.add_trace(\n     go.Bar(x=val_counts.index, \n            y=val_counts.values,\n            marker_color=px.colors.qualitative.Set2,\n            showlegend=False\n    ),\n     row=1, col=2\n)\n\nfig['layout']['annotations'][0].update(text='Sentiment', x=0.225, y=0.47, showarrow=False, font_size=15, opacity=0.9)\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:48:20.189148Z","iopub.execute_input":"2021-09-04T15:48:20.189569Z","iopub.status.idle":"2021-09-04T15:48:21.779164Z","shell.execute_reply.started":"2021-09-04T15:48:20.189524Z","shell.execute_reply":"2021-09-04T15:48:21.777868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, this dataset is **unbalanced**.","metadata":{}},{"cell_type":"markdown","source":"<a id='var_vis'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Variables visualization\n</h2>","metadata":{}},{"cell_type":"markdown","source":"### Location","metadata":{}},{"cell_type":"code","source":"#20 locations that appear the most\ncounts = train[\"Location\"].value_counts().sort_values(ascending=False)[:20]\n\ndf = train.loc[train[\"Location\"].isin(counts.index)]\ndf = df.groupby(by=[\"Location\",\"Sentiment\"])[\"OriginalTweet\"].count().reset_index(name=\"Count\")\n\nfig = px.bar(df, x='Location', y='Count', color='Sentiment', \n             title=\"Top 20 <b>locations that appear the most<b>\", \n             color_discrete_sequence=[\"#F96C6C\", \"#F9B06C\", \"#F9F96C\", \"#AFF96C\", \"#3DF961\"],\n             category_orders={\n                 \"Location\": counts.index, \n                 \"Sentiment\": [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n             }\n        )\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:48:21.782768Z","iopub.execute_input":"2021-09-04T15:48:21.783213Z","iopub.status.idle":"2021-09-04T15:48:21.956127Z","shell.execute_reply.started":"2021-09-04T15:48:21.783178Z","shell.execute_reply":"2021-09-04T15:48:21.9552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tweets are generally from **London** and the **United States**.","metadata":{}},{"cell_type":"markdown","source":"### TweetAt","metadata":{}},{"cell_type":"code","source":"counts = pd.to_datetime(train[\"TweetAt\"]).value_counts().sort_index()\nfig = px.bar(counts, x=counts.index, y=counts.values, title=\"<b>Number of tweets by date<b>\")\nfig.update_layout(xaxis_title=\"Date\", yaxis_title=\"Number of tweets\")\nfig.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-04T15:48:21.957869Z","iopub.execute_input":"2021-09-04T15:48:21.958323Z","iopub.status.idle":"2021-09-04T15:48:22.049948Z","shell.execute_reply.started":"2021-09-04T15:48:21.958292Z","shell.execute_reply":"2021-09-04T15:48:22.049206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that many tweets date from **May**.","metadata":{}},{"cell_type":"markdown","source":"### OriginalTweet","metadata":{}},{"cell_type":"code","source":"df = train.copy()\ndf['tweet_len'] = df[\"OriginalTweet\"].apply(lambda x : len(x))\ndf['tweet_word_count'] = df[\"OriginalTweet\"].apply(lambda x : len(x.split(' ')))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:22.05096Z","iopub.execute_input":"2021-09-04T15:48:22.051458Z","iopub.status.idle":"2021-09-04T15:48:22.216259Z","shell.execute_reply.started":"2021-09-04T15:48:22.051428Z","shell.execute_reply":"2021-09-04T15:48:22.215225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, \n                    subplot_titles=(\"Distribution of <b>tweets length<b>\", \"Distribution of the <b>number of words<b>\"))\n\ndisplot1 = ff.create_distplot(\n    [df['tweet_len']], \n    ['tweet_len'], \n    bin_size=10, \n    show_rug=False,\n    colors=['#7FA6EE']\n)\n\ndisplot2 = ff.create_distplot(\n    [df['tweet_word_count']], \n    ['tweet_word_count'], \n    bin_size=10, \n    show_rug=False,\n    colors=['#EFD07F']\n)\n\n# tweet_len distribution plot\nfig.add_trace(\n    go.Histogram(displot1['data'][0]), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(displot1['data'][1], line=dict(color='blue', width=0.5)), \n    row=1, col=1\n)\n\n# tweet_word_count distribution plot\nfig.add_trace(\n    go.Histogram(displot2['data'][0]), \n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(displot2['data'][1], line=dict(color='orange', width=0.5)), \n    row=1, col=2\n)\n\nfig.update_xaxes(title_text = \"Length\", row=1, col=1)\nfig.update_xaxes(title_text = \"Number\", row=1, col=2)\nfig.update_layout(showlegend=False)\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:48:22.217664Z","iopub.execute_input":"2021-09-04T15:48:22.217982Z","iopub.status.idle":"2021-09-04T15:48:23.479711Z","shell.execute_reply.started":"2021-09-04T15:48:22.217952Z","shell.execute_reply":"2021-09-04T15:48:23.478409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='data_preprocessing'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Data Preprocessing\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='pretreatment'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Pretreatment / Corpus cleaning\n</h2>","metadata":{}},{"cell_type":"code","source":"def pretreatment(text, lang='english', keepStopWords=set(), stem=True, lemma=False, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    #Make text lowercase\n    text = text.lower()\n    #Remove text in square brackets\n    text = re.sub('\\[.*?\\]', '', text)\n    #Remove links\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    #Remove punctuations\n    punc = string.punctuation  # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n    punc += '\\n\\r\\t'\n    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n    #Remove numbers\n    text = re.sub('[0-9]+', '', text)\n    #Removal of accents and non-standard characters\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    \n    #Lemmatization\n    if lemma :\n        tokens = word_tokenize(text)\n        doc = nlp(\" \".join(tokens)) \n        text = ' '.join([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n        \n    #Stemming    \n    if stem :\n        stemmer = SnowballStemmer(lang)\n        tokens = word_tokenize(text)\n        text = ' '.join([stemmer.stem(item) for item in tokens])\n    \n    #Remove stop words\n    stopwords_list = set(stopwords.words(lang)) - keepStopWords\n    words = word_tokenize(text)\n    text = ' '.join([word for word in words if word not in stopwords_list])\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:23.481598Z","iopub.execute_input":"2021-09-04T15:48:23.481938Z","iopub.status.idle":"2021-09-04T15:48:23.493526Z","shell.execute_reply.started":"2021-09-04T15:48:23.481887Z","shell.execute_reply":"2021-09-04T15:48:23.492233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lemmatizer\n* **[spacy.load()](https://spacy.io/api/top-level#spacy.load)**\n> Load a pipeline using the name of an installed package, a string path or a Path-like object.\n\n**Useful links :** \n* https://spacy.io/usage/processing-pipelines\n* https://spacy.io/api/token#attributes","metadata":{}},{"cell_type":"code","source":"import spacy\n!python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner']) # for lemmatization","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-04T15:48:23.49513Z","iopub.execute_input":"2021-09-04T15:48:23.495458Z","iopub.status.idle":"2021-09-04T15:48:38.161322Z","shell.execute_reply.started":"2021-09-04T15:48:23.495428Z","shell.execute_reply":"2021-09-04T15:48:38.159864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test function","metadata":{}},{"cell_type":"code","source":"text = \"\"\"   \\tHi I will test my CLEANING FUNCTION.\\n\\n\n[I want to thank you for taking your time to read this notebook] \\n\n* Here is the link to this notebook : https://www.kaggle.com/pascalzg/coronavirus-tweets-nlp !!\nWhat do you think of this notebook? ?? =)\\n\nIt's 7:09 pm, I have to take a break to go eat.\\n\nHere are some French words with accents: J'ai mangÃ© une baguette et c'Ã©tait trÃ¨s bon !! miaaam\nÎ± Î² Î³ Î´ Îµ  Î¶Î· Î¸\nI ate cheese.\nIs what I write amazing, incredible, unmissable ? I hope so because that is the end of this text HAHA.\n\"\"\"\n\nprint(f\"{print_color.BOLD}{print_color.UNDERLINE}Original text :{print_color.END}\\n\\n{text}\\n\")\nprint(f\"{print_color.BOLD}{print_color.UNDERLINE}Cleaned text :{print_color.END}\\n\\n{pretreatment(text, stem=True, lemma=False)}\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-04T15:48:38.163282Z","iopub.execute_input":"2021-09-04T15:48:38.163621Z","iopub.status.idle":"2021-09-04T15:48:38.198804Z","shell.execute_reply.started":"2021-09-04T15:48:38.163586Z","shell.execute_reply":"2021-09-04T15:48:38.197889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It seems that our function works !**","metadata":{}},{"cell_type":"markdown","source":"### Cleaning tweets","metadata":{}},{"cell_type":"code","source":"keep_words = {'not', 'could', 'would'} # Can be useful in Sentiment Analysis\n\ndf['tweet_clean'] = df['OriginalTweet'].apply(pretreatment, keepStopWords=keep_words, stem=True, lemma=False)\ntest['tweet_clean'] = test['OriginalTweet'].apply(pretreatment, keepStopWords=keep_words, stem=True, lemma=False)\n\ndf[[\"OriginalTweet\", \"tweet_clean\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:48:38.200197Z","iopub.execute_input":"2021-09-04T15:48:38.200495Z","iopub.status.idle":"2021-09-04T15:49:41.293102Z","shell.execute_reply.started":"2021-09-04T15:48:38.200459Z","shell.execute_reply":"2021-09-04T15:49:41.292039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='target_process'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Target Processing\n</h2>\n\nTo make the prediction easier, we will reduce the number of classes for our target by renaming some classes as :\n* Extremely Positive -> Positive\n* Extremely Negative -> Negative","metadata":{}},{"cell_type":"code","source":"replace_map = {\"Sentiment\": {\"Extremely Positive\" : \"Positive\", \n                             \"Extremely Negative\" : \"Negative\"} \n              }\n\ndf[\"target\"] = pd.DataFrame(df[\"Sentiment\"]).replace(replace_map)\ntest[\"target\"] = pd.DataFrame(test[\"Sentiment\"]).replace(replace_map)\n\nprint(f\"{df['Sentiment'].value_counts()}\\n\\n{df['target'].value_counts()}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:49:41.294457Z","iopub.execute_input":"2021-09-04T15:49:41.294757Z","iopub.status.idle":"2021-09-04T15:49:41.343606Z","shell.execute_reply.started":"2021-09-04T15:49:41.294725Z","shell.execute_reply":"2021-09-04T15:49:41.342388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_counts = df[\"Sentiment\"].value_counts()\ntarget_counts = df[\"target\"].value_counts()\n\nfig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n        subplot_titles=(\"Sentiment distribution\", \"Target distribution\")\n    )\n\nfig.add_trace(\n     go.Bar(x=sentiment_counts.index, \n            y=sentiment_counts.values,\n            marker_color=['#37E984','#F89898','#F4F77F','#4DC215','#E73F3F'],\n            showlegend=False\n    ),\n     row=1, col=1\n)\n\nfig.add_trace(\n     go.Bar(x=target_counts.index, \n            y=target_counts.values,\n            marker_color=['#37E984','#F89898','#F4F77F'],\n            showlegend=False\n    ),\n     row=1, col=2\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:49:41.345779Z","iopub.execute_input":"2021-09-04T15:49:41.346089Z","iopub.status.idle":"2021-09-04T15:49:41.415346Z","shell.execute_reply.started":"2021-09-04T15:49:41.346059Z","shell.execute_reply":"2021-09-04T15:49:41.414108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The target column is still **not balanced**.\n* Now, let's **encode** our target column.","metadata":{}},{"cell_type":"code","source":"target_map = {\"Negative\" : 0, \"Neutral\" : 1, \"Positive\" : 2}\n\ndf[\"encoded_target\"] = df['target'].map(target_map)\ntest[\"encoded_target\"] = test['target'].map(target_map)\n\ndf[[\"target\", \"encoded_target\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:49:41.416951Z","iopub.execute_input":"2021-09-04T15:49:41.417331Z","iopub.status.idle":"2021-09-04T15:49:41.455549Z","shell.execute_reply.started":"2021-09-04T15:49:41.417296Z","shell.execute_reply":"2021-09-04T15:49:41.454213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df[\"tweet_clean\"]\ny_train = df[\"encoded_target\"]\nX_test = test[\"tweet_clean\"]\ny_test = test[\"encoded_target\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:49:41.456882Z","iopub.execute_input":"2021-09-04T15:49:41.457238Z","iopub.status.idle":"2021-09-04T15:49:41.463093Z","shell.execute_reply.started":"2021-09-04T15:49:41.457206Z","shell.execute_reply":"2021-09-04T15:49:41.461886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='tokens_vis'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Tokens visualization\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='top_words'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Top Words \n</h2>","metadata":{}},{"cell_type":"code","source":"def get_top_grams(corpus, ngram_range=(1,1), nbwords=10):\n    vect = CountVectorizer(ngram_range=ngram_range)\n    counts = vect.fit_transform(corpus).toarray().sum(axis=0)\n    \n    # Sort the index of the nbwords most frequent words (note : argsort() is an ascending sort)\n    argsort_descending = counts.argsort()[::-1][:nbwords]\n    names = np.array(vect.get_feature_names())[argsort_descending]\n    counts_sorted = counts[argsort_descending]\n    return names, counts_sorted\n\ndef plot_bar(names, counts_sorted, color_groups, title):\n    fig = go.Figure(go.Bar(x=counts_sorted, \n                           y=names,\n                           orientation='h',\n                           marker_color = ['#097394']*color_groups[0] + ['#52B1CE']*color_groups[1] + ['#A8E0F2']*color_groups[2]),\n                   )\n    \n    fig.update_traces(marker_line_color = '#077092', opacity=0.8)\n    fig['layout']['yaxis']['autorange'] = \"reversed\"\n    \n    fig.update_layout(title=title)\n    fig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:49:41.464717Z","iopub.execute_input":"2021-09-04T15:49:41.465057Z","iopub.status.idle":"2021-09-04T15:49:41.47593Z","shell.execute_reply.started":"2021-09-04T15:49:41.465024Z","shell.execute_reply":"2021-09-04T15:49:41.474799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unigrams","metadata":{}},{"cell_type":"code","source":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(1,1), nbwords=nb_words)\nplot_bar(names, counts, (1,4,nb_words), f\"Top {nb_words} <b>Unigram<b>\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:49:41.477076Z","iopub.execute_input":"2021-09-04T15:49:41.477387Z","iopub.status.idle":"2021-09-04T15:49:49.137814Z","shell.execute_reply.started":"2021-09-04T15:49:41.477355Z","shell.execute_reply":"2021-09-04T15:49:49.136609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigrams","metadata":{}},{"cell_type":"code","source":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(2,2), nbwords=nb_words)\nplot_bar(names, counts, (1,5,nb_words), f\"Top {nb_words} <b>Bigrams<b>\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:49:49.13954Z","iopub.execute_input":"2021-09-04T15:49:49.139865Z","iopub.status.idle":"2021-09-04T15:50:36.375616Z","shell.execute_reply.started":"2021-09-04T15:49:49.139834Z","shell.execute_reply":"2021-09-04T15:50:36.374255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trigrams","metadata":{}},{"cell_type":"code","source":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(3,3), nbwords=nb_words)\nplot_bar(names, counts, (2,4,nb_words), f\"Top {nb_words} <b>Trigrams<b>\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:50:36.380467Z","iopub.execute_input":"2021-09-04T15:50:36.380888Z","iopub.status.idle":"2021-09-04T15:51:47.938285Z","shell.execute_reply.started":"2021-09-04T15:50:36.380847Z","shell.execute_reply":"2021-09-04T15:51:47.93716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='wordcloud'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   WordCloud \n</h2>","metadata":{}},{"cell_type":"code","source":"# https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n\nmask = np.array(Image.open('../input/wordcloudmasks/mask.jpg'))\n\nwordcloud = WordCloud(background_color='white',\n                      max_words=100,\n                      mask=mask)\n\nwordcloud.generate(' '.join(text for text in df['tweet_clean']))\n\nplt.figure(figsize=(20,10))\nplt.title(\"Top words\", fontdict={'size': 20})\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:51:47.940618Z","iopub.execute_input":"2021-09-04T15:51:47.940924Z","iopub.status.idle":"2021-09-04T15:51:55.940812Z","shell.execute_reply.started":"2021-09-04T15:51:47.940879Z","shell.execute_reply":"2021-09-04T15:51:55.939359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='clustering'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Clustering\n</h1>","metadata":{}},{"cell_type":"code","source":"# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2)) # Unigrams + Bigrams\nvectors = vectorizer.fit_transform(df['tweet_clean'])\n\nprint(\"Number of texts :\",vectors.shape[0],\"\\nNumber of words (Unigrams + Bigrams) :\", vectors.shape[1])\nprint(\"Number of non-zero entries :\",vectors.nnz)\nprint(\"Sparsity measurement :\",int(vectors.nnz/float(vectors.shape[0])),\"active words per text out of\", vectors.shape[1],\"!\")\n\n# To find the words\nprint('\\n',[(i,vectorizer.get_feature_names()[i]) for i in np.random.randint(vectors.shape[1], size=5)])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:51:55.942959Z","iopub.execute_input":"2021-09-04T15:51:55.943398Z","iopub.status.idle":"2021-09-04T15:52:03.783538Z","shell.execute_reply.started":"2021-09-04T15:51:55.943355Z","shell.execute_reply":"2021-09-04T15:52:03.782341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='kmeans'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   KMeans\n</h2>","metadata":{}},{"cell_type":"code","source":"n_clusters = 3\nn_words = 6\nseed = 0\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=seed, max_iter=10)\nkmeans.fit(vectors)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(kmeans.labels_):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()\n    \n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i//2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i//2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i//2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:52:03.78546Z","iopub.execute_input":"2021-09-04T15:52:03.785979Z","iopub.status.idle":"2021-09-04T15:52:23.577126Z","shell.execute_reply.started":"2021-09-04T15:52:03.785929Z","shell.execute_reply":"2021-09-04T15:52:23.576006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='lsa'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Latent semantic analysis (LSA)\n</h2>","metadata":{}},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n\nn_clusters = 3\nn_words = 6\nseed = 0\n\nsvd = TruncatedSVD(n_components=n_clusters, n_iter=10, random_state=seed)\nreduc_svd = svd.fit_transform(vectors)\n\nlabels = np.argmax(reduc_svd, axis=1)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(labels):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()   \n\n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i//2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i//2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i//2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:52:23.578631Z","iopub.execute_input":"2021-09-04T15:52:23.578989Z","iopub.status.idle":"2021-09-04T15:52:28.139645Z","shell.execute_reply.started":"2021-09-04T15:52:23.578952Z","shell.execute_reply":"2021-09-04T15:52:28.138096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='lda'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Latent Dirichlet Allocation (LDA)\n</h2>","metadata":{}},{"cell_type":"markdown","source":"### **[Latent Dirichlet Allocation : sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)**","metadata":{}},{"cell_type":"code","source":"n_clusters = 3\nn_words = 6\nseed = 0\n\nlda = LatentDirichletAllocation(n_components=n_clusters, max_iter=10, random_state=seed)\nreduc_lda = lda.fit_transform(vectors)\n\nlabels = np.argmax(reduc_lda, axis=1)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(labels):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()   \n\n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i//2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i//2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i//2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:52:28.14166Z","iopub.execute_input":"2021-09-04T15:52:28.142146Z","iopub.status.idle":"2021-09-04T15:54:36.239178Z","shell.execute_reply.started":"2021-09-04T15:52:28.142099Z","shell.execute_reply":"2021-09-04T15:54:36.237763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **[Latent Dirichlet Allocation - Gensim](https://radimrehurek.com/gensim/models/ldamodel.html)**\n\n> Step-by-step explanation : https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n\n* **gensim.utils.simple_preprocess(doc, deacc=False, min_len=2, max_len=15)**\n> Convert a document into a **list of tokens**.<br/>\n> This lowercases, tokenizes, de-accents (optional).\n\n* **gensim.corpora.Dictionary().doc2bow(document, allow_update=False, return_missing=False)**\n> Convert document into the **bag-of-words (BoW)** format = list of (token_id, token_count) tuples.","metadata":{}},{"cell_type":"code","source":"def tokenization(texts):\n    for text in texts:\n        yield(simple_preprocess(str(text)))  # deacc=True removes punctuations\n\n# List of tokenized tweets\ndata_words = list(tokenization(df['tweet_clean']))\nprint(\"Tokenization of the first tweet :\",data_words[0])\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n\n# Create Corpus\ntexts = data_words\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\nprint(\"and his Term Document Frequency :\",corpus[0])\n\n# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:54:36.241196Z","iopub.execute_input":"2021-09-04T15:54:36.241721Z","iopub.status.idle":"2021-09-04T15:54:41.516505Z","shell.execute_reply.started":"2021-09-04T15:54:36.24167Z","shell.execute_reply":"2021-09-04T15:54:41.515389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **print_topics(num_topics=20, num_words=10)**\n> Get the most significant topics (alias for show_topics() method)","metadata":{}},{"cell_type":"code","source":"seed = 0\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=3, \n                                           random_state=seed,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\n# Print the most significant topics\npprint(f\"The most significant topics :\\n{lda_model.print_topics()}\")\ndoc_lda = lda_model[corpus]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-04T15:54:41.518007Z","iopub.execute_input":"2021-09-04T15:54:41.51835Z","iopub.status.idle":"2021-09-04T15:57:52.312462Z","shell.execute_reply.started":"2021-09-04T15:54:41.518315Z","shell.execute_reply":"2021-09-04T15:57:52.309632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WordClouds\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate_from_frequencies(dict(topics[i][1]))\n    axs[i//2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i//2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i//2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-04T15:57:52.314352Z","iopub.execute_input":"2021-09-04T15:57:52.314861Z","iopub.status.idle":"2021-09-04T15:57:52.792646Z","shell.execute_reply.started":"2021-09-04T15:57:52.314811Z","shell.execute_reply":"2021-09-04T15:57:52.791428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **log_perplexity(chunk, total_docs=None)**\n> Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n\n* **[CoherenceModel()](https://radimrehurek.com/gensim/models/coherencemodel.html).get_coherence()**\n> Get coherence value based on pipeline parameters.","metadata":{}},{"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:57:52.794193Z","iopub.execute_input":"2021-09-04T15:57:52.79449Z","iopub.status.idle":"2021-09-04T15:58:10.190444Z","shell.execute_reply.started":"2021-09-04T15:57:52.794458Z","shell.execute_reply":"2021-09-04T15:58:10.189272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **[pyLDAvis.gensim_models](https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/gensim_models.py).prepare(topic_model, corpus, dictionary, doc_topic_dist=None, **kwargs)**\n> Transforms the Gensim TopicModel and related corpus and dictionary into the data structures needed for the visualization.","metadata":{}},{"cell_type":"code","source":"# Visualize the topics\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word) # sort_topics=False, to not reorder by size\n# vis","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:58:10.192274Z","iopub.execute_input":"2021-09-04T15:58:10.192623Z","iopub.status.idle":"2021-09-04T15:58:21.520676Z","shell.execute_reply.started":"2021-09-04T15:58:10.192589Z","shell.execute_reply":"2021-09-04T15:58:21.519118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='vectorization'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Vectorization\n</h1>\n\nThere are different ways to vectorize a text, for example, by frequency or by prediction :\n\n* **by frequency :**\n    * **CountVectorizer (scikit-learn) :**\n        * counts the number of times the words appear\n        * favors the most frequent words\n        * ignores a bit the rare words that could have been interesting<br/><br/>\n    * **TfidfVectorizer (scikit-learn) :**\n        * considers the global weighting of words\n        * penalizes the most frequent words\n        * does not understand the semantic meaning of words <br/><br/>\n* **by prediction :**\n    * Solves the problem of the **semantic meaning** of words\n    * Uses **embeddings** to vectorize texts before classification<br/>\n    * **Continuous Bag of Word (CBOW) :**\n        * predicts a **word given its context**<br/>\n    * **Skip-Gram (SG) :**\n        * predicts a **context given a word**\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id='countvect'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Count Vectorizer\n</h2>\n\n**[Count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)** counts the number of times the words appear.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Pipelines\npip_uni = Pipeline([ ('countUni', CountVectorizer(ngram_range=(1, 1))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('countBi', CountVectorizer(ngram_range=(2, 2))), ('clf', LinearSVC()) ])\npip_tri = Pipeline([ ('countTri', CountVectorizer(ngram_range=(3, 3))), ('clf', LinearSVC()) ])\npip = Pipeline([ ('countUniBi', CountVectorizer(ngram_range=(1, 2))), ('clf', LinearSVC()) ])\n\n# Lists\nnames = [\"Unigrams\",\"Bigrams\",\"Trigrams\",\"Uni+Bigrams\"]\npipelines = [pip_uni, pip_bi, pip_tri, pip]\nres = []\n\n# Results\nfor i, p in enumerate(pipelines) :\n    val_score = cross_val_score(p, df['tweet_clean'], df[\"encoded_target\"]).mean()\n    res.append({'n_grams':names[i],'Score':val_score})\n\nresults = pd.DataFrame(res).sort_values(by='Score', ascending=False)\nresults.style.background_gradient(\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T15:58:21.522835Z","iopub.execute_input":"2021-09-04T15:58:21.523336Z","iopub.status.idle":"2021-09-04T16:02:41.524934Z","shell.execute_reply.started":"2021-09-04T15:58:21.523285Z","shell.execute_reply":"2021-09-04T16:02:41.523655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we get the best score by taking into account **Unigrams + Bigrams**.<br/>\nTaking just the unigrams is also good.","metadata":{}},{"cell_type":"markdown","source":"<a id='tfidfvect'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   TF-IDF Vectorizer\n</h2>\n\n**[Tf-idf vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)** considers the global weighting of words.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Pipelines\npip_uni = Pipeline([ ('tfidfUni', TfidfVectorizer(ngram_range=(1, 1))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('tfidfBi', TfidfVectorizer(ngram_range=(2, 2))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('tfidfTri', TfidfVectorizer(ngram_range=(3, 3))), ('clf', LinearSVC()) ])\npip = Pipeline([ ('tfidfUniBi', TfidfVectorizer(ngram_range=(1, 2))), ('clf', LinearSVC()) ])\n\n# Lists\nnames = [\"Unigrams\",\"Bigrams\",\"Trigrams\",\"Uni+Bigrams\"]\npipelines = [pip_uni, pip_bi, pip_tri, pip]\nres = []\n\n# Results\n\nfor i, p in enumerate(pipelines) :\n    val_score = cross_val_score(p, df['tweet_clean'], df[\"encoded_target\"]).mean()\n    res.append({'n_grams':names[i],'Score':val_score})\n\nresults = pd.DataFrame(res).sort_values(by='Score', ascending=False)\nresults.style.background_gradient(\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:02:41.526873Z","iopub.execute_input":"2021-09-04T16:02:41.527213Z","iopub.status.idle":"2021-09-04T16:04:49.526084Z","shell.execute_reply.started":"2021-09-04T16:02:41.527173Z","shell.execute_reply":"2021-09-04T16:04:49.525109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, **unigrams** and **unigrams+bigrams** are the most efficient.","metadata":{}},{"cell_type":"markdown","source":"<a id='cbow'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Continuous Bag of Word (CBOW)\n</h2>\n\n> **Word2Vec** is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors.<br/>\n> **[Gensim](https://radimrehurek.com/gensim/models/word2vec.html)** has one of Word2Vec fastest implementation.\n\n**CBOW** predicts a word given its context.","metadata":{}},{"cell_type":"code","source":"texts = [text.split() for text in df[\"tweet_clean\"]]\n\n# the following configuration is the default configuration\nw2v_cbow = gensim.models.word2vec.Word2Vec(sentences=texts,\n                                        vector_size=100, window=5,     ### here we train a cbow model \n                                        min_count=5,                      \n                                        sample=0.001, workers=3,\n                                        sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n                                        cbow_mean=1,\n                                        epochs=5)\n\n# print(w2v_cbow.wv.key_to_index) # get vocabulary","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:04:49.527522Z","iopub.execute_input":"2021-09-04T16:04:49.527826Z","iopub.status.idle":"2021-09-04T16:04:56.18757Z","shell.execute_reply.started":"2021-09-04T16:04:49.527796Z","shell.execute_reply":"2021-09-04T16:04:56.186456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most similars words\n\n* **most_similar(positive=None, negative=None, topn=10, ...)**\n> Find the top-N most similar keys. Positive keys contribute positively towards the similarity, negative keys negatively.<br/>\n> This method computes **cosine similarity** between a simple mean of the projection weight vectors of the given keys and the vectors for each key in the model.<br/>\nThe most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`.","metadata":{}},{"cell_type":"code","source":"words = ['covid', 'market', 'famili']\nn = 5\nfor word in words :\n    print(f\"Top {n} similar words with {word} :\\n{w2v_cbow.wv.most_similar(word, topn=n)}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:04:56.191055Z","iopub.execute_input":"2021-09-04T16:04:56.191366Z","iopub.status.idle":"2021-09-04T16:04:56.211789Z","shell.execute_reply.started":"2021-09-04T16:04:56.191336Z","shell.execute_reply":"2021-09-04T16:04:56.210496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w2v_cbow.wv.most_similar(positive=['covid','futur'], negative=['increas'], topn=n))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:04:56.217623Z","iopub.execute_input":"2021-09-04T16:04:56.220621Z","iopub.status.idle":"2021-09-04T16:04:56.23716Z","shell.execute_reply.started":"2021-09-04T16:04:56.220545Z","shell.execute_reply":"2021-09-04T16:04:56.235685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict output word\n\n* **predict_output_word(context_words_list, topn=10)**\n> Get the probability distribution of the center word given context words.<br/>\n> **Note :** this performs a CBOW-style propagation, even in SG models, and doesnâ€™t quite weight the surrounding words the same as in training â€“ so itâ€™s just one crude way of using a trained model as a predictor.","metadata":{}},{"cell_type":"code","source":"print(w2v_cbow.predict_output_word(['impact'],topn=n))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:04:56.243236Z","iopub.execute_input":"2021-09-04T16:04:56.246046Z","iopub.status.idle":"2021-09-04T16:04:56.267246Z","shell.execute_reply.started":"2021-09-04T16:04:56.245968Z","shell.execute_reply":"2021-09-04T16:04:56.266049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='sg'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Skip-Gram (SG)\n</h2>\n\n**SG** predicts a context given a word.","metadata":{}},{"cell_type":"code","source":"texts = [text.split() for text in df[\"tweet_clean\"]]\n\nw2v_sg = gensim.models.word2vec.Word2Vec(sentences=texts,\n                                vector_size=100, window=5,               ### here we train a sg model \n                                min_count=5,                      \n                                sample=0.001, workers=3,\n                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n                                cbow_mean=1,\n                                epochs=5)\n\n# print(w2v_sg.wv.key_to_index) # get vocabulary","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:04:56.272744Z","iopub.execute_input":"2021-09-04T16:04:56.275512Z","iopub.status.idle":"2021-09-04T16:05:16.518758Z","shell.execute_reply.started":"2021-09-04T16:04:56.275445Z","shell.execute_reply":"2021-09-04T16:05:16.517671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most similar words\n\n* **most_similar(positive=None, negative=None, topn=10, ...)**\n> Find the top-N most similar keys. Positive keys contribute positively towards the similarity, negative keys negatively.<br/>\n> This method computes **cosine similarity** between a simple mean of the projection weight vectors of the given keys and the vectors for each key in the model.<br/>\nThe most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`","metadata":{}},{"cell_type":"code","source":"words = ['covid', 'market', 'famili']\nn = 5\nfor word in words :\n    print(f\"Top {n} similar words with {word} :\\n{w2v_sg.wv.most_similar(word, topn=n)}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.52019Z","iopub.execute_input":"2021-09-04T16:05:16.5205Z","iopub.status.idle":"2021-09-04T16:05:16.536981Z","shell.execute_reply.started":"2021-09-04T16:05:16.520469Z","shell.execute_reply":"2021-09-04T16:05:16.535493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w2v_sg.wv.most_similar(positive=['covid','futur'], negative=['increas'], topn=n))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.538971Z","iopub.execute_input":"2021-09-04T16:05:16.539534Z","iopub.status.idle":"2021-09-04T16:05:16.548433Z","shell.execute_reply.started":"2021-09-04T16:05:16.539488Z","shell.execute_reply":"2021-09-04T16:05:16.547041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict output word\n\n* **predict_output_word(context_words_list, topn=10)**\n> Get the probability distribution of the center word given context words.<br/>\n> **Note :** this performs a CBOW-style propagation, even in SG models, and doesnâ€™t quite weight the surrounding words the same as in training â€“ so itâ€™s just one crude way of using a trained model as a predictor.","metadata":{}},{"cell_type":"code","source":"print(w2v_sg.predict_output_word(['impact'],topn=n))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.550888Z","iopub.execute_input":"2021-09-04T16:05:16.551462Z","iopub.status.idle":"2021-09-04T16:05:16.569391Z","shell.execute_reply.started":"2021-09-04T16:05:16.551377Z","shell.execute_reply":"2021-09-04T16:05:16.568166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='learn_embeddings'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Testing the learned embeddings\n</h2>\n\nIs **great** really closer to **good** than to **bad** ?","metadata":{}},{"cell_type":"code","source":"print(\"ModÃ¨le CBOW, great and good:\",w2v_cbow.wv.similarity(\"great\",\"good\"))\nprint(\"ModÃ¨le CBOW, great and bad:\",w2v_cbow.wv.similarity(\"great\",\"bad\"))\nprint(\"ModÃ¨le SG, great and good:\",w2v_sg.wv.similarity(\"great\",\"good\"))\nprint(\"ModÃ¨le SG, great and bad:\",w2v_sg.wv.similarity(\"great\",\"bad\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.571327Z","iopub.execute_input":"2021-09-04T16:05:16.571787Z","iopub.status.idle":"2021-09-04T16:05:16.582107Z","shell.execute_reply.started":"2021-09-04T16:05:16.571743Z","shell.execute_reply":"2021-09-04T16:05:16.581196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sentiment classification\n\nSince we have only **word vectors** and that **sentences are made of multiple words**, we need to **aggregate** them.","metadata":{}},{"cell_type":"code","source":"def vectorize(text, model, mean=False):\n    \"\"\" This function should vectorize one tweet.\"\"\"\n    text = text.split()\n    vec = np.zeros(model.vector_size)\n    cpt = 0\n    for word in text:\n        if word in model.wv.key_to_index:\n            vec += model.wv.get_vector(word)\n            cpt += 1\n            \n    if (mean == True) and (cpt != 0):\n        return vec / cpt\n\n    return vec\n\ndef evaluation(models, models_name, X_train, X_test, y_train, y_test):\n    res = []\n    svc = LinearSVC(dual=False)\n    for mi in range(len(models)):\n        x_train = [vectorize(text, models[mi]) for text in X_train]\n        x_test = [vectorize(text, models[mi]) for text in X_test]\n        svc.fit(x_train, y_train)\n        ypred = svc.predict(x_test)\n        res.append({'Model':models_name[mi],'Aggregation':'sum','Score':accuracy_score(y_test, ypred)})\n        \n        X_mean = [vectorize(text, models[mi], True) for text in X_train]\n        X_test_mean = [vectorize(text, models[mi], True) for text in X_test]\n        svc.fit(X_mean, y_train)\n        ypred = svc.predict(X_test_mean)\n        res.append({'Model':models_name[mi],'Aggregation':'mean','Score':accuracy_score(y_test, ypred)})\n        \n    return res \n\n# Let's see what a tweet vector looks like.\nprint(vectorize(X_train.iloc[0], w2v_sg))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.583537Z","iopub.execute_input":"2021-09-04T16:05:16.583832Z","iopub.status.idle":"2021-09-04T16:05:16.615152Z","shell.execute_reply.started":"2021-09-04T16:05:16.583804Z","shell.execute_reply":"2021-09-04T16:05:16.613547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels = [w2v_cbow, w2v_sg]\nnames = [\"w2v_cbow\", \"w2v_sg\"]\nres = evaluation(models, names, X_train, X_test, y_train, y_test)\npd.DataFrame(res).sort_values(by='Score', ascending=False).style.background_gradient(\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:05:16.617559Z","iopub.execute_input":"2021-09-04T16:05:16.618926Z","iopub.status.idle":"2021-09-04T16:07:14.65024Z","shell.execute_reply.started":"2021-09-04T16:05:16.618829Z","shell.execute_reply":"2021-09-04T16:07:14.649222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to table of contents](#table_contents)\n\n<a id='modeling'></a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Modeling\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id='class_rebalancing'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Class rebalancing\n</h2>\n\n> **[Distribution of classes](#target_process) : { 0 : 15398 , 1 : 7713 , 2 : 18046 }**\n\nSince our **classes are not balanced**, we have to try to remedy this so that the models can learn properly.<br/>\n\n___\n\nTo deal with imbalanced datasets, we can do **undersampling** or **oversampling** techniques, as :\n\n* **[Undersampling](https://imbalanced-learn.org/stable/references/under_sampling.html) :** *RandomUnderSampler, NearMiss, EditedNearestNeighbours, AllKNN, TomekLinks, ...*\n* **[Oversampling](https://imbalanced-learn.org/stable/references/over_sampling.html) :** *RandomOverSampler, SMOTE, ADASYN, SMOTENC, BorderlineSMOTE, ...*\n* **[Combine under and over sampling](https://imbalanced-learn.org/stable/references/combine.html) :** *SMOTEENN, SMOTETomek*\n> *Useful article : https://medium.com/analytics-vidhya/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1*\n\n* For **data augmentation in NLP**, there are also : *Synonym Replacement, Random Deletion, Random Swap, Random Insertion, ...*\n> *You can take a look at this notebook : https://www.kaggle.com/swarajshinde/eda-data-augmentation-techniques-for-text-nlp*\n\n---\n\nSo, we will try to see which techniques will work best for our case with LinearSVC.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Evaluation function\ndef eval_model(clf, names, list_xtrain, list_ytrain, x_test, y_test) :\n    \"\"\" Returns a sorted DataFrame with an f1-score for each (classifier - resampling technique). \"\"\"\n    results = []\n    for name, x, y in zip(names, list_xtrain, list_ytrain) :\n        start = time.time()\n        clf.fit(x, y)\n        pred = clf.predict(x_test)\n        results.append({'Name': name, 'F1-score':f1_score(y_test, pred, average='micro')})\n        print(f\"{print_color.BOLD}{name}{print_color.END} execution time : {print_color.DARKCYAN}{time.time() - start} seconds{print_color.END}\")\n    return pd.DataFrame(results).sort_values(by='F1-score', ascending=False)\n\n# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX_train_tfidf = vectorizer.fit_transform(X_train, y_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# List of re-sampling techniques\nnames = [\"RandUnderSampler\", \"NearMiss\", \"AllKNN\", \"RandOverSampler\", \"BorderSMOTE\", \"SMOTE\", \"ASASYN\", \"SMOTEENN\", \"SMOTETomek\"]\nunder_strategy = {0: 7713, 1: 7713, 2: 7713}\nover_strategy = {0: 18046, 1: 18046, 2: 18046}\nseed = 142\n\ntechniques = [RandomUnderSampler(sampling_strategy=under_strategy, random_state=seed), NearMiss(version=3), AllKNN(), #Undersampling\n             RandomOverSampler(sampling_strategy=over_strategy, random_state=seed), BorderlineSMOTE(random_state=seed), #Oversampling\n             SMOTE(random_state=seed), ADASYN(sampling_strategy='minority', random_state=seed), #Oversampling\n             SMOTEENN(random_state=seed), SMOTETomek(random_state=seed)] #Combined techniques\n\n# Re-sampling\nlist_xtrain = []\nlist_ytrain = []\n\nfor i, tech in enumerate(techniques):\n    start = time.time()\n    x, y = tech.fit_resample(X_train_tfidf, y_train)\n    list_xtrain.append(x)\n    list_ytrain.append(y)\n    print(f\"{print_color.BOLD}{names[i]}{print_color.END} execution time : {print_color.GREEN}{time.time() - start} seconds{print_color.END}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:07:14.652076Z","iopub.execute_input":"2021-09-04T16:07:14.652814Z","iopub.status.idle":"2021-09-04T16:14:22.822916Z","shell.execute_reply.started":"2021-09-04T16:07:14.65275Z","shell.execute_reply":"2021-09-04T16:14:22.821576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's add some others data :**\n* **Original** (not balanced data)\n* **Undersampling :** Random undersampling of **majority class to middle class**\n* **Under+SMOTE :** Random undersampling of **majority class to middle class** + Oversampling (*BorderlineSMOTE*) of **minority class to middle class**","metadata":{}},{"cell_type":"code","source":"# Random Undersampling\nsampling_strategy = {0: 15398, 1: 7713, 2: 15398}\nundersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\nX_under, y_under = undersample.fit_resample(X_train_tfidf, y_train)\n\n# Borderline SMOTE (Oversampling)\nsm = BorderlineSMOTE()\nX_under_sm, y_under_sm = sm.fit_resample(X_under, y_under)\n\n# More data\nnames += [\"Original\", \"Undersampling\", \"Under+SMOTE\"]\nlist_xtrain += [X_train_tfidf, X_under, X_under_sm]\nlist_ytrain += [y_train, y_under, y_under_sm]         ","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:14:22.824322Z","iopub.execute_input":"2021-09-04T16:14:22.824627Z","iopub.status.idle":"2021-09-04T16:14:35.191829Z","shell.execute_reply.started":"2021-09-04T16:14:22.824599Z","shell.execute_reply":"2021-09-04T16:14:35.190593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Results (F1-score)**","metadata":{}},{"cell_type":"code","source":"%%time\nlinSVC = LinearSVC(random_state=seed)\nres = eval_model(linSVC, names, list_xtrain, list_ytrain , X_test_tfidf, y_test)\nres.style.background_gradient(\"Greens\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:14:35.192953Z","iopub.execute_input":"2021-09-04T16:14:35.193233Z","iopub.status.idle":"2021-09-04T16:15:02.601194Z","shell.execute_reply.started":"2021-09-04T16:14:35.193205Z","shell.execute_reply":"2021-09-04T16:15:02.599865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best data\nxtrain_best = list_xtrain[res.index[0]]\nytrain_best = list_ytrain[res.index[0]]\nprint(names[res.index[0]])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:15:02.602606Z","iopub.execute_input":"2021-09-04T16:15:02.602916Z","iopub.status.idle":"2021-09-04T16:15:02.61031Z","shell.execute_reply.started":"2021-09-04T16:15:02.602872Z","shell.execute_reply":"2021-09-04T16:15:02.608988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data augmentation with **RandomOverSampler** looks good.","metadata":{}},{"cell_type":"markdown","source":"<a id='xgboost'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   XGBoost\n</h2>","metadata":{}},{"cell_type":"code","source":"%%time\n\nxgboost = xgb.XGBClassifier(num_class=3, learning_rate=0.1, max_depth=10,\n                            use_label_encoder=False, eval_metric='mlogloss')\n\nxgboost.fit(xtrain_best, ytrain_best)\n\n# Predictions\ny_pred = xgboost.predict(X_test_tfidf)\n\n# Plot confusion matrix\nplot_confusion_matrix(xgboost, X_test_tfidf, y_test)\n\n# Print Classification report\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:15:02.611608Z","iopub.execute_input":"2021-09-04T16:15:02.611963Z","iopub.status.idle":"2021-09-04T16:18:30.832452Z","shell.execute_reply.started":"2021-09-04T16:15:02.611923Z","shell.execute_reply":"2021-09-04T16:18:30.831164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='linsvc'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Linear Support Vector Classification\n</h2>","metadata":{}},{"cell_type":"code","source":"%%time\n\nlinSVC = LinearSVC(random_state=seed)\n\n# Fit the pipeline\nlinSVC.fit(xtrain_best, ytrain_best)\n\n# Predictions\ny_pred = linSVC.predict(X_test_tfidf)\n\n# Plot confusion matrix\nplot_confusion_matrix(linSVC, X_test_tfidf, y_test)\n\n# Print Classification report\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:18:30.83458Z","iopub.execute_input":"2021-09-04T16:18:30.835057Z","iopub.status.idle":"2021-09-04T16:18:34.635353Z","shell.execute_reply.started":"2021-09-04T16:18:30.835008Z","shell.execute_reply":"2021-09-04T16:18:34.634051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='lstm'></a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   LSTM\n</h2>","metadata":{}},{"cell_type":"markdown","source":"Thanks to **@AndresHG** for the following code.\n\n* **[tf.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)**\n> Text tokenization utility class.\n\n* **[tf.keras.preprocessing.sequence.pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)**\n> Pads sequences to the same length.","metadata":{}},{"cell_type":"code","source":"texts = df[\"tweet_clean\"]\ntarget = df[\"encoded_target\"]\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(texts)\nsequences=tokenizer_obj.texts_to_sequences(texts)\n\ntweet_pad = pad_sequences(sequences,\n                          truncating='post',\n                          padding='post')\n\nvocab_length = len(tokenizer_obj.word_index) + 1\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:18:34.636845Z","iopub.execute_input":"2021-09-04T16:18:34.637183Z","iopub.status.idle":"2021-09-04T16:18:47.396706Z","shell.execute_reply.started":"2021-09-04T16:18:34.637149Z","shell.execute_reply":"2021-09-04T16:18:47.39561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    tweet_pad, \n    target, \n    test_size=0.25\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:18:47.39825Z","iopub.execute_input":"2021-09-04T16:18:47.398611Z","iopub.status.idle":"2021-09-04T16:18:47.410384Z","shell.execute_reply.started":"2021-09-04T16:18:47.39858Z","shell.execute_reply":"2021-09-04T16:18:47.40911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('../input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions\n        \nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in tokenizer_obj.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:18:47.412184Z","iopub.execute_input":"2021-09-04T16:18:47.412801Z","iopub.status.idle":"2021-09-04T16:19:09.926062Z","shell.execute_reply.started":"2021-09-04T16:18:47.412762Z","shell.execute_reply":"2021-09-04T16:19:09.925107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model from https://www.kaggle.com/mariapushkareva/nlp-disaster-tweets-with-glove-and-lstm/data\n\ndef glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation = \"softmax\"))\n    \n    loss = SparseCategoricalCrossentropy()\n    optimizer = Adam(learning_rate = 1e-3)\n\n    model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n    \n    return model\n\nmodel = glove_lstm()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:19:09.92729Z","iopub.execute_input":"2021-09-04T16:19:09.927599Z","iopub.status.idle":"2021-09-04T16:19:10.450239Z","shell.execute_reply.started":"2021-09-04T16:19:09.927571Z","shell.execute_reply":"2021-09-04T16:19:10.449317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model and train!!\n\nmodel = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 10,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:19:10.451854Z","iopub.execute_input":"2021-09-04T16:19:10.452301Z","iopub.status.idle":"2021-09-04T16:34:49.041126Z","shell.execute_reply.started":"2021-09-04T16:19:10.452254Z","shell.execute_reply":"2021-09-04T16:34:49.039775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nax[0].set_title(\"Loss\")\nax[0].plot(history.history['loss'], label=\"Training loss\")\nax[0].plot(history.history['val_loss'], label=\"validation loss\")\nax[0].legend(loc='best')\n\nax[1].set_title(\"Accuracy\")\nax[1].plot(history.history['accuracy'], label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'],label=\"Validation accuracy\")\nax[1].legend(loc='best')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:34:49.043104Z","iopub.execute_input":"2021-09-04T16:34:49.043428Z","iopub.status.idle":"2021-09-04T16:34:49.353654Z","shell.execute_reply.started":"2021-09-04T16:34:49.043399Z","shell.execute_reply":"2021-09-04T16:34:49.352425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.argmax(model.predict(X_test), axis = 1)\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, pred)}\")\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T16:36:39.561669Z","iopub.execute_input":"2021-09-04T16:36:39.562112Z","iopub.status.idle":"2021-09-04T16:36:43.707017Z","shell.execute_reply.started":"2021-09-04T16:36:39.562073Z","shell.execute_reply":"2021-09-04T16:36:43.705777Z"},"trusted":true},"execution_count":null,"outputs":[]}]}