{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Dealing with Imbalanced Datasets**\n\nImbalanced class problem are very common in classification model where one class count of response variable is very less in comparision to other class. Such as in banking Fraud detection, health care medicial diagnosis of rare disease etc where the fraud counts are very less in comparision to non fraud rows. It has been observed that positive cases of being deafult or fraud is approximately to 2-3% of the total data. So in such scenario sometimes machine learning algorithm fails to learn the underlying pattern and could not correctly identify the cases where real default occurs.\n\nSo to deal with this kind of problem is to oversample the minority class of response variable and make it as 50:50(class=0 :class =1) or 60:40(class=0 :class =1)\n\nWe have many ways to deal this , few techniques are as below:"},{"metadata":{},"cell_type":"markdown","source":"**1. Resampling techniques - Undersampling majority class**\n\nLet us consider a fraud detection dataset where we have\nTotal Observation = 2000\nNon Fradulent rows = 1660\nFradulent rows = 40\n\nSo here we can see the fradulent rows are only 2% of the total dataset.\n\nUndersampling majority class is a technique where we will take some 10% or 15% from samples without replacement from Non Fraud instances and combining them with the Fradulent rows.\n\n 10% of 2000 = 200\n \n Total Observation = 40 + 200 = 240\n Fraudulent rows% = 40/240 = 16.6%\n \n Now we have significant increase in the Fradulent data set count. \n \n **Disadvantages:**\n \n*  Due to less number of data we will have bias problem, as a result machine learning algorithm will fail to learn    many underlying pattern and cannot able to predict for new data.\n\n*  Many useful data will be missed.\n "},{"metadata":{},"cell_type":"markdown","source":"**2. Resampling Techniques - Oversampling minorty class**\n\nIn Oversampling minority class we will increasing the fraud rows to such an extent that it will be 1:1 ratio with non fraud rows so as to attain equal representation of both the classes.\n\nNon Fradulent rows = 1660\n\nIncreasing the Fradulent rows to 1660 to have equal ratio between both.\n\nLet us learn by solving one example. We will use the **Credit Card Fraud Detection Dataset** available on Kaggle for our operations.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\nfrom sklearn.utils import resample ## Used for sampling the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = cc['Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = cc.drop(['Class'], axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Preparing the Training and test datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20, random_state = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us run Logistic regression and evaluate the performance metrics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nlr_model = LogisticRegression(solver='liblinear').fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred = lr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Metrics:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, lr_pred))\nprint(\"F1 Score:\", f1_score(Y_test,lr_pred))\nprint(\"Recall Score:\",recall_score(Y_test, lr_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Let us run Random Forest and evaluate the performance Metrics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest Classifier \n\nrf = RandomForestClassifier(n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = rf.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest Metrics:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, rf_pred))\nprint(\"Recall Score:\", recall_score(Y_test, rf_pred))\nprint(\"F1 Score:\", f1_score(Y_test, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Oversampling minorty class** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate our training data back together\n\nX = pd.concat([X_train, Y_train], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we will apply resample function from sklearn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see the Fraud and Non Fraud rows are same in count. Now we will run classifier algorithm and check whether the metrics parameter changed or not.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = upsampled.Class\nX_train = upsampled.drop('Class', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nlr_model2 = LogisticRegression(solver='liblinear').fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred2 = lr_model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Metrics after Oversampling minority class:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, lr_pred2))\nprint(\"F1 Score:\", f1_score(Y_test,lr_pred2))\nprint(\"Recall Score:\",recall_score(Y_test, lr_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest Classifier \n\nrf = RandomForestClassifier(n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model2 = rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred2 = rf_model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest Metrics after Oversampling minority class:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(Y_test, rf_pred2))\nprint(\"Recall Score:\", recall_score(Y_test, rf_pred2))\nprint(\"F1 Score\", f1_score(Y_test, rf_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If we observe here, after Oversampling of minority class the accuracy has reduced but the Recall has significantly increased which serve some of our purpose of classifications model. The percentage of FN( False Negative) has reduced a lot.**\n\n**Recall is define as TP / (TP + FN)**"},{"metadata":{},"cell_type":"markdown","source":"**3. Generation Synthetic Samples - SMOTE (Synthetic Minority Oversampling Technique)**\n\nA technique similar to upsampling is to create synthetic samples. Here we will use imblearnâ€™s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data.\n\n* Works by creating synthetic samples from the minor class (no-subscription) instead of creating copies.\n* Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate input features and target\ny = cc.Class\nX = cc.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=27)\n\nsm = SMOTE(random_state=27, ratio=1.0)\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Running Logistic regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred_smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nsmote_pred = lr_pred_smote.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Metrics after SMOTE:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(y_test, smote_pred))\nprint(\"F1 Score:\", f1_score(y_test,smote_pred))\nprint(\"Recall Score:\",recall_score(y_test, smote_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Running Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred_smote = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\nrf_smote_pred = rf_pred_smote.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest Metrics after SMOTE:\")\nprint(\"\")\nprint(\"Accuracy Score:\",accuracy_score(y_test, rf_smote_pred))\nprint(\"Recall Score:\", recall_score(y_test, rf_smote_pred))\nprint(\"F1 Score\", f1_score(y_test, rf_smote_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see the Recall score has increased significantly after the SMOTE**"},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSION**\n\nWe explored 3 different methods for dealing with imbalanced datasets:\n\n1. Undersampling majority class\n2. Oversampling minorty class\n3. Generation Synthetic Samples - SMOTE\n\nThere are lot of methods to deal with Imbalanced dataset. We have to choose whoich best suits your problem.\n\n"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}