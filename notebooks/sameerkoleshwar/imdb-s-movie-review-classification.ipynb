{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis on IMDB data using SVM","metadata":{}},{"cell_type":"markdown","source":"### Dataset Exploration and preprocessing","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport seaborn as sns\nimport  dask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords_eng = stopwords.words('english')\nstopwords_eng.append('br')\n#stopwords_eng","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent = nltk.sent_tokenize(df.iloc[4]['review'])\nsent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = []\nreview = []\nfor i in sent:\n    review.extend(nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', i).lower()))\n    #ps.stem(word)\n    review = [ps.stem(word) for word in review if word not in stopwords_eng]\n    \nprint(review)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nfor i in range(5000):\n    #review = re.sub('[^a-zA-Z]',' ',df['review'][i]) # chalega\n    #review = review.lower()\n    #review = review.split()\n    #review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    #review=' '.join(review)\n    review = nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', df['review'][i]).lower())\n    #ps.stem(word)\n    review = [ps.stem(word) for word in review if word not in stopwords_eng]\n    review=' '.join(review)\n    corpus.append(review)\n#corpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nprep_data = pd.read_csv(\"../input/imdbmoviereviewpreprocessedstemming50k/prepd_data.csv\")\nprep_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = list(prep_data['review'])\ncorpus[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000)\nX = cv.fit_transform(corpus).toarray()\n\n# rows of X will be corresponding sentences and columns will be corresponding to word\ny = prep_data['sentiment_label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X[0].sum())\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#corpus has 68992 unique words , how many of them you want to consider(depending upon frequency)?????\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.5, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_corpus=[]\nfor i in range(45000,50000):    \n    review1 = nltk.word_tokenize(re.sub('[^a-zA-Z]', ' ', df['review'][i]).lower())\n    #ps.stem(word)--\n    review1 = [ps.stem(word) for word in review1 if word not in stopwords_eng]\n    review1 =' '.join(review1)\n    test_corpus.append(review1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X2 = cv.transform(test_corpus).toarray()\n\n# rows of X will be corresponding sentences and columns will be corresponding to word\n    \ny2 = pd.get_dummies(df['sentiment'])\ny2 = y2.iloc[45000:50000,1].values\nX2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred2 = model1.predict(X2)\ncm2 = confusion_matrix(y2,y_pred2)\naccuracy2= accuracy_score(y2,y_pred2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **TF-IDF**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\nX = cv.fit_transform(list(prep_data['review'])).toarray()\ny = prep_data['sentiment_label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.5, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier \nmodel2 = SGDClassifier().fit(x_train,y_train,sample_weight=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_pred_svm = model2.predict(x_test)\ncm = confusion_matrix(y_test,y_pred_svm)\naccuracy= accuracy_score(y_test,y_pred_svm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear SVM from Scratch ###","metadata":{}},{"cell_type":"code","source":"\"\"\"\nMulticlass SVMs (Crammer-Singer formulation).\nA pure Python re-implementation of:\nLarge-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex.\nMathieu Blondel, Akinori Fujino, and Naonori Ueda.\nICPR 2014.\nhttp://www.mblondel.org/publications/mblondel-icpr2014.pdf\n\"\"\"\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_random_state\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef projection_simplex(v, z=1):\n    \"\"\"\n    Projection onto the simplex:\n        w^* = argmin_w 0.5 ||w-v||^2 s.t. \\sum_i w_i = z, w_i >= 0\n    \"\"\"\n    # For other algorithms computing the same projection, see\n    # https://gist.github.com/mblondel/6f3b7aaad90606b98f71\n    n_features = v.shape[0]\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u) - z\n    ind = np.arange(n_features) + 1\n    cond = u - cssv / ind > 0\n    rho = ind[cond][-1]\n    theta = cssv[cond][-1] / float(rho)\n    w = np.maximum(v - theta, 0)\n    return w\n\n\nclass MulticlassSVM(BaseEstimator, ClassifierMixin):\n\n    def __init__(self, C=1, max_iter=50, tol=0.05,\n                 random_state=None, verbose=0):\n        self.C = C\n        self.max_iter = max_iter\n        self.tol = tol,\n        self.random_state = random_state\n        self.verbose = verbose\n\n    def _partial_gradient(self, X, y, i):\n        # Partial gradient for the ith sample.\n        g = np.dot(X[i], self.coef_.T) + 1\n        g[y[i]] -= 1\n        return g\n\n    def _violation(self, g, y, i):\n        # Optimality violation for the ith sample.\n        smallest = np.inf\n        for k in range(g.shape[0]):\n            if k == y[i] and self.dual_coef_[k, i] >= self.C:\n                continue\n            elif k != y[i] and self.dual_coef_[k, i] >= 0:\n                continue\n\n            smallest = min(smallest, g[k])\n\n        return g.max() - smallest\n\n    def _solve_subproblem(self, g, y, norms, i):\n        # Prepare inputs to the projection.\n        Ci = np.zeros(g.shape[0])\n        Ci[y[i]] = self.C\n        beta_hat = norms[i] * (Ci - self.dual_coef_[:, i]) + g / norms[i]\n        z = self.C * norms[i]\n\n        # Compute projection onto the simplex.\n        beta = projection_simplex(beta_hat, z)\n\n        return Ci - self.dual_coef_[:, i] - beta / norms[i]\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Normalize labels.\n        self._label_encoder = LabelEncoder()\n        y = self._label_encoder.fit_transform(y)\n\n        # Initialize primal and dual coefficients.\n        n_classes = len(self._label_encoder.classes_)\n        self.dual_coef_ = np.zeros((n_classes, n_samples), dtype=np.float64)\n        self.coef_ = np.zeros((n_classes, n_features))\n\n        # Pre-compute norms.\n        norms = np.sqrt(np.sum(X ** 2, axis=1))\n\n        # Shuffle sample indices.\n        rs = check_random_state(self.random_state)\n        ind = np.arange(n_samples)\n        rs.shuffle(ind)\n\n        violation_init = None\n        for it in range(self.max_iter):\n            violation_sum = 0\n\n            for ii in range(n_samples):\n                i = ind[ii]\n\n                # All-zero samples can be safely ignored.\n                if norms[i] == 0:\n                    continue\n\n                g = self._partial_gradient(X, y, i)\n                v = self._violation(g, y, i)\n                violation_sum += v\n\n                if v < 1e-12:\n                    continue\n\n                # Solve subproblem for the ith sample.\n                delta = self._solve_subproblem(g, y, norms, i)\n\n                # Update primal and dual coefficients.\n                self.coef_ += (delta * X[i][:, np.newaxis]).T\n                self.dual_coef_[:, i] += delta\n\n            if it == 0:\n                violation_init = violation_sum\n\n            vratio = violation_sum / violation_init\n\n            if self.verbose >= 1:\n                print(\"iter\", it + 1, \"violation\", vratio)\n\n            if vratio < self.tol:\n                if self.verbose >= 1:\n                    print(\"Converged\")\n                break\n\n        return self\n\n    def predict(self, X):\n        decision = np.dot(X, self.coef_.T)\n        pred = decision.argmax(axis=1)\n        return self._label_encoder.inverse_transform(pred)\n\n\n'''if __name__ == '__main__':\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    clf = MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\n    clf.fit(X, y)\n    print(clf.score(X, y))\n    '''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\nclassifier.fit(x_train, y_train)\nprint(classifier.score(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_pred1=classifier.predict(x_test)\ncm = confusion_matrix(y_test,y_pred1)\naccuracy= accuracy_score(y_test,y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **SVM from scratch**\n\nlink: https://github.com/python-engineer/MLfromscratch/blob/master/mlfromscratch/svm.py\n","metadata":{}},{"cell_type":"code","source":"class SVM:\n\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        \n        y_ = np.where(y <= 0, -1, 1)\n        \n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n                if condition:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n                    self.b -= self.lr * y_[idx]\n\n\n    def predict(self, X):\n        approx = np.dot(X, self.w) - self.b\n        return np.sign(approx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3 = SVM(n_iters=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"score: \", model3.score(x_test, y_test))\ny_pred1 = model3.predict(x_test)\ncm = confusion_matrix(y_test,y_pred1)\naccuracy= accuracy_score(y_test,y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cm)\nprint(\"accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **SVM from andrew ng**\n\nlink: https://github.com/kenextra/SVMAlgorithm/blob/master/SVM.ipynb","metadata":{}},{"cell_type":"code","source":"def linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n    \ndef polynomial_kernel(x, y, p=3):\n    return (1 + np.dot(x, y)) ** p\n\ndef gaussian_kernel(x, y, sigma=5.0):\n    numerator = np.linalg.norm(x-y)**2\n    denominator = 2 * (sigma ** 2)\n    return np.exp(-numerator / denominator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SVM(object):\n\n    def __init__(self, kernel=linear_kernel, tol=1e-3, C=0.1,\n                 max_passes=5, sigma=0.1):\n\n        self.kernel = kernel\n        self.tol = tol\n        self.C = C\n        self.max_passes = max_passes\n        self.sigma = sigma\n        self.model = dict()\n\n    def __repr__(self):\n        return (f\"{self.__class__.__name__}(\"\n                f\"kernel={self.kernel.__name__}, \"\n                f\"tol={self.tol}, \"\n                f\"C={self.C}, \"\n                f\"max_passes={self.max_passes}, \"\n                f\"sigma={self.sigma}\"\n                \")\")\n\n    def svmTrain(self, X, Y):\n        # Data parameters\n        m = X.shape[0]\n\n        # Map 0 to -1\n        Y = np.where(Y == 0, -1, 1)\n\n        # Variables\n        alphas = np.zeros((m, 1), dtype=float)\n        b = 0.0\n        E = np.zeros((m, 1), dtype=float)\n        passes = 0\n\n        # Pre-compute the kernel matrix\n        if self.kernel.__name__ == 'linear_kernel':\n            print(f'Pre-computing {self.kernel.__name__} kernel matrix')\n            K = X @ X.T\n\n        elif self.kernel.__name__ == 'gaussian_kernel':\n            print(f'Pre-computing {self.kernel.__name__} kernel matrix')\n            X2 = np.sum(np.power(X, 2), axis=1).reshape(-1, 1)\n            K = X2 + (X2.T - (2 * (X @ X.T)))\n            K = np.power(self.kernel(1, 0, self.sigma), K)\n\n        else:\n            # Pre-compute the Kernel Matrix\n            # The following can be slow due to lack of vectorization\n            print(f'Pre-computing {self.kernel.__name__} kernel matrix')\n            K = np.zeros((m, m))\n\n            for i in range(m):\n                for j in range(m):\n                    x1 = np.transpose(X[i, :])\n                    x2 = np.transpose(X[j, :])\n                    K[i, j] = self.kernel(x1, x2)\n                    K[i, j] = K[j, i]\n\n        print('Training...')\n        print('This may take 1 to 2 minutes')\n\n        while passes < self.max_passes:\n            num_changed_alphas = 0\n\n            for i in range(m):\n\n                E[i] = b + np.sum(alphas * Y * K[:, i].reshape(-1, 1)) - Y[i]\n\n                if (Y[i] * E[i] < -self.tol and alphas[i] < self.C) or (Y[i] * E[i] > self.tol and alphas[i] > 0):\n                    j = np.random.randint(0, m)\n                    while j == i:\n                        # make sure i is not equal to j\n                        j = np.random.randint(0, m)\n\n                    E[j] = b + np.sum(alphas * Y *\n                                      K[:, j].reshape(-1, 1)) - Y[j]\n\n                    # Save old alphas\n                    alpha_i_old = alphas[i, 0]\n                    alpha_j_old = alphas[j, 0]\n\n                    # Compute L and H by (10) or (11)\n                    if Y[i] == Y[j]:\n                        L = max(0, alphas[j] + alphas[i] - self.C)\n                        H = min(self.C, alphas[j] + alphas[i])\n                    else:\n                        L = max(0, alphas[j] - alphas[i])\n                        H = min(self.C, self.C + alphas[j] - alphas[i])\n                    if L == H:\n                        # continue to next i\n                        continue\n\n                    # compute eta by (14)\n                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n                    if eta >= 0:\n                        # continue to next i\n                        continue\n\n                    # compute and clip new value for alpha j using (12) and (15)\n                    alphas[j] = alphas[j] - (Y[j] * (E[i] - E[j])) / eta\n\n                    # Clip\n                    alphas[j] = min(H, alphas[j])\n                    alphas[j] = max(L, alphas[j])\n\n                    # Check if change in alpha is significant\n                    if np.abs(alphas[j] - alpha_j_old) < self.tol:\n                        # continue to the next i\n                        # replace anyway\n                        alphas[j] = alpha_j_old\n                        continue\n\n                    # Determine value for alpha i using (16)\n                    alphas[i] = alphas[i] + Y[i] * \\\n                        Y[j] * (alpha_j_old - alphas[j])\n\n                    # Compute b1 and b2 using (17) and (18) respectively.\n                    b1 = b - E[i] - Y[i] * (alphas[i] - alpha_i_old) * \\\n                        K[i, j] - Y[j] * (alphas[j] - alpha_j_old) * K[i, j]\n\n                    b2 = b - E[j] - Y[i] * (alphas[i] - alpha_i_old) * \\\n                        K[i, j] - Y[j] * (alphas[j] - alpha_j_old) * K[j, j]\n\n                    # Compute b by (19).\n                    if 0 < alphas[i] < self.C:\n                        b = b1\n                    elif 0 < alphas[j] < self.C:\n                        b = b2\n                    else:\n                        b = (b1 + b2) / 2\n                    num_changed_alphas = num_changed_alphas + 1\n\n            if num_changed_alphas == 0:\n                passes = passes + 1\n            else:\n                passes = 0\n\n            print('.', end='', flush=True)\n\n        print('\\n DONE! ')\n\n        # Save the model\n        idx = alphas > 0\n        self.model['X'] = X[idx.reshape(1, -1)[0], :]\n        self.model['y'] = Y[idx.reshape(1, -1)[0]]\n        self.model['kernelFunction'] = self.kernel\n        self.model['b'] = b\n        self.model['alphas'] = alphas[idx.reshape(1, -1)[0]]\n        self.model['w'] = np.transpose(np.matmul(np.transpose(alphas * Y), X))\n        # return model\n\n    def svmPredict(self, X):\n        if X.shape[1] == 1:\n            X = np.transpose(X)\n\n        # Dataset\n        m = X.shape[0]\n        p = np.zeros((m, 1))\n        pred = np.zeros((m, 1))\n\n        if self.model['kernelFunction'].__name__ == 'linear_kernel':\n            p = X.dot(self.model['w']) + self.model['b']\n\n        elif self.model['kernelFunction'].__name__ == 'gaussian_kernel':\n            # Vectorized RBF Kernel\n            # This is equivalent to computing the kernel\n            # on every pair of examples\n            X1 = np.sum(np.power(X, 2), axis=1).reshape(-1, 1)\n            X2 = np.transpose(np.sum(np.power(self.model['X'], 2), axis=1))\n            K = X1 + (X2.T - (2 * (X @ (self.model['X']).T)))\n            K = np.power(self.model['kernelFunction'](1, 0, self.sigma), K)\n            K = np.transpose(self.model['y']) * K\n            K = np.transpose(self.model['alphas']) * K\n            p = np.sum(K, axis=1)\n\n        else:\n            for i in range(m):\n                prediction = 0\n                for j in range(self.model['X'].shape[0]):\n                    prediction = prediction + self.model['alphas'][j] \\\n                        * self.model['y'][j] * \\\n                        self.model['kernelFunction'](np.transpose(\n                            X[i, :]), np.transpose(self.model['X'][j, :]))\n\n                p[i] = prediction + self.model['b']\n\n        # Convert predictions into 0 and 1\n        pred[p >= 0] = 1\n        return pred\n\n    def predict(self, X):\n        if X.shape[1] == 1:\n            X = np.transpose(X)\n\n        # Dataset\n        m = X.shape[0]\n        p = np.zeros((m, 1))\n        pred = np.zeros((m, 1))\n\n        if self.model['kernelFunction'].__name__ == 'linear_kernel':\n            p = X.dot(self.model['w']) + self.model['b']\n\n        elif self.model['kernelFunction'].__name__ == 'gaussian_kernel':\n            # Vectorized RBF Kernel\n            # This is equivalent to computing the kernel\n            # on every pair of examples\n            X1 = np.sum(np.power(X, 2), axis=1).reshape(-1, 1)\n            X2 = np.transpose(np.sum(np.power(self.model['X'], 2), axis=1))\n            K = X1 + (X2.T - (2 * (X @ (self.model['X']).T)))\n            K = np.power(self.model['kernelFunction'](1, 0, self.sigma), K)\n            K = np.transpose(self.model['y']) * K\n            K = np.transpose(self.model['alphas']) * K\n            p = np.sum(K, axis=1)\n\n        else:\n            for i in range(m):\n                prediction = 0\n                for j in range(self.model['X'].shape[0]):\n                    prediction = prediction + self.model['alphas'][j] \\\n                        * self.model['y'][j] * \\\n                        self.model['kernelFunction'](np.transpose(\n                            X[i, :]), np.transpose(self.model['X'][j, :]))\n\n                p[i] = prediction + self.model['b']\n\n        # Convert predictions into 0 and 1\n        pred[p >= 0] = 1\n        return pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}