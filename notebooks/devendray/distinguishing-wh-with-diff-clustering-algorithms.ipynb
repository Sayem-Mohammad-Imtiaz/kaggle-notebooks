{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"source":"## Objective \nLearn various types of clustering algorithms as available in sklearn, We will use \"World Happiness Report data\" as dataset for clustering algorithms.\n\n**List of Clustering methods **\n\n**1. K-Means\n2. Mean Shift\n3. Mini Batch K-Means\n4. Spectral Clustering \n5. DBSCAN\n6. Affinity Propagation\n7. Birch\n8. Gaussian Mixture Modeling**\n\n### Import Libraries\n","cell_type":"markdown","metadata":{"_uuid":"4e517108cbe6aa5802d11ef341cb57f734f841ab","_cell_guid":"82e6629a-682e-4b0f-9549-0cabe52d78cc"}},{"source":"import numpy as np                   # Data manipulation\nimport pandas as pd                  # DataFrame manipulation\nimport time                          # To time processes \nimport warnings                      # To suppress warnings\nimport matplotlib.pyplot as plt      # For Graphics\nimport seaborn as sns\nfrom sklearn import cluster, mixture # For clustering \nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n%matplotlib inline\nwarnings.filterwarnings('ignore')","cell_type":"code","outputs":[],"metadata":{"_uuid":"a489809d1a86a708cfefaffc1a3ee9cd388b0b7d","_cell_guid":"ed4ba527-1f19-4c12-885e-6561c203cb37"},"execution_count":17},{"source":"### Read Data From File ","cell_type":"markdown","metadata":{"_uuid":"67bffb177fbb62078b13e4e599d5032fc60f769e","_cell_guid":"ecf7fa9c-930c-4c04-a320-904b26754f88"}},{"source":"#os.chdir(\"E:/Big_Data/Code/Python/exercise/Clustering_Algorithms\")\nWHR_data = pd.read_csv(\"../input/2017.csv\", header = 0)\n\n#To play with \"World Happiness Report\" Data set, we will create a copy \nWHR_data_copy = WHR_data.copy(deep = True)\n\n# preview Data\nprint(WHR_data.info())\nWHR_data.shape\nWHR_data.head()\nWHR_data.sample(15)\n\n","cell_type":"code","outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"50ba7548193a9d7743ceafa7b2daac56dcdd859d","_cell_guid":"ae8cbe84-3a9e-4149-8e2a-0a0216c6288c","_kg_hide-output":false},"execution_count":18},{"source":"WHR_data.columns\nplt.figure(figsize=(12,8))\nsns.heatmap(WHR_data.corr())","cell_type":"code","outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"c29381a4c210dd038b72fbec0e0910a486184d1f","_cell_guid":"345329b9-0244-4be5-8cf4-de9a0c1d2bb8","scrolled":true},"execution_count":19},{"source":"\nsns.pairplot(WHR_data[['Country', 'Happiness.Rank', 'Happiness.Score', 'Whisker.high','Whisker.low', 'Economy..GDP.per.Capita.', 'Family']]);\n","cell_type":"code","outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"a4e417471f2db0e7537d6f1a0b811001d840ae53","_cell_guid":"b8e09240-8674-4958-867a-65df16a34d12","collapsed":true,"_kg_hide-output":false},"execution_count":null},{"source":"from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n# For offline use\nimport cufflinks as cf\n\ncf.go_offline()\nWHR_data[['Happiness.Score','Whisker.high','Family','Freedom','Dystopia.Residual']].iplot(kind='spread')","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"### Explore & Scale","cell_type":"markdown","metadata":{"_uuid":"629fc9d25e21b72ea9ec2001f9c0a776f7408f21","_cell_guid":"f850891c-e6c2-465c-847e-55ac70949a8f"}},{"source":"\n\n#Ignore Country and Happiness_Rank Columns\n\nWHR_data = WHR_data.iloc[:,2:]\n\nprint(\"\\n \\n Dimenstion of dataset  : WHR_data.shape\")\nWHR_data.shape\n\nWHR_data.dtypes\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"66baeef7c18a74668ee82db02bc65dc4e3f4ddc2","_cell_guid":"0b8146db-d9ea-424a-b523-cde6561e7a15"},"execution_count":20},{"source":"### Normalize Dataset\nNormalize dataset for easier parameter selection, \n\nCreating Object of StandardScaler with default parameters\n :- class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n\nfit_transform(X[, y])\tFit to data, then transform it. \nfit_transform to X and y with optional parameters fit_params and returns a transformed version of X.\n","cell_type":"markdown","metadata":{"_uuid":"afaedb466b1c64328a394aef2ee52f78088de938","_cell_guid":"fe3ab2db-3c4f-4704-a8ef-d4da435e12ea"}},{"source":"# Instantiate Scaler Object \nss = StandardScaler()\n# Fit and transform \nss.fit_transform(WHR_data)\nWHR_data.sample(20)","cell_type":"code","outputs":[],"metadata":{"_uuid":"e98390aec3cf3264d21f061567e0f011305240fc","_cell_guid":"87774c53-f0cd-4c8c-915c-106747c38e9b"},"execution_count":21},{"source":"## Begin Clustering \n\nWe are performing clustering of unlabeled data thorugh sklearn.cluster module .\n\nList of Clustering methods \n\n1. K-Means\n2. Mean Shift\n3. Mini Batch K-Means\n4. Spectral Clustering \n5. DBSCAN\n6. Affinity Propagation\n7. Birch\n8. Gaussian Mixture Modeling \n\n\nEach Clustering algorithm comes in two variants, A class that implements the fit method to learn the clusters on train data, and a function , that given the train data, returns an arrays of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the labels_attribute","cell_type":"markdown","metadata":{"_uuid":"ca02814f7ea9abc757e4f4eed42f751b7ebce4e4","_cell_guid":"1d47e89a-4374-4ab5-a7aa-cd9473ce39a0"}},{"source":"##### K-Means\nThe k-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean \\mu_j of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from X, although they live in the same space. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum of squared criterion:\n\n![alt_text](http://scikit-learn.org/stable/_images/math/1886f2c69775746ac7b6c1cdd88c53c676839015.png)","cell_type":"markdown","metadata":{"_uuid":"df6c958f4ff5e4e80818f2932161e5c5291573ca","_cell_guid":"3f29c67f-9001-4449-bc26-b570aa0b512a"}},{"source":"##### Mean Shift\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\nGiven a candidate centroid x_i for iteration t, the candidate is updated according to the following equation:\n\n![alt_text](http://scikit-learn.org/stable/_images/math/df67cad6c90923bd6d5dd1ba1cc98b73ba772bd8.png)\n\nWhere N(x_i) is the neighborhood of samples within a given distance around x_i and m is the mean shift vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:\n\n![alt_text](http://scikit-learn.org/stable/_images/math/64f9e995cea2b11641d37f2ec1cfcf1d590d2797.png)\n","cell_type":"markdown","metadata":{"_uuid":"d185ccd9dc1846d22f3dafae7601ae03feb5bab4","_cell_guid":"168758a1-9662-4097-839f-a4692dd60281"}},{"source":"##### Mini Batch K-Means\n\nThe MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.\n\nMiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced. In practice this difference in quality can be quite small","cell_type":"markdown","metadata":{"_uuid":"9a76e5e21a513219a300325ac7354e076e0a6ebc","_cell_guid":"e8357efd-342c-4906-87c0-3710b433cd0d"}},{"source":"\n##### Spectral Clustering\n\nSpectralClustering does a low-dimension embedding of the affinity matrix between samples, followed by a KMeans in the low dimensional space. It is especially efficient if the affinity matrix is sparse and the pyamg module is installed. SpectralClustering requires the number of clusters to be specified. It works well for a small number of clusters but is not advised when using many clusters.","cell_type":"markdown","metadata":{"_uuid":"7e27a271417f52839af44c915d342dd4f6d96ad5","_cell_guid":"8807919e-be87-4e39-864e-e9ad29589ca4"}},{"source":"##### DBSCAN\nThe DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped","cell_type":"markdown","metadata":{"_uuid":"63ea686baf47d64e2e9bf8d94cbfcdbb2ed95ee8","_cell_guid":"23cb28fd-cf58-4ae4-b6d7-7f826bc3d128"}},{"source":"##### Affinity Propagation\nAffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.","cell_type":"markdown","metadata":{"_uuid":"4e92549a2e7d5a7684e9e9d3166f9330cffde343","_cell_guid":"4b06cdda-710e-4179-a4ad-7cf706ae9fe3"}},{"source":"##### Birch\nThe Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.","cell_type":"markdown","metadata":{"_uuid":"8d3bc698d1b9c317303ddbf46cdb3454b9e99e08","_cell_guid":"c8899d7b-da98-4cbf-8d7e-d63e96cca3ee"}},{"source":"##### Gaussian Mixture Modeling\n\nA Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.\n","cell_type":"markdown","metadata":{"_uuid":"adc990a77c073efec480205ff2a2c89be4241494","_cell_guid":"1b38fe90-3b3f-47d9-8146-9374bd72a629"}},{"source":"\n### Define Cluster Class\n","cell_type":"markdown","metadata":{"_uuid":"d46537077bff74337b55ba7c647d194ab020f246","_cell_guid":"095a6589-afbd-47f7-b93c-ed439da68550"}},{"source":"\n# Define CluserMethod class : which returns the clustering result based on input \n\nclass ClusterMethodList(object) :\n    def get_cluster_instance(self, argument,input_data,X):\n        method_name = str(argument).lower()+ '_cluster'\n        method = getattr(self,method_name,lambda : \"Invalid Clustering method\")\n        return method(input_data,X)\n    \n    def kmeans_cluster(self,input_data,X):\n        km = cluster.KMeans(n_clusters =input_data['n_clusters'] )\n        return km.fit_predict(X)\n   \n    def meanshift_cluster(self,input_data,X):\n        ms = cluster.MeanShift(bandwidth=input_data['bandwidth'])\n        return  ms.fit_predict(X)\n    \n    def minibatchkmeans_cluster(self,input_data,X):\n        two_means = cluster.MiniBatchKMeans(n_clusters=input_data['n_clusters'])\n        return two_means.fit_predict(X)\n   \n    def dbscan_cluster(self,input_data,X):\n        db = cluster.DBSCAN(eps=input_data['eps'])\n        return db.fit_predict(X)\n    \n    def spectral_cluster(self,input_data,X):\n        sp = cluster.SpectralClustering(n_clusters=input_data['n_clusters'])\n        return sp.fit_predict(X)\n   \n    def affinitypropagation_cluster(self,input_data,X):\n        affinity_propagation =  cluster.AffinityPropagation(damping=input_data['damping'], preference=input_data['preference'])\n        affinity_propagation.fit(X)\n        return affinity_propagation.predict(X)\n       \n    \n    def birch_cluster(self,input_data,X):\n        birch = cluster.Birch(n_clusters=input_data['n_clusters'])\n        return birch.fit_predict(X)\n   \n    def gaussian_mixture_cluster(self,input_data,X):\n        gmm = mixture.GaussianMixture( n_components=input_data['n_clusters'], covariance_type='full')\n        gmm.fit(X)\n        return  gmm.predict(X)\n\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"56a07e07baf28a1c28a2170a4a920d2372f81013","_cell_guid":"44125f8a-73cd-4c8b-bc97-f9310ae718ab","collapsed":true},"execution_count":null},{"source":"# Define Clustering Prcoess\n\ndef startClusteringProcess(list_cluster_method,input_data,no_columns,data_set):\n    fig,ax = plt.subplots(no_rows,no_columns, figsize=(10,10)) \n    cluster_list = ClusterMethodList()\n    i = 0\n    j=0\n    for cl in list_cluster_method :\n        cluster_result = cluster_list.get_cluster_instance(cl,input_data,data_set)\n        #convert cluster result array to DataFrame\n        data_set[cl] = pd.DataFrame(cluster_result)\n        ax[i,j].scatter(data_set.iloc[:, 4], data_set.iloc[:, 5],  c=cluster_result)\n        ax[i,j].set_title(cl+\" Cluster Result\")\n        j=j+1\n        if( j % no_columns == 0) :\n            j= 0\n            i=i+1\n    plt.subplots_adjust(bottom=-0.5, top=1.5)\n    plt.show()\n\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"eb6b91fb839140c80b15bfe24289b251b45f59ed","_cell_guid":"2bc595a5-25b3-4f92-b883-be8e2407bbeb","collapsed":true},"execution_count":null},{"source":"#### Initialize Input Parameters for Clustering ","cell_type":"markdown","metadata":{"_uuid":"02f29478341986dd63d01b5c7a517f8143a70886","_cell_guid":"34ced735-10cb-4d7b-a45f-2722e3f5b90c"}},{"source":"list_cluster_method = ['KMeans',\"MeanShift\",\"MiniBatchKmeans\",\"DBScan\",\"Spectral\",\"AffinityPropagation\",\"Birch\",\"Gaussian_Mixture\"]\n# For Graph display \nno_columns = 2\nno_rows = 4\n# NOT all algorithms require this parameter\nn_clusters= 3\nbandwidth = 0.1 \n# eps for DBSCAN\neps = 0.3\n## Damping and perference for Affinity Propagation clustering method\ndamping = 0.9\npreference = -200\ninput_data = {'n_clusters' :  n_clusters, 'eps' : eps,'bandwidth' : bandwidth, 'damping' : damping, 'preference' : preference}\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"ef22a37564588db341f1c1b970c01e5bf31edd0e","_cell_guid":"97dac6f0-9254-4dcb-aa58-371a9f197dcf","collapsed":true},"execution_count":null},{"source":"#### Plot Graph","cell_type":"markdown","metadata":{"_uuid":"c3df04a530d75d0cd220623f538111557aaaf0bf","_cell_guid":"af9f71db-7261-4e38-a100-07182d84ad7f"}},{"source":"# Start Clustering Process\nstartClusteringProcess(list_cluster_method,input_data,no_columns,WHR_data)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"6f2cfdb74a4cf2e43ffb011164a30b4918b8e2eb","_cell_guid":"c5c69f55-2447-42d7-bd32-4b341b71335f","collapsed":true},"execution_count":null},{"source":"WHR_data.insert(0,'Country',WHR_data_copy.iloc[:,0])\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"4671e821211424efe200ac08cbd52d584720da96","_cell_guid":"fa130d64-10b8-4ea4-92cc-d1041147ae57","collapsed":true},"execution_count":null},{"source":"WHR_data.iloc[:,[0,11,12,13,14,15,16,17,18]]","cell_type":"code","outputs":[],"metadata":{"_uuid":"d071856f35fda16a3a8d90be0e7ce289264e7ddc","_cell_guid":"bcec9b33-773e-4f9f-bdd5-f6d0afc6b302","collapsed":true},"execution_count":null},{"source":"### Gaussian Mixture Clustering Visualization","cell_type":"markdown","metadata":{"_uuid":"4a429a4d1cfd0b1f75f3aa36be67ebdb3b906aa7","_cell_guid":"7ec4f4c0-e77f-4aae-a3a1-ce6e0778db97"}},{"source":"\n\ndata = dict(type = 'choropleth', \n           locations = WHR_data['Country'],\n           locationmode = 'country names',\n           z = WHR_data['Gaussian_Mixture'], \n           text = WHR_data['Country'],\n           colorbar = {'title':'Cluster Group'})\nlayout = dict(title = 'Gaussian Mixture Clustering Visualization', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3)\n\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"60156c5522e9cbc33457a8b74a6efcebe67fda64","_cell_guid":"939855df-fc60-42fd-a65a-cf5d2bf7a705","collapsed":true},"execution_count":null},{"source":"### K-Means Clustering Visualization","cell_type":"markdown","metadata":{"_uuid":"35fe0e302da62ff566f634d6896957793cd0d4b1","_cell_guid":"adcca901-1b0b-458d-98b9-89ca7584c424"}},{"source":"data = dict(type = 'choropleth', \n           locations = WHR_data['Country'],\n           locationmode = 'country names',\n           z = WHR_data['KMeans'], \n           text = WHR_data['Country'],\n           colorbar = {'title':'Cluster Group'})\nlayout = dict(title = 'K-Means Clustering Visualization', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"4da8997bbf37581bedf5a47a922e9de459042c69","_cell_guid":"85f2a93a-0164-4401-9ce1-91009e699667","collapsed":true},"execution_count":null},{"source":"### Global Happiness Score Map","cell_type":"markdown","metadata":{"_uuid":"c7ca62eab45c43ac55bca39088517d10f31dba18","_cell_guid":"9747c3d7-7d8c-4461-8e47-d632e58b7647"}},{"source":"\ndata = dict(type = 'choropleth', \n           locations = WHR_data['Country'],\n           locationmode = 'country names',\n           z = WHR_data['Happiness.Score'], \n           text = WHR_data['Country'],\n           colorbar = {'title':'Happiness'})\nlayout = dict(title = 'Global Happiness Score', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"6f70f35cbda8f9c8975ce6824f4d95562a4f0612","_cell_guid":"aae5af8b-16ee-4d06-b599-57c279b28ee4","collapsed":true},"execution_count":null},{"source":"","cell_type":"code","outputs":[],"metadata":{"_uuid":"1cd57ac7def4497b0c9d19b9dde108d7b0f914f2","_cell_guid":"63f3cc2c-f6aa-4405-b155-3f1a90026149","collapsed":true},"execution_count":null},{"source":"","cell_type":"code","outputs":[],"metadata":{"_uuid":"c03b22a92770cfd4c292151e13c0c94be6629c20","_cell_guid":"576ab4d9-a9d6-4ae6-9f53-eac0923a3eac","collapsed":true},"execution_count":null}]}