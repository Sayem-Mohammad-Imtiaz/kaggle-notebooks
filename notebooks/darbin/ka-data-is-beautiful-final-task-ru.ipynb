{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Загрузка данных \n(минимизация расхода памяти, исключение предупреждений и удаление лишних столбцов)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/dataisbeautiful/r_dataisbeautiful_posts.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получаем **предупреждение** о смешаных типах в столбцах!\n\nПопробуем решить проблему, которая генерирует это предупреждение.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"оценка данных, которые содержатся в столбцах с предуреждением о смешанных типах (столбцы 5 и 7)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,5].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Очевидно, что тип этого поля ([5]) - object","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,7].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"тип этого поля - object\n\nтакже очевидно, что этот столбец можно полностью удалить из анализа, т.к. в нём содержится всего 1 значение с именем и 33331 пустых скобок","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"посчитаем количество уникальных id и сравним с общим числом id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"числа совпадают, значит id - уникальное поле","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Повторяем загрузку файла так,что бы использовалось меньше памяти и загрузка прошла без warning\nДля этого удаляем не нужный столбец 7 (что бы лишний раз не сбрасывать его позже) и определяем тип для столбца 5\nТакже удаляем столбец id (столбец 0) - он нам не нужен, будем использовать индексы вместо него\nНе удаляем столбец full_link (столбец 9), хотя он по сути повторяет столбец 'title' с добавлением url (используем его в дальнейшем для анализа пропусков в title)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/dataisbeautiful/r_dataisbeautiful_posts.csv\", \n                 usecols=[1,2,3,4,5,6,8,9,10,11],\n                 dtype={5:object})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"данные успешно прогружены без предупреждений","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"memory usage сократилось на 18% с 15.6Мб до 12.8Мб","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Трансформация данных и заполнение пропусков","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"заменим True на 1, False на 0 для over_18","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.over_18.replace(True,1,inplace = True)\ndf.over_18.replace(False,0,inplace = True)\ndf.over_18.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"проверяем пропуски в cтолбцах","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Заменим NaN в total_awards_received на 0 (часто это архивные сообщения и обычно там нулевое количество наград)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.total_awards_received.fillna(0, inplace = True)\n\n# проверим\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"строки с пустым title","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['title'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"восстановим пропущенные title из url","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ndf[df['title'].isna()]['full_link']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"восстановить title из ссылки не получится, т.к. в конце url для этого сообщения также стоит Null","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# заполняем пустой title пустой строкой, что бы была корректная обработка в WordCloud\ndf.title.fillna(\" \",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"посмотрим сообщения для строк с пропусками","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('author_flair_text is NaN: \\n', df[df['author_flair_text'].isna()]['full_link'].head(), \"\\n\")\nprint('removed_by is NaN: \\n', df[df['removed_by'].isna()]['full_link'].head(), \"\\n\")\nprint('author_flair_text is NaN: \\n', df[df['total_awards_received'].isna()]['full_link'].head(), \"\\n\")\nprint('author_flair_text is NaN and score >10 000: \\n', df[df['total_awards_received'].isna() & (df['score'] > 10000)]['full_link'].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Вывод: предварительно пропуски лучше оставить как есть, т.к. очевидных замен нет","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"сбрасваем поле full_link и created_utc, т.к. они нам больше не пригодятся","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['full_link'], axis = 1)\ndf = df.drop(['created_utc'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"объединение поля title и author для последующего расчёта частотности употребляемых слов","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['title'] + ' ' + df['author']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Кодировка категориальных переменных","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Закодируем все строковые столбцы за исключением title, author и text (мы их будем использовать в дальнейшем для текстового анализа),для author создадим отдельную закодированную колонку author_encoded","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['author_encoded'] = df['author']\n\n# перекодируем NaN в строку \"not available\" (object type) (иначе кодировщик отказывается работать с NaN)\ndf.author_flair_text.fillna(\"not available\",inplace = True)\ndf.removed_by.fillna(\"not available\",inplace = True)\ndf_to_encode = df[['author_encoded', 'author_flair_text', 'removed_by']]\ndf_to_encode.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Подключаем класс для предобработки данных\nfrom sklearn import preprocessing\n\n# Напишем функцию, которая принимает на вход DataFrame, кодирует числовыми значениями категориальные признаки\n# и возвращает обновленный DataFrame и сами кодировщики.\ndef number_encode_features(init_df):\n    result = init_df.copy() # копируем нашу исходную таблицу\n    encoders = {}\n    for column in result.columns:\n        if result.dtypes[column] == np.object: # np.object -- строковый тип / если тип столбца - строка, то нужно его закодировать\n            encoders[column] = preprocessing.LabelEncoder() # для колонки column создаем кодировщик\n            result[column] = encoders[column].fit_transform(result[column]) # применяем кодировщик к столбцу и перезаписываем столбец\n    return result, encoders\n\ndf_encoded, encoders = number_encode_features(df_to_encode) # Теперь encoded data содержит закодированные кат. признаки \nencoders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Объединяем закодированные переменные и исходную базу с нужными переменными для анализа","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['author_encoded', 'author_flair_text', 'removed_by'], axis = 1).join(df_encoded)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Анализ данных","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Данные сильно не сбалансированны\n\nЕсли всем сообщениям присвоить 0 (т.е. всем проставить over_18 = False), то **Accuracy такого решения будет 99.487%**, хотя модель при этом предсказывать over_18 = 1 не будет конечно же","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# описание данных\nprint(df.groupby('over_18')['over_18'].count())\n\n# процент over_18\nover_18_count = df[df['over_18'] == 1]['over_18'].count()\ntotal = df['over_18'].count()\nover_18_share = over_18_count/total\nprint(\"Доля данных, показывающих целевую группу \\\"over_18\\\" {0:.4f}\".format(over_18_share))\n\n# гистограмма\ndf[df['over_18'] == 0]['over_18'].astype(int).hist(label='False', grid = False, bins=1, rwidth=0.8)\ndf[df['over_18'] == 1]['over_18'].astype(int).hist(label='True', grid = False, bins=1, rwidth=0.8)\nplt.xticks((0,1),('False', 'True'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на распределение велечин по признакам в наших данных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(15,12), bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмтроим матрицу корреляций","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(8, 6))\nspearman = df.corr(method = 'spearman')\nsns.heatmap(spearman, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Данные слабокоррелированны. Исключение составляет корреляция между num_comments и score - на уровне 0.48","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Балансировка выборки","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.argwhere вернет индексы тех элементов массива y (целевой переменной), где значение 0\nnot_over_18 = df['over_18'].astype(int).to_numpy()\nnot_over_18_ids = np.argwhere(not_over_18 == 0).flatten()\nprint('Всего over_18 = False: ', len(not_over_18_ids))\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Перемешаем массив с выбранным random state (чтоб в дальнейшем у нас совпадали выборки) выберем в нем \"лишние\" id тех, кто not over_18 (кто портит нам прогноз алгоритма). \n# Кол-во \"лишних\" = кол-во оставшихся - кол-во over_18.\nfrom sklearn.utils import shuffle\n\nnot_over_18_ids = shuffle(not_over_18_ids, random_state = 42)\n# найдем \"лишних\", для этого обрежем найденные id на кол-во over_18 (внутри len)\nnot_over_18_ids = not_over_18_ids[len(np.argwhere(not_over_18 == 1).flatten()):]\nprint(len(not_over_18_ids))\n# отображаем кол-во и сами id, которые мы должны выкинуть\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверим, сбалансированны ли классы\n# по идее (оставшиеся) - (\"лишние\") = (over_18)\nlen(np.argwhere(not_over_18 == 0).flatten()) - len(not_over_18_ids) == len(np.argwhere(not_over_18 == 1).flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Разделение выборки на X и y\n\nX = df.drop(['over_18'],axis = 1)\ny = df['over_18']\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# выбрасываем лишних по индексам\nX = X.drop(X.index[not_over_18_ids])\ny = y.drop(y.index[not_over_18_ids])\n\n# отобразим итоговый размер признаков датасета\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь видим, что классы сбалансированы.\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Трансформация данных после балансировки (более тяжёлые алгоритмы)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### One Hot Encoding для категориальных переменных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Категориальные признаки\ncat_cols = ['author_encoded', 'author_flair_text', 'removed_by']\n\n# Кодируем категориальные признаки\nohe_df = pd.DataFrame(index=X.index)\nohe = OneHotEncoder(handle_unknown='ignore')\n\nfor col in cat_cols:\n    ohe.fit(X[[col]])\n    ohe_result = pd.DataFrame(ohe.transform(X[[col]]).toarray(),\n                              columns=ohe.get_feature_names(input_features=[col]),\n                              index=X.index)\n    ohe_df = ohe_df.join(ohe_result)\n\nohe_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Нормализация числовых переменных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnum_cols = ['score', 'total_awards_received', 'num_comments']\nstd_df = pd.DataFrame(index=X.index)\nscaler = StandardScaler()\n\nfor col in num_cols:\n    scaler.fit(X[[col]])\n    std_result = pd.DataFrame(scaler.transform(X[[col]]),\n                              columns=[col],\n                              index=X.index)\n    std_df = std_df.join(std_result)\n\nstd_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Объединение выборок (OneHoteEncoding, Standard Scaler и исходных данных)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['author_encoded', 'author_flair_text', 'removed_by', 'score', 'total_awards_received', 'num_comments']\nX_prepared = X.drop(cols_to_drop, axis=1).join(std_df).join(ohe_df)\nX = X_prepared\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Разделение выборки на train и test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Часто употребляемые слова для over_18 контента\n\nПосмотрим какие слова чаще всего употребляются, когда мы имеем дело с контентом для взрослых (over_18)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# title для X train, у которых over_18 = true\ntrain_true_title = X_train.loc[y_train[y_train == 1].index,:]['title']\n\n# author для X train, у которых over_18 = true\ntrain_true_author = X_train.loc[y_train[y_train == 1].index,:]['title']\n\n# text для X train, у которых over_18 = true\ntrain_true_text = X_train.loc[y_train[y_train == 1].index,:]['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Облако наиболее часто употребляемых слов во взрослом контенте (в title)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n\ndef wordcloud_img(data):\n    plt.figure(figsize = (20,20))\n    wc = WordCloud(min_font_size = 3, \n                   background_color=\"white\",  \n                   max_words = 3000, \n                   width = 1000, \n                   height = 600, \n                   stopwords = STOPWORDS).generate(str(\" \".join(data)))\n    plt.imshow(wc,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud_img(train_true_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Облако наиболее часто употребляемых слов во взрослом контенте (в поле author)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud_img(train_true_author)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"10 слов для контента over_18","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def top10words(data):\n    wc = WordCloud(min_font_size = 3,  \n                   max_words = 3000 , \n                   width = 1000 , \n                   height = 600 , \n                   stopwords = STOPWORDS).generate(str(\" \".join(data)))\n    text_true = wc.process_text(str(\" \".join(data)))\n    print('10 слов \\n', list(text_true.keys())[:10])\n    print('Общее количество слов: ', len(text_true.keys()), '\\n')\n\n# поле title\nprint(top10words(train_true_title))\n# поле author\nprint(top10words(train_true_author))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Очевидно, что надо объединить эти 2 поля (title и author объединено в поле text) для более точной статистики частотности, т.к. в поле author также есть компрометирующие слова","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(min_font_size = 3,  \n                   max_words = 3000, \n                   width = 1600, \n                   height = 800, \n                   stopwords = STOPWORDS).generate(str(\" \".join(train_true_text)))\ntrain_dictionary = wc.process_text(str(\" \".join(train_true_text)))\n# количество слов в словаре\nn_of_words = len(train_dictionary.keys())\n\nprint('10 слов \\n', list(train_dictionary.keys())[:10])\nprint('Общее количество слов: ', n_of_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"сформируем словарь наиболее употребляемых слов в категории over_18","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сортируем словарь по частоте употребления\ntrain_dictionary_sorted = sorted(train_dictionary.items(), key = lambda word:(word[1], word[0]))\n# Топ 20% наиболее употребляемых слов в сообщениях over_18\ntrain_top_words = train_dictionary_sorted[round(n_of_words*0.8):]\n\n# слова-исключения\nexclusion_words =['data', 'year', 'month', 'week', 'day', 'post', 'every', 'average', 'word', 'the world', \n                  'years of', 'new', 'graph', 'US', 'time', 'result', 'by state', 'UK', 'tags', 'countries', \n                  \"I've\", 'comment', 'last year', 'Chart', 'Countries', 'State', 'Map of', 'We', 'analysis of']\n\n# сортированный словарь топ 20% наиболее употребляемых слов в категории over_18\nans_true = []\nfor i in train_top_words:\n    # исключаем некоторые слова из словаря\n    if i[0] in exclusion_words: continue\n    ans_true.append(i[0])\n\n# Топ 20% наиболее употребляемых слов в категории over_18\nans_true[len(ans_true)-20:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"предскажем значения over_18 для тестовой выборки основываясь только на тексте в сообщении\n\nесли в text присутствует слово из словаря, то мы ставим 1, иначе - 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor i in X_test['text']:\n    x = i.split()\n    for j in x:\n        if j in ans_true:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\nlen(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"сравним с размером тестовой выборки","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"посчитаем количество верно предсказанных значений","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in range(len(predictions)):\n    y_test = list(y_test)\n    if(predictions[i] == int(y_test[i])):\n        count += 1\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"рассчитаем Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = (count/len(predictions))*100\nprint('Accuracy с использованием Word Cloud и Исключениями равна', np.round(accuracy, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Улучшение модели с частотой слов","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"создадим словарь стоп-слов (которые не влияют на анализ, такие как he, have, it, the и т.п.), \nдобавим к стоп-словам пунктуацию","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport string\n\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stemming и Lemmatization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Используем Stemming (поиск основы слова)\nLemmatization (приведение к нормальной форме: существительных к единственному числу, мужскому роду, глаголов к инфинитиву и т.п) \nдля сокращения количества словоформ: am, are, is  ->  be; car, cars, car's, cars'  ->  car etc.\nhttps://ru.wikipedia.org/wiki/Стемминг\nhttps://ru.wikipedia.org/wiki/Лемматизация","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# функция для определения прилагательных (ADJ), глаголов (VERB), существительных (NOUN) и наречий (ADV)\nfrom nltk.corpus import wordnet as wn\n\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('V'):\n        return wn.VERB\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    else:\n        return wn.NOUN\n\n# Лемматизация \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n\n# (отбрасываем всё лишнее в предложении, приводим слова к нормальной формеи получаем их список)\n# 'Once upone a time a man walked into a door' -> ['upone', 'time', 'man', 'walk', 'door']\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text   \n\n# Объединяем лемматизированный список в предложение\n# ['upone', 'time', 'man', 'walk', 'door'] -> 'upone time man walk door '\ndef join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Запуск лемматизации и создание нового поля с лемматизированными предложениями 'text_lemma'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['text_lemma'] = X_train['text'].apply(lemmatize_words).apply(join_text)\nX_test['text_lemma'] = X_test['text'].apply(lemmatize_words).apply(join_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"примеры лемматизированных предложений","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['text', 'text_lemma']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Присвоение веса словам после улучшения модели с частотой","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"С TFIDFVectorizer - каждому слову ставит частотность появления, \nпри этом значение увеличивается пропорционально счету, но смещается на частоту слова в корпусе ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Присвоение весов словам с использованием TfidfVectorizer\ntv = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\ntv_X_train = tv.fit_transform(X_train['text_lemma'])\ntv_X_test = tv.transform(X_test['text_lemma'])\n\nprint('TfidfVectorizer_train:', tv_X_train.shape)\nprint('TfidfVectorizer_test:', tv_X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Обединение текстовых и обработанных переменных","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Объединим, полученные ранее обработанные переменные (OneHotENcoding, Нормирования и кодирования и т.п.) с новыми текстовыми переменными для того, что бы у нас был наиболее полный массив для анализа","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# сбросим поля,которые мы больше не будем использовать (мы преобразовали их в другие формы)\ncols_to_drop = ['title', 'author', 'text','text_lemma']\n\nX_train_dropped = X_train.drop(cols_to_drop, axis=1)\nX_test_dropped = X_test.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Нормализуем данные в tv_X_train/test\nscaler_tv = StandardScaler()\ntv_X_train_scaled = scaler_tv.fit_transform(tv_X_train.toarray())\ntv_X_test_scaled = scaler_tv.transform(tv_X_test.toarray())\n\n# трансформируем tv_X_train/test sparse numpy матрицы в pandas Data Frame\ntv_X_train_pd_scaled = pd.DataFrame(data=tv_X_train_scaled, \n                             index=X_train.index, \n                             columns=np.arange(0, np.size(tv_X_train_scaled,1)))\ntv_X_test_pd_scaled = pd.DataFrame(data=tv_X_test_scaled, \n                             index=X_test.index, \n                             columns=np.arange(0, np.size(tv_X_test_scaled,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# объединение ранее обработанных переменных с текстовыми переменными\nX_train = X_train_dropped.join(tv_X_train_pd_scaled)\nX_test = X_test_dropped.join(tv_X_test_pd_scaled)\n\n# проверяем\nX_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Линейная регрессия с использованием улучшенной модели частоты слов","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Тренируем логистическую регрессию только на <u>текстовых переменных</u>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n\n# для Tf-idf\nlr_tfidf=lr.fit(tv_X_train, y_train)\nprint(lr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Предсказываем значения в тестовой выборке","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# для Tf-idf\nlr_tfidf_predict=lr.predict(tv_X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# для Tf-idf\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# для Tf-idf\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\n# для Tf-idf\nplot_confusion_matrix(lr_tfidf, tv_X_test, y_test,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### R^2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test, lr_tfidf_predict)\nprint (f\"R2 score / LR = {r2}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отрицательный R^2 говорит о том, что у нас маленькая выборка (1320) относительно количества использованных переменных (11701) и для идеального и расчёта надо сокращать количество переменных либо делать аугментацию.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Mean Absolute Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmeanae = mean_absolute_error(y_test, lr_tfidf_predict)\nprint (\"MAE (Mean Absolute Error) {0}\".format(meanae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Тренируем логистическую регрессию на <u>всех переменных</u>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n\nlr_all=lr.fit(X_train, y_train)\nprint(lr_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_all_predict=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nlr_all_score=accuracy_score(y_test,lr_all_predict)\nprint(\"lr_all_score :\",lr_all_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nlr_all_report=classification_report(y_test,lr_all_predict,target_names=['0','1'])\nprint(lr_all_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\nplot_confusion_matrix(lr_all, X_test, y_test, display_labels=['0','1'], cmap=\"Blues\", values_format = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### R^2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2 = r2_score(y_test, lr_all_predict)\nprint (f\"R2 score для всех переменных / LR = {r2}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отрицательный R^2 говорит о том, что у нас маленькая выборка (1320) относительно количества использованных переменных (13140) и для идеального и расчёта надо сокращать количество переменных либо делать аугментацию.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Mean Absolute Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmeanae = mean_absolute_error(y_test, lr_all_predict)\nprint (\"MAE (Mean Absolute Error) для всех переменных {0}\".format(meanae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Тренируем KNN на всех переменных","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Построим сетку для перебора гиперпараметра для нахождения лучшего гиперпараметра KNN, пройдем 3 значения (от 1 до 30)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\n# Зададим сетку - среди каких значений выбирать наилучший параметр.\nknn_grid = {'n_neighbors': np.array(np.linspace(1, 30, 3), dtype='int')} # перебираем по параметру <<n_neighbors>>, по сетке заданной np.linspace\n\n# Создаем объект кросс-валидации\ngs = GridSearchCV(knn, knn_grid, cv=3)\n\n# Обучаем его\ngs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Функция отрисовки графиков\ndef grid_plot(x, y, x_label, title, y_label):\n    plt.figure(figsize=(12, 6))\n    plt.grid(True)\n    plt.plot(x, y, 'go-')\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Строим график зависимости качества от числа соседей","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_plot(knn_grid['n_neighbors'], gs.cv_results_['mean_test_score'], 'n_neighbors', 'KNeighborsClassifier', 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отобразим лучший параметр (y gs есть атрибуты best_param, best_score)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Инициализируем отдельный классификатор с лучшим значением, найденным на предыдущего перебора","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn = KNeighborsClassifier(n_neighbors=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# предсказания\nclf_knn.fit(X_train, y_train)\ny_knn = clf_knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(lr_all, X_test, y_test, display_labels=['0','1'], cmap=\"Blues\", values_format = '')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}