{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and more for the goodreads data"},{"metadata":{},"cell_type":"markdown","source":"# 1. Start"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read csv, use error_bad_lines=False because there are some errors\ndf = pd.read_csv('/kaggle/input/goodreadsbooks/books.csv', error_bad_lines=False, index_col='bookID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print head of data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.where(df == 102).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. What types of languages are there?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all unique values for \"language_code\"\nlist_of_lang = df['language_code'].unique()\nprint(list_of_lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many in each\nlangs = df['language_code'].value_counts()\nprint(langs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n\n# Set the width and height of the figure\nplt.figure(figsize=(14,6))\n\n# Bar chart showing average arrival delay for Spirit Airlines flights by month\n#sns.barplot(x=flight_data.index, y=flight_data['NK'])\nsns.barplot(x=langs.index, y=langs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Convert en-US and en-GB in eng"},{"metadata":{},"cell_type":"markdown","source":"It seems unnecessary that there so many 'different' english languages, so we change all of them to just eng"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all en-XX with eng\ndf_better = df.copy()\ndf_better.replace(to_replace=('en-US', 'en-GB', 'en-CA'), value='eng',inplace=True)\ndf_better['language_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_better.where(df_better == 102).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. change '  num_pages' to 'num_pages'"},{"metadata":{},"cell_type":"markdown","source":"For some reason, there are 2 spaces in 'num_pages'. We want to get rid of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename the oddly named column\ndf_better.rename(columns = {'  num_pages':'num_pages'}, inplace = True) \nprint(df_better.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.where(df == 102).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Explore number of pages in Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using seaborn\nfig, ax = plt.subplots(figsize=(16, 4))\nsns.histplot(df_better['num_pages'],  bins=60, kde=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data seems skewed, let's visualize it better"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize skewedness\nfrom scipy import stats\nstats.probplot(df_better['num_pages'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data indeed is skewed."},{"metadata":{},"cell_type":"markdown","source":"# 6. are there any duplicates??"},{"metadata":{"trusted":true},"cell_type":"code","source":"# complete duplicates\ndf_better.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only title duplicates\ndf_better.duplicated(subset = 'title').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show ranking of title duplicates\ndf_better['title'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only authors duplicates\ndf_better.duplicated(subset = 'authors').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show ranking of author duplicates\ndf_better['authors'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only isbn duplicates\nduple_isbn = df_better.duplicated(subset = 'isbn13')\nduple_isbn.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Correlations??"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_better.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#only use sensible columns for correlation\ndf_better_corr = df_better.select_dtypes(exclude=['object']).copy()\ndf_better_corr.drop(['isbn13'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_better_corr.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get correlations and show heatmap\ncorr=df_better_corr.corr(method='pearson')\nsns.heatmap(data=corr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mega scatterplot\nsns.pairplot(df_better_corr, height = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#other graphic\nsns.jointplot(x=\"average_rating\", y=\"num_pages\", data = df_better, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# same without outliers\nsns.jointplot(x=\"average_rating\", y=\"num_pages\", data = df_better[df_better.num_pages < 1000], color = 'darkcyan', kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. add a better rating system (WR)"},{"metadata":{},"cell_type":"markdown","source":"This rating system takes into account the number of ratings. So a book with one single 5.0 rating won't be the best:\n\nWeighted rating (WR) = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C , where:\n\n* R = average for the movie (mean) = (Rating)\n* v = number of votes for the movie = (votes)\n* m = minimum votes required to be listed in the Top 250 (currently 3000)\n* C = the mean vote across the whole report (currently 6.9)\n\nfrom: https://stats.stackexchange.com/questions/6418/rating-system-taking-account-of-number-of-votes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define variables\nm = 100\n\n# calculate C (assuming this has to be done before dropping rows according to m)\nC = df_better['average_rating'].mean()\nprint('mean vote across report', C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define weighted rating function\ndef WR(R, v, m, C):\n    WR = (v/(v+m))*R+(m/(v+m))*C\n    return WR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out how many ratings below 3000\nm_out = df_better['ratings_count'].where(df_better['ratings_count'] > m).isna()\nm_out_sum = m_out.sum()\nprint('books below m ratins:', m_out_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with ratings<m\n# This will make all nan that is below m\ndf_better_WR = df_better.copy()\ndf_better_WR['ratings_count'].where(df_better['ratings_count'] > m, inplace=True)\n# drops rows with any nan\ndf_better_WR.dropna(axis=0, how='any', inplace=True)\ndf_better_WR.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add column with WR\n\n# get a list with all WR\nWR_list = WR(df_better['average_rating'], df_better['ratings_count'], m, C)\n# add list as column\ndf_better_WR['WR'] = WR_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirm new column\ndf_better_WR.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list top three according to WR:\ndf_better_WR.nlargest(3, 'WR', keep='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Rating predictions"},{"metadata":{},"cell_type":"markdown","source":"We cannot use WR here because it depends on the number of ratings"},{"metadata":{},"cell_type":"markdown","source":"## 9.1 First approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for nan\ndf_better_WR.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Create X and y\ny = df_better_WR['average_rating'].copy()\nX = df_better_WR[['num_pages', 'ratings_count', 'text_reviews_count']].copy()\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\n\n# Define the model\nmy_model = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1) \n\n# Fit the model\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\n\n# Get predictions\npredictions = my_model.predict(X_valid)\n\n# Calculate MAE\nmae = mean_absolute_error(predictions, y_valid)  \n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.2 Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 9.2.1 Number of books per author"},{"metadata":{},"cell_type":"markdown","source":"First idea is to create a new feature from the author data, like number of books per author"},{"metadata":{"trusted":true},"cell_type":"code","source":"## create dictionary from author's number of books\nauthor_dict = df_better_WR['authors'].value_counts().to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add column to DF according to dict\ndf_better_WR['no_books_author'] = df_better_WR['authors'].map(author_dict)\n# add this column to X\nX2 = X.copy()\nX2['no_books_author'] = df_better_WR['no_books_author'].copy()\ny2 = y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new training\n\n# Break off validation set from training data\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(X2, y2, train_size=0.8, test_size=0.2, random_state=0)\n\n# Define the model\nmy_model2 = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1)\n\n# Fit the model\nmy_model2.fit(X_train2, y_train2, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid2, y_valid2)], \n             verbose=False)\n\n\n# Get predictions\npredictions2 = my_model2.predict(X_valid2)\n\n# Calculate MAE\nmae2 = mean_absolute_error(predictions2, y_valid2) \n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Improved only slightly"},{"metadata":{},"cell_type":"markdown","source":"### 9.2.2 Include year"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_better_WR['publication_date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert last 4 strings of publication date to int for new feature 'publication year'\ndf_better_WR['publication_year'] = df_better_WR['publication_date'].map(lambda x: x[-4:])\ndf_better_WR['publication_year'] = df_better_WR['publication_year'].astype('int32') \n  \n# create new X\nX3 = X2.copy()\nX3['publication_year'] = df_better_WR['publication_year']\ny3 = y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do the machine learning stuff\n\nX_train3, X_valid3, y_train3, y_valid3 = train_test_split(X3, y3, train_size=0.8, test_size=0.2, random_state=0)\nmy_model3 = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1)\nmy_model3.fit(X_train3, y_train3, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid3, y_valid3)], \n             verbose=False)\npredictions3 = my_model3.predict(X_valid3)\n\n# Calculate MAE\nmae3 = mean_absolute_error(predictions3, y_valid3) \n\n# print MAE\nprint(\"Mean Absolute Error:\" , mae3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"even better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mae, mae2, mae3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MAE got better with every added feature."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}