{"cells":[{"metadata":{},"cell_type":"markdown","source":"----------\n**Heart Disease Analysis and Prediction**\n=====================================\n\n***LGB : CV - Auc = .948 Recall = .915 Precision = .867 Accuracy = .871***\n\n***CB  : CV - Auc = .916 Recall = .897 Precision = .843 Accuracy = .851***\n\n***Vincent Lugat***\n\n*July 2019*\n\n----------"},{"metadata":{},"cell_type":"markdown","source":"![](https://image.noelshack.com/fichiers/2019/27/6/1562430213-logo-heart-care.jpg)"},{"metadata":{},"cell_type":"markdown","source":"![](http://)"},{"metadata":{},"cell_type":"markdown","source":"- <a href='#1'>1. Libraries and Data</a>  \n    - <a href='#1.1'>1.1. Loading libraries</a> \n    - <a href='#1.2'>1.2. Reading data</a> \n- <a href='#2'>2. Exploratory Data Analysis (EDA)</a> \n    - <a href='#2.1'>2.1. Head, describe and shape</a> \n    - <a href='#2.2'>2.2. Target count</a> \n    - <a href='#2.3'>2.3. Correlation Matrix</a> \n    - <a href='#2.4'>2.4. Univariate analysis</a> \n        - <a href='#2.4.1'>2.4.1. Numerical data</a> \n        - <a href='#2.4.2'>2.4.2. Categorical data</a> \n    - <a href='#2.5'>2.5. Bivariate analysis (vs target)</a> \n        - <a href='#2.5.1'>2.5.1. Numerical data</a> \n        - <a href='#2.5.2'>2.5.2. Categorical data</a> \n    - <a href='#2.6'>2.6. Multivariate analysis</a> \n        - <a href='#2.6.1'>2.6.1. Numerical data</a> \n        - <a href='#2.6.2'>2.6.2. Categorical data</a> \n        - <a href='#2.6.3'>2.6.3. Numerical & categorical data</a> \n- <a href='#3'>3. Feature engineering and preprossesing</a>\n    - <a href='#3.1'>3.1. Feature engineering</a> \n    - <a href='#3.2'>3.2. Preprocessing</a> \n- <a href='#4'>4. Light GBM</a>\n    - <a href='#4.1'>4.1. Confusion matrix function </a> \n    - <a href='#4.2'>4.2. Light GBM - Hyperparameters </a> \n    - <a href='#4.3'>4.3. Light GBM - 5 folds </a>\n    - <a href='#4.4'>4.4. Light GBM - Features importance</a>\n- <a href='#5'>5. SHAP</a>\n- <a href='#6'>6. Recall augmentation </a> \n    - <a href='#6.1'>6.1. Light GBM : Scale_pos_weight augmentation </a> \n- <a href='#7'>7. CatBoost</a>\n    - <a href='#7.1'>7.1. CatBoost - Hyperparameters </a> \n    - <a href='#7.2'>7.2. CatBoost - 5 folds </a>\n    - <a href='#7.2'>7.2. CatBoost - Features importance </a>"},{"metadata":{},"cell_type":"markdown","source":"1. age : age in years\n2. sex : (1 = male; 0 = female)\n3. cp : chest pain type (4 values) \n4. trestbps : resting blood pressure (in mm Hg on admission to the hospital)\n5. chol : serum cholestoral in mg/dl \n6. fbs : fasting blood sugar > 120 mg/dl : (1 = true; 0 = false)\n7. restecg : resting electrocardiographic results (values 0,1,2)\n8. thalach : maximum heart rate achieved \n9. exang : exercise induced angina \n10. oldpeak : ST depression induced by exercise relative to rest \n11. the slope of the peak exercise ST segment \n12. ca : number of major vessels (0-3) colored by flourosopy \n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Librairies and data</a> "},{"metadata":{},"cell_type":"markdown","source":"## <a id='1.1'>1.1. Loading libraries</a> "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# Python libraries\n# Classic,data manipulation and linear algebra\nfrom datetime import datetime\nfrom scipy import interp\nimport pandas as pd\nimport numpy as np\nimport itertools\n\n# Plots\nimport shap\nshap.initjs()\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\n# Data processing, metrics and modeling\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, auc\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nimport catboost as cb\n\n# Filter werning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\n\n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='1.2'>1.2. Reading data</a> "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Dataset\ndata = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Exploratory Data Analysis (EDA)</a>  "},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.1'>2.1. Head, describe and shape</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.head(), data.describe(), data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.2'>2.2. Target count</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"colors = ['darkturquoise', 'darkorange']\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize']=(15,8)\n\nax = sns.countplot(x='target', data=data, palette=colors, alpha=0.9, edgecolor=('white'), linewidth=2)\nax.set_ylabel('count', fontsize=12)\nax.set_xlabel('target', fontsize=12)\nax.grid(b=True, which='major', color='grey', linewidth=0.2)\nplt.title('Target count', fontsize=18)\nplt.show()\n\ntarget_0 = len(data[data.target == 0])\ntarget_1 = len(data[data.target == 1])\nprint(\"Percentage Haven't Heart Disease: {:.2f}%\".format((target_0 / (len(data.target))*100)))\nprint(\"Percentage Have Heart Disease: {:.2f}%\".format((target_1 / (len(data.target))*100)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.3'>2.3. Correlation matrix</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix \nf, (ax1, ax2) = plt.subplots(1,2,figsize =(15, 8))\ncorr = data.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nheatmapkws = dict(linewidths=0.1) \nsns.heatmap((data[data['target'] ==1]).corr(), vmax = .8, square=True, ax = ax2, cmap = 'YlGnBu', mask=mask, **heatmapkws);\nax1.set_title('Disease', fontsize=18)\nsns.heatmap((data[data['target'] ==0]).corr(), vmax = .8, square=True, ax = ax1, cmap = 'afmhot', mask=mask,**heatmapkws);\nax2.set_title('Healthy', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.4'>2.4. Univariate analysis</a> "},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.4.1.'>2.4.1. Numerical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.distplot(data[feature], ax=ax[i//2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n\n    # Get the two lines from the ax[i//2,i%2]es to generate shading\n    l1 = ax[i//2,i%2].lines[0]\n\n    # Get the xy data from the lines so that we can shade\n    x1 = l1.get_xydata()[:,0]\n    y1 = l1.get_xydata()[:,1]\n    ax[i//2,i%2].fill_between(x1,y1, color=\"goldenrod\", alpha=0.8)\n\n    #grid\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n    \n    ax[i//2,i%2].set_title('Distribution of {}'.format(feature), fontsize=18)\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n    ax[i//2,i%2].set_xlabel('Modality', fontsize=12)\n\n    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\n    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.4.2.'>2.4.2. Categorical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(4,2,figsize=(12,12))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    colors = ['darkturquoise']\n    sns.countplot(x=feature,data=data,ax=ax[i//2,i%2], palette = colors, alpha=0.8, edgecolor=('white'), linewidth=2)\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.2)\n    ax[i//2,i%2].set_title('Count of {}'.format(feature), fontsize=18)\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n    ax[i//2,i%2].set_xlabel('modality', fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.5'>2.5. Bivariate analysis (vs target)</a> "},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.5.1.'>2.5.1. Numerical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\n\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak','age']):\n    sns.distplot(data[data['target']==0][(feature)], ax=ax[i//2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n    sns.distplot(data[data['target']==1][(feature)], ax=ax[i//2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n\n    # Get the two lines from the ax[i//2,i%2]es to generate shading\n    l1 = ax[i//2,i%2].lines[0]\n    l2 = ax[i//2,i%2].lines[1]\n\n    # Get the xy data from the lines so that we can shade\n    x1 = l1.get_xydata()[:,0]\n    y1 = l1.get_xydata()[:,1]\n    x2 = l2.get_xydata()[:,0]\n    y2 = l2.get_xydata()[:,1]\n    ax[i//2,i%2].fill_between(x2,y2, color=\"darkorange\", alpha=0.6)\n    ax[i//2,i%2].fill_between(x1,y1, color=\"darkturquoise\", alpha=0.6)\n\n    #grid\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n    \n    ax[i//2,i%2].set_title('{} vs target'.format(feature), fontsize=18)\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n    ax[i//2,i%2].set_xlabel('Modality', fontsize=12)\n\n    #sns.despine(ax[i//2,i%2]=ax[i//2,i%2], left=True)\n    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\n    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,2,figsize=(12,12))\nf.delaxes(ax[2,1])\ncolors=['darkturquoise','darkorange']\nfor i,feature in enumerate(['age','thalach','chol','trestbps','oldpeak']):\n    sns.boxplot(x='target', y=feature, data=data , ax=ax[i//2,i%2], palette=colors, boxprops=dict(alpha=0.8))\n    sns.stripplot(y=feature, x='target', \n                          data=data,\n                          ax=ax[i//2,i%2],\n                          jitter=True, marker='o',\n                          alpha=0.6, \n                          color=\"springgreen\")\n\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.2)\n    ax[i//2,i%2].set_title('Count of {}'.format(feature), fontsize=18)\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n    ax[i//2,i%2].set_xlabel('Modality', fontsize=12)\n    \n    sns.despine()\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.4)\n\n    ax[i//2,i%2].set_title(str(feature)+' '+'vs'+' '+'target', fontsize=18)\n    ax[i//2,i%2].set_ylabel(\"count\", fontsize=12)\n    ax[i//2,i%2].set_xlabel(('target'), fontsize=12)\n\n    plt.setp(ax[i//2,i%2].artists, edgecolor = 'white')\n    plt.setp(ax[i//2,i%2].lines, color='white')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.5.2.'>2.5.2. Categorical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(4,2,figsize=(12,12))\n\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    colors = ['darkturquoise', 'darkorange']\n    sns.countplot(x=feature,data=data,hue='target',ax=ax[i//2,i%2], palette = colors, alpha=0.7, edgecolor=('white'), linewidth=2)\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.4)\n    ax[i//2,i%2].set_title('Count of {} vs target'.format(feature), fontsize=18)\n    ax[i//2,i%2].legend(loc='best')\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n    ax[i//2,i%2].set_xlabel('modality', fontsize=12)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='2.6'>2.6. Multivariate analysis</a> "},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.6.1.'>2.6.1. Numerical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def scatterplot(var1,var2,var3,var4):\n    f,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    #f.delaxes(ax[2,1])\n    \n    colors = ['darkturquoise','darkorange']\n    ax1 = sns.scatterplot(x = data[var1], y = data[var2], hue = \"target\",\n                        data = data,  ax=ax1, palette=colors, alpha=0.8, edgecolor=\"white\",linewidth=0.1)\n    ax1.grid(b=True, which='major', color='lightgrey', linewidth=0.2)\n    ax1.set_title(str(var1)+' '+'vs'+' '+str(var2)+' '+'vs target', fontsize=18)\n    ax1.set_xlabel(str(var1), fontsize=12)\n    ax1.set_ylabel(str(var2), fontsize=12)\n\n    ax2 = sns.scatterplot(x = data[var3], y = data[var4], hue = \"target\",\n                        data = data,  ax=ax2, palette=colors, alpha=0.8, edgecolor=\"white\",linewidth=0.1)\n    ax2.grid(b=True, which='major', color='lightgrey', linewidth=0.2)\n    ax2.set_title(str(var3)+' '+'vs'+' '+str(var4)+' '+'vs target', fontsize=18)\n    ax2.set_xlabel(str(var1), fontsize=12)\n    ax2.set_ylabel(str(var2), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplot('age','thalach','age', 'chol')\nscatterplot('age','trestbps','age', 'oldpeak')\nscatterplot('thalach','trestbps','thalach', 'chol')\nscatterplot('thalach','oldpeak','chol', 'trestbps')\nscatterplot('chol','oldpeak','trestbps', 'oldpeak')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.6.2.'>2.6.2. Categorical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def multivariate_count(var):\n    for x in [\n        #'sex',\n        'cp',\n        'fbs',\n        #'restecg',\n        #'exang',\n        #'slope',\n        #'ca',\n        'thal']:\n        ax = sns.catplot(x=x, hue=\"target\", col=var, \n               data=data, kind=\"count\", palette = ['darkturquoise', 'darkorange'], alpha=0.7, edgecolor=('white'), linewidth=2)\n        ax.fig.suptitle(str(var)+' vs '+str(x)+' vs target', fontsize=18) \n    \n        plt.subplots_adjust(top=0.8)\n        plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multivariate_count('exang')\n#multivariate_count('fbs')\n#multivariate_count('cp')\n#multivariate_count('ca')\n#multivariate_count('restecg')\n#multivariate_count('slope')\n#multivariate_count('thal')\nmultivariate_count('sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.6.3.'>2.6.3. Numerical & categorical data</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def multivariate_swarn(col,x,y):\n    g = sns.FacetGrid(data, col=col, hue='target', palette = ['darkturquoise', 'darkorange'], height=5)\n    ax = g.map(sns.swarmplot, x, y, alpha=0.7, edgecolor=('white'), linewidth=0.1)\n    ax.fig.suptitle(str(x)+' vs '+str(y)+' by '+str(col)+' vs target', fontsize=18) \n    plt.subplots_adjust(top=0.8)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multivariate_swarn('fbs','exang','oldpeak')\n#multivariate_swarn('exang','thal','thalach')\n#multivariate_swarn('sex','thal','chol')\n#multivariate_swarn('slope','ca','age')\n\nmultivariate_swarn('fbs','ca','oldpeak')\n#multivariate_swarn('exang','cp','thalach')\n#multivariate_swarn('sex','restecg','chol')\n#multivariate_swarn('slope','thal','age')\n\nmultivariate_swarn('fbs','ca','age')\n#multivariate_swarn('exang','cp','oldpeak')\n#multivariate_swarn('sex','restecg','thalach')\n#multivariate_swarn('slope','thal','chol')\n\nmultivariate_swarn('exang','cp','age')\n#multivariate_swarn('restecg','ca','oldpeak')\n#multivariate_swarn('ca','sex','thalach')\n#multivariate_swarn('cp','slope','chol')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Feature engineering and Preprocessing</a>  "},{"metadata":{},"cell_type":"markdown","source":"## <a id='3.1'>3.1. Feature engineering</a> "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['oldpeak_x_age'] = data['oldpeak'] * data['age']\ndata['oldpeak_div_age'] = data['oldpeak'] / data['age']\ndata['ca_div_age'] = data['ca'] / data['age']\ndata['oldpeak_square'] = data['oldpeak'] * data['oldpeak']\n\ndata = data.fillna(0)\n\nfeatures = list(data)\nfeatures.remove('target')\n\nidx = features \nfor df in [data]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['count'] = df[idx].count(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='3.1'>3.1. Preprocessing</a> "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocessing(dataset, y):\n    target_col = [y]\n    cat_cols   = dataset.nunique()[dataset.nunique() < 5].keys().tolist()\n    cat_cols   = [x for x in cat_cols ]\n    #numerical columns\n    num_cols   = [x for x in dataset.columns if x not in cat_cols + target_col]\n    #Binary columns with 2 values\n    bin_cols   = dataset.nunique()[dataset.nunique() == 2].keys().tolist()\n    #Columns more than 2 values\n    multi_cols = [i for i in cat_cols if i not in bin_cols]\n\n    #Label encoding Binary columns\n    le = LabelEncoder()\n    for i in bin_cols :\n        dataset[i] = le.fit_transform(dataset[i])\n\n    #Duplicating columns for multi value columns\n    dataset = pd.get_dummies(data = dataset,columns = multi_cols )\n\n    #Scaling Numerical columns\n    std = StandardScaler()\n    scaled = std.fit_transform(dataset[num_cols])\n    scaled = pd.DataFrame(scaled,columns=num_cols)\n\n    #dropping original values merging scaled values for numerical columns\n    df_dataset_og = dataset.copy()\n    dataset = dataset.drop(columns = num_cols,axis = 1)\n    dataset = dataset.merge(scaled,left_index=True,right_index=True,how = \"left\")\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = preprocessing(data,'target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = data\nfeatures = list(train_df)\nfeatures.remove('target')\ntarget = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>4. Light GBM</a>  "},{"metadata":{},"cell_type":"markdown","source":"## <a id='4.1'>4.1. Confusion Matrix function</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title, fontsize=12)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='4.2'>4.2. Light GBM - Hyperparameters</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.06,\n                'feature_fraction': 0.7,\n                'bagging_freq': 6,\n                'scale_pos_weight': 1,         \n                'bagging_fraction': 0.3,\n                'max_depth':-1,\n                'objective': 'binary',\n                'n_jobs': -1,\n                'n_estimators':5000,\n                'metric':'auc',\n                'save_binary': True,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 1,\n                'is_unbalance': False,\n                'boost_from_average': True\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='4.3'>4.3. Light GBM - 5 folds </a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('LGBM modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      eval_metric = 'auc',\n                      early_stopping_rounds = 100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = shap.TreeExplainer(model).shap_values(train_df.iloc[val_idx][features])\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('LGBM P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='LGBM Confusion matrix [averaged/folds]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='4.4'>4.4. Light GBM - Features importance </a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:37].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n            edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('LGBM Features importance (averaged/folds)', fontsize=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>5. SHAP</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(\nshap.force_plot(explainer.expected_value, shap_values[0,:], train_df.iloc[val_idx][features].iloc[0,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[1,:], train_df.iloc[val_idx][features].iloc[1,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[2,:], train_df.iloc[val_idx][features].iloc[2,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[3,:], train_df.iloc[val_idx][features].iloc[3,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[4,:], train_df.iloc[val_idx][features].iloc[4,:],figsize=(10, 5)),\nshap.force_plot(explainer.expected_value, shap_values[5,:], train_df.iloc[val_idx][features].iloc[5,:],figsize=(10, 5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values, train_df.iloc[val_idx][features],figsize=(10, 5), plot_cmap='RdBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>6. Recall augmentation </a> "},{"metadata":{},"cell_type":"markdown","source":"## <a id='6.1'>6.1. Light GBM : Scale_pos_weight augmentation </a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.06,\n                'feature_fraction': 0.7,\n                'bagging_freq': 6,\n                'scale_pos_weight': 1.75,         \n                'bagging_fraction': 0.3,\n                'max_depth':-1,\n                'objective': 'binary',\n                'n_jobs': -1,\n                'n_estimators':5000,\n                'metric':'auc',\n                'save_binary': True,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 1,\n                'is_unbalance': False,\n                'boost_from_average': True\n})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('LGBM modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      eval_metric = 'auc',\n                      early_stopping_rounds = 100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = shap.TreeExplainer(model).shap_values(train_df.iloc[val_idx][features])\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('LGBM P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='LGBM Confusion matrix [averaged/folds]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='7'>7. CatBoost </a> "},{"metadata":{},"cell_type":"markdown","source":"## <a id='7.1'>7.1. CatBoost - Hyperparameters</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cb.CatBoostClassifier(**{\n                                'learning_rate':0.05,\n                                'max_depth':2,\n                                'n_estimators':100,\n                                'eval_metric': 'AUC',\n                                'bootstrap_type': 'Bayesian',\n                                'use_best_model':True,\n                                'bagging_temperature': 1,\n                                'objective': 'Logloss',\n                                'od_type': 'Iter',\n                                'l2_leaf_reg': 2,\n                                'allow_writing_files': False})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='7.2'>7.2. CatBoost - 5 folds</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(6,4)\n\nprint('CatBoost modeling...')\nstart_time = timer(None)\n\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\nfeature_importance_df = pd.DataFrame()\ni = 1\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print('Fold:', fold_ )\n    model = model.fit(train_df.iloc[trn_idx][features], target.iloc[trn_idx],\n                      eval_set = (train_df.iloc[val_idx][features], target.iloc[val_idx]),\n                      verbose = 100,\n                      early_stopping_rounds=100)\n    \n    oof[val_idx] =  model.predict_proba(train_df.iloc[val_idx][features])[:,1]\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.target[val_idx], oof[val_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.target[val_idx], oof[val_idx])\n    y_real.append(train_df.target[val_idx])\n    y_proba.append(oof[val_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.target[val_idx], oof[val_idx]))\n    accuracies.append(accuracy_score(train_df.target[val_idx], oof[val_idx].round()))\n    recalls.append(recall_score(train_df.target[val_idx], oof[val_idx].round()))\n    precisions.append(precision_score(train_df.target[val_idx], oof[val_idx].round()))\n    f1_scores.append(f1_score(train_df.target[val_idx], oof[val_idx].round()))\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.target[val_idx], oof[val_idx].round()))\n    \n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n# Timer end    \ntimer(start_time)\n\n# Roc curve\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % ((mean_auc)),lw=2, alpha=1)\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('LGBM - ROC by folds', fontsize=18)\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.grid(b=True, which='major', color='grey', linewidth=0.4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('CB P|R curve by folds', fontsize=18)\nplt.legend(loc=\"lower left\")\n\n# Confusion matrix \nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='CB Confusion matrix [averaged/folds]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='7.3'>7.3. CatBoost - Features importance </a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:37].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n            edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('CB - Features importance (averaged/folds)', fontsize=18)\nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}