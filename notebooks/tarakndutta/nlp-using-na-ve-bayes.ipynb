{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1: Understanding the \n* Natural Language Processing (NLP) works by converting words (texts) into numbers.\n* These numbers are then used to train an AI/ML model to make predictions.\n* In this case, we will analyze thousands of Twitter tweets to predict people's sentiment"},{"metadata":{},"cell_type":"markdown","source":"## 2. Importing the Libraries and Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ntweets_df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is labelled for hatred/negetive tweets, hence \"label: 1\"  indicates negetive tweets and \"label: 0\" is positive sentiments."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df['tweet']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are analyzing the \"tweets\" and the labels, we don't reuqire the \"id\" columns. Hence, lets drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.drop('id', axis=1, inplace=True)\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploring the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.hist(bins=30, figsize=(15,5), color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that all the labels are discreate values of 0 and 1 as discussed earlier, hence this is binary class problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(tweets_df['label'], label='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that this is class bias situation meaning data with label: 1 is much more that label: 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's save the length of each tweets(character) in a sepearte column\ntweets_df['length'] = tweets_df['tweet'].apply(len)\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a histogram on the character count/tweet length\ntweets_df['length'].plot(bins=50, kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like majority of the tweets are between 70 to 100 character."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets seperate the positive and negative tweets in different lists."},{"metadata":{"trusted":true},"cell_type":"code","source":"positive = tweets_df[tweets_df['label']==0]\npositive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = tweets_df[tweets_df['label']==1]\nnegative","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Plot the WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = tweets_df['tweet'].to_list()\nsentences[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'sentences' is still separate by commas and spaces lets join them such that it is single corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"single_sentence = ' '.join(sentences)\nsingle_sentence[:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install WordCloud\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the WordCloud for all tweets\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_sentence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#TODO: WordCloud details"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the WorldCloud for positive words\npositive_sentences = positive['tweet'].to_list()\nsingle_positive = ' '.join(positive_sentences)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_positive))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the WorldCloud for negative words\nnegative_sentences = negative['tweet'].to_list()\nsingle_negative = ' '.join(negative_sentences)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(single_negative))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Data Cleaning: Remove Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets test our string punctuation with a test string\nTest = 'Good morning beautiful people :)... I am having fun learning Machine learning and AI!!'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_punc_remove = ''.join([c for c in Test if c not in string.punctuation])\ntest_punc_remove","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we have removed all the punctuation now."},{"metadata":{},"cell_type":"markdown","source":"## 6. Data Cleaning: Remove Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let imprt the stopwords and see them\nfrom nltk.corpus import stopwords\nstopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_punc_clean = [word for word in test_punc_remove.split() if word.lower() not in stopwords.words('english')]\ntest_punc_clean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Create a function for Step 5 and Step 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"def message_cleaning(message):\n    punc_removed = [char for char in message if char not in string.punctuation]\n    punc_removed_join = ''.join(punc_removed)\n    punc_removed_join_clean = [word for word in punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n    return punc_removed_join_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's apply the function to our tweet dataset\ntweets_df_clean = tweets_df['tweet'].apply(message_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tweets_df_clean[5]) # cleaned up version\nprint(tweets_df['tweet'][5]) # show the orignal version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Count Vectorization or Tokenization\nIn order to use textual data for predictive modeling, the text must be parsed to remove certain words – this process is called tokenization. These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer=message_cleaning, dtype='uint8')\ntweets_countvectorizer = vectorizer.fit_transform(tweets_df['tweet']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_countvectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_countvectorizer.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Now this tweets_countvectorizer is the features for our model"},{"metadata":{},"cell_type":"markdown","source":"## 8. Naive Bayes\nNaive Bayes is a classification technique based on Bayes' Theorem. Bayes’ theorem is based conditional probability which states the likelihood the occurrence of event “A” given another event “B” has already happened.\nThere are 3 type of Naïve Bayes:\n* Gaussian ->The model assume that the data follows normal distribution and all our features are continuous.\n* Bernoulli -> It assumes that all our features are binary such that they only take two values: 0s and 1s.\n* Multinomial -> It assumes that the data has discreate value such as ratings between 1 to 5.\n\nMore on Naive Bayes can be found in my article here: https://medium.com/analytics-vidhya/na%C3%AFve-bayes-classifiers-fafde4f0a411"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define the features(X) and labels(y) for our model\nX = tweets_countvectorizer\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = tweets_df['label']\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Train the Naive Bayes Classifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nNB_classifier = MultinomialNB()\nNB_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Check our Model Accuracy through Confusion Matrix\nA confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.\n\nMore details can be found in my article here: https://medium.com/analytics-vidhya/clarity-in-confusion-matrix-17fb1da6dabf"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions(precision=3)\nfrom sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = NB_classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_predict_test)\nsns.heatmap(cm, annot=True, fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Special thanks to Ryan Ahmed from Coursera https://www.coursera.org/projects/twitter-sentiment-analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}