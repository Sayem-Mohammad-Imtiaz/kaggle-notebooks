{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/unsupervised-learning-on-country-data/Country-data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array((df[['income', 'gdpp']]).astype(float))\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n = X.shape[1]\nfor j in range(n):\n    X[:, j] -= np.mean(X[:, j])\n    print(np.mean(X[:, j]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing Centroids","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 3\n(m, n) = X.shape\nmu = np.random.randint(1, 10, (k, n))\nprint(mu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Closest Centroids","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_closest_centroids(X, mu):\n    m = X.shape[0]\n    k = mu.shape[0]\n    c = np.zeros([m, 1])\n    distance = np.zeros([m, k])\n    for i in range(m):\n        for j in range(k):\n            distance[i, j] = np.sum((X[i, :] - mu[j, :])**2)\n        dist = list(distance[i, :])\n        c[i, 0] = dist.index(min(dist))\n    return c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = find_closest_centroids(X, mu)\nprint(c[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute Centroids","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_centroids(X, c, mu):\n    (k, n) = mu.shape\n    m = X.shape[0]\n    for i in range(k):\n        points = []\n        for j in range(m):\n            if c[j, 0] == i:\n                points.append(j)\n        for j in range(n):\n            mu[i, j] = np.mean(X[points, j])\n    return mu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_centroids(X, c, mu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimization Objective / Cost Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost_function(X, mu, c):\n    m = X.shape[0]\n    J = 0\n    for i in range(m):\n        idx = int(c[i, 0])\n        J += np.sum((X[i, :] - mu[idx, :])**2)\n    return J / m","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Iterations showing clusters imporoved by K-Means Algorithm after every iteration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 3\nmax_iters = 25\nnp.random.seed(0)\nmu = np.random.randint(1, 10, (k, n))\nfor i in range(max_iters):\n    idx = find_closest_centroids(X, mu)\n    centroids = compute_centroids(X, idx, mu)\n    plt.figure(figsize = (12, 8))\n    color = ['r', 'g', 'b']\n    mark = ['+', 'o', '*']\n    for i in range(k):\n        points = []\n        for j in range(m):\n            if idx[j, 0] == i:\n                points.append(j)\n        plt.scatter(X[points, 0], X[points, 1], c = color[i], marker = mark[i], s = 100)\n    plt.xlabel(\"Income\")\n    plt.ylabel(\"GDP\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As it can be observed from the last two plots that they are exactly same, hence concludes the K-Means Algorithm trying to find best clusters for the given data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Final K-Means Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ncolor = ['r', 'g', 'b']\nmark = ['+', 'o', '*']\nfor i in range(k):\n    points = []\n    for j in range(m):\n        if idx[j, 0] == i:\n            points.append(j)\n    plt.scatter(X[points, 0], X[points, 1], c = color[i], marker = mark[i], s = 100)\nplt.xlabel(\"Income\")\nplt.ylabel(\"GDP\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_function(X, centroids, idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means by Monte-Carlo Method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 3\n(m, n) = X.shape\nindexes = []\ncosts = []\nfor i in range(100):\n    mu = np.random.randint(1, 10, (k, n))\n    idx = find_closest_centroids(X, mu)\n    if (0 not in idx) or (1 not in idx) or (2 not in idx):\n        pass\n        #print(\"something's missing\")\n    else:\n        centroids = compute_centroids(X, idx, mu)\n        J = cost_function(X, centroids, idx)\n        #print(J)\n        costs.append(J)\n        indexes.append(idx)\ni_min = costs.index(min(costs))\nbest_clusters = indexes[i_min]\nprint(f\"minimum cost: {costs[i_min]}\")\n\nplt.figure(figsize = (12, 8))\ncolor = ['r', 'g', 'b']\nmark = ['+', 'o', '*']\nfor i in range(k):\n    points = []\n    for j in range(m):\n        if best_clusters[j, 0] == i:\n            points.append(j)\n    plt.scatter(X[points, 0], X[points, 1], c = color[i], marker = mark[i], s = 100)\nplt.xlabel(\"Income\")\nplt.ylabel(\"GDP\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It can be easily observed that even after using 100 random samples, it cannot give a better result. Still the iteration method of finding best position for cluster centroids gives a better output with just 5 iterations.**\n\nHence our implementation of K-Means Clustering from Scratch works just fine.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Prinicipal Component Analysis (PCA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = np.array(df.drop(['country'], axis = 1))\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = features.shape[0]\nsigma = np.dot(features.T, features) / m\nprint(sigma.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"u, s, v = np.linalg.svd(sigma)\nprint(u.shape, s.shape, v.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = range(1, 9)\nvariance = []\nfor i in dim:\n    v = np.sum(s[:i]) / np.sum(s)\n    variance.append(v)\nprint(variance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***For taking a suitable dimension for PCA variance retained should be greater than 0.99, which is we're getting for 2 dimension for columns of the data. Hence we can reduce the data upto 2D.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d = 2\nu_reduce = u[:, 0:d]\nprint(u_reduce.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting reduced dimension data with all training examples & the new features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"z = np.dot(features, u_reduce)\nprint(z.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying K-Means Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_iters = 5\nk = 3\n(m, n) = z.shape\nnp.random.seed(0)\nmu = np.random.randn(k, n)\nfor i in range(max_iters):\n    idx = find_closest_centroids(z, mu)\n    centroids = compute_centroids(z, idx, mu)\n    plt.figure(figsize = (12, 8))\n    color = ['r', 'g', 'b']\n    mark = ['+', 'o', '*']\n    for i in range(k):\n        points = []\n        for j in range(m):\n            if idx[j, 0] == i:\n                points.append(j)\n        plt.scatter(z[points, 0], z[points, 1], c = color[i], marker = mark[i], s = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, no wonder the results are not good, because we only use PCA in case we do not get our desired results with the data. If we'll apply PCA to every problem unnecessarily, then it will end up getting worse, still the target of the implementation was to get understanding of how PCA works and how is it implemented & that is achieved.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}