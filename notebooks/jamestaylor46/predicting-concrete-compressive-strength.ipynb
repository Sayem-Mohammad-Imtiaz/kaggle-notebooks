{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting the compressive strength of concrete\n## Introduction\n\nConcrete quality is typically defined by it's compressive strength. Civil engineers will carry out rigorous testing using varying combinations of raw materials and curing time. The process of testing concrete compressive strength can be found [here](http://www.civilengineeringforum.me/compressive-strength-test-of-concrete/). With curing time taking up to 91 days in some cases, the whole process is very time consuming and labour intensive.\n\nThere is a clear opportunity for digital simulation to reduce wait time and total number of combinations. With the data set acquired we can learn about the relations between variables and develop a predictive model. Highlighting potentially optimal combinations to then be used in physical testing will provide enormous benefit and significantly reduce labour and testing costs.\n\n## Data Collection\n\nIn this project we will be using the [Concrete Compressive Strength data set](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength) from the UCI Machine Learning Repository.\n\nThe data contains over 1,000 instances of concrete each with 9 variables (including compressive strength). \n"},{"metadata":{},"cell_type":"markdown","source":"## Initial Setup and Checks"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/yeh-concret-data/Concrete_Data_Yeh.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully data does not contain any missing values so there is no need for imputing. However, the field titles are overly descriptive for our purposes so let's rename them to be more succinct."},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_col_names = ['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n                     'Coarse Aggregrate', 'Fine Aggregate', 'Age', 'Compressive Strength']\n\ndf.columns = updated_col_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nThe first, and arguably most important, step in a data science project is to explore the data in an attempt to gain insights that will guide the rest of the project. EDA generally includes ensuring the data is clean and usable, visualising features and their relationships, observing distributions, etc.\n\nFirst of all, let's check the disribution of our target variable and take a closer look at the input features and try to identify any correlations."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot(df['Compressive Strength'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution looks close to normal so no need to make any changes here."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance, there doesn't seem to be any high correlation between any 2 features. Although Cement and Compressive Strength look like they may have some correlation. Let's determine the Pearson Correlation coefficients to get a numerical value of the strength of the correlations."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corr = df.corr()\n\nplt.figure(figsize = (10,8))\nsns.heatmap(corr, cmap = 'Blues', annot = True)\nplt.title('Pearson Correlation Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the highest positive correlations is between **Compressive Strength** and **Cement**. \n\n**Age** and **Superplasticizer** also have a positive impact on the **Compressive Strength**.  \n\nBy sorting for the largest magnitude values (both positive and negative) we can focus on the strong correlations with the target variable, and the strong correlations between other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_sorted = corr.unstack().sort_values(kind='quicksort', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(corr_sorted[corr_sorted!=1].head(10))\nprint(corr_sorted[corr_sorted!=1].tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, there aren't many high correlations with Compressive Strength (CS). The only significant ones are the following 3 top features.\n\n|Feature|Correlation w/ CS|\n|:---|:---|\n|Cement|0.50|\n|Superplasticizer|0.37|\n|Age|0.33|"},{"metadata":{},"cell_type":"markdown","source":"Looking at the top 3 with the highest negative correlation, perhaps water could be a useful variable, Fine Aggregate and Age are unlikely to be helpful in isolation.  \n\n|Feature|Correlation w/ CS|\n|:---|:---|\n|Water|-0.29|\n|Fine Aggregate|-0.17|\n|Age|-0.16|"},{"metadata":{},"cell_type":"markdown","source":"Some other notable strong correlations include the following.\n\n|Features|Correlation|\n|:---|:---|\n|Superplasticizer / Water|-0.66|\n|Fine Aggregate / Water|-0.45|\n|Cement / Fly Ash|-0.40|\n|Fly Ash / Superplasticizer|0.38|"},{"metadata":{},"cell_type":"markdown","source":"Already we can determine the 3 most postive and negative influences on Compressive Strength.\n\nAdditionally, there are strong correlations between some of the feature variables. We can see a highly negative correlation between **Superplasticizer** and **Water**, but a positive correlation between **Superplasticizer** and **Fly Ash**.\n\nPerhaps plotting these relationships in a visual way will help to gain more insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(y=\"Compressive Strength\", x=\"Cement\", hue=\"Water\", size=\"Age\", data=df, ax=ax, sizes=(50, 300),\n                palette='RdYlGn', alpha=0.9)\nax.set_title(\"Compressive Strength vs Cement, Age, Water\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot we can make some sensible observations on the relationships between these variables and compressive strength:  \n\n* **Compressive Strength** correlates positively with **Cement**\n* **Compressive Strength** correlates positively with **Age**, though less than **Cement**\n* Older **Cement** tends to require more **Water**, as shown by the larger green data points\n* **Compressive Strength** correlates negatively with **Water**\n* High **Compressive Strength** with a low **Age** requires more **Cement**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(y=\"Compressive Strength\", x=\"Fine Aggregate\", hue=\"Fly Ash\", size=\"Superplasticizer\", data=df, ax=ax, sizes=(50, 300),\n                palette='RdYlBu', alpha=0.9)\nax.set_title(\"Compressive Strength vs Fine Aggregate, Fly Ash, Superplasticizer\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1)) # Moved outside the chart so it doesn't cover any data\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot we can make further observations on the relationships between this second set of variables and compressive strength:  \n\n* **Compressive strength** correlates negatively with **Fly Ash**\n* **Compressive strength** correlates positively with **Superplasticizer**\n\nThrough some very simple charts we have discovered relationships between ingredients that allow us to make predictions on what our future model will value when seeking a high compressive strength.  \n\nIt is likely that the ideal concrete mixture (when prioritising compressive strength) will consist of:\n\n* Large quantity of **Cement**\n* Potentially a long aging process however this comes at the cost of adding **Water**, which negatively impacts the strength\n* Large quantity of **Superplasticizer**"},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nPrior to fitting a machine learning model, steps must be taken to prepare the data to get the most out of the models.\n\n* Splitting data into train and test sets ensures that we evaluate our model on unseen data.\n* Scaling the features to have a mean residing at 0 and a standard deviation of 1 helps to bring a consistency to our variables. This stops the model putting higher weights on greater values, and vice versa."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the features and target variable\n\ncols = df.columns.drop('Compressive Strength')\nX = df[cols]\ny = df['Compressive Strength']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\n\nNow that the data is prepared, we can fit different models on the training data and compare their performance on predicting the test data. To evaluate the models we'll use a variety of metrics suitable to a regression problem like this.\n\nThis diagram from the [scikit-learn website](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) can be helpful tool when it comes to deciding which models to initially compare.\n\nSince we \"predicting a quantity\" we are solving a regression problem. We also have less than 100k samples so **Lasso** and **Elastic Net** are the recommended estimators. Just to cover the basics and completeness, we'll also throw in basic **Linear Regression** and **Ridge**"},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression\n\nIn this section we will evaluate 4 different linear regression models:\n* Simple\n* Ridge\n* Lasso \n* Elastic Net \n\n### Simple (Ordinary Least Squares)\nAs the hallmark of any regression problem, linear regression is a simple but effective way of making predictions when you expect your data to have mostly linear relationships.\n\n$$\ny = \\alpha + \\beta x = \\sum_{i=0}^{n} \\alpha + \\beta_i x_i\n$$\n\nWhere $y$ is the target variable we are looking to predict, we use each feature variable ($x_i$) multiplied by a coefficient ($\\beta_i$) plus the consistent error ($\\alpha$).\n\nSometimes the Ordinary Least Squares method can suffer from a low bias and a consequently high variance. To remedy some of this, we can introduce regularisation in the form of Ridge and Lasso regression.\n\n### Ridge\nRidge regression introduces a penalty on the size of coefficients which can address some of the problems faced when using simple regression. In simple terms, the introduction of a term $\\lambda$ is multiplied by the coefficient and added to the overall error, therefore encouraging a lower coefficient that is less sensitive to change.\n\nTo identify the best value of $\\lambda$ we will use 10-fold cross validation.\n\n### Lasso\nLasso regression is very similar to ridge regression. They key difference is that instead of $\\lambda$ being multiplied by the coefficient squared, it is instead multiplied by the magnitude. This importantly means that instead of less valuable features asymptotically reaching 0 with ridge regression, they can be completely nullified to 0 with lasso regression.\n\nOnce again we will use 10-fold CV to identify the best value for $\\lambda$.\n\nIn conclusion, ridge performs well when most variables are useful, and lasso performs well when you have a lot of useless variables. It is unclear which approach will work best in our problem since we have a rather small amount of variables, but some of them seem to have little impact on the compressive strength.\n\n### Elastic Net\nIf we find that the positives of ridge and lasso are both worthwhile, then perhaps elastic net regression is the way to go. This method combines the positives of the 2 previously mentioned into a nice package.\n\nWith this model we'll need to identify to individual values of lambda, $\\lambda_1$ and $\\lambda_2$ with cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_alpha = RidgeCV(cv=10)\nridge_alpha.fit(X_train, y_train)\nalpha = ridge_alpha.alpha_\n\nridge = Ridge(alpha=alpha, random_state=42)\nridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_alpha = LassoCV(cv=10)\nlasso_alpha.fit(X_train, y_train)\nalpha = lasso_alpha.alpha_\n\nlasso = Lasso(alpha=alpha, random_state=42)\nlasso.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EN_alpha = ElasticNetCV(cv=10)\nEN_alpha.fit(X_train, y_train)\nalpha= EN_alpha.alpha_\n\nEN = ElasticNet(alpha=alpha, random_state=42)\nEN.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we look at the results, let's see how each model values each feature by visualising the coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_coef (models):\n    \n    coefs = {}\n    rows = []\n    fig, ax = plt.subplots(figsize=(16,8))\n    offset = 0\n    width = 0.23\n    x = np.arange(len(X.columns))\n    \n    # Creating a neat table to view\n    for model in models:\n        coefs[type(model).__name__] = model.coef_\n        \n    coefs_table = pd.DataFrame.from_dict(coefs, orient='index')\n    coefs_table.columns = X.columns\n    \n    # Using the table to create a chart\n    for i in range(len(models)):\n        increment = width\n        ax.bar(x - width + offset, coefs_table.iloc[i], width=width, label=type(models[i]).__name__)\n        offset = offset + increment\n        \n    ax.set_ylabel('Coefficient')\n    ax.set_xlabel('Features')\n    ax.set_title('Feature Coefficients')\n    ax.set_xticks(x)\n    ax.set_xticklabels(X.columns)\n    ax.legend()\n    \n    return coefs_table","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"models = [lr, ridge, lasso, EN]\n\ncoefs_table = plot_coef(models)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"coefs_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the behaviour between these models is similar since they are all linear regression models. However, some of our previous expectations of coefficients are displayed. The ridge model attempts to reduce the coefficients where possible. The lasso reduces them even further and even brings some down to 0 when necessary. Then finally, the elastic net lands somewhere in the middle.\n\nIts time for the moment of truth, let's see the results of each model's predictions on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pred (models, X_test, y_test):\n    \n    results = {}\n    \n    for model in models:\n        y_pred = model.predict(X_test)\n        results[type(model).__name__] = [mean_squared_error(y_test, y_pred)**(1/2), \n                                         mean_absolute_error(y_test, y_pred), \n                                         r2_score(y_test, y_pred)]\n        \n        results = pd.DataFrame(results, index=['RMSE','MAE','R2'])\n        \n    return results","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"make_pred(models, X_test, y_test).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although all of our models have performed similarly, it depends on which scoring metric you look at the determine the overall winner. Let's explore the different scoring metrics."},{"metadata":{},"cell_type":"markdown","source":"#### Root Mean Square Error (RMSE)\nRMSE is the square root of the error function that the regression algorithms are trying to reduce. It is an absolute measure of how well the model fits the data.\n\n#### Mean Absolute Error (MAE)\nSimilarly to RMSE, MAE looks at the sum of the value of errors. However, since we are not squaring the value and instead taking the absolute value, it is more forgiving to large prediction errors.\n\n#### R Square\nR Square measures how much variability in a dependent variable can be explained by the model. It is a good metric to determine the fit on dependent variables. But, it does not take into consideration overfitting."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nUsing machine learning models we have simulated the compressive strength of concrete using countless combinations of ingredients and aging process.\n\nThrough EDA we have discovered that cement, age and superplasticizer have a positive impact on the overall strength. Water is found to have a negative impact, but is often present in older concrete due to the curing processes.\n\nAfter utilising linear regression models it has become evident that blast furnace slag is much more important than previously thought, and superplasticizer the opposite. These insights are not intuitive and could be profound and beneficial in a business context.\n\nAll of our linear regression models performed similarly in the scoring metrics and would all be suitable in a production environment for making further predictions and testing.\n\nTo improve, perhaps decision trees and ensemble methods could be used to increase the prediction score."},{"metadata":{},"cell_type":"markdown","source":"### References\n\nData: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength  \nConcrete Testing: http://www.civilengineeringforum.me/compressive-strength-test-of-concrete/  \nScikit-learn: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}