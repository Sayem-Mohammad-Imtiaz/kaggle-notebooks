{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Diabetes](https://www.diabetes.co.uk/wp-content/uploads/2019/01/How-to-Bring-Down-High-Blood-Sugar-Levels-1.png)"},{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers, <br>\n    In this notebook I tried to create an optimal model for Diabetes Prediction from given .csv data.<br>\n    Points covered are:<br>\n     0] Exploratory Data Analysis and Visualization<br>\n     1] Data Normalized Distribution<br>\n     2] Data Up-Sampling  for Imbalance data<br>\n     3] Feature Engineering and Selection<br>\n     4] Fine tuning of Models.<br>\n <br>\n If you found this notebook helpful, your *Upvote Will Encourage Me* !!! ðŸ˜€ðŸ˜‡ðŸ˜Š\n \n ## Problem Statement : Diabetes Prediction\n "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelBinarizer , StandardScaler ,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report , accuracy_score,plot_confusion_matrix\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.linear_model import LogisticRegression , RidgeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier , VotingClassifier , RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overlooking whole data in a single window."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks major of the attributes contain Non-Normally Distributed data points.\nAlso Data is seriously imbalance."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Outcome\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Checking for null values distribution...*"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(data.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Checking for feature importance of attributes...\nBy feeding data to Classifier\n*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = data.drop([\"Outcome\"],axis=1)\nY = data[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBR = XGBClassifier()\nXGBR.fit(X,Y)\nfeatures = XGBR.feature_importances_\nColumns = X.columns\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\n\nplt.figure(figsize=(16,6))\nplt.title(label=\"XGB\")\n#plt.bar([x for x in range(len(features))],features)\nplt.bar([x for x in (Columns)],features)\nplt.show()\n\nplot_importance(XGBR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say Skin \"Thickness\" is least important attribute.\n\nWhere \"Glucose\", \"DiabetesPedigreeFunction\",\"BMI\" are oe of the most attributes.\n\nLets benchmark dataset i.e. train Classifier without any explicit featue engineering or modification in data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop([\"Outcome\"],axis=1)\nY = data[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier()\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To performance lets adjust distribution of these attributes->\n\n\"Pregnancies\",\"Glucose\",\"SkinThickness\",\"Insulin\",\"DiabetesPedigreeFunction\",\"Age\"\n\nTaking log value of data-points of these features will distribute them normally."},{"metadata":{"trusted":true},"cell_type":"code","source":"#\"Pregnancies\",\"Glucose\",\"SkinThickness\",\"Insulin\",\"DiabetesPedigreeFunction\",\"Age\"\ndata_new = data.copy()\n\ndata_new[\"Pregnancies\"].hist()\nplt.show()\ndata_new[\"Glucose\"].hist()\nplt.show()\ndata_new[\"SkinThickness\"].hist()\nplt.show()\ndata_new[\"Insulin\"].hist()\nplt.show()\ndata_new[\"DiabetesPedigreeFunction\"].hist()\nplt.show()\ndata_new[\"Age\"].hist()\nplt.show()\n\n\ndata_new[\"Pregnancies\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"Pregnancies\"]]\ndata_new[\"Glucose\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"Glucose\"]]\ndata_new[\"SkinThickness\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"SkinThickness\"]]\ndata_new[\"Insulin\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"Insulin\"]]\ndata_new[\"DiabetesPedigreeFunction\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"DiabetesPedigreeFunction\"]]\ndata_new[\"Age\"] = [np.log(i) if i!=0 else 0 for i in data_new[\"Age\"]]\n\nprint(\"=\"*10,\"\\nAfter normal distibution operation\\n\")\n\ndata_new[\"Pregnancies\"].hist()\nplt.show()\ndata_new[\"Glucose\"].hist()\nplt.show()\ndata_new[\"SkinThickness\"].hist()\nplt.show()\ndata_new[\"Insulin\"].hist()\nplt.show()\ndata_new[\"DiabetesPedigreeFunction\"].hist()\nplt.show()\ndata_new[\"Age\"].hist()\nplt.show()\n\nsns.pairplot(data_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n#\"Pregnancies\",\"Glucose\",\"SkinThickness\",\"Insulin\",\"DiabetesPedigreeFunction\",\"Age\"\n\ndata_boxcox = data.copy()\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"Pregnancies\"].values]\ndata_boxcox[\"Pregnancies\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"Pregnancies\"],fitted_lambda)\ndata_boxcox[\"Pregnancies\"].hist()\nplt.show()\n\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"Glucose\"].values]\ndata_boxcox[\"Glucose\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"Glucose\"],fitted_lambda)\ndata_boxcox[\"Glucose\"].hist()\nplt.show()\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"SkinThickness\"].values]\ndata_boxcox[\"SkinThickness\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"SkinThickness\"],fitted_lambda)\ndata_boxcox[\"SkinThickness\"].hist()\nplt.show()\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"Insulin\"].values]\ndata_boxcox[\"Insulin\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"Insulin\"],fitted_lambda)\ndata_boxcox[\"Insulin\"].hist()\nplt.show()\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"DiabetesPedigreeFunction\"].values]\ndata_boxcox[\"DiabetesPedigreeFunction\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"DiabetesPedigreeFunction\"],fitted_lambda)\ndata_boxcox[\"DiabetesPedigreeFunction\"].hist()\nplt.show()\n\nto_convert = [i if i!=0 else 1 for i in data_boxcox[\"Age\"].values]\ndata_boxcox[\"Age\"],fitted_lambda= boxcox(to_convert,lmbda=None)\ninv_boxcox(data_boxcox[\"Age\"],fitted_lambda)\ndata_boxcox[\"Age\"].hist()\nplt.show()\n\nsns.pairplot(data_boxcox)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_new.describe())\nprint(data_boxcox.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Term Normally Distributed data referes to <br>\n1] Satndard Deviation of data = 1 and Mean of data = 0. <br>\n2] Graph plot of data gives Bell Curve<br>\n\n![Bell Curve](https://i.pinimg.com/originals/dd/5e/f9/dd5ef94c82281d75ff0bce252c6be136.jpg)\n<br>\n<br>\nIntuition behind the the Nromal Data Distribution in simple langugae is Most of the data is<br>\nat near Mean of the whole data. <br>\n\n68% data points relie between -1 and 1<br>\n95% data points reliw between -2 and 2<br>\n\nStandard deviation ensures that all the data points are grouped together and having specific range.<br>\nInfact Stadard deviation = 1 removes outliers from data. Outliers are the points which not fit <br>\nin the normal range of points. It is simply away from the mean.<br>\n\n\nFrom above graphs after logarithm value method and BoxCox method both are removing outliers from data.<br>\nGraph shows data after BoxCox method data is normally distributed. <br>\nWhere mathematical values are showing log value has removed outliers <br>\n\nWe will try both methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_boxcox.drop([\"Outcome\"],axis=1)\nY = data_boxcox[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier()\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding Classification report\n![Metrics](https://static.packt-cdn.com/products/9781785282287/graphics/B04223_10_02.jpg)\n![Accuracy](https://miro.medium.com/max/1594/0*qLxAWTs-gZjQvTi4.jpg)\n![Precision](https://miro.medium.com/max/1104/1*5PvyyMvH5n42XICQrlXOzw.png)\n![Recall](https://lawtomated.com/wp-content/uploads/2019/10/Recall_1.png)\n![F1 Score](https://datascience103579984.files.wordpress.com/2019/04/capture3-24.png)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_new.drop([\"Outcome\"],axis=1)\nY = data_new[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier()\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy increased significantly!!!.ðŸ˜€ <br>\nOne more noticable thing is improvement in performance of model for second class. <br>\nLook at Precision, Recall and F1 Score is increased for first class but also for second class. <br>\n\nNow lets tackle second problem which is Imabalance Data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Count of Negative class: \",list(data[\"Outcome\"]).count(0))\nprint(\"Count of Positive class: \",list(data[\"Outcome\"]).count(1))\ndata[\"Outcome\"].hist()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is highly biased towards Negative outcome i.e \"0\" than positive \"1\".<br>\nOut of 768 records 500 records holds Negative outcome. Where only 268 records holds positive outcome. <br>\n\n*To overcome this problem we can Up-Sample or Down-Sample data points according to Minority and Majority.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To keep BoxCox data as it is to use the same for later.\ndata_bal = data_boxcox.copy()\n\n#Getting seperated data with 1 and 0 status.\ndf_majority = data_bal[data_bal.Outcome==0]\ndf_minority = data_bal[data_bal.Outcome==1]\n\n#Here we are downsampling the Majority Class Data Points. \n#i.e. We will get equal amount of datapoint as Minority class from Majority class\n\ndf_manjority_downsampled = resample(df_majority,replace=False,n_samples=268,random_state=123)\ndf_downsampled = pd.concat([df_manjority_downsampled,df_minority])\nprint(\"Downsampled data:->\\n\",df_downsampled.Outcome.value_counts())\n\n#Here we are upsampling the Minority Class Data Points. \n#i.e. We will get equal amount of datapoint as Majority class from Minority class\ndf_monority_upsampled = resample(df_minority,replace=True,n_samples=500,random_state=123)\ndf_upsampled = pd.concat([df_majority,df_monority_upsampled])\nprint(\"Upsampled data:->\\n\",df_upsampled.Outcome.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_downsampled.drop([\"Outcome\"],axis=1)\nY = df_downsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier()\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_upsampled.drop([\"Outcome\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier()\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy and other metrics too says it all. <br>\nUp-Sampling is helpfull in our case to make data balanced. <br>\nNow to create an optimal model Fine-Tuning of model Classifer is needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_upsampled.drop([\"Outcome\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\nXGBR = XGBClassifier(learning_rate =0.1,n_estimators=100000,max_depth=6,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBR.fit(X_train,y_train)\ny_pred = XGBR.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(XGBR,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_upsampled.drop([\"Outcome\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=27)\n\n#RF = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nRF = RandomForestClassifier(n_estimators=10000,random_state=42,n_jobs=1000,max_depth=70,bootstrap=True)\nRF.fit(X_train,y_train)\ny_pred = RF.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.5 % addition is also a good step.<br>\n# Peak points till now:-> <br>\nBenchmark : 75.97 <br>\nXGB : 87.50 <br>\nRF : 88.50"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets scale the data\nStSc = StandardScaler()\nMnMx = MinMaxScaler()\n\nX = df_upsampled.drop([\"Outcome\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=27)\n\nX_train , X_test = MnMx.fit_transform(X_train) , MnMx.fit_transform(X_test)\n\nRF = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\n#RF = RandomForestClassifier(n_estimators=10000,random_state=42,n_jobs=1000,max_depth=70,bootstrap=True)\nRF.fit(X_train,y_train)\ny_pred = RF.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heat map of dataset with relative importance\nmatrix = data_boxcox.drop([\"Outcome\"],axis=1).corr()\n#f , ax = plt.subplots(figsize=(18,6))\nplt.figure(figsize=(18,8))\nsns.heatmap(matrix,vmax=0.8,square=True,cmap=\"BuPu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make action of Feature-Engineering. <br>\nRemember attributes \"Pregnancies\" , \"SkinThickness\" , \"Insulin\" are having less imporance so lets send them for rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = df_upsampled.drop([\"Outcome\" , \"Pregnancies\" , \"SkinThickness\" ,\"Insulin\"],axis=1) # 0.89\nX = df_upsampled.drop([\"Outcome\" ,\"BloodPressure\", \"Pregnancies\"  ,\"SkinThickness\" ,\"Insulin\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=27)\n\nRF = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\n#RF = RandomForestClassifier(n_estimators=10000,random_state=42,n_jobs=1000,max_depth=70,bootstrap=True)\nRF.fit(X_train,y_train)\ny_pred = RF.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(accuracy_score(y_pred,y_test))\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues)\nplot_confusion_matrix(RF,X_test,y_test,display_labels=[\"Diabetic\",\"Non-Diabetic\"],cmap=plt.cm.Blues,normalize='true')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_upsampled.drop([\"Outcome\" ,\"BloodPressure\", \"Pregnancies\"  ,\"SkinThickness\" ,\"Insulin\"],axis=1)\nY = df_upsampled[\"Outcome\"]\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=27)\n\n\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\n\nRF = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nRF.fit(X_train,y_train)\ny_pred = RF.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier(random_state=0)\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test))\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Magical Outliers Removing Technique ðŸš€\n<br>\nIn this method outliers will be removed from original data and will directly fitted in model without any<br>\nexplicit feature engineering or sampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_ = data.copy()\ndata_.loc[(data_.SkinThickness<5)& (data_.Outcome==0), 'SkinThickness']=int(data_[(data_.Outcome==0)]['SkinThickness'].mean())\ndata_.loc[(data_.SkinThickness<5)& (data_.Outcome==1), 'SkinThickness']=int(data_[(data_.Outcome==1)]['SkinThickness'].mean())\ndata_.loc[(data_.Insulin==0)& (data_.Outcome==0), 'Insulin']=int(data_[(data_.Outcome==0)]['Insulin'].mean())\ndata_.loc[(data_.Insulin==0)& (data_.Outcome==1), 'Insulin']=int(data_[(data_.Outcome==1)]['Insulin'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(data_[[\"Pregnancies\",\"BloodPressure\",\"Glucose\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]])\nY = np.array(data_.Outcome)\nX_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n\n\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\nModel1 = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nModel1.fit(X_train,y_train)\ny_pred = Model1.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier(random_state=0)\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test))\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results ðŸ’¹ðŸ“ˆ\n<br>\nBenchmark : 75.97 <---> *Without any processing* <br>\nXGBoost : 87.50 <---> *After Distribution Normalization + Up-Sampling + Feature Selection* <br>\nXGBoost & Random Forest : 89.00 <---> *After Distribution Normalization + Up-Sampling + Feature Selection + Fine Tuning + Random State in Data Spliting*<br>\nGradient Boosting Classifier : 92.20 <---> *After removing outliers*"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(list(data.SkinThickness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visit for outlier removal techniques<br> \nhttps://www.kaggle.com/akhileshdkapse/starter-guide-eda-acc-87-precision-92/notebook#Removing-outliers-! <br>\nAnd <br>\nhttps://www.kaggle.com/abdulrahmanahajj/diabetes-acc-92-auc-0-914 "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}