{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Image](https://azbigmedia.com/wp-content/uploads/2019/10/short-term-personal-loans.png)"},{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers,<br>ðŸ˜€ðŸ™‚ðŸ˜€\nIn this notebook I tried to create an optimal model for Loan Approval Prediction from given .csv data.<br>\nPoints covered are:<br>\n* Exploratory Data Analysis and Visualization<br>\n* Data Normalized Distribution<br>\n* Data Up-Sampling for Imbalance data<br>\n* Feature Engineering and Selection<br>\n* Fine tuning of Models.<br>\n\nIf you found this notebook helpful, your Up-Vote Will Encourage Me !!! ðŸ˜€ðŸ˜‡ðŸ˜Š<br>\n\n## Problem Statement: Loan Approval Prediction Problem\nType: Binary Classification<br>\nLoan approval prediction is classic problem to learn and apply lots of data analysis techniques to <br>\ncreate best Classification model.<br>\n\nGiven with the dataset consisting of details of applicants for loan and status whether the loan application is approved or not.<br>\nBasis on the a binary classification model is to be created with maximum accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic and most important libraries\nimport pandas as pd , numpy as np\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler , MinMaxScaler\nfrom collections import Counter\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly\n\n#Classifiers\nfrom sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier , VotingClassifier , RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression , RidgeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n#Model evaluation tools\nfrom sklearn.metrics import classification_report , accuracy_score , confusion_matrix\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.model_selection import cross_val_score\n\n#Data processing functions\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(r\"/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\")\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = px.scatter_matrix(data[\"ApplicantIncome\"])\nfig.update_layout(width=700,height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems need to work on data preperation\n\n-Loan Amount column does is not fit in Normal Distribution\n\n-Outliers in Applicant's Income and Co-applicant's income\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(data.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normal Distribution\nCentral limit theorem\nIn simple language we can say that maximum amount of data / or maximum number of data points are near the Mean of the all\ndata points.\n\nTo validate he normal distribution of the data:-\nMean Mode Median are Equal.\\n\n\nWe can gen identified the distribution of entire data with the help of Mean and Standard Deviation.\n\nWhen the data is normally distributed maximum data is centralized near the mean value of the data.\n\nTo get understanding of distribtuion we can simply plot Distribution plot i.e. Simple Histogram.\n\nNormally Distributed data represents a Bell Shaped curve.\n\nAlso Mean , Mode , Median on Normaly Distributed data are equal (Mean=Mode=Median)\n\nOne more method is to calculate mean which should be 0 or near to 0 and Standard deviation 1 or near 1.\n\nMean = sum(All Data Points)/count(Data Points)\n\nStandard Deviation = Root of { sum [Square (each data point - mean of whole data) ] }\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if the non-categorical variables are Normally Distributed or Not. i.e. Checking outliers...\n\nprint(\"Data distribution analysis:->---------------------------------------\\n\")\nprint(\"\\nMean:->\\n\")\nprint(\"ApplicantIncome: \",np.mean(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.mean(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.mean(data[\"LoanAmount\"]))\n\nprint(\"\\nMode:->\\n\")\nprint(\"ApplicantIncome: \",stats.mode(data[\"ApplicantIncome\"])[0])\nprint(\"CoapplicantIncome: \",stats.mode(data[\"CoapplicantIncome\"])[0])\nprint(\"LoanAmount: \",stats.mode(data[\"LoanAmount\"])[0])\n\nprint(\"\\nMedian:->\\n\")\nprint(\"ApplicantIncome: \",np.median(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.median(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.median(data[\"LoanAmount\"]))\n\nprint(\"\\nStandard Deviation:->\\n\")\nprint(\"ApplicantIncome: \",np.std(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.std(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.std(data[\"LoanAmount\"]))\n\nfig = px.histogram(data[\"ApplicantIncome\"],x =\"ApplicantIncome\" ,y = \"ApplicantIncome\" )\nfig.update_layout(title=\"ApplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"CoapplicantIncome\"],x =\"CoapplicantIncome\" ,y = \"CoapplicantIncome\" )\nfig.update_layout(title=\"CoapplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"LoanAmount\"],x =\"LoanAmount\" ,y = \"LoanAmount\" )\nfig.update_layout(title=\"LoanAmount\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graphs found these variables are not normaly distributed.\n\nFoud right-skewed distribution in these three variabels.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nfig = px.bar(data,x=data[\"Gender\"])\nfig.show()\n\nfig = px.bar(data,x=data[\"Married\"])\nfig.show()\n\nfig = px.bar(data,x=data[\"Education\"],color=\"Education\")\nfig.show()\n\nfig = px.bar(data,x=data[\"Self_Employed\"])\nfig.show()\n\nfig = px.bar(data,x=data[\"Dependents\"])\nfig.show()\n\nfig = px.bar(data,x=data[\"Property_Area\"])\nfig.show()\n\nfig = px.bar(data,x=data[\"Loan_Status\"],color=\"Loan_Status\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare data for model training i.e. removing ouliers , filling null values , removing skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"Gender\"].value_counts())\nprint(data[\"Married\"].value_counts())\nprint(data[\"Self_Employed\"].value_counts())\nprint(data[\"Dependents\"].value_counts())\nprint(data[\"Credit_History\"].value_counts())\nprint(data[\"Loan_Amount_Term\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"->Taking mode of values in a column will be best way to fill null values.\n->Not mean because values are not ordinal but are categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling all Nan values with mode of respective variable\ndata[\"Gender\"].fillna(data[\"Gender\"].mode()[0],inplace=True)\ndata[\"Married\"].fillna(data[\"Married\"].mode()[0],inplace=True)\ndata[\"Self_Employed\"].fillna(data[\"Self_Employed\"].mode()[0],inplace=True)\ndata[\"Loan_Amount_Term\"].fillna(data[\"Loan_Amount_Term\"].mode()[0],inplace=True)\ndata[\"Dependents\"].fillna(data[\"Dependents\"].mode()[0],inplace=True)\ndata[\"Credit_History\"].fillna(data[\"Credit_History\"].mode()[0],inplace=True)\n\n#All values of \"Dependents\" columns were of \"str\" form now converting to \"int\" form.\ndata[\"Dependents\"] = data[\"Dependents\"].replace('3+',int(3))\ndata[\"Dependents\"] = data[\"Dependents\"].replace('1',int(1))\ndata[\"Dependents\"] = data[\"Dependents\"].replace('2',int(2))\ndata[\"Dependents\"] = data[\"Dependents\"].replace('0',int(0))\n\ndata[\"LoanAmount\"].fillna(data[\"LoanAmount\"].median(),inplace=True)\n\nprint(data.isnull().sum())\n\n#Heat map for null values\nplt.figure(figsize=(10,6))\nsns.heatmap(data.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Treating outliers and Converting data to Normal Distribution\n#Before removing outlier\n\nprint(\"\\nMean:->\\n\")\nprint(\"ApplicantIncome: \",np.mean(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.mean(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.mean(data[\"LoanAmount\"]))\n\nprint(\"\\nMode:->\\n\")\nprint(\"ApplicantIncome: \",stats.mode(data[\"ApplicantIncome\"])[0])\nprint(\"CoapplicantIncome: \",stats.mode(data[\"CoapplicantIncome\"])[0])\nprint(\"LoanAmount: \",stats.mode(data[\"LoanAmount\"])[0])\n\nprint(\"\\nMedian:->\\n\")\nprint(\"ApplicantIncome: \",np.median(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.median(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.median(data[\"LoanAmount\"]))\n\nprint(\"\\nStandard Deviation:->\\n\")\nprint(\"ApplicantIncome: \",np.std(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.std(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.std(data[\"LoanAmount\"]))\n\nfig = px.histogram(data[\"ApplicantIncome\"],x =\"ApplicantIncome\" ,y = \"ApplicantIncome\" )\nfig.update_layout(title=\"ApplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"CoapplicantIncome\"],x =\"CoapplicantIncome\" ,y = \"CoapplicantIncome\" )\nfig.update_layout(title=\"CoapplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"LoanAmount\"],x =\"LoanAmount\" ,y = \"LoanAmount\" )\nfig.update_layout(title=\"LoanAmount\")\nfig.show()\n\n####################################################################################################\n#Getting log value :->\n\ndata[\"ApplicantIncome\"] = np.log(data[\"ApplicantIncome\"])\n#As \"CoapplicantIncome\" columns has some \"0\" values we will get log values except \"0\"\ndata[\"CoapplicantIncome\"] = [np.log(i) if i!=0 else 0 for i in data[\"CoapplicantIncome\"]]\ndata[\"LoanAmount\"] = np.log(data[\"LoanAmount\"])\n####################################################################################################\n\nprint(\"---------------------------After converting to Normal Distributed data----------------------\")\n\nprint(\"\\nMean:->\\n\")\nprint(\"ApplicantIncome: \",np.mean(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.mean(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.mean(data[\"LoanAmount\"]))\n\nprint(\"\\nMode:->\\n\")\nprint(\"ApplicantIncome: \",stats.mode(data[\"ApplicantIncome\"])[0])\nprint(\"CoapplicantIncome: \",stats.mode(data[\"CoapplicantIncome\"])[0])\nprint(\"LoanAmount: \",stats.mode(data[\"LoanAmount\"])[0])\n\nprint(\"\\nMedian:->\\n\")\nprint(\"ApplicantIncome: \",np.median(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.median(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.median(data[\"LoanAmount\"]))\n\nprint(\"\\nStandard Deviation:->\\n\")\nprint(\"ApplicantIncome: \",np.std(data[\"ApplicantIncome\"]))\nprint(\"CoapplicantIncome: \",np.std(data[\"CoapplicantIncome\"]))\nprint(\"LoanAmount: \",np.std(data[\"LoanAmount\"]))\n\nplt.figure(figsize=(10,4))\nfig = px.histogram(data[\"ApplicantIncome\"],x =\"ApplicantIncome\" ,y = \"ApplicantIncome\" )\nfig.update_layout(title=\"ApplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"CoapplicantIncome\"],x =\"CoapplicantIncome\" ,y = \"CoapplicantIncome\" )\nfig.update_layout(title=\"CoapplicantIncome\")\nfig.show()\n\nfig = px.histogram(data[\"LoanAmount\"],x =\"LoanAmount\" ,y = \"LoanAmount\" )\nfig.update_layout(title=\"LoanAmount\")\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that Bell Curve for all three variables and data is normally distributed now."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Gender\"] = le.fit_transform(data[\"Gender\"])\ndata[\"Married\"] = le.fit_transform(data[\"Married\"])\ndata[\"Education\"] = le.fit_transform(data[\"Education\"])\ndata[\"Self_Employed\"] = le.fit_transform(data[\"Self_Employed\"])\ndata[\"Property_Area\"] = le.fit_transform(data[\"Property_Area\"])\ndata[\"Loan_Status\"] = le.fit_transform(data[\"Loan_Status\"])\n\n#data = pd.get_dummies(data)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance\n\nIn order to create best predictive model we need to best understand the available data and \nget most information from the data.\n\nIn multivariate data it is important to understand the iortance of varialbes and \nhow much they are contributing towards the target variable. Such that we can remove unnecessary variables to increase\nmodel performance.\n\nMany times dataset consists of exta columns which do not identically serve information to classify the data.\nThis leads in Wrong Assumption of model while training.\n\nTo understand the importance of the data we are going to use Machine Learning classifiers and \nthen will plot bar graph based on importance.\n\nAlso XGBoost has built-in Feature Importance Plotting tool which we are going to use.\n\nUsing more than one classifier will increase the confidence on our assumption of which variables to keep\nand which to remove."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing data into Input X variables and Target Y variable\nX = data.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = data[\"Loan_Status\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature importance by XGBoost:->\\n\")\nXGBR = XGBClassifier()\nXGBR.fit(X,y)\nfeatures = XGBR.feature_importances_\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.title(label=\"XGBC\")\nplt.bar([x for x in range(len(features))],features)\nplt.show()\n\nplot_importance(XGBR)\n\nprint(\"Feature importance by Random Forest:->\\n\")\nRF = RandomForestClassifier()\nRF.fit(X,y)\nfeatures = RF.feature_importances_\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.title(label=\"RF\")\nplt.bar([x for x in range(len(features))],features)\nplt.show()\n\nprint(\"Feature importance by Decision Tree:->\\n\")\nDT = DecisionTreeClassifier()\nDT.fit(X,y)\nfeatures = DT.feature_importances_\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.title(label=\"DT\")\nplt.bar([x for x in range(len(features))],features)\nplt.show()\n\nprint(\"Feature importance by Suppoprt Vector Machine:->\\n\")\nSVM = SVC(kernel=\"linear\")\nSVM.fit(X,y)\nfeatures = SVM.coef_[0]\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.bar([x for x in range(len(features))],features)\nplt.show()\n\nprint(\"Feature importance by Logistic Regression:->\\n\")\nLOGC = LogisticRegression()\nLOGC.fit(X,y)\nfeatures = LOGC.coef_[0]\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.title(label=\"LOGC\")\nplt.bar([x for x in range(len(features))],features)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From feature importance => Credit History , ApplicantIncome , CoapplicantIncome, LoanAmount are the most important features"},{"metadata":{},"cell_type":"markdown","source":"# Is data Balanced ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heat map of dataset with relative importance\nmatrix = data.drop([\"Gender\",\"Married\",\"Dependents\",\"Education\",\"Self_Employed\"],axis=1).corr()\n#f , ax = plt.subplots(figsize=(18,6))\nplt.figure(figsize=(18,8))\nsns.heatmap(matrix,vmax=0.8,square=True,cmap=\"BuPu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems Application income and Loan Amount is correlated , also Coapplication income correlated with Loan Aount then \nCredit history is corrleated with Loan Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"A = list(data.Loan_Status).count(1)\nB = list(data.Loan_Status).count(0)\nprint(\"Count of 1<Approved>: \",A,\"\\nCount of 0<Rejected>: \",B)\n\nfig = px.bar((A,B),x=[\"Approved\",\"Rejected\"],y=[A,B],color=[A,B])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that data is highly Imbalanced.\n\nWhen the target classes does not have equal count then the data is considered as imbalanced data.\n\nFrom above graph it seems that dataset contains more records with Approved Loan_Status than Rejected Loan_Status.\n422 over 192\n\nIf data would have maximum of 20-30 records difference that time this imabalnced would be ignorable.\n\nWhich will lead to make wrong assumptions by model and also model will be biased after training.\nWe will overcome this issue by balancing the data.\n\nTo overcome this problem we will balance the data using Resampling technique with Upsample and Downsample."},{"metadata":{"trusted":true},"cell_type":"code","source":"#To keep original data as it is to use the same for later.\nnew_data = data.copy()\n\n#Getting seperated data with 1 and 0 status.\ndf_majority = new_data[new_data.Loan_Status==1]\ndf_minority = new_data[new_data.Loan_Status==0]\n\n#Here we are downsampling the Majority Class Data Points. \n#i.e. We will get equal amount of datapoint as Minority class from Majority class\n\ndf_manjority_downsampled = resample(df_majority,replace=False,n_samples=192,random_state=123)\ndf_downsampled = pd.concat([df_manjority_downsampled,df_minority])\nprint(\"Downsampled data:->\\n\",df_downsampled.Loan_Status.value_counts())\n\n#Here we are upsampling the Minority Class Data Points. \n#i.e. We will get equal amount of datapoint as Majority class from Minority class\ndf_monority_upsampled = resample(df_minority,replace=True,n_samples=422,random_state=123)\ndf_upsampled = pd.concat([df_majority,df_monority_upsampled])\nprint(\"Upsampled data:->\\n\",df_upsampled.Loan_Status.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Standardization / Normalization\n\nData normalization is required when the vriable values are in very distinct range.\n\nFor Ex. Suppose we have 2 columns \"Age\" and \"Income\"\n\nWhere value range of \"Age\" lying in 0-100 Approx.\nand value range of \"Income\" lying in 20,000 to 100,000\n\nAt this time model will perform poorly on testig data as all input values are not in same value range.\n\nSo not every time but whenever we get such type of data we need to normalized it i.e. Rescale it.\n\nWidely used scaling tools are Min-Max Scaler and Standard-Scaler\n\nData Normalization is done by Min-Max Scaler which  scales all th values between 0 to 1 range.\n\nData standardization is done by Standard-Scaler which scales the data so that Mean of observed data is 0 and Standard Deviation is 1.\n\nAs our data is not much normally distributed we will choose Standardization using Standard-Scaler aiming that it will reduce\nmore skewness and contribute in accuracy gain."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experimental Modeling\n\nIn order to gain maximum posible accuracy one needs to conduct much emor experiments.\n\nWe will pass data on by one with different state i.e. \n\n-Only Scaled data\n\n-Scaled + Down Sampled Data\n\n-Scaled + Up Sampled Data\n\n-Scaled + Up Sampled Data + Selected feature with respective importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 1: Only Scaled data with all variables\n\n#X = new_data.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",'Property_Area'],axis=1)\nX = new_data.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = new_data[\"Loan_Status\"]\ncounter = Counter(y)\nprint(\"Counter: \",counter)\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n#Scaling data here:------------->\n\nStSc = StandardScaler()\nX_train  = StSc.fit_transform(X_train)\nX_test  = StSc.fit_transform(X_test)\n\n#Check mean is 0 and Standard deviation is 1\nprint(\"After Standardization\\nMean \",np.mean(X_train),\"Standard Deviation \",np.std(X_train),\"\\n\")\n\n#Voting ensemble mathod. Combining all tree based algorithms.\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\nModel1 = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nModel1.fit(X_train,y_train)\ny_pred = Model1.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier()\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test),\"\\n\")\n\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 2: Sclaed + Down Sampled Data\n\n#X = df_downsampled.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",'Property_Area'],axis=1)\nX = df_downsampled.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = df_downsampled.Loan_Status\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n\n#Scaling data here:------------->\n\nStSc = StandardScaler()\nX_train  = StSc.fit_transform(X_train)\nX_test  = StSc.fit_transform(X_test)\n\n#Check mean is 0 and Standard deviation is 1\nprint(\"After Standardization\\nMean \",np.mean(X_train),\"Standard Deviation \",np.std(X_train),\"\\n\")\n\n#Voting ensemble mathod. Combining all tree based algorithms.\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\nModel1 = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nModel1.fit(X_train,y_train)\ny_pred = Model1.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier()\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test))\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Experiment 3: Sclaed + Up Sampled Data\n\n#X = df_upsampled.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",'Property_Area'],axis=1)\nX = df_upsampled.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = df_upsampled.Loan_Status\nprint(len(X),len(y))\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n#Scaling data here:------------->\n\nStSc = StandardScaler()\nX_train  = StSc.fit_transform(X_train)\nX_test  = StSc.fit_transform(X_test)\n\n#Check mean is 0 and Standard deviation is 1\nprint(\"After Standardization\\nMean \",np.mean(X_train),\"Standard Deviation \",np.std(X_train),\"\\n\")\n\n#Voting ensemble mathod. Combining all tree based algorithms.\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\nModel1 = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nModel1.fit(X_train,y_train)\ny_pred = Model1.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier()\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test))\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experiment 4: Sclaed + Selected features with respective importance\n#Droping features which are less important and keeping features as per importance analysis.\nX = new_data.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",\"Property_Area\"],axis=1)\n#X = new_data.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = new_data.Loan_Status\nprint(len(X),len(y))\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n#Scaling data here:------------->\n\nStSc = StandardScaler()\nX_train  = StSc.fit_transform(X_train)\nX_test  = StSc.fit_transform(X_test)\n\n#Check mean is 0 and Standard deviation is 1\nprint(\"After Standardization\\nMean \",np.mean(X_train),\"Standard Deviation \",np.std(X_train),\"\\n\")\n\n#Voting ensemble mathod. Combining all tree based algorithms.\nmodels = []\nmodels.append((\"XGB\",XGBClassifier()))\nmodels.append((\"RF\",RandomForestClassifier()))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"ADB\",AdaBoostClassifier()))\nmodels.append((\"GB\",GradientBoostingClassifier()))\n\nensemble = VotingClassifier(estimators=models)\nensemble.fit(X_train,y_train)\ny_pred = ensemble.predict(X_test) \nprint(classification_report(y_pred,y_test))\nprint(\"Voting Ensemble:>\",accuracy_score(y_pred,y_test))\n\n\n\nSVM = SVC(kernel=\"linear\",class_weight=\"balanced\",probability=True)\nSVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"SVM:>\",accuracy_score(y_pred,y_test))\n\n\nXGBC = XGBClassifier(learning_rate =0.1,n_estimators=10000,max_depth=4,min_child_weight=6,gamma=0,subsample=0.6,colsample_bytree=0.8,\n reg_alpha=0.005, objective= 'binary:logistic', nthread=2, scale_pos_weight=1, seed=27)\nXGBC.fit(X_train,y_train)\ny_pred = XGBC.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"XGBoost:>\",accuracy_score(y_pred,y_test))\n\nModel1 = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=1000,max_depth=70,bootstrap=True)\nModel1.fit(X_train,y_train)\ny_pred = Model1.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"RandomForestClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel2 = GradientBoostingClassifier(random_state=0)\nModel2.fit(X_train,y_train)\ny_pred = Model2.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GradientBoostingClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n max_features=1.0, max_leaf_nodes=10,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.10,\n presort=False, random_state=27, splitter='best')\nModel3.fit(X_train,y_train)\ny_pred = Model3.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"DecisionTreeClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel4 = AdaBoostClassifier()\nModel4.fit(X_train,y_train)\ny_pred = Model4.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"AdaBoostClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel5 = LinearDiscriminantAnalysis()\nModel5.fit(X_train,y_train)\ny_pred = Model5.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"LinearDiscriminantAnalysis:>\",accuracy_score(y_pred,y_test))\n\nKNN = KNeighborsClassifier(leaf_size=1,p=2,n_neighbors=20)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))\n\n\nModel7 = GaussianNB()\nModel7.fit(X_train,y_train)\ny_pred = Model7.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"GaussianNB:>\",accuracy_score(y_pred,y_test))\n\n\nModel8 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\nModel8.fit(X_train,y_train)\ny_pred = Model8.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"Logistic Regression:>\",accuracy_score(y_pred,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Hyperparameters tuning for KNN\n\n#X = new_data.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",\"Property_Area\"],axis=1)\nX = new_data.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = new_data.Loan_Status\nprint(len(X),len(y))\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n\n\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, cv=10)\n#Fit the model\nbest_model = clf.fit(X_train,y_train)\n#Print The value of best Hyperparameters\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n\nLS = best_model.best_estimator_.get_params()['leaf_size']\nP = best_model.best_estimator_.get_params()['p']\nNum = best_model.best_estimator_.get_params()['n_neighbors']\n\nKNN = KNeighborsClassifier(leaf_size=LS,p=P,n_neighbors=Num)\nKNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(\"KNeighborsClassifier:>\",accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning SVM parameters\n\n#X = new_data.drop([\"Loan_ID\",\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Loan_Amount_Term\",\"Loan_Status\",\"Property_Area\"],axis=1)\nX = new_data.drop([\"Loan_Status\",\"Loan_ID\"],axis=1)\ny = new_data.Loan_Status\nprint(len(X),len(y))\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n\n\nmodel = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion ðŸš€ðŸ’¹ðŸŽ“\nResut Summary is as below:----->\nAlgorithm : Accuracy\n\nExperiment 1 : Scaled data only\n\n    Support Vector Machine       83.116\n    Decision Tree                83.1168\n    Linear Discriminant Analysis 83.166\n    KNearest Neighbors           83.766\n    Gaussian Naivey Bayes        83.116\n    Logistic Regression          83.116\n\nExperiment 2: Sclaed + Down Sampled Data\n\n    AdaBoost         73.95\n    Decision Tree    72.91\n    Voting Ensemble  71.87\n\n\nExperiment 3: Sclaed + Up Sampled Data\n\n    Random Forest only 83.88\n\nExperiment 4: Sclaed + Selected features with respective importance\n\n    Support Vector Machine       83.11\n    Decision Tree                83.11\n    AdaBoost                     82.46\n    Linear Discriminant Analysis 83.11\n    KNearest Neighbors           83.11\n    Gaussian Naivey Bayes        83.11\n    Logistic Regression          83.11\n\nAlso after parameter tuning with\n\n    KNN 83.11\n\nAfter all possible experiments Maximum accuracy achieved By making data balanced as Up Sampling. Surprisingly only\nRandom forest performed well in that state of the data. \n\nSurprisingly feature selection doesn't make increase in accuracy.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}