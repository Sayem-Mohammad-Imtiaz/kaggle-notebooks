{"cells":[{"metadata":{"id":"P2K2aSeBLeFW","colab_type":"text"},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"id":"1WU0qA_eLeFe","colab_type":"text"},"cell_type":"markdown","source":"Can we predict whether it is going to rain tomorrow from weather data obtained today? This dataset contains daily weather observations from numerous Australian weather stations. The target variable RainTomorrow means: Did it rain the next day? Yes or No.\n\nWe are going to train diffrent binary classification algorithms from the supervised dataset provided and determine how accurate we can predict if it is going to rain tomorrow given today's weather conditions. Let's begin."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"id":"vzp9Yx0RLeFf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e1e613e8-36c6-4fe2-b696-e428f619d9dc"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for plotting\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/weatherAUS.csv')\nprint('Dataset dimensions: ', df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"KXRjcEWSLeFk","colab_type":"text"},"cell_type":"markdown","source":"## Data Fields\n\n* Date: The date of observation\n* Location: The common name of the location of the weather station\n* MinTemp: The minimum temperature in degrees celsius\n* MaxTemp: The maximum temperature in degrees celsius\n* Rainfall: The amount of rainfall recorded for the day in mm\n* Evaporation: The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n* Sunshine: The number of hours of bright sunshine in the day.\n* WindGustDir: The direction of the strongest wind gust in the 24 hours to midnight\n* WindGustSpeed: The speed (km/h) of the strongest wind gust in the 24 hours to midnight\n* WindDir9am: Direction of the wind at 9am\n* WindDir3pm: Direction of the wind at 3pm\n* WindSpeed9am: Wind speed (km/hr) averaged over 10 minutes prior to 9am\n* WindSpeed3pm: Wind speed (km/hr) averaged over 10 minutes prior to 3pm\n* Humidity9am: Humidity (percent) at 9am\n* Humidity3pm: Humidity (percent) at 3pm\n* Pressure9am: Atmospheric pressure (hpa) reduced to mean sea level at 9am\n* Pressure3pm: Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n* Cloud9am: Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n* Cloud3pm: Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n* Temp9am: Temperature (degrees C) at 9am\n* Temp3pm: Temperature (degrees C) at 3pm\n* RainToday: Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n* RISK_MM: The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\".\n* RainTomorrow: The target variable. Did it rain tomorrow?"},{"metadata":{"id":"GSidVyBiLeFm","colab_type":"text"},"cell_type":"markdown","source":"# Explore the Data"},{"metadata":{"trusted":true,"id":"-mfpxa4sLeFn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":510},"outputId":"098978e6-c1f9-4fff-aec9-8931ebe71a96"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JjXPs2YQLeFs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"9b15e071-fdab-4ad4-e58e-e864587578c6"},"cell_type":"code","source":"#check the counts for each column to check if the dataset is complete\ndf.count().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"id":"bxuRa3OdLeFw","colab_type":"text"},"cell_type":"markdown","source":"From above we can see that Sunshine, Evaporation, Cloud3pm, and Cloud9am columns have less than 60% of the rows populated. So let's drop these columns. Also drop Risk_MM as this indicates  the amount of rainfall in millimeters for the next day. This value is used to determine the target variable \"RainTomorrow\". So it should be ignored here as this would give the model a false accuracy. \nWe will fill the missing values of columns that we didn't drop later"},{"metadata":{"trusted":true,"id":"dQPvv6wjLeFx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b08291ac-0cfe-492f-f907-da7f69e7843b"},"cell_type":"code","source":"print('Prior to dropping the columns :',df.shape)\ndf = df.drop(columns=['Sunshine','Evaporation','Cloud3pm','Cloud9am','Location','RISK_MM','Date', 'RISK_MM'],axis=1)\nprint('After dropping the columns :',df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sFAS77eHLeFz","colab_type":"code","colab":{}},"cell_type":"code","source":"# split the train and test sets\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(df, test_size = 0.2, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Nf2qqKNDLeF1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"6a25c30f-4215-4475-8888-0eb6e7bd67d0"},"cell_type":"code","source":"# Explore the train_set\nrain = train_set.copy()\nrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"YgLPuiUHLeF3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"outputId":"60d4d29b-5e9a-4abf-cbba-4f0a19a4c875"},"cell_type":"code","source":"rain.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"PbquJ_sILeF6","colab_type":"text"},"cell_type":"markdown","source":"All the numerical falls within reasonable ranges Therefore no need to alter any numerical values. Now let's investigate categorical data for any abnormlaities."},{"metadata":{"trusted":true,"id":"OVZxadQ5LeF6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"90367e9b-6680-4595-c075-1d1fb15c87ac"},"cell_type":"code","source":"rain['WindGustDir'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"51gcmTQDLeF9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"d1a9ef73-0f3d-4ca3-98c3-79e5047eb1db"},"cell_type":"code","source":"rain['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"SAnzeEk0LeF_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"c4d75928-3b35-497b-c8eb-eecde001c874"},"cell_type":"code","source":"rain['WindDir3pm'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WT51Te1SLeGE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"26380518-ad97-4551-e822-6436c148909e"},"cell_type":"code","source":"rain['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"UMmK2SZoLeGI","colab_type":"text"},"cell_type":"markdown","source":"There aren't any abnormalities or unnecessary categories in the categorical columns. So no need to alter any categorical varaibles.\nLet's now investigate the null values in each column. We need to remove or change these null values before training models."},{"metadata":{"trusted":true,"id":"XwFqcAtmLeGJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"5153fddb-9e27-4f26-89cd-477066433dc3"},"cell_type":"code","source":" rain.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"uCYt-pUpLeGL","colab_type":"text"},"cell_type":"markdown","source":"It seems that all the columns have null values. Since we don't want to drop lot of data points we need to replace these null values with appropriate values."},{"metadata":{"trusted":true,"id":"MpzvPlDwLeGM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":741},"outputId":"55015cae-a0b8-4757-adfe-d09149877cdc"},"cell_type":"code","source":"# Correlation Matrix\nimport scipy.stats as ss\nimport seaborn as sns\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\nheaders = list(rain)\nprint(headers)\ncoeff_matrix = []\nfor header in headers:\n    coeff_list = []\n    for item in headers:\n        coeff = cramers_v(rain[header], rain[item])\n        coeff_list.append(coeff)\n    coeff_matrix.append(coeff_list)\n    \nnp_arr = np.array(coeff_matrix)\nplt.figure(figsize=(20,12))\nax = sns.heatmap(np_arr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"04YBIpsaLeGO","colab_type":"text"},"cell_type":"markdown","source":"From the correlation matrix above RainToday seem to have a very high correlation with RainFall, which make sense as RainFall provides the amount of rain we got today. Also RainToday has a high correlation with many other columns in this data set. So let's drop RainToday column."},{"metadata":{"trusted":true,"id":"4f21FG00LeGP","colab_type":"code","colab":{}},"cell_type":"code","source":"train_set = train_set.drop(\"RainToday\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Qi5sbossLeGQ","colab_type":"text"},"cell_type":"markdown","source":"# Prepare the Data"},{"metadata":{"trusted":true,"id":"AjREkSzyLeGS","colab_type":"code","colab":{}},"cell_type":"code","source":"#Alternative to DataframeSelector. Can be replaced later\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sQ3aiCCoLeGT","colab_type":"code","colab":{}},"cell_type":"code","source":"# Create the feature set and target for training\nX = train_set.drop(\"RainTomorrow\", axis = 1)\ny = train_set[\"RainTomorrow\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"3JxIV2kDLeGV","colab_type":"raw"},"cell_type":"markdown","source":"First let's generate our categorical variable pipeline. For categorical variables we decieded to change the null values to most frequent values in the column. We also used OneHotEncoder to encode categorical data."},{"metadata":{"trusted":true,"id":"W451s2cgLeGV","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ncat_pipeline = Pipeline([\n        (\"select_cat\", DataFrameSelector([\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"])),\n        (\"imp\", SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"HWz09XU9LeGX","colab_type":"raw"},"cell_type":"markdown","source":"For numerical variables we decieded to change the null values to the mean in the column. We also used StandardScaler to normalize numerical values"},{"metadata":{"trusted":true,"id":"8GnGgpcqLeGX","colab_type":"code","colab":{}},"cell_type":"code","source":"num_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\",\n                                              \"Pressure9am\", \"Pressure3pm\", \"Temp9am\", \"Temp3pm\"])),\n        (\"imp\", SimpleImputer(missing_values=np.nan, strategy='mean')),\n        ('scaler', StandardScaler()),\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"j6fpniwVLeGZ","colab_type":"code","colab":{}},"cell_type":"code","source":"preprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kEzGYmoLLeGa","colab_type":"code","colab":{}},"cell_type":"code","source":"X_train_prepared = preprocess_pipeline.fit_transform(X)\ny_train_prepared = y.map({'Yes':1, 'No':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"21MyHvPILeGb","colab_type":"code","colab":{}},"cell_type":"code","source":"#Test set\nX_test = test_set.drop(\"RainTomorrow\", axis = 1)\ny_test = test_set[\"RainTomorrow\"].copy()\n\nX_test_prepared = preprocess_pipeline.fit_transform(X_test)\ny_test_prepared = y_test.map({'Yes':1, 'No':0})\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"884hBDZHLeGd","colab_type":"text"},"cell_type":"markdown","source":"# SHORT-LIST PROMISING MODELS and FINE-TUNE THE SYSTEM"},{"metadata":{"trusted":true,"id":"kcDmAbdrLeGe","colab_type":"code","colab":{}},"cell_type":"code","source":"#Plt ROC Curve\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, classification_report, roc_auc_score\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZZm-ynTHOd34","colab_type":"text"},"cell_type":"markdown","source":"We are choosing roc_auc or area under the ROC curve as the metric to measure the performance of each model. AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting roc_auc is as the probability that the model ranks a random positive example more highly than a random negative example. This metric is chosen because we want the model to identify as mamy possible positive cases  (rain tomorrow) as possible. Positive case here being it will rain tomorrow and negative case being it will not rain tomorrow.\n\nroc_auc is desirable for the following two reasons:\n\n*   roc_auc is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n*   roc_auc is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."},{"metadata":{"id":"B1omCwWlLeGf","colab_type":"text"},"cell_type":"markdown","source":"### LogisticRegression"},{"metadata":{"id":"6CDSBxJ8LeGg","colab_type":"text"},"cell_type":"markdown","source":"The first classifier to test is Logistic Regression. Logistic regression is appropriate to conduct regression analysis when the dependent variable is dichotomous (binary). This dataset is not very large and we are testing for both l1 and l2 penalties. Therefore ‘liblinear’ is used as the solver. We are tuning the parameters C and penalty of the LogisticRegression classifier.\n* C: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n* penalty: Used to specify the norm used in the penalization.(‘l1’ or ‘l2’)\n\nModel tuning is commented out as it takes long time to run. Uncomment and run if tuning is necessary "},{"metadata":{"trusted":true,"id":"uheGJCNwLeGh","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# #tuning\n# param_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"] }\n\n# #training\n# log = LogisticRegression(solver = 'liblinear')\n# logreg_cv=GridSearchCV(log,param_grid,cv=5,scoring= 'roc_auc')\n# logreg_cv.fit(X_train_prepared,y_train_prepared)\n# print(logreg_cv.best_params_)\n# print(logreg_cv.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4S7FSJxWLeGi","colab_type":"text"},"cell_type":"markdown","source":"The optimal parameters obtained from tuning are {'C': 1.0, 'penalty': 'l1'} with a roc_auc value of 0.85495.\n\nWe train the LogisticRegression regression model with given parameters and the training set then test with the test set."},{"metadata":{"trusted":true,"id":"EsHDQps3LeGi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"63107f17-828f-461a-90d9-7afc911d2a93"},"cell_type":"code","source":"#training\nlog_best_model = LogisticRegression(C = 1.0, penalty = 'l1', solver = 'liblinear')\nlog_best_model.fit(X_train_prepared,y_train_prepared)\n\n#testing\ny_pred_prob = log_best_model.predict_proba(X_test_prepared)[:,1]\nfpr_log, tpr_log, thresholds = roc_curve(y_test_prepared, y_pred_prob, pos_label= 1)\nprint(\"Roc_auc_score {}\".format(roc_auc_score(y_test_prepared, y_pred_prob)))\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_log, tpr_log, \"LogisticRegression\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"a4at-oTqLeGk","colab_type":"text"},"cell_type":"markdown","source":"LogisticRegression classifier gives a roc_auc (area under the curve) value of 0.85575."},{"metadata":{"id":"PmPA3UhbLeGl","colab_type":"text"},"cell_type":"markdown","source":"### RandomForestClassifier"},{"metadata":{"id":"oGrrPtqiLeGl","colab_type":"text"},"cell_type":"markdown","source":"A random forest is a classifier that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement when bootstrap=True (default). We are tuning the parameters max_depth and n_estimators of the RandomForestClassifier classifier.\n\n* max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n* n_estimators: The number of trees in the forest."},{"metadata":{"trusted":true,"id":"ZhSGRF78LeGl","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# #tuning\n# param_grid = {\n#     'bootstrap': [True],\n#     'max_depth': [50, 60 ,70 ,80, 90],\n#     'n_estimators': [20, 50, 100, 150, 200]\n# }\n\n# rf = RandomForestClassifier(random_state=123)\n# rf_cv=GridSearchCV(rf,param_grid,cv=5,scoring= 'roc_auc')\n# rf_cv.fit(X_train_prepared,y_train_prepared)\n# print(rf_cv.best_params_)\n# print(rf_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"id":"yt2MRHWZLeGn","colab_type":"text"},"cell_type":"markdown","source":"The optimal parameters obtained from tuning are {'bootstrap': True, 'max_depth': 60, 'n_estimators': 300} with a roc_auc value of 0.87432.\n\nWe train the RandomForestClassifier classifier with given parameters and the training set then test with the test set."},{"metadata":{"trusted":true,"id":"RNZYgqfQLeGo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"1ca619af-d0f9-4e8c-c181-3da25a014bec"},"cell_type":"code","source":"#training\nrf_best_model = RandomForestClassifier(bootstrap = True, max_depth = 60, n_estimators = 300, random_state=123)\nrf_best_model.fit(X_train_prepared,y_train_prepared)\n\n#testing\ny_pred_prob = rf_best_model.predict_proba(X_test_prepared)[:,1]\nfpr_rf, tpr_rf, thresholds = roc_curve(y_test_prepared, y_pred_prob, pos_label= 1)\nprint(\"Roc_auc_score {}\".format(roc_auc_score(y_test_prepared, y_pred_prob)))\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_rf, tpr_rf, \"Random Forest\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"gMyExfRFLeGq","colab_type":"text"},"cell_type":"markdown","source":"RandomForestClassifier classifier gives a roc_auc value of 0.87900. RandomForestClassifier gives a higher roc_auc than LogisticRegression. \n\nIn addition to classifying RandomForestClassifier can also be used to determine feature importance. Here we plot the significance of each feature when classifying the given features set into positive and negative classes."},{"metadata":{"trusted":true,"id":"xAI-B-ZwLeGq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":412},"outputId":"32d5e7c6-3b03-43dc-de1e-7505a6a35783"},"cell_type":"code","source":"feature_importances = rf_best_model.feature_importances_\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X.columns, feature_importances):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZND7PykMLeGs","colab_type":"text"},"cell_type":"markdown","source":"### LGBMClassifier"},{"metadata":{"id":"n-onlWScLeGt","colab_type":"text"},"cell_type":"markdown","source":"Light GBM is a gradient boosting framework that uses tree based learning algorithm. It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. Parameters assigned are,\n\n* objective: Specifies the application of your model, whether it is a regression problem or classification problem. This is binary classification problem so 'binary' is assigned.\n* metric: Specifies loss for model building. 'binary_logloss' is appropriate for loss for binary classification.\n* boosting: Defines the type of algorithm you want to run. 'dart' is used for better accuracy.\n\nParameters tuned are,\n\n* min_data_in_leaf: Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n* max_depth: The maximum depth of the tree.\n* learning_rate: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. \n\n\n\n"},{"metadata":{"trusted":true,"id":"thA8fkbGLeGt","colab_type":"code","colab":{}},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n# #tuning\n# param_grid = {\n#     \"min_data_in_leaf\":[50,100,200, 300, 400],\n#     \"max_depth\":[8, 10, 20, 50],\n#     \"learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.4]\n# }\n\n# lgbm = LGBMClassifier(application = 'binary', metric = 'binary_logloss', boosting = 'dart')\n# lgbm_cv=GridSearchCV(lgbm, param_grid,cv=5,scoring= 'roc_auc')\n# lgbm_cv.fit(X_train_prepared,y_train_prepared)\n# print(lgbm_cv.best_params_)\n# print(lgbm_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"id":"9SjOc0ZELeGu","colab_type":"text"},"cell_type":"markdown","source":"The optimal parameters obtained from tuning are {'learning_rate': 0.4, 'max_depth': 10, 'min_data_in_leaf': 300} with a roc_auc value of 0.87731.\n\nWe train the LGBMClassifier classifier with given parameters and the training set then test with the test set."},{"metadata":{"trusted":true,"id":"nAVGmuK3LeGv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"7c1754b3-90af-49bc-90ac-2ba95d455870"},"cell_type":"code","source":"#training\nlgbm_best_model = LGBMClassifier(application = 'binary', metric = 'binary_logloss', boosting = 'dart', min_data_in_leaf = 300, max_depth = 10, learning_rate = 0.4)\nlgbm_best_model.fit(X_train_prepared,y_train_prepared)\n\n#testing\n\ny_pred_prob = lgbm_best_model.predict_proba(X_test_prepared)[:,1]\nfpr_lgbm, tpr_lgbm, thresholds = roc_curve(y_test_prepared, y_pred_prob, pos_label= 1)\nprint(\"Roc_auc_score {}\".format(roc_auc_score(y_test_prepared, y_pred_prob)))\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_lgbm, tpr_lgbm, \"LGBMClassifier\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4JvaCYHOLeGw","colab_type":"text"},"cell_type":"markdown","source":"LGBMClassifier classifier gives a roc_auc value of 0.87829. This is slightly less than RandomForestClassifier. However, LGBMClassifier runs faster than RandomForestClassifier."},{"metadata":{"id":"STsI6vLKLeGx","colab_type":"text"},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{"id":"tn341bqbLeGy","colab_type":"text"},"cell_type":"markdown","source":"Lastly we try a simple Neural network model to predict if it is going to rain tomorrow. Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. We are using a Sequential Neural Netwrok with nodes and layers. The optimal Neural Network is obtained by chnaging the number of nodes and layers in the model. Then these optimal number of nodes and layers are used to train the Neural network with training data.\n\nFirst we need to figure out what optimizer to use. Our choices are,\n\n* adam optimizer\n* Stochastic gradient descent optimizer with different learning rates."},{"metadata":{"trusted":true,"id":"Ekjb5-VQLeGy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":714},"outputId":"3645b81d-9538-4dae-d3be-96c9d6e6a695"},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\n\nn_cols =  X_train_prepared.shape[1]\ntarget =  to_categorical(y_train_prepared)\n\ndef get_new_model():\n    model = Sequential()\n    model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return model\n\nlr_to_test = [.000001, 0.01, 0.1, 0.2, 0.3]\n\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    # Build new model to test, unaffected by previous models\n    model = get_new_model()\n    # Create SGD optimizer with specified learning rate: my_optimizer\n    my_optimizer = SGD(lr=lr) \n    # Compile the model\n    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy') \n    #model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['binary_accuracy'])\n    model.fit(X_train_prepared, target)\n\n\n# With adam optimizer\nprint(\"Testing model with adam optimizer\")\nmodel = get_new_model()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\nmodel.fit(X_train_prepared, target)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZW2zcpq2LeG0","colab_type":"text"},"cell_type":"markdown","source":"adam optimizer gives the lowest loss. Therefore we are using the 'adam' optimizer in our deep learning model. Now we need to train the model and validate it. We will increase the number of nodes and layers to get the best posiible validation score possible."},{"metadata":{"trusted":true,"id":"rEC-he1vLeG1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"ca9fffb4-3195-42c9-9811-2bf47a1ecf88"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nearly_stopping_monitor = EarlyStopping(patience=2) \n\n# Without adding any nodes or layers\nmodel = get_new_model()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_prepared, target, validation_split=0.3, epochs=20, callbacks = [early_stopping_monitor])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"VEiI56oiLeG8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"d8b5f224-20a8-478a-a347-c6890619f93d"},"cell_type":"code","source":"# Increasing the number of nodes\nmodel = Sequential()\nmodel.add(Dense(120, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_prepared, target, validation_split=0.3, epochs=20, callbacks = [early_stopping_monitor])","execution_count":null,"outputs":[]},{"metadata":{"id":"9-zzAPrCLeG_","colab_type":"text"},"cell_type":"markdown","source":"Increasing the number of nodes to 120 decreased the loss of the model. Therefore we are going to use 120 as the number of nodes. Next we are going to increase the number of layers."},{"metadata":{"trusted":true,"id":"xwS-RCtWLeG_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"24f4a156-88be-4968-c2cb-58a8f701f970"},"cell_type":"code","source":"# Increasing number of layers\nmodel = Sequential()\nmodel.add(Dense(120, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_prepared, target, validation_split=0.3, epochs=20, callbacks = [early_stopping_monitor])","execution_count":null,"outputs":[]},{"metadata":{"id":"I_nJz-I6iLMq","colab_type":"text"},"cell_type":"markdown","source":"Increasing the number of layers did not decrease the loss of the model. Therefor we are going to use the same number of layers as the base model. Now let's train and test our Neural Network."},{"metadata":{"trusted":true,"id":"c9FxFQOBLeHD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":635},"outputId":"26b1d29c-2b92-4bd3-ba10-2be3dd24e96d"},"cell_type":"code","source":"#training\nmodel = Sequential()\nmodel.add(Dense(120, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_prepared, target, validation_split=0.3, epochs=20, callbacks = [early_stopping_monitor])\n\n#testing\ny_pred_prob = model.predict_proba(X_test_prepared)[:,1]\nfpr_nn, tpr_nn, thresholds = roc_curve(y_test_prepared, y_pred_prob, pos_label= 1)\nprint(\"Roc_auc_score {}\".format(roc_auc_score(y_test_prepared, y_pred_prob)))\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_nn, tpr_nn, \"Neural Network\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"74qTcNziLeHG","colab_type":"text"},"cell_type":"markdown","source":"Neural Network gives a roc_auc value of 0.0.87594. This is slightly less than both LGBMClassifier and RandomForestClassifier."},{"metadata":{"id":"kRr3Mut0LeHG","colab_type":"text"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"id":"l1n-ULwlLeHH","colab_type":"text"},"cell_type":"markdown","source":"The best classifier to predict if it is going to rain tomorrow given weather data set provided is the RandomForestClassifer with parameters {'bootstrap': True, 'max_depth': 60, 'n_estimators': 300}. It gives the best roc_auc value which provides the highest probability in identifying positive cases (rain tomorrow)"},{"metadata":{"trusted":true,"id":"b6wtSky3LeHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":567},"outputId":"266ba0e5-f4d5-49c1-ae7f-5c859b318770"},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Best model to predict\nrf_best_model = RandomForestClassifier(bootstrap = True, max_depth = 60, n_estimators = 300, random_state=123)\nrf_best_model.fit(X_train_prepared,y_train_prepared)\n\ny_pred_prob = rf_best_model.predict_proba(X_test_prepared)[:,1]\nfpr_rf, tpr_rf, thresholds = roc_curve(y_test_prepared, y_pred_prob, pos_label= 1)\nprint(\"Roc_auc_score {}\".format(roc_auc_score(y_test_prepared, y_pred_prob)))\n\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr_rf, tpr_rf, \"Random Forest\")\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.show()\n\nprint(classification_report(y_test_prepared, rf_best_model.predict(X_test_prepared)))","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Team_project.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}