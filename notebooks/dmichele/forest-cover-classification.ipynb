{"metadata":{"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{"height":"299px","width":"265px"},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"190.391px"},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Forest Cover Type Multi-class Classification](https://www.kaggle.com/uciml/forest-cover-type-dataset)<a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Features-explanation\" data-toc-modified-id=\"Features-explanation-1.1\">Features explanation</a></span></li><li><span><a href=\"#Reversing-one-hot-encoding-to-label-encoding\" data-toc-modified-id=\"Reversing-one-hot-encoding-to-label-encoding-1.2\">Reversing one-hot encoding to label encoding</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\">Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cover-Type-distribution\" data-toc-modified-id=\"Cover-Type-distribution-2.1\">Cover Type distribution</a></span></li><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-2.2\">Numerical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-Analysis\" data-toc-modified-id=\"Correlation-Analysis-2.2.1\">Correlation Analysis</a></span></li><li><span><a href=\"#Inputting-Hillshade_3pm-with-Random-Forest-Classifier\" data-toc-modified-id=\"Inputting-Hillshade_3pm-with-Random-Forest-Classifier-2.2.2\">Inputting Hillshade_3pm with Random Forest Classifier</a></span></li><li><span><a href=\"#Features-distribution\" data-toc-modified-id=\"Features-distribution-2.2.3\">Features distribution</a></span></li></ul></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-2.3\">Categorical Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Features-distribution\" data-toc-modified-id=\"Features-distribution-2.3.1\">Features distribution</a></span></li><li><span><a href=\"#Chi-square-test-for-categorical-data\" data-toc-modified-id=\"Chi-square-test-for-categorical-data-2.3.2\">Chi-square test for categorical data</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-3\">Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-3.1\">Model Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#F1-score\" data-toc-modified-id=\"F1-score-3.1.1\">F1-score</a></span></li><li><span><a href=\"#Normalized-Confusion-Matrix\" data-toc-modified-id=\"Normalized-Confusion-Matrix-3.1.2\">Normalized Confusion Matrix</a></span></li><li><span><a href=\"#Stratified-k-fold-cross-validation\" data-toc-modified-id=\"Stratified-k-fold-cross-validation-3.1.3\">Stratified k-fold cross-validation</a></span></li><li><span><a href=\"#Learning-curve\" data-toc-modified-id=\"Learning-curve-3.1.4\">Learning curve</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.2\">Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-with-RFE\" data-toc-modified-id=\"Feature-selection-with-RFE-3.2.1\">Feature selection with RFE</a></span></li><li><span><a href=\"#Polynomial-logistic-regression\" data-toc-modified-id=\"Polynomial-logistic-regression-3.2.2\">Polynomial logistic regression</a></span></li><li><span><a href=\"#Resampling\" data-toc-modified-id=\"Resampling-3.2.3\">Resampling</a></span></li></ul></li><li><span><a href=\"#KNN\" data-toc-modified-id=\"KNN-3.3\">KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dimensionality-reduction-with-PCA\" data-toc-modified-id=\"Dimensionality-reduction-with-PCA-3.3.1\">Dimensionality reduction with PCA</a></span></li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.3.2\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-3.4\">Decision Tree Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-3.4.1\">Feature importance</a></span></li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.4.2\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-3.5\">Bagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.5.1\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-3.6\">Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-3.6.1\">Feature importance</a></span></li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.6.2\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-3.7\">Boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.7.1\">Hyperparameters tuning</a></span></li><li><span><a href=\"#Further-improvement-with-feature-engineering\" data-toc-modified-id=\"Further-improvement-with-feature-engineering-3.7.2\">Further improvement with feature engineering</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.8\">Support Vector Machines</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVC\" data-toc-modified-id=\"SVC-3.8.1\">SVC</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.8.2\">SVM</a></span></li><li><span><a href=\"#Minibatch-Gradient-Descent-optimizer\" data-toc-modified-id=\"Minibatch-Gradient-Descent-optimizer-3.8.3\">Minibatch Gradient Descent optimizer</a></span></li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.8.4\">Hyperparameters tuning</a></span></li><li><span><a href=\"#Exact-SVM-on-undersampled-data\" data-toc-modified-id=\"Exact-SVM-on-undersampled-data-3.8.5\">Exact SVM on undersampled data</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\">Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Models-comparison\" data-toc-modified-id=\"Models-comparison-4.1\">Models comparison</a></span></li><li><span><a href=\"#Further-improvement\" data-toc-modified-id=\"Further-improvement-4.2\">Further improvement</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\">References</a></span></li></ul></div>","metadata":{"toc":true}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly\nimport plotly.graph_objs as go\nimport sklearn\nimport re\nfrom plotly import tools\nfrom joblib import dump, load\nfrom sklearn import preprocessing\nfrom IPython.display import Image\nfrom scipy.stats import chi2_contingency\nfrom plotly.subplots import make_subplots\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nimport plotly.express as px\n\npio.renderers.default = \"svg\"\n\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Forest Cover Type\" dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All measurements are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest and amount to over half a million.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('covtype.csv')\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset comprehends 54 cartographic variables plus the class label, 10 of which are quantitative features while the remaining 44 correspond to 2 qualitative variables one-hot encoded.\nIn this work our goal is to classify the *cover type* based on predictor variables of each observation (30 x 30 meter cell).\n<br>Since we have 7 possible Cover Type in given areas, we are approaching a *multiclass classification problem*","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features explanation","metadata":{}},{"cell_type":"markdown","source":"In this section, we briefly present the information regarding the meaning of each feature and its admissible values.","metadata":{}},{"cell_type":"markdown","source":"- Elevation: Elevation in meters.\n- Aspect: Aspect in degrees azimuth.\n- Slope: Slope in degrees.\n- Horizontal_Distance_To_Hydrology: Horizontal distance in meters to nearest surface water features.\n- Vertical_Distance_To_Hydrology: Vertical distance in meters to nearest surface water features.\n- Horizontal_Distance_To_Roadways: Horizontal distance in meters to the nearest roadway.\n- Hillshade_9am: hillshade index at 9am, summer solstice. Value out of 255.\n- Hillshade_Noon: hillshade index at noon, summer solstice. Value out of 255.\n- Hillshade_3pm: shade index at 3pm, summer solstice. Value out of 255.\n- Horizontal_Distance_To_Fire_Point*: horizontal distance in meters to nearest wildfire ignition points.\n- Wilderness_Area#: wilderness area designation.\n- Soil_Type#: soil type designation.","metadata":{}},{"cell_type":"markdown","source":"Wilderness_Area feature is one-hot encoded to 4 binary columns (0 = absence or 1 = presence), each of these corresponds to a wilderness area designation.\n<br>Areas are mapped to value in the following way:\n1. Rawah Wilderness Area\n2. Neota Wilderness Area\n3. Comanche Peak Wilderness Area\n4. Cache la Poudre Wilderness Area","metadata":{}},{"cell_type":"markdown","source":"The same goes for Soil_Type feature which is encoded as 40 one-hot encoded binary columns (0 = absence or 1 = presence) and each of these represents soil type designation.\n<br>All the possible options are:\n\n1. Cathedral family - Rock outcrop complex, extremely stony\n1. Vanet - Ratake families complex, very stony\n1. Haploborolis - Rock outcrop complex, rubbly\n1. Ratake family - Rock outcrop complex, rubbly\n1. Vanet family - Rock outcrop complex complex, rubbly\n1. Vanet - Wetmore families - Rock outcrop complex, stony\n1. Gothic family\n1. Supervisor - Limber families complex\n1. Troutville family, very stony\n1. Bullwark - Catamount families - Rock outcrop complex, rubbly\n1. Bullwark - Catamount families - Rock land complex, rubbly. \n1. Legault family - Rock land complex, stony\n1. Catamount family - Rock land - Bullwark family complex, rubbly\n1. Pachic Argiborolis - Aquolis complex\n1. unspecified in the USFS Soil and ELU Survey\n1. Cryaquolis - Cryoborolis complex\n1. Gateview family - Cryaquolis complex\n1. Rogert family, very stony\n1. Typic Cryaquolis - Borohemists complex\n1. Typic Cryaquepts - Typic Cryaquolls complex\n1. Typic Cryaquolls - Leighcan family, till substratum complex\n1. Leighcan family, till substratum, extremely bouldery\n1. Leighcan family, till substratum - Typic Cryaquolls complex\n1. Leighcan family, extremely stony\n1. Leighcan family, warm, extremely stony\n1. Granile - Catamount families complex, very stony\n1. Leighcan family, warm - Rock outcrop complex, extremely stony\n1. Leighcan family - Rock outcrop complex, extremely stony\n1. Como - Legault families complex, extremely stony\n1. Como family - Rock land - Legault family complex, extremely stony\n1. Leighcan - Catamount families complex, extremely stony\n1. Catamount family - Rock outcrop - Leighcan family complex, extremely stony\n1. Leighcan - Catamount families - Rock outcrop complex, extremely stony\n1. Cryorthents - Rock land complex, extremely stony\n1. Cryumbrepts - Rock outcrop - Cryaquepts complex\n1. Bross family - Rock land - Cryumbrepts complex, extremely stony\n1. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony\n1. Leighcan - Moran families - Cryaquolls complex, extremely stony\n1. Moran family - Cryorthents - Leighcan family complex, extremely stony\n1. Moran family - Cryorthents - Rock land complex, extremely stony","metadata":{}},{"cell_type":"markdown","source":"Cover_Type: forest cover type designation, its possible values are between 1 and 7, mapped in the following way:\n1. Spruce/Fir\n1. Lodgepole Pine\n1. Ponderosa Pine\n1. Cottonwood/Willow\n1. Aspen\n1. Douglas-fir\n1. Krummholz","metadata":{}},{"cell_type":"code","source":"class_names=('1. Spruce/Fir',\n             '2. Lodgepole Pine',\n             '3. Ponderosa Pine',\n             '4. Cottonwood/Willow',\n             '5. Aspen',\n             '6. Douglas-fir',\n             '7. Krummholz')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reversing one-hot encoding to label encoding","metadata":{}},{"cell_type":"markdown","source":"First of all, we reverse one-hot encoding of categorical variables into label encoding to obtain a database better suited for tree-based models fitting and qualitative features plotting.","metadata":{}},{"cell_type":"code","source":"label_encoded_WA = (df.iloc[:,10:14].values == 1).nonzero()[1]\nlabel_encoded_ST = (df.iloc[:,14:54].values == 1).nonzero()[1]\ncolumns = [x for x in range(10,14)]\nle_df = df.drop(columns=df.columns[columns])\ncolumns = [x for x in range(14,54)]\nle_df.drop(columns=df.columns[columns], inplace=True)\nle_df.insert(len(le_df.columns)-1, 'Wilderness_Area', label_encoded_WA)\nle_df.insert(len(le_df.columns)-1, 'Soil_Type', label_encoded_ST)\n\n# Save to csv\nle_df.to_csv('le_covtype.csv')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read previous pre-computed label encoded dataset\nle_df = pd.read_csv('le_covtype.csv', index_col=0)\nle_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"EDA makes it possible to visualize with the naked eye data distribution and interactions both among features themselves and among features and the label. We can achieve it through the visual representation of descriptive statistics such as maximum and minimum, median, quartiles for quantitative predictors. In contrast, histograms and pie charts are better suited for qualitative ones.","metadata":{}},{"cell_type":"code","source":"def histogram_feature(feature, title, barmode='group', image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n#     categories = le_df['Wilderness_Area'].unique()\n    traces = []\n    for c in classes_sorted:\n        traces.append(go.Histogram(\n            histfunc=\"count\",\n    #         x=categories,\n            x=le_df[le_df.Cover_Type == c][feature],\n            name=class_names[c-1],\n            marker_color = plotly.colors.DEFAULT_PLOTLY_COLORS[c-1]\n        ))\n    layout = go.Layout(\n        title=title,\n        barmode=barmode\n    )\n    fig = go.Figure(traces, layout)\n    if (image):\n        pio.write_image(fig, 'images/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def percentage_barchart_feature(feature, title, features_name=None, image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n    traces = []\n    features_range = list(le_df[feature].unique())\n    features_range.sort()\n    features_name = features_range if (features_name == None) else features_name\n    if (len(features_range) != len(features_name)):\n        raise ValueError(\"features_names len must correspond to the number \\\n                         of unique values of this feature\")\n    totals_wilderness = [len(le_df[le_df[feature] == i]) for i in features_range]\n    y = []\n    for c in classes_sorted:\n        y.append([100 * len(le_df[(le_df[feature] == i) & (le_df['Cover_Type'] == c)]) / \n                totals_wilderness[i] for i in range(4)])\n\n    fig = go.Figure(data=[\n        go.Bar(name=class_names[c-1], x=features_name, y=y[c-1],\n               marker_color=plotly.colors.DEFAULT_PLOTLY_COLORS[c-1]) for c in classes_sorted\n    ])\n    # Change the bar mode\n    fig.update_layout(title_text=title, barmode='stack')\n    if (image):\n        pio.write_image(fig, 'images/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def boxplot_feature(feature, title, image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n    traces = []\n    i = 0\n    for c in classes_sorted:\n        rgb_str = re.findall(\"\\d+\", plotly.colors.DEFAULT_PLOTLY_COLORS[i])\n        rgb = tuple(int(x) for x in rgb_str)\n        i += 1\n        traces.append(go.Box(\n            y=le_df[le_df.Cover_Type == c][feature],\n            name=class_names[c-1],\n            marker_color = '#%02x%02x%02x' % rgb\n#             marker_color=[plotly.colors.DEFAULT_PLOTLY_COLORS[c]]\n        ))\n    layout = go.Layout(\n        title=title\n    )\n    fig = go.Figure(traces, layout)\n    if (image):\n        pio.write_image(fig, 'images/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cover Type distribution","metadata":{}},{"cell_type":"markdown","source":"Let's take a brief overview of the class label's data distribution.","metadata":{}},{"cell_type":"markdown","source":"**Cover Type number of observation**","metadata":{}},{"cell_type":"code","source":"display(df['Cover_Type'].value_counts())\nhistogram_feature('Cover_Type', 'Cover Type')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rawah and Comanche Peak areas (WA 1 and WA 3) are the most common in the whole dataset due to their assortment of tree species. These areas typically have lodgepole pine (class 2) as primary species, followed by spruce/fir(class 1) and aspen (class 5).\nDataset is consequently *extremely imbalanced* towards the first two classes.","metadata":{}},{"cell_type":"markdown","source":"Wilderness areas number of observation: ","metadata":{}},{"cell_type":"code","source":"(le_df.Wilderness_Area+1).value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Features ","metadata":{}},{"cell_type":"markdown","source":"Before starting let's check that there are no nan value predictors.","metadata":{}},{"cell_type":"code","source":"df.isna().values.any()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Analysis","metadata":{}},{"cell_type":"markdown","source":"Correlation analysis is a method to evaluate first-order interaction between numerical features.<br>In particular, *Pearson's correlation* is a measure of the linear relationship strength between two numerical variables.\n<br>\n<br>\n\\begin{equation}\n    \\rho_{X,Y} =  \\frac{\\text{cov}(X,Y)}{\\sigma_X\\sigma_Y} = \\frac{\\text{E}[(X - \\mu_X)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y} \\end{equation}\n<br>\nwhere $\\mu_X$ and $\\mu_Y$ are the expected values $\\sigma_X$ $\\sigma_Y$ standard deviations.\nThe correlation coefficient can assume values that range from -1 to 1, where the extremes respectively represent a strong or a negative relationship, while 0 implies the absence.\n<br>In particular, a resulting positive coefficient means that an increase in one variable corresponds to an increase in the second one, proportionally to the strength.\nContrarily, a negative coefficient will result from variables that move in the opposite direction, therefore as x increase, y decreases, and vice versa.\nNotice that even a strong correlation cannot prove causation between two variables.\n","metadata":{}},{"cell_type":"markdown","source":"#### Hotmap <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"*Hotmap* provides an intuitive representation of Pearson's correlation coefficient between each pair of numerical features.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title(\"Heatmap\")\n_ = sns.heatmap(data=le_df.iloc[:, :10].corr(), annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scatterplot matrix <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"The *scatterplot* is another valuable tool to inspect the existing relationship between features. It consists of a graph of two continuous variables in which the explanatory variable is plotted on the x-axis and the dependent one on the y-axis.\nNext, will explore all combination of two quantitative variables searching for patterns in data.","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(30,30))\nplt.title(\"Pairplot\")\nscatt_df = pd.concat([df.iloc[:, :10], df.iloc[:, 54]], axis=1)\nscatt_df.rename(columns={'Horizontal_Distance_To_Hydrology': 'hor_dist_hyd',\n                         'Vertical_Distance_To_Hydrology': 'ver_dist_hyd',\n                         'Horizontal_Distance_To_Roadways': 'hor_dist_road',\n                         'Horizontal_Distance_To_Fire_Points': 'hor_dist_fire'\n                        }, inplace=True)\nsns.set(style=\"ticks\")\n_ = sns.pairplot(scatt_df, vars=scatt_df.columns[slice(0,10,1)], markers='.',\n                 hue='Cover_Type', plot_kws=dict(edgecolor=\"none\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots, we can notice some dependencies between:\n1. hillshades and Aspect \n1. hillshades and Slope\n1. between hillshades themselves<br>\n\nHillshade, in particular, determines surface lighting during the different phases of the day, hence it allows to read land morphology and Aspect and Slope are features about morphology, that's might be a possible reason behind the interaction of these features.","metadata":{}},{"cell_type":"markdown","source":"We can also notice from both pairplot and hotmap:\n\n1. a weak positive correlation between Elevation and Horizontal_Distance_To_Hydrology\n2. a weak positive correlation between Elevation and Horizontal_Distance_To_Roadways\n3. a week positive correlation between Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Firepoints\n4. a moderate positive correlation between Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology\n<br>\n\n<p>Highly elevated lands may be more distant from water, reasonably also from roads. As regards the last two interactions, they seem to be less intuitive.\n</p>","metadata":{}},{"cell_type":"markdown","source":"### Inputting Hillshade_3pm with Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"Eyeballing hillshades scatterplots, we can notice an *unusual number of zero values* such that many of them could be missing values.","metadata":{}},{"cell_type":"code","source":"print('Hillshade_9am zero values: {}'.format(len(le_df[le_df['Hillshade_9am'] == 0])))\nprint('Hillshade_Noon zero values: {}'.format(len(le_df[le_df['Hillshade_Noon'] == 0])))\nprint('Hillshade_3pm zero values: {}'.format(len(le_df[le_df['Hillshade_3pm'] == 0])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hillshade_9am and Hillshade_Noon zeros cardinalities are negligible; as a result, we will only handle Hillshade_3pm supposed missing values, by predicting all the Hillshade_3pm zero values. \nActual zero values appeared indistinguishable from the others, thus, they will be overwritten together with missing values. \nBesides, if a nearby value is predicted by the classifier rather than 0, it should not be such a big deal for Cover_Type prediction.\nRandom Forest Classifier details will be covered later in a later section.","metadata":{}},{"cell_type":"code","source":"ax = sns.scatterplot(x=\"Slope\", y=\"Hillshade_3pm\", markers='.',\n                     hue=\"Cover_Type\", palette=sns.color_palette()[:7],\n                     data=df).set_title(\"A closer look to Slope-Hillshade_3pm joint distribution\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_test = le_df[le_df['Hillshade_3pm'] == 0]\nmissing_train = le_df[le_df['Hillshade_3pm'] != 0]\nmissing_y_tr = missing_train['Hillshade_3pm']\nmissing_y_test = missing_test['Hillshade_3pm']\nmissing_train.drop(columns=['Hillshade_3pm'])\nmissing_test.drop(columns=['Hillshade_3pm'])\nmissing_train.drop(columns=['Cover_Type'])\nmissing_test.drop(columns=['Cover_Type'])\nmissing_X_tr = missing_train.values\nmissing_X_test = missing_test.values\nrfInputer = RandomForestClassifier(n_estimators=30, n_jobs=8, random_state=0) \nrfInputer.fit(missing_X_tr, missing_y_tr)\nmissing_pred = rfInputer.predict(missing_X_test)\ndel rfInputer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le_df.loc[le_df.Hillshade_3pm == 0, 'Hillshade_3pm'] = missing_pred\ndf.loc[df.Hillshade_3pm == 0, 'Hillshade_3pm'] = missing_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.scatterplot(x=\"Slope\", y=\"Hillshade_3pm\", markers='.',\n                     hue=\"Cover_Type\", palette=sns.color_palette()[:7],\n                     data=df).set_title(\"Slope-Hillshade_3pm joint distribution after predicting zero values\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After inputting the predicted values of Hillshade_3pm, data seems to follow the overall joint distribution better.","metadata":{}},{"cell_type":"markdown","source":"### Features distribution","metadata":{}},{"cell_type":"markdown","source":"#### Elevation <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"title = 'Elevation boxplot'\nboxplot_feature('Elevation', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows that Elevation is an excellent predictor because each of these tree species has a separate elevation interquartile range.\nSome classes have even non-overlapping minimum-maximum range values like:\n- Krummholz (class 7) and Ponderosa Pine (class 3) \n- Krummholz (class 7) and Douglas-fir (class 6)\n- Cottonwood/Willow (class 4) and Aspen (class 5)\n- Cottonwood/Willow (class 4) and Spruce/Fir (class 1)","metadata":{}},{"cell_type":"markdown","source":"#### Aspect <a class=\"tocSkip\"/>\nAspect is the orientation of the slope, measured clockwise in degrees from 0 to 360, where 0 is north-facing, 90 is east-facing, 180 is south facing and 270 is west-facing.","metadata":{}},{"cell_type":"code","source":"title = 'Aspect boxplot'\nboxplot_feature('Aspect', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a fixed set of values for all forest species because aspect(azimuth) measures are expressed in degree. Despite this, we can identify different medians and interquartile ranges for each class.","metadata":{}},{"cell_type":"markdown","source":"#### Slope <a class=\"tocSkip\"/>\nThe slope is the steepness or the degree of incline of a surface.","metadata":{}},{"cell_type":"code","source":"title = 'Slope boxplot'\nboxplot_feature('Slope', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly to the previous predictor, slope measure is expressed in degree, although we now have a narrow range of values between 0 and 66 degrees.","metadata":{}},{"cell_type":"markdown","source":"#### Distance to Hydrology <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"title = 'Horizontal_Distance_To_Hydrology boxplot'\nboxplot_feature('Horizontal_Distance_To_Hydrology', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Vertical_Distance_To_Hydrology boxplot'\nboxplot_feature('Vertical_Distance_To_Hydrology', title, image=False)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset, distance features are recorded in meters, including Vertical_Distance_To_Hydrology, which also presents *negative values* for specific area measurements situated under the nearest water surface.","metadata":{}},{"cell_type":"markdown","source":"#### Distance to Roadways <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"title = 'Horizontal_Distance_To_Roadways boxplot'\nboxplot_feature('Horizontal_Distance_To_Roadways', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows that Spruce/Fir, Lodgepole Pine and Krummholz are located in the most distant areas from roadways and have a more generous range of values in their distribution compared to classes 3, 4, 5 and 6.","metadata":{}},{"cell_type":"markdown","source":"#### Horizontal_Distance_To_Fire_Points <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"title = 'Horizontal_Distance_To_Fire_Points'\nboxplot_feature('Horizontal_Distance_To_Fire_Points', title, image=False)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly Horizontal_Distance_To_Fire_Points and Horizontal_Distance_To_Roadways have a similar range of values for every class.","metadata":{}},{"cell_type":"markdown","source":"#### Hillshades <a class=\"tocSkip\"/>\nHillshading is a technique used to visualize terrain as shaded relief, illuminating it with a hypothetical light source.\nAs a result, the orientation of the surface to the light source determines the illumination value for that area.\nIn our case, we have three hillshade values depending on the position of the sun, the light source, during different times of the day, in particular, 9 am, 12 am, and 3 pm.","metadata":{}},{"cell_type":"code","source":"title = 'Hillshade_9am'\nboxplot_feature('Hillshade_9am', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Hillshade_Noon'\nboxplot_feature('Hillshade_Noon', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Hillshade_3pm'\nboxplot_feature('Hillshade_3pm', title, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Outliers","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we will not make any assumption on the outliers by leaving them as they are in the dataset.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Features","metadata":{}},{"cell_type":"markdown","source":"### Dataset integrity check <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"To check the integrity of categorical data, next, we sum for each row all the one-hot encoded columns expecting for each qualitative predictor a value exactly equal to 1(we sum several 0 values and at most a 1 value).","metadata":{}},{"cell_type":"code","source":"check_WA = np.sum(df.values[:, 10:14], axis=1)\ncheck_WA_unique = np.unique(check_WA)\nprint(f\"number of active Wilderness_Area's categories for each row {check_WA_unique}\")\ncheck_ST = np.sum(df.values[:, 14:54], axis=1)\ncheck_ST_unique = np.unique(check_ST)\nprint(f\"number of active Soil Type's categories for each row {check_ST_unique}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features distribution","metadata":{}},{"cell_type":"markdown","source":"#### Wilderness Area <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"title = 'Wilderness Area Distribution'\nfeatures_name = ['Rawah', 'Neota', 'Comanche Peak', 'Cache la Poudre']\npercentage_barchart_feature('Wilderness_Area', title, features_name, image=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As remarked before, the located tree species depend on given Wilderness_Area. In this plot, we can observe how different percentages of the cover type vary according to the wilderness area.<br>\nWilderness_Area may have a reduced predicting power in some classes like Lodgepole Pine or Spruce/Fir Cover type, which are spread all across different areas. At the same time may be very discriminative for Cottonwood/Willow, which can be found only on Cache la Poudre areas.","metadata":{}},{"cell_type":"markdown","source":"#### Soil Type <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"soil_types = [le_df[le_df['Soil_Type'] == i] for i in range(40)]\nspecs = [[{\"type\": \"domain\"} for i in range(5)] for i in range(8)]\ncolors = plotly.colors.DEFAULT_PLOTLY_COLORS\nlabels = class_names \nsubplot_titles = ['        ST' + str(i+1) for i in range(40)]\nfig = make_subplots(rows=8, cols=5, specs=specs, subplot_titles=subplot_titles)\ni=0\nfor row in range(8):\n    for col in range(5):\n        values = []\n        for c in range(1,8):\n            mask = soil_types[(row) * 5 + (col)]['Cover_Type'] == c\n            value = len(soil_types[(row) * 5 +(col)][mask.values])\n            value = None if (value == 0) else value\n            values.append(value)\n        i += 1\n        sub_fig = go.Pie(labels=labels, values=values)\n        fig.add_trace(sub_fig, row=row+1, col=col+1)\n        fig.update_traces(marker=dict(colors=colors,\n                                      line=dict(color='#000000',\n                                                width=1)))\nfig.update_layout(height=1000, width=1000, title_text=\"Soil Type's label percentage\")\n#dynamic rendering\n# fig.show(render='notebook')\n\n# static rendering\npio.show(fig, height=1000, width=1000)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These pie charts show for each Soil Type the corresponding distribution of forest cover type. In other words, we have different percentages of the composition of cover types, which differ according to the soil.\nThese charts show that almost all soils include more than two possible cover type, which indicates this feature may be beneficial for classification, nevertheless in combination with other predictors.","metadata":{}},{"cell_type":"markdown","source":"### Chi-square test for categorical data","metadata":{}},{"cell_type":"markdown","source":"By looking at the previous two charts, we could claim that it is very likely to be a dependency between each categorical features, Wilderness_Area or Soil_Type, and the class label Cover_Type.\n<br>Our assumption may be verified by testing *the null hypothesis of independence* $H_0$, i.e. no relationship exists between them, whereas the alternative hypothesis is that the variables are related.\nThe chi-square test involves computing the chi-square statistic and then comparing the obtained value with the critical one in $\\chi^2$ distribution with $DF = (R - 1)(C - 1)$ and a chosen $\\alpha$ ($\\alpha = 0.05$ provides us sufficient evidence in rejecting the hypothesis).","metadata":{}},{"cell_type":"markdown","source":"The chi-square statistic is computed on the squared difference between observed and expected values in each cell of the contingency table.","metadata":{}},{"cell_type":"markdown","source":"\\begin{equation}\n\\chi^2 = \\sum\\limits_{i=1, j=1}^{i=R, j=C} \\frac{( O_{ij} - E_{ij} )^2}{ E_{ij}}\n\\end{equation}","metadata":{}},{"cell_type":"markdown","source":"#### Wilderness_Area Cover_Type Chi2 Test <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"contingency = pd.crosstab(le_df['Cover_Type'],\n                          le_df['Wilderness_Area'])\ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Soil_Type  Cover_Type Chi2 test <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"contingency = pd.crosstab(le_df['Cover_Type'],\n                          le_df['Soil_Type'])  \ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both cases, we can assume $\\text{p-value} < 0.01 $ which indicates substantial evidence against $H_0$, consequently, we reject it.","metadata":{}},{"cell_type":"markdown","source":"#### Soil_Type Wilderness_Area Chi2 test <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Let's examine if a dependency exists even between the pair of features Soil_Type and Wilderness_Area","metadata":{}},{"cell_type":"code","source":"contingency = pd.crosstab(le_df['Wilderness_Area'], \n                            le_df['Soil_Type'])  \ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$H_0$ can be rejected in this case, too.\nThus, an *evident dependency* exists not only between each of the two categorical features and class labels Cover_Type, but also between them.","metadata":{}},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"markdown","source":"In finding good performing classification models we will reproduce the following workflow:\n- dataset splitting in *training*, and *test* set\n\nThen for each statistical model, we perform:\n- *hyperparameter optimization* (either manual, grid-search or random-search optimization) via *stratified 5-fold-cross validation* on the training set.\n- train model on whole training set\n- test model on unseen test data.\n\nLastly, we will:\n- compare results on testing data\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom imblearn.pipeline import Pipeline as imbPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA as sklearnPCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom xgboost import XGBClassifier\nfrom sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.kernel_approximation import Nystroem\nimport graphviz\nimport xgboost as xgb\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.kernel_approximation import Nystroem, RBFSampler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The division is the same for both different encoded datasets (one-hot encoded and label encoded) motivated by the need to obtain comparable test metrics on the same test data, which in this case amount to 20% of the dataset.\n<br>Before starting to fit models on one-hot encoded data, we perform a little adjustment by *dummy encoding* qualitative features,  removing a column from both Wilderness_Area and Soil_Type. The necessity of uncorrelated features motivates this operation.","metadata":{}},{"cell_type":"code","source":"#label encoded split\ny_le = le_df['Cover_Type'].copy()\ny_le -= 1\nX_le = le_df.drop(columns=['Cover_Type'])\nX_le= le_df.iloc[:, :-1]\nX_train_le, X_test_le, y_train_le, y_test_le = train_test_split(X_le, y_le, test_size=.2, random_state=0)\n\n# dummy encoded split\ndf = df.drop(columns=['Wilderness_Area1', 'Soil_Type1'])\ny_he = df['Cover_Type'].copy()\ny_he -= 1\nX_he = df.iloc[:, :-1]\nX_train_he, X_test_he, y_train_he, y_test_he = train_test_split(X_he, y_he, test_size=.2, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical and numerical columns are identified in the dataset for further application of different model-dependent preprocessing methods.","metadata":{}},{"cell_type":"code","source":"categorical_cols = [cname for cname in X_he.columns \n                    if X_he[cname].nunique() < 3]\nnumerical_cols = [cname for cname in X_he.columns\n                 if X_he[cname].nunique() >= 3]\n\nX_he[numerical_cols] = X_he[numerical_cols].apply(pd.to_numeric, downcast='float')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"*Evaluation metrics* are crucial in expressing and comparing model performances. Metrics also provide, during the validation phase, valuable feedback for *hyperparameters optimization*.\nIn choosing the appropriate metric, the class *imbalance* should be seriously considered.\n<br>Accuracy metric is a typical case in which a high score may be misleading if reached with a poor model biased towards predicting majority classes, which in our dataset amount to 85% of all instances.\n<br>Consequently, in such cases, *f1-score* can be a fair metric that behaves impartially with imbalanced data and can be easily adaptable to a multiclass classification problem.\n\\begin{equation}\n\\end{equation}\n\n\\begin{equation}\nF_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n\\qquad\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\end{equation}\n\nwhere $precision = \\frac{TP}{TP + FP}$ and $recall = \\frac{TP}{TP + FN}$","metadata":{}},{"cell_type":"markdown","source":"### F1-score","metadata":{}},{"cell_type":"markdown","source":"F1-score is the harmonic mean of precision and recall, where an F1 score reaches its best value at 1, whenever perfect precision and recall are achieved, and worst at 0.\nAlthough this metric is well defined for binary classification, it can also be used in a multiclass setup with little adaptations.\n<br>Several options among *macro*, *micro*, and *weighted* average are possible in our context. Unfortunately, the last two easily lead, like accuracy metrics, to provide higher scores to classifiers biased toward predicting majority classes, almost ignoring the remaining ones.\n<br>The *macro* average tends less to be biased toward the most populated classes because it is calculated by only averaging the F1-score calculated for each class, thus assigning to each cover type the same weight regardless of its number of observations.\nWe still mention accuracy, secondly, in models comparison for its easy interpretability.","metadata":{}},{"cell_type":"markdown","source":"### Normalized Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"A *confusion matrix* is used to asses overall model performances evaluating model inferences class by class, by showing how many instances have been correctly classified or misclassified with respect to the actual class.\nThen for a fixed row $i$, we can evaluate on each column $j$ how many observations were assigned to $j$-th class, i.e. the $_{ij}$ indexed cell represents the number of actual class $i$ occurrences classified as $j$.\nWe now introduce a much intuitive variation for imbalanced data normalizing by row. Ratios, as opposed to absolute numbers, are immediately comparable also for classes with different cardinality.\nInterestingly it turns out that *normalized confusion matrix* main diagonal values correspond to recall for each class.","metadata":{}},{"cell_type":"code","source":"def conf_matrix(y_true, y_pred):\n    data = confusion_matrix(y_true, y_pred)\n    # broadcast norm all over confusion matrix\n    data = data / np.sum(data, axis=1)[:, np.newaxis]\n    df_cm = pd.DataFrame(data, columns=np.unique(y_true)+1, index = np.unique(y_true)+1)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    plt.figure(figsize = (7,4))\n    sns.set(font_scale=1.2)#for label size\n    ax = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 12},\n               cbar=False)# font size\n    \n    _ = ax.set_title('Normalized Confusion Matrix')\n    precision_scores = precision_score(y_true, y_pred, average=None)\n    recall_scores = recall_score(y_true, y_pred, average=None)\n    print(\"precision scores: {}\".format(np.around(precision_scores, decimals=3)))\n    print(\"recall scores: {}\".format(np.around(recall_scores, decimals=3)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stratified k-fold cross-validation","metadata":{}},{"cell_type":"markdown","source":"*Stratification* seeks to ensure that each fold is representative of the full dataset, also reducing variance between each fold evaluation, which generally leads to a better estimate of test evaluation.<br>\nIn particular stratified method involves randomly dividing the training set into $k$ groups, preserving the percentage of samples for each class: one of these folds is kept as a validation set, whereas the others are kept for training. This procedure is repeated $k$ times, alternating each time the validation phase on a different fold and then computing the final estimate by averaging the $k$ evaluations.\n\\begin{equation}\nCV_{(n)} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} {F_1}_i \n\\end{equation}\n<p>The choice of $k = 5$ is motivated by empirical evidence that this choice yield test error estimates that suffer neither from excessively high bias nor from very high variance.","metadata":{}},{"cell_type":"code","source":"def stratified_K_cross(X, y, classifier):\n    cv_train_score = []\n    cv_test_score = []\n    skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        classifier.fit(X_train, y_train)\n        score = classifier.score(X_train, y_train)\n        cv_train_score.append(score)\n        score = classifier.score(X_test, y_test)\n        cv_test_score.append(score)\n    return cv_test_score, cv_train_score \n\ndef print_cv_scores(scores):\n    print('K-cross validation scores:')\n    print('accuracies: {}'.format(scores['test_acc']))\n    print('f1_scores: {}'.format(scores['test_f1']))\n    print('mean accuracy: {0:.3f} (+/-{1:.3f})'.format(np.mean(scores['test_acc']),\n                                             np.std(scores['test_acc'])))\n    print('mean f1_score: {0:.3f} (+/-{1:.3f})'.format(np.mean(scores['test_f1']),\n                                             np.std(scores['test_f1'])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nscoring = {'acc' : 'accuracy',\n           'f1' : 'f1_macro'}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning curve","metadata":{}},{"cell_type":"markdown","source":"The learning curve purpose is to graphically show how the error or similarly the accuracy in our case behave with the increase of training data. It helps in determining whether the model could benefit from adding with more data as a sort of regularization for models with high variance errors.\nWe will see that often the gap between validation and training curve will become smaller as a consequence of adding more training data.\n<br>Please note that we always expect higher accuracy on training data with respect to validation data, for the reason that we indirectly seek to maximize it.\nDespite this, if we have a wider gap between the two curves when training on a smaller subset of data, this probably means that the model has enough learning capacity to find patterns in the training data that are due to random chance (noise). This phenomenon is called *overfitting*.","metadata":{}},{"cell_type":"code","source":"# @ignore_warnings(category=ConvergenceWarning)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\ndef plot_learning_curve(estimator, X, y, title=\"\", cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),\n                        ):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    \n    fig = go.Figure() \n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    leg=True\n        \n    p1 = go.Scatter(x=train_sizes, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=train_sizes, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=train_sizes, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=train_sizes, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=train_sizes, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training score\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=train_sizes, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"Cross-validation score\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(title=title,\n                      xaxis_title=\"Training size\",\n                      yaxis_title=\"Accuracy score\"\n                     )\n    pio.show(fig)\n  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_best_n_classifiers(grid_search, n):\n\n    print(\"Grid scores on development set:\")\n    indexes = np.argsort(grid_search.cv_results_['mean_test_f1_macro'])[::-1][:n]\n    means_accuracy = grid_search.cv_results_['mean_test_accuracy'][indexes]\n    stds_accuracy = grid_search.cv_results_['std_test_accuracy'][indexes]\n    means_f1_macro = grid_search.cv_results_['mean_test_f1_macro'][indexes]\n    stds_f1_macro = grid_search.cv_results_['std_test_f1_macro'][indexes]\n    params = [grid_search.cv_results_['params'][i] for i in indexes]\n\n    for mean_a, std_a, mean_f, std_f, param in zip(means_accuracy, stds_accuracy,\n                                                    means_f1_macro, stds_f1_macro, params):\n        print(\"accuracy %0.3f (+/-%0.03f)\"\n              % (mean_a, std_a * 2))\n        print(\"f1 %f (+/-%0.03f) for %r\"\n              % (mean_f, std_f * 2, param))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Logistic regression models the probability that an instance belongs to a particular category.\nIn logistic model logistic function is used.\n\\begin{equation}\np( X ) = \\frac{ e^{{\\beta_0} + {\\beta_1 X}} } { 1 + e^{{\\beta_0} + {\\beta_1 X}} }\n\\end{equation}\n\nAfter some manipulation, we can obtain a linear model but on a non-linear scale of probability.\n\n\\begin{equation}\n\\log\\left(\\frac{p( X )}{1-p(x)}\\right) = \\beta_0 + \\beta_1 X\n\\end{equation}\n\n\nTo fit this model maximum likelihood method is used. The intuition is that we search for best parameters $\\beta_0$, $\\beta_1$ that maximize the probability of obtaining the given dataset. \n\n\\begin{equation}\n\\hat{\\beta}_0, \\hat{\\beta}_1 = \\arg \\max\\limits_{\\beta_0 \\in B_0, \\beta_1 \\in B_1} \\mathcal{L} (\\beta_0, \\beta_1 | x_1, ..., x_n)\n\\end{equation}\n\nAlthough we have done some transformation to the original logistic function, logarithm does not alter the solution and makes log-likelihood faster to compute.\n\nIn this work we should classify instances into more than two classes, then two possible approaches are allowed:\n- $B$  one class vs the rest classifiers, with $B = |\\{C\\}|$\n- multinomial logistic regression\n\\begin{equation}\nsoftmax(x)_i = \\frac{e^{x_i}}{\\sum_{j}^{B}e^{x_j}},    i = 1, ... B\n\\end{equation}\n\nWe go for *multinomial logistic regression* as it usually leads to better results in shorter training time.","metadata":{}},{"cell_type":"markdown","source":"### First a little preprocessing <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"For logistic regression, we will use the *dummy encoded* version of the dataset after that some preprocessing is performed, especially for the numerical feature, which needs to be standardized. Standardization is mandatory for *saga* solver because it is susceptible to features scale.","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Debug(BaseEstimator, TransformerMixin):\n    \n    def transform(self, X):\n        display(pd.DataFrame(X).head())\n        print(X.shape)\n        return X\n    \n    def fit(self, X, y=None, **fit_params):\n        return self","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple model baseline  <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"We start with a baseline model with `penalty='l2'`, `C=1` and `class weights='balanced'`.\n1. the reason behind the choice of saga solver is the higher speed in fitting coefficients for large datasets and the availability of multiclass logistic regression implementation.\n2. C, the inverse of regularization strength(l2 in our case) will not make any significant difference because, as we will see, the model does not have enough capacity to overfit data. \n3. balanced class weights will contrast class imbalance by applying a weight penalty for misclassification inversely proportional to class frequencies in data.","metadata":{}},{"cell_type":"code","source":"logistic_model = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000, class_weight='balanced')\nclf_lr = Pipeline(steps=[('preprocessor', preprocessor),\n#                                       ('dbg', Debug()),\n                                      ('classifier', logistic_model)])\n\nscores = cross_validate(clf_lr, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The coefficients of the fitted model provide us some information on which are the most predictive features.","metadata":{}},{"cell_type":"code","source":"clf_lr.fit(X_train_he, y_train_he)\nprint('10 most discriminative feature per class')\ncoeff_df = pd.DataFrame()\nfor i in range(1, 7+1):\n    train_df = X_train_he\n    idxs = np.argsort(np.abs(logistic_model.coef_[i-1]))[::-1]\n    coeff_i = pd.DataFrame(train_df.columns[idxs])\n    coeff_i.columns = ['Feature_class' + str(i)]\n    coeff_i[\"Coefficient_class\" + str(i)] = pd.Series(logistic_model.coef_[i-1][idxs])\n    coeff_i[\"Coefficient_class\" + str(i)] = round(coeff_i[\"Coefficient_class\" + str(i)], 4)\n#     print(\"class {}\".format(class_names[i-1]))\n#     coeff_i = coeff_i.reindex(coeff_i[\"Coefficient\"].abs().sort_values(ascending=False).index)\n    coeff_df = pd.concat([coeff_df, coeff_i[:10]], axis=1)\ndisplay(coeff_df.transpose())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In particular, coefficients found through maximum likelihood fitting might help us understanding how much each feature affects the class logit. The idea is that positive coefficients for a given class and feature indicate that the class is more likely to happen with a predictor increase, while negative coefficients indicate the opposite.","metadata":{}},{"cell_type":"markdown","source":"### Feature selection with RFE","metadata":{}},{"cell_type":"markdown","source":"*Recursive Feature Elimination* is a technique aimed at decreasing the number of features to build simpler models. At each step, a logistic regression model is fitted, then all but the least predictive coefficients are kept, while this one is pruned. This process is repeated until the target number of features is reached.<br>\nWe found that reducing the number of features to 40 could be a good compromise in terms of model complexity and f1-score with respect to the baseline model.","metadata":{}},{"cell_type":"code","source":"logistic_model = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000, class_weight='balanced')\npreprocessor.fit(X_train_he, y_train_he)\nX = preprocessor.transform(X_train_he)\nselector = RFE(logistic_model, 40, step=1)\n_ = selector.fit(X, y_train_he)\nprint('Selected feature:')\nprint(X_train_he.columns[selector.support_].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rX = selector.transform(X)\n_ = logistic_model.fit(rX, y_train_he)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_validate(logistic_model, rX, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\n\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the results that adding more regularization to the model does not improve our score metric. Probably we are in the presence of a high bias error, thus our classifier is missing relevant relationships between features and target outputs. So next, we will try a polynomial regression model with a higher learning capacity to overcome such a problem.","metadata":{}},{"cell_type":"markdown","source":"### Polynomial logistic regression","metadata":{}},{"cell_type":"markdown","source":"Polynomial logistic regression will also evaluate polynomial numerical features dependencies. Here we will fit the classifier on degree-2 polynomial, taking into account squares terms and products between the variables.","metadata":{}},{"cell_type":"code","source":"poly = PolynomialFeatures(2, interaction_only=False)\npoly_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', \n         Pipeline([\n             ('polynomial', poly),\n             ('scaler', StandardScaler())\n         ]),\n         numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_model_pf = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', class_weight='balanced')\n\nclf_lr_pf = Pipeline([(\"polynomial_features\", poly_preprocessor),\n#                      (\"debug\", Debug()),\n                     (\"logistic_regression\", logistic_model_pf)\n                    ])\nscores = cross_validate(clf_lr_pf, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definitely, quadratic terms provide a slight improvement to the model. ","metadata":{}},{"cell_type":"markdown","source":"### Resampling","metadata":{}},{"cell_type":"markdown","source":"*Data resampling methods* are an alternative approach to handle class imbalance by rebalancing them. \nThere are two different main approaches: *undersampling* and *oversampling*.\n\n<img src=\"images/resampling.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\nSource: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n\n*Undersampling* means resampling data by discarding instances in the dataset from majority classes. It may lead to the loss of relevant information for our task.\nOn the opposite *oversampling* techniques generates new synthetic data from minority classes rebalancing the dataset.","metadata":{}},{"cell_type":"markdown","source":"#### Random undersamping <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"It is the simplest, but also the fastest method which undersamples the majority classes by randomly picking without replacement.\n<br>Next, we will train and validate the model from the previous section on a random undersampled version of the dataset.","metadata":{}},{"cell_type":"code","source":"logistic_model_rus = LogisticRegression(random_state=0, C=1000, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000)\n\nclf_lr_rus = imbPipeline([\n    (\"polynomial_features\", poly_preprocessor),\n    ('near_miss', RandomUnderSampler(random_state=42)),\n    ('classifier', logistic_model_rus)])\n\nscores = cross_validate(clf_lr_rus, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtained worse results compared to balanced class weights. This because by naively undersampling to minority class cardinality, we are discarding too much data, and consequently, we risk to miss some valuable patterns from training data. ","metadata":{}},{"cell_type":"markdown","source":"<img src=\"images/1_6UFpLFl59O9e3e38ffTXJQ.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n\nSource: https://www.researchgate.net/publication/287601878/figure/fig1/AS:316826589384744@1452548753581/The-schematic-of-NRSBoundary-SMOTE-algorithm.png","metadata":{}},{"cell_type":"markdown","source":"SMOTE technique involves determining, for each sample of the minority classes, the k-nearest point, measuring each vector distance between every pair of sample-neighbors and multiplying it by a random number between 0 and 1 obtaining a new sample as a result. $k$ is a hyperparameter that we found works pretty well if $k=10$.\n<br>At this point, we try to tackle class imbalance with SMOTE oversampling rather than class weights, still fitting a logistic regression model on degree-2 polynomial features.","metadata":{}},{"cell_type":"code","source":"logistic_regression_sm = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial')\nclf_lr_sm = imbPipeline([(\"polynomial_features\", poly_preprocessor),\n                     (\"smote\", SMOTE(random_state=42, k_neighbors=10, n_jobs=8)),\n                     (\"logistic_regression\", logistic_regression_sm)\n                    ])\nscores = cross_validate(clf_lr_sm, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Let's see how last obtained classifier behaves with the increase of training data","metadata":{}},{"cell_type":"code","source":"plot_learning_curve(clf_lr_sm,  X_train_he, y_train_he,title='SMOTE 2-degree polynomial Logistic Regression', cv=skf, n_jobs=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot highlights an underfitting problem. We have a high error, indeed, in both training and validation.\nAs a result, adding more data would not benefit the model, which suffers from a high bias error.","metadata":{}},{"cell_type":"markdown","source":"### Testing the best model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"The best model according to the estimation of validation metrics is composed of *SMOTE* with $k=10$ neighbours for tackling class imbalance and *degree-2 polynomial logistic regression* ","metadata":{}},{"cell_type":"code","source":"# clf_lr_sm.fit(X_train_he, y_train_he)\ny_pred = clf_lr_sm.predict(X_test_he)\ntest_f1 = f1_score(y_test_he, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_he, y_pred)\nconf_matrix(y_test_he, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying to resolve the problems derived from unbalanced data with SMOTE, we fall in the opposite problem: oversampled minority classes suffer from low precision and high recall. Further improvement can be performed by tuning the oversampling ratio between the majority and the resampled minority classes.","metadata":{}},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"markdown","source":"KNN Classifier is a method that attempts to estimate the conditional distribution of label $Y$ based on $X$ features. Provided $K$ and a new observation $\\mathscr{x_0}$ the model search for a neighborhood $\\mathcal{N_0}$ \nof $K$ nearest observations in the training data that are closest to $\\mathscr{x_0}$. \n\\begin{equation}\nPr(Y = j|X = \\mathscr{x_0}) = \\frac{1}{K} \\sum\\limits_{i \\in \\mathcal{N_0}} I(y_i = j)\n\\end{equation}\n\nWith the growth of $K$, we move from *flexible boundaries* to almost linear ones. Low $K$s will result in high-variance low-bias classifiers, whereas for big $K$ it is the opposite. A reliable method to establish a good $K$ is empirically choosing it from data with cross-validation.","metadata":{}},{"cell_type":"markdown","source":"### Baseline model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"We start the hyperparameter optimization form a baseline KNN classifier with $K=6$.\n<br>Notice that we decide empirically to not preprocess data because no matter which scaler we use, it worsens the classifier performance rather than helping the model not to be biased towards features with a higher magnitude.<br>","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=6, p=2, weights='distance', n_jobs=8)\nscores = cross_validate(knn, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now considering dataset dimensions, *Principal Component Analysis* will be beneficial to mitigate the curse of dimensionality and to improve model performance in computing prediction. This because KNN during the predictions phase will compute euclidean or l1 distance among $\\mathscr{x}_0$ and all the points in the training set, hence reducing the number of features will result in lower distances and fewer dimensions involved in computations.","metadata":{}},{"cell_type":"markdown","source":"### Dimensionality reduction with PCA","metadata":{}},{"cell_type":"markdown","source":"PCA is a dimensionality reduction method. Its key idea is to find a *lower-dimension representation* that explains as much as possible the variability in data.\n<br>Each dimension, also called components, found by PCA is a linear combination of starting *p* features and it is orthogonal with respect to the other m found.\nTherefore each principal components has the form:\n\\begin{equation}\nZ_m = \\phi_{1m}X_1 + \\phi_{2m}X_2 + ... + \\phi_{pm}X_p\n\\end{equation}\n\nThe loadings constitute the loading vector $\\phi_m = (\\phi_{1m} \\phi_{2m} ... \\phi_{pm})^T$ which define a direction where the data vary the most.\n<br> They are nomalized: &nbsp;&nbsp;&nbsp;&nbsp;$\\sum\\limits_{j=1}^p \\phi_{jm}^2 = 1$.\n\n\n##### PCA transformation steps: <a class=\"tocSkip\"/>\n1. Standardize the data X\n2. Obtain the eigenvectors and eigenvalues either:\n    1. by factorizing the covariance matrix: $X_{std}^TX_{std} = W\\hat{\\Sigma}^2W^T$\n    1. by performing Singular Value Decomposition on $X_{std} = U\\Sigma{}W^T$\n3. Sort eigenvalues in descending order and choose the first k, with k equal to the selected number of components \n4. Construct the projection matrix $W_k$ by selecting from $W$ the eigenvectors corresponding to the chosen $k$ greater eigenvalues.\n5. Transform the original dataset $X_{std}$ via multiplying it by $W_k$ to obtain a k-dimensional feature space","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_std = (preprocessor.fit_transform(X_he))\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nfor ev in eig_vecs:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The Proportion of Variance Explained <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"We will consider *PVE* in deciding how many components to keep to fit the model on top of them, in replacement to our original features. Since we are using PCA for a classification problem, a common way of choosing $k$ is by determining it in an end-to-end fashion via cross-validation.\n<br>A valid alternative is to analyze cumulative PVE and keep principal components until an *elbow shape* occurs, usually at about 85% of the total variance.","metadata":{}},{"cell_type":"code","source":"tot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,20)],\n    y=var_exp,\n    name='Individual'\n)\n\ntrace2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,20)], \n    y=cum_var_exp,\n    name='Cumulative'\n)\n\ndata = [trace1, trace2]\n\nlayout=dict(\n    title='Explained variance by different principal components',\n    yaxis=dict(\n        title='Explained variance in percent'\n    ),\n    annotations=list([\n        dict(\n            x=1.16,\n            y=1.05,\n            xref='paper',\n            yref='paper',\n            text='Explained Variance',\n            showarrow=False,\n        )\n    ])\n)\n\nfig = dict(data=data, layout=layout)\npio.show(fig, filename='selecting-principal-components', renderer='notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can recognize an elbow shape in correspondence of the 8th component with a cumulative PVE of 87.58%","metadata":{}},{"cell_type":"markdown","source":"#### PCA as Exploratory Data Analysis <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"PCA tool may also be used to visualize up to 3 PC dimensions by plotting data after projection into the new subspace.","metadata":{}},{"cell_type":"code","source":"sklearn_pca = sklearnPCA(n_components=3)\nX_pca = sklearn_pca.fit_transform(X_std)\ny = y_he\ndata_pca = pd.DataFrame(X_pca, columns = ['PC1', 'PC2','PC3'])\ndata_pca['Label'] = y_he + 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x=\"PC1\", y=\"PC2\",\n           data=data_pca,\n           fit_reg=False,\n           markers='.',\n           hue='Label', # color by cluster\n           legend=True,\n           scatter_kws={\"s\": 10}\n          )\npcaplot = plt.gca()\n_ = pcaplot.set_title('Visualizing first two Principal Components')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data point in this projection are rather overlapped that it is not even possible to see points from all the classes. Despite this ,it may be interesting to notice that all classes have a sort of U shape once projected on first two PCs.","metadata":{}},{"cell_type":"code","source":"def scatter_plot(plot, data, x_label, y_label, z_label, class_label, c, m, label):\n#     print(data['Label'] == class_label)\n    data_pca[data_pca['Label'] == class_label]\n    x = data[ data['Label'] == class_label ]['PC1'] #groupby Name column x_label\n#     display(x)\n    y = data[ data['Label'] == class_label ]['PC3']\n    z = data[ data['Label'] == class_label ]['PC2']\n    # s: size point; alpha: transparent 0, opaque 1; label:legend\n    plot.scatter(x,y,z,color=c, edgecolors='face', s=5, alpha=0.9, marker=m,label=label)\n    plot.set_xlabel(x_label)\n    plot.set_ylabel(y_label)\n    plot.set_zlabel(z_label)\n    return\n\nplot = plt.figure(figsize=(9,9)).gca(projection='3d')\nplt.title('Visualizing first three Principal Components')\n# scatter_plot\ncolors = ['b', 'o', 'g', 'r', 'v', 'b']\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 1,'royalblue','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 2,'orange','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 3,'g','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 4,'red','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 5,'mediumpurple','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 6,'brown','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 7,'hotpink','.','0')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By adding third component in this plot we can explain about 58% of variance in data.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters tuning ","metadata":{}},{"cell_type":"markdown","source":"After validating different models, it turns out that *seven principal components* allow us to obtain almost the same accuracy of the original model with 52 features, though requiring less computational time during predictions.\nPlus, PCA on *scaled feature* will always lead to worse results compared to unscaled ones. For this reason, exceptionally in this pipeline, we will apply dimensionality reduction directly on *raw features*, then we will fit a KNN classifier on top of the first 7 PCs.\nThis means that features with a wider range of values as elevation and different distance features play a decisive role in determining the PCs and consequently the class labels.","metadata":{}},{"cell_type":"markdown","source":"The remaining hyperparameters to tune are:\n1. `n_neighbors` : represent the $K$ nearest neighbors evaluated during prediction\n2. `weights` : if `uniform` all neighbors have equal weight, while `distance` gives different importance to each voted based on the inverse of distance, so closer points became more important\n3. `p`: `1` for using l1 distance and `2` for l2 distance","metadata":{}},{"cell_type":"markdown","source":"#### How k affects the classifier performances? <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Let's visualize how $K$ affects our classification metric, f1-score.","metadata":{}},{"cell_type":"code","source":"def plot_k_cross_val(X, y, title=\"\", cv=None, n_jobs=1):\n    \n    fig = go.Figure() \n    ks = [i for i in range (1,11)]\n    train_scores = np.array([]).reshape(0,5)\n    test_scores = np.array([]).reshape(0,5)\n    for k in ks:\n        sklearn_pca = sklearnPCA(n_components=7)\n        knn_cv= KNeighborsClassifier(n_neighbors=k, p=2, weights='distance', n_jobs=8)\n        knn_pca_pipeline = make_pipeline(sklearn_pca ,knn_cv)\n        scores = cross_validate(knn_pca_pipeline, X, y, cv=cv,\n                                scoring=scoring, return_train_score=True,\n                                n_jobs=1)\n        train_scores = np.concatenate((train_scores, scores['train_f1'].reshape(1, 5)), axis=0)\n        test_scores = np.concatenate((test_scores, scores['test_f1'].reshape(1, 5)), axis=0)\n        \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n#     if(colnum==1):\n#         leg=True\n#     else:\n#         leg=False\n    leg=True\n        \n    p1 = go.Scatter(x=ks, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=ks, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=ks, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=ks, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=ks, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=ks, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"validation\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"k neighbors\",\n        yaxis_title=\"Macro F1-score\",\n    )\n    pio.show(fig)\n  \nplot_k_cross_val(X_train_he, y_train_he, cv=skf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph curves show that with $K < 4$ we have a higher variance-error contribution because of the very flexible class boundaries. On the other side, for $K > 4$, we risk to overgeneralize data and increase the bias error.","metadata":{}},{"cell_type":"markdown","source":"#### Grid search <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"We perform hyperparameters selection by fitting and evaluating several models, characterized by specific parameters, through stratified K-cross validation.\nIn grid search model are build by picking every single parameter from a grid of candidates exploring all the possible combination in an exhaustive search.\n<br>\nRecall that validation metrics involved in the evaluation are the previously mentioned: f1-score and accuracy.","metadata":{}},{"cell_type":"code","source":"sklearn_pca = sklearnPCA(n_components=7)\nknn_pca = KNeighborsClassifier(n_jobs=8)\nknn_pca_pipeline = make_pipeline(sklearn_pca ,knn_pca)\nparam_grid = {\n    'kneighborsclassifier__n_neighbors': [1, 4, 6, 8, 10],\n    'kneighborsclassifier__weights' : ['uniform', 'distance'],\n    'kneighborsclassifier__p' : [1, 2]\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search = GridSearchCV(estimator=knn_pca_pipeline,\n                           param_grid=param_grid,\n                           scoring=['accuracy', 'f1_macro'],\n                           refit='f1_macro',\n                           cv=skf,\n                           n_jobs=8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train_he, y_train_he)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine which are the top five performing classifiers.","metadata":{}},{"cell_type":"code","source":"print_best_n_classifiers(grid_search, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"sklearn_pca = sklearnPCA(n_components=7)\nknn_pca = KNeighborsClassifier(n_jobs=8, n_neighbors=4, p=1, weights='distance')\nknn_pca_pipeline = make_pipeline(sklearn_pca ,knn_pca)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curve(grid_search.best_estimator_,X_train_he, y_train_he, title=\"KNN Classifier\", cv=skf, n_jobs=8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since $K=4$ leads to flexible boundaries, the model trained on a subset of data easily tends to *overfit data*.\n<br>As we can see, by adding more training observation, the gap between training and validation scores shrinks more and more as a result of variance error reduction.\n<br>Unfortunately, we are not so far from the score obtained with the baseline model, though PCA makes model predictions faster.","metadata":{}},{"cell_type":"markdown","source":"#### Testing the best model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"grid_search.best_estimator.fit(X_train_he, y_train_he)\ny_pred = knn_pca_pipeline.predict(X_test_he)\ntest_f1 = f1_score(y_test_he, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_he, y_pred)\nconf_matrix(y_test_he, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN classifier performs overall well on test data, except for class 4, which has a lower recall, caused by many misclassifications of class 4 instances as class 3.","metadata":{}},{"cell_type":"markdown","source":"## Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"little_tree = tree.DecisionTreeClassifier(max_depth=2)\nlittle_tree.fit(X_train_le, y_train_le)\nclass_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine',\n               'Cottonwood/Willow', 'Aspen', 'Douglas-fir', \n               'Krummholz']\ndot_data = tree.export_graphviz(little_tree, out_file=None,\n                               feature_names=le_df.columns[:12],\n                               class_names=class_names,\n                               filled=True, rounded=True)\n# graph\npydot_graph = graph_from_dot_data(dot_data)\npydot_graph.write_png('images/original_tree.png')\n# pydot_graph.set_size('\"5,5!\"')\n# pydot_graph.write_png('images/resized_tree.png')\nImage('images/original_tree.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A decision tree model stratifies the data space in different regions as a result of various *recursive binary splits*, which compose a series of decision rules.\n<br>In an effort to grow the optimum tree, it is computationally infeasible to evaluate every combination of possible splits, as a consequence, they are performed in a top-down greedy fashion.\nWhen building a tree two possible measure are used to evaluate *node purity* and so the goodness of a future split: \n1. *Gini index*:\n    \\begin{equation}\n    G = \\sum\\limits_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n    \\end{equation}\n    <br>where $\\hat{p}_{mk}$ represents the portion of training observation in the *m*-th region that are from the *k*-th class\n2. *Cross entropy*:\n    \\begin{equation}\n    D = - \\sum\\limits_{k=1}^{K} \\hat{p}_{mk} \\log{\\hat{p}_{mk}}\n    \\end{equation}\n<br>In performing a split for a given node, we need to \n    determine which feature split allows the best improvement in terms of purity for such node.<br>\n    Tree models are therefore built by selecting the current best possible split *greedily*, at each step.\n</p>","metadata":{}},{"cell_type":"markdown","source":"### Baseline model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"In this section, our baseline model will be a decision tree classifier trained on top of SMOTE oversampled training data, which in general provides better performance than class weights on original imbalanced data.","metadata":{}},{"cell_type":"code","source":"sm_tree = SMOTE(k_neighbors=10, n_jobs=8, random_state=42)\ndt_pipeline = imbPipeline(steps=[('smote', sm_tree),\n                               ('tree', DecisionTreeClassifier(random_state=0))])\nscores = cross_validate(dt_pipeline, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance","metadata":{}},{"cell_type":"markdown","source":"*Feature importance* is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n<br>This pie plot, with its easy interpretability, shows us for every feature, how much it leads to a *purity* improvement in child nodes after splitting its range of values.","metadata":{}},{"cell_type":"code","source":"dt_pipeline.fit(X_train_le, y_train_le)\nfeature_importance = dict(zip(le_df.columns, dt_pipeline.steps[1][1].feature_importances_))\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\npio.show(fig, renderer='notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Elevation feature proves to be by far the most discriminative predictor because, as seen before in EDA, each class feature distribution profoundly differs from the others. \n<br>The same goes for Horizontal-Distance_To_Fire_Points and Horizontal_Distance_To_Roadways, though in less magnitude.\n<br>Finally, Soil_Type discriminating power, strictly depends on individual soil value.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters tuning","metadata":{}},{"cell_type":"markdown","source":"In training a decision tree, we got a variety of hyperparameters to optimize:\n1. `criterion`:  evaluation split function, e.g. Gini or cross-entropy\n1. `min_samples_split`: minimum number of samples required to split a node\n1. `max_depth`: the maximum depth which model is allowed to reach\n1. `splitter`: strategy used to split feature interval of values, 'best' and 'random' splits are possible options. In the first case at each step best split is chosen among features, evaluating for each one the most purity improving branch. Otherwise, with the second option, a random split is performed for each feature, then the best among them is chosen.\n1. `max_features`: number of features to take into account in performing a split, e.g. all n_features, sqrt(n_features), etc.\n\n<br>We will also try two different `k_neighbors` values for SMOTE oversampling.\nIn addition, we will search over two different `k_neighbors` values for SMOTE oversampling.\nWith this dataset, cross-entropy has proven to work slightly better than Gini index.\nThe same goes for `min_samples_split = 3`, \n`splitter = best` and `max_features = n_features`.\nBy fixing these parameters, we can save a lot of computational time, consequently grid searching only over `max_depth`.\n<br>On the next plot, we will see in detail how the model behaves as trees depth upper bound changes.","metadata":{}},{"cell_type":"code","source":"def plot_depth_cross_val(X, y, title=\"\", cv=None, n_jobs=8):\n    \n    fig = go.Figure() \n    ds = [i for i in range (1,41, 5)]\n    train_scores = np.array([]).reshape(0,5)\n    test_scores = np.array([]).reshape(0,5)\n    for d in ds:\n        dt_pipeline = DecisionTreeClassifier(max_depth=d,random_state=0)\n        scores = cross_validate(dt_pipeline, X, y, cv=cv,\n                                scoring=scoring, return_train_score=True,\n                                n_jobs=n_jobs)\n        train_scores = np.concatenate((train_scores, scores['train_f1'].reshape(1, 5)), axis=0)\n        test_scores = np.concatenate((test_scores, scores['test_f1'].reshape(1, 5)), axis=0)\n        \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    leg=True\n        \n    p1 = go.Scatter(x=ds, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=ds, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=ds, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=ds, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=ds, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=ds, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"validation\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"depth size\",\n        yaxis_title=\"Macro F1-score\",\n    )\n    pio.show(fig, renderer='notebook')\n  \nplot_depth_cross_val(X_train_le, y_train_le, cv=skf)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model can't overfit even with increasing of `max_depth`. It might be due to the large dataset proportions and `min_samples_split=3` setting.","metadata":{}},{"cell_type":"code","source":"decision_tree_param = {\n    'tree__criterion': ['entropy'],\n    \"tree__min_samples_split\": [3],\n    \"tree__max_depth\": [20, 24, 28, 32],\n    'tree__splitter' : ['best'],\n    'tree__max_features' : [None],\n    'smote__k_neighbors' : (5, 10)\n}\ndt_pipeline = imbPipeline(steps=[('smote', SMOTE(n_jobs=1, random_state=42)),\n                                 ('tree', DecisionTreeClassifier(random_state=0))])\n\ngrid_search_dt = GridSearchCV(estimator=dt_pipeline,\n                           param_grid=decision_tree_param,\n                           scoring=['accuracy', 'f1_macro'],\n                           refit='f1_macro',\n                           cv=3,\n                           n_jobs=8)\n_ = grid_search_dt.fit(X_train_le, y_train_le)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top five classifiers performances in the validation phase:","metadata":{}},{"cell_type":"code","source":"print_best_n_classifiers(grid_search_dt, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning Curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"plot_learning_curve(grid_search_dt.best_estimator_,  X_train_le, y_train_le, title='Decision Tree Classifier', cv=skf, n_jobs=8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite the model may overfit with the smaller training set, considering the more conspicuous gap with training accuracy, it benefits from adding more data like a sort of regularization. \n<br>This generally happens because bigger datasets make model harder to overfit, thus encouraging better generalization.","metadata":{}},{"cell_type":"markdown","source":"#### Testing the best model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"y_pred = grid_search_dt.best_estimator_.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision tree classifier is somewhat balanced in terms of recall between different classes, but again a not negligible portion of class 4 was misclassified as class 3. Overall macro f1-score is also affected by lower than average precisions in classes 4, 5 and 6.","metadata":{}},{"cell_type":"markdown","source":"## Bagging","metadata":{}},{"cell_type":"markdown","source":"For the *Central Limit Theorem*, given a set of *n* independent observation $Z_1, ..., \nZ_n$, each with variance $\\sigma^2$, the variance of the mean $\\overline{Z}$ of the observations is given by $\\sigma^2/n$.\n<br>Similarly, by averaging prediction from different classifiers, we expect a *reduction of variance*.\n<br>In the *bootstrap method*, we emulate the process of obtaining a new independent sample set from the population by sampling with replacement from the original dataset. This technique is usually applied in the calculation of estimator properties such as sample mean, e.g. estimating standard error or confidence interval.","metadata":{}},{"cell_type":"markdown","source":"We will employ the bootstrap technique for resampling the original dataset into B training datasets and train a different tree on top of each one.\nSince we are interested in obtaining a single prediction for $\\mathscr{x_0}$, we then aggregate by voting as a sort of average each prediction $\\hat{f}^{*b}(\\mathscr{x_0})$, i.e. the resulting prediction will be the class predicted by the most of different bagging trees. \nThis method actually provides a variance error reduction in predictions.","metadata":{}},{"cell_type":"markdown","source":"### Baseline model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"As a baseline, we will use an ensemble model of 10 decision trees(without any max length constraint), each one trained on top of a bootstrapped dataset, i.e. sampled with replacement from the original one.","metadata":{}},{"cell_type":"code","source":"bagging = BaggingClassifier(n_jobs=8)\nscores = cross_validate(bagging, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters tuning","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters:\n1. `n_estimators`: number of trees, actually is not a critical parameter in this setup, because an high value would not lead to overfitting\n2. `bootstrap`: whether to sample with replacement or not from original dataset\n\n<p>\nAfter several attempts of different meaningful hyperparameters, it turns out that ensembling trees with params taken from the previous section best classifier outperform both baseline and the other models tried.\n</p>","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion='entropy', max_depth=32,\n                           max_features=None, min_samples_split=3,\n                           splitter='best') \nbagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\nscores = cross_validate(bagging_prev_dt, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"bagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\nplot_learning_curve(bagging_prev_dt,  X_train_le, y_train_le, title='Bagging Classifier', cv=skf, n_jobs=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly we have better accuracy mostly with a smaller dataset as a result of reducing variance error.\nOverall we have a similar trend to the decision tree learning curve, though employing ensembling, we achieve an overall better accuracy score.","metadata":{}},{"cell_type":"markdown","source":"#### Testing best parameters <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"bagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\n\nbagging_prev_dt.fit(X_train_le, y_train_le)\ny_pred = bagging_prev_dt.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By mean of Bootstrap aggregation, we have an overall improvement f1-score, also for class 5 wherein we can notice, in the normalized confusion matrix, that despite its recall is decreased, precision raised from 0.839 to 0.942.","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"markdown","source":"Similarly to bagging models in random forest classifier, we build B trees on bootstrapped data. Still, this model usually provides a significant improvement by trying *to decorrelate* trees predictions by choosing a random subset of possible features when evaluating a potential split, usually $m\\approx{\\sqrt{p}}$ with $p$ total number of predictor (12 in our case).<br>\nThrough this method is possible to make a robust classifier even in the cases where a strong predictor is present in dataset (e.g Elevation for some classes).<br>\nBy choosing local best predictors we are trying to exploit different patterns, avoiding always splitting data by the same most predictive variables.","metadata":{}},{"cell_type":"markdown","source":"### Baseline model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Random Forest Classifier with default parameters is composed of 100 random trees grown with no depth constraint on bootstrapped data.","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0, class_weight='balanced', n_jobs=8)\nscores = cross_validate(random_forest, X_train_le, y_train_le, cv=skf, scoring=scoring,\n                        return_train_score=True,n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance","metadata":{}},{"cell_type":"markdown","source":"In this setup, Elevation is still the most predictive feature, even though it has lost 5.3%. Soil_Type and many other features, on the contrary, have gained some further importance. This happens because we are not always considering Elevation while splitting data, but only when it falls in the $m$ chosen predictors.","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0, class_weight='balanced', n_jobs=8)\nrandom_forest.fit(X_train_le, y_train_le)\nfeature_importance = random_forest.feature_importances_\nfeature_importance = {X_train_le.columns[i] : feature_importance [i] for i in range(len(X_train_le.columns))}\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\npio.show(fig, render='notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters tuning","metadata":{}},{"cell_type":"markdown","source":"Exploring the vast space of possible parameters by a grid search would be too expensive in computational time. A good compromise could be a *random search* of 20 iterations, in which we just build 20 different models by random choosing from a preselected subset of parameters.","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters\n1. `n_estimators` : number of trees\n1. `min_samples_split` : the minimum number of samples required in splitting a node\n1. `min_samples_leaf` : in evaluating a new possible split, it guarantees a minimum number of samples in a node in order to be considered a leaf\n1. `max_features` : number of features to take into account in performing a split, `auto` corresponds to `sqrt(n_features)`\n1. `max_depth` : the maximum depth which each tree in the model can reach\n1. `class_weight` : with `balanced` we try to tackle class imbalance by applying high weight penalties to misclassification of more infrequent classes. \n1. `bootstrap` : whether to sample with replacement or not from original the dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=50, stop=500, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# class_weight for handling imbalanced dataset\nclass_weight = [None, 'balanced'] \n# # Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n               'class_weight' : class_weight\n              }\n# random_grid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(n_jobs=8, random_state=0)\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                               n_iter=20, scoring=['accuracy', 'f1_macro'],\n                               cv=skf, refit='f1_macro', random_state=42,\n                               n_jobs=1\n                              )\nrf_random.fit(X_train_le, y_train_le)\nprint_best_n_classifiers(rf_random, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the top 5 models, we may conclude that neither bootstrap nor class weights won't help too much in this setup.\n<br>Secondly, it must be mentioned that like bagging also random forest never overfits with the increasing of `n_estimators`, nevertheless a great number of trees impacts considerably training time.\n<br>In this case, 150 or more trees are enough.","metadata":{}},{"cell_type":"markdown","source":"#### Learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"plot_learning_curve(rf_random.best_estimator_,  X_train_le, y_train_le, \"Random forest\", cv=skf, n_jobs=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest, like other tree-based methods, behaves well from adding more data.","metadata":{}},{"cell_type":"markdown","source":"#### Testing best parameters <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto',\n                            max_depth=40, class_weight=None, bootstrap=False, n_jobs=8, random_state=0)\nrf.fit(X_train_le, y_train_le)\ny_pred = rf.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest overall performs slightly worse than bagging method in  terms of accuracy and averaged f1-score in both validation and test phase. \n<br>It is very likely that this is due to random found hyperparameters. Searching deeply for better hyperparameters would be very time-consuming, especially for models with the same or  higher number of trees.","metadata":{}},{"cell_type":"markdown","source":"## Boosting","metadata":{}},{"cell_type":"markdown","source":"Boosting uses a separate ensemble technique, in which trees are aggregated sequentially. Each tree is now fitted to *the residual* originated from the model at the previous step, rather than to class label $Y$. The model will benefit from this in areas where the residual is bigger.\n<br>In each training iteration, a new tree is grown and added to the model multiplied by a shrinkage parameter $\\lambda$ that controls the learning rate, then the model is updated and the new residual is computed.\n<br>The number of trees needed is strictly dependent on data and $\\lambda$.\nTherefore the resulting model will be:\n\\begin{equation}\n\\sum\\limits_{b=1}^{B} \\lambda \\hat{f}^b(\\mathscr{x})\n\\end{equation}\n<br>\nwhere $B$ is the total number of trees composing the model and $\\hat{f}^b(\\mathscr{x})$ the contribution of the b-th tree to the model.","metadata":{}},{"cell_type":"markdown","source":"### Baseline model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"We start from training using a baseline model with balanced class weights and default parameters, 100 10-depth boosted trees.","metadata":{}},{"cell_type":"code","source":"class_weights = compute_class_weight('balanced', np.unique(y_train_le), y_train_le)\nweight_array = [class_weights[y_train_le.iloc[i]-1] for i in range(len(y_train_le.values))]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline sensata\nbst = XGBClassifier(max_depth=10, n_estimators=100, random_state=0, objective='multi:softmax', n_jobs=8)\nscores = cross_validate(bst, X_train_le, y_train_le, cv=skf, scoring=scoring,\n                        return_train_score=True, fit_params={'sample_weight': weight_array},\n                        n_jobs=1)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters tuning ","metadata":{}},{"cell_type":"markdown","source":"Similarly to the previous classifier, the models have a large number of parameters, then we are forced to random search in order to explore a significant portion of the hyperparameter space in a feasible amount of time. ","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters to optimize:\n1. `subsample`: subsample: the fraction of the training samples(randomly selected) that will be used to train each tree\n1. `reg_lambda`: L2 regularization term on weights\n1. `n_estimators`: number of trees which constitute the model, an high value could lead to overfitting\n1. `min_child_weight`: regularization term. If the tree partition step results in a leaf node with the sum of instance weights less than this parameter, then the building process will give up further partitioning\n1. `max_depth`: the maximum depth which each tree in the model can reach\n1. `learning_rate`: the shrinkage parameter $\\lambda$\n1. `gamma`: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is the more conservative the algorithm will be\n1. `colsample_bytree`: the fraction of features (randomly selected) that will be used to train each tree\n1. `colsample_bylevel`: subsample at each level of depth, the features are selected from the ones chosen for the current tree","metadata":{}},{"cell_type":"code","source":"parameters = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100]\n}\nxgb_clf = XGBClassifier(objective='multi:softmax', nthread=8)\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nrandom_search = RandomizedSearchCV(xgb_clf, param_distributions=parameters,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n_ = random_search.fit(X_train_le, y_train_le)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_best_n_classifiers(random_search, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through a random search, we discover a combination of parameters by means of which we can achieve a good validation f1-score. Model capacity results particularly sensitive to `n_estimators` and `max_depth`, hence by fixing the number of trees to 100 (to have reasonable training time) and slightly increase `max_depth` until symptoms of overfitting occur, we can improve the model further.","metadata":{}},{"cell_type":"code","source":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 100, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\nscores = cross_validate(bestXGBC, X_train_he, y_train_he, cv=skf, scoring=scoring,\n                        return_train_score=True, n_jobs=8)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 50, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\nplot_learning_curve(bestXGBC, X_train_le, y_train_le, \"Gradient Boosting\", cv=skf, n_jobs=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Boosting learning curve has similar trend to the previous two ensemble tree-based method.","metadata":{}},{"cell_type":"markdown","source":"#### Testing best parameters <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"bestXGBC.fit(X_train_le, y_train_le)\ny_pred = bestXGBC.predict(X_test_le)\nprint(conf_matrix(y_test_le, y_pred))\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering the overall f1-score and confusion matrix result, here we got slightly better results, although we suffer from less sensitivity for class 4 and 5.","metadata":{}},{"cell_type":"markdown","source":"### Further improvement with feature engineering","metadata":{}},{"cell_type":"markdown","source":"We can enlarge feature space by also considering *interactions between features*, thus introducing linear combinations between them to the original dataset. This is particularly effective for tree-based models, which can only perform split by considering one feature at time and cannot extract such dependencies.\nIn particular, we will provide to the classifier the following features: \n- mean_hill : mean of hillshades\n- distance_hydro : euclidean distance to hydrology\n- sum_dist_road_fire : sum between roadways and fire points distances\n- diff_dist_road_fire : absolute distance between roadways and fire points distances\n- sum_dist_road_hydro : sum between roadways and hydrology distances \n- diff_dist_road_hydro : absolute distance between roadways and hydrology distances \n- sum_dist_fire_hydro : sum between fire points and hydrology distances\n- diff_dist_fire_hydro : absolute distance between fire points and hydrology distances","metadata":{}},{"cell_type":"code","source":"mean_hill = (le_df['Hillshade_9am'] + le_df['Hillshade_Noon'] + le_df['Hillshade_3pm']) / 3\ndistance_hydro = ((le_df['Vertical_Distance_To_Hydrology']) ** 2 + le_df['Horizontal_Distance_To_Hydrology']) **(1/2)\nsum_dist_road_fire = le_df['Horizontal_Distance_To_Roadways'] + le_df['Horizontal_Distance_To_Fire_Points']\ndiff_dist_road_fire = abs(le_df['Horizontal_Distance_To_Roadways'] - le_df['Horizontal_Distance_To_Fire_Points'])\nsum_dist_road_hydro = le_df['Horizontal_Distance_To_Roadways'] + le_df['Horizontal_Distance_To_Hydrology']\ndiff_dist_road_hydro = abs(le_df['Horizontal_Distance_To_Roadways'] - le_df['Horizontal_Distance_To_Hydrology'])\nsum_dist_fire_hydro = le_df['Horizontal_Distance_To_Fire_Points'] + le_df['Horizontal_Distance_To_Hydrology']\ndiff_dist_fire_hydro = abs(le_df['Horizontal_Distance_To_Fire_Points'] - le_df['Horizontal_Distance_To_Hydrology'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le_df['mean_hill'] = mean_hill\nle_df['distance_hydro'] = distance_hydro\nle_df['sum_dist_road_fire'] = sum_dist_road_fire\nle_df['diff_dist_road_fire'] = diff_dist_road_fire\nle_df['sum_dist_road_hydro'] = sum_dist_road_hydro\nle_df['diff_dist_road_hydro'] = diff_dist_fire_hydro\nle_df['sum_dist_fire_hydro'] = sum_dist_fire_hydro\nle_df['diff_dist_fire_hydro'] = sum_dist_road_fire\nle_df.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label encoded split\ny_le = le_df['Cover_Type'].copy()\ny_le -= 1\nX_le = le_df.drop(columns=['Cover_Type'])\nX_train_le_new, X_test_le_new, y_train_le_new, y_test_le_new = train_test_split(X_le, y_le, test_size=.2, random_state=0)\n                                           ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we just fit the previous best performing Xgboost model, and test it again.","metadata":{}},{"cell_type":"code","source":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 100, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\n_ = bestXGBC.fit(X_train_le_new, y_train_le_new)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance <a class=\"tocSkip\"/>\n<br>Some of the newly created features have reached considerable feature importance and are better suited for splitting data. They are:\n- 'diff_dist_fire_hydro'\n- 'sum_dist_road_fire'\n- 'sum_dist_road_hydro' \n- 'diff_fist_road_fire',\n- 'sum_dist_fire_hydro'\n","metadata":{}},{"cell_type":"code","source":"feature_importance = bestXGBC.feature_importances_\nfeature_importance = {X_train_le_new.columns[i] : feature_importance [i] for i in range(len(X_train_le_new.columns))}\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test set evaluation <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"y_pred = bestXGBC.predict(X_test_le_new)\nconf_matrix(y_test_le_new, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By adding predictors linear combinations, we effectively help our model in determining sharper decision boundaries. Feature engineering brought an increase in every single class f1-score.","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"markdown","source":"### SVC","metadata":{}},{"cell_type":"markdown","source":"Maximal Margin Classifier is a binary classifier based on a *separating hyperplane* that divides p-dimensional space in two, one for each class.\n\\begin{equation}\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p = 0  \n\\end{equation}\n<br>\nSo in the training phase \nwe will calculate the best orientation for the hyperplane, while \nin test phase given a new observation $\\mathscr{x_0}$ we just assign it to first or second class depending on which side of the hyperplane is located.\n\\begin{equation}\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0 \\\\ or \n\\\\  \n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0  \n\\end{equation}\n\nIn numerous cases, a perfect separating hyperplane does not exist, hence the need to introduce a *soft margin* in SVC, which is a generalization of Maximal Margin Classifier. In Support Vector Classifier an observation is allowed to be located on the incorrect side of the margin or hyperplane, thus providing a certain degree of tolerance for misclassification of some instances. The mathematical formulation is: \n<br><br>\n\\begin{equation}\n\\begin{aligned}\n& \\min\\limits {\\beta, \\beta_0} \\frac{1}{2}\\lVert{\\beta}\\rVert^2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n& \\text{subject to}\\enspace \\xi_i > 0, y_i(x_i^T \\beta + \\beta_0) > 1 - \\xi_i, \\forall i\n\\end{aligned}\n\\end{equation}\n\n\n<br>\nwhere $\\xi_1, ..., \\xi_n$ are the *slack variables* that \nmodel the possibility for an observation to be on the other side \nof margin or hyperplane, in fact we have $\\xi_i = 0$ if i-th instance of data is correctly classified while $\\xi_i > 0$ \nif misclassified. In this way, $C$ plays a critical role as a regularization term in deciding how much data points could be misclassified to ensure a bigger margin. In particular, $C=\\infty$ correspond to *Maximal Margin Classifier* problem, while lower $C$ will introduce more bias error in some cases allowing better generalization from training data.\n$C$ parameter will be tuned empirically from data with cross-validation.\n<br>Multiclass SVM or multiclass SVC are an extension from the basic one in which two possible approaches could be used:\n1. *one-versus-one classifier*: $\\binom{K}{2}$ SVMs are trained, one for each pair of classes and final classification is performed by assign the most frequent predicted class to new test observation $\\mathscr{x_0}$\n1. *one-versus-rest classifier*: K classifiers are fitted and each of these will separate k-th class from the K-1 rest. The final classification is done by choosing the class result the most distant from the margin.\n\nThe second approach is chosen in this notebook because of the large amount of time required for training.","metadata":{}},{"cell_type":"markdown","source":"### Baseline Model <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Data standardization plays a decisive role with SVC and SVM classifiers since they are very sensible from feature scaling. If trained with unstandardized data, hyperplane orientation would depend more on feature range of data than on its distribution.\nPreprocessing aside, our baseline model will be a linear SVC with `squared_hinge` loss, `C = 10^5` with `l2` penalty and `class_weights='balanced'`.","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = svm.LinearSVC(loss='squared_hinge', dual=False, penalty='l2', C=10000.0,\n                    multi_class='ovr', class_weight='balanced', max_iter=1000, \n                    random_state=0)\nsvc_clf = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('svc', svc)])\nscores = cross_validate(svc_clf, X_train_he, y_train_he, cv=skf, scoring=scoring,\n                        return_train_score=True, n_jobs=8)\nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"markdown","source":"In some cases, we could have a data distribution in which linear boundaries cannot effectively separate classes. \n<br>In these scenarios, it is still possible to train a model with extra variables used to describe *non-linear dependencies*.<br>\nActually, with the additional variables we are still finding a linear decision boundary, but in a *higher-dimensional space* that projected on the starting one will lead to a non-linear hypersurface.","metadata":{}},{"cell_type":"markdown","source":"#### Kernels <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"It can be shown that SVC can be represented as\n\\begin{equation}\n    f(x) = \\beta_0 + \\sum\\limits_{i=1}^n \\alpha_i \\langle x,x_i\\rangle\n\\end{equation}\nwhere $\\alpha_i$ and $\\beta_0$ are parameters that need to be estimated by means of $\\binom{n}{2}$ inner products between training observations.\n<br>It is shown that is possible to replace inner products with *kernel* K, which is a measure of the similarity between two observations and it turns out that $\\alpha_i \\neq 0$ in the solution only for support vectors, hence we will obtain:\n\\begin{equation}\n    f(x) = \\beta_0 + \\sum\\limits_{i \\in S}^n \\alpha_i K(x,x_i)\n\\end{equation}\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Kernel Approximation <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"The main problem of *kernel methods* is its high computational cost connected with kernel matrices. The cost is at least quadratic in the number of training data points, but most kernel methods include computation of matrix inversion or eigenvalue decomposition and the cost becomes cubic in the number of training data.<br>\nKernel approximation, using a subset of data as a basis, allow us to better scale on large datasets and to approximate feature mappings of certain kernels as a result of a non-linear transformation of the input.\n<br>*Nystroem Method* consists in approximating the kernel matrix $\\hat{K}\\in\\mathbb{R}^{nxn}$, that represents data in the kernel method, with $\\tilde{K}\\in\\mathbb{R}^{nxq}$ of rank $q$.<br>\n\nIn order to compute that kernel, we need first to choose $q$ via `n_components` parameter and then we are going to train an SVC on top of the newly obtained features.","metadata":{}},{"cell_type":"markdown","source":"##### Kernel decision boundaries visualization <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"In the next plots, we pick a subset of first 1000 instances from the training set and we train an SVC on four different kernel mapping approximations on top of the first two principal components.\n<br>In such a way, we can visualize and get an idea about linear, *polynomial* and *RBF* (both Nystroem and Fourier approximation) kernel approximations decision boundaries.","metadata":{}},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nfrom sklearn.kernel_approximation import (RBFSampler,\n                                          Nystroem)\nfrom sklearn.linear_model import SGDClassifier\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v', 'P', 'D')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan', 'yellow', 'purple')\n    cmap = ListedColormap(colors[:])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=colors[idx],\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        # plot all samples\n        if not versiontuple(np.__version__) >= versiontuple('1.9.0'):\n            X_test, y_test = X[list(test_idx), :], y[list(test_idx)]\n            warnings.warn('Please update to NumPy 1.9.0 or newer')\n        else:\n            X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c='',\n                    alpha=1.0,\n                    linewidths=1,\n                    marker='o',\n                    s=55, label='test set')\n        \n\npca = sklearnPCA(n_components=2)\nX_train_he_pca = pca.fit_transform(preprocessor.fit_transform(X_train_he.iloc[:1000, :]))\n\nmodels = []\nmodels.append(Pipeline([(\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", Nystroem(kernel='polynomial', degree=3, random_state=0)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", Nystroem(kernel='rbf', random_state=0, gamma=0.5)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", RBFSampler(random_state=0, gamma=0.5)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))])) \nnsamples=1000\nplt.tight_layout()\nplt.figure(figsize=(18, 14))\ntitles = ['Linear SVC', 'Nystroem polynomial(degree=3) approximation',\n          'Nystroem rbf approximation gamma=0.5', 'Fourier rbf approximation gamma=0.5']\nfor i in range(4):\n    models[i].fit(X_train_he_pca, y_train_he.iloc[:nsamples])\n    plt.subplot(2, 2, i + 1)\n    plot_decision_regions(X_train_he_pca, y_train_he[:1000], models[i])\n    plt.title(titles[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Minibatch Gradient Descent optimizer","metadata":{}},{"cell_type":"markdown","source":"Kernel approximation method is not enough to allow the model to scale efficiently. \n<br>With Stochastic Gradient Descent estimator instead, we follow a different optimization strategy: in particular, while minimizing the objective function, we update SVC weights parameter based on gradients of a small subset of the training set called batch, rather than on the whole dataset as previously done in SVC. This process is iteratively repeated until it either converge or reaches max iterations stop condition.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"images/stochastic gradient descent.png\"/>\nSource: https://www.i2tutorials.com/deep-learning-interview-questions-and-answers/explain-brief-about-mini-batch-gradient-descent/","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters tuning ","metadata":{}},{"cell_type":"markdown","source":"In this section we optimize through random search SGDC without kernels and later we look for improvement via  kernel approximation with the purpose of selecting the best hyperparameters for this model.","metadata":{}},{"cell_type":"markdown","source":"#### Linear Classifier <a class=\"tocSkip\">","metadata":{}},{"cell_type":"markdown","source":"SGDC hyperparameters\n1. `alpha`: It is the constant weight which multiplies the regularization term. It has a behavior similar to C\n2. `loss`: *hinge* is regular SVC loss while 'squared_hinge' (or L2-SVM) penalizes violated margin more strongly, so quadratically instead of linearly\n3. `penalty`: penalty parameter stands for regularization type, so *l1*, *l2* or *elasticnet*, which corresponds to using both terms\n4. `average`: if true regular SGD is done. When optimization is finished, the averaged weights among the step are used instead of the last found.\n5. `class_weight`: with 'balanced' we try to tackle class \nimbalance by applying high weight penalties to misclassification of more infrequent classes.","metadata":{}},{"cell_type":"code","source":"param = {\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Polynomial Classifier<a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Nystroem polynomial kernel hyperparameters:\n1. `gamma`: kernel coefficient\n1. `n_components`: number of points used to construct the mapping in this approximation instead that all the training set.\n1. `degree`: degree of polynomial kernel\n","metadata":{}},{"cell_type":"code","source":"param = {\n    'nystroem__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'nystroem__degree': [2, 3],\n    'nystroem__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nfeature_map_nystroem = Nystroem(kernel='polynomial', random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, feature_map_nystroem,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RBF Classifier <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"Nystroem and RBFSampler radial basis function kernel approximation hyperparameters:\n1. `gamma`: kernel coefficient\n1. `n_components`: number of points used to construct the mapping in this approximation instead that all the training set.\n","metadata":{}},{"cell_type":"code","source":"param = {\n    'nystroem__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'nystroem__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nfeature_map_nystroem = Nystroem(kernel='rbf', random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, feature_map_nystroem,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf,\n                                   refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Nystroem RBF random search <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"print_best_n_classifiers(random_search, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RBF Sampler random search <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"from sklearn.kernel_approximation import RBFSampler\nparam = {\n    'rbfsampler__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'rbfsampler__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nrbf_sampler = RBFSampler(random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, rbf_sampler,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n\n# random_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although various approaches were tried, it still difficult to effectively optimize a good classifier due to the long training times.\n<br>Substantial improvement in metrics via SVM approximation can be achieved by moving closer to the exact SVM classifier, increasing the rank of kernel transformation matrix and the maximum number of iteration for SGD Classifier. However, this direction would be hugely expensive in terms of computational time.","metadata":{}},{"cell_type":"markdown","source":"#### SGDC with RBF kernel approximation learning curve <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"sklearn_pca = sklearnPCA(n_components=30)\nrbf_sampler = RBFSampler(random_state=0, n_components=100, gamma=0.001)\nsgd_classifier = SGDClassifier(penalty='l1', loss='squared_hinge', alpha=1e-07, max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(StandardScaler(), sklearn_pca, rbf_sampler,\n                        sgd_classifier)\n\nplot_learning_curve(svm_clf,  X_train_he.values, y_train_he.values, title='SVM Classifier', cv=skf, n_jobs=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually, our SGDC with squared hinge loss and RBF low-rank approximation kernel since the beginning of the curve(37K training size) almost does not benefit from a bigger training set.\nWe may exploit such insight for further experiments by undersampling data.","metadata":{}},{"cell_type":"markdown","source":"### Exact SVM on undersampled data","metadata":{}},{"cell_type":"markdown","source":"80K represents a reasonable size for fitting an exact SVM, therefore in the following experiment, we fit an RBF SVM classifier without approximations on top of about 80K samples from an undersampled dataset. We will, therefore, random undersample data by taking 15k training instances except for minority classes: 3, 4 and 6 for which we will consider all the available samples.","metadata":{}},{"cell_type":"code","source":"def dict_rus(y):\n    ratios = {0:15000, 1:15000, 2:15000, 6:15000}\n    ratios[3] = len(y[y == 3]) \n    ratios[4] = len(y[y == 4]) \n    ratios[5] = len(y[y == 5]) \n    return ratios","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nprint('Undersampled class label distribution:')\nrus = RandomUnderSampler(sampling_strategy=dict_rus, random_state=42)\nsvm_clf = svm.SVC(C=1000, gamma=0.1)\nsklearn_pca = sklearnPCA(n_components=30)\nscaler = StandardScaler()\nrus_x = scaler.fit_transform(X_train_he)\nrus_x = sklearn_pca.fit_transform(rus_x)\nrus_x, rus_y = rus.fit_resample(rus_x, y_train_he)\n\n_ = plt.hist(rus_y+1, bins=7, alpha=0.7, ec='k')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparamter tuning <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"markdown","source":"<br>In order to tune $C$ and $\\gamma$ hyperparameters faster we manaully search them on svm by means of cross-validation on top of the undersampled balanced set (2178 samples for each class).\n<br>Consequently, in test phase we train a svm classifier with chosen parameters on about 80k training set and evaluate it on the test split.","metadata":{}},{"cell_type":"code","source":"scores = {}\nscores['test_f1'] = np.array([])\nscores['test_acc'] = np.array([])\nfor tr_idxs, val_idxs in skf.split(X_train_he, y_train_he):\n    X_train, y_train = X_train_he.values[tr_idxs], y_train_he.values[tr_idxs]\n    X_val, y_val = X_train_he.values[val_idxs], y_train_he.values[val_idxs]\n    \n    rus = RandomUnderSampler(random_state=42)\n    svm_clf = svm.SVC(C=1000, gamma=0.1)\n    sklearn_pca = sklearnPCA(n_components=30)\n    scaler = StandardScaler()\n    rus_x = scaler.fit_transform(X_train)\n    rus_x = sklearn_pca.fit_transform(rus_x)\n    rus_x, rus_y = rus.fit_resample(rus_x, y_train)\n    svm_clf.fit(rus_x, rus_y)\n    rus_test = scaler.transform(X_val)\n    rus_test = sklearn_pca.transform(rus_test)\n    y_pred = svm_clf.predict(rus_test)\n    svm_clf.fit(rus_x, rus_y)\n    f1 = f1_score(y_val, y_pred, average='macro')\n    acc = accuracy_score(y_val, y_pred)\n    scores['test_f1'] = np.append(scores['test_f1'], f1) \n    scores['test_acc'] = np.append(scores['test_acc'], acc)\n    \nprint_cv_scores(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testing best parameters <a class=\"tocSkip\"/>","metadata":{}},{"cell_type":"code","source":"rus = RandomUnderSampler(sampling_strategy=dict_rus, random_state=42)\npreprocessor = StandardScaler()\nsvm_clf = svm.SVC(C=1000, gamma=0.1)\nsklearn_pca = sklearnPCA(n_components=30)\nrus_x = preprocessor.fit_transform(X_train_he)\nrus_x = sklearn_pca.fit_transform(rus_x)\nrus_x, rus_y = rus.fit_resample(rus_x, y_train_he)\n\nsvm_clf.fit(rus_x, rus_y)\n\nrus_test = preprocessor.transform(X_test_he)\nrus_test = sklearn_pca.transform(rus_test)\ny_pred = svm_clf.predict(rus_test)\nprint(conf_matrix(y_test_he, y_pred))\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_he, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_he, y_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we followed two possible approaches to deal with datasets proportions, kernel approximations with SGDC and exact SVMs trained on undersampled data. \nThe experiments with undersampled data, as we notice, lead us to better results, though with an inter-class f1-score standard deviation of 0.80. Further improvement could be made by using more training data or employing more complex undersampling methods in order to lose as little information as possible from the original data.","metadata":{}},{"cell_type":"markdown","source":"# Conclusions","metadata":{}},{"cell_type":"markdown","source":"Our models have captured many dependencies between features and labels, thus we probably have a low intrinsic irreducible error in forest data gathered, considering that we have reached about 0.95 f1-score and 0.98 accuracy.\nMoreover, dataset size helps us to avoid high variance errors arising from overfitting except for models with a lot of learning capacity, such as ensemble learning models.","metadata":{}},{"cell_type":"markdown","source":"## Models comparison","metadata":{}},{"cell_type":"markdown","source":"Almost all classifiers except SVM and XGBoost have allowed us to obtain relatively good results since baseline models, whereas the previously mentioned ones make hyperparameter optimization definitely harder. Moreover, we have seen that tree models and KNN benefits a lot from *complex non-linear dependencies*, whereas linear classifiers (logistic regression and SVC) cannot. Next, we summarize the overall results obtained using the chosen metrics in the following bar chart:","metadata":{}},{"cell_type":"code","source":"acc = [0.620, 0.970, 0.945, 0.970, 0.969, 0.972, 0.977, 0.826]\nf1 =  [0.541, 0.944, 0.915, 0.946, 0.945, 0.946, 0.955, 0.799]\nmodel = [\"SMOTE polynomial LR\", \"pca KNN\",\n         \"SMOTE Decision Tree\", \"Bagging\", \"Random Forest\",\n         \"XGBoost\", \"XGBoost FE\", \"RUS SVM\"\n        ]\ndata = pd.DataFrame(data = {'classifier': model, 'accuracy': acc, 'f1-score':f1})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1 = px.bar(data, x='classifier' , y='f1-score', color='f1-score',\n             labels={'f1-score':'f1-score'}, height=400)\nfig2 = px.bar(data, x='classifier' , y='accuracy', color='accuracy',\n             labels={'accuracy':'accuracy'}, height=400)\n\ntrace1 = fig1['data'][0]\ntrace2 = fig2['data'][0]\nfig = make_subplots(rows=2, cols=1)\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=2, col=1)\nfig.update_layout(height=700)\nfig.update_yaxes(title_text=\"f1-score\", row=1, col=1)\nfig.update_yaxes(title_text=\"accuracy\", row=2, col=1)\npio.show(fig, renderer='notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The models which achieve the highest f1-score on test split are all the ensembling models and KNN; XGboost was slightly better than the rest, nevertheless making explicit feature interaction allowed us to reach higher f1-score and accuracy in predicting the forest cover.","metadata":{}},{"cell_type":"markdown","source":"## Further improvement","metadata":{}},{"cell_type":"markdown","source":"- It would be worth it to try different strategies in handling outliers, rather than simply leave them as they are.\n- Throughout the model selection, the most challenging task was dealing with unbalanced data. Finer tuning of parameters seems the only way to obtain a lower standard deviation in f1-score macro among all classes and consequently a more unbiased classifier. <br>In this direction could be reasonable a better tuning of class weights or resampling ratios by means of cross-validation. Another possible countermeasure could be choosing a metric that penalizes classifiers with higher inter-class f1-score standard deviation.<br>For this reason, our best classifier performs worst than average in detecting class 4 and 5 instances as we can see from XGBoost test confusion matrix and although several attempts were tried in order to help the model to improve its sensitivity, these have led to higher sensitivity for these classes at the expenses of precision, hence lowering their overall f1-scores.\n- Further new variables could be added on the XGBoost model like soil characteristics as the ones in soil descriptions in EDA (rubbly, very stony, extremely stony, etc.), categorical interactions or even less meaningful interaction in a brute-force approach. Best predictors can be further selected by examining feature importance and removing the less meaningful ones in an attempt to keep the dataset with a manageable number of columns. ","metadata":{}},{"cell_type":"markdown","source":"# References ","metadata":{}},{"cell_type":"markdown","source":"1. Boslaugh, Sarah: Statistics in a Nutshell. O'Reilly (2012)\n1. James, G., Witten, D., Hastie, T., Tibshirani, R.: An Introduction to Statistical Learning. Springer (2013)\n1. https://www.e-education.psu.edu/geog480/node/490\n1. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n1. https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n1. https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html\n1. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n1. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n1. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n1. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n1. https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n1. https://xgboost.readthedocs.io/en/latest/python/python_api.html\n1. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n1. http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n1. https://scikit-learn.org/stable/modules/kernel_approximation.html","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}