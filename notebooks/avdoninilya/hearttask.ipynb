{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\nfrom catboost import CatBoostClassifier\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataframe was parsed correctly. All fields has numeric data type (int64/float64). We don't see missed data. <br>\nFind out which features are numeric and which one are categorical.","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    print(f\"{col}: {len(df[col].value_counts())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = [\"age\",\n            \"trestbps\",\n            \"chol\",\n            \"thalach\",\n            \"oldpeak\",\n            \"ca\"]\n\ncat_cols = [\"sex\",\n            \"cp\",\n            \"fbs\",\n            \"restecg\",\n            \"exang\",\n            \"slope\",\n            \"thal\"]\n\nX_cols = num_cols + cat_cols\ntarget_col = \"target\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[X_cols]\ny = df[target_col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking balance: classes are balanced\ndf[target_col].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature overview","metadata":{}},{"cell_type":"code","source":"f, axs = plt.subplots(2, 3, figsize=(12, 4))\nfor i, col  in enumerate(num_cols):\n    df[col].hist(ax=axs.reshape(-1)[i], bins=10)\n    axs.reshape(-1)[i].set_title(col)\n\nf.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(2, 3, figsize=(10, 8))\nfor i, col  in enumerate(num_cols):\n    sns.boxplot(y=col, x=\"target\", data=df,  orient='v', ax=axs.reshape(-1)[i])\n\nf.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, x_vars=set(num_cols), y_vars=set(num_cols), hue=target_col);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"ca\", \"oldpeak\", \"thalach\" seems to be significant features","metadata":{}},{"cell_type":"code","source":"f, axs = plt.subplots(3, 3, figsize=(10, 8))\nfor i, col in enumerate(cat_cols):\n    sns.countplot(x=col, data=df, hue=target_col, ax=axs.reshape(-1)[i])\nf.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"cp\", \"thal\" seems to be significant features","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 12))  \nsns.heatmap(np.round(df.corr(), 2), annot=True, ax=ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"trestbps\", \"chol\", \"fbs\", \"restecg\" columns seems to be useless for classification because they are not correlate with target and there were no correlations with other features in pairplot.","metadata":{}},{"cell_type":"markdown","source":"\"ca\", \"oldpeak\", \"thalach\", \"cp\", \"thal\" are correlated with target","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# categorical columns \"sex\", \"fbs\", \"exang\" are already encoded\nohe_cols = [\"cp\", \"restecg\", \"slope\", \"thal\"]\n\nclass OHE_Transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, ohe_cols):\n        self.ohe_cols = ohe_cols\n        self.ohe = OneHotEncoder(sparse=False)\n\n    def fit(self, X, y=None):\n        self.ohe.fit(X[self.ohe_cols])\n        return self\n\n    def transform(self, X):\n        encodedX = self.ohe.transform(X[self.ohe_cols])\n        encoded_cols = self.ohe.get_feature_names(self.ohe_cols)\n        encodedX_df = pd.DataFrame(encodedX, columns=encoded_cols, index=X.index)\n        newX = pd.concat([X[X.columns.difference(self.ohe_cols)], encodedX_df], axis=1)\n        return newX\n\nclass ColumnNameSaver(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if hasattr(X, \"columns\"):\n            self.cols = X.columns\n        else:\n            self.cols = [f\"col_{i}\" for i in range(X.shape[1])]\n        return X\n\ndef ColumnRemoveFunc(X, remove_cols=None):\n    if remove_cols is None:\n        return X\n    \n    newX = X[X.columns.difference(remove_cols)]\n    return newX\n    \ndef ColumnRemoveTransformer(remove_cols):\n    return FunctionTransformer(ColumnRemoveFunc, kw_args={\"remove_cols\":remove_cols})\n    \ndef ShowLogregCoef(pipe):\n    cols = pipe[\"cols\"].cols\n    coefs = pipe[\"logreg\"].coef_.reshape(-1)\n    coefs = np.abs(np.round(coefs, 4))\n    a = pd.DataFrame(coefs, index=cols)\n\n    fig, ax = plt.subplots(figsize=(4, 8)) \n    sns.heatmap(a, annot=True, ax=ax)\n\ndef PrintRocAucScore(model, X, y):\n    roc_aucs = cross_val_score(model, X, y, cv=10, scoring=\"roc_auc\", n_jobs=-1)\n    print(f\"scores: {' '.join(roc_aucs.round(5).astype('str'))}\")\n    print(f\"avg: {roc_aucs.mean().round(5)}\")\n    print(f\"std: {roc_aucs.std().round(5)}\")\n\ndef ShowRocCurvePlot(model, X_train, X_test, y_train, y_test):\n    y_test_proba = model.predict_proba(X_test)[:, 1]\n    y_train_proba = model.predict_proba(X_train)[:, 1]\n\n    train_auc = metrics.roc_auc_score(y_train, y_train_proba)\n    test_auc = metrics.roc_auc_score(y_test, y_test_proba)\n\n    plt.figure(figsize=(10,7))\n    plt.plot(*metrics.roc_curve(y_train, y_train_proba)[:2], label='train AUC={:.4f}'.format(train_auc))\n    plt.plot(*metrics.roc_curve(y_test, y_test_proba)[:2], label='test AUC={:.4f}'.format(test_auc))\n    legend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cluster analysis","metadata":{}},{"cell_type":"code","source":"clustering_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler())]\n)\nXPreprocessed = clustering_pipeline.fit_transform(X)\nXPreprocessed = pd.DataFrame(XPreprocessed, columns=clustering_pipeline[\"cols\"].cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Z = linkage(XPreprocessed, 'ward')\nfig = plt.figure(figsize=(25, 10))\ndn = dendrogram(Z, color_threshold=23)\nplt.axhline(y=35, linestyle='--', color='b', label=\"main clusters\") \nplt.axhline(y=23, linestyle='--', color='r', label=\"outliers\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 2 small outlier clusters and 2 main clusters","metadata":{}},{"cell_type":"code","source":"df[\"main_cluster\"] = fcluster(Z, 35, criterion=\"distance\")\ndf[\"outlier_cluster\"] = fcluster(Z, 23, criterion=\"distance\")\ndf[\"outlier_cluster\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster 1 and 4 are outliers. Remove them from the data.","metadata":{}},{"cell_type":"code","source":"df.drop(df[df[\"outlier_cluster\"].isin([1, 4])].index, inplace=True)\ndf.drop(columns=\"outlier_cluster\", inplace=True)\n# update X and y\nX = df[X_cols]\ny = df[target_col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(\"main_cluster\").mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = df.groupby(\"main_cluster\").mean()\nb = df.describe()\nclusters_df = pd.concat([df.groupby(\"main_cluster\").mean(), df.describe()], axis=0)\nclusters_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clusters could be separated by features: cp, exang, oldpeak, slope, ca, thalach, thal. <br>\nCluster1 tends to be target class 0 (target mean = 0.21) <br>\nCluster2 tends to be target class 1 (target mean = 0.77) <br>\nMaybe there are more than 2 clusters but we don't have enough data to find them.","metadata":{}},{"cell_type":"code","source":"clusters_df[[\"cp\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thalach\", \"thal\", \"target\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LogisticRegression","metadata":{}},{"cell_type":"code","source":"logreg_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler()),\n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ShowLogregCoef(logreg_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all we should remove useless features gathered from categorical features after OHE (1 from each feature)\nbecause it could be calculated using others. These features don't give us any additional information.\nSo, removing \"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\" and rerun the model.","metadata":{}},{"cell_type":"code","source":"logreg_pipeline = Pipeline(\n    \n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler()),\n     \n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are still several features with low coef: \"age\", \"fbs\", \"slope_2\", \"thal_2\". Try SVD method to ensure.","metadata":{}},{"cell_type":"code","source":"preprocessing_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"cols\", ColumnNameSaver()),\n     (\"scaler\", StandardScaler())]\n)\nXPreprocessed = preprocessing_pipeline.fit_transform(X, y)\nXPreprocessed\n\nU, S, V = np.linalg.svd(XPreprocessed)  # svd разложение \nprint(np.round(S, 2))\nplt.grid()\nplt.axis([0, len(S) + 1, -1, 38])\nplt.plot(range(1, len(S) + 1), S, '--o')\nplt.axvline(x=14, color='g', linestyle='--', label=\"little info threshold\")\nplt.axvline(x=17, color='r', linestyle='--', label=\"useless threshold\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVD analysis confirms that removed features were totally useless. There is also a small step around N=14, so 3 more features could be removed too. Moreover, we could see that there is one feature which seems to be the most importnant (\"ca\"). The optimal feature number is between 13 and 17.","metadata":{}},{"cell_type":"code","source":"logreg_pipeline = Pipeline(\n    [(\"ohe\", OHE_Transformer(ohe_cols)),\n     (\"rmcol\", ColumnRemoveTransformer([\"cp_1\", \"restecg_0\", \"slope_0\", \"thal_1\"])),\n     (\"scaler\", StandardScaler()),\n     (\"pca\", PCA(n_components=14)),\n     (\"cols\", ColumnNameSaver()),\n     (\"logreg\", LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"saga\"))]\n)\n\nlogreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing logreg params\nparams = [\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"l2\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [None],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    },\n\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"l1\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [None],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    },\n\n    {\n        \"logreg__C\": [100, 10, 1, 0.1, 0.01, 0.001],\n        \"logreg__penalty\": [\"elasticnet\"],\n        \"logreg__class_weight\": [\"balanced\", None],\n        \"logreg__l1_ratio\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n        \"pca__n_components\": [13, 14, 15, 16, 17],\n    }\n]\n\n\nlogreg_grid = GridSearchCV(logreg_pipeline, params, cv=10, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\nlogreg_grid.fit(X, y)\nprint(logreg_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {\n    'logreg__C': 0.01, \n    'logreg__class_weight': 'None', \n    'logreg__l1_ratio': None, \n    'logreg__penalty': 'l2', \n    'pca__n_components': 13}\n\nlogreg_pipeline.set_params(**best_params)\nPrintRocAucScore(logreg_pipeline, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\nlogreg_pipeline.fit(X_train, y_train)\nShowRocCurvePlot(logreg_pipeline, X_train, X_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_pipeline.fit(X, y)\nShowLogregCoef(logreg_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean ROC AUC score on cv is ~0.91. Weights shows that there is 1 main feature. Others don't influence much on target.","metadata":{}},{"cell_type":"markdown","source":"# Catboost","metadata":{}},{"cell_type":"code","source":"# we don't preprocess categorical features here somehow, because catboost is able to do it better\ncatboost_pipeline = Pipeline(\n    [(\"cb\", CatBoostClassifier(loss_function='Logloss',\n                               verbose=False,\n                               cat_features=cat_cols,\n                               random_seed = 42,\n                               eval_metric='AUC'))]\n)\n\nPrintRocAucScore(catboost_pipeline, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"cb__depth\": [4, 6, 8],\n    \"cb__learning_rate\": [0.01, 0.1, 1],\n    \"cb__l2_leaf_reg\": [0.1, 1, 10, 50],\n    \"cb__iterations\": [100, 200, 400],\n}\n\ncb_grid = GridSearchCV(catboost_pipeline, params, cv=5, verbose=4, scoring='roc_auc', refit=True, n_jobs=-1)\ncb_grid.fit(X, y)\nprint(cb_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {\"cb__depth\": 4, \n               \"cb__iterations\": 100, \n               \"cb__l2_leaf_reg\": 10, \n               \"cb__learning_rate\": 0.1}\n\ncatboost_pipeline.set_params(**best_params)\nPrintRocAucScore(catboost_pipeline, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mean ROC AUC score on cv is ~0.91","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\ncatboost_pipeline.fit(X_train, y_train)\nShowRocCurvePlot(catboost_pipeline, X_train, X_test, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we don't have much data (only 303 samples) it is hard to estimate quality of models. Depending on the split we could get underfitting or overfitting roc curves. The avarage ROC AUC of both models is about 0.91","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}