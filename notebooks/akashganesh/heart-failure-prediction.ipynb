{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\n\nThe aim of this project to collect clinical data and predict the possibilty of heart failure based on certain health parameters. \nAlong the way, certain data insights are also demonstrated to get a clear picture of the data before making predictions.\nIn the project, we will use 3 main ML models namely - Logistic Regression , KNN and Decistion Tree Classfier to determine the accuracy of the prediction"},{"metadata":{},"cell_type":"markdown","source":"# Initialization"},{"metadata":{},"cell_type":"markdown","source":"**Importing necessary packages**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting feature importances**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=15,6\nsns.set_style('darkgrid')\n\nx=df.iloc[:,:-1]\ny=df.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_imp=pd.Series(model.feature_importances_,index=x.columns)\nfeat_imp.plot(kind='barh')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the given data we select only 4 factors - Age ,Time, Serum Creatinine and Ejection Fraction for our analysis**"},{"metadata":{},"cell_type":"markdown","source":"# Data Insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box Plot for Ejection Fraction\nsns.boxplot(df['ejection_fraction'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find that there are two outliers in the above boxplot. Therefore we remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ejection_fraction']=df[df['ejection_fraction']<70]\nsns.boxplot(df['ejection_fraction'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that the outliers have been removed**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for age\nsns.boxplot(df['age'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No outliers in age"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Age\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = df['age'],\n    xbins=dict( # bins used for histogram\n        start=40,\n        end=95,\n        size=2\n    ),\n    marker_color='#e8aa60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Age Distribution',\n    xaxis_title_text='Age',\n    yaxis_title_text='Count', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    plot_bgcolor='#000000',\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False }\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets categorize the above histogram by DEATH_EVENT\n\nimport plotly.express as px\nfig = px.histogram(df, x=\"age\", color=\"DEATH_EVENT\", hover_data=df.columns)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Serum Creatinine\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = df['serum_creatinine'],\n    xbins=dict( # bins used for histogram\n        start=0.5,\n        end=9.4,\n        size=0.2\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Serum Creatinine Distribution',\n    xaxis_title_text='Serum Creatinine',\n    yaxis_title_text='Count', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    plot_bgcolor='#000000',\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False }\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram in comparison to DEATH_EVENT\n\nfig = px.histogram(df, x=\"serum_creatinine\", color=\"DEATH_EVENT\",marginal='violin', hover_data=df.columns)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Platelets\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = df['platelets'],\n    xbins=dict( # bins used for histogram\n        start=25000,\n        end=850000,\n        size=10000\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Platelets Distribution',\n    xaxis_title_text='Platelets',\n    yaxis_title_text='Count', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    plot_bgcolor='#000000',\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False }\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram of platelets as a function of DEATH_EVENT\n\nfig = px.histogram(df, x=\"platelets\", color=\"DEATH_EVENT\",marginal='violin', hover_data=df.columns)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['time'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(\n    x = df['time'],\n    xbins=dict( # bins used for histogram\n        start=4,\n        end=285,\n        size=5\n    ),\n    marker_color='#e8ab60',\n    opacity=1\n))\n\nfig.update_layout(\n    title_text='Time Distribution',\n    xaxis_title_text='Time',\n    yaxis_title_text='Count', \n    bargap=0.05, # gap between bars of adjacent location coordinates\n    plot_bgcolor='#000000',\n    xaxis =  {'showgrid': False },\n    yaxis = {'showgrid': False }\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=px.pie(df, values='diabetes',names='DEATH_EVENT', title='Diabetes VS Death Event',width=600, height=400)\nfig2=px.pie(df, values='DEATH_EVENT',names='diabetes',width=500, height=400)\n\nfig1.show()\n\nfig2.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above pie chart shows that 32% of people who have diabetes die of heart failure whereas 68% dont**\n**Also 58.3% of people who die of heart failure dont have diabetes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=px.pie(df, values='smoking',names='DEATH_EVENT', title='Smoking VS Death Event',width=600, height=400)\nfig2=px.pie(df, values='DEATH_EVENT',names='smoking',width=500, height=400)\n\nfig1.show()\n\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The first piechart shows that only 31.3% of the smokers die of heart failure**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=px.pie(df, values='high_blood_pressure',names='DEATH_EVENT', title='High BP VS Death Event',width=600, height=400)\nfig2=px.pie(df, values='DEATH_EVENT',names='high_blood_pressure',width=500, height=400)\n\nfig1.show()\n\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is also interesting to note that 59.4% of deaths related to heart failure occur to people without high blood pressure**"},{"metadata":{},"cell_type":"markdown","source":"# Training and testing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We select the following features\n\nFeatures=['time','ejection_fraction','serum_creatinine','age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\nx=df.iloc[:,[0,4,7,11]].values\ny=df.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data into train and test set\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test=np.nan_to_num(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying out different learning models"},{"metadata":{},"cell_type":"markdown","source":"1.Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(max_iter=10000)\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the value for the test set\n\ny_pred=classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making Confusion matrix and predicting accuracy score\n\nmylist=[]\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\nac=accuracy_score(y_test,y_pred)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using logistic regression, we get 85% accuracy**"},{"metadata":{},"cell_type":"markdown","source":"**2.K Nearest Neighbours**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the optimum number of neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nlist1=[]\nfor neighbors in range(1,10):\n    classifier=KNeighborsClassifier(n_neighbors=neighbors,metric='minkowski')\n    classifier.fit(x_train,y_train)\n    y_pred=classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(1,10)),list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=KNeighborsClassifier(n_neighbors=7,metric='minkowski')\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the confusion matrix and accuracy score\n\ncm=confusion_matrix(y_test,y_pred)\nac=accuracy_score(y_test,y_pred)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get 83% accuracy with KNN"},{"metadata":{},"cell_type":"markdown","source":"3.Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(x_test)\n\nmylist=[]\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\nac=accuracy_score(y_test,y_pred)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Decsion Tree Classifier we get 83.33% Accuracy"},{"metadata":{},"cell_type":"markdown","source":"**Among the three models tried above, we find that the logistic regression gives us the best possible accuracy of 85%**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}