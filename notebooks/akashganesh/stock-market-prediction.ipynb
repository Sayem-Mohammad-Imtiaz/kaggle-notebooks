{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndjia= pd.read_csv('../input/stocknews/upload_DJIA_table.csv',parse_dates=['Date'], index_col='Date')\ndjia.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts=djia['Open']\nplt.plot(ts)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences (we’ll see such cases later). So, more formally, we can check stationarity using the following:**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Time Series Stationarity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What is time series stationarity?**\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First of all we define time series(TS). A TS is a collection of data points collected at specific time intervals i.e they are time varying data.\nA time series is said to be stationary if its statistical properties such as mean, variance remain constant over time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Plotting Rolling Statistics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We can plot the moving average or moving variance and see if it varies with time. By moving average/variance I mean that at any instant ‘t’, we’ll take the average/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dickey Fuller Test","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We’ll be using the rolling statistics plots along with Dickey-Fuller test results a lot so I have defined a function which takes a TS as input and generated them for us. Please note that I’ve plotted standard deviation instead of variance to keep the unit similar to mean.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    # Determining Rolling Statistics\n    rolmean=timeseries.rolling(window=12).mean()\n    rolstd=timeseries.rolling(window=12).std()\n    \n    #Plot Rolling Statistics\n    orig=plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Plot Dickey-Fuller Test\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest=adfuller(timeseries , autolag='AIC')\n    dfoutput=pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key]=value\n    print (dfoutput)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Though the variation in standard deviation is small, mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Making the time series stationary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**To make a TS stationary, we must first understand what makes it non-stationary.There are 2 reasons**\n\nTrend and Seasonality","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Therefore to get a stationary TS we must identify and eliminate Trend and Seasonality**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Estimating and Eliminating Trend","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Here we use transformation. Using transformation such as log or square root function can penalize higher values more than smaller values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking log transformation\n\nts_log=np.log(ts)\n\nplt.plot(ts_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\n\nts_log = np.log(ts)\nfig = plt.figure(constrained_layout = True)\ngs_1 = gridspec.GridSpec(2, 3, figure = fig)\nax_1 = fig.add_subplot(gs_1[0, :])\nax_1.plot(ts_log)\nax_1.set_xlabel('Year')\nax_1.set_ylabel('Data')\nplt.title('Logarithmic time series')\n\nax_2 = fig.add_subplot(gs_1[1, :])\nax_2.plot(ts)\nax_1.set_xlabel('Year')\nax_1.set_ylabel('Data')\nplt.title('Original time series')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Moving Average**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this approach, we take average of ‘k’ consecutive values depending on the frequency of time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mov_avg = ts_log.rolling(window=12).mean()\nplt.plot(ts_log)\nplt.plot(mov_avg, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets, linear_model\n\nts_wi = ts_log.reset_index()\ndf_values = ts_wi.values\ntrain_y = df_values[:,1]\ntrain_y = train_y[:, np.newaxis]\ntrain_x = ts_wi.index\ntrain_x = train_x[:, np.newaxis]\nregr = linear_model.LinearRegression()\nregr.fit(train_x, train_y)\npred = regr.predict(train_x)\nplt.plot(ts_wi.Date, pred)\nplt.plot(ts_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Eliminating Trend**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Eliminating trends are absolutely necessary as TS are time dependent and developing a regression model requires stationarity.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are 3 ways to eliminate trends\n1. Aggregation - taking average of a time period\n2. Smoothing - taking rolling averages\n3. Polynomial fitting - taking a regression model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since we have already done moving averages, let us use it (smoothing) to eliminate the trends","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_log_moving_avg_diff= ts_log-mov_avg\nts_log_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_log_moving_avg_diff)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decomposing**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Seasonal decomposing is the fastest way to remove trend and seasonality components from a time serie to becoming it stationary.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log,freq=4, model='additive')\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ts_decompose = residual\nts_log_diff = ts_log - ts_log.shift(1)\nts_decompose = ts_log_diff\nts_decompose.dropna(inplace=True)\ntest_stationarity(ts_decompose)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forecasting the time series","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since the series is now stationary, we can perform forecasting on the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n2. A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Using ARIMA to forecast the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(ts_log_diff, nlags=20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method='ols')\n\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the ‘p’ and ‘q’ values as:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. p – The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=2.\n2. q – The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=2.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, lets make 2 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AR Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(ts_log, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MA Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(ts_log, order=(0, 1, 2))  \nresults_MA = model.fit(disp=-1)  \nplt.plot(ts_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that RSS for AR model is slightly better","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Taking it back to original scale","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_AR.fittedvalues, copy=True)\nprint (predictions_ARIMA_diff.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint (predictions_ARIMA_diff_cumsum.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}