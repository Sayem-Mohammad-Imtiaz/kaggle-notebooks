{"cells":[{"metadata":{},"cell_type":"markdown","source":"- The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/pima-indians-diabetes-database/diabetes.csv'\ndf = pd.read_csv(path)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What insights can we get out of this table?\n- Minimum of Plasma glucose concentration, Diastolic blood pressure, Triceps skinfold thickness, 2-Hour serum insulin and Body mass index equals to zero!!!\n- I don't know about Glucose, Insulin, SkinThickness but I do know that BloodPressure and BMI cannot be zero\n- So it's better to replace those 0's with Nans"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values\n- Insulin and SkinThickness have the highest percentage of missing values among others"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df[\"Glucose\"] == 0.0, \"Glucose\"] = np.NAN\ndf.loc[df[\"BloodPressure\"] == 0.0, \"BloodPressure\"] = np.NAN\ndf.loc[df[\"SkinThickness\"] == 0.0, \"SkinThickness\"] = np.NAN\ndf.loc[df[\"Insulin\"] == 0.0, \"Insulin\"] = np.NAN\ndf.loc[df[\"BMI\"] == 0.0, \"BMI\"] = np.NAN\ndf.isna().sum() / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression Imputation\n#### Random Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n\ndef random_imputation(df, feature):\n\n    number_missing = df[feature].isnull().sum()\n    observed_values = df.loc[df[feature].notnull(), feature]\n    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n    \n    return df\n\nfor feature in missing_columns:\n    df[feature + '_imp'] = df[feature]\n    df = random_imputation(df, feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Regression Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\nrandom_data = pd.DataFrame(columns = [\"Ran\" + name for name in missing_columns])\n\nfor feature in missing_columns:\n        \n    random_data[\"Ran\" + feature] = df[feature + '_imp']\n    parameters = list(set(df.columns) - set(missing_columns) - {feature + '_imp'})\n    \n    model = linear_model.LinearRegression()\n    model.fit(X = df[parameters], y = df[feature + '_imp'])\n    \n    #Standard Error of the regression estimates is equal to std() of the errors of each estimates\n    predict = model.predict(df[parameters])\n    std_error = (predict[df[feature].notnull()] - df.loc[df[feature].notnull(), feature + '_imp']).std()\n    \n    random_predict = np.random.normal(size = df[feature].shape[0], \n                                      loc = predict, \n                                      scale = std_error)\n    random_data.loc[(df[feature].isnull()) & (random_predict > 0), \"Ran\" + feature] = random_predict[(df[feature].isnull()) & \n                                                                            (random_predict > 0)]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the plots above that we have introduced some degree of variability into the variables and retained the native distribution as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig, axes = plt.subplots(nrows = 5, ncols = 2)\nfig.set_size_inches(16, 10)\n\nfor index, variable in enumerate(missing_columns):\n    sns.histplot(df[variable].dropna(), kde = False, ax = axes[index, 0])\n    sns.histplot(random_data[\"Ran\" + variable], kde = False, ax = axes[index, 0], color = 'red')\n    axes[index, 0].set(xlabel = variable + \" / \" + variable + '_imp')\n    \n    sns.boxplot(data = pd.concat([df[variable], random_data[\"Ran\" + variable]], axis = 1),\n                ax = axes[index, 1])\n    \n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our new dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df = pd.concat([random_data, df[['Outcome', 'Pregnancies', 'Age', 'DiabetesPedigreeFunction']]], axis=1)\ncols = imputed_df.columns\n# reorder columns in the dataset\nimputed_df = imputed_df[cols[:5].tolist() + cols[6:].tolist() + cols[5:6].tolist()]\ncols = imputed_df.columns\nimputed_df.columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] + cols[5:].tolist()\nimputed_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Variance among features\n- Insulin feature has a really high variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df[imputed_df.columns[:-1]].var().plot(kind='bar', figsize=(16,4), colormap='Set2',\n                                          xlabel='Features', ylabel='Variance', title='Variance among features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis\n#### Pregnancies feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,1, figsize=(14,5))\n\ng = sns.countplot(x='Pregnancies', hue='Outcome', data=imputed_df, palette=sns.color_palette(), ax=axes);\ng.set_title('Pregnancies by Outcome');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Box plots \n- It seems Insulin and Glucose have the most effect on the outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_metrics = [['Glucose','BloodPressure'], ['SkinThickness', 'Insulin'], ['BMI', 'Age']]\n\ndef box_plot_func(axes, metric):\n    bp_dict = imputed_df.boxplot(column=f'{metric}', by='Outcome', fontsize=12, ax=axes,\\\n                         vert=False, return_type='both', patch_artist = True);\n    # colors for boxplots\n    colors = ['b','r']\n    for row_key, (ax,row) in bp_dict.iteritems():\n        ax.set_xlabel('')\n        for i,box in enumerate(row['boxes']):\n            box.set_facecolor(colors[i])\n    \n    ax.set_title(\"Boxplot of \" + f\"{metric}\")\n    ax.set_xlabel(f\"{metric}\")\n    ax.set_ylabel('Rent amount')\n    plt.suptitle(\"\")\n\n    \nfig, axes = plt.subplots(3,2, figsize=(18,20))\nfor i in range(3):\n    for j in range(2):\n        box_plot_func(axes[i,j], list_of_metrics[i][j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pearson correlation for binary categorical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax) = plt.subplots(1, figsize=(18,8))\nsns.heatmap(imputed_df.corr(), ax=ax, annot = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of our features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axes) = plt.subplots(2, 4, figsize=(18,6)) \ncol = 0\nfor i in range(2):\n    for j in range(4):\n        sns.histplot(imputed_df[imputed_df.columns[col]], ax=axes[i,j], color='blue',  kde=True)\n        col += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Proportions of 1's and 0's\n- Dataset is skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df.groupby('Outcome')['Outcome'].size().plot(kind='pie', autopct='%1.1f%%');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/dev/test "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX, y = imputed_df.drop('Outcome', axis=1), imputed_df['Outcome']\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n# x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, stratify=y_temp, test_size=0.5)\nprint('Train', x_train.shape, y_train.shape)\n# print('Development', x_valid.shape, y_valid.shape)\nprint('Test', x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature selection\n- f_classification\n- Chi2\n- Mutual Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import chi2, f_classif, mutual_info_classif, SelectKBest, SelectPercentile\n\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, scoring):\n        valid_scoring = {\n            'f_classif': f_classif,\n            'chi2': chi2,\n            'mutual_info_classif': mutual_info_classif\n        }\n        \n        self.selection = SelectPercentile(\n            valid_scoring[scoring], percentile=int(n_features * 100))\n        \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n\n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n\nufs = UnivariateFeatureSelction(n_features=0.1,scoring=\"chi2\")\n\nufs.fit(x_train, y_train)\nx_train_transformed = ufs.transform(x_train)\n# x_valid_transformed = ufs.transform(x_valid)\nx_test_transformed  = ufs.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import cross_val_score, KFold\n\nclf = make_pipeline(StandardScaler(), LogisticRegression())\nroc_auc_score = cross_val_score(clf, x_train_transformed, y_train, cv=10, scoring='roc_auc')\nroc_auc_score_min = np.min(roc_auc_score)\nroc_auc_score_max = np.max(roc_auc_score)\nroc_auc_score_mean = np.mean(roc_auc_score)\nprint(f'Min score: {roc_auc_score_min}, Max score: {roc_auc_score_max}, Mean: {roc_auc_score_mean}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of all models"},{"metadata":{"trusted":true},"cell_type":"code","source":"### empty list\nclfs = []\n\nclfs.append((\"RandomForestClassifier\",\n             Pipeline([(\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\",\n             Pipeline([(\"GradientBoosting\", GradientBoostingClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\",\n             Pipeline([(\"DART\", DecisionTreeClassifier())])))\n\nclfs.append((\"LogisticRegression\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogisticRegression\", LogisticRegression())]))) \n\nclfs.append((\"SVM\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"SVM\", SVC())]))) \n\n### Metrics\n# roc_auc\nscoring = 'roc_auc'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds)\n    cv_results = cross_val_score(model, x_train_transformed, y_train, cv= kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n#     msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n#     print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(16,6))\nfig.suptitle('Classification Algorithm Comparison', fontsize=14)\nax = fig.add_subplot(111)\nsns.boxplot(data=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithm\", fontsize=14)\nax.set_ylabel(\"ROC-AUC of Models\", fontsize=14)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}