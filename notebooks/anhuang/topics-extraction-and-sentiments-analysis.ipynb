{"cells":[{"metadata":{"_uuid":"e71accc862ca88af738b297c0a78f19c1279f47c","_cell_guid":"6ee65d6b-d2ae-41db-aea4-c76168ea2951"},"cell_type":"markdown","source":"# Abstract\nThis data provides about 1.1 million news headlines and their publish dates from 2003 to 2017. \nDifferent big events happened every year,  such as Iraq War in 2003 and Finacial Crisis started from 2008.  These kinds of events are hot topics which should be  published in huge numbers. This would be reflected in the numbers of headlines that contain them. Different headlines that reported different  events may also contain different sentiments, So, there might be some correlations between events and sentiment.\n \ngoal\n1.  Finding each year's 20 most frequent  words. \n2. Using Vader Sentiment Analysis Tool in NLTK to do sentiment analysis in each year's headlines.\n3. Plotting graphs to show how the hot topics change with time and how they impacts the sentiment. "},{"metadata":{"_uuid":"edd4d89d426372c7c9923ffe860b16476816e39a","_cell_guid":"11a3682d-34b6-414a-8ba3-7e6df7c4a3d2"},"cell_type":"markdown","source":"# Step1.Input Data\n\nSince publish data contains month and day that are unnecessary and slow down our calculation, we use \"text['publish_date']/10000\" and \"text['publish_date'] = text['publish_date'].astype(int)\" to cut down them. \n\n"},{"metadata":{"_uuid":"9cb0bce33b3e078eaf9bb593f265bf2ea13133b1","_kg_hide-input":false,"_kg_hide-output":false,"_cell_guid":"ac9fffef-d727-4851-912b-ad80e958abcf","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\nimport random \nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\n\ntext = pd.read_csv('../input/abcnews-date-text.csv')\ntext['publish_date'] = text['publish_date']/10000\ntext['publish_date'] = text['publish_date'].astype(int)\ntext.head()\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"9faf3da206d4d640043910c4fef0f8523e113a7b","_cell_guid":"8e703298-9ec3-4bcb-ba41-5b6ceb62845a"},"cell_type":"markdown","source":"# Step2. Seekinging 20 Most Frequent Word Each Year\n\nSpliting each year's headlines into words and filtering them with stopwords. Then lemmatizing and appending  them into lists.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"stop_words = stopwords.words(\"english\")\nlemmatizer = WordNetLemmatizer()\nresult = []\nstart = 0\nend = 0\nfor i in range(2003,2018):\n    word_list = {}\n    temp = text.loc[text['publish_date']==i]\n    temp = temp['headline_text']\n    start = end\n    lenth = len(temp)\n    end = end + lenth\n    for j in range(start,end):\n        token = temp[j].split()\n        for w in token:\n            if w not in stop_words:\n                w = lemmatizer.lemmatize(w)\n                if w not in word_list:\n                    word_list[w]=1\n                else:\n                    word_list[w]+=1\n    count_list = sorted(word_list.items(),key = lambda x:x[1],reverse = True)\n    temp_list = list(zip(*count_list[0:20]))\n    result.append(list(temp_list[0]))\n    print(i)\n    print(count_list[0:20])","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9cb0417c29d8d83c7be1e3798729f6754799d5ca","_cell_guid":"471ed485-cb05-465b-9885-21d7449ed1b1"},"cell_type":"markdown","source":"# Step3. Sentiment Analysis Each Year\nUsing vader sentiment tool in nltk to analyze each headline's sentimental components and counted the annual average values. "},{"metadata":{"_uuid":"d5d57ad7adab679ad1aebb821c3b1a6d0c6044d7","_cell_guid":"ee944644-9b88-4ae3-b546-6ef3445461ef","trusted":true},"cell_type":"code","source":"se = SentimentIntensityAnalyzer()\nneg_change = []\nneu_change = []\npos_change = []\ncompound_change = []\nstart = 0\nend = 0\nfor i in range(2003,2018):\n    temp = text.loc[text['publish_date']==i]\n    temp = temp['headline_text']\n    start = end\n    lenth = len(temp)\n    end = end + lenth\n    neg = 0.0\n    pos = 0.0\n    neu = 0.0\n    compound = 0.0\n    for j in range(start,end):\n        Sentiment = se.polarity_scores(temp[j])\n        neg = neg + Sentiment['neg']\n        neu = neu + Sentiment['neu']\n        pos = pos + Sentiment['pos']\n        compound = compound + Sentiment['compound']\n    neg_change.append(neg/lenth)\n    pos_change.append(pos/lenth)\n    neu_change.append(neu/lenth)\n    compound_change.append(compound/lenth)\n    print(i)\n    print('neg:%-6.3f,neu:%-6.3f,pos:%-6.3f,compound:%-6.3f'%(neg/lenth,neu/lenth,pos/lenth,compound/lenth))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"614a82366eda763f440f41a797c03559b41c7e5d","collapsed":true,"_cell_guid":"bbb476d1-ba2c-4207-8621-4727ecf6a1c3"},"cell_type":"markdown","source":"# Step4. Visualization and Analysis\n"},{"metadata":{"_uuid":"57d1fbb84a249feb9524012e03f4a8949bbdb1c9","scrolled":true,"_cell_guid":"dcfe9b79-e0cd-49e4-ac5a-b53482f202df","trusted":true},"cell_type":"code","source":"year = [i for i in range(2003,2018)]\n\nstack_bottom = []\nfor i in range(0,len(neg_change)):\n    stack_bottom.append(neg_change[i] + neu_change[i])\nb1 = plt.bar(year, neg_change)\nb2 = plt.bar(year, neu_change, bottom = neg_change)\nb3 = plt.bar(year, pos_change, bottom = stack_bottom)\n\nfor i in year:\n    k = i-2003\n    for j in range(0,20):\n        plt.text(i-0.3,0.85-0.03*(j+1) ,result[k][j])\nplt.title('Sentiment Change Bars')\nplt.xlabel('years')\nplt.ylabel('sentiment rate')\nplt.legend([b1,b2,b3],['neg','neu','pos'])\nplt.gcf().set_size_inches(18,10)\nplt.show()\n\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70f7a96dd96efa0f4faf98693b20ec675285ce69"},"cell_type":"code","source":"year = [i for i in range(2003,2018)]\nl1 = plt.plot(year,neg_change,label='neg')\nl2 = plt.plot(year,neu_change,label='neu')\nl3 = plt.plot(year,pos_change,label='pos')\nfor i in year:\n    k = i-2003\n    for j in range(0,20):\n        plt.text(i-0.2,0.85-0.03*(j+1) ,result[k][j])\nplt.title('Sentiment Change Curves')\nplt.xlabel('years')\nplt.ylabel('sentiment rate')\nplt.legend([b1,b2,b3],['neg','neu','pos'],loc='lower left')\nplt.gcf().set_size_inches(18,10)\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"dca0a7728faa5e6af4a76c05c3a2d7be7926e317"},"cell_type":"markdown","source":"As we can see, there are some topics are constantly published every year, such as police and govt\n. \nSome topics only appear in certain years, like:\n1. 'iraq' in 2003 and 2004 when iraq war erupted\n2. After the Finacial crisis in 2008, 'interview' is always in a high place until 2014.\n3. 'election' in 2016 and 'trump' in 2017.\n\nSentiment analysis;\nThe negative sentiment tends to increase from 2003 and reaches its peak in 2009. Then it starts to decrease until 2014. "}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}