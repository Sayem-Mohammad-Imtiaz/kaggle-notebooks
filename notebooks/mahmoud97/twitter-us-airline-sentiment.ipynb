{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nTwitter US Airline Sentiment competition:\nAnalyze how travelers in February 2015 expressed their feelings on Twitter.\n\nThe aim from this competition to classify this Tweets to  “positive”, “neutral”, or “negative” feelings using ML,DL models.\n\nThis kernel consists of the following parts :\n\n  -Explore data\n\n  -Data Preprocessing\n\n  -ML model\n\n  -DL model"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n# import needed libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\n\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score  # Perform classification with SVM, kernel=linear\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading data\ndata = pd.read_csv(\"/kaggle/input/twitter-airline-sentiment/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1-Explore Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the shape of the data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns and corresponding data types and null values ...\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a  samples of the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_counts = data.airline_sentiment.value_counts()\nnumber_of_tweets = data.tweet_id.count()\nprint(sentiment_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualize some columns that correlated to airline_sentiment 'label'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the values of airline_sentiment and count for each value ...\nprint(data['airline_sentiment'].value_counts())\ndata['airline_sentiment'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As noticed above *negative* tweets have the most count, this may lead to unbalancing in training process"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the airlines and count for eache one\nprint(data['airline'].value_counts())\ndata['airline'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"United airline have most account, other airline have little differences.\n\nThis visualize not lead to good information about the relationship between airline and the sentiment of tweets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the airlines with airline_sentiment\ndata.groupby(['airline', 'airline_sentiment']).size().unstack().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above visualization may giving more useful information, where we can notice that there is an ailines such as United airline have the most negative Tweets "},{"metadata":{},"cell_type":"markdown","source":"\n\n\nIf we look at 'negativereason' column we can show what is the most reason count, then what is the most reason that responsible on negative reason"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the 'negativereason' values counts\ndata['negativereason'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The 'negative_reason' feature  have value when the sentiment is negative, and NAN for other sentiments.\ndata.groupby(['negativereason', 'airline_sentiment']).size().unstack().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers Services Issue is the most negative reason that lead to negative feeling in travellers tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing ...\n\"\"\" \n-Remove punctuations, special characters. (only letters still in the text)\n-Tokenizing text\n-Convert words to lower case\n-Remove stopwords\n-Lemmatization\n\"\"\"\n\n# from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(tweet):\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n    tokenized_words = only_letters.split()\n    # to lower case\n    words_lc = [l.lower() for l in tokenized_words]\n    # remove stopwords\n    clean_words = [w for w in words_lc if w not in stop_words]\n\n    lemmatize_words = [wordnet_lemmatizer.lemmatize(t) for t in clean_words]\n    final_text=''.join(w+\" \" for w in lemmatize_words)\n    return final_text\n\n# add new columns ...\ndata['cleaned_tweet'] = data.text.apply(clean_data)\ndata[['text', 'cleaned_tweet']].head()\n\n\nX=data.cleaned_tweet.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapping each sentiment to integer label\ndef sentiment_to_label(sentiment):\n    return {\n        'negative': 0,\n        'neutral': 1,\n        'positive': 2\n    }[sentiment]\n\n\nY = data.airline_sentiment.apply(sentiment_to_label).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3- ML model\n\n\n\nTo build a machine learning model that will make classification, we can't deal directly with words in that case, so it's should be to convert the text 'tweets' into a numbers form. We can apply this process by some methods such that: CountVectorizer & TfidfVectorizer. That we will use it with different machine learning models.\n\nIn this kernel, we use Logistic regression, and SVM. Later on we can apply another ML modls .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data to train, test\nX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.3, random_state=0)\n\nprint(\"X_train:\" ,X_train.shape)\nprint(\"X_test: \",X_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample of X_traing data \nX_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CountVectorizer : used t convert a collection of text documents to a matrix of token counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define and apply CountVectorizer on training data\ncv = CountVectorizer(analyzer = \"word\")\ntrain_features= cv.fit_transform(X_train)\ntest_features=cv.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert from sparse (contain a lot of zeros) to dense\ntrain_features_2_array=train_features.toarray()\ntest_features_2_array= test_features.toarray()\nprint(train_features_2_array.shape)\nprint(test_features_2_array.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM model\nprint(\"Training SVM model  based on CountVectorizer ..\")\nsvm_classifier = svm.SVC(kernel=\"rbf\", C=0.025, probability=True)\nsvm_classifier.fit(train_features_2_array, Y_train)\n\nsvm_tr_prediction = svm_classifier.predict(train_features_2_array)\nsvm_ts_prediction = svm_classifier.predict(test_features_2_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" SVM OUTPUT USING CountVectorizer  ... \\n\\n\")\n\nsvm_tr_accuracy = accuracy_score(svm_tr_prediction, Y_train)\nprint(\" SVM training accuracy : \", svm_tr_accuracy)\n\nsvm_ts_accuracy = accuracy_score(svm_ts_prediction ,Y_test)\nprint(\" SVM testing accuracy : \", svm_ts_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_report = classification_report(Y_test, svm_ts_prediction)\nprint(svm_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training Logistic Regression model based on CountVectorizer')\nLR_classifier = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=250)\nLR_classifier.fit(train_features_2_array, Y_train)\n\nLR_tr_prediction = LR_classifier.predict(train_features_2_array)\nLR_ts_prediction = LR_classifier.predict(test_features_2_array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_tr_accuracy = accuracy_score(LR_tr_prediction,Y_train)\nprint(\" Logistic Regression training accuracy is: \" ,LR_tr_accuracy)\n\nLR_ts_accuracy = accuracy_score(LR_ts_prediction,Y_test)\nprint(\" Logistic Regression testing accuracy is: \",LR_ts_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_report = classification_report(Y_test, LR_ts_prediction)\nprint(svm_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TF-IDF :Term Frequency Inverse Document Frequency\n\nTerm Frequency: This summarizes how often a given word appears within a document.\n\nInverse Document Frequency: This downscales words that appear a lot across documents.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define and apply TfidfVectorizer on training data\n\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(X_train)\ntrain_tfidf_features =  tfidf_vect.transform(X_train)\ntest_tfidf_features =  tfidf_vect.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf_features_2_array=train_features.toarray()\ntest_tfidf_features_2_array= test_features.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training SVM model  based on TfidfVectorizer ..\")\n\nsvm_classifier = svm.SVC(kernel=\"rbf\", C=0.025, probability=True)\nsvm_classifier.fit(train_tfidf_features_2_array, Y_train)\n\nsvm_tr_prediction = svm_classifier.predict(train_tfidf_features_2_array)\nsvm_ts_prediction = svm_classifier.predict(test_tfidf_features_2_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" SVM OUTPUT USING TfidfVectorizer ... \\n\\n\")\n\nsvm_tr_accuracy = accuracy_score(svm_tr_prediction, Y_train)\nprint(\" SVM training accuracy : \", svm_tr_accuracy)\n\nsvm_ts_accuracy = accuracy_score(svm_ts_prediction,Y_test)\nprint(\" SVM testing accuracy : \", svm_ts_accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(Y_test, svm_ts_prediction)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training Logistic Regression model based on TfidfVectorizer ..')\nLR_classifier = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200)\nLR_classifier.fit(train_tfidf_features_2_array, Y_train)\n\nLR_tr_predictions = LR_classifier.predict(train_tfidf_features_2_array)\nLR_ts_predictions = LR_classifier.predict(test_tfidf_features_2_array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Logistic Regression OUTPUT USING TfidfVectorizer ... \\n\\n\")\nLR_tr_accuracy = accuracy_score(LR_tr_predictions,Y_train)\nprint(\" Logistic Regression Train accuracy is: \" ,LR_tr_accuracy)\n\nLR_ts_accuracy = accuracy_score(LR_ts_predictions,Y_test)\nprint(\" Logistic Regression Test accuracy is: \",LR_ts_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(Y_test, LR_ts_predictions)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4- DL Model\n\n\n\nThere are different deep learning (DL) methods that we can be applied it to this data such as: RNN(LSTM) & CNN .\n\nDownload GloVe pre-trained word vectors from \"https://nlp.stanford.edu/projects/glove/\"\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('/kaggle/input/glove-embedding-vectors/glove.840B.300d.txt')\n\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') \n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')\nprint('Loaded %s word vectors.' % len(embeddings_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode train texts and test texts using the a tokenizer\nMAX_NUM_WORDS = 1000\nMAX_SEQUENCE_LENGTH = 135 #from the stats we found previously\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(X)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nsequences_train = tokenizer.texts_to_sequences(X_train)\nX_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n\nsequences_test = tokenizer.texts_to_sequences(X_test)\nX_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n\n#convert labels to one hot vectors\nlabels_train = to_categorical(np.asarray(Y_train))\nlabels_test = to_categorical(np.asarray(Y_test))\n\nprint(\"train data :\")\nprint(X_train_seq.shape)\nprint(labels_train.shape)\n\nprint(\"test data :\")\nprint(X_test_seq.shape)\nprint(labels_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find number of unique words in our tweets\nvocab_size = len(word_index) + 1 \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define size of embedding matrix: number of unique words x embedding dim (300)\nembedding_matrix = np.zeros((vocab_size, 300))\n\n# fill in matrix\nfor word, i in word_index.items():  # dictionary\n    embedding_vector = embeddings_index.get(word) # gets embedded vector of word from GloVe\n    if embedding_vector is not None:\n        # add to matrix\n        embedding_matrix[i] = embedding_vector # each row of matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DL model: pass the encoded data to an embedding layer and use the Glove pre_trained weights, then pass the \n# output to an LSTM layer follwed by 2 dense layers.\n# the optimizer used is Adam, since it achivied higher accurcies usually.\n\ncell_size= 256\ndeepLModel1 = Sequential()\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=300, weights=[embedding_matrix],\n                           input_length = MAX_SEQUENCE_LENGTH, trainable=False)\ndeepLModel1.add(embedding_layer)\ndeepLModel1.add(LSTM(cell_size, dropout = 0.2))\ndeepLModel1.add(Dense(64,activation='relu'))\ndeepLModel1.add(Flatten())\ndeepLModel1.add(Dense(3, activation='softmax'))\ndeepLModel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\ndeepLModel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\ndeepLModel1_history = deepLModel1.fit(X_train_seq, labels_train, validation_split = 0.25,\n                    epochs=50, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find train and test accuracy\nloss, accuracy = deepLModel1.evaluate(X_train_seq, labels_train, verbose=False)\nprint(\"Training Accuracy: \",accuracy)\n\nloss, accuracy = deepLModel1.evaluate(X_test_seq, labels_test, verbose=False)\nprint(\"Testing Accuracy: \",accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npredictions_test = deepLModel1.predict_classes(X_test_seq)\n#print other performance measures, espically the data is unbalanced\nprint(classification_report(predictions_test , Y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFuture work :\n\n-Make other visualization to more explore data and get more information\n\n-Apply other ML models: Decision tree Naive Bayes,..etc.\n\n-Try another pre-trained embedding vectors.\n\n-Apply more DL techniques such as CNN.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}