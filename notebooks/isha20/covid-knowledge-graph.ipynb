{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport seaborn as sns\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport spacy\nimport os\nimport gc\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom spacy.matcher import Matcher \nfrom collections import  Counter\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=\"../input/CORD-19-research-challenge/\"\ndata=pd.read_csv(path+\"metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop=set(stopwords.words('english'))\n\ndef build_corpus(df,col=\"title\"):\n    corpus=[]\n    lem=WordNetLemmatizer()\n    stop=set(stopwords.words('english'))\n    new= df[col].dropna().str.split()\n    new=new.values.tolist()\n    corpus=[lem.lemmatize(word.lower()) for i in new for word in i if(word) not in stop]\n    \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most common words in the abstracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# x1=[]\n# x2=[]\n# corpus= build_corpus(data, \"abstract\")\n# counter= Counter(corpus)\n# common= counter.most_common()\n# for word, count in common[:10]:\n#     if(word not in stop):\n#         x1.append(word)\n#         x2.append(count)\n\n# sns.barplot(x=x1, y=x2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def prepare_similarity(vectors):\n#     similarity=cosine_similarity(vectors)\n#     return similarity\n\n# def get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n#     # find the index of sentence in list\n#     index = sentence_list.index(sentence)\n#     # get the corresponding row in similarity matrix\n#     similarity_row = np.array(similarity_matrix[index, :])\n#     # get the indices of top similar\n#     indices = similarity_row.argsort()[-topN:][::-1]\n#     return [(i,sentence_list[i]) for i in indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=\"../input/cord-19-eda-parse-json-and-generate-clean-csv/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_comm=pd.read_csv(path+\"clean_comm_use.csv\",nrows=1000)\nclean_comm['source']='clean_comm'\nbiox = pd.read_csv(path+\"biorxiv_clean.csv\")\nbiox['source']='biorx'\n\narticles=pd.concat([biox,clean_comm])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del biox,clean_comm\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles.fillna(\"Unknown\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# titles=[]\ntexts=[]\nfor i in range(100):\n#     titles.append(articles.iloc[i]['title'])\n    texts.append(articles.iloc[i]['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean(txt):\n    txt=re.sub(r'\\n',' ',txt)\n    txt=re.sub(r'\\([^()]*\\)',' ',txt)\n    txt=re.sub(r'https?:\\S+\\sdoi',' ',txt)\n    return txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts=list(map(clean,texts))\ntext_list=' '.join(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Constructing knowldege graphs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp=spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_entities(sent):\n#     ## chunk 1\n#     ent1 = \"\"\n#     ent2 = \"\"\n\n#     prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n#     prv_tok_text = \"\"   # previous token in the sentence\n\n#     prefix = \"\"\n#     modifier = \"\"\n\n#   #############################################################\n  \n#     for tok in nlp(sent):\n#         ## chunk 2\n#         # if token is a punctuation mark then move on to the next token\n#         if tok.dep_ != \"punct\":\n#           # check: token is a compound word or not\n#           if tok.dep_ == \"compound\":\n#             prefix = tok.text\n#             # if the previous word was also a 'compound' then add the current word to it\n#             if prv_tok_dep == \"compound\":\n#                    prefix = prv_tok_text + \" \"+ tok.text\n      \n#       # check: token is a modifier or not\n#         if tok.dep_.endswith(\"mod\") == True:\n#             modifier = tok.text\n#             # if the previous word was also a 'compound' then add the current word to it\n#             if prv_tok_dep == \"compound\":\n#               modifier = prv_tok_text + \" \"+ tok.text\n\n#           ## chunk 3\n#         if tok.dep_.find(\"subj\") == True:\n#             ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"      \n\n#           ## chunk 4\n#         if tok.dep_.find(\"obj\") == True:\n#             ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n\n#           ## chunk 5  \n#           # update variables\n#         prv_tok_dep = tok.dep_\n#         prv_tok_text = tok.text\n#   #############################################################\n\n#     return [ent1.strip(), ent2.strip()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_relation(sent):\n\n#   doc = nlp(sent)\n\n#   # Matcher class object \n#   matcher = Matcher(nlp.vocab)\n\n#   #define the pattern \n#   pattern = [{'DEP':'ROOT'}, \n#             {'DEP':'prep','OP':\"?\"},\n#             {'DEP':'agent','OP':\"?\"},  \n#             {'POS':'ADJ','OP':\"?\"}] \n\n#   matcher.add(\"matching_1\", None, pattern) \n\n#   matches = matcher(doc)\n#   k = len(matches) - 1\n\n#   span = doc[matches[k][1]:matches[k][2]] \n\n#   return(span.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\n\nimport spacy\nnlp=spacy.load('en_core_web_sm')\n\ndef get_entity_pairs(text, coref=True):\n    # preprocess text\n    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n    text = nlp(text)\n    \n    def refine_ent(ent, sent):\n        unwanted_tokens = (\n            'PRON',  # pronouns\n            'PART',  # particle\n            'DET',  # determiner\n            'SCONJ',  # subordinating conjunction\n            'PUNCT',  # punctuation\n            'SYM',  # symbol\n            'X',  # other\n        )\n        ent_type = ent.ent_type_  # get entity type\n        if ent_type == '':\n            ent_type = 'NOUN_CHUNK'\n            ent = ' '.join(str(t.text) for t in\n                           nlp(str(ent)) if t.pos_\n                           not in unwanted_tokens and t.is_stop == False)\n        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n            refined = ''\n            for i in range(len(sent) - ent.i):\n                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n                    refined += ' ' + str(ent.nbor(i))\n                else:\n                    ent = refined.strip()\n                    break\n\n        return ent, ent_type\n\n    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n    ent_pairs = []\n    for sent in sentences:\n        sent = nlp(sent)\n        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n        spans = spacy.util.filter_spans(spans)\n        with sent.retokenize() as retokenizer:\n            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n                                            'dep': span.root.dep}) for span in spans]\n        deps = [token.dep_ for token in sent]\n\n        # limit our example to simple sentences with one subject and object\n        if (deps.count('obj') + deps.count('dobj')) != 1\\\n                or (deps.count('subj') + deps.count('nsubj')) != 1:\n            continue\n\n        for token in sent:\n            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n                continue\n            subject = [w for w in token.head.lefts if w.dep_\n                       in ('subj', 'nsubj')]  # identify subject nodes\n            if subject:\n                subject = subject[0]\n                # identify relationship by root dependency\n                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n                if relation:\n                    relation = relation[0]\n                    # add adposition or particle to relationship\n#                     if relation.nbor(1).pos_ in ('ADP', 'PART'):\n#                         relation = ' '.join((str(relation), str(relation.nbor(1))))\n                else:\n                    relation = 'unknown'\n\n                subject, subject_type = refine_ent(subject, sent)\n                token, object_type = refine_ent(token, sent)\n\n                ent_pairs.append([str(subject), str(relation), str(token),\n                                  str(subject_type), str(object_type)])\n\n    ent_pairs = [sublist for sublist in ent_pairs\n                          if not any(str(ent) == '' for ent in sublist)]\n    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n                                             'subject_type', 'object_type'])\n    print('Entity pairs extracted:', str(len(ent_pairs)))\n\n    return pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.max_length=1000000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs=get_entity_pairs(text_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs['relation'].value_counts()[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndef draw_kg(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\n    node_deg = nx.degree(k_graph)\n    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n    plt.figure(num=None, figsize=(120, 90), dpi=80)\n    nx.draw_networkx(\n        k_graph,\n        node_size=[int(deg[1]) * 500 for deg in node_deg],\n        arrowsize=20,\n        linewidths=1.5,\n        pos=layout,\n        edge_color='red',\n        edgecolors='black',\n        node_color='white',\n        )\n    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n                  pairs['relation'].tolist()))\n    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n                                 font_color='red')\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# def draw_kg(pairs,c1='red',c2='blue',c3='yellow'):\n#     k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n#             create_using=nx.MultiDiGraph())\n  \n#     node_deg = nx.degree(k_graph)\n#     degrees= list(k_graph.out_degree())\n#     print(degrees)\n#     layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n#     plt.figure(num=None, figsize=(50, 40), dpi=80)\n#     nx.draw_networkx(\n#         k_graph,\n#         node_size=[int(deg[1]) * 500 for deg in node_deg],\n#         arrowsize=20,\n#         linewidths=1.5,\n#         pos=layout,\n#         edge_color=c1,\n#         edgecolors=c2,\n#         node_color=c3,\n#         )\n#     labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n#                   pairs['relation'].tolist()))\n#     nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n#                                  font_color='red')\n#     plt.axis('off')\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A directed multigraph network is created with nodes sized in proportion to degree centrality.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_kg(pairs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing Markov Clustering on Knowledge Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install markov_clustering[drawing]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import markov_clustering as mcl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\nadj_matrix = nx.to_numpy_matrix(k_graph)\nres = mcl.run_mcl(adj_matrix)\nclusters = mcl.get_clusters(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCL(pairs):\n    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n            create_using=nx.MultiDiGraph())\n    adj_matrix = nx.to_numpy_matrix(k_graph)\n    res = mcl.run_mcl(adj_matrix)\n    clusters = mcl.get_clusters(res)\n#     mcl.drawing.draw_graph(adj_matrix, clusters, edge_color=\"red\",node_size=40, with_labels=True)\n    mcl.draw_graph(adj_matrix, clusters, node_size=150, with_labels=True, edge_color=\"silver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MCL(pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}