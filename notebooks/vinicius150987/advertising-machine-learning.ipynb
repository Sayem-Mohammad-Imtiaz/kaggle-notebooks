{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style= 'darkgrid', palette='deep')\nimport warnings\nwarnings.filterwarnings('ignore')\nbins = range(0,100,10)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/advertising.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"It’s often said that “data is the fuel of machine learning.” This isn’t quite true: data is like the crude oil of machine learning which means it has to be refined into features — predictor variables — to be useful for training a model. Without relevant features, you can’t train an accurate model, no matter how complex the machine learning algorithm. The process of extracting features from a raw dataset is called feature engineering. More about Feature Engineering [here](https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a user columns\ndf_user = pd.DataFrame(np.arange(0, len(df_feature)), columns=['user'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature = pd.concat([df_user, df_feature], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Creating continent columns\n\nThe column Country has 237 unique values. And so on, these countries can be organized in group.\nFor this, we will create a new colum called continent.\nBut firstly is necessary remove parantheses (as Antarctica) for the package country convert work properly.\nLet's take out it from our data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.groupby('Country')['Country'].unique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing parentheses from Country\ndef removeAfterParentheses(string):\n    \"\"\"\n    input is a string \n    output is a string with everything after comma removed\n    \"\"\"\n    return string.split('(')[0].strip()\ndf_feature.Country = df_feature.Country.apply(removeAfterParentheses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the remove parentheses \ndf_feature.groupby('Country')['Country'].unique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = df_feature.groupby('Country')['Country'].unique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#Installing country_converter package\n!pip install country_converter --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The country converter (coco) - a Python package for converting country names between different classification schemes.\nMore about country_converter [here](https://github.com/konstantinstadler/country_converter)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting Countries continent\nimport country_converter as coco\ncc = coco.CountryConverter()\ncontinent = np.array([])\nfor i in range(0, len(df_feature)):\n    continent= np.append(continent, cc.convert(names=df_feature['Country'][i], to='Continent' ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature['continent'] = pd.DataFrame(continent) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reorganizing the columns\ndf_feature = df_feature[['user','Daily Time Spent on Site', 'Age', 'Area Income',\n       'Daily Internet Usage', 'Ad Topic Line', 'City', 'Male', 'Country', 'continent',\n       'Timestamp', 'Clicked on Ad']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use date_converter package will be necessary install easy-date package.\nMore about easy-date [here.](https://pypi.org/project/easy-date/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Installing date_converter package\n!pip install easy-date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting string format to Datatime format \nimport date_converter\n\nfor i in range(0,len(df_feature)):\n    df_feature['Timestamp'][i] = date_converter.string_to_datetime(df_feature['Timestamp'][i], '%Y-%m-%d %H:%M:%S')\ntime_new = df_feature['Timestamp'].iloc[0]\ndf_feature['Hour'] = df_feature['Timestamp'].apply(lambda time_new: time_new.hour)\ndf_feature['Month'] = df_feature['Timestamp'].apply(lambda time_new: time_new.month)\ndf_feature['Day'] = df_feature['Timestamp'].apply(lambda time_new: time_new.weekday())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How many percentage the user have been spending on site? Creating % spending time columns\ndf_feature.columns\ndf_feature['% spending time'] =  ((df_feature['Daily Time Spent on Site'] / df_feature['Daily Internet Usage']) * 100 )\ndf_feature = df_feature[['user', 'Daily Time Spent on Site','Daily Internet Usage',\n                                        '% spending time','Age','Area Income',\n                                        'Ad Topic Line', 'City', 'Male', 'Country',\n                                        'continent', 'Timestamp','Hour', 'Month', 'Day','Clicked on Ad']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_chart(feature1, feature2):\n    g = pd.crosstab(df_feature[feature1], df_feature[feature2]).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 / df.shape[0]:.2f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature1))\n    plt.xlabel('{}'.format(feature1))\n    plt.tight_layout()\n    plt.ylabel('Quantity')\n    \ndef bar_chart_group(feature):\n    g = pd.crosstab(pd.cut(df_feature[feature], bins), df_feature['Clicked on Ad']).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 / df.shape[0]:.2f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature))\n    plt.xlabel('{}'.format(feature))\n    plt.tight_layout()\n    plt.ylabel('Quantity')\n\ndef bar_chart_hour(feature):\n    bins_hour = np.arange(0,25,12)\n    g = pd.crosstab(pd.cut(df_feature[feature], bins_hour), df_feature['Clicked on Ad']).plot(kind='bar', figsize=(10,10), rot = 45)\n    ax = g.axes\n    for p in ax.patches:\n     ax.annotate(f\"{p.get_height() * 100 / df.shape[0]:.2f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='gray', rotation=0, xytext=(0, 10),\n         textcoords='offset points') \n    plt.grid(b=True, which='major', linestyle='--')\n    plt.legend(['Clicked on Ad',\"Did not Clicked on Ad\"])\n    plt.title('Clicked on Ad for {}'.format(feature))\n    plt.xlabel('{}'.format(feature))\n    plt.tight_layout()\n    plt.ylabel('Quantity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking latitude and longitude\nfrom geopy.geocoders import Nominatim\nlat = np.array([])\nlon = np.array([])\ncountry = np.array([])\n\nfor i in range(0, len(countries)):\n    geolocator = Nominatim(user_agent='tito', timeout=100)\n    location = geolocator.geocode(countries.index[i], timeout=100)\n    lat = np.append(lat, location.latitude)\n    lon = np.append(lon, location.longitude)\n    country = np.append(country, countries.index[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Map\nimport folium\ndata = pd.DataFrame({\n'lat':lat,\n'lon':lon,\n'name':country})\ndata.head()    \n\nm = folium.Map(location=[20, 0], tiles=\"Mapbox Bright\", zoom_start=2 , )\ncountry_map = list(zip(data['name'].values, data['lat'].values, data['lon'].values))\n# add features\nfor country_map in country_map:\n    folium.Marker(\n        location=[float(country_map[1]), float(country_map[2])],\n        popup=folium.Popup(country_map[0], parse_html=True),\n        icon=folium.Icon(icon='home')\n    ).add_to(m)   \n    \nm  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart('Male','Clicked on Ad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart('continent', 'Clicked on Ad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart('Day', 'Clicked on Ad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart('Month', 'Clicked on Ad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart_group('Age')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart_group('% spending time')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_chart_hour('Hour')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.drop(['user', 'Male', 'Clicked on Ad'], axis=1).hist(figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.groupby('continent')['Area Income'].sum().sort_values().plot(kind='bar', figsize=(10,10), rot=45)\nplt.title('Area income per Continent')\nplt.grid(b=True, which='major', linestyle='--')\nplt.tight_layout()\nplt.ylabel('Quantity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation with independent Variable \ndf2 = df_feature.drop(['user', 'Clicked on Ad', 'Ad Topic Line', 'City'], axis=1)\ndf2.corrwith(df_feature['Clicked on Ad']).plot.bar(\n        figsize = (10, 10), title = \"Correlation with Clicked on Ad\", fontsize = 15,\n        rot = 45, grid = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = df2.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Pie Plots \ndf_feature.columns\ndf2 = df_feature.drop(['user', 'Daily Time Spent on Site', 'Daily Internet Usage',\n       '% spending time', 'Age', 'Area Income', 'Ad Topic Line', 'City' , 'Country',\n       'Timestamp', 'Hour', 'Clicked on Ad'], axis=1)\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Pie Chart Distributions', fontsize=20)\nfor i in range(1, df2.shape[1] + 1):\n    plt.subplot(6, 3, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(df2.columns.values[i - 1])\n   \n    values = df2.iloc[:, i - 1].value_counts(normalize = True).values\n    index = df2.iloc[:, i - 1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analyses"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature['Clicked on Ad'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countNotClicked = len(df_feature[df_feature['Clicked on Ad'] == 0])     \ncountClicked  = len(df_feature[df_feature['Clicked on Ad'] == 1]) \nprint('Percentage of not Clicked on Ad: {:.2f}%'.format((countNotClicked/len(df_feature)) * 100)) \nprint('Percentage of Clicked on Ad: {:.2f}%'.format((countClicked/len(df_feature)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.groupby(df_feature['Clicked on Ad']).mean().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df_feature.isnull(), yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_percentage = (df_feature.isnull().sum()/len(df_feature) * 100)\nnull_percentage = pd.DataFrame(null_percentage, columns = ['Percentage Null Values (%)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_percentage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature.columns\nX = df_feature.drop(['user', 'Clicked on Ad', 'Ad Topic Line', 'City',\n              'Country', 'Timestamp'], axis=1)\ny = df_feature['Clicked on Ad']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get Dummies\nX = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Avoiding Dummies Trap\nX = X.drop(['continent_not found'], axis=1)\nX.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the Dataset into the Training Set and Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = pd.DataFrame(sc_x.fit_transform(X_train), columns=X.columns.values)\nX_test = pd.DataFrame(sc_x.transform(X_test), columns=X.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building \n### Comparing Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression(random_state = 0, penalty = 'l1')\nlr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = lr_classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## K-Nearest Neighbors (K-NN)\n#Choosing the K value\nerror_rate= []\nfor i in range(1,40):\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(np.mean(error_rate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## K-Nearest Neighbors (K-NN)\nfrom sklearn.neighbors import KNeighborsClassifier\nkn_classifier = KNeighborsClassifier(n_neighbors=35, metric='minkowski', p= 2)\nkn_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = kn_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['K-Nearest Neighbors (minkowski)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (Linear)\nfrom sklearn.svm import SVC\nsvc_linear_classifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nsvc_linear_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svc_linear_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (rbf)\nfrom sklearn.svm import SVC\nsvc_rbf_classifier = SVC(random_state = 0, kernel = 'rbf', probability= True)\nsvc_rbf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svc_rbf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngb_classifier = GaussianNB()\ngb_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gb_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\ndt_classifier.fit(X_train, y_train)\n\n#Predicting the best set result\ny_pred = dt_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Installing pydotplus package\n!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plotting Decision Tree\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dt_classifier, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'gini')\nrf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = rf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ada Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nad_classifier = AdaBoostClassifier()\nad_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = ad_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ada Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngr_classifier = GradientBoostingClassifier()\ngr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gr_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gradient Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Xg Boosting\nfrom xgboost import XGBClassifier\nxg_classifier = XGBClassifier()\nxg_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = xg_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Xg Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Ensemble Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nvoting_classifier = VotingClassifier(estimators= [('lr', lr_classifier),\n                                                  ('kn', kn_classifier),\n                                                  ('svc_linear', svc_linear_classifier),\n                                                  ('svc_rbf', svc_rbf_classifier),\n                                                  ('gb', gb_classifier),\n                                                  ('dt', dt_classifier),\n                                                  ('rf', rf_classifier),\n                                                  ('ad', ad_classifier),\n                                                  ('gr', gr_classifier),\n                                                  ('xg', xg_classifier),],\nvoting='soft')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clf in (lr_classifier,kn_classifier,svc_linear_classifier,svc_rbf_classifier,\n            gb_classifier, dt_classifier,rf_classifier, ad_classifier, gr_classifier, xg_classifier,\n            voting_classifier):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting Test Set\ny_pred = voting_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ensemble Voting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Classifier\nprint('The best classifier is:')\nprint('{}'.format(results.sort_values(by='Accuracy',ascending=False).head(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying K-fold validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=svc_linear_classifier, X=X_train, y=y_train,cv=10)\naccuracies.mean()\naccuracies.std()\nprint(\"SVM (Linear) Accuracy: %0.3f (+/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EXTRA: Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy Paradox"},{"metadata":{},"cell_type":"markdown","source":"Accuracy is not the best way to measure a perfomance of model. It´s because Accuracy Paradox. More about Accuracy Paradox [here.](https://towardsdatascience.com/accuracy-paradox-897a69e2dd9b)"},{"metadata":{},"cell_type":"markdown","source":"# Cumulative Accuracy Profile (CAP)"},{"metadata":{},"cell_type":"markdown","source":"For figure out Accuracy Paradox, we will use the Cumulative Accuracy Profile (CAP). More about Cumulative Accuracy Profile (CAP) [here.](https://en.wikipedia.org/wiki/Cumulative_accuracy_profile)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Cumulative Accuracy Profile (CAP)\ny_pred_proba = svc_linear_classifier.predict_proba(X=X_test)\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\ndef capcurve(y_values, y_preds_proba):\n    num_pos_obs = np.sum(y_values)\n    num_count = len(y_values)\n    rate_pos_obs = float(num_pos_obs) / float(num_count)\n    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n    xx = np.arange(num_count) / float(num_count - 1)\n    \n    y_cap = np.c_[y_values,y_preds_proba]\n    y_cap_df_s = pd.DataFrame(data=y_cap)\n    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n    \n    print(y_cap_df_s.head(20))\n    \n    yy = np.cumsum(y_cap_df_s[0]) / float(num_pos_obs)\n    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n    \n    percent = 0.5\n    row_index = int(np.trunc(num_count * percent))\n    \n    val_y1 = yy[row_index]\n    val_y2 = yy[row_index+1]\n    if val_y1 == val_y2:\n        val = val_y1*1.0\n    else:\n        val_x1 = xx[row_index]\n        val_x2 = xx[row_index+1]\n        val = val_y1 + ((val_x2 - percent)/(val_x2 - val_x1))*(val_y2 - val_y1)\n    \n    sigma_ideal = 1 * xx[num_pos_obs - 1 ] / 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n    sigma_model = integrate.simps(yy,xx)\n    sigma_random = integrate.simps(xx,xx)\n    \n    ar_value = (sigma_model - sigma_random) / (sigma_ideal - sigma_random)\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n    ax.plot(xx,yy, color='red', label='User Model')\n    ax.plot(xx,xx, color='blue', label='Random Model')\n    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n    \n    plt.xlim(0, 1.02)\n    plt.ylim(0, 1.25)\n    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n    plt.xlabel('% of the data')\n    plt.ylabel('% of positive obs')\n    plt.legend()     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"capcurve(y_test,y_pred_proba[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(svc_linear_classifier, random_state=0).fit(X_test,y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(svc_linear_classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"For feature selection, we wil use the Recursive Feature Elimination (RFE). More about Recursive Feature Elimination (RFE) [here.](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\n\n# Model to Test\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\n\n# Select Best X Features\nrfe = RFE(classifier, n_features_to_select=None)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Model to the Training Set\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM RFE (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Formatting Final Results\ndf_feature.columns\nuser_identifier = df_feature['user']\nfinal_results = pd.concat([y_test, user_identifier], axis = 1).dropna()\nfinal_results['predicted'] = y_pred\nfinal_results = final_results[['user', 'Clicked on Ad', 'predicted']].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_results.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}