{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nCal_house_file_path= '../input/california-housing-prices/housing.csv'\nCal_house_data= pd.read_csv(Cal_house_file_path)\nCal_house_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cal_house_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets analyse this data closely. First let's check for missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"Cal_house_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since total_bedrooms contains missing values, we must clean the data. We have two options: 1) Because it only has 207 rows out of 20640 that are missing, we can just drop the rows that have missing values. Or 2) we can use imputation. "},{"metadata":{},"cell_type":"markdown","source":"It's more simple just to drop the rows so let's make a dataframe of just dropping the 207 rows of null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping rows with null\nCal_house_data_nonull= Cal_house_data.dropna(axis = 0, how ='any')\nprint(\"Cal_house_data length:\", len(Cal_house_data), \"\\nCal_house_data_nonull length:\",  \n       len(Cal_house_data_nonull), \"\\nNumber of rows dropped: \", \n       (len(Cal_house_data)-len(Cal_house_data_nonull))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let't see if imputation is an option to do. But I have a hunch that putting the median value wouldn't be too accurate so lets see if theres another variable that we can corelate with the sale value."},{"metadata":{},"cell_type":"raw","source":"Breaking down the distributions of each column data:"},{"metadata":{},"cell_type":"markdown","source":"Coordinate and Price Distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,12))\nplt.scatter(Cal_house_data['longitude'],Cal_house_data['latitude'],c=Cal_house_data['median_house_value'],s=Cal_house_data['population']/10, cmap='inferno')\nplt.colorbar()\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Coodinates of Houses and Their Median House Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph makes sense as the top [10 most expensive cities in California](https://moneyinc.com/most-expensive-cities-in-california-in-2019/) are San Francisco, Newport Beach, San Jose, Oakland, Santa Cruz, Napa, Montecito, Los Angeles, Santa Barbara, and San Diego."},{"metadata":{},"cell_type":"markdown","source":"Now, because sale price is what we are truly looking for as the output, let's analyse the distribution of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(Cal_house_data['median_house_value'],color='blue')\nplt.xlabel('Median House Value')\nplt.title('Distribution of Median House Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are outliers at around median_house_value 500000. Let's take out the outliers then."},{"metadata":{"trusted":true},"cell_type":"code","source":"Cal_house_data[Cal_house_data['median_house_value']>450000]['median_house_value'].value_counts().head()\nCal_house_data_outlier=Cal_house_data.loc[Cal_house_data['median_house_value']<500001,:]\nplt.figure(figsize=(15,5))\nsns.distplot(Cal_house_data_outlier['median_house_value'], color = 'blue')\nplt.xlabel('Median House Value')\nplt.title('Distribution of Median House Value with Outliers Taken Out')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets look at the population column data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.boxplot(y='population',data=Cal_house_data_outlier)\nplt.ylabel('Population')\nplt.title('Distribution of Population Data')\nplt.plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can se that there are also outliers in the population data. Let's see how many outliers there are."},{"metadata":{"trusted":true},"cell_type":"code","source":"Cal_house_data_outlier[Cal_house_data_outlier['population']>15000]['population'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take out those outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"Cal_house_value_and_pop_outliers=Cal_house_data_outlier.loc[Cal_house_data['median_house_value']<15037,:]\nCleaned_cal_house=Cal_house_value_and_pop_outliers\nplt.figure(figsize=(10,5))\nsns.boxplot(y='population',data=Cleaned_cal_house)\nplt.ylabel('Population')\nplt.title('Distribution of Population Data without Outliers')\nplt.plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've cleaned the data of most outliers let's see the correlation between all the variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Cleaned_cal_house= Cleaned_cal_house.dropna(axis = 0, how ='any')\nplt.figure(figsize=(11,7))\nsns.heatmap(cbar=False,annot=True,data=Cleaned_cal_house.corr()*100,cmap='coolwarm')\nplt.title('Corelation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to Preprocess Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Cleaned_cal_house=pd.concat([pd.get_dummies(Cleaned_cal_house['ocean_proximity'],drop_first=True),Cleaned_cal_house],axis=1).drop('ocean_proximity',axis=1)\nCleaned_cal_house['income per working population']=Cleaned_cal_house['median_income']/(Cleaned_cal_house['population']-Cleaned_cal_house['households'])\nCleaned_cal_house['bed per house']=Cleaned_cal_house['total_bedrooms']/Cleaned_cal_house['total_rooms']\nCleaned_cal_house['h/p']=Cleaned_cal_house['households']/Cleaned_cal_house['population']\ndef type_building(x):\n    if x<=10:\n        return \"new\"\n    elif x<=30:\n        return 'mid old'\n    else:\n        return 'old'\nCleaned_cal_house=pd.concat([Cleaned_cal_house,pd.get_dummies(Cleaned_cal_house['housing_median_age'].apply(type_building),drop_first=True)],axis=1)\nx=Cleaned_cal_house.drop('median_house_value',axis=1).values\ny=Cleaned_cal_house['median_house_value'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(x,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nms=MinMaxScaler()\nX_train=ms.fit_transform(X_train)\nX_valid=ms.transform(X_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nmy_model_xgb = XGBRegressor(objective = 'neg_mean_squared_error')\nparam_dist = {'n_estimators': stats.randint(150, 500),\n              'learning_rate': stats.uniform(0.01, 0.07),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': stats.uniform(0.5, 0.45),\n              'min_child_weight': [1, 2, 3]\n             }\n\nmodel_xgb_crossval = RandomizedSearchCV(my_model_xgb, param_distributions = param_dist, n_iter = 25, scoring = 'neg_mean_squared_error', error_score = 0, verbose = 3, n_jobs = -1)\n\nmodel_xgb_crossval.fit(X_train, y_train)\nfrom sklearn.metrics import mean_squared_error\nmodel_xgb_crossval_fitted = model_xgb_crossval.predict(X_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_2 = mean_squared_error(predictions_2, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean Absolute Error:\" , mae_2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}