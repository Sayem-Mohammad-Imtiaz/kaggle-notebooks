{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommender System that return top related research papers according to query"},{"metadata":{},"cell_type":"markdown","source":"**Hello! Welcome to this kernel. **\n<p>I am a beginner in data science so if you have any suggestions or thoughts you want to share please do not hesitate to leave a comment!! This is also one of my methond to learn more knowledge! I am currently a student, and this project is actually one of my courses' final accessment. I just thought it would nice to post it here too! "},{"metadata":{"id":"gtoP78aorXa1"},"cell_type":"markdown","source":"# Goal\n<p>For this project, we get over 45000 biomedical papers as the dataset. This is a very large dataset and it is hard to find valuable information directly from this large dataset. Therefore, I want to build a <b>recommender system</b> that can give recommendations on what papers to read according to a specific query."},{"metadata":{"id":"LTrhVDURd2pJ"},"cell_type":"markdown","source":"Import important libraries"},{"metadata":{"id":"5G0eXxYRd6z1","outputId":"2a03988f-8fb0-4c35-eb69-fd03030638e1","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#nltk.download(\"stopwords\")\n#nltk.download('wordnet')\n#nltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom gensim.models import word2vec\nfrom sklearn import metrics\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\nimport json\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"2hSrlsE_d7qf"},"cell_type":"markdown","source":"# Load and read the csv file"},{"metadata":{"id":"NedFcZZRcyg0","outputId":"a449982b-5400-4817-9f4a-833fd25bcd73","trusted":true},"cell_type":"code","source":"# load the meta data from the CSV file \ndf=pd.read_csv(\"../input/covid19-json-to-csv-file/df.csv\")\nprint (df.shape)\n\ndf[\"abstract\"] = df[\"abstract\"].str.lower()\ndf['title'] = df['title'].str.lower()\ndf['full_text'] = df['full_text'].str.lower()\n#show 10 lines of the new dataframe\nprint (df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"5MlmoQgxfdr1"},"cell_type":"markdown","source":"Next we will read the matadata csv file that contains around 45000 papers. Some of the papers does not have the full text. We only need the \"title\" and \"abstract\" column from this dataframe. "},{"metadata":{"id":"pAFT1kfxEieC","outputId":"5eb15a84-3d0e-490c-d9c7-48a5ccd838d0","trusted":true},"cell_type":"code","source":"metadata=pd.read_csv(\"../input/CORD-19-research-challenge/metadata.csv\", usecols=['title','abstract'])\nmetadata[\"abstract\"] = metadata[\"abstract\"].str.lower()\nmetadata['title'] = metadata['title'].str.lower()\nprint(metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"3-_BqpVafsiF"},"cell_type":"markdown","source":"Here we merge the two dataframe together. The rows that have the same title from both column will merged, and others will become NaN if there is no match row. "},{"metadata":{"id":"o3u8fnqwfO_v","outputId":"6969af25-2ba3-4f22-9a14-f894bb65c884","trusted":true},"cell_type":"code","source":"papers = pd.merge(df, metadata, how = 'left')\npapers","execution_count":null,"outputs":[]},{"metadata":{"id":"JkgqZKQQhPvR","trusted":true},"cell_type":"code","source":"papers=papers.dropna()\npapers","execution_count":null,"outputs":[]},{"metadata":{"id":"avYdShLBeBV1"},"cell_type":"markdown","source":"# 1. Data Cleaning"},{"metadata":{"id":"b4FsgKlwIfgM"},"cell_type":"markdown","source":"For the cleaning part, we will remove the following parts:\n*  remove stopwords, and add nonrelevent word into the stopwords list in order to remove them\n*  remove punctuations such as \":+=%\"\n*  remove urls from the columns\n*  and lemmatize the word in each row "},{"metadata":{"id":"E1fVOJSjI7io","trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\nstop |= set(['title','abstract','preprint','biorxiv','read','author','funder','copyright','holder','https','license','et','al','may',\n             'also','medrxiv','granted','reuse','rights','used','reserved','peer','holder','figure','fig','table','doi','within'])\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"id":"xuK3nUsedEcz","trusted":true},"cell_type":"code","source":"def data_preprocessing(text):\n    text = ' '.join(re.sub('https?://\\S+|www\\.\\S+','',text).split())\n    text = text.replace('\\n', '')\n    text = re.sub(\"[!@#$+%*:()/<.=,â€”']\", '', text)\n    text = ' '.join([word for word in text.split() if word not in stop])\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"t684RIFNfcul","trusted":true},"cell_type":"code","source":"papers['title'] = papers['title'].apply(lambda x: data_preprocessing(x))\npapers['abstract'] = papers['abstract'].apply(lambda x: data_preprocessing(x))\npapers['full_text'] = papers['full_text'].apply(lambda x: data_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"kGzJrn2xlh7a","outputId":"e2bc6a2c-aa14-4721-928c-49b5cfe01f4b","trusted":true},"cell_type":"code","source":"papers.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"id":"TCxgJDMeJqNw"},"cell_type":"markdown","source":"After applying the data_preprecessing function to the \"title\", \"abstract\", and \"full_text\" column, we get our clean text. Each row of the three columns are in lower case, stopwords and punctuations removed, and lemmatized."},{"metadata":{"id":"xbimjJw7uA6P"},"cell_type":"markdown","source":"# 2. Data Visualization and Exploratory Data Analysis"},{"metadata":{"id":"8zNNy-2CudAu"},"cell_type":"markdown","source":"In this part i will do the data visualization and exploratory data analysis. For my task, my aim is to make recommendations based on a specific query. Therefore, for the EDA, i will take a look at how the data is distributed based on the published year and paper content."},{"metadata":{"id":"auqEuNzbLooM"},"cell_type":"markdown","source":"### 2.1 Word Cloud\n<p>In this section, i will generate a word cloud based on the content in the abstract column, the title column, and the full_text column. "},{"metadata":{"id":"Fk152-OIu75n","outputId":"fe4b4450-ccc8-4382-9abb-01cb448924d7","trusted":true},"cell_type":"code","source":"contentCorpus = papers.full_text.values\nplt.figure(figsize = (12, 8))\nwordcloud = WordCloud(width = 3000,height = 2000,background_color=\"white\",max_words=1000).generate(str(contentCorpus))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Figure 1. Full_text Corpus Word Cloud')","execution_count":null,"outputs":[]},{"metadata":{"id":"E8TlP5m19lyx"},"cell_type":"markdown","source":"Figure 1 is the word cloud generated from the full text column. From this we can see that \"sequence\",\"virus\",\"protein\",\"sample\" are the most common words among all the body text. This word cloud provide us a general idea of what are the literature's content."},{"metadata":{"id":"BkBxjh2d9dV1","outputId":"2f72a9bc-034a-4e74-c851-85e213658220","trusted":true},"cell_type":"code","source":"contentCorpus = papers.abstract.values\nplt.figure(figsize = (12, 8))\nwordcloud = WordCloud(width = 3000,height = 2000,background_color=\"white\",max_words=1000).generate(str(contentCorpus))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Figure 2. Abstract Corpus Word Cloud')","execution_count":null,"outputs":[]},{"metadata":{"id":"QcnjksRTM2FO"},"cell_type":"markdown","source":"From Figure 2, we can see the word cloud generated from the abstract. We can see that the word \"ibv\", \"sequence\",\"virus\",\"sample\",\"dna\",\"isolate\" are some of the largest words in the word cloud."},{"metadata":{"id":"Vyl7krbtMG5E","outputId":"aa140932-d3e5-4052-9d28-62a91f1c960a","trusted":true},"cell_type":"code","source":"contentCorpus = papers.title.values\nplt.figure(figsize = (12, 8))\nwordcloud = WordCloud(width = 3000,height = 2000,background_color=\"white\",max_words=10000).generate(str(contentCorpus))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Figure 3. Title Corpus Word Cloud')","execution_count":null,"outputs":[]},{"metadata":{"id":"eFAbaj0YOI2_"},"cell_type":"markdown","source":"Figure 3 is the title column's word cloud. \"Infections\",\"replications\",\"rna\",\"pseudoknots\" are the most common words in the column."},{"metadata":{"id":"KXqAlNkxP1pu"},"cell_type":"markdown","source":"### 2.2 Countplot for different virus discussed among the published papers\n<p>From Figure 1,2,and 3 we can have a general idea of what the papers are talking about. In this section, I will categorize the different virus that those paper discussed. By knowing this, we would know how many of them are talking about the covid-19, how many discussed about other virus.\n<p>The following code do the virus assignemnt part, I added a column \"virs\" to record the topic for each paper. I determined the virus of each paper by looking for specific keywords in the full_text. For example, if an article writes about \"covis-19\" in the full_text, I will assign this paper to topic covid-19."},{"metadata":{"id":"iW-wHK64xVlU","trusted":true},"cell_type":"code","source":"papers['virus'] = np.where(papers.full_text.str.contains('covid-19|covid|wuhan'), 'covid-19',\n              np.where(papers.full_text.str.contains('alphacoronavirus|alpha-cov'), 'alphacoronavirus',\n              np.where(papers.full_text.str.contains('betacoronavirus|mers|mers-cov|sars|sars-cov|sars-cov2'), 'betacoronavirus',\n              np.where(papers.full_text.str.contains('gammacoronavirus|ibv'), 'gammacoronavirus',\n              \"None\"))))","execution_count":null,"outputs":[]},{"metadata":{"id":"etwKNb8bBT0v","outputId":"7f3ca274-6e56-423e-80b2-d87e32323cad","trusted":true},"cell_type":"code","source":"papers['virus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"7e7172OT8VlP","outputId":"a3eb18cb-960d-4a2f-ea2c-365d3aab3f96","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 8))\nax = sns.countplot(x=\"virus\", data=papers)\nax.set_title('Figure 4. Distribution of different virus covered in the papers')\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"id":"JF2dO42WQal9"},"cell_type":"markdown","source":"Figure 4 is the visualization of the distribution of differnt virus disscussed among the dataset. As we can see from the figure, the betacoronavirus has the most count. The species in this virus are MERS and SARS which were the two outbreaked dieases. Covid-19 is disscussed a lot too, around 1000 papers mentioned the covid-19 already. Around 400-500 papers are taking about the gamma and alpha coronavirus. "},{"metadata":{"id":"FJKU9JTZRVOF"},"cell_type":"markdown","source":"### 2.3 Distribution of the topic covered in the metadata\nFrom above, we know that around 1000 papers are talking about covid-19 and others are related to other coronaviruses. In this section, we will take a look at the topic covered in those papers. I determined the topic of each paper by looking for specific keywords in the abstract. For example, if an article writes about \"transmission\" in the abstract, I will assign this paper to topic transmission.\n<p>Similar to the above section, the topic column will be added and assigned corrsponding values. After the assignemnt step, i got 7467 papers talking about \"genetics|origin|evolution\", 2085 papers talking about \"transmission\", 6469 papers talking about \"vaccines|therapeutics\", 219 papers talking about \"incubation\", 788 papers talking about \"non-pharmaceutical interventions\", 5430 papers taking about \"medical care\", and 326 papers talking about \"ethical|social\". Lastly, 2073 papers were not assigned to any topic."},{"metadata":{"id":"wdI8naEBXXAG","trusted":true},"cell_type":"code","source":"papers['topic'] = np.where(papers.abstract.str.contains('transmission|transmitting'), 'transmission',\n              np.where(papers.abstract.str.contains('incubation'), 'incubation',\n              np.where(papers.abstract.str.contains('vaccines|vaccine|vaccination|therapeutics|therapeutic|drug|drugs'), 'vaccines|therapeutics',\n              np.where(papers.abstract.str.contains('gene|origin|evolution|genetics|genomes|genomic'), 'genetics|origin|evolution',\n              np.where(papers.abstract.str.contains('npi|npis|interventions|distancing|isolating|isolation|isolate|mask'), 'non-pharmaceutical interventions',\n              np.where(papers.abstract.str.contains('ards|ecmo|respirators|eua|clia|ventilation|cardiomyopathy|ai'), 'medical care',\n              np.where(papers.abstract.str.contains('ethical|social|media|rumor|misinformation|ethics|multidisciplinary'), 'ethical|social',\n              \"None\")))))))","execution_count":null,"outputs":[]},{"metadata":{"id":"Smmb0olRKJUc","outputId":"be994f2c-95a9-441b-e37a-49ff17dde5bc","trusted":true},"cell_type":"code","source":"papers['topic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"xPSHPim5UkRY","outputId":"4e61e85c-2895-4e89-ceb8-1f854cda6530","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nax = sns.countplot(x=\"topic\", data=papers)\nax.set_title('Figure 5. Distribution of different topics covered in the matadata')\nplt.xticks(rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"id":"lVN4kcm-TCrS"},"cell_type":"markdown","source":"From Figure 5, we can see that topic \"genetics|origin|evolution\" has the most count, followed by \"transmission\", \"vaccines|therapeutics\", and \"medical care\". \"transmission\", \"non-pharmaceutical interventions\", \"ethical|social\", and \"incubation\" has the least count. "},{"metadata":{"id":"7sSSkUXyVeo3"},"cell_type":"markdown","source":"# 3. Model selection and fitting to data"},{"metadata":{"id":"MzqqLD_XTjFu"},"cell_type":"markdown","source":"Now, even we have a general idea of what the articals in metadata are talking about, the quantity of papers are still too large. Researchers will have hard time find the paper or the topic they want to read in this many articles. Therefore, my goal was therefore important. The steps that i will take to achieve my goal is described as follow:\n* tokenized the sentense in each row of the \"title\", \"abstract\", and \"full_text\"\n* create three new columns called \"title_tokenized\", \"abstract_tokenized\", and \"full_text_tokenized\"\n* implement word embedding method (word2vec) as features, here i used joining (averaging) vectors from the words of each sentense. (I used the abstract column to do the training as the full_text would run a really long time and have similar results.)\n* append the vectors of each row to a new column called \"abstract_embedding\", \"title_embedding\", and \"full_text_embedding\"\n* embedding the qurey phrase to vector form by using **word2vec**\n* calculate the cosine similary between the query vector and each row of the entire abstract embedding column\n* append the similarity scores to a new column called \"cosine_score\"\n* sort the column and rank the top 10 paper titles with highest cosine score.\n\n\n"},{"metadata":{"id":"AlbjbN3Itu-K","trusted":true},"cell_type":"code","source":"tokenized_sentences_title = [sentence.split() for sentence in papers['title'].values]\ntokenized_sentences_abstract = [sentence.split() for sentence in papers['abstract'].values]\ntokenized_sentences_full_text = [sentence.split() for sentence in papers['full_text'].values]","execution_count":null,"outputs":[]},{"metadata":{"id":"0v-1tqllvMsK","trusted":true},"cell_type":"code","source":"papers['title_tokenized'] = tokenized_sentences_title\npapers['abstract_tokenized'] = tokenized_sentences_abstract\npapers['full_text_tokenized'] = tokenized_sentences_full_text","execution_count":null,"outputs":[]},{"metadata":{"id":"gv0i3CwnvoKE","trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(tokenized_sentences_abstract, size = 100, min_count=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"BcYnNsngCgWm","trusted":true},"cell_type":"code","source":"def buildWordVector(word_list, size):\n    #function to average all words vectors in a given paragraph\n    vec = np.zeros(size)\n    count = 0.\n    for word in word_list:\n        if word in model.wv:\n            vec += model.wv[word]\n            count += 1.\n    if count != 0:\n        vec /= count\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers['title_embedding'] = papers['title_tokenized'].apply(lambda x: buildWordVector(x, size = 100))\npapers['abstract_embedding'] = papers['abstract_tokenized'].apply(lambda x: buildWordVector(x, size = 100))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"P2KD5A2oeAj_","outputId":"f6340afe-16a3-406b-d81c-2b5fb6393d21","trusted":true},"cell_type":"code","source":"papers.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"3ryet8cVWGjC","trusted":true},"cell_type":"code","source":"def embedding_query(query):\n    query = query.split(' ')\n    query_vec = np.zeros(100).reshape((1,100))\n    count = 0\n    for word in query:\n        if word in model.wv:\n            query_vec += model.wv[word]\n            count += 1.\n    if count != 0:\n        query_vec /= count\n    return query_vec","execution_count":null,"outputs":[]},{"metadata":{"id":"k3wzuXFbzGsO","trusted":true},"cell_type":"code","source":"# reference: https://www.kaggle.com/mathijs02/recommend-a-paper-by-using-word-embeddings\ndef get_similarity(query,n_top):\n    query_vec = embedding_query(query)\n    papers[\"cos_sim\"] = papers['abstract_embedding'].apply(\n        lambda x: metrics.pairwise.cosine_similarity(\n            [x],query_vec.reshape(1,-1))[0][0])\n    top_list = (papers.sort_values(\"cos_sim\", ascending=False)\n                [[\"title\",\"abstract\",\"cos_sim\"]]\n                .drop_duplicates()[:n_top])\n    return top_list","execution_count":null,"outputs":[]},{"metadata":{"id":"5gophuKlB-_Z","outputId":"2dae46a9-0eb2-4103-c5da-64513dd3d7b4","trusted":true},"cell_type":"code","source":"get_similarity('transmission incubation in human ',10)","execution_count":null,"outputs":[]},{"metadata":{"id":"wN3j2RGRHMMy"},"cell_type":"markdown","source":"# 4. Deriving insights about policy and guidance to tackle the outbreak based on model findings"},{"metadata":{"id":"V3wmFTnlwNQx"},"cell_type":"markdown","source":"In the previous section, I created a recommender system with word embedding feature type. The dataset is huge, it contains over 45000 papers and 35000 of them have full text. If the researchers use the dataset directly, it would be a super hard and time spending mission. By the recommender system built in the previous section, researchers could just providing a specific query, or a paper title. Then the model will calculate the cosine similarity between the query and the dataset and return the top 10 or 20 similar papers for researchers to read. This indeed minimize their time and would be benefit for fighting against the COVID-19.\n<p>Next, I will run following querys related to the COVID-19 as some examples:\n\n* Risk factors of the novel coronavirus 2019\n* covid-19 genetics, origin, or evolution \n* Drugs or medicines to treat COVID-19 patients\n"},{"metadata":{"id":"PERuIl64XakZ"},"cell_type":"markdown","source":"By running the above queries, we get our top 10 recommendation. (The result is shown below)\n<p>For the first query, \"The risk factors of covid-19\", we get pretty great recommendations on the paper. We can see many of the recommendated papers contain \"risk\" and we can see that they described differnt kinds of risk factors. For researchers, governments, and healthcare professionals who are interested in reading the risk factors of the novel cornoavirus, i would suggest them reading the above articles. \n<p>For the second query, it asks about the genetics, origins, or evolutions of the covid-19. The results are also satisfying, as we can see the recommendated papers are about the genetic diversity, about the virus evolution. Therefore, if researchers want to find out the potential genetics, origins or evolution of the novel virus, I would recommend them reading those papers.\n<p>For the third query, it is about drugs or medicines to treat covid-19 patients. The returned recommended papers are very good as well. We can see some of the papers recommend chinese medicine. Thus, for healthcare professionals or the public heath department, if they need any information of the drug and medicine to treat patients, I would highly recommend them to read the above paper. \n<p>To conclude, there are over 45000 papers related to the new coronavirus, it would be a waste of time for the researchers to go through them one by one. A recommender system like this notebook did would save a lot of time. This would be a very fast way for researchers, governments, healthcare professionals to find more information about a similar, relevant material. "},{"metadata":{"id":"oN26KL5tvjqF","outputId":"048684ab-394f-4d2d-8796-47b576daf0ad","trusted":true},"cell_type":"code","source":"get_similarity('risk covid-19',10)","execution_count":null,"outputs":[]},{"metadata":{"id":"6K2bTr5Bwj5E","outputId":"28830ce1-9e2f-4d82-908d-b508b0d26eef","trusted":true},"cell_type":"code","source":"get_similarity('covid-19 genetics origin evolution',10)","execution_count":null,"outputs":[]},{"metadata":{"id":"lPE4vhJs2Mxr","outputId":"42887184-77f7-4725-8ca4-ac835bff3e45","trusted":true},"cell_type":"code","source":"get_similarity('drugs medicine to treat covid-19 patients',10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"hou_1000605749_finalproject.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":4}