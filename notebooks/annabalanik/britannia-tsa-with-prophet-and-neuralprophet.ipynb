{"cells":[{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/ourownstory/neural_prophet.git  \n!pip install livelossplot    #it will allow us to use plot_live_live parameter \n#in the train function to get live training and validation plots.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Important note**\n**This is my first Time Series Analysis project where I use predictors, I am new to this topic. If you have any suggestions how to improve this work in those algorithms and EDA I used here, please, leave your comment. It will be highly appreciated.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom neuralprophet import NeuralProphet\nfrom fbprophet import Prophet\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook we are going to look at time series EDA and forecasting using 3 ways, including NeuralProphet - a new version of Prophet from Facebook AI. Our goal is to look at this model in work having only:\n\n1) 2 features - date and y for NeuralProphet;\n\n2) 2 features for Prophet;\n\n3) using some extra features after FeatureEngineering.\n\nLet's start!\n\nOur dataset is stock market data of the Nifty-50 index from NSE (National Stock Exchange) India over the last 21 years (2000 - 2020), particularly, for Britannia.\n\nFrom the [official website](http://britannia.co.in/about-us/overview): \"Britannia Industries is one of India’s leading food companies with a 100 year legacy and annual revenues in excess of Rs. 9000 Cr. Britannia is among the most trusted food brands, and manufactures India’s favorite brands like Good Day, Tiger, NutriChoice, Milk Bikis and Marie Gold which are household names in India. Britannia’s product portfolio includes Biscuits, Bread, Cakes, Rusk, and Dairy products including Cheese, Beverages, Milk and Yoghurt\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full = pd.read_csv('/kaggle/input/nifty50-stock-market-data/BRITANNIA.csv', parse_dates=[\"Date\"])\ndf_full.set_index(\"Date\", drop=False, inplace=True)\ndf_full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable that we are going to predict is **VWAP -  volume weighted average price**.\nThe volume weighted average price helps in comparing the current price of the stock to a benchmark, making it easier for investors to make decisions on when to enter and exit the market. Also, the VWAP can assist investors to determine their approach towards a stock (active or passive) and make the right trade at the right time.\n\nFirst, we need to look at the data and check missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.VWAP.plot(figsize=(14, 7)) #plotting the target through the time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.info() #checking the info about data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, the graph shows some sharp things, they are changepoints.\n\nAnd there are missing values - around 50% is missing in Trades and 10% - in Deliverable Volume and %Deliverable. We will fill them with mean values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.fillna(df_full.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_full[[\"Date\", \"VWAP\"]]\ndf.rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}, inplace=True)\ntest_length = 365   #this is the period in days which we will predict\ndf_train = df.iloc[:-test_length] #splitting data into train and test sets\ndf_test = df.iloc[-test_length:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nprophet_model = NeuralProphet(\n    n_changepoints=80,\n    yearly_seasonality=True, #include in model yearly seasonality\n    weekly_seasonality=False, #skipping weeks\n    daily_seasonality=True,   #including days\n    batch_size=64,            #this is NN parameter\n    learning_rate=1.0)      #another NN parameter\n\nmetrics = nprophet_model.fit(df_train, freq=\"D\", plot_live_loss=True, epochs=120)\nfuture_df = nprophet_model.make_future_dataframe(df_train, periods = test_length, \n                                                n_historic_predictions=len(df_train)) \npreds_df_1 = nprophet_model.predict(future_df)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nprophet_model.plot(preds_df_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df_1.set_index(\"ds\", drop=False, inplace=True)\npreds_df_1[[\"yhat1\"]].plot(figsize=(14, 7))\ndf.y.plot(figsize=(14, 7), legend = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next cell we will create rolling features, using mean and std for 7 and 30 days. \nThis idea was used from this [notebook](https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial/comments). The difference is that author shifted data only for 1 position which is good if you want to predict next day value. In our case period for predictions is 1 year, 365 days, so we used that period for shifting to make sure we don't use any data from test_set, it is designated in 'test_length' variable.\n\nAt first, we will build a classical Prophet model using only Date and VWAP (renamed as 'ds' and 'y' respectively as algorithm requires)."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df_full.reset_index(drop=True, inplace=True)\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"Trades\"]\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_7d = df_full[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df_full[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_7d = df_rolled_7d.mean().shift(test_length).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(test_length).reset_index().astype(np.float32)\n\ndf_std_7d = df_rolled_7d.std().shift(test_length).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(test_length).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df_full[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df_full[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df_full[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df_full[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf_full.fillna(df_full.mean(), inplace=True)\n\ndf_full.set_index(\"Date\", drop=False, inplace=True)\ndf_full.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_tr = df_full.iloc[:-test_length] #another splitting of data into train and validation sets\ndf_val = df_full.iloc[-test_length:]\n\nfeatures = [\"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \n            \"Low_std_lag7\",\"Volume_mean_lag7\", \"Volume_std_lag7\", \n            \"Turnover_mean_lag7\",\"Turnover_std_lag7\", \"Trades_mean_lag7\", \n            \"Trades_std_lag7\",\"High_mean_lag30\", \"High_std_lag30\", \n            \"Low_mean_lag30\", \"Low_std_lag30\",\"Volume_mean_lag30\", \n            \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n            \"Turnover_std_lag30\", \"Trades_mean_lag30\", \"Trades_std_lag30\"]\n  #additional rolling features that will be used with regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fbp_1 = Prophet() #classical Prophet without any tuning\nmodel_fbp_1.fit(df_train) \n\nforecast_1 = model_fbp_1.predict(df_test)\ndf_test[\"Forecast_Prophet_1\"] = forecast_1.yhat.values #adding yhat to df_test for further fraphs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[[\"y\", \"Forecast_Prophet_1\"]].plot(figsize=(14, 7)) #plotting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's have a look at predicted and ytrue for our period \nforecast_1.set_index(\"ds\", drop=False, inplace=True)\nforecast_1[[\"yhat\"]].plot(figsize=(14, 7))\ndf.y.plot(figsize=(14, 7), legend = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fbp_2 = Prophet() #Prophet with regressor\nfor feature in features:\n    model_fbp_2.add_regressor(feature)\n\nmodel_fbp_2.fit(df_tr[[\"Date\", \"VWAP\"] + features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast_2 = model_fbp_2.predict(df_val[[\"Date\", \"VWAP\"] + features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"Forecast_Prophet_2\"] = forecast_2.yhat.values #adding predictions to test-set for a further graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting predictions made with regressor and ytrue\nforecast_2.set_index(\"ds\", drop=False, inplace=True) \nforecast_2[[\"yhat\"]].plot(figsize=(14, 7))\ndf.y.plot(figsize=(14, 7), legend = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting two predictions made by Prophet - without (\"Forecast_Prophet_1\") \n#and with regressors (\"Forecast_Prophet_2\")\ndf_test[[\"y\", \"Forecast_Prophet_1\", 'Forecast_Prophet_2']].plot(figsize=(14, 7)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint(\"Results for the test set:\")\nprint(\"MAE of Prophet without regressors:\", mean_absolute_error(df_test.y, df_test.Forecast_Prophet_1))\nprint(\"MAE of Prophet with regressors:\", mean_absolute_error(df_test.y, df_test.Forecast_Prophet_2))\nprint(\"MAE of NeuralProphet:\", mean_absolute_error(df_test.y, preds_df_1.yhat1.iloc[-test_length:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The common feature of all algorithms is that they **failed** at including decreasing of values (for the last years) for predictions, all the models considered those low values as noise and predicted even more increasing of VWAP. \n\nWe can see from the graphs that Prophet with regressor did the worst job, predicting VWAPover 8000 at some points while the true values weren't higher than 3600. \n\nProphet (classical one, without regressor) did slightly better job in terms of MAE and a way better at generalisation.\n\n**NeuralProphet** beats both of them in terms of generalisation and MAE."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}