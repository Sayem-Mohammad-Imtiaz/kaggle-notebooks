{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#这一部分是导入数据的步骤，可以忽略\n# 此处将numpy和pandas包进行导入 其中numpy为一个数组处理模块，pandas为一个数据框处理模块\n# 数据框概念来自于数据分析原语言R,其形式和Excel的一张表相似\n# import 1  as 2  此时1为要导入的包，2为1的别名，as实际上是给1一个简单的别名，易于我们在后面进行调用。\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"读取csv文件\n\n此处调用pandas的read_csv函数，其中可以指定是否读取首行首列，是否解析时间，是否指定索引\n\n此处我们只是读取csv，并未指定其他参数，实际上使用的是默认的参数，具体的参数使用请见[官网](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/nab/realKnownCause/realKnownCause/ec2_request_latency_system_failure.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"这个数据是一台电脑的CPU使用率数据，可以知道数据范围应该为0-100"},{"metadata":{"trusted":true},"cell_type":"code","source":"#查看csv文件内容\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"通过查看内容我们可以看到，数据里面有两列数据，其中一列为时间戳，一列为值。可以知道我们现在检测的只是其值的异常，这里检测的并不是时间序列上的异常。这是因为我们的数据在时间序列上：1、无趋势性  2、无周期性  3、无季节性  ，所以检测的只是值的异常，而非从时间序列上对异常值进行观测"},{"metadata":{"trusted":true},"cell_type":"code","source":"#查看描述性统计信息\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"通过描述性统计我们可以看到，总共有4032行数据，其中平均值为45（代表平均使用率为45%），标准差为2.28(代表方差较小，可知数据大多集中分布在平均值左右)，最小值为22，最大值为99，中位数为45."},{"metadata":{},"cell_type":"markdown","source":"25%的值小于43, 50%小于45 ， 75%小于46. 那么可以先考虑从统计学意义上计算异常值。计算IQR=Q3-Q1=46-43=3 ,所以正常范围为43-1.5\\*3，46+1.5\\*3，则为38.5-50.5，超出此范围则可认为是异常值。"},{"metadata":{"trusted":true},"cell_type":"code","source":"#转换为时间戳格式\n\ndf['timestamp']=pd.to_datetime(df['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#画出线形图\n\nimport plotly.express as px\n\npx.line(df,x='timestamp',y='value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们可以从图中发现三个特别明显的异常值"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hour']=df['timestamp'].dt.hour\npx.box(df,x='hour',y='value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"大量的异常值出现在三点"},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(df['value'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"画出直方图，可以看出数据多分布在40-50以内"},{"metadata":{"trusted":true},"cell_type":"code","source":"#从sklearn中导入相应的算法\n# 导入OneClassSVM\nfrom sklearn.svm import OneClassSVM\n# 导入IsolationForest\nfrom sklearn.ensemble import IsolationForest\n# 导入LocalOutlierFactor\nfrom sklearn.neighbors import LocalOutlierFactor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们使用EM值来评估异常检测的性能"},{"metadata":{"trusted":true},"cell_type":"code","source":"#此部分不用看\nimport numpy as np\nfrom sklearn.metrics import auc\n\n#Source:https://github.com/ngoix/EMMV_benchmarks/blob/master/em.py\n\ndef em(t, t_max, volume_support, s_unif, s_X, n_generated):\n    EM_t = np.zeros(t.shape[0])\n    n_samples = s_X.shape[0]\n    s_X_unique = np.unique(s_X)\n    EM_t[0] = 1.\n    for u in s_X_unique:\n        # if (s_unif >= u).sum() > n_generated / 1000:\n        EM_t = np.maximum(EM_t, 1. / n_samples * (s_X > u).sum() -\n                          t * (s_unif > u).sum() / n_generated\n                          * volume_support)\n    amax = np.argmax(EM_t <= t_max) + 1\n    if amax == 1:\n        print(\"failed to achieve t_max\")\n        amax = -1\n    AUC = auc(t[:amax], EM_t[:amax])\n    return AUC, EM_t, amax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#查看数据框样式\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可见共有4032行数据，每行数据有两列，即是两个维度"},{"metadata":{"trusted":true},"cell_type":"code","source":"#此部分不用看\n# parameters of the algorithm:\nn_generated = 100000\nt_max = 0.9\n\nlim_inf = df['value'].values.min(axis=0)\nlim_sup = df['value'].values.max(axis=0)\nvolume_support = (lim_sup - lim_inf).prod()\nt = np.arange(0, 100 / volume_support, 0.01 / volume_support)\nunif = np.random.uniform(lim_inf, lim_sup,size=(n_generated, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One Class SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#实例化算法类\none_svm=OneClassSVM()\n# 拟合并预测\none_svm_result=one_svm.fit_predict(df['value'].values.reshape(-1,1))\n#定义新数据框，存储预测结果\none_svm_result_df=pd.DataFrame()\none_svm_result_df['timestamp']=df['timestamp']\none_svm_result_df['value'] = df['value']\n\n#把-1的标签修改为1，使得后面的画图过程更加标准，不需要修改\none_svm_result_df['anomaly']  = [1 if i==-1 else 0 for i in one_svm_result]\n\n#此部分不用看\ns_X_ocsvm = one_svm.decision_function(df['value'].values.reshape(-1,1)).reshape(1, -1)[0]\ns_unif_ocsvm = one_svm.decision_function(unif).reshape(1, -1)[0]\nauc_ocsvm, em_ocsvm, amax_ocsvm = em(t, t_max, volume_support,s_unif_ocsvm, s_X_ocsvm, n_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will store the EM values for all the models in a list\n#此部分不用看\nem_values=[]\nmodel_name=[]\nem_values.append(em_ocsvm.mean())\nmodel_name.append(\"One Clas SVM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#统计anomaly列中的值\none_svm_result_df['anomaly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#导入plotly进行画图\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=one_svm_result_df['timestamp'], y=one_svm_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=one_svm_result_df[one_svm_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using One Class SVM')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"可见异常值较多，并非我们所期望的，于是重新调整nu值，修改参数来拟合结果。、"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 对此处nu值进行多次尝试,也可尝试其他参数值，找到看起来最好的结果即可，算法并不能完全好的拟合数据，这是正常的情况\none_svm=OneClassSVM(nu=0.03)\none_svm_result=one_svm.fit_predict(df['value'].values.reshape(-1,1))\none_svm_result_df=pd.DataFrame()\none_svm_result_df['timestamp']=df['timestamp']\none_svm_result_df['value'] = df['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\none_svm_result_df['anomaly']  = [1 if i==-1 else 0 for i in one_svm_result]\ns_X_ocsvm = one_svm.decision_function(df['value'].values.reshape(-1,1)).reshape(1, -1)[0]\ns_unif_ocsvm = one_svm.decision_function(unif).reshape(1, -1)[0]\nauc_ocsvm, em_ocsvm, amax_ocsvm = em(t, t_max, volume_support,s_unif_ocsvm, s_X_ocsvm, n_generated)\n\n#we will store the EM values for all the models in a list\n\nem_values=[]\nmodel_name=[]\nem_values.append(em_ocsvm.mean())\nmodel_name.append(\"One Clas SVM\")\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=one_svm_result_df['timestamp'], y=one_svm_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=one_svm_result_df[one_svm_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using One Class SVM')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到用ocsvm来进行预测时得到的效果是不太好，此时就可以考虑使用其他的算法进行处理，而不要只使用这一种算法"},{"metadata":{"trusted":true},"cell_type":"code","source":"#例如我使用统计学的方式进行处理，认为超出范围的即为异常值\n\ndef detect_IQR(df,feature):\n    下四分位数 = df[feature].quantile(q=0.25)\n    上四分位数 = df[feature].quantile(q=0.75)\n    IQR = 上四分位数-下四分位数\n    下界点= 下四分位数 - 3*IQR\n    上界点 = 上四分位数 + 3*IQR\n    \n    return 下界点,上界点\n下界点,上界点 = detect_IQR(df,\"value\")\nprint(下界点,上界点)\nIQR_result_df=pd.DataFrame()\nIQR_result_df['timestamp']=df['timestamp']\nIQR_result_df['value'] = df['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\nIQR_result_df.loc[(IQR_result_df[\"value\"]<下界点) | (IQR_result_df[\"value\"]>上界点),\"anomaly\"] = 1\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=IQR_result_df['timestamp'], y=IQR_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=IQR_result_df[IQR_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using IQR')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到利用统计学进行计算得到的结果是比较好的，也是比较符合我们的期望的。"},{"metadata":{"trusted":true},"cell_type":"code","source":"#下面我们使用统计学计算出的异常值认为是真实的异常值，对于OneClassSVM进行参数的调整。\ndf_true = IQR_result_df\nprint(df_true.head())\n#处理空值\ndf_true.loc[df_true[\"anomaly\"].isnull(),\"anomaly\"] = 0\ndf_true[\"anomaly\"] = df_true[\"anomaly\"].astype(\"int\")\nprint(df_true[\"anomaly\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n\nX_trainval,X_test,y_trainval,y_test = train_test_split(df_true[\"value\"].values.reshape(-1,1),df_true[\"anomaly\"].values.reshape(-1,1),random_state=0)\nX_train ,X_val,y_train,y_val = train_test_split(X_trainval,y_trainval,random_state=1)\n# grid search start\nbest_score = 0\nfor nu in [0.001,0.0001,0.004,0.01,0.03,0.05,0.00001,0.000001]:\n    # 对于每种参数可能的组合，进行一次训练\n    ocsvm = OneClassSVM(nu=nu)\n    # 5 折交叉验证\n#     scores = cross_val_score(ocsvm,X_trainval,y_trainval,cv=5,scoring='recall_micro')\n#     score = scores.mean()\n#     print(nu)\n#     print(score)\n    trainX = X_train[y_train==0].reshape(-1,1)\n    ocsvm.fit(trainX)\n    yhat = ocsvm.predict(X_val)\n    # mark inliers 1, outliers -1\n    y_val[y_val == 1] = -1\n    y_val[y_val == 0] = 1\n    # calculate score\n    score = f1_score(y_val, yhat, pos_label=-1)\n    print(nu,score)\n#     score = precision_score(y_val, yhat, pos_label=-1)\n#     score = recall_score(y_val, yhat, pos_label=-1)\n    # 找到表现最好的参数\n    # 找到表现最好的参数\n    if score > best_score:\n        best_score = score\n        best_parameters= {'nu':nu}\n        print(best_parameters,best_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到nu值越小，f1值越大，说明此时效果比较好。\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ocsvm = OneClassSVM(nu=0.000001)\n#选取全部为正例的数据，用这部分数据来拟合模型。\ntrainX = X_train[y_train==0].reshape(-1,1)\nocsvm.fit(trainX)\ny_true = ocsvm.predict(df_true[\"value\"].values.reshape(-1,1))\none_svm_result_df=pd.DataFrame()\none_svm_result_df['timestamp']=df_true['timestamp']\none_svm_result_df['value'] = df_true['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\none_svm_result_df['anomaly']  = [1 if i==-1 else 0 for i in y_true]\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=IQR_result_df['timestamp'], y=IQR_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=IQR_result_df[IQR_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using OneClassSVM')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_svm_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"总体上来看我们认为ocsvm的效果并不是最好\n\n### Isolation Forest\n\n因此我们可以选择使用孤立森林来进行异常检测\n\n* The lower, the more abnormal.\n* Negative scores represent outliers, positive scores represent inliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"iso=IsolationForest()\niso_result=iso.fit_predict(df['value'].values.reshape(-1,1))\niso_result_df=pd.DataFrame()\niso_result_df['timestamp']=df['timestamp']\niso_result_df['value'] = df['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\niso_result_df['anomaly']  = [1 if i==-1 else 0 for i in iso_result]\ns_X_iso = iso.decision_function(df['value'].values.reshape(-1,1)).reshape(1, -1)[0]\ns_unif_iso = iso.decision_function(unif).reshape(1, -1)[0]\nauc_iso, em_iso, amax_iso = em(t, t_max, volume_support,s_unif_iso, s_X_iso, n_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"em_values.append(em_iso.mean())\nmodel_name.append(\"Isolation Forest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iso_result_df['anomaly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=iso_result_df['timestamp'], y=iso_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=iso_result_df[iso_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='使用孤立森林检测CPU使用率异常')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iso=IsolationForest(contamination=0.008)\niso_result=iso.fit_predict(df['value'].values.reshape(-1,1))\niso_result_df=pd.DataFrame()\niso_result_df['timestamp']=df['timestamp']\niso_result_df['value'] = df['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\niso_result_df['anomaly']  = [1 if i==-1 else 0 for i in iso_result]\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=iso_result_df['timestamp'], y=iso_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=iso_result_df[iso_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='使用孤立森林检测CPU使用率异常')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看到孤立森林算法比较好的找到了这个数据集中的异常值\n### Local Outlier Factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"lof=LocalOutlierFactor(novelty=True)\nlof.fit(df['value'].values.reshape(-1,1))\nlof_result=lof.predict(df['value'].values.reshape(-1,1))\nlof_result_df=pd.DataFrame()\nlof_result_df['timestamp']=df['timestamp']\nlof_result_df['value'] = df['value']\n\n#Inliers are labeled 1, while outliers are labeled -1.\nlof_result_df['anomaly']  = [1 if i==-1 else 0 for i in lof_result]\n\n#decision_function is not available when novelty=False. If we make novelty=True, then fit_predict\n#is not available\n\n\"\"\"\nThe decision_function method is also defined from the scoring function, \nin such a way that negative values are outliers and non-negative ones are inliers.\n\"\"\"\ns_X_lof = lof.decision_function(df['value'].values.reshape(-1,1))\ns_unif_lof = lof.decision_function(unif).reshape(1, -1)\nauc_lof, em_lof, amax_lof = em(t, t_max, volume_support,s_unif_lof, s_X_lof, n_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"em_values.append(em_lof.mean())\nmodel_name.append(\"LOF\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lof_result_df['anomaly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=lof_result_df['timestamp'], y=lof_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=lof_result_df[lof_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using LOF')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GMM\n\nSource: [Link to Github](https://github.com/rhasanbd/Anomaly-Detection-LOF-IsolationForest-FactMCD-GMM/blob/master/Anomaly%20Detection-LOF-IsolationForest-FastMCD-GMM.ipynb)\n\nTo determine whether a data point is an anomaly we need to compute the log-likelihood of the given data.\n\nWe use the \"score\" method of GMM to compute the per-sample average log-likelihood of the data.\n\nThen, compare the likelihood values with the density threshold.\n\nwe identify the outliers using the first percentile lowest density as the threshold. I.e., approximately 1% of the instances will be flagged as anomalies."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\ngm = GaussianMixture(random_state=0)\ngm.fit(df['value'].values.reshape(-1,1))\n\ndensities = gm.score_samples(df['value'].values.reshape(-1,1))\ndensity_threshold = np.percentile(densities, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gm_result= [-1 if i<density_threshold else 0 for i in densities]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gm_result_df=pd.DataFrame()\ngm_result_df['timestamp']=df['timestamp']\ngm_result_df['value'] = df['value']\n\ngm_result_df['anomaly']  = [1 if i==-1 else 0 for i in gm_result]\ns_X_gm = gm.score_samples(df['value'].values.reshape(-1,1)).reshape(1, -1)[0]\ns_unif_gm = gm.score_samples(unif).reshape(1, -1)[0]\nauc_gm, em_gm, amax_gm = em(t, t_max, volume_support,s_unif_gm, s_X_gm, n_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gm_result_df['anomaly'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"em_values.append(em_gm.mean())\nmodel_name.append(\"GMM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=gm_result_df['timestamp'], y=gm_result_df['value'],\n                    mode='lines',\n                    name='lines'))\n\na=gm_result_df[gm_result_df['anomaly']==1]\n\nfig.add_trace(go.Scatter(x=a.timestamp, y=a.value,\n                    mode='markers',\n                    name='markers'))\n\nfig.update_layout(title='Anomaly detection using GMM')\nfig.show(\"notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result={}\n\nfinal_result={'Model Name':model_name,'EM Value':em_values}\nfinal_result_df=pd.DataFrame(final_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higher EM value corresponds to a better model. In this case, Isolation Forest has performed the best followed by GMM. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}