{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**I believe that this is where the data comes from since it has all the same features: http://rstudio-pubs-static.s3.amazonaws.com/24341_184a58191486470cab97acdbbfe78ed5.html**\n\n**I also believe that the same dataset was posted here on Kaggle 3 years ago. If it is then this is a bit worrying because its a copy of another dataset and in the Discussions of the other dataset people were saying that the target values were actually swapped.\n https://www.kaggle.com/ronitf/heart-disease-uci**\n\n\n\n\nYou can use this link to see the definitions of all the features as well, just scroll down a tiny bit and they should all be there. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No null values in the dataset. Lets see a statistical summary of the data.","metadata":{}},{"cell_type":"code","source":"df.drop(['sex', 'output', 'fbs', 'restecg', 'exng'], axis = 1).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly lets look at if there is a balance in values for the target output.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\ndf_o = df.copy()\ndf_o['output'] = df_o['output'].map({0:'Lower chance of Heart Attack', 1:'Higher chance of Heart Attack'})\nsns.countplot(data = df_o, x = 'output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like the target feature is fairly balanced so we won't need to worry about imbalanced data.","metadata":{}},{"cell_type":"markdown","source":"**First let's take a look at the distribution of age and gender of everyone in the dataset so we can figure out what we're dealing with**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (14, 8),  dpi = 200)\nax = sns.countplot(data = df, x = 'age', )\nax.set(ylim = (0, 20))\nplt.yticks(np.arange(0, 20));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like most ages are in the range of 51 - 59 years old. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12, 7), dpi = 100)\nsns.countplot(data = df, x = 'sex', palette='spring')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not clear from the data which gender is which but there is an imbalance of about 110 meaning that the data may be more biased towards one gender. ","metadata":{}},{"cell_type":"markdown","source":"**What type of chest pain causes an exercise induced angina? How correlated are these features with the Chance of a Heart Attack?**","metadata":{}},{"cell_type":"code","source":"df_c = df[['cp', 'exng', 'output']].copy()\ndf_c['cp'] = df_c['cp'].map({0:'typical angina', 1:'atypical angina', 2:'non-anginal pain', 3:'asymptomatic'})\ndf_c['exng'] = df_c['exng'].map({0:'No', 1:'Yes'})\ndf_c.columns = ['Type of Chest Pain', 'Exercise Induced Angina', 'Chance of Heart Attack']\ndf_c['Chance of Heart Attack'] = df_c['Chance of Heart Attack'].map({0:'Lower', 1:'Higher'})\ndf_c.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 7), dpi = 150)\nsns.countplot(data = df_c, x = 'Type of Chest Pain', hue = 'Exercise Induced Angina', palette = 'Accent');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So clearly there is a significant correlation between type of chest pain and if angina is induced by exercise. If someone has a typical angina they will most likely induce angina again from exercising, but if they have any other type of chest pain, they most likely will not ending up inducing angina from exercise.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols = 2, dpi = 100, figsize = (13, 6))\nsns.countplot(data = df_c, x = 'Type of Chest Pain', hue = 'Chance of Heart Attack', ax = ax[0], palette = 'autumn');\nax[0].set_xticklabels(ax[0].get_xticklabels(), fontsize = 8)\nsns.countplot(data = df_c, x = 'Exercise Induced Angina', hue = 'Chance of Heart Attack', ax = ax[1], palette = 'autumn');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"... I think this is starting to support the theory that the data for the chance of a Heart Attack is actually swapped. According to the first graph, people who experience typical angina have a lower chance of a heart attack than those who do experience typical angina. Not only that, but according to the second graph, people who do not get angina from exercising are more likely to have a heart attack. Both of these conclusions are the opposite of what they should be.","metadata":{}},{"cell_type":"markdown","source":"**Does age affect cholestrol and blood pressure levels? How do these features relate to the chance of a heart attack?  **","metadata":{}},{"cell_type":"code","source":"df_age = df[['age', 'trtbps', 'chol', 'output']].copy()\ndf_age.columns = ['Age', 'Blood Pressure', 'Cholesterol in mg/dl', 'Chance of Heart Attack']\ndf_age['Chance of Heart Attack'] = df_age['Chance of Heart Attack'].map({0:'Lower', 1:'Higher'})\ndf_age = df_age.sort_values(by = 'Chance of Heart Attack', ascending=False)\ndf_age.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data = df_age, x = 'Age', y = 'Cholesterol in mg/dl', hue = 'Chance of Heart Attack', palette='dark', height = 10, s = 100, alpha = 0.5)\nsns.jointplot(data = df_age, x = 'Age', y = 'Blood Pressure', hue = 'Chance of Heart Attack', palette='dark', height = 10, s = 100, alpha = 0.5)\nsns.jointplot(data = df_age, x = 'Cholesterol in mg/dl', y = 'Blood Pressure', hue = 'Chance of Heart Attack', palette='dark', height = 10, s = 100, alpha = 0.5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that these features have little correlation with each others. \n\nFrom the first graph we can see that age does not seem to have much of a relation with cholestrol and both of these variables have little effect on whether the person has a higher or lower chance to have a heart attack.\n\nThe same result applies to the second graph. Age and Blood Pressure have little correlation and don't seem to really affect the chance of a heart attack.\n\nThe third graph shows us that cholestrol and blood pressure also have little relation with each other.\n\nI do want to mention that this is fairly surprising. It's common knowledge that high blood pressure and high cholestrol levels increase the chance for a heart attack and yet the graphs don't seem to support this.\n\nSomething interesting to note here, though, is the patient with a cholestrol level of about 580 mg! Normal levels of cholesterol are 200 or less so 580 is a signifiantly high amount of cholesterol. We'll keep this patient because cholestrol levels of 580 mg are possible.","metadata":{}},{"cell_type":"markdown","source":"**Lets move on to seeing how the maximum heart rate achieved and electrocardiographic results affect the chances of someone having a heart attack.**","metadata":{}},{"cell_type":"code","source":"df_heart = df[['thalachh', 'restecg', 'output']].copy()\ndf_heart.columns = ['Maximum Heart Rate Achieved', 'Resting Electrocardiographic Results', 'Chance of Heart Attack']\ndf_heart['Chance of Heart Attack'] = df_heart['Chance of Heart Attack'].map({0:'Lower', 1:'Higher'})\ndf_heart = df_heart.sort_values(by = 'Chance of Heart Attack', ascending=False)\ndf_heart['Resting Electrocardiographic Results'] = df_heart['Resting Electrocardiographic Results'].map({0:'Normal', 1:'ST-T Wave Abnormality', 2:'Probable or Definite Left Ventricular Hypertrophy'})\ndf_heart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First lets just see the relation between 'Maximum Heart Rate Achieved' and 'Resting Electrocardiographic Results","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(11, 7), dpi = 150)\nsns.barplot(data = df_heart, x = 'Resting Electrocardiographic Results', y = 'Maximum Heart Rate Achieved', palette = 'Dark2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There doesn't seem to be much of a correlation between the electrocardiographic results and the maximum heart rate. Lets move on to comparing the 2 features to the chances of having a heart attack.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8), dpi = 160)\nsns.boxplot(data = df_heart, x = 'Resting Electrocardiographic Results', y = 'Maximum Heart Rate Achieved', hue = 'Chance of Heart Attack', palette = 'rainbow')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resting Electrocardiographic Results does not appear to have much of a relation to the chance of heart attack. However, the maximum heart rate achieved has a significant correlation with the chance of a heart attack. As the maximum heart rate increases, there is a higher chance that the patient will have a heart attack.\n\nOne question though is why does data for 'Probable or Definite Left Ventricular Hypertrophy' have such small boxplots? Lets take a closer look at the data for these rows.","metadata":{}},{"cell_type":"code","source":"df_heart[df_heart['Resting Electrocardiographic Results'] == 'Probable or Definite Left Ventricular Hypertrophy']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that there are only 4 rows in the entire dataset that fall under 'Probable or Definite Left Ventricular Hypertrophy' in the electrocardiographic results. Without enough data for left ventricular hypertrophy, we cannot make any conclusions from the boxplot with this value.","metadata":{}},{"cell_type":"markdown","source":"**Lets now check how the features 'Slope' and 'oldpeak' affect the chance of a heart attack. Since these 2 features are not defined in the description, I will be using the definition from the link I gave at the top.**\n\nOldpeak: ST depression induced by exercise relative to rest.\n\nSlope: Slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping).","metadata":{}},{"cell_type":"code","source":"df_exer = df[['oldpeak', 'slp', 'output']].copy()\ndf_exer.columns = ['ST depression induced by exercise relative to rest', 'Slope of the peak exercise ST segment', 'Chance of Heart Attack']\ndf_exer['Chance of Heart Attack'] = df_exer['Chance of Heart Attack'].map({0:'Lower', 1:'Higher'})\ndf_exer = df_exer.sort_values(by = 'Chance of Heart Attack', ascending=False)\ndf_exer['Slope of the peak exercise ST segment'] = df_exer['Slope of the peak exercise ST segment'].map({0:'Upsloping', 1:'Flat', 2:'Downsloping'})\ndf_exer.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll start, again, by comparing the 2 features and then moving on to seeing if they have a strong correlation with 'Chance of Heart Attack'.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (9, 6), dpi = 100)\nsns.barplot(data = df_exer, x = 'Slope of the peak exercise ST segment', y = 'ST depression induced by exercise relative to rest', order=['Downsloping', 'Flat', 'Upsloping'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There does seem to be a significant correlation between the slope of the peak and Depression induced by exercise. As we can see, Downsloping has a low ST depression level, Flat has a medium ST depression level, and Upsloping has a high ST Depression level. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (11, 7), dpi = 150)\nsns.violinplot(data = df_exer, x = 'Slope of the peak exercise ST segment', y = 'ST depression induced by exercise relative to rest', hue = 'Chance of Heart Attack', palette = 'flare', order=['Downsloping', 'Flat', 'Upsloping'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting with the flat slope we can see that chances of a heart attack are lower when there is a higher ST Depression induced.\n\nFor the down slope it seems that the ST Depression level does not affect the chances of a heart attack. One thing to notice though is the high amount of outliers in downsloping, we can see how thin the violinplot stretches as ST Depression gets higher.\n\nLastly for the up slope, ther is a clear correlation, meaning that the higher the ST Depression level, the lower the chance of heart attack. The up slope in peak exercise also has a mostly normal distribution and both median ST depression levels are higher then most of the other slopes.\n\nIn conclusion, it seems the ST Depression levels are a fairly good predictor of the chances of a Heart Attack (higher ST Depression Levels mean less of a chance of Heart Attacks). The Slope is not as accurate of a predictor but we can see that downsloping causes a fairly high increase in Heart Attacks (We can see this by looking at how stretched the downslope for higher chance of a heart attack is)","metadata":{}},{"cell_type":"markdown","source":"**So out of all the features we've analyzed it looks like ST Depression, Maximum Heart Rate, Exercise Induced Angina, and Chest pain type have a strong correlation with 'Chances of a Heart Attack'. Are there any other features that have a high correlation as well? If so, lets graph it.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 6), dpi = 150)\nsns.heatmap(df.corr().drop('output', axis = 1).loc[['output'], :].transpose(), annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we've already found that the features oldpeak, thalachh, cp, and exng are highly correlated. The only one that we still need to look at is caa (number of major vessels (0-3) colored by flourosopy)","metadata":{}},{"cell_type":"code","source":"df_ca = df[['caa', 'output']].copy()\ndf_ca.columns = ['Number of Major Vessels (0-3) Colored by Flourosopy', 'Chance of Heart Attack']\ndf_ca['Chance of Heart Attack'] = df_ca['Chance of Heart Attack'].map({0:'Lower', 1:'Higher'})\ndf_ca = df_ca.sort_values(by = 'Chance of Heart Attack', ascending=False)\ndf_ca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ca['Number of Major Vessels (0-3) Colored by Flourosopy'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note how some of the values are 4. From what I got from the other Kaggle dataset, 4 means that the value is missing. So we will not include these rows in the graph and drop them from our dataset.","metadata":{}},{"cell_type":"code","source":"df = df[df['caa'] != 4]\ndf_ca['Number of Major Vessels (0-3) Colored by Flourosopy'] = df_ca[df_ca['Number of Major Vessels (0-3) Colored by Flourosopy'] != 4]\nplt.figure(figsize = (12, 6), dpi = 100)\nsns.countplot(data = df_ca, x = 'Number of Major Vessels (0-3) Colored by Flourosopy', hue = 'Chance of Heart Attack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that having 0 major vessels that are colored by flourosopy is a sign that a patient may have a higher chance of a heart attack. The more major vessels that are colored, however, the more likely a patient has a lower chance of a heart attack.","metadata":{}},{"cell_type":"markdown","source":"**Alright that's enough EDA for now. Let's move on to classifying the chance of a heart attack.**","metadata":{}},{"cell_type":"markdown","source":"Note that I'm going to be filling in the parameters with values I've already tested with GridSearchCV. It would take too long to run with Grid Search which is why I'm doing this.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nX_l = df.drop('output', axis = 1)\ny_l = df['output'].map({0:'Less chance of Heart Attack', 1:'More chance of a Heart Attack'})\n\n\n\ndums = ['cp', 'restecg', 'slp', 'caa', 'thall']\nfor i in dums:\n    col = pd.get_dummies(df[i], drop_first=True)\n    X_l = pd.concat([X_l, col], axis = 1)\n    X_l = X_l.drop(i, axis = 1)\n\n\nX_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_l, y_l, test_size=0.25, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline([('sc', StandardScaler()), ('clf', LogisticRegression(C = 0.1, l1_ratio = 0.1, max_iter = 1000, penalty = 'elasticnet', solver = 'saga'))])\npipe.fit(X_train_l, y_train_l)\n#param_grid = {'clf__C':[0.1, 1, 10, 100, 1000], 'clf__penalty':['l1', 'l2', 'elasticnet'], 'clf__class_weight': [None, 'balanced'], 'clf__solver':['saga'], 'clf__max_iter':[1000], 'clf__l1_ratio':[0, 0.1, 0.5, 0.9, 1] }\n#grid = GridSearchCV(pipe, param_grid)\n#grid.fit(X_train_l, y_train_l)\n#grid.best_params_","metadata":{"tags":["outputPrepend"],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, plot_confusion_matrix\nfig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(pipe, X_test_l, y_test_l, ax = ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pipe.predict(X_test_l)\nprint(classification_report(y_test_l, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So Logistic Regression performs fairly well with an 88% accuracy rate. There do seem to be more false positives than false negatives but all in all Logistic Regression did fairly well on the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\npipe = Pipeline([('sc', StandardScaler()), ('sv', SVC())])\npipe.fit(X_train_l, y_train_l)\n#param_grid = {'sv__C':[0.001, 0.1, 1.0, 10, 100], 'sv__kernel':['poly', 'rbf', 'sigmoid'], 'sv__degree':[2,3,4,5], 'sv__gamma':['scale', 'auto'], 'sv__class_weight':[None, 'balanced']}\n#grid = GridSearchCV(pipe, param_grid)\n#grid.fit(X_train_l, y_train_l)\n#grid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(pipe, X_test_l, y_test_l, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pipe.predict(X_test_l)\nprint(classification_report(y_test_l, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM's have an 89% accuracy which is better than the accuracy of Logistic Regression but only by 1%. It's also much closer to being equal in the amount of false negatives and false positives.","metadata":{}},{"cell_type":"code","source":"X_tree = df.drop('output', axis = 1)\ny_tree = df['output'].map({0:'Less chance of Heart Attack', 1:'More chance of a Heart Attack'})\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tree, y_tree, test_size=0.25, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap = True, class_weight= 'balanced_subsample', max_features = 'auto', min_samples_leaf = 3, min_samples_split =  4 )\n#param_grid = {'n_estimators':[100, 300, 500, 1000], 'criterion':['gini', 'entropy'], 'min_samples_split':[2,3,4], 'min_samples_leaf':[1,2,3], 'max_features':['auto', 'sqrt', 'log2'], 'bootstrap':[True, False], 'class_weight':['balanced', 'balanced_subsample', None]}\n#grid = GridSearchCV(rfc, param_grid)\n#grid.fit(X_train_t, y_train_t)\n#grid.best_params_\nrfc.fit(X_train_t, y_train_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(rfc, X_test_t, y_test_t, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rfc.predict(X_test_t)\nprint(classification_report(y_test_t, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So Random Forest ends up with an 85% accuracy which is worse than all of the other models, this seems to be mainly due to having more false negatives than other models.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(criterion = 'friedman_mse', learning_rate = 0.01, loss = 'deviance', max_depth = 2, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 3)\n#param_grid = {'loss':['deviance', 'exponential'], 'learning_rate':[0.01, 0.1, 1, 10, 100], 'n_estimators':[100, 300, 500], 'criterion':['friedman_mse', 'mse'], 'max_depth':[2,3,4,5], 'max_features':['auto', 'sqrt', 'log2'], 'min_samples_split':[2,3,4], 'min_samples_leaf':[1,2,3]}\n#grid = GridSearchCV(gbc, param_grid)\n#grid.fit(X_train_t, y_train_t)\n#grid.best_params_\ngbc.fit(X_train_t, y_train_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(gbc, X_test_t, y_test_t, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = gbc.predict(X_test_t)\nprint(classification_report(y_test_t, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So while most of the models end up having 1 more in the false negative section, Gradient Boosting ended up having 1 more in the false positive section getting an 87% accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nadc = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 0.1, n_estimators = 100)\n#param_grid = {'n_estimators':[50, 100, 300, 500], 'learning_rate':[0.1, 1, 10, 100, 1000], 'algorithm':['SAMME', 'SAMME.R']}\n#grid = GridSearchCV(adc, param_grid)\n#grid.fit(X_train_t, y_train_t)\n#grid.best_params_\nadc.fit(X_train_t, y_train_t)","metadata":{"tags":["outputPrepend"],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 6), dpi = 100)\nplot_confusion_matrix(adc, X_test_t, y_test_t, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = adc.predict(X_test_t)\nprint(classification_report(y_test_t, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adaboosting seems to do as well as most of the others with an 87% accuracy and, like Gradient boosting, has 1 more false negative than false positive","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmc = MinMaxScaler()\nX_n = df.drop('output', axis = 1)\ny_n = df['output']\n\ndums = ['cp', 'restecg', 'slp', 'caa', 'thall']\nfor i in dums:\n    col = pd.get_dummies(df[i], drop_first=True)\n    X_n = pd.concat([X_n, col], axis = 1)\n    X_n = X_n.drop(i, axis = 1)\n\n\nX_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n, y_n, test_size=0.25, random_state=42)\n\nX_train_n = mc.fit_transform(X_train_n)\nX_test_n = mc.transform(X_test_n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(80, activation = 'relu'))\nmodel.add(Dropout(rate = 0.5))\n\nmodel.add(Dense(40, activation = 'relu'))\nmodel.add(Dropout(rate = 0.5))\n\nmodel.add(Dense(20, activation = 'relu'))\nmodel.add(Dropout(rate = 0.5))\n\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 25, verbose=1)\nmodel.fit(x = X_train_n, y = y_train_n, epochs = 600, validation_data=(X_test_n, y_test_n), callbacks=[early_stop])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_classes(X_test_n)\nprint(classification_report(y_test_n, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neural Networks ends up with an 91% accuracy which is the best out of all models! A lower false positive and false negative rate than most of the models as well.","metadata":{}},{"cell_type":"markdown","source":"So in the end, Neural Networks is the model that does best with an 91% accuracy. Thank you for your time!","metadata":{}}]}