{"cells":[{"metadata":{},"cell_type":"markdown","source":"IN THIS CODE WE ARE GOING TO PREPROCESS OUR ORIGINAL DATABASE, THE CORD-19 CHALLENGE ONE, SO IT CAN BE EASIER PROCESSED LATER FOR OUT OBJECTIVE. \n\nWe are goiing to develop this pre-process in 3: \n1. Cleanser\n2. Eliminate duplicates\n3. Select English languae\n\nAnd as an extra , we are going to write down our personalized STOPWORDS for later. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*********************\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***************\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**STEP 1: CLEANSER**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before developping our code we are going to clean it. Therefore, we are going to use this previous notebook: https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/data?\nWe are almost using the same things that where developed, but with sigle differences. As we want to focus on the BIORXIV part and on the abstracts.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. IMPORT LIBRARIES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. CREATE FUNCTIONS\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Biorxiv: Exploration**\n\n> -> load all of the json files into a list of nested dictionaries (each dict is an article).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_dir = '/kaggle/input/cord1933k/biorxiv_medrxiv/biorxiv_medrxiv/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = all_files[0]\nprint(\"Dictionary keys:\", file.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"->* Abstract*\nThe abstract dictionary is fairly simple:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(file['abstract'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> eventually, we can look at other elements, as for example the body text\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dictionary \nprint(\"body_text type:\", type(file['body_text']))\nprint(\"body_text length:\", len(file['body_text']))\nprint(\"body_text keys:\", file['body_text'][0].keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The content: the body text is separated into a list of small subsections, each containing a section and a text key. Since multiple subsection can have the same section, we need to first group each subsection before concatenating everything.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"body_text content:\")\npprint(file['body_text'][:2], depth=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see what the grouped section titles are for the example above:\n\ntexts = [(di['section'], di['text']) for di in file['body_text']]\ntexts_di = {di['section']: \"\" for di in file['body_text']}\nfor section, text in texts:\n    texts_di[section] += text\n\npprint(list(texts_di.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> or the metadata","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dictionary\nprint(all_files[0]['metadata'].keys())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_files[0]['metadata']['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"authors = all_files[0]['metadata']['authors']\npprint(authors[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use of format name and affiliation\nfor author in authors:\n    print(\"Name:\", format_name(author))\n    print(\"Affiliation:\", format_affiliation(author['affiliation']))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#another example\n#pprint(all_files[4]['metadata'], depth=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> last element, the bibliography","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bibs = list(file['bib_entries'].values())\npprint(bibs[:2], depth=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#format_authors(bibs[1]['authors'], with_affiliation=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following function let you format the bibliography all at once. It only extracts the title, authors, venue, year, and separate each entry of the bibliography with a ;.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bib_formatted = format_bib(bibs[:5])\nprint(bib_formatted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Biorxiv: Generate CSV\n\nIn this section, I show you how to manually generate the CSV files.\nAs you can see, it's now super simple because of the format_ helper functions.\nIn the next sections, I show you have to generate them in 3 lines using the load_files and generate_clean_dr helper functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncol_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.to_csv('biorxiv_clean.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> we cab do tge same for Generate Custom (PMC), Commercial, Non-commercial licenses, it will be the blow code repeated for each of them (x3)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pmc_dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/'\n#pmc_files = load_files(pmc_dir)\n#pmc_df = generate_clean_df(pmc_files)\n#pmc_df.to_csv('clean_pmc.csv', index=False)\n#pmc_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BUT WE ARE NOT INTERESTED","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now the data is clean , we have decided we are going to focus on the ABSTRACT from the BIORXIV part, and that we'll do. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean = pd.read_csv('/kaggle/input/cord1933k/biorxiv_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#verification everything is allright\n#biorxiv_clean.head(2)\n#first paper example\n#biorxiv_clean.text[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*****************\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**STEP 2: ELIMINATE DUPLICATES**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we have the CSV datafram, we are going to analize the content.For that , we use a counter.  This part is highly inspired on this previous notebook,  https://www.kaggle.com/maksimeren/covid-19-literature-clustering from which we extracted what interested us. \n\n-> Adding word count columns for both abstract and body_text can be useful parameters later:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract_word_count'] = biorxiv_clean['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = biorxiv_clean['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=biorxiv_clean['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> When we look at the unique values above, we can see that tehre are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n\n(Thank you Desmond Yeoh for recommending the below approach on Kaggle)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['body_text'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**STEP 3: SELECT ENGLISH**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"-> Handling multiple languages\nNext we are going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text  #CAMBIAR ESTO. \nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50: \n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> Lets take a look at the language distribution in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be dropping any language that is not English.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['language'] == 'en'] \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**********************************","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**EXTRA: OUR PERSONALIZED STOPWORDS  (TO BE USED  IN THE PROCESSING PART)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Stopwords can be extracted from various libraries. Dos not matter which one we choose to use, as far as it is the english ones. \n\nWe also need to customize it a bit so they are the most accurate they can be. \n\nThis part was inspired in various notebooks such as: \n\nhttps://www.kaggle.com/mobassir/mining-covid-19-scientific-papers \nhttps://www.kaggle.com/imdevskp/covid-19-analysis-visualization-comparisons\nhttps://www.kaggle.com/imdevskp/covid-19-analysis-visualization-comparisons\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**-> OPTION 1:** from wordcloud. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nstop_words = set(STOPWORDS)\n#https://www.kaggle.com/gpreda/cord-19-solution-toolbox\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"but we cannot customize them!!! Therefore:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**-> OPTION 2:** from NLTK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis',\n                   'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n                   'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n                   'al.', 'Elsevier', 'PMC', 'CZI', 'www'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First visualization of words of the BIORXIV papers so we can have a first idea of what we may find -> WordCloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stop_words,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(biorxiv_clean['abstract'], title = 'biorxiv_clean - papers Abstract - frequent words (400 sample)')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}