{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0692f4bf-a05f-49e5-5af2-ccab908de34d"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fad22b9a-629d-a9f6-64a8-300658113828"},"outputs":[],"source":"data = pd.read_csv(\"../input/data.csv\",header=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4582bb45-75f6-8c02-b9a6-410e552694c0"},"outputs":[],"source":"print(data.head(5))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30a42af8-8b13-39e0-af94-e566ecfc336a"},"outputs":[],"source":"data.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3fdc542-95d6-70b0-c444-c173bd49470b"},"outputs":[],"source":"data.drop(\"Unnamed: 32\",axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cf81b8d-d190-3f7c-3b28-381205ad680f"},"outputs":[],"source":"#we don't need id either\ndata.drop(\"id\", axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"264e3645-83a7-8ab8-4d9a-ffdaf5eb6a76"},"outputs":[],"source":"#let's divide the data into 3 parts i.e. mean, se and worst\nmean= list(data.columns[1:11])\nse= list(data.columns[11:21])\nworst= list(data.columns[21:31])\nprint(mean)\nprint(se)\nprint(worst)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6a471f3-b083-d387-4f11-42a545c630b4"},"outputs":[],"source":"#let's start with the mean part\ndata['diagnosis']= data['diagnosis'].map({'M':1, 'B':0})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8858b8da-00d1-fcc3-2ae4-4228ec462828"},"outputs":[],"source":"import seaborn as sns\nsns.countplot(data['diagnosis'], label=\"Count\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a363b4d-abf5-b61d-5541-85d4892a2c1d"},"outputs":[],"source":"data1 = data[mean]\nvals= data1.values\nX = vals[:, 0:10]\ny = data['diagnosis'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bd2455d-a381-e765-2125-41a119fdd4d6"},"outputs":[],"source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b14ebbd2-3358-059a-6a27-e878fa5ca610"},"outputs":[],"source":"data1.head(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"eae1e15f-0469-d1ac-a7c0-4b7c795891b4"},"source":"**Features are radius_mean, perimeter_mean, area_mean, concavity_mean and concave points_mean**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bab58b7c-09e7-9bf3-fd2c-b4cbd9f82117"},"outputs":[],"source":"# for random forest\npredrfc = ['radius_mean','perimeter_mean','area_mean','concavity_mean','concave points_mean']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e7a52b8-a87a-37f4-185d-01f25ccf3c32"},"outputs":[],"source":"#for logistic regression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nrfe = RFE(model, 5)\nfit = rfe.fit(X, y)\nprint(\"Num Features: %d\" % (fit.n_features_))\nprint(\"Selected Features: %s\" % (fit.support_))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))\npredlog = ['radius_mean','perimeter_mean','concavity_mean','concave points_mean','symmetry_mean']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98f78358-9b2f-6276-a789-48122fb6ad69"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ef3ee00-714c-433e-0a3e-d1fbefadaaad"},"source":"Model Classification \n=========="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63b7d9b7-25ab-0304-399c-447e2ca7bfee"},"outputs":[],"source":"from sklearn import metrics\nfrom sklearn.cross_validation import KFold\ndef classification_model(model, data, predictors, outcome):\n    model.fit(data[predictors],data[outcome])\n    predictions = model.predict(data[predictors])\n    accuracy = metrics.accuracy_score(predictions,data[outcome])\n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n    #Perform k-fold cross-validation with 5 folds\n    kf = KFold(data.shape[0], n_folds=5)\n    error = []\n    for train, test in kf:\n        \n        # Filter training data\n        train_predictors = (data[predictors].iloc[train,:])\n    \n        # The target we're using to train the algorithm.\n        train_target = data[outcome].iloc[train]\n    \n        # Training the algorithm using the predictors and target.\n        model.fit(train_predictors, train_target)\n    \n        #Record error from each cross-validation run\n        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n    \n        print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n    \n    #Fit the model again so that it can be refered outside the function:\n    model.fit(data[predictors],data[outcome]) "},{"cell_type":"markdown","metadata":{"_cell_guid":"812da2d6-5320-8709-6afa-f597189a7653"},"source":"LOGISTIC REGRESSION\n---"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d46ce7c6-2560-2fa1-b8e3-e79f76c1c3de"},"outputs":[],"source":"predictor_var = predlog\noutcome_var='diagnosis'\nmodel=LogisticRegression()\nclassification_model(model,train,predictor_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b262ba1e-a71f-66b5-a2af-6186da4425bb"},"source":"Using just one predictor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9d2e920-4458-e978-1d24-a861888036f4"},"outputs":[],"source":"predictor_var = ['radius_mean']\nmodel=LogisticRegression()\nclassification_model(model,train,predictor_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9a330cb-ad99-6555-debd-00377cde8b46"},"source":"DECISION TREE CLASSIFIER\n---"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b2f8937-7e38-9831-c826-853f9aa8b58e"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\npredictor_var = predrfc\noutcome_var='diagnosis'\nmodel=DecisionTreeClassifier()\nclassification_model(model,train,predictor_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"deee65f1-eb7a-fb51-1a30-109dd0b5b6da"},"source":"above we see overfitting.\nUsing just one predictor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5e4268f-444d-d118-64a7-b4a2081aa8f3"},"outputs":[],"source":"predictor_var = ['radius_mean']\nmodel=DecisionTreeClassifier()\nclassification_model(model,train,predictor_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"03148322-8ba5-7632-27b2-9b1eb34361b9"},"source":"RANDOM FOREST\n---"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8862811-af38-f66b-68b9-405915df0ad1"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\npredictor_var = predrfc\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model, train,predictor_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ffed25c-02b6-9f60-5773-d341f740b3bc"},"outputs":[],"source":"# using only one predictor\n\npredictor_var = ['radius_mean']\nmodel = RandomForestClassifier(n_estimators=100)\nclassification_model(model, train,predictor_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1bac3f1-5192-114b-e063-ea57075e6a28"},"outputs":[],"source":"# using all the features\n\npredictor_var = mean\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model, train,predictor_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6df0fb18-7bb5-0860-8b48-7a783b214d51"},"source":"Hence we see that when all the features are used, we get better accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"f420e3cd-3a13-f7ec-f0bf-210320f62550"},"source":"Using on the test dataset\n---"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09eefa71-847b-ecbd-7d9e-c2add65813c9"},"outputs":[],"source":"predictor_var = mean\nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(model, test,predictor_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbaed84a-f36e-5352-d4fb-3f8804136f25"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}