{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Criticism Needed","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport joblib\nfrom lightgbm import LGBMRegressor\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.077273Z","iopub.execute_input":"2021-05-26T14:49:22.077627Z","iopub.status.idle":"2021-05-26T14:49:22.086327Z","shell.execute_reply.started":"2021-05-26T14:49:22.077598Z","shell.execute_reply":"2021-05-26T14:49:22.085142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.092904Z","iopub.execute_input":"2021-05-26T14:49:22.093313Z","iopub.status.idle":"2021-05-26T14:49:22.78665Z","shell.execute_reply.started":"2021-05-26T14:49:22.093273Z","shell.execute_reply":"2021-05-26T14:49:22.785402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.789318Z","iopub.execute_input":"2021-05-26T14:49:22.789803Z","iopub.status.idle":"2021-05-26T14:49:22.84713Z","shell.execute_reply.started":"2021-05-26T14:49:22.789737Z","shell.execute_reply":"2021-05-26T14:49:22.845796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"### we are going to replace incorrect values and fill missing values","metadata":{}},{"cell_type":"code","source":"df.salary_range = df.salary_range.fillna(\"0-0\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.852125Z","iopub.execute_input":"2021-05-26T14:49:22.852507Z","iopub.status.idle":"2021-05-26T14:49:22.864273Z","shell.execute_reply.started":"2021-05-26T14:49:22.852474Z","shell.execute_reply":"2021-05-26T14:49:22.862377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.salary_range.replace({'40000':'40000-40000','Oct-15':'102-102','9-Dec':'102-102','3-Apr':'102-102','4-Apr':'102-102','8-Sep':'102-102','4-Jun':'102-102','10-Oct':'102-102','Oct-20':'102-102','Jun-18':'102-102','11-Nov':'102-102','11-Dec':'102-102','2-Apr':'102-102','2-Jun':'102-102','Dec-25':'102-102','10-Nov':'102-102'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.866272Z","iopub.execute_input":"2021-05-26T14:49:22.866768Z","iopub.status.idle":"2021-05-26T14:49:22.896223Z","shell.execute_reply.started":"2021-05-26T14:49:22.866705Z","shell.execute_reply":"2021-05-26T14:49:22.894674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.salary_range.replace({'102-102':'0-0'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.900028Z","iopub.execute_input":"2021-05-26T14:49:22.900497Z","iopub.status.idle":"2021-05-26T14:49:22.916623Z","shell.execute_reply.started":"2021-05-26T14:49:22.900445Z","shell.execute_reply":"2021-05-26T14:49:22.914859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.location.fillna(\"NO LOCATION\", inplace=True)\ndf.department.fillna(\"NO DEPARTMENT\", inplace=True)\ndf.company_profile.fillna(\"NO PROFILE\", inplace=True)\ndf.description.fillna(\"NO DESCRIPTION\", inplace=True)\ndf.requirements.fillna(\"NO REQUIREMENTS\", inplace=True)\ndf.benefits.fillna(\"NO BENEFITS\", inplace=True)\ndf.employment_type.fillna(\"NO EMP_TYPE\", inplace=True)\ndf.required_experience.fillna(\"NO EXPERIENCE\", inplace=True)\ndf.required_education.fillna(\"NO EDUCATION\", inplace=True)\ndf.industry.fillna(\"NO INDUSTRY\", inplace=True)\ndf.function.fillna(\"NO FUNCTIONS\", inplace=True)\ndf.telecommuting.replace({0:\"NO telecommuting\",1:\"YES telecommuting\"},inplace=True)\ndf.has_company_logo.replace({0:\"NO logo\",1:'YES logo'},inplace=True)\ndf.has_questions.replace({0:\"NO questions\",1:\"Yes question\"},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:22.92092Z","iopub.execute_input":"2021-05-26T14:49:22.921589Z","iopub.status.idle":"2021-05-26T14:49:22.975441Z","shell.execute_reply.started":"2021-05-26T14:49:22.921467Z","shell.execute_reply":"2021-05-26T14:49:22.97398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to get probability of output from each columns\nive used count vectorizer on the strings and then used logistic regression on it to get the predicted probability","metadata":{}},{"cell_type":"code","source":"# function to get probability of efeatures\n\ndef get_feat_prob(X,y):\n    # vectorize\n    vec = CountVectorizer(max_features=10000)\n    vec.fit(X)\n    trn,tst,trn_o,tst_o = train_test_split(X, y, test_size=0.4, random_state=20)\n    # transform\n    trn_abs = vec.transform(trn)\n    tst_abs = vec.transform(tst)\n    X_abs = vec.transform(X)\n    # fit to model\n    clf = LogisticRegression(C = 10, n_jobs=-1)\n    clf.fit(trn_abs, trn_o)\n    # predict score\n    tst_preds = clf.predict(tst_abs)\n    print(\"score: \", f1_score(tst_o, tst_preds, average='micro'))\n    print(\"confusion matrix: \\n\\n\", confusion_matrix(tst_o, tst_preds))\n    # predict probabilities and make a dataframe of 1's probability\n    all_preds = clf.predict_proba(X_abs)\n    all_preds_df = pd.DataFrame(all_preds, columns=['zero','one'])\n          \n    return all_preds_df['one']\n#     return clf\n    \ny = df['fraudulent']","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:49:46.279808Z","iopub.execute_input":"2021-05-26T14:49:46.280154Z","iopub.status.idle":"2021-05-26T14:49:46.291241Z","shell.execute_reply.started":"2021-05-26T14:49:46.280124Z","shell.execute_reply":"2021-05-26T14:49:46.289083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i have a better idea... lets check for each of the categorical columns first...\neach_proba = []\nfor col in df.iloc[:,:18]:\n    if df[col].dtype == 'object':\n        print(\"\\n-----------\",col,\"-----------\")\n        each_proba.append(get_feat_prob(df[col],y))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:50:51.616149Z","iopub.execute_input":"2021-05-26T14:50:51.617Z","iopub.status.idle":"2021-05-26T14:51:34.704724Z","shell.execute_reply.started":"2021-05-26T14:50:51.616948Z","shell.execute_reply":"2021-05-26T14:51:34.703274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make list of df's into single dataframe\neach_proba_df = pd.concat(each_proba[:7] ,axis=1) ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:51:49.751048Z","iopub.execute_input":"2021-05-26T14:51:49.751464Z","iopub.status.idle":"2021-05-26T14:51:49.76038Z","shell.execute_reply.started":"2021-05-26T14:51:49.751429Z","shell.execute_reply":"2021-05-26T14:51:49.758887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"each_proba_df.columns = df.columns[1:8]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:51:50.824894Z","iopub.execute_input":"2021-05-26T14:51:50.82526Z","iopub.status.idle":"2021-05-26T14:51:50.830796Z","shell.execute_reply.started":"2021-05-26T14:51:50.82523Z","shell.execute_reply":"2021-05-26T14:51:50.829532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"each_proba_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:51:52.591353Z","iopub.execute_input":"2021-05-26T14:51:52.591788Z","iopub.status.idle":"2021-05-26T14:51:52.614083Z","shell.execute_reply.started":"2021-05-26T14:51:52.59174Z","shell.execute_reply":"2021-05-26T14:51:52.612554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I forgot to use benefits, but then when i added it i noticed that the accuracy decreased so i continued without **benefits** column\n\nand the rest of the columns haven't given good results as shown above so i have used them in another way below\n","metadata":{}},{"cell_type":"markdown","source":" i haven't used **'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education'** columns so i thought of using them here... it did improve our accuracy in predicting the fraud data to 95% so i have kept and used this... ( overall accuracy was 99% in both fraud and not fraud)","metadata":{}},{"cell_type":"code","source":"left = df.iloc[:,9:-3]\nleft.head()\n\nX_left = pd.get_dummies(left)\ny = df.fraudulent\n\nX_trainl,X_testl,y_trainl,y_testl = train_test_split(X_left,y, test_size=0.3,random_state=42)\n\nclf_left = XGBClassifier()\nclf_left.fit(X_trainl,y_trainl)\nprint(clf_left.score(X_testl,y_testl))\nprint(confusion_matrix(y_testl,clf_left.predict(X_testl)))\n\nX_col_left = pd.DataFrame(clf_left.predict(X_left))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:53:35.541021Z","iopub.execute_input":"2021-05-26T14:53:35.541465Z","iopub.status.idle":"2021-05-26T14:53:36.515215Z","shell.execute_reply.started":"2021-05-26T14:53:35.541433Z","shell.execute_reply":"2021-05-26T14:53:36.514281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The below function is uses clustering approach to cluster our data into how many ever groups it can (in our case 2) and apply the passes models onto each cluster so that we can see what works best","metadata":{}},{"cell_type":"code","source":"def clustering_approach(X,y, models,type = \"none\"):\n    \n    dfs = {}\n    X_cls = {}\n    y_cls = {}\n    X_scaled = {}\n    X_train, X_test, y_train, y_test = {},{},{},{}\n    y_pred = {}\n    models_out = {}\n    \n    # create knn model and predict\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X,y)\n    df = pd.concat([X,y],axis=1) # so we can later separate x and y for each cluster\n    df['knn_clf'] = knn_clf.predict(X)\n    no_cls = knn_clf.classes_\n    \n    # get the dataframes, apply std.scaler, form train, test sets, apply models\n    for cls in knn_clf.classes_:\n        print(\"--------------The {} cluster's results-------------------\".format(cls),end=\"\\n\\n\")\n        dfs[cls] = df[df['knn_clf'] == cls].iloc[:,:-1]\n        \n        X_cls[cls] = dfs[cls].iloc[:,:-1]\n        y_cls[cls] = dfs[cls].iloc[:,-1]\n        scaler = StandardScaler()\n        X_scaled[cls] = scaler.fit_transform(X_cls[cls])\n#         X_scaled[cls] = pd.DataFrame(X_scaled[cls],columns=df.columns[:-1])\n        \n        X_train[cls],X_test[cls],y_train[cls],y_test[cls] = train_test_split(X_scaled[cls],y_cls[cls],test_size=0.4,random_state=42)\n        print(y_train[cls].value_counts())\n        print(y_test[cls].value_counts())\n        \n        # type can be used for analyzing... eg: confusion matrix\n        for model in models:\n            model.fit(X_train[cls], y_train[cls])\n            y_pred[cls] = model.predict(X_test[cls])\n            print(model)\n            print(model.score(X_test[cls],y_test[cls]))\n            print(confusion_matrix(y_pred[cls], y_test[cls]), end=\"\\n\\n\")\n            print((cross_val_score(model,X_scaled[cls],y_cls[cls])).mean())\n            models_out[str(model) + str(cls)] = model\n            \n    \n    return [X_train, X_test, y_train, y_test,knn_clf, models_out]\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:53:43.774513Z","iopub.execute_input":"2021-05-26T14:53:43.774896Z","iopub.status.idle":"2021-05-26T14:53:43.79079Z","shell.execute_reply.started":"2021-05-26T14:53:43.774862Z","shell.execute_reply":"2021-05-26T14:53:43.789674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new = pd.concat([each_proba_df,X_col_left], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:53:44.989335Z","iopub.execute_input":"2021-05-26T14:53:44.98968Z","iopub.status.idle":"2021-05-26T14:53:44.995051Z","shell.execute_reply.started":"2021-05-26T14:53:44.989651Z","shell.execute_reply":"2021-05-26T14:53:44.994241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier(), XGBClassifier(verbosity = 0),LGBMClassifier(),SVC(), GaussianNB()] \n\nX_train, X_test, y_train, y_test,clusterer, models = clustering_approach(X_new,y,models)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:53:46.466729Z","iopub.execute_input":"2021-05-26T14:53:46.467252Z","iopub.status.idle":"2021-05-26T14:54:12.632473Z","shell.execute_reply.started":"2021-05-26T14:53:46.467219Z","shell.execute_reply":"2021-05-26T14:54:12.631338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ive chosen to go with GaussianNB and LGBMClassifier for respective clusters","metadata":{}},{"cell_type":"code","source":"m = [[6577 +3, 2+11],[ 218+9, 312+20]]\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:54:12.634245Z","iopub.execute_input":"2021-05-26T14:54:12.634739Z","iopub.status.idle":"2021-05-26T14:54:12.639554Z","shell.execute_reply.started":"2021-05-26T14:54:12.634706Z","shell.execute_reply":"2021-05-26T14:54:12.638344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m[0][0]+m[1][1])/(m[0][0]+m[0][1]+m[1][1]+m[1][0])\n\n# accuracy","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:54:12.641005Z","iopub.execute_input":"2021-05-26T14:54:12.641546Z","iopub.status.idle":"2021-05-26T14:54:12.658187Z","shell.execute_reply.started":"2021-05-26T14:54:12.641509Z","shell.execute_reply":"2021-05-26T14:54:12.657189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m[1][1])/(m[0][1]+m[1][1])\n\n# accuracy of predicting frauds correctly","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:54:12.659475Z","iopub.execute_input":"2021-05-26T14:54:12.660076Z","iopub.status.idle":"2021-05-26T14:54:12.675455Z","shell.execute_reply.started":"2021-05-26T14:54:12.660037Z","shell.execute_reply":"2021-05-26T14:54:12.673969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Does anyone have any suggestions on which of the above 2 models would be best and why?\n\nI've selected GaussianNB and LGBMClassifier because they are increasing the accuracy of predicting frauds correctly... using others will increase our general accuarcy to 99.4% or 99.5% but our accuracy of predicting frauds will go down to 90% or 91%\n\nour goal should be to protect people from frauds then to classify some real but maybe in properly described job offers as real","metadata":{}},{"cell_type":"markdown","source":"## Thank you","metadata":{}},{"cell_type":"markdown","source":"### Do leave an upvote if this was worth your time","metadata":{}}]}