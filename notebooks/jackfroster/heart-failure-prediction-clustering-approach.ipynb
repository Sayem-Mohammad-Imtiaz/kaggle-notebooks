{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import probplot","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download link:   https://www.kaggle.com/andrewmvd/heart-failure-clinical-data/download\n# api command to download data:   kaggle datasets download -d andrewmvd/heart-failure-clinical-data\n\ndf = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n# all are numeric features\n# no null values...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check for suspicious values that can be null\ndf.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.pairplot(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns:\n    cat_num = df[i].value_counts()\n    print(\"graph for %s: total = %d\" % (i, len(cat_num)))\n    chart = sns.barplot(x=cat_num.index, y=cat_num)\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['age']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['creatinine_phosphokinase']); # outliers... log transform","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['ejection_fraction'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['platelets'])   # outliers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['serum_creatinine'])   # outliers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['serum_sodium'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(['time'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(df['age'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(df['platelets'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(df['creatinine_phosphokinase'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['creatinine_phosphokinase'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(np.log(df['creatinine_phosphokinase']),plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(np.log(df['creatinine_phosphokinase']));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['creatinine_phosphokinase'] = np.log(df['creatinine_phosphokinase'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(df['ejection_fraction'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(df['serum_creatinine'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['serum_creatinine'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(np.log(df['serum_creatinine']),plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(np.log(df['serum_creatinine']));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### creatinine_phosphokinase: In summary, renal injury with high serum CPK values becomes a true concern when levels of CPK reach 5,000 IU/L and the patient has serious co-morbid disease such as volume depletion, sepsis or acidosis. Otherwise, values of up to 20,000 IU/L may be tolerated without untoward event.\n\n","metadata":{}},{"cell_type":"code","source":"df[df['creatinine_phosphokinase'] == 7861.000000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['platelets'] == 850000.000000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### i want to try logistic regression so i wanted to drop these values... but we already have less values so we'll just scale these and create a model","metadata":{}},{"cell_type":"code","source":"df.drop([1,109], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_scaled = pd.DataFrame(X_scaled,columns=df.columns[:-1])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probplot(X_scaled['serum_creatinine'],plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler\n\n# min_max = MinMaxScaler()\n# s_c_scaled = min_max.fit_transform(X_scaled[['serum_creatinine']])\n# s_c , _ = boxcox(s_c_scaled.T[0])\n# probplot(s_c,plot=plt);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.3,random_state=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_clf = LogisticRegression()\nlog_clf.fit(X_train,y_train)\nlog_pred = log_clf.predict(X_test)\nlog_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train,y_train)\ndt_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kn_clf = KNeighborsClassifier()\nkn_clf.fit(X_train,y_train)\nkn_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(log_pred,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lets work on hyper parameter tuning of Logistic regression","metadata":{}},{"cell_type":"code","source":"log_clf = RidgeClassifier()\nlog_clf.fit(X_train,y_train)\nlog_pred = log_clf.predict(X_test)\nlog_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train,y_train)\nrf_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### random forest grid search","metadata":{}},{"cell_type":"code","source":"# n_estimators = [10, 100, 1000]\n# max_features = ['sqrt', 'log2']\n# # define grid search\n# grid = dict(n_estimators=n_estimators,max_features=max_features)\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=rf_clf, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# grid_result = grid_search.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid_result.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(max_features='log2', n_estimators=1000)\nrf_clf.fit(X_train,y_train)\nrf_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using knn for clustering approach","metadata":{}},{"cell_type":"code","source":"kn_clf = KNeighborsClassifier()\nkn_clf.fit(X_scaled,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kn_clf.predict(X_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(kn_clf.classes_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['knn_clf'] = kn_clf.predict(X_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df[df['knn_clf'] == 1].iloc[:,:-1]\ndf2 = df[df['knn_clf'] == 0].iloc[:,:-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = df.iloc[:,:-1]\ny1 = df.iloc[:,-1]\n\nscaler = StandardScaler()\nX_scaled1 = scaler.fit_transform(X1)\n\nX_scaled1 = pd.DataFrame(X_scaled1,columns=df.columns[:-1])\n\nX2 = df.iloc[:,:-1]\ny2 = df.iloc[:,-1]\n\nscaler = StandardScaler()\nX_scaled2 = scaler.fit_transform(X2)\n\nX_scaled2 = pd.DataFrame(X_scaled2,columns=df.columns[:-1])\n\nX_train1,X_test1,y_train1,y_test1 = train_test_split(X_scaled1,y1,test_size=0.3,random_state=3)\nX_train2,X_test2,y_train2,y_test2 = train_test_split(X_scaled2,y2,test_size=0.3,random_state=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def try_models(Xtr,Xts,ytr,yts):\n    for model in [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),KNeighborsClassifier(n_neighbors=2)]:\n        print(model)\n        model.fit(Xtr,ytr)\n        print(model.score(Xts,yts))\n        y_pred = model.predict(Xts)\n        print(confusion_matrix(y_pred,yts))\n        print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"class 1: ***********************************\")\ntry_models(X_train1,X_test1,y_train1,y_test1)\nprint(\"class 2: ***********************************\")\ntry_models(X_train2,X_test2,y_train2,y_test2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tests = []\ny_preds = []\n# for i in range(2):\nmodel = RandomForestClassifier(max_features='log2', n_estimators=1000)\nmodel.fit(X_train1,y_train1)\ny_preds.extend(model.predict(X_test1))\ny_tests.extend(y_test1)\nmodel = RandomForestClassifier(max_features='log2', n_estimators=1000)\nmodel.fit(X_train2,y_train2)\ny_preds.extend(model.predict(X_test2))\ny_tests.extend(y_test2)\n\nprint(accuracy_score(y_preds,y_tests))\nprint(confusion_matrix(y_preds,y_tests))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### knn seems to perform well with a accuracy of 86%","metadata":{}},{"cell_type":"markdown","source":"### although we've go 4 False negatives... we have gotten more of False positives...","metadata":{}},{"cell_type":"code","source":"df.DEATH_EVENT.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1 means a person is dead and 0 means alive... we have less number of deaths but still our death classification is better then not_dead classification... ","metadata":{}},{"cell_type":"markdown","source":"### Lets try to balance our dataset and see what comes...","metadata":{}},{"cell_type":"code","source":"!pip install imblearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Treat imbalanced dataset","metadata":{}},{"cell_type":"code","source":"smt = SMOTETomek(random_state=42)\nX_res,y_res = smt.fit_resample(X,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(y_res)['DEATH_EVENT'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clustering_approach(X,y, models,type = \"none\"):\n    \n    dfs = {}\n    X_cls = {}\n    y_cls = {}\n    X_scaled = {}\n    X_train, X_test, y_train, y_test = {},{},{},{}\n    y_pred = {}\n    models_out = {}\n    \n    # create knn model and predict\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X,y)\n    df = pd.concat([X,y],axis=1) # so we can later separate x and y for each cluster\n    df['knn_clf'] = knn_clf.predict(X)\n    no_cls = knn_clf.classes_\n    \n    # get the dataframes, apply std.scaler, form train, test sets, apply models\n    for cls in knn_clf.classes_:\n        print(\"--------------The {} cluster's results-------------------\".format(cls),end=\"\\n\\n\")\n        dfs[cls] = df[df['knn_clf'] == cls].iloc[:,:-1]\n        \n        X_cls[cls] = dfs[cls].iloc[:,:-1]\n        y_cls[cls] = dfs[cls].iloc[:,-1]\n        scaler = StandardScaler()\n        X_scaled[cls] = scaler.fit_transform(X_cls[cls])\n#         X_scaled[cls] = pd.DataFrame(X_scaled[cls],columns=df.columns[:-1])\n    \n        X_train[cls],X_test[cls],y_train[cls],y_test[cls] = train_test_split(X_scaled[cls],y_cls[cls],test_size=0.3,random_state=3)\n        print(\"here\")\n        # type can be used for analyzing... eg: confusion matrix\n        for model in models:\n            model.fit(X_train[cls], y_train[cls])\n            y_pred[cls] = model.predict(X_test[cls])\n            print(model)\n            print(model.score(X_test[cls],y_test[cls]))\n            print(confusion_matrix(y_pred[cls], y_test[cls]), end=\"\\n\\n\")\n            models_out[str(model) + str(cls)] = model\n            \n    \n    return [X_train, X_test, y_train, y_test,knn_clf, models_out]\n        \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightgbm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info_vals = mutual_info_classif(X_res,y_res)\nmutual_val_df = pd.DataFrame({\"vals\":mutual_info_vals},index=X.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nmutual_val_df.vals.sort_values(ascending=False).plot(kind='bar');\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_res[['time','serum_creatinine','ejection_fraction','platelets','age','serum_sodium','creatinine_phosphokinase']].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodels = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier(), XGBClassifier(verbosity = 0),LGBMClassifier(),SVC()] \nX_train, X_test, y_train, y_test,clusterer, models = clustering_approach(X_res[['time','serum_creatinine','ejection_fraction','platelets','age','serum_sodium','creatinine_phosphokinase']],y_res,models)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks like random forest has worked out for both the clusters","metadata":{}},{"cell_type":"markdown","source":"### Lets find the best params for our models....","metadata":{}},{"cell_type":"code","source":"# n_estimators = [10, 100, 1000,1500]\n# max_features = np.arange(1,20)\n# # define grid search\n# grid = dict(n_estimators=n_estimators,max_features=max_features)\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# grid_result = grid_search.fit(X_res, y_res)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid_result.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets find the total accuarcy and performance now!","metadata":{}},{"cell_type":"code","source":"# grid_result = grid_search.fit(X_train[1], y_train[1])\n# grid_result.best_estimator__","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ny_tests = []\ny_preds = []\ni=0\nmodel = XGBClassifier(verbosity = 0)\nmodel.fit(X_train[i],y_train[i])\ny_preds.extend(model.predict(X_test[i]))\ny_tests.extend(y_test[i])\ni=1\nmodel = RandomForestClassifier(max_features='log2', n_estimators=1000)\nmodel.fit(X_train[i],y_train[i])\ny_preds.extend(model.predict(X_test[i]))\ny_tests.extend(y_test[i])\n    \nprint(accuracy_score(y_preds,y_tests))\nprint(confusion_matrix(y_preds,y_tests))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['RandomForestClassifier()0']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models['RandomForestClassifier()1']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusterer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import joblib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = clusterer\n# filename = \"clusterer\" \n# joblib.dump(model, filename)\n\n# model = models['RandomForestClassifier()0']\n# filename = \"RF_model1\"\n# joblib.dump(model, filename)\n\n# model = models['RandomForestClassifier()1']\n# filename = \"RF_model2\"\n# joblib.dump(model, filename)","metadata":{},"execution_count":null,"outputs":[]}]}