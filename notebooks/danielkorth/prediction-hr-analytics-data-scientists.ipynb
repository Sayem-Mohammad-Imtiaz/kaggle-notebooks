{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import and first overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# plt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv', index_col='enrollee_id')\nX_test = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv', index_col='enrollee_id')\ntrain.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nI will first take a look at the distribution of the data, and then create a ColumnTransformer in the end that will handle the transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = train.count().sum()\ntotal_values = np.product(train.shape)\npercentages = train.isna().sum().reset_index().rename(columns={'index': 'Column', 0:'Missing'})\npercentages['Percentage'] = percentages['Missing']/train.shape[0]*100\nprint(f'Amount of total missing data in train set: {missing_values}\\nRelative amount of missing data: {missing_values/total_values*100:.3f}%')\nprint('-'*20)\nprint(f'Missing data per column:\\n\\n',percentages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.city.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* way too many unique variables for One Hot Encoding, and data is probably rather useless anyways. Let's drop the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.city_development_index.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* since this is a numerical variable that is already between 0 and 1, we shouldn't worry about it"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(data, column):\n    fig, ax = plt.subplots(1,2, figsize=(12,6))\n    perc = percentages.query(f'Column == \"{column}\"')['Percentage'].values\n    sns.countplot(data=data, x=column, ax=ax[0])\n    ax[1].pie(train[column].value_counts().values, labels=train[column].value_counts().index, autopct='%1.1f%%')\n    fig.tight_layout()\n    fig.suptitle(f'Column: \\'{column}\\'   -   Missing data: {perc[0]:.2f}%', fontsize=30)\n    fig.subplots_adjust(top=0.88)\n    fig.show()\nplot(train, 'gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* a lot of missing values, but the data is also very male dominated, so most frequent imputation should not add too much bias\n* One Hot Encoding should be the way to go"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'relevent_experience')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Target Encoding seems reasonable here, since there can be a ordering.<br> 'Has relevant experience' (-> 1) is better to have in a job than not (-> 0) "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'enrolled_university')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* lets just do most frequent imputation to not delete the data, I don't think the values in this column matter too much\n* One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'education_level')\n# pd.crosstab(index=train.education_level, columns=train.enrolled_university)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* same as before/above for missing values\n* Encoding from Primary School (-> 0) to Phd (-> 4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'major_discipline')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* a lot of data is missing, but the STEM is already by far the most prominent value, so most frequent imputation won't add a lot of bias to the data\n* One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'experience')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* very balanced column counts, and also few data missing plus I have a feeling this column might be important. Dropping the rows with missing data is probably the best, as imputation will be very biased (its also only 65 datapoints)\n* convert <1 to 0 and >20 to 21, and treat it as a numeric variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'company_size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* a lot of data missing, and imputing would introduce a lot of bias. Let's drop the whole column, as this probably doesn't correlate with our target value too much aswell (company size seems to be more a personal preference)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'company_type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* same as above, although Pvt Ltd is dominant - lets still drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(train, 'last_new_job')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most Frequent Imputation not the best, but let's give it a try\n* convert 'never' to 0 and '>4' to 5 and treat it as a numerical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.training_hours.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* continuous value with no missing data, leave it as is"},{"metadata":{},"cell_type":"markdown","source":"## Create the ColumnTransformer\n\nI will also add another feature that might be of high importance for prediction:<br>'Experience per last new jobs' - This measures how long a employee stays at his job on average<br>(this assumes that Experience=Years of having a job)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MapTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Encodes the following ordinal variables: \n       \n           relevant_experience\n           education_level\n           experience\n           last_new_job\n       \n       Returns:\n        encoded variables\n       \"\"\"\n    def __init__(self):\n        self.rel_ex_mapping = {'Has relevent experience':1, 'No relevent experience':0}\n        self.ed_level_mapping = {'Primary School': 0, 'High School':1, 'Graduate':2, 'Masters':3, 'Phd':4}\n        self.ex_mapping = {'<1':0, '>20':21}\n        self.last_n_job_mapping = {'never':0, '>4':5}\n    \n    def fit(self, x, y=None):\n        return self\n    \n    def transform(self, x, y=None):\n        df = pd.DataFrame(x, columns=['relevent_experience', 'education_level', 'experience', 'last_new_job'])\n        df['relevent_experience'] = df['relevent_experience'].replace(self.rel_ex_mapping)\n        df['education_level'] = df['education_level'].replace(self.ed_level_mapping)\n        df['experience'] = df['experience'].replace(self.ex_mapping).astype(float)\n        df['last_new_job'] = df['last_new_job'].replace(self.last_n_job_mapping).astype(float)\n        df['experience_per_job'] = df['experience'] / [x+1 for x in df['last_new_job']]\n        \n        return df\n    \n    def get_feature_names(self):\n        return ['relevent_experience', 'education_level', 'experience', 'last_new_job', 'experience_per_job']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DROPPING\nfor x in [train, X_test]:\n    x.drop(columns=['city', 'company_size', 'company_type'], inplace=True)\n    x.dropna(axis=0, subset=['experience'], inplace=True)\n    \nX_train = train.loc[:, train.columns != 'target']\ny_train = train.pop('target')\n    \n# PIPELINE/COLUMNTRANSFORMER\npipeline_imp_ohe = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npipeline_imp_map = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('mapper', MapTransformer())\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('imp_ohe', pipeline_imp_ohe, ['gender', 'enrolled_university', 'major_discipline']),\n    ('imp_map', pipeline_imp_map, ['relevent_experience', 'education_level', 'experience', 'last_new_job']),\n], remainder='passthrough')\n\n\n# DEMO OF WHAT DATA LOOKS LIKE AFTER PREPROCESSING\ndemo = preprocessor.fit_transform(X_train)\npd.DataFrame(demo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMETER TUNING\n# params = {'model__n_estimators': [100, 300, 500, 800],\n#           'model__max_depth': [5, 8, 15, 25, None], \n#           'model__min_samples_split':[2, 5, 10, 15, 100],\n#           'model__min_samples_leaf': [1, 2, 5, 10] } \n\n# randomforest_pipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', RandomForestClassifier())\n# ])\n# randomforest_pipeline.fit(X_train, y_train)\n\n# clf = GridSearchCV(randomforest_pipeline, params, cv=3, verbose=1, n_jobs=-1, scoring='accuracy')\n# scores_m1 = clf.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m1.best_score_:.3f}%')\n# print(f'Best Config: {scores_m1.best_params_}')\n\n\n#OUTCOME\nrandomforest_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', RandomForestClassifier(n_estimators= 500, max_depth= 8, min_samples_leaf=1, min_samples_split=2))\n])\nscores_m1 = cross_val_score(randomforest_pipeline, X_train, y_train, cv=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # HYPERPARAMETER TUNING\n# params = {'model__C':[0.1,1,10],\n#           'model__kernel':['sigmoid'],\n#           'model__degree':[2,3,5],\n#           'model__gamma': [1, 0.1, 0.01] }\n\n# svmpipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', SVC())\n# ])\n# svmpipeline.fit(X_train, y_train)\n\n# clt = GridSearchCV(svmpipeline, params, cv=3, verbose=1, n_jobs=-1, scoring='accuracy')\n# scores_m2 = clt.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m2.best_score_:.3f}%')\n# print(f'Best Config: {scores_m2.best_params_}')\n\n# #OUTCOME\nsvmpipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', SVC(C= .1, kernel= 'rbf', degree=5, gamma=0.1))\n])\nscores_m2 = cross_val_score(svmpipeline, X_train, y_train, cv=3)\nscores_m2.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n# Model 3: XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMETER TUNING\n# params = {'model__learning_rate':[0.5,0.1,0.05, 0.01],\n#           'model__n_estimators':[100, 300, 500, 800],\n#           'model__max_depth':[3,5,8] }\n# fit_params = {'early_stopping_rounds': 5}\n\n# xgbpipeline = Pipeline([\n#     ('preprocessor', preprocessor),\n#     ('model', XGBClassifier())\n# ])\n# xgbpipeline.fit(X_train, y_train)\n\n# clt = GridSearchCV(xgbpipeline, params, cv=3) #fit_params=fit_params)\n# scores_m3 = clt.fit(X_train, y_train)\n\n# print(f'Best accuracy: {scores_m3.best_score_:.3f}%')\n# print(f'Best Config: {scores_m3.best_params_}')\n\n#OUTCOME\nxgbpipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', XGBClassifier(learning_rate= 0.1, n_estimators= 100, max_depth=3, use_label_encoder=False))])\nscores_m3 = cross_val_score(xgbpipeline, X_train, y_train, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'RESULTS:\\nRandom Forest Classifier: {scores_m1.mean():.3f}% accuracy')\nprint(f'Support Vector Classifier: {scores_m2.mean():.3f}% accuracy')\nprint(f'XGBoost Classifier: {scores_m3.mean():.3f}% accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/sample_submission.csv')\n\nxgbpipeline.fit(X_train, y_train)\nsolutions = xgbpipeline.predict(X_test)\n\nX_test['target'] = solutions\nsubmission = X_test.reset_index()[['enrollee_id', 'target']]\n\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style='text-align: center'>That you for reading this notebook to the end!<br>Feel free to upvote and leave a comment.</h2><h4 style='text-align: center'>Also please tell me what I could've done better...<h4>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}