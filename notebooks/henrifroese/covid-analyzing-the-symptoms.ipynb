{"cells":[{"metadata":{},"cell_type":"markdown","source":"The aim of this notebook is to give an overview of symptoms reported relating to Covid-19 from the given dataset. I re-use code partly from \n- https://www.kaggle.com/salmanhiro/covids-incubation-transmission-related-articles\n- https://www.kaggle.com/dgunning/browsing-the-papers-with-a-bm25-search-index\n- "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Going through metadata to find articles relevant to Covid Symptoms"},{"metadata":{},"cell_type":"markdown","source":"## Cleaning and Filtering"},{"metadata":{},"cell_type":"markdown","source":"As you can see, the metadata file lists all articles in the dataset with some of the most relevant information; the \"sha\"-value lets us find/load a specific article from the dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"title\" and \"abstract\" columns provide information about the article's content that we'll use to find relevant articles. Of course, without access to the full text, the article will not be of much help. We can get the full text if either the doi is present or has_full_text is True."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of articles:\", len(df))\nprint(\"Number of articles without full text:\", len(df[df[\"has_full_text\"].isnull() & df[\"doi\"].isnull()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The articles without full text won't be useful, so we'll drop these. We'll also drop columns we won't be using."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[[\"sha\", \"source_x\", \"title\", \"abstract\", \"doi\", \"has_full_text\"]]\ndf = df[df[\"has_full_text\"].notna() | df[\"doi\"].notna()]\nprint(len(df), \"articles left\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the articles with full text access, we'll now check how many articles are missing abstracts and titles."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of articles with missing abstracts:\", len(df[df[\"abstract\"].isnull()]))\nprint(\"Number of articles with missing titles:\", len(df[df[\"title\"].isnull()]))\nprint(\"Number of articles with missing title AND abstract:\", len(df[df[\"title\"].isnull() & df[\"abstract\"].isnull()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can still use articles without abstracts in the metadata file as long as at least the title is present. It seems as though the articles without titles are exactly the articles with missing title AND abstract (these will not be useful at all). Let's drop those."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df[\"abstract\"].notna() | df[\"title\"].notna()]\nprint(len(df), \"articles left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we only have articles left that either have an abstract or title in the metadata file and whose full text is available. Let's clean the titles and abstracts!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef remove_punc_and_lower(s):\n    try:\n        if s:\n            return s.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).lower()  # replacing punctuation w/ space (and making lowercase)\n    except:\n        print(s)\n        \ndef doi_url(d): \n    return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n\ndf['doi'] = df['doi'].apply(lambda s: doi_url(s) if pd.notnull(s) else s)\ndf['title'] = df['title'].apply(lambda s: remove_punc_and_lower(s) if pd.notnull(s) else s)\ndf['abstract'] = df['abstract'].apply(lambda s: remove_punc_and_lower(s) if pd.notnull(s) else s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"//TODO"},{"metadata":{},"cell_type":"markdown","source":"## Selecting Relevant Articles\n\nWe first define a list of keywords relating to \"Symptoms\" that are of interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"symptoms_keywords = {\"symptom\", \"symptoms\", \"symptomatology\", \"semiology\", \"sign\", \"signs\", \"manifestation\"}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following function will now check whether one of the keywords is present in either the abstract or the title of an article."},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_keywords(row, keywords=symptoms_keywords):\n    text = \"\"\n    if pd.notnull(row[\"abstract\"]):\n        text += row[\"abstract\"]\n    if pd.notnull(row[\"title\"]):\n        text += row[\"title\"]\n    for word in text.split():\n        if word in keywords:\n            return True\n    return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = df.apply(filter_keywords, axis=1)  # mask; true for rows that contain the keywords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> _df_symptoms_ is now a dataframe of articles that contain keywords relevant to *symptoms*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_symptoms = df[m]\ndf_symptoms.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of relevant articles:\", len(df_symptoms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_symptoms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (short) EDA of the Metadata\n\nWe now take a quick look at the metadata of articles relevant to Covid-19 Symptoms."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = \" \".join(df_symptoms.title.values) + \" \".join(df_symptoms.title.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install wordcloud\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n!pip install mpld3\nimport mpld3\nmpld3.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords_eng = set(stopwords.words(\"english\"))\ncorpus_without_stopwords = \" \".join([word for word in corpus.split() if word not in stopwords_eng])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nc = Counter(corpus_without_stopwords.split())\nc.most_common(10)\npd.DataFrame(c.most_common(10), columns=[\"word\", \"occurences\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# //TODO"},{"metadata":{},"cell_type":"markdown","source":"# Using The Articles"},{"metadata":{},"cell_type":"markdown","source":"The first objective is to get all the articles with their full text."},{"metadata":{},"cell_type":"markdown","source":"## Full-Text EDA"},{"metadata":{},"cell_type":"markdown","source":"## SciBERT etc."},{"metadata":{},"cell_type":"markdown","source":"## Comorbidities"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}