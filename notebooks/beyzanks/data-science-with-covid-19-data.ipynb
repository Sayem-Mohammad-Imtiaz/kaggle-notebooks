{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\nfrom datetime import datetime #datetime module\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Introduction to Python"},{"metadata":{},"cell_type":"markdown","source":"## Reading Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv\")\ndf.head(10) # first 10 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # information about datas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail() # default -> last 5 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns # names of data columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count() # data count in columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Province/State'].value_counts(sort=True,ascending=True) # sort Province/State value in ascending order ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.size # returns size of dataframe which is equivalent to total number of elements. That is rows x columns.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(12,12))\nsns.heatmap(df.corr(),annot=True,linewidth=.5,fmt='.1f',cbar=True,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Matplotlib"},{"metadata":{},"cell_type":"markdown","source":"**LINE PLOT**\n\nLine plot is better when x axis is time."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Deaths.plot(kind = 'line', color = 'red',label ='Deaths',linewidth=1,alpha = 0.5,grid = True,linestyle = '--')\nplt.legend(loc='upper right') # puts label into plot\nplt.xlabel('x axis')    # name of xlabel\nplt.ylabel('y axis')    # name of xlabel\nplt.title('Line Plot')  # title of plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SCATTER PLOT**\n\nScatter is better when there is correlation between two variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(kind='scatter', x=\"Deaths\", y='Recovered', alpha=0.4, color='blue')\nplt.xlabel('Deaths') # name of xlabel\nplt.ylabel('Recovered') # name of ylabel\nplt.title('Deaths Recovered Scatter Plot') #title of plot\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HISTOGRAM**\n\nHistogram is better when we need to see distribution of numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Deaths.plot(kind = 'hist',bins = 100,figsize = (15,15))\nplt.title(\"Histogram\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATE PLOT**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(x='ObservationDate',y='Recovered',color = 'green',label ='Recovered',linewidth=1,alpha = 0.5,grid = True,linestyle = '--')\nplt.title('', color='black')\nplt.xticks(rotation = 90) # rotates the labels 90 degrees.\n#plt.tight_layout() \nplt._show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Other date plot\n'''df['ObservationDate'] = df['ObservationDate'].map(lambda x: datetime.strptime(str(x), '%m/%d/%Y'))\nx = df['ObservationDate']\ny = df['Recovered']\n\nplt.plot(x,y,color='pink')# plot\nplt.gcf().autofmt_xdate()# beautify the x-labels\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"currency = {\n    \"Dolar\" : \"USD\",\n    \"Türk Lirası\" : \"TR\",\n    \"Euro\" : \"EUR\",\n    \"Sterlin\" : \"GBP\"\n}\nprint('My dictionary :',currency)\nprint(currency.keys())\nprint(currency.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = currency.get('Euro') # get 'Euro''s value\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,v in currency.items(): # print key and value in dictionary\n    print(k+' : '+v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"Sterlin\" in currency: # check 'Sterlin' in dictionary\n  print(\"Yes, 'Sterlin' is one of the keys in the currency dictionary\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"currency['Kanada Doları']='CAD' # adding item\nprint(currency)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#currency.clear() #remove dictionary\n#del currency # delete dictionary ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[:8] # 0-8 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering\nx = df['Deaths']>5000 \ndf[x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df['Recovered']>5000) & (df['Deaths']<500)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby(['ObservationDate','Country/Region']).sum().loc[lambda df: df['Deaths'] > 4000]) # data selection (date format -> %m %d %Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(n=6, weights='Deaths') # selecting random samples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## While and For Loops"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\n\nwhile True:\n    print(i,\"Data Science\")\n    i +=1\n    if i==6:\n        break    #break the loop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print columns names with while loop\ni=0\nwhile i<len(df.columns):\n    print(\"Column\",i, ':' ,df.columns[i])\n    i +=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print columns names with for loop\nfor col in df.columns:\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For pandas we can achieve index and value\nfor index,value in df[['Province/State']][0:5].iterrows():\n    print(index,\" : \",value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate over rows with iterrows()\nfor index, row in df.head(6).iterrows():\n     # access data using column names\n     print(index+1, row['Province/State'], row['Country/Region'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Python Data Science Toolbox"},{"metadata":{},"cell_type":"markdown","source":"## User Defined Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Tuple\nx = (\"C\",\"Java\",\"Python\")\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (\"C\",\"Java\",\"Python\")\ny = list(x)\ny[0] = \"C++\"\nx = tuple(y)\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Values of tuple with for loop\nfor i in x:\n    print(i)\n\nprint(\"\\n\")\n(l1,l2,l3)=x\nprint(\"Values:\",l1,l2,l3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scope"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 2 #Global scope\n\ndef f(y):\n    result = y**x \n    return result\n\nprint(f(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 1\n\ndef func():\n    for count in range(6):\n        count +=1\n    return count\n\nprint(count) # count = 1 global scope\nprint(func()) # count = 6 local scope\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How can we learn what is built in scope\nimport builtins\ndir(builtins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Nested Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def function1(): # outer function\n    print (\"Hello from outer function\")\n    def function2(): # inner function\n        print (\"Hello from inner function\")\n    function2()\n\nfunction1()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example finding number's square with nested function\ndef func1(x):\n    def func2():\n        result = x**2\n        return result\n    return func2()\n\n#number = int(input(\"Please enter a number \")) with input value -> func1(number)\nprint(f\"{5}'s square =\",func1(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Default and Flexible Arguments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# default argument\ndef func(lang='Python'):\n    return lang\n\nprint(\"Programing language:\" ,func())\nprint(\"Programing language:\", func('C++'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flexible arguments *args\ndef func(*args):\n    for i in args:\n        print(i)\n\nfunc(3,5,8,11)\nlist1=[1,2,3,4,5,6]\nfunc(list1)\nlist2=[1,2,3,4,5,6],[2,5,7,8,9,10]\nfunc(list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flexible arguments **kwargs that is dictionary\ndef func(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key} -> {value}\")\n        \nfunc(Class = 'Data Science', Part = '2')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lambda Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lambda function\nnegative = lambda x : -x\nprint(negative(5))\n\nresult = lambda a,b,c : a*b*c\nprint(result(2,5,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Anonymous Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math # for sqrt function\n\nnum_list=[0,4,16,36]\nfunc = map(lambda x:math.sqrt(x),num_list)\nprint(tuple(func))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Iterators"},{"metadata":{"trusted":true},"cell_type":"code","source":"# iteration example\nsubject = \"DataScience\"\nit=iter(subject)\nprint(next(it))\nprint(next(it))\nprint(next(it))\nprint(next(it))\nprint(*it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zip example zip() -> zip lists\nlist1=[\"Dolar\",\"Türk Lirası\",\"Euro\",\"Sterlin\"]\nlist2=[\"USD\",\"TR\",\"EUR\",\"GBP\"]\nz = tuple(zip(list1,list2))\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"un_zip = zip(*z)\nun_list1,un_list2 = tuple(un_zip)\nprint(un_list1)\nprint(un_list2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## List Comprehension"},{"metadata":{"trusted":true},"cell_type":"code","source":"list1 = [1,2,5,8,14,21]\nlist2 = [print(f\"{i}: Odd\") if i%2!=0 else print(f\"{i}: Even\") for i in list1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list comprehension for covid_19_data dataset\nthreshold = sum(df.Deaths)/len(df.Deaths) # average Deaths\nprint(\"Average Deaths:\",threshold) \n# List values between 60000 and 80000 according to deaths level(high or low)\ndf[\"Deaths_Level\"] = [\"High\" if i > threshold else \"Low\" for i in df.Deaths] \ndf.loc[60000:80000,[\"Deaths_Level\",\"Deaths\",\"Country/Region\"]] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Result**<br>\nAs a result, in this table we can see high deaths level in the US, UK and Italy compared to other countries. (because of Covid-19)"},{"metadata":{},"cell_type":"markdown","source":"# 3.Cleaning Data"},{"metadata":{},"cell_type":"markdown","source":"## Diagnose Data for Cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(15) #first 15 data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(15) #last 15 data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In these tables, we can see original data and the deaths level column we added in part 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape # (row,column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # information about datas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For example lets look frequency of countries\nprint(df['Country/Region'].value_counts(dropna =False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # There may be problems with statistics account because there are too many 0 values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mask(df == 0).describe() # 0 values are masked.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For example: compare deaths covid-19 that are deaths_level high or not\n# Black line at top is max\n# Blue line at top is 75%\n# Green line is median (50%)\n# Blue line at bottom is 25%\n# Black line at bottom is min\n# There are no outliers\ndf.boxplot(figsize=(15,15),column='Deaths',by = 'Deaths_Level') # There may be problems because there are too many 0 values and data.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tidy Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Firstly I create new data from covid_19 data to explain melt more easily.\ndata_new = df.loc[55591:55601]    # I only take 10 rows into new data\ndata_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets melt\n# id_vars = what we do not wish to melt\n# value_vars = what we want to melt\nmelted = pd.melt(frame=data_new,id_vars = 'Country/Region', value_vars= ['Deaths','Recovered'])\nmelted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this table, we can see deaths and recovered numbers by country/region."},{"metadata":{},"cell_type":"markdown","source":"## Pivoting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index is name\n# I want to make that columns are variable\n# Finally values in columns are value\nmelted.pivot_table(index = 'Country/Region', columns = 'variable',values='value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenating Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = df.head()\ndata2= df.tail()\nconc_data_row = pd.concat([data1,data2],axis =0,ignore_index =True) # axis = 0 : adds dataframes in row\nconc_data_row # Concatenating data1 and data2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = df['Country/Region'].tail()\ndata2= df['Deaths'].tail()\ndata3= df['Recovered'].tail()\nconc_data_col = pd.concat([data1,data2,data3],axis =1) # axis = 1 : adds dataframes in column\nconc_data_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Types "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert object(str) to categorical and float to int.\ndf['Deaths_Level'] = df['Deaths_Level'].astype('category')\ndf['Confirmed'] = df['Confirmed'].astype('int')\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data and Testing With Assert "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at does covid_19 data have nan value\n# As you can see there are 68558 entries. However Province/State has 44125 non-null object so it has 24433 null object.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets chech Province/State\ndf[\"Province/State\"].value_counts(dropna =False)\n# As you can see, there are 24433 NAN value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop nan values\ndata1=df   # also we will use df to fill missing value so I assign it to data1 variable\ndata1[\"Province/State\"].dropna(inplace = True)  # inplace = True means we do not assign it to new variable. Changes automatically assigned to data\n# So does it work ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check with assert statement\n# Assert statement:\nassert  df[\"Province/State\"].notnull().all() # returns nothing because we drop nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Province/State\"].fillna('empty',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert  df[\"Province/State\"].notnull().all() # returns nothing because we do not have nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # With assert statement we can check a lot of thing. For example\nassert df.columns[0] == 'SNo' #True\n#assert df.Deaths_Level.dtypes == np.int #False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Province/State\"].value_counts(dropna = False) # now there isn't nan value in table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Pandas Foundation"},{"metadata":{},"cell_type":"markdown","source":"## Building Data Frames From Scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data frames from dictionary\nlanguage = [\"English\",\"German\",\"Turkish\"]\nlevel = [\"B2\",\"B1\",\"C2\"]\nlist_label = [\"language\",\"level\"]\nlist_col = [language,level]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\nprint(data_dict)\ndata = pd.DataFrame(data_dict)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add new columns with list comprehension \ndata[\"completed\"]=[\"Yes\" if i == \"C2\"  else \"No\" for i in data.level]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Broadcasting\ndata[\"necessary\"] = \"Yes\" #Broadcasting entire column\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all data \ndata1 = df.loc[:,[\"Confirmed\",\"Deaths\",\"Recovered\"]]\ndata1.plot()\n# it is confusing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplots\ndata1.plot(subplots = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot  \ndata1.plot(kind = \"scatter\",x=\"Confirmed\",y = \"Deaths\",alpha=0.4,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram plot  \ndata1.plot(kind = \"hist\",y = \"Deaths\",bins = 30,range= (0,500),density = True) # density was used instead of normed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind = \"hist\",y = \"Deaths\",bins = 50,range= (0,500),density = True,ax = axes[0],color=\"yellow\")\ndata1.plot(kind = \"hist\",y = \"Deaths\",bins = 50,range= (0,500),density = True,ax = axes[1],color=\"yellow\",cumulative = True) #cumulative total\nplt.savefig('graph.png')\nplt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Indexing Pandas Time Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_list = [\"2020-11-29\",\"2020-07-30\"]\nprint(type(time_list[1])) # As you can see date is string\n# however we want it to be datetime object\ndatetime_object = pd.to_datetime(time_list)\nprint(type(datetime_object))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# In order to practice lets take covid-19 data and add it a time list\ndata2 = df\ndatetime_object = pd.to_datetime(data2.ObservationDate) # convert ObservationDate that is object to pandas time series\ndata2[\"date\"] = datetime_object\n# lets make date as index\ndata2= data2.set_index(\"date\")\ndata2.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can select according to our date index\nprint(data2.loc[\"2020-06-11\"])\nprint(data2.loc[\"2020-06-11\":\"2020-07-30\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this output, we can see results 11.06.2020 and between 11.06.2020-30.07.2020"},{"metadata":{},"cell_type":"markdown","source":"## Resampling Pandas Time Series "},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"M\").mean() # average covid-19 results by months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data2.resample(\"M\").first().interpolate(\"linear\")\n#data2.resample(\"M\").mean().interpolate(\"linear\")\n# we didn't use interpolate because data already include all months","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.Manipulating Data Frames with Pandas "},{"metadata":{},"cell_type":"markdown","source":"## Indexing Data Frames "},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndata = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv\")\ndata= data.set_index(\"SNo\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using square brackets\ndata[\"Country/Region\"][55543]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using column attribute and row label\ndata.Deaths[55543]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc accessor\ndata.loc[1,[\"Country/Region\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting only some columns\ndata[[\"Country/Region\",\"Deaths\",\"Recovered\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Slicing Data Frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"# difference between selecting columns: series and dataframes\nprint(type(data[\"Confirmed\"]))     # series\nprint(type(data[[\"Confirmed\"]]))   # data frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# slicing and indexing series\ndata.loc[55000:55010,\"Country/Region\":\"Deaths\"]   # 10 and \"Deaths\" are inclusive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reverse slicing \ndata.loc[55010:55000:-1,\"Country/Region\":\"Deaths\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from something to end\ndata.loc[55000:55010,\"Deaths\":] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filtering Data Frames"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating boolean series\nboolean = data.Deaths > 40000\ndata[boolean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combining filters\nfirst_filter = data.Deaths < 5000\nsecond_filter = data.Recovered > 50000\ndata[first_filter & second_filter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filtering column based others\ndata.Deaths[data.Recovered>20000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transforming Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plain python functions\ndef square(n):\n    return n**2\ndata.Confirmed.apply(square)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# or we can use lambda function\ndata.Confirmed.apply(lambda n : n**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining column using other columns\ndata[\"gap\"] = data.Recovered - data.Deaths\ndata.loc[55000:55010]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Index Objects and Labeled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# our index name is this:\nprint(data.index.name)\n# lets change it\ndata.index.name = \"Index\"\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite index\n# if we want to modify index we need to change all of them.\ndata.head()\n# first copy of our data to data3 then change index \ndata3 = data.copy()\n# lets make index start from 100. It is not remarkable change but it is just example\ndata3.index = range(100,233710,2)\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data= data.set_index(\"#\") or  data.index = data[\"#\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Indexing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets read data frame one more time to start from beginning\ndata = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv\")\ndata.head()\n# As you can see there is index. However we want to set one or more column to be index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting index : Country/Region is outer Province/State is inner index\ndata1 = data.set_index([\"Country/Region\",\"Province/State\"]) \ndata1.head(10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pivoting Data Frames"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {\"job\":[\"Engineer\",\"Engineer\",\"Chef\",\"Chef\"],\"gender\":[\"M\",\"F\",\"M\",\"F\",],\"experience\":[0,5,12,8],\"age\":[22,28,40,32]}\ndf = pd.DataFrame(dic)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivoting\ndf.pivot(index=\"job\",columns = \"gender\",values=\"age\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking and Unstacking DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.set_index([\"job\",\"gender\"])\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# level determines indexes\ndf1.unstack(level=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.unstack(level=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change inner and outer level index position\ndf2 = df1.swaplevel(0,1)\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Melting Data Frames"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.melt(df,id_vars=\"job\",value_vars=[\"age\",\"experience\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categoricals and GroupBy"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# according to job take means of other features\ndf.groupby(\"job\").mean()   # mean is aggregation / reduction method\n# there are other methods like sum, std,max or min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can only choose one of the feature\ndf.groupby(\"job\").age.max() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can choose multiple features\ndf.groupby(\"job\")[[\"age\",\"experience\"]].min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# as you can see gender is object\n# However if we use groupby, we can convert it categorical data. \n# Because categorical data uses less memory, speed up operations like groupby\n#df[\"gender\"] = df[\"gender\"].astype(\"category\")\n#df[\"job\"] = df[\"job\"].astype(\"category\")\n#df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}