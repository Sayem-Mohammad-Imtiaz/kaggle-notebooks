{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IMPORT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport tarfile\nimport urllib\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,median_absolute_error,r2_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport tensorflow as tf\nfrom tensorflow.estimator import LinearRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOAD DATA\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\nfetch_housing_data()\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA STRUCTURE"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = load_housing_data()\nhousing.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins = 50, figsize= (20,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Outliers \n\nI think from histogram the median_house_value has outliers near to 500000$"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = housing[housing['median_house_value']< 500001]\nhousing.reset_index(drop=True,inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITTING DATA "},{"metadata":{},"cell_type":"markdown","source":"### It`s better here to use stratified sampling in splitting to represent all classes in our test set data.\n### for example if we should represent all categories in median_income data, we must classify this data into categories \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#based on median_icome data, we try to classify it into five categories\nhousing['income_category'] = pd.cut(housing['median_income'],bins = [0,1.5,3,4.5,6,np.inf],labels = [1,2,3,4,5])\n#splitting\nss_split = StratifiedShuffleSplit(n_splits = 1 , test_size = 0.20, random_state = 42)\nfor train_index, test_index in ss_split.split(housing, housing['income_category']):\n    train_set = housing.loc[train_index]\n    test_set = housing.loc[test_index]\n#dropping income_category attribute\nfor set in (train_set,test_set):\n    set.drop('income_category',axis=1,inplace = True )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VISUALIZING & EXPLORING DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a copy of training data\ntrain_set.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.1)\ntrain_set.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.5, s= train_set['population']/100, \n             label = 'population' , figsize = (20,10) , c = 'median_house_value', cmap = plt.get_cmap('jet'), \n             colorbar= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE SELECTION & CORRELATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = ['median_house_value','median_income','total_rooms','housing_median_age','latitude']\nscatter_matrix(train_set[attributes],figsize = (12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ATTRIBUTE COMBINATIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"#try out various attribute combinations\ntrain_set['rooms_per_household'] = train_set['total_rooms']/train_set['households']\ntrain_set['bedrooms_per_room'] = train_set['total_bedrooms']/train_set['total_rooms']\ntrain_set['population_per_household'] = train_set['population']/train_set['households']\ncorr_matrix = train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try out various attribute combinations\ntest_set['rooms_per_household'] = test_set['total_rooms']/test_set['households']\ntest_set['bedrooms_per_room'] = test_set['total_bedrooms']/test_set['total_rooms']\ntest_set['population_per_household'] = test_set['population']/test_set['households']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPROCESSING\n### DATA CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Splitting X & y from Data sets to extract the dependent variable y away from processing data\n\n#y_train = train_set['median_house_value'].values\n#X_train = train_set.copy()\n#X_train.drop('median_house_value', axis = 1, inplace = True)\n#y_test = test_set['median_house_value'].values\n#X_test = test_set.copy()\n#X_test.drop('median_house_value', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute = SimpleImputer(missing_values=np.nan, strategy = 'median')\n#train_num = X_train.drop('ocean_proximity', axis =1)\n#impute.fit_transform(train_num)\n#train_num.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute.statistics_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FEATURE SCALING\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#std_scaler = StandardScaler()\n#train_num_array = std_scaler.fit_transform(train_num)\n#train_num = pd.DataFrame(train_num_array , columns = train_num.columns , index = train_num.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HANDLING TEXT & CATEGORICAL ATTRIBUTES"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_num['ocean_proximity'] = X_train['ocean_proximity']\n#X_train = train_num.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#category_trans = make_column_transformer((OneHotEncoder(),['ocean_proximity']),remainder = 'passthrough')\n#X_train = category_trans.fit_transform(X_train)\n##X_train = pd.DataFrame(housing_array , columns = housing.columns , index = housing.index)\n#X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPROCESSING PIPELINE \n\nTHIS IS AN ALTERNATIVE SOLUTION WHICH IS MORE FLEXIBLE, SIMPLE, AND CONSUMINNG LESS TIME FOR DATA PREPROCESSING OPERATIONS SUCH DATA CLEANING, FEATURE SCALING, AND HANDLING TEXT \n\nIT DEPENDS ON PIPELINE CLASS FROM PIPELINE MODULE RELATED TO SKLEARN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting X & y from Data sets to extract the dependent variable y away from processing data\n\ny_train = train_set['median_house_value'].values\nX_train = train_set.copy()\nX_train.drop('median_house_value', axis = 1, inplace = True)\ny_test = test_set['median_house_value'].values\nX_test = test_set.copy()\nX_test.drop('median_house_value', axis = 1, inplace = True)\n\ntrain_num = X_train.drop('ocean_proximity', axis =1)\ntest_num = X_test.drop('ocean_proximity', axis =1)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attributes = list(train_num)\ncat_attributes = ['ocean_proximity']\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),('std_scaler', StandardScaler())])\npipeline = ColumnTransformer([('num_Pipeline', num_pipeline,num_attributes),\n                                 ('category', OneHotEncoder(),cat_attributes)],remainder='passthrough')\nX_train= pipeline.fit_transform(X_train)\n\nX_train.shape                                                     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test= pipeline.transform(X_test)\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAINING A MACHINE LEARNING MODEL "},{"metadata":{},"cell_type":"markdown","source":"### 1.LinearRegression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_model = LinearRegression()\nreg_model.fit(X_train,y_train)\ny_pred = reg_model.predict(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',reg_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.as we see there is underfitting in the model \n\n2.the scores of both train and test are not good\n\n''''also the mean square error of test and predicted data is about 60,500$, \nit`s a big error compared to range of median_housing_value'''"},{"metadata":{},"cell_type":"markdown","source":"### 2.Ridge Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge()\nridge_model.fit(X_train,y_train)\ny_pred = ridge_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',ridge_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.as we see there is underfitting in the model \n\n2.the scores of both train and test are not good\n\n''''also the mean square error of test and predicted data is about 60,500$, \nit`s a big error compared to range of median_housing_value'''"},{"metadata":{},"cell_type":"markdown","source":"### 3.Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeRegressor(random_state=42)\ntree_model.fit(X_train,y_train)\ny_pred = tree_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',tree_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the model seems it has no error!!!!!!\nI think it has badly overfitting.\nSo we have to evaluate it using cross validation test."},{"metadata":{},"cell_type":"markdown","source":"###### Cross validation Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_score = cross_val_score(tree_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ntree_rmse_score = np.sqrt(-tree_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dispaly_scores(scores):\n    print('scores : ', scores)\n    print('mean = ', scores.mean())\n    print('standard deviation = ',scores.std())\n\ndispaly_scores(tree_rmse_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it`s more bad than Linear Regression as it has rmse about 64,500$\nthe model has badly overfit the data.\n\nlet`s try random forest model "},{"metadata":{},"cell_type":"markdown","source":"### 4.Random Forest Regressor Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_model = RandomForestRegressor()\nforest_model.fit(X_train,y_train)\ny_pred = forest_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',forest_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Cross validation Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_score = cross_val_score(forest_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nforest_rmse_score = np.sqrt(-forest_score)\ndispaly_scores(forest_rmse_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.SVR Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_model = SVR()\nsvr_model.fit(X_train,y_train)\ny_pred = svr_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',svr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.Gradient Boosting Regressor Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_model = GradientBoostingRegressor()\ngbr_model.fit(X_train,y_train)\ny_pred = gbr_model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the training score = ',gbr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Cross validation Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_score = cross_val_score(gbr_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ngbr_rmse_score = np.sqrt(-gbr_score)\ndispaly_scores(gbr_rmse_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FINE TUNE THE MODEL \n\ntechnique to mix feature selection and hyperparameter tuning in the same procedure, considering the feature set as a hyperparameter itself. "},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"tune_pipeline = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',RandomForestRegressor(random_state = 42))])\n\ngrid_search = GridSearchCV( estimator = tune_pipeline, param_grid = {'selector__k':[14,16] , \n  'model__n_estimators':np.arange(360,370,10),'model__max_depth':[15]}, n_jobs=-1, scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_absolute_error', cv=5, verbose=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train,y_train)\nprint('the best parameters : ',grid_search.best_params_)\nprint('the best score = ', np.sqrt(-grid_search.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVR "},{"metadata":{"trusted":true},"cell_type":"code","source":"tune_pipeline_svr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',SVR())])\n\ngrid_search_svr = GridSearchCV( estimator = tune_pipeline_svr, param_grid = {'selector__k':[14,16] , \n  'model__kernel':['linear'],'model__C':[5000,10000],'model__epsilon':[0.3,3]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_svr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_svr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_svr.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------the Random Forest Regressor is better than  SVR model-------------------\n\nlet`s try \n#### Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"tune_pipeline_gbr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',GradientBoostingRegressor(random_state=42))])\n\ngrid_search_gbr = GridSearchCV( estimator = tune_pipeline_gbr, param_grid = {'selector__k':[14,16] , \n  'model__loss':['ls'],'model__max_depth':[6,7],'model__learning_rate':[0.1,0.2],'model__n_estimators':[500]}, n_jobs=-1, scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_absolute_error', cv=5, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_gbr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_gbr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_gbr.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the Gradient Boosting Regressor is better than Random Forest Regressor Model\n\nit has rmse about 44,000$,  compared to  ,45,800$ from Random Forest"},{"metadata":{},"cell_type":"markdown","source":"#### Ridge Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tune_pipeline_ridge = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',Ridge(random_state=42))])\n\ngrid_search_ridge = GridSearchCV( estimator = tune_pipeline_ridge, param_grid = {'selector__k':[15,16] , \n  'model__alpha':[0.5,1]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_ridge.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_ridge.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_ridge.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_gbr.best_estimator_.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EVALUATE THE MODEL ON THE TEST SET\n\n##### here we use the Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search_gbr.best_estimator_\ny_pred = final_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(y_test,y_pred))\nmae = mean_absolute_error(y_test,y_pred)\nmedian_ae = median_absolute_error(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rmse)\nprint(mae)\nprint(median_ae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Linear Regressor"},{"metadata":{},"cell_type":"markdown","source":"After loading data and exploring it, we will start from data splitting step "},{"metadata":{},"cell_type":"markdown","source":"## Train & Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_set['median_house_value']\nX_train = train_set.copy()\nX_train.drop('median_house_value', axis = 1, inplace = True)\ny_test = test_set['median_house_value']\nX_test = test_set.copy()\nX_test.drop('median_house_value', axis = 1, inplace = True)\nX_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num = X_train.drop('ocean_proximity', axis =1)\ntest_num = X_test.drop('ocean_proximity', axis =1)\nnum_attributes = list(train_num)\ncat_attributes = ['ocean_proximity']\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),('std_scaler', StandardScaler())])\nprocessing_pipeline = ColumnTransformer([('num_Pipeline', num_pipeline,num_attributes)],remainder='passthrough')\n\nX_train_ = processing_pipeline.fit_transform(X_train)\nX_test_ = processing_pipeline.transform(X_test)\nX_train = pd.DataFrame(X_train_,columns=num_attributes+['ocean_proximity'],index=X_train.index)\nX_test = pd.DataFrame(X_test_,columns=num_attributes+['ocean_proximity'],index=X_test.index)\nX_train[[i for i in X_train.columns if i not in ['ocean_proximity']]] = X_train[[i for i in X_train.columns if i not in ['ocean_proximity']]].apply(pd.to_numeric)\nX_test[[i for i in X_test.columns if i not in ['ocean_proximity']]] = X_test[[i for i in X_test.columns if i not in ['ocean_proximity']]].apply(pd.to_numeric)\nX_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature columns processing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns_numeric = [tf.feature_column.numeric_column(m) for m in train_num.columns]\nfeature_columns_categorical = [tf.feature_column.categorical_column_with_hash_bucket('ocean_proximity',\n                                                                                     hash_bucket_size=1000)]\nfeature_columns = feature_columns_numeric + feature_columns_categorical\nfeature_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Input Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feed_input(features_dataframe, target_dataframe, num_of_epochs=10, shuffle=True, batch_size=32):\n  def input_feed_function():\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n    if shuffle:\n      dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n    return dataset\n  return input_feed_function\n\ntrain_feed_input = feed_input(X_train, y_train)\ntrain_feed_input_testing = feed_input(X_train, y_train, num_of_epochs=1, shuffle=False)\ntest_feed_input = feed_input(X_test, y_test, num_of_epochs=1, shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_model = LinearRegressor(feature_columns)\nregression_model.train(train_feed_input,steps=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions = regression_model.predict(train_feed_input_testing)\ntest_predictions = regression_model.predict(test_feed_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions_series = pd.Series([p['predictions'][0] for p in train_predictions])\ntest_predictions_series = pd.Series([p['predictions'][0] for p in test_predictions])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\ntest_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.reset_index(drop=True, inplace=True)\ntrain_predictions_df.reset_index(drop=True, inplace=True)\n\ny_test.reset_index(drop=True, inplace=True)\ntest_predictions_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_with_predictions_df = pd.concat([y_train, train_predictions_df], axis=1)\ntest_labels_with_predictions_df = pd.concat([y_test, test_predictions_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_errors_and_r2(y_true, y_pred):\n  mean_squared_err = (mean_squared_error(y_true, y_pred))\n  root_mean_squared_err = np.sqrt(mean_squared_err)\n  r2 = round(r2_score(y_true, y_pred)*100,0)\n  return mean_squared_err, root_mean_squared_err, r2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mean_squared_error, train_root_mean_squared_error, train_r2_score_percentage = calculate_errors_and_r2(y_train, train_predictions_series)\ntest_mean_squared_error, test_root_mean_squared_error, test_r2_score_percentage = calculate_errors_and_r2(y_test, test_predictions_series)\n\nprint('Training Data Mean Squared Error = ', train_mean_squared_error)\nprint('Training Data Root Mean Squared Error = ', train_root_mean_squared_error)\nprint('Training Data R2 = ', train_r2_score_percentage)\n\nprint('Test Data Mean Squared Error = ', test_mean_squared_error)\nprint('Test Data Root Mean Squared Error = ', test_root_mean_squared_error)\nprint('Test Data R2 = ', test_r2_score_percentage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally from these models we can say the best model till now is GRADIENT BOOSTING REGRESSOR with:\n# cross validation score reaches 98.3% and test score equal to 83%"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}