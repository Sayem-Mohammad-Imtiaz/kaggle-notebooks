{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is SMS spam?\n\nSMS spam is any unwanted or unsolicited text message sent indiscriminately to your mobile phone, often for commercial purposes. It can take the form of a simple message, a link to a number to call or text, a link to a website for more information or a link to a website to download an application.\n\n![](https://static.techspot.com/images2/news/thumbs/2013/07/2012-11-28-teaserd0b.jpg)\n\nWe have often received similar messages in our phones- and this really makes us irriated. \nBut what if we had a way to identify if a message is spam as soon as we saw it?\nWhat if we can smoothly ignore the messgaes- and read only the one ones that are meaningful.\n\nThis is our target right now:\n\nWe start with this mini project of ours:\n# SMS Spam Classifier\n\n![](https://miro.medium.com/max/1153/1*BZ0JACBXzTv4t-bdBy3o8A.png)\n\nSo as you have seen in the pic- we hsall be checking out two algorithms, and identify which one would be the best apporach to solve the problems- \n\n### Then lets take a cup of coffee, and start coding...\n\n![](https://www.tech-mugs.com/wp-content/uploads/2020/06/33719-9.jpg)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Importing the libraries\n\nWe are importing the libraries as and when required. While we read through this, we will look into a brief understanding of each library and its scope.\n\n**NumPy** is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n\n**Pandas** is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:35.013864Z","iopub.execute_input":"2021-05-23T09:06:35.014317Z","iopub.status.idle":"2021-05-23T09:06:35.040723Z","shell.execute_reply.started":"2021-05-23T09:06:35.014222Z","shell.execute_reply":"2021-05-23T09:06:35.039946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n\nYou can read on the NLTK documentation by clicking this link: https://www.nltk.org/","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:35.041965Z","iopub.execute_input":"2021-05-23T09:06:35.042386Z","iopub.status.idle":"2021-05-23T09:06:36.799394Z","shell.execute_reply.started":"2021-05-23T09:06:35.042355Z","shell.execute_reply":"2021-05-23T09:06:36.798418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the dataset\n\n![](data:image/jpeg;base64,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)","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:36.80292Z","iopub.execute_input":"2021-05-23T09:06:36.803244Z","iopub.status.idle":"2021-05-23T09:06:36.856217Z","shell.execute_reply.started":"2021-05-23T09:06:36.80321Z","shell.execute_reply":"2021-05-23T09:06:36.855286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: What we see here is we have two columns of data-\n\n**Message-** The text message which is to be categorized\n\n**Category-** The classification whether the text message is a spam or not spam (Lol, Ham!)\n\nBut for a Machine Learning algorithm, it will not be easy to demarcate the statements as Spam or Ham- instead- our target would be to assign if the message is a spam or not.\nSo let us apply a preprocessing- via which the two categories are marked by 0 and 1- which will in turn help the algorithm to best identify the inputs\n\nWe will be using the preprocessing technique of LabelEncoder for the same\nYou can read on Label Encoder using the follwoing link: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata['Category']=le.fit_transform(data['Category'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:36.857678Z","iopub.execute_input":"2021-05-23T09:06:36.857963Z","iopub.status.idle":"2021-05-23T09:06:36.872372Z","shell.execute_reply.started":"2021-05-23T09:06:36.857934Z","shell.execute_reply":"2021-05-23T09:06:36.87129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the message\n\nThe texts we recieve have multiple factors that are unwanted and unhelpful. providing that data to the machine learning algorthim will make it just suffer. Instead lets take on some approach to clean up the text as much as possible. \n\nSo what methods should we adopt to clean out our message?\n\n1. Firstly, we need to take a look into the text message to find if there are any unwanted url links. This information is generally not useful- so we shall be removing them i the first step\\\n2. Symbols- For a text classifier- we genrally tend to remove all the punctuatuions, as they are of no use in a text predictor\n3. Lets remove all other characters than alphabets-numbers wont be a crucial input to the algorithm.\n4. Finally, we will need to lemmatize the words. Now as expected- you would want to know the meaning of lemmatize.\n\n# Lemmatization\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n\nText preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n\nApplications of lemmatization are:\n\n* Used in comprehensive retrieval systems like search engines.\n* Used in compact indexing\n\n![](https://cdn-images-1.medium.com/max/1024/1*ES5bt7IoInIq2YioQp2zcQ.png)\n\nSo now, let us clean up our text message!","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"wordnet=WordNetLemmatizer()\ncorpus=[]\nfor i in range(0,len(data)):\n    review = re.sub(r'https?://\\S+|www.\\S+', '', data[\"Message\"][i])\n    review = re.sub(r'<.*?>', '', review)\n    review = re.sub(r'[^a-zA-Z]+', ' ', review)\n    review = re.sub(r'[0-9]', '', review)\n    review=review.lower()\n    review=review.split()\n    review=[str(wordnet.lemmatize(word)) for word in review if not word in stopwords.words('english')]\n    review=' '.join(review)\n    corpus.append(review)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:36.873985Z","iopub.execute_input":"2021-05-23T09:06:36.874405Z","iopub.status.idle":"2021-05-23T09:06:50.159141Z","shell.execute_reply.started":"2021-05-23T09:06:36.874364Z","shell.execute_reply":"2021-05-23T09:06:50.158228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separating the dependent and independent variables:\n1. X-> **Independent Variables**- The Basis in which the outputs have to be calculated\n2. X->**Dependent Variable**-The output being calculated from the inputs\n\nGenerally, X and y are the standard norms that we use in order to depict the Independent and Dependent variables repectively. However, it is completely the coder choice as to which will be the option you wish to choose.\n\n![](https://www.wikihow.com/images/thumb/7/71/Identify-Dependent-and-Independent-Variables-Step-2.jpg/v4-460px-Identify-Dependent-and-Independent-Variables-Step-2.jpg.webp)","metadata":{}},{"cell_type":"code","source":"y=data[\"Category\"]\nX=pd.DataFrame(corpus,columns=['text'])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.160403Z","iopub.execute_input":"2021-05-23T09:06:50.160673Z","iopub.status.idle":"2021-05-23T09:06:50.166166Z","shell.execute_reply.started":"2021-05-23T09:06:50.160647Z","shell.execute_reply":"2021-05-23T09:06:50.165016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we shall be distributing our data into two parts-\n1. **Train Set**- The Train Set is the data that the model is trained with (or in other words, the data from which the model learns)\n2. **Test Set**- The Test Set is the data which the model has to see, and predict the output.\n\nThis is kind of similar to studying:\n1. We read some texts, from where we understand the concepts and know about the topics (Training)\n2. Then, we appear for the exam where we apply our knowledge and are able to Test our understanding.\n\nThe sklearn library of python provides an amazing option- **Train_test_split**\n\nYou can reacd more about the module using this link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.168303Z","iopub.execute_input":"2021-05-23T09:06:50.168623Z","iopub.status.idle":"2021-05-23T09:06:50.333682Z","shell.execute_reply.started":"2021-05-23T09:06:50.168596Z","shell.execute_reply":"2021-05-23T09:06:50.33271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now how do we process the statements?\n\nWhenever we have textual data, we need to apply several pre-processing steps to the data to transform words into numerical features that work with machine learning algorithms. The pre-processing steps for a problem depend mainly on the domain and the problem itself, hence, we don’t need to apply all steps to every problem.\n\nIn this article, we are going to see text preprocessing in Python. We will be using the Count Vectorizer module here.\n\nYou can read more about it using the link: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=3000)\ntrain_X=cv.fit_transform(X_train['text']).toarray()\ntest_X=cv.transform(X_test['text']).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.335341Z","iopub.execute_input":"2021-05-23T09:06:50.335655Z","iopub.status.idle":"2021-05-23T09:06:50.496653Z","shell.execute_reply.started":"2021-05-23T09:06:50.335625Z","shell.execute_reply":"2021-05-23T09:06:50.495843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets Take a look at the output of the CountVectorizer- \nAnd the dimensions of each table-","metadata":{}},{"cell_type":"code","source":"train_X","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.497926Z","iopub.execute_input":"2021-05-23T09:06:50.498448Z","iopub.status.idle":"2021-05-23T09:06:50.50525Z","shell.execute_reply.started":"2021-05-23T09:06:50.498415Z","shell.execute_reply":"2021-05-23T09:06:50.504295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: It just forms an array of 0s and 1s. You shall be able to get the entire understanding of the array on readin gthe documentation mentioned above","metadata":{}},{"cell_type":"code","source":"print(train_X.shape)\nprint(test_X.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.506689Z","iopub.execute_input":"2021-05-23T09:06:50.506988Z","iopub.status.idle":"2021-05-23T09:06:50.516857Z","shell.execute_reply.started":"2021-05-23T09:06:50.506961Z","shell.execute_reply":"2021-05-23T09:06:50.515907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Naive Bayes Algorithm\n\n![](https://miro.medium.com/max/1200/1*ZW1icngckaSkivS0hXduIQ.jpeg)\n\nCheck out the documentation on Naive Bayes Classifier from this link: https://scikit-learn.org/stable/modules/naive_bayes.html\n\nIn the next step, we shall be incorporating the Naive Bayes Classifier and training it on the Train Data. Once trained, it will be used to predict the outputs from the test data set","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB().fit(train_X,y_train)\ny_pred_nb=nb.predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.518237Z","iopub.execute_input":"2021-05-23T09:06:50.518565Z","iopub.status.idle":"2021-05-23T09:06:50.846772Z","shell.execute_reply.started":"2021-05-23T09:06:50.518535Z","shell.execute_reply":"2021-05-23T09:06:50.845377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we shall be introducing a new concept- **Confusion Matrix**\n\n\"A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing\"\n\nRead more on the documentation by clicking on the link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n\nWe shall generate the report, and for visualizing it, we will be using the heatmap feature of the seaborn library. Let's take a look:-","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred_nb)\nsns.heatmap(cm,cmap='BuPu',annot=True,fmt='d')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:06:50.848596Z","iopub.execute_input":"2021-05-23T09:06:50.849024Z","iopub.status.idle":"2021-05-23T09:06:51.20808Z","shell.execute_reply.started":"2021-05-23T09:06:50.848979Z","shell.execute_reply":"2021-05-23T09:06:51.207124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\nIts now time that we analyze the results:-\n1. 948 Non-Spam mails have been correctly classified\n2. 149 Spam mails have been correctly classified\n3. 7 Non-Spam mails have been classified as Spam mails (False Positives or Type I Error)\n4. 11 Spam mails have been classified as Non-Spam (False Negatives or Type II Error)\n\nNow before deep-diving, we shall take a look into the next model!","metadata":{}},{"cell_type":"markdown","source":"### Importing the Random Forest Classifier\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png)\n\nYou shall be able to read about this in more details in the documentation. Kindly find the link: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\nSo, like Naive Bayes algorithm, we shall be applying the same procedure in here as well. Lets first train the model, and then check its output using a confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(random_state=0).fit(train_X,y_train)\ny_pred_rfc=rfc.predict(test_X)\ncm=confusion_matrix(y_test,y_pred_rfc)\nsns.heatmap(cm,cmap='BuPu',annot=True,fmt='d')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:10:54.583741Z","iopub.execute_input":"2021-05-23T09:10:54.584097Z","iopub.status.idle":"2021-05-23T09:11:05.296529Z","shell.execute_reply.started":"2021-05-23T09:10:54.584067Z","shell.execute_reply":"2021-05-23T09:11:05.295459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\nUsing the Random Forest Classifier:\n1. 955 Non-Spam mails have been correctly classified\n2. 141 Spam mails have been correctly classified\n3. 0 Non-Spam mails have been classified as Spam mails (False Positives or Type I Error)\n4. 19 Spam mails have been classified as Non-Spam (False Negatives or Type II Error)\n\nSo as we have both the set of results- now it would be a crucial step to understand","metadata":{}},{"cell_type":"markdown","source":"# Understanding the Results","metadata":{}},{"cell_type":"markdown","source":"Now since, we have checked the outputs from the two algorithms- the main question that arises is- **which model is better?**\n\nThe best approach in order to undertsand and estimate this is by evaluating the mtrics. Usually the metrics to evaluate the performance of a Classification Problem are- **Precision** and **Recall**\n\nSo what is Precision and Recall?\n\n![](https://www.researchgate.net/publication/336402347/figure/fig3/AS:812472659349505@1570719985505/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.ppm)\n\nWikipedia actually gives a pretty good understanding on the concepts of Precision and Recall. You can check it from the link here: https://en.wikipedia.org/wiki/Precision_and_recall\n\nNow instead of calculating it mannually, lets do it using modules of the sklearn.metrics library. For reading, refer to:\n1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n2. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n\nLet's implement these and check the results","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nprint('******** Naive Bayes Classifier **********')\nprint('Precision Score: ',precision_score(y_test,y_pred_nb))\nprint('Recall Score: ',recall_score(y_test,y_pred_nb))\nprint('****************************************')\nprint('******** Random Forest Classifier **********')\nprint('Precision Score: ',precision_score(y_test,y_pred_rfc))\nprint('Recall Score: ',recall_score(y_test,y_pred_rfc))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:07:02.43608Z","iopub.execute_input":"2021-05-23T09:07:02.436651Z","iopub.status.idle":"2021-05-23T09:07:02.454651Z","shell.execute_reply.started":"2021-05-23T09:07:02.436608Z","shell.execute_reply":"2021-05-23T09:07:02.453903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interesting!**\n\nWe see:\n1. Precision of Random Forest Classifier > Precision of Naive Bayes Classifier\n2. Recall of Naive Bayes Classifier > Recall of Random Forest Classifier\n\nNow that is a confusion!\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRphl-UcxSTfsc9Xk9SBY98_xHjNMXEBJ9Atw&usqp=CAU)\n\nNow comes the inputs of a Data Scientist:\n\nImagine you have implemented this system in your device, you can have 2 situations:\n\n1. Most of your Spam mails are being stopped, but some of your important mails (which are ideally Non-Spam) are also getting stopped- since they are being tagged as Spam\n2. The bar is set slightly low- only confirmed Spam mails are being blocked, but its ensured that other mails (Non-Spam) are not getting blocked\n\nWhat will be your choice?\n\n![](https://cdn3.iconfinder.com/data/icons/artificial-intelligence-2-3/48/101-512.png)\n\nIn most cases, a user would want the first choice- in order to ensure that a minimal (or better if absoltely none) important mails are tagged in as **spam**- and rejected. Going by that understanding... We must target that we have least numbers of False Positives.\n\nOr indirectly, **Precision would be the Key metrics**\n\nSo by that logic- **Random Forest Classifier would be the model we choose to solve this problem**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nWe have learned a number of concepts in this Notebook:\n\n1. How to process and clean text data. Since it will not be understood by the computers, how can we modify the data we have into computer suppported format\n2. How do we implement a Machine Learning model, and measure the performance of the model\n3. If we have inputs from two or more models, on what grounds should we choose the best model\n\n## Please upvote if you liked the guidance! \nThere will be many more to follow this!\n\n![](https://i.pinimg.com/originals/0f/ab/3e/0fab3e4f7e9e7d3f199c49f10308ac05.gif)","metadata":{}}]}