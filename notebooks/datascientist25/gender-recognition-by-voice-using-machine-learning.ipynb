{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/voicegender/voice.csv')\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of target varibles\ncolors = ['pink','Lightblue']\ndata_y = df[df.columns[-1]]\nplt.pie(data_y.value_counts(),colors=colors,labels=['female','male'])\nplt.axis('equal')\nprint (df['label'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot see comparision in labels by other features\ndf.boxplot(column = 'meanfreq',by='label',grid=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# plot correlation matrix (here i used the headmape using seaborn)\ncorrelation =df.corr()\nsns.heatmap(correlation)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning part:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_test_split is responsible to split the data into (Train and Test)\nfrom sklearn.model_selection import train_test_split\nX = df[df.columns[:-1]].values\n#y = df['lable']\ny = df[df.columns[-1]].values\n# We will divide the data into 70-30% into train and test data\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Random forest\n# Importing Random forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier()\nrand_forest.fit(Xtrain, ytrain)\ny_pred = rand_forest.predict(Xtest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import matrix to calcualte accuracy\nfrom sklearn import metrics, neighbors\nfrom sklearn.metrics import accuracy_score\nprint(metrics.accuracy_score(ytest, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10-fold cross validation\n# Importing Bausian Classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nCVFirst = GaussianNB()\nCVFirst = CVFirst.fit(Xtrain, ytrain)\ntest_result = cross_val_score(CVFirst, X, y, cv=10, scoring='accuracy')\nprint('Accuracy obtained from 10-fold cross validation is:',test_result.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{},"cell_type":"markdown","source":" Exceptable range of voice freq for a human as per will is betwwen 0.085\nand 0.255KHz and hence we will identify the variable which has the frequency\ninformation and remove them assuming it to be a outlier based on domain knowledge.\nAs per the station given in wiki we can say that typical adult male will\nhave a fundamental frequency from 85 to 189 Hz and typical adult female from 165 to 255 Hz."},{"metadata":{"trusted":true},"cell_type":"code","source":"male_funFreq_outlier_index = df[((df['meanfun'] < 0.085) | (df['meanfun'] > 0.180)) &\n                               (df['label'] == 'male')].index\nfemale_funFreq_outlier_index = df[((df['meanfun'] < 0.165)  | (df['meanfun'] > 0.255)) &\n                                 (df['label'] == 'female')].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_to_remove = list(male_funFreq_outlier_index) + list(female_funFreq_outlier_index)\nlen(index_to_remove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_x = df[df.columns[0:20]].copy()\ndata2 = data_x.drop(['kurt','centroid','dfrange'],axis=1).copy()\ndata2.head(3)\ndata2 = data2.drop(index_to_remove,axis=0)\n\n#y = df['lable']\ndata_y = pd.Series(y).drop(index_to_remove,axis=0)\nXtrain, Xtest, ytrain, ytest = train_test_split(data2, data_y, test_size=0.30 )\nclf1 = RandomForestClassifier()\nclf1.fit(Xtrain, ytrain)\ny_pred = clf1.predict(Xtest)\nprint(metrics.accuracy_score(ytest, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Decision Trees Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf2 = DecisionTreeClassifier()\nclf2.fit(Xtrain, ytrain)\ny_predict = clf2.predict(Xtest)\nprint(metrics.accuracy_score(ytest, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf3 = GaussianNB()\nclf3 = clf3.fit(Xtrain, ytrain)\ny_predd = clf3.predict(Xtest)\nprint(metrics.accuracy_score(ytest,y_predd))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing linear Regression classifier\nfrom sklearn.linear_model import LogisticRegression\nclf4 = LogisticRegression()\nclf4.fit(Xtrain,ytrain)\ny_predict4 = clf4.predict(Xtest)\nprint(metrics.accuracy_score(ytest,y_predict4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation with same classifire as first time \ntest_result = cross_val_score(clf3, data2, data_y, cv=10, scoring='accuracy')\nprint('Accuracy obtained from 10-flod cross validation is:',test_result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cros validation on the best result\ntest_result = cross_val_score(clf2, data2, data_y, cv=10,scoring = 'accuracy')\nprint('Accuracy obtained from 10-fold validation is:',test_result.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab as pl\nlabels = ['female', 'male']\ncm = confusion_matrix(ytest,y_pred,labels)  #ypred for RandomForest\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax =ax.matshow(cm)\npl.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\npl.xlabel('Predicted')\npl.ylabel('True')\npl.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(ytest, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of male and female\nsns.FacetGrid(df, hue='label',size=5).map(sns.kdeplot,\"meanfun\").add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we're doing flat-clustering , our task is a bit easier since we can tell the machine that we want it category\n\nfrom sklearn.cluster import KMeans\nfrom matplotlib import style\nstyle.use(\"ggplot\")\n\ndata_x = np.array(df[['meanfreq','meanfun']])\nkmeans = KMeans(n_clusters= 2)\nkmeans.fit(data_x)\n\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n#print(centroids)\n#print(labels) # 0-male, 1-Female( the machine has assigned on its own.)\n\ncolors = [\"g.\",\"b.\"]  #green = male\n\nfor i in range(len(data_x)):\n    plt.plot(data_x[i][0], data_x[i][1], colors[labels[i]], markersize = 10)\n\n    \nplt.scatter(centroids[:,0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\nplt.ylabel('meanfun')\nplt.xlabel('meanfun')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}