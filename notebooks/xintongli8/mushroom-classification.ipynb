{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Xintong Li**\n\n**Python Classification Project**\n\n**11/24/2020**"},{"metadata":{},"cell_type":"markdown","source":"# Mushroom Classification\n\n1. Introduction\n2. Libraries\n3. Exploratory Analysis\n    * 3a. Statistical Summary\n    * 3b. Bar Plot\n    * 3c. Correlation Heatmap\n4. Test & Train Split\n5. Machine Learning Models\n6. Experiment Results & Next Steps\n7. Methods to Improve Models\n    * 7a. Parameter Tunning\n    * 7b. Comparing results before and after Parameter Tunning\n8. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"As someome who loves mushroom, I want to explore what the key features of a poisonous mushroom. I will be doing Exploratory Analysis to detect any interesting patterns or relationships between each features and its target variable (classes: edible=e, poisonous=p). On top of that, I will apply different Machine Learning Algrithoms to find the best model to predict whether a mushroom is edible or poisonous. Some models I will be using are KNN, Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Stochastic Gradient Descentn and AdaBoost.\n\nThis mushroom data set I found is from Kaggle: https://www.kaggle.com/uciml/mushroom-classification. It includes 23 features and 8124 rows. Here is a list of the features' description from Kaggle:\n\n**Attribute Information: (classes: edible=e, poisonous=p)**\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* bruises: bruises=t,no=f\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n* gill-spacing: close=c,crowded=w,distant=d\n* gill-size: broad=b,narrow=n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* stalk-shape: enlarging=e,tapering=t\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* veil-type: partial=p,universal=u\n* veil-color: brown=n,orange=o,white=w,yellow=y\n* ring-number: none=n,one=o,two=t\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"},{"metadata":{},"cell_type":"markdown","source":"## 2. Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# General libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt\n\n# Models \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport graphviz\nfrom sklearn import tree\n\n\n%matplotlib inline\nsns.set(color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"# read file\ndf = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3a. Statistical Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# display top 5 rows \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display bottom 5 rows\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the data type\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking duplicated data\ndf.shape\nduplicate_rows = df[df.duplicated()]\nprint(\"number of duplicated rows: \", duplicate_rows) \n\n### no duplicated rows","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# checking null values\nprint(df.isnull().sum())\n\n### no missing values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find unique values for each variable  \ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will remove veil-type since it only has 1 unique value. It will not be very useful for this project. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['veil-type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use labelencoding to convert categorical data to numerical \nfrom sklearn.preprocessing import LabelEncoder\ndf_encoded = df.copy()\nle=LabelEncoder()\nfor col in df_encoded.columns:\n    df_encoded[col] = le.fit_transform(df_encoded[col])\n\ndf_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3b. Bar Plots"},{"metadata":{},"cell_type":"markdown","source":"In this part, I'm using Bar Plots to understand the difference between each features based on our target variable (hue = 'class').\n\nAccording to these plots, the amount of edible and poisonous are very similar, which helps us to prevent bias towards one class. Mushrooms with a cap-shape b has a higher count on being edible, whereas cap-shape k has a higher count on being poisonous. Cap-surface with a f has a higher count for being edible. \n\nA significanr difference in gill-color = buff between edible and poisonous. It shows us mushrooms with gill-color = buff might have a more chance to be considered for poisonous. The assumption applies to when stalk-surface-above-ring = k. \n\nSpore-print-color also has significant difference between edible and posnonous. k and n have a higher edible count, where h and w have a higher poisonous count."},{"metadata":{"trusted":true},"cell_type":"code","source":"# barplots with hus of class (e,p)\nsns.catplot(x='class',kind='count',palette='ch:.25',data=df)\nsns.catplot(x='cap-shape',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='cap-surface',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='bruises',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='odor',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-attachment',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-spacing',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-size',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-shape',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-root',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-surface-above-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-surface-below-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-color-above-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-color-below-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='veil-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='ring-number',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='ring-type',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='spore-print-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='population',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='habitat',kind='count',palette='ch:.25',data=df,hue='class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3c. Correlation Matrix"},{"metadata":{},"cell_type":"markdown","source":"In this part, I created a correlation matrix heatmap to get a better understanding on the relationships between each feature. Based on the heatmap, veil-color and gill-attachment are highly positively correlated with a correlation of 0.9. Gill-spacing and population, gill-color and target variable are highly negatively correlated with a correlation of -0.53. Ring-type and bruises also have a high negative correlation with our target variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = np.triu(df_encoded.corr())\nplt.subplots(figsize=(20,15))\nsns.heatmap(df_encoded.corr(), annot=True, mask=matrix, xticklabels=True, yticklabels=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Train and Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into x and y\nx = df_encoded.iloc[:, 1:]\ny = df_encoded.iloc[:, 0:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Machine Learning Models"},{"metadata":{},"cell_type":"markdown","source":"### KNN Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train, y_train)  #fit the model\n    pred=model.predict(X_test) #make prediction on test set\n    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curve = pd.DataFrame(rmse_val) \ncurve.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When k = 2, RMSE has the smallest value of 0.021. It is safe to say that k=3 will give us the best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = KNeighborsClassifier(n_neighbors=2)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"KNN accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_scm = svm.SVC(kernel='linear') # Linear Kernel\nclf_scm.fit(X_train, y_train)\ny_pred = clf_scm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_clf = DecisionTreeClassifier()\n\nparameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100]}\n\ntree_clf.fit(X_train,y_train) # fit the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the best model use scoring = balanced_accuracy\ngrid_cv = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv.fit(X_train,y_train) \n\nprint_results(grid_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(grid_cv.best_estimator_, out_file=None, filled=True)\n\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_SGD = SGDClassifier()\nclf_SGD.fit(X_train, y_train)\ny_pred = clf_SGD.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SGD accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"ac = AdaBoostClassifier()\nac.fit(X_train,y_train)\ny_pred = ac.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AdaBoost accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Experiment Results"},{"metadata":{},"cell_type":"markdown","source":"After applying these models, I got some great results. KNN, Random Forest, and AdaBoost have perfect prediction on the testing data set, with accruacy and macro average of 1. Stochastic Gradient Descent has the worst performance among all, with an accruacy and macro average of 0.94.\n\nNext step, I will apply hyperparamter tunning on the models and comparing the results between different methods. My goal is to discover if these methods will help my models to perform better."},{"metadata":{},"cell_type":"markdown","source":"## 7. Methods to Improve Models"},{"metadata":{},"cell_type":"markdown","source":"### 7 a. Applying Hyperparameter Tunning"},{"metadata":{},"cell_type":"markdown","source":"#### 7 a a. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression()\n\nparameters = {\"solver\":['newton-cg', 'lbfgs', 'liblinear'],\n              \"penalty\": ['l2'],\n              \"C\": [100, 10, 1.0, 0.1, 0.01]}\n\nlr_clf.fit(X_train,y_train) \n\ngrid_cv1 = GridSearchCV(estimator=lr_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv1.fit(X_train,y_train) \n\nprint_results(grid_cv1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression(C = 100,penalty = 'l2',solver='newton-cg')\nlr_clf= lr_clf.fit(X_train,y_train)\ny_pred1 = lr_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 a b. Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = SVC()\n\nparameters = {\n    'C': [0.1, 0.5, 1, 2, 5, 10, 20],\n    'gamma': ['scale', 'auto', 1, 2, 3]\n    }\n\nsvm_clf.fit(X_train,y_train) \n\ngrid_cv2 = GridSearchCV(estimator=svm_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv2.fit(X_train,y_train) \n\nprint_results(grid_cv2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = SVC(kernel ='rbf', C = 2, gamma = 'auto')\nsvm_clf= svm_clf.fit(X_train,y_train)\ny_pred2 = svm_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 a c. Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_clf = DecisionTreeClassifier()\n\nparameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100]}\n\ntree_clf.fit(X_train,y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_cv1 = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv1.fit(X_train,y_train) \n\nprint_results(grid_cv1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1 = DecisionTreeClassifier(max_depth=10,min_samples_split=50)\nclf1= clf1.fit(X_train,y_train)\ny_pred3 = clf1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 a d. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n\nrf_parameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100],\n             \"n_estimators\":[5, 50, 250, 500]}\n\ngrid_cv4 = GridSearchCV(estimator=rf, param_grid=rf_parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv4.fit(X_train,y_train) \n\nprint_results(grid_cv4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(max_depth=10,min_samples_split=50,n_estimators=50)\nrf_clf = rf_clf.fit(X_train,y_train)\ny_pred4 = rf_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 a e. Stochastic Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier()\nsgd.fit(X_train,y_train)\n\nparameters = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], \n                'penalty': ['l2'],\n                'n_jobs': [-1]}\n\ngrid_cv5 = GridSearchCV(estimator=sgd, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv5.fit(X_train,y_train) \n\nprint_results(grid_cv5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf = SGDClassifier(alpha=0.001, n_jobs=-1, penalty= 'l2')\nsgd_clf = sgd_clf.fit(X_train,y_train)\ny_pred5 = sgd_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 a f. Adaboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"ab = AdaBoostClassifier()\nab.fit(X_train,y_train)\n\nparameters = {'n_estimators':[500,1000,2000],\n              'learning_rate':[.001,0.01,.1]}\n\ngrid_cv6 = GridSearchCV(estimator=ab, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv6.fit(X_train,y_train) \n\nprint_results(grid_cv6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab = AdaBoostClassifier(n_estimators=1000,learning_rate=0.1)\nab = ab.fit(X_train,y_train)\ny_pred6 = ab.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7 b. Comparing Results: before and after parameter tunning "},{"metadata":{},"cell_type":"markdown","source":"#### 7 b b. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"After applying parameter tunning, f1-score and accuracy for Logistic Regression increased 0.02. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 b c. Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"f1-score and accuracy increased by 0.03. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred2))\nprint(classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 b d. Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"f1-score and accuracy increased by 0.02. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred3))\nprint(classification_report(y_test, y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 b e. Random Forest "},{"metadata":{},"cell_type":"markdown","source":"Accuracy score on testing set decreased by 0.03. However, f1-score stayed the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 b f. SGD"},{"metadata":{},"cell_type":"markdown","source":"Accuracy and f1-score stayed the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SGD accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred5))\nprint(classification_report(y_test, y_pred5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7 b e. AdaBoost"},{"metadata":{},"cell_type":"markdown","source":"Accuracy and f1-score stayed the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Adaboost accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred6))\nprint(classification_report(y_test, y_pred6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The best models I got were AdaBoost and Support Vector Machine after parameter tunning, with a perfect accuracy and f1-score. The features that effect the target variable the most are: gill-size and gill-color. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}