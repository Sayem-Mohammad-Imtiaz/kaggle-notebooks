{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Statistical Approach to for predicting IMDB"},{"metadata":{},"cell_type":"markdown","source":"1. [Introduction](#introduction)<br>\n    1.1 [Background](#background)<br>\n    1.2 [Data Description](#datadescription)<br>\n    1.3 [Problem Statement](#problemstatement)<br>\n2. [Data Exploration](#dataexploration)<br>\n    2.1 [Data Loading](#dataloading)<br>\n    2.2 [Data Profile](#dataprofile)<br>\n    2.3 [Data Cleaning](#datacleaning)<br>\n3. [Regression Model Building](#rmb)<br>\n    3.1 [Splitting the Dataset](#std)<br>\n    3.2 [Scaling to avoid Euclidean Distance problem](#s)<br>\n    3.3 [Feature Elimination](#fe)<br>\n    3.4 [Simple Linear Regression](#slr)<br>\n    3.5 [Support Vector Machines with Linear, Polynomial and RBF Kernels](#svmr)<br>\n    3.6 [Ensemble Models](#em)<br>\n     3.6.1 [Gradient Boosting with Hyperparameter Tuning](#gbr)<br>\n     3.6.2 [Random Forest with Hyperparameter Tuning](#rbr)<br>\n    3.7 [XGBoost with Hyperparameter Tuning](#xgbr)<br>\n    3.8 [Interpreting Results of a Regresison Model](#irr)<br>\n4. [Building a Classificaiton Model](#bc)<br>\n    4.1 [Logistic Regression](#lr)<br>\n    4.2 [Support Vector machines with Linear, Polynomial adn RBF Kernels](#svmc)<br>\n    4.3 [Ensemble Models](#emc)<br>\n     4.3.1 [Random Forest with Hyperparameter Tuning](#rfc)<br>\n     4.3.2 [Gradient Boosting with Hyperparameter Tuning](#gbc)<br>\n    4.4 [XGBoost with Hyperparameter Tuning](#xgbc)<br>\n    4.5 [Interpreting Results of Classification Model](#ircm)\n5. [Conclusion](#conclusion)<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id='introduction'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 1 Introduction"},{"metadata":{},"cell_type":"markdown","source":"<a id='bakground'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Background"},{"metadata":{},"cell_type":"markdown","source":"A commercial success movie not only entertains audience, but also enables film companies to gain tremendous profit. A lot of factors such as good directors, experienced actors are considerable for creating good movies. However, famous directors and actors can always bring an expected box-office income but cannot guarantee a highly rated imdb score."},{"metadata":{},"cell_type":"markdown","source":"<a id='datadescription'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Data Description"},{"metadata":{},"cell_type":"markdown","source":"The dataset is from Kaggle website. It contains 28 variables for 5043 movies, spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors/actresses. “imdb_score” is the response variable while the other 27 variables are possible predictors."},{"metadata":{},"cell_type":"markdown","source":"|Variable Name |\tDescription|\n| --- | --- |\n|movie_title\t | Title of the Movie|\n|duration\t| Duration in minutes|\n|director_name\t| Name of the Director of the Movie|\n|director_facebook_likes |\tNumber of likes of the Director on his Facebook Page|\n|actor_1_name |\tPrimary actor starring in the movie|\n|actor_1_facebook_likes |\tNumber of likes of the Actor_1 on his/her Facebook Page|\n|actor_2_name |\tOther actor starring in the movie|\n|actor_2_facebook_likes\t| Number of likes of the Actor_2 on his/her Facebook Page|\n|actor_3_name |\tOther actor starring in the movie|\n|actor_3_facebook_likes |\tNumber of likes of the Actor_3 on his/her Facebook Page|\n|num_user_for_reviews |\tNumber of users who gave a review|\n|num_critic_for_reviews |\tNumber of critical reviews on imdb|\n|num_voted_users | \tNumber of people who voted for the movie|\n|cast_total_facebook_likes |\tTotal number of facebook likes of the entire cast of the movie|\n|movie_facebook_likes |\tNumber of Facebook likes in the movie page|\n|plot_keywords |\tKeywords describing the movie plot|\n|facenumber_in_poster |\tNumber of the actor who featured in the movie poster|\n|color |\tFilm colorization. ‘Black and White’ or ‘Color’|\n|genres |\tFilm categorization like ‘Animation’, ‘Comedy’, ‘Romance’, ‘Horror’, ‘Sci-Fi’, ‘Action’, ‘Family’|\n|title_year |\tThe year in which the movie is released (1916:2016)|\n|language |\tEnglish, Arabic, Chinese, French, German, Danish, Italian, Japanese etc|\n|country |\tCountry where the movie is produced|\n|content_rating |\tContent rating of the movie|\n|aspect_ratio |\tAspect ratio the movie was made in|\n|movie_imdb_link |\tIMDB link of the movie|\n|gross |\tGross earnings of the movie in Dollars|\n|budget |\tBudget of the movie in Dollars|\n|imdb_score |\tIMDB Score of the movie on IMDB|"},{"metadata":{},"cell_type":"markdown","source":"<a id='problemstatement'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"Based on the massive movie information, it would be interesting to understand what are the important factors that make a movie more successful than others. So, we would like to analyze what kind of movies are more successful, in other words, get higher IMDB score. \n\nIn this notebook we are going to build two different kind of models, Regression and Classification. Under each kind of model we are going to start from a basic model to advanced model and also a description of why we choose advanced one. \n\nUnder Regression we are goint to fit Regression line to our data and find the continous target variable imdb_score.\n\nUnder Classification we are  going to fit the Classification Model to our data and the Classify the imdb_score in to three categories. "},{"metadata":{},"cell_type":"markdown","source":"|imdb_score | Classify |\n| --- | ---|\n|1-3 | Flop Movie|\n|3-6 | Average Movie|\n|6-10 | Hit Movie|"},{"metadata":{},"cell_type":"markdown","source":"<a id='dataexploration'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"<a id='dataloading'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data Loading"},{"metadata":{"code_folding":[],"trusted":true},"cell_type":"code","source":"#importing the libraries that we use\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the dataset\ndataset = pd.read_csv('../input/imdb-5000-movie-dataset/movie_metadata.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='dataprofile'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data Profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop_duplicates(inplace = True)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='datacleaning'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Data Cleaning is a most important part of building a model. Here we do the standard preprocessing steps of the Data cleaning to make sure our model is not feeded crap."},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Missing Value Treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = [col for col in dataset.columns if dataset[col].dtype != 'object']\ncategorical_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols, numerical_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[numerical_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[categorical_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.color.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_mode = dataset['color'].mode().iloc[0]\ndataset.color.fillna(color_mode, inplace = True)\ndataset.color.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.director_name.nunique(), dataset.director_name.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['director_name'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.num_critic_for_reviews.min(), dataset.num_critic_for_reviews.max(), dataset.num_critic_for_reviews.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_critic_for_reviews_median = dataset['num_critic_for_reviews'].median()\ndataset.num_critic_for_reviews.fillna(num_critic_for_reviews_median, inplace = True)\ndataset.num_critic_for_reviews.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.duration.min(), dataset.duration.max(), dataset.duration.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duration_median = dataset.duration.median()\ndataset.duration.fillna(duration_median, inplace = True)\ndataset.duration.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.director_facebook_likes.min(), dataset.director_facebook_likes.max(), dataset.director_facebook_likes.median(),dataset.director_facebook_likes.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"director_facebook_likes_mean = dataset.director_facebook_likes.mean()\ndataset.director_facebook_likes.fillna(director_facebook_likes_mean, inplace = True)\ndataset.director_facebook_likes.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.actor_3_facebook_likes.min(), dataset.actor_3_facebook_likes.max(), dataset.actor_3_facebook_likes.median(),dataset.actor_3_facebook_likes.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_3_facebook_likes_mean = dataset.actor_3_facebook_likes.mean()\ndataset.actor_3_facebook_likes.fillna(actor_3_facebook_likes_mean, inplace = True)\ndataset.actor_3_facebook_likes.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['actor_2_name'])\ndataset.actor_2_name.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.actor_1_facebook_likes.min(), dataset.actor_1_facebook_likes.max(), dataset.actor_1_facebook_likes.median(),dataset.actor_1_facebook_likes.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_1_facebook_likes_mean = dataset.actor_1_facebook_likes.mean()\ndataset.actor_1_facebook_likes.fillna(actor_1_facebook_likes_mean, inplace = True)\ndataset.actor_1_facebook_likes.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.gross.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.gross.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['gross'])\ndataset.gross.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['budget'])\ndataset.budget.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['actor_3_name'])\ndataset.actor_3_name.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"facenumber_in_poster_median = dataset.facenumber_in_poster.median()\ndataset.facenumber_in_poster.fillna(facenumber_in_poster_median, inplace = True)\ndataset.facenumber_in_poster.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.plot_keywords.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.language.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.language.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"language_mode = dataset.language.mode().iloc[0]\ndataset.language.fillna(language_mode, inplace = True)\ndataset.language.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna(axis = 0, subset = ['plot_keywords'])\ndataset.plot_keywords.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.content_rating.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.content_rating.fillna('Not Rated', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.aspect_ratio.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aspect_ratio_mode = dataset.aspect_ratio.mode().iloc[0]\ndataset.aspect_ratio.fillna(aspect_ratio_mode, inplace = True)                                                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.reset_index(inplace = True, drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Profile Report after missing value treatment "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dealing with Null Data amount we have lost 25% of the given data. Let's deal with converting the Data in to numericals to feed our model. "},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Converting Categoricals to Numericals to feed our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols, categorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us deal with the categorical_cols first by converting them in to numericals."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.color.unique(), dataset.color.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we see there are only 2 different categorical variables available in the color variable. We can just map color to 1 and 0 to black and white"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['color'] = dataset.color.map({'Color' : 1 , ' Black and White' : 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.director_name.unique(), dataset.director_name.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"director_name_value_counts = dataset.director_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"director_name_value_counts  = pd.DataFrame(director_name_value_counts).reset_index().rename(columns = {'index': 'director_name', 'director_name':'director_name_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.merge(dataset, director_name_value_counts,left_on = 'director_name', right_on = 'director_name', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'director_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.actor_2_name.unique(), dataset.actor_2_name.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_2_name_value_counts = dataset.actor_2_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_2_name_value_counts  = pd.DataFrame(actor_2_name_value_counts).reset_index().rename(columns = {'index': 'actor_2_name', 'actor_2_name':'actor_2_name_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.merge(dataset, actor_2_name_value_counts,left_on = 'actor_2_name', right_on = 'actor_2_name', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'actor_2_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.genres.unique(), dataset.genres.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column genres has huge amount of values unique values. Let us divide this feature in to 2 different features with main_genre and the genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['main_genre'] = dataset.genres.str.split('|').str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.main_genre.unique(), dataset.main_genre.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets convert both the columns in to the numbericals. The main_genre and the genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset['main_genre'] = le.fit_transform(dataset.main_genre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_value_counts = dataset.genres.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_value_counts  = pd.DataFrame(genres_value_counts).reset_index().rename(columns = {'index' : 'genres', 'genres' : 'genres_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.merge(dataset, genres_value_counts,left_on = 'genres', right_on = 'genres', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'genres')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.actor_1_name.unique(), dataset.actor_1_name.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable actor_1_name is also having high cardinaity, hence we decide to change it in to the number of counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_1_name_value_counts = dataset.actor_1_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_1_name_value_counts = pd.DataFrame(actor_1_name_value_counts).reset_index().rename(columns = {'index' : 'actor_1_name', 'actor_1_name' : 'actor_1_name_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.merge(dataset, actor_1_name_value_counts,left_on = 'actor_1_name', right_on = 'actor_1_name', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'actor_1_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.movie_title.unique(), dataset.movie_title.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see out of 3816 records, we have 3749 unique records which in not helpful for us for making predictions. So we drop the column from our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'movie_title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.actor_3_name.unique(), dataset.actor_3_name.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This variable also has high cadinality. So changing it in to the value counts variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_3_name_value_counts = dataset.actor_3_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actor_3_name_value_counts = pd.DataFrame(actor_3_name_value_counts).reset_index().rename(columns = {'index' : 'actor_3_name', 'actor_3_name' : 'actor_3_name_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset= pd.merge(dataset, actor_3_name_value_counts,left_on = 'actor_3_name', right_on = 'actor_3_name', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'actor_3_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.plot_keywords.unique(), dataset.plot_keywords.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking in to the variable, we can see has a high cardinality which is unstable and we can delete such variable and mainly, we need to extract the main_plot_keywords of all in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['main_plot_keyword'] = dataset.plot_keywords.str.split('|').str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'plot_keywords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.main_plot_keyword.unique(), dataset.main_plot_keyword.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see the extracted main Plot keyword also consists of high cardinality but is stable. we can replace it with the value counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_plot_keyword_value_counts = dataset.main_plot_keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_plot_keyword_value_counts = pd.DataFrame(main_plot_keyword_value_counts).reset_index().rename(columns = {'index' : 'main_plot_keyword', 'main_plot_keyword' : 'main_plot_keyword_value_counts'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.merge(dataset, main_plot_keyword_value_counts, left_on = 'main_plot_keyword', right_on = 'main_plot_keyword', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'main_plot_keyword')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.movie_imdb_link.unique(), dataset.movie_imdb_link.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This variable movie_imdb_link is however unique the whole. So considering it will not help out prediciting variable we drop it off."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns = 'movie_imdb_link')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.language.unique(), dataset.language.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Language variable has only 38 unique values and is consistent. So, we just do label encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\ndataset['language'] = le1.fit_transform(dataset.language)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.country.unique(), dataset.country.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Country variable has only 47 unique values and is consistent. So, we just do label encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle2 = LabelEncoder()\ndataset['country'] = le2.fit_transform(dataset.country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.content_rating.unique(),dataset.content_rating.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Content rating has only 12 unique variables and can be done label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle3 = LabelEncoder()\ndataset['content_rating'] = le3.fit_transform(dataset.content_rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.4 Profile Report after data cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we look in to the profile report we are now having warnings of about the skewness and the zeros. This will be wiped off after doing a scaling operation after dealing with spiltting the dataset. All the unwanted variables will also be removed during the Feature elimination"},{"metadata":{},"cell_type":"markdown","source":"<a id='rmb'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 3. Regression Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetR = dataset.copy() #lets keep our original dataset for reference. Here datasetR is for Regression model\ndatasetC = dataset.copy() #Here datasetC is for classification model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='std'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Splitting the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = datasetR.pop('imdb_score')\nX = datasetR\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='s'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Scaling to avoid Euclidean Distance Problem"},{"metadata":{},"cell_type":"markdown","source":"We do scaling after we aplit the dataset as we donot want to make our training set metrics to fit the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns, index=X_train.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_train.columns, index = X_test.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building our model. As we are having many number of features, out of which there will be only some useful. Lets do some feature selection for our Regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='fe'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Feature Elimination "},{"metadata":{},"cell_type":"markdown","source":"We dont want our model to feed with all the variables which might mot help in prediction. We do remove variables having High Collinearity and use only variables useful for our model by doing the Recursive Feature Elimination."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing variables with high colinearity\ndef correlation(dataset, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in dataset.columns:\n                    del dataset[colname] # deleting the column from the dataset\ncorrelation(X_train,0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the required libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)            # running RFE\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_rfe = X_train.columns[rfe.support_]\ncol_rfe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a X_train dataframe with rfe varianles\nX_train_rfe = X_train[col_rfe]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='slr'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Simple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable for using the stats model\nimport statsmodels.api as sm\nX_train_rfe_constant = sm.add_constant(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_rfe_constant).fit()   # Running the linear model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_rfe = X_test[col_rfe]\nX_test_rfe_constant = sm.add_constant(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_linear = lm.predict(X_test_rfe_constant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_linear.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_linear.min(), y_pred_linear.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred_linear, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking in to the stats, We observe that the r2 score is low of about 0.37 aafter having all consistent variables and the regression line is not fitting the data correctly. So we have to go for much advanced curved model such as support vector machine and ensemble algorithms to make our model to fit the data correctly."},{"metadata":{},"cell_type":"markdown","source":"<a id='svmr'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Support Vector Machines with Linear, Polynomial, RBF Kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr_rbf = SVR(kernel='rbf', gamma=0.1)\nsvr_lin = SVR(kernel='linear', gamma='auto')\nsvr_poly = SVR(kernel='poly', gamma='auto', degree=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_rbf.fit(X_train_rfe, y_train)\ny_pred_svm_rbf = svr_rbf.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_rbf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_rbf.min(), y_pred_svm_rbf.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred_svm_rbf, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_lin.fit(X_train_rfe, y_train)\ny_pred_svm_lin = svr_lin.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_lin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_lin.min(), y_pred_svm_lin.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred_svm_lin, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_poly.fit(X_train_rfe, y_train)\ny_pred_svm_poly = svr_poly.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_poly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm_poly.min(), y_pred_svm_poly.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred_svm_poly, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='em'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.6 Ensemble Models"},{"metadata":{},"cell_type":"markdown","source":"<a id='gbr'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 3.6.1 Gradient Boosting with Hyper Parameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nn_trees=200\ngradientboost = ensemble.GradientBoostingRegressor(loss='ls',learning_rate=0.03,n_estimators=n_trees,max_depth=4)\ngradientboost.fit(X_train_rfe,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gb=gradientboost.predict(X_test_rfe)\nerror=gradientboost.loss_(y_test,y_pred_gb) ##Loss function== Mean square error\nprint(\"MSE:%.3f\" % error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred_gb, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gb.min(), y_pred_gb.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'loss' : ['ls'],\n    'max_depth' : [3, 4, 5],\n    'learning_rate' : [0.01, 0.001],\n    'n_estimators': [100, 200, 500]\n}\n# Create a based model\ngb = ensemble.GradientBoostingRegressor()\n# Instantiate the grid search model\ngrid_search_gb = GridSearchCV(estimator = gb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_gb.fit(X_train_rfe, y_train)\ngrid_search_gb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_gb_pred = grid_search_gb.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_test.values, grid_search_gb_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='rbr'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 3.6.2 Random Forest with Hyper Parameter Tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators = 500)\nrf_regressor.fit(X_train_rfe, y_train)\nrf_pred = rf_regressor.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(rf_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets tweek in to the hyperparameter tuning of the RandomForestRegressor to find the best parameters of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_rf.fit(X_train_rfe, y_train)\ngrid_search_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_grid_pred_rf = grid_search_rf.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_grid_pred_rf, y_test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='xgbr'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.7 XGBoost with Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxg_model = xgb.XGBRegressor(n_estimators = 500)\nxg_model.fit(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = xg_model.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(results, y_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_model.score(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [3, 4],\n    'learning_rate' : [0.1, 0.01, 0.05],\n    'n_estimators' : [100, 500, 1000]\n}\n# Create a based model\nmodel_xgb= xgb.XGBRegressor()\n# Instantiate the grid search model\ngrid_search_xgb = GridSearchCV(estimator = model_xgb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_xgb.fit(X_train_rfe, y_train)\ngrid_search_xgb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = grid_search_xgb.predict(X_test_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_test.values, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='irr'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 3.8 Interpreting Results of Regression Model"},{"metadata":{},"cell_type":"markdown","source":"Considering XG Boost as a final model with very less error rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = grid_search_xgb.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking in to all the metrics almost we have seen that XGBRegressor with \"{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500}\" these parameters has given the best results with mean squared error of 0.404. The Feature Importance given by this model is shown above."},{"metadata":{},"cell_type":"markdown","source":"<a id='bc'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 4. Building a Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetC.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To Build a classification Model I would like to reuse the preprocessed data from the Regression Model.\nHowever I am going to replace the target variable and create a new target variable for our classification Model."},{"metadata":{},"cell_type":"markdown","source":"|imdb_score | Classify |\n| --- | ---|\n1-3 | Flop Movie\n3-6 | Average Movie\n6-10 | Hit Movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_classification = y_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_classification = pd.cut(y_train_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_classification = y_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_classification = pd.cut(y_test_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have created the target variable and now we will re use the independent variables form the Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe_classification = X_train_rfe.copy()\nX_test_rfe_classification = X_test_rfe.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='lr'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regresion is a linear algorithm does basically a binary classification. In order to use the Logistic Regression for Multiclass Classification we need to use the parameter solver as 'saga'. There are also other parameters for solver to do multiclass classification, I used saga as it also does L2 regularisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogit_model = LogisticRegression(solver = 'saga', random_state = 0)\nlogit_model.fit(X_train_rfe_classification, y_train_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_logit_pred = logit_model.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_logit_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_logit_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_logit_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_logit_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='asvmc'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Support Vector Classifier with Linear, Polynomial, RBF"},{"metadata":{},"cell_type":"markdown","source":"Support Vector Classifier also basically does binary classification. In order to achieve the multi classification, we need to use the decision_function_shape as 'ovo'. The original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_linear_model = SVC(kernel='linear', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_linear_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_linear_pred = svc_linear_model.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_svc_linear_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_linear_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_linear_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_linear_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_poly_model = SVC(kernel='poly', C=100, gamma= 'scale', degree = 3, decision_function_shape='ovo', random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_poly_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_poly_pred = svc_poly_model.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_svc_poly_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_poly_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_poly_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_poly_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_rbf_model = SVC(kernel='rbf', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_rbf_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_rbf_pred = svc_rbf_model.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_svc_rbf_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_rbf_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_rbf_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_rbf_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='emc'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Ensemble Models"},{"metadata":{},"cell_type":"markdown","source":"<a id='rfc'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.1 Random Forest Classifier with Hyper Parameter tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500, 1000],\n    'random_state' :[0]\n}\n# Create a based model\nrf_model_classification = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search_rf_model_classificaiton = GridSearchCV(estimator = rf_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_rf_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rf_classification_pred = grid_search_rf_model_classificaiton.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rf_classification_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_rf_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_rf_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_rf_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='gbc'></a>"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.2 Gradient Boost Classifier with Hyper Parameter Tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [10, 50, 90],\n    'max_features': [3],\n    'min_samples_leaf': [3],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500],\n    'learning_rate' : [0.1, 0.2],\n    'random_state' : [0]\n}\n# Create a based model\ngbc_model_classification = GradientBoostingClassifier()\n# Instantiate the grid search model\ngrid_search_gbc_model_classificaiton = GridSearchCV(estimator = gbc_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_gbc_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_gbc_model_pred = grid_search_gbc_model_classificaiton.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_gbc_model_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_gbc_model_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_gbc_model_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_gbc_model_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='xgbc'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 4.4 XG Boost Classifier with Hyper Parameter Tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n     'objective' : ['multi:softmax', 'multi:softprob'],\n     'n_estimators': [100, 500, 1000],\n     'random_state': [0]\n}\n# Create a based model\nxgb_model_classification = XGBClassifier()\n# Instantiate the grid search model\ngrid_search_xgb_model_classificaiton = GridSearchCV(estimator = xgb_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_xgb_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_xgb_classification_pred = grid_search_xgb_model_classificaiton.predict(X_test_rfe_classification)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_xgb_classification_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncount_misclassified = (y_test_classification != y_xgb_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_xgb_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_xgb_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see that the Gradient Boost with Hyper Parameter seems to give us the best Results. This is because the nature of Ensemble models tend to being overfitted. However we consider the final model for our classification as Gradient Boosting Classifier."},{"metadata":{},"cell_type":"markdown","source":"<a id='ircm'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 4.5 Interpreting Results of Classfication Model"},{"metadata":{},"cell_type":"markdown","source":"Considering Gradient Boosting classifier as the final model with 83 % accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = grid_search_gbc_model_classificaiton.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='conclusion'></a>"},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"After Looking in to the feature importance of the best models in the Regression and Classification Model we see that both the models have given almost the same amount of importance to the respective features, considering XGBosot Regressor and Gradient Boost Classiifier. The results of all Regression and Classification Models are as follows:\n\n|Regression Model|Mean_squared_error|\n| --- | --- |\n|Simple Linear Regression |0.70|\n|SVRegressor Linear|0.72|\n|SVRegressor Polynomial|0.93|\n|SVRegressor RBF|0.68|\n|Gradient Boost|0.43|\n|Random Forest|0.45|\n|XGBoost|0.40|\n\n|Classification  Model|MisClassifications|Accuracy|Precision|Recall|F1-Score|\n| --- | --- | --- | --- | --- | --- |\n| Logistic Regression | 190 | 0.75 | 0.47 | 0.40 | 0.41 |\n| SVC Linear | 181 | 0.76 | 0.47 | 0.45 | 0.46 |\n| SVC Polynomial | 143 | 0.81 | 0.52 | 0.50 | 0.51 |\n| SVC RBF | 146 | 0.81 | 0.51 | 0.50 | 0.50 |\n| Random Forest | 130 | 0.83 | 0.54 | 0.50 | 0.51 |\n| Gradient Boosting | 127 | 0.83 | 0.54 | 0.51 | 0.52 |\n| XGBoost | 139 | 0.82 | 0.52 | 0.51 | 0.51 |\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}