{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Weight of Evidence Encoding & Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to apply weight of evidence encoding to a data set and run a logistic regression model. This combination, which for all intents and purposes amounts to a risk score card, often results in a solid baseline model for classification problems without too much effort.\n\nRead [https://medium.com/@sundarstyles89/weight-of-evidence-and-information-value-using-python-6f05072e83eb] if you'd like to learn more."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import the necessary libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\n\nimport sklearn.linear_model as skl_lm\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score\nfrom sklearn.model_selection import train_test_split\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# initialize some package settings\nsns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the data and check the first 5 rows\ndf = pd.read_csv('../input/data.csv', index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# remove the 'Unnamed: 32' column\ndf = df.drop('Unnamed: 32', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the 'target' variable to numeric\ndf['target'] = df['diagnosis'].apply(lambda x : 1 if x == 'M' else 0)  # Convert to numeric\ndf = df.drop('diagnosis',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The weight of evidence encoding below is drawn from [https://github.com/Sundar0989/WOE-and-IV/blob/master/WOE_IV.ipynb]."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import packages\nimport pandas as pd\nimport numpy as np\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nmax_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weight of Evidence encode the data\nfinal_iv, IV = data_vars(df, df.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the variable names and their respective Information Value\nIV.sort_values('IV')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_vars_list = df.columns.difference(['target'])\ntransform_prefix = '' # leave this value blank if you need to replace the original column values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in transform_vars_list:\n    small_df = final_iv[final_iv['VAR_NAME'] == var]\n    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n    replace_cmd = ''\n    replace_cmd1 = ''\n    for i in sorted(transform_dict.items()):\n        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n    replace_cmd = replace_cmd + '0'\n    replace_cmd1 = replace_cmd1 + '0'\n    if replace_cmd != '0':\n        try:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n        except:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into training and testing sets\nX = df\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels as sm\nimport pandas as pd\nimport sklearn.preprocessing as preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\n\nlogreg = LogisticRegression(fit_intercept = False, C = 1e15, class_weight='balanced')\nmodel_log = logreg.fit(X_train, y_train)\nmodel_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_log.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\nprint(confusion_matrix(y_test,y_pred))  \nprint(classification_report(y_test,y_pred)) \nprint(\"The accuracy score is\" + \" \"+ str(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}