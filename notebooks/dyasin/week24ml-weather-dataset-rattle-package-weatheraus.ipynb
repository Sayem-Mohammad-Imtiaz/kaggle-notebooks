{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/dyasin/week24ml-weather-dataset-rattle-package-weatheraus/edit/run/68212956","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error,roc_curve,roc_auc_score,classification_report,r2_score,confusion_matrix\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n# Plotly for interactive graphics \nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport missingno as msno\n# Disabling warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-16T22:31:23.621695Z","iopub.execute_input":"2021-07-16T22:31:23.62234Z","iopub.status.idle":"2021-07-16T22:31:25.008162Z","shell.execute_reply.started":"2021-07-16T22:31:23.622222Z","shell.execute_reply":"2021-07-16T22:31:25.006882Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After performing all data processing and cleaning steps, and after splitting train /test.\n\n# Logistic regression\n# Naive Bayes\n# K-Nearest Neighbor (KNN)\n# Support Vector Mechanism (SVM) Get predictions using Machine Learning models and compare these scores.\n# Which of these models is the best?","metadata":{}},{"cell_type":"markdown","source":"# DATA READING AND EXPLORING","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\ndf = data.copy().sample(10000)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.009941Z","iopub.execute_input":"2021-07-16T22:31:25.010311Z","iopub.status.idle":"2021-07-16T22:31:25.657081Z","shell.execute_reply.started":"2021-07-16T22:31:25.010278Z","shell.execute_reply":"2021-07-16T22:31:25.655957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.659212Z","iopub.execute_input":"2021-07-16T22:31:25.659606Z","iopub.status.idle":"2021-07-16T22:31:25.689065Z","shell.execute_reply.started":"2021-07-16T22:31:25.659566Z","shell.execute_reply":"2021-07-16T22:31:25.688064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"No. of unique values\": list(df.nunique())}, index=df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.690649Z","iopub.execute_input":"2021-07-16T22:31:25.690939Z","iopub.status.idle":"2021-07-16T22:31:25.727223Z","shell.execute_reply.started":"2021-07-16T22:31:25.690911Z","shell.execute_reply":"2021-07-16T22:31:25.726222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.select_dtypes(include='object'):   \n    print(i,'-->',df[i].unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.728512Z","iopub.execute_input":"2021-07-16T22:31:25.728852Z","iopub.status.idle":"2021-07-16T22:31:25.749596Z","shell.execute_reply.started":"2021-07-16T22:31:25.728821Z","shell.execute_reply":"2021-07-16T22:31:25.748474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = len(df[df['RainTomorrow'] == 'Yes'])\nn = len(df[df['RainTomorrow'] == 'No'])\nprint(y,n)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.750776Z","iopub.execute_input":"2021-07-16T22:31:25.751124Z","iopub.status.idle":"2021-07-16T22:31:25.763342Z","shell.execute_reply.started":"2021-07-16T22:31:25.751092Z","shell.execute_reply":"2021-07-16T22:31:25.76253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.764367Z","iopub.execute_input":"2021-07-16T22:31:25.764829Z","iopub.status.idle":"2021-07-16T22:31:25.794902Z","shell.execute_reply.started":"2021-07-16T22:31:25.764796Z","shell.execute_reply":"2021-07-16T22:31:25.793884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.797376Z","iopub.execute_input":"2021-07-16T22:31:25.797658Z","iopub.status.idle":"2021-07-16T22:31:25.869175Z","shell.execute_reply.started":"2021-07-16T22:31:25.797631Z","shell.execute_reply":"2021-07-16T22:31:25.867848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\nprint(df.ndim)\nprint(df.size)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.871466Z","iopub.execute_input":"2021-07-16T22:31:25.871855Z","iopub.status.idle":"2021-07-16T22:31:25.880111Z","shell.execute_reply.started":"2021-07-16T22:31:25.871813Z","shell.execute_reply":"2021-07-16T22:31:25.878832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we can see the variables in the model and set the variables for the model. \n# We can also remove some variables.\ndf.corr()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.881625Z","iopub.execute_input":"2021-07-16T22:31:25.882138Z","iopub.status.idle":"2021-07-16T22:31:25.933382Z","shell.execute_reply.started":"2021-07-16T22:31:25.882091Z","shell.execute_reply":"2021-07-16T22:31:25.93236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SOME VISUALIZATION","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (16,6)) \nsns.heatmap(df.corr(),robust=True,fmt='.1g',linewidths=1.3,linecolor = 'gold', annot=True,);","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:25.934695Z","iopub.execute_input":"2021-07-16T22:31:25.934998Z","iopub.status.idle":"2021-07-16T22:31:27.808327Z","shell.execute_reply.started":"2021-07-16T22:31:25.934972Z","shell.execute_reply":"2021-07-16T22:31:27.807233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VISUALIZATION OF NAN  VALUES\nmsno.matrix(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:27.809842Z","iopub.execute_input":"2021-07-16T22:31:27.810245Z","iopub.status.idle":"2021-07-16T22:31:28.626229Z","shell.execute_reply.started":"2021-07-16T22:31:27.810205Z","shell.execute_reply":"2021-07-16T22:31:28.625143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop missing values in the RainToday and RainTomorrow\ndf.dropna(subset=['RainToday', 'RainTomorrow'],axis=0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:28.627725Z","iopub.execute_input":"2021-07-16T22:31:28.628168Z","iopub.status.idle":"2021-07-16T22:31:28.654478Z","shell.execute_reply.started":"2021-07-16T22:31:28.628122Z","shell.execute_reply":"2021-07-16T22:31:28.653481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"RainToday\",data=df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:28.655808Z","iopub.execute_input":"2021-07-16T22:31:28.656137Z","iopub.status.idle":"2021-07-16T22:31:28.817961Z","shell.execute_reply.started":"2021-07-16T22:31:28.656107Z","shell.execute_reply":"2021-07-16T22:31:28.816954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"RainTomorrow\",data=df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:28.819511Z","iopub.execute_input":"2021-07-16T22:31:28.819908Z","iopub.status.idle":"2021-07-16T22:31:28.9846Z","shell.execute_reply.started":"2021-07-16T22:31:28.819868Z","shell.execute_reply":"2021-07-16T22:31:28.983483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multivariate imputation\n* In multivariate imputation, we use ML Algorithms and before that we need to encode the categorical variables.(Çok değişkenli atamada, ML Algoritmaları kullanıyoruz ve bundan önce kategorik değişkenleri kodlamamız gerekiyor.)","metadata":{}},{"cell_type":"code","source":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # .shape[0] yazilmaz ise unique olan degerlerin listelerini getirir.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n    str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:28.985979Z","iopub.execute_input":"2021-07-16T22:31:28.986327Z","iopub.status.idle":"2021-07-16T22:31:29.071622Z","shell.execute_reply.started":"2021-07-16T22:31:28.986296Z","shell.execute_reply":"2021-07-16T22:31:29.070573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding for univariate variables.\nfrom sklearn.preprocessing import LabelEncoder\n\nlbe = LabelEncoder()\ndf[\"RainToday_label\"] = lbe.fit_transform(df[\"RainToday\"])\ndf[\"RainTomorrow_label\"] = lbe.fit_transform(df[\"RainTomorrow\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:29.073179Z","iopub.execute_input":"2021-07-16T22:31:29.073595Z","iopub.status.idle":"2021-07-16T22:31:29.08823Z","shell.execute_reply.started":"2021-07-16T22:31:29.073551Z","shell.execute_reply":"2021-07-16T22:31:29.087118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding for variables with more than 2 categories\n\n#drop variables with so many countries for the sake of time and memory consumption\ndf.drop(['Location','WindDir9am','WindDir3pm'], axis=1, inplace=True) \n\ndf = pd.get_dummies(df, drop_first=True, columns = ['WindGustDir'], prefix = ['WindGustDir'])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:29.089592Z","iopub.execute_input":"2021-07-16T22:31:29.090032Z","iopub.status.idle":"2021-07-16T22:31:29.113578Z","shell.execute_reply.started":"2021-07-16T22:31:29.09Z","shell.execute_reply":"2021-07-16T22:31:29.112486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # .shape[0] yazilmaz ise unique olan degerlerin listelerini getirir.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n    str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:29.114799Z","iopub.execute_input":"2021-07-16T22:31:29.115121Z","iopub.status.idle":"2021-07-16T22:31:29.196203Z","shell.execute_reply.started":"2021-07-16T22:31:29.115091Z","shell.execute_reply":"2021-07-16T22:31:29.194983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DecisionTreeRegressor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor\n\n#drop unnecessary columns and date columns\ndf_imputation = df.drop(['Date','RainToday','RainTomorrow'], axis=1) \n\n#define variables to keep the index and the columns\nindex = df_imputation.index\ncolumns = df_imputation.columns\n\n#imputation steps\nimp_tree = IterativeImputer(random_state=0, estimator=DecisionTreeRegressor())\nimp_tree.fit(df_imputation)\ndf_imputed = imp_tree.transform(df_imputation)\n\n#transform imputed data in array format to dataframe\ndf_imputed_tree = pd.DataFrame(df_imputed, index=index, columns=columns)\n\ndf_imputed_tree.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:31:29.197861Z","iopub.execute_input":"2021-07-16T22:31:29.198296Z","iopub.status.idle":"2021-07-16T22:32:09.290202Z","shell.execute_reply.started":"2021-07-16T22:31:29.19825Z","shell.execute_reply":"2021-07-16T22:32:09.289264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_imputed_tree.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.291311Z","iopub.execute_input":"2021-07-16T22:32:09.291598Z","iopub.status.idle":"2021-07-16T22:32:09.311397Z","shell.execute_reply.started":"2021-07-16T22:32:09.29157Z","shell.execute_reply":"2021-07-16T22:32:09.310077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_imputed_tree.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.312812Z","iopub.execute_input":"2021-07-16T22:32:09.313193Z","iopub.status.idle":"2021-07-16T22:32:09.323852Z","shell.execute_reply.started":"2021-07-16T22:32:09.313162Z","shell.execute_reply":"2021-07-16T22:32:09.322695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTION WITH CLASSIFICATION METHODS\n# Preparation dependent and independent variables","metadata":{}},{"cell_type":"code","source":"df2 = df_imputed_tree.copy()\nx_dat = df2.drop(['RainTomorrow_label'],axis=1)\ny = df2[\"RainTomorrow_label\"].values","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.330138Z","iopub.execute_input":"2021-07-16T22:32:09.330461Z","iopub.status.idle":"2021-07-16T22:32:09.33818Z","shell.execute_reply.started":"2021-07-16T22:32:09.330427Z","shell.execute_reply":"2021-07-16T22:32:09.336986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(df2)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.342122Z","iopub.execute_input":"2021-07-16T22:32:09.342437Z","iopub.status.idle":"2021-07-16T22:32:09.392955Z","shell.execute_reply.started":"2021-07-16T22:32:09.342409Z","shell.execute_reply":"2021-07-16T22:32:09.392005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalization","metadata":{}},{"cell_type":"code","source":"#If there is a outlier values, it must be done before coming here.\nx = (x_dat-np.min(x_dat))/(np.max(x_dat)-np.min(x_dat)).values","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.394137Z","iopub.execute_input":"2021-07-16T22:32:09.394418Z","iopub.status.idle":"2021-07-16T22:32:09.405935Z","shell.execute_reply.started":"2021-07-16T22:32:09.394391Z","shell.execute_reply":"2021-07-16T22:32:09.40507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LETS TRY CLASSIFICATIONS METHODS\n* Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\n* We'll train it (find the patterns) on the training set.\n\n* And we'll test it (use the patterns) on the test set.\n\n* We're going to try machine learning models:\n\n1) Logistic Regression\n\n2) Naive Bayes\n\n3) K-Nearest Neighbor (KNN)\n\n4) Support Vector Mechanism (SVM)","metadata":{}},{"cell_type":"markdown","source":"# 1) LOGISTIC REGRESSION","metadata":{}},{"cell_type":"markdown","source":"# A) Train-test splitting","metadata":{}},{"cell_type":"code","source":"y = df.RainTomorrow_label.values\nx_dat = df.drop([\"RainTomorrow_label\"],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.407028Z","iopub.execute_input":"2021-07-16T22:32:09.407343Z","iopub.status.idle":"2021-07-16T22:32:09.413614Z","shell.execute_reply.started":"2021-07-16T22:32:09.407315Z","shell.execute_reply":"2021-07-16T22:32:09.412528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit,GridSearchCV\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.414886Z","iopub.execute_input":"2021-07-16T22:32:09.415204Z","iopub.status.idle":"2021-07-16T22:32:09.43021Z","shell.execute_reply.started":"2021-07-16T22:32:09.415174Z","shell.execute_reply":"2021-07-16T22:32:09.429169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B) Modeling of Logistic Regression Method","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression() # Default olanlar gelir.C var..\nlr_model.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.431501Z","iopub.execute_input":"2021-07-16T22:32:09.431803Z","iopub.status.idle":"2021-07-16T22:32:09.74763Z","shell.execute_reply.started":"2021-07-16T22:32:09.431774Z","shell.execute_reply":"2021-07-16T22:32:09.746586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver = 'liblinear')\nlr.fit(x_train,y_train)\ny_pred = lr.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.749361Z","iopub.execute_input":"2021-07-16T22:32:09.750082Z","iopub.status.idle":"2021-07-16T22:32:09.819542Z","shell.execute_reply.started":"2021-07-16T22:32:09.750018Z","shell.execute_reply":"2021-07-16T22:32:09.81836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr #We can see what there is in lr(icinde hangi secenekler vargormek icin).","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.82145Z","iopub.execute_input":"2021-07-16T22:32:09.822217Z","iopub.status.idle":"2021-07-16T22:32:09.830588Z","shell.execute_reply.started":"2021-07-16T22:32:09.822165Z","shell.execute_reply":"2021-07-16T22:32:09.829513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.intercept_  # Constant coefficient(Sabit katsayi).","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.832697Z","iopub.execute_input":"2021-07-16T22:32:09.833457Z","iopub.status.idle":"2021-07-16T22:32:09.84696Z","shell.execute_reply.started":"2021-07-16T22:32:09.833408Z","shell.execute_reply":"2021-07-16T22:32:09.845753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.coef_   # Variable coefficients(Degisken katsayilari). # Bu katsayilar denklemin katsayilari. \n           # Ornegin (-) olanlar ters yonde etkiliyor.","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.849445Z","iopub.execute_input":"2021-07-16T22:32:09.850287Z","iopub.status.idle":"2021-07-16T22:32:09.862677Z","shell.execute_reply.started":"2021-07-16T22:32:09.850223Z","shell.execute_reply":"2021-07-16T22:32:09.861575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# C) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","metadata":{}},{"cell_type":"code","source":"l_score = accuracy_score(y_test,y_pred)\nl_score\n# The y predicted by the y in the test are compared(test deki y ile tahmin edilen yler karsilastiriliyor.\n# Dogru tahmin etme yuzdesi bulunuyor)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.867829Z","iopub.execute_input":"2021-07-16T22:32:09.870729Z","iopub.status.idle":"2021-07-16T22:32:09.884247Z","shell.execute_reply.started":"2021-07-16T22:32:09.870654Z","shell.execute_reply":"2021-07-16T22:32:09.883145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_l = confusion_matrix(y_test,y_pred)# We found the numbers of guessing with confusion matrix.\n                                    # (Confusion matrixle tahmin etme sayilarini bulduk.)\nc_l                                ","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.888316Z","iopub.execute_input":"2021-07-16T22:32:09.892199Z","iopub.status.idle":"2021-07-16T22:32:09.914359Z","shell.execute_reply.started":"2021-07-16T22:32:09.892134Z","shell.execute_reply":"2021-07-16T22:32:09.912964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir.\ny_true = y_test\ny_pred = lr.predict(x_test)\ncmlr = confusion_matrix(y_true, y_pred)\nf,ax = plt.subplots(figsize=(6,6))\nsns.heatmap(cmlr, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:09.919491Z","iopub.execute_input":"2021-07-16T22:32:09.922208Z","iopub.status.idle":"2021-07-16T22:32:10.230359Z","shell.execute_reply.started":"2021-07-16T22:32:09.922136Z","shell.execute_reply":"2021-07-16T22:32:10.229619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLASSICICATION REPORT","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.231939Z","iopub.execute_input":"2021-07-16T22:32:10.232364Z","iopub.status.idle":"2021-07-16T22:32:10.247368Z","shell.execute_reply.started":"2021-07-16T22:32:10.232321Z","shell.execute_reply":"2021-07-16T22:32:10.246269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# D) LOOK AT ALL PREDICTION VALUE ON TEST DATA","metadata":{}},{"cell_type":"code","source":"lr.predict(x_test)[0:10] # Our predictions in the top 10 data test(Ilk 10 datatest deki tahminlerimiz).","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.248749Z","iopub.execute_input":"2021-07-16T22:32:10.249162Z","iopub.status.idle":"2021-07-16T22:32:10.264863Z","shell.execute_reply.started":"2021-07-16T22:32:10.249117Z","shell.execute_reply":"2021-07-16T22:32:10.263569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.predict_proba(x_test)[0:10] # 1.si 0 olma 2.si 1 olma olasiligi oranlari.","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.266421Z","iopub.execute_input":"2021-07-16T22:32:10.271486Z","iopub.status.idle":"2021-07-16T22:32:10.289701Z","shell.execute_reply.started":"2021-07-16T22:32:10.271431Z","shell.execute_reply":"2021-07-16T22:32:10.28841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# E) TUNING THE PREDICTION\n* We can tune our prediction.","metadata":{}},{"cell_type":"code","source":"y_probs = lr.predict_proba(x_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.293645Z","iopub.execute_input":"2021-07-16T22:32:10.294116Z","iopub.status.idle":"2021-07-16T22:32:10.307493Z","shell.execute_reply.started":"2021-07-16T22:32:10.294069Z","shell.execute_reply":"2021-07-16T22:32:10.306567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = [1 if i>0.60 else 0 for i in y_probs]\ny_pred[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.309022Z","iopub.execute_input":"2021-07-16T22:32:10.309835Z","iopub.status.idle":"2021-07-16T22:32:10.319374Z","shell.execute_reply.started":"2021-07-16T22:32:10.309788Z","shell.execute_reply":"2021-07-16T22:32:10.318477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.320794Z","iopub.execute_input":"2021-07-16T22:32:10.32145Z","iopub.status.idle":"2021-07-16T22:32:10.342894Z","shell.execute_reply.started":"2021-07-16T22:32:10.321407Z","shell.execute_reply":"2021-07-16T22:32:10.341834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy_score(y_test,y_pred)\n# log_score = accuracy_score(y_test,y_pred)\nlog_score = accuracy_score(y_test,y_pred)\nprint (\"log score=\",log_score)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.344527Z","iopub.execute_input":"2021-07-16T22:32:10.345196Z","iopub.status.idle":"2021-07-16T22:32:10.35447Z","shell.execute_reply.started":"2021-07-16T22:32:10.345153Z","shell.execute_reply":"2021-07-16T22:32:10.35347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation (10 katli ) yaparsak\nlr_finalscore = cross_val_score(lr_model, x_test, y_test, cv = 10).mean()\nlr_finalscore","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.356293Z","iopub.execute_input":"2021-07-16T22:32:10.35696Z","iopub.status.idle":"2021-07-16T22:32:10.934538Z","shell.execute_reply.started":"2021-07-16T22:32:10.356916Z","shell.execute_reply":"2021-07-16T22:32:10.933501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) NAIVE BAYES METHOD\n* In machine learning, Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n* Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem.","metadata":{}},{"cell_type":"markdown","source":"# A) Train-test splitting","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model = nb_model.fit(x_train, y_train)\nnb_model","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.936219Z","iopub.execute_input":"2021-07-16T22:32:10.936862Z","iopub.status.idle":"2021-07-16T22:32:10.961202Z","shell.execute_reply.started":"2021-07-16T22:32:10.936821Z","shell.execute_reply":"2021-07-16T22:32:10.960196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B) Modeling of Naive Bayes Method","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\ny_pred = nb.predict(x_test)\ny_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.96283Z","iopub.execute_input":"2021-07-16T22:32:10.963498Z","iopub.status.idle":"2021-07-16T22:32:10.986602Z","shell.execute_reply.started":"2021-07-16T22:32:10.963456Z","shell.execute_reply":"2021-07-16T22:32:10.985428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"?nb","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:10.988175Z","iopub.execute_input":"2021-07-16T22:32:10.988849Z","iopub.status.idle":"2021-07-16T22:32:10.998401Z","shell.execute_reply.started":"2021-07-16T22:32:10.988787Z","shell.execute_reply":"2021-07-16T22:32:10.99702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# C) Lets control the succes(score) prediction(accuracy_score,confusion m.) on test_data","metadata":{}},{"cell_type":"code","source":"n_score = accuracy_score(y_test,y_pred)\nn_score","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.003471Z","iopub.execute_input":"2021-07-16T22:32:11.006407Z","iopub.status.idle":"2021-07-16T22:32:11.020334Z","shell.execute_reply.started":"2021-07-16T22:32:11.006347Z","shell.execute_reply":"2021-07-16T22:32:11.019066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_nb = confusion_matrix(y_test,y_pred)\nc_nb\n# We found the numbers to guess with confusion matrix(confusion matrixle tahmin etme sayilarini bulduk).","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.025076Z","iopub.execute_input":"2021-07-16T22:32:11.02578Z","iopub.status.idle":"2021-07-16T22:32:11.0402Z","shell.execute_reply.started":"2021-07-16T22:32:11.025735Z","shell.execute_reply":"2021-07-16T22:32:11.038882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HEATMAP IN CONFUSION MATRIX\n* We can see the confusion matrix in Heatmap.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix   #Hepsi icin yapilabilir\ny_true = y_test\ny_pred = nb.predict(x_test)\ncmnb = confusion_matrix(y_true, y_pred)\nf,ax = plt.subplots(figsize=(6,6))\nsns.heatmap(cmnb, annot=True)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.041784Z","iopub.execute_input":"2021-07-16T22:32:11.042212Z","iopub.status.idle":"2021-07-16T22:32:11.304948Z","shell.execute_reply.started":"2021-07-16T22:32:11.04217Z","shell.execute_reply":"2021-07-16T22:32:11.304191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOOK AT ALL PREDICTION VALUE ON TEST DATA","metadata":{}},{"cell_type":"code","source":"nb.predict(x_test)[0:10] # Our predictions in the top 10 data test(Ilk 10 datatest deki tahminlerimiz).","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.306239Z","iopub.execute_input":"2021-07-16T22:32:11.306817Z","iopub.status.idle":"2021-07-16T22:32:11.317814Z","shell.execute_reply.started":"2021-07-16T22:32:11.30676Z","shell.execute_reply":"2021-07-16T22:32:11.316799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb.predict_proba(x_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.319099Z","iopub.execute_input":"2021-07-16T22:32:11.319423Z","iopub.status.idle":"2021-07-16T22:32:11.333637Z","shell.execute_reply.started":"2021-07-16T22:32:11.319395Z","shell.execute_reply":"2021-07-16T22:32:11.332761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# E) TUNING THE PREDICTION\n* WE can tune our prediction","metadata":{}},{"cell_type":"code","source":"y_probs = nb.predict_proba(x_test)[:,1]\ny_pred = [1 if i>0.60 else 0 for i in y_probs]\ny_pred[0:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.334827Z","iopub.execute_input":"2021-07-16T22:32:11.335149Z","iopub.status.idle":"2021-07-16T22:32:11.349071Z","shell.execute_reply.started":"2021-07-16T22:32:11.335121Z","shell.execute_reply":"2021-07-16T22:32:11.348118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_tuned_bestscore = accuracy_score(y_test,y_pred)\nnb_tuned_bestscore","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.350435Z","iopub.execute_input":"2021-07-16T22:32:11.351117Z","iopub.status.idle":"2021-07-16T22:32:11.359739Z","shell.execute_reply.started":"2021-07-16T22:32:11.351072Z","shell.execute_reply":"2021-07-16T22:32:11.359023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmnb_best = confusion_matrix(y_test,y_pred) \ncmnb_best","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.360926Z","iopub.execute_input":"2021-07-16T22:32:11.361412Z","iopub.status.idle":"2021-07-16T22:32:11.378124Z","shell.execute_reply.started":"2021-07-16T22:32:11.361374Z","shell.execute_reply":"2021-07-16T22:32:11.37711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation (10 katli ) yaparsak\nnb_finalscore=cross_val_score(nb_model, x_test, y_test, cv = 10).mean()\nnb_finalscore","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.379396Z","iopub.execute_input":"2021-07-16T22:32:11.379755Z","iopub.status.idle":"2021-07-16T22:32:11.451688Z","shell.execute_reply.started":"2021-07-16T22:32:11.379725Z","shell.execute_reply":"2021-07-16T22:32:11.450812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) KNN METHOD\n* In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression.\n* A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n* In this method we need to choose k value.It means that we chose k number of points of classes which are nearest to the out test point. We can call this small data set. We count the number of classes in the small dataset and determine the highest number of class. Finally we can say our test point belongs to the class.\n\n* While choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting.\n\n* Coding is the same for all supervised classes and we jus need to change the last part of the code.\n\n* K=1 SECERSEK OVERFITTING OLABILIR, K= BUYUK SECERSEK UNDERFITTING OLABILIR","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model = knn_model.fit(x_train, y_train)\n?knn_model","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.452859Z","iopub.execute_input":"2021-07-16T22:32:11.453136Z","iopub.status.idle":"2021-07-16T22:32:11.497015Z","shell.execute_reply.started":"2021-07-16T22:32:11.45311Z","shell.execute_reply":"2021-07-16T22:32:11.495771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A) Prediction of KNN","metadata":{}},{"cell_type":"code","source":"y_pred = knn_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:11.498283Z","iopub.execute_input":"2021-07-16T22:32:11.498595Z","iopub.status.idle":"2021-07-16T22:32:12.159928Z","shell.execute_reply.started":"2021-07-16T22:32:11.498565Z","shell.execute_reply":"2021-07-16T22:32:12.158813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B) Accuracy score of KNN","metadata":{}},{"cell_type":"code","source":"KNN = accuracy_score(y_test, y_pred)\nKNN","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:12.161177Z","iopub.execute_input":"2021-07-16T22:32:12.161487Z","iopub.status.idle":"2021-07-16T22:32:12.168266Z","shell.execute_reply.started":"2021-07-16T22:32:12.161456Z","shell.execute_reply":"2021-07-16T22:32:12.167333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:12.169724Z","iopub.execute_input":"2021-07-16T22:32:12.170057Z","iopub.status.idle":"2021-07-16T22:32:12.184904Z","shell.execute_reply.started":"2021-07-16T22:32:12.170015Z","shell.execute_reply":"2021-07-16T22:32:12.183915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# C) Model Tuning of KNN","metadata":{}},{"cell_type":"code","source":"knn_params = {\"n_neighbors\": np.arange(1,50)}","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:12.186112Z","iopub.execute_input":"2021-07-16T22:32:12.186385Z","iopub.status.idle":"2021-07-16T22:32:12.192617Z","shell.execute_reply.started":"2021-07-16T22:32:12.186358Z","shell.execute_reply":"2021-07-16T22:32:12.191458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:32:12.193953Z","iopub.execute_input":"2021-07-16T22:32:12.194289Z","iopub.status.idle":"2021-07-16T22:34:14.212565Z","shell.execute_reply.started":"2021-07-16T22:32:12.194258Z","shell.execute_reply":"2021-07-16T22:34:14.211609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The best score:\" + str(knn_cv.best_score_))\nprint(\"The best parameters: \" + str(knn_cv.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:14.213759Z","iopub.execute_input":"2021-07-16T22:34:14.214027Z","iopub.status.idle":"2021-07-16T22:34:14.219221Z","shell.execute_reply.started":"2021-07-16T22:34:14.214001Z","shell.execute_reply":"2021-07-16T22:34:14.218321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(4)  # We choose 4 neigboors. Because we get better result than 1, 2 and 3. \nknn_tuned = knn.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:14.220361Z","iopub.execute_input":"2021-07-16T22:34:14.220636Z","iopub.status.idle":"2021-07-16T22:34:14.267339Z","shell.execute_reply.started":"2021-07-16T22:34:14.220609Z","shell.execute_reply":"2021-07-16T22:34:14.266305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_finalscore = knn_tuned.score(x_test, y_test)\nknn_finalscore","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:14.268662Z","iopub.execute_input":"2021-07-16T22:34:14.26898Z","iopub.status.idle":"2021-07-16T22:34:14.872697Z","shell.execute_reply.started":"2021-07-16T22:34:14.268948Z","shell.execute_reply":"2021-07-16T22:34:14.871772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) SVM(SUPPORT VECTOR MACHINES)\n* Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.\n* SVM is used fo both regression and classification problems, but generally for classification. There is a C parameter inside the SVM algoritma and the default value of C parameter is 1. If C is small, it causes the misclassification. If C is big, it causes ovetfitting. So we need to try C parameter to find best value.\n* SVM, hem regresyon hem de sınıflandırma problemleri için kullanılır, ancak genellikle sınıflandırma için kullanılır. SVM içerisinde C parametresi vardır ve C parametresinin default değeri 1'dir. C'nin küçük olması yanlış sınıflandırmaya neden olur. C büyükse overfitting e neden olur. Bu yüzden en iyi değeri bulmak için C parametresini denememiz gerekiyor.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_model = SVC().fit(x_train,y_train)#we choose default c:1,kernel:'rbf',dagree:3...\n?svm_model","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:14.875719Z","iopub.execute_input":"2021-07-16T22:34:14.876048Z","iopub.status.idle":"2021-07-16T22:34:16.775501Z","shell.execute_reply.started":"2021-07-16T22:34:14.876005Z","shell.execute_reply":"2021-07-16T22:34:16.774318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A) Prediction of SVC","metadata":{}},{"cell_type":"code","source":"y_pred = svm_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:16.776712Z","iopub.execute_input":"2021-07-16T22:34:16.776996Z","iopub.status.idle":"2021-07-16T22:34:17.223155Z","shell.execute_reply.started":"2021-07-16T22:34:16.77697Z","shell.execute_reply":"2021-07-16T22:34:17.222219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B) Accuracy score of SVM","metadata":{}},{"cell_type":"code","source":"SVC = accuracy_score(y_test,y_pred)\nSVC","metadata":{"execution":{"iopub.status.busy":"2021-07-16T22:34:17.224479Z","iopub.execute_input":"2021-07-16T22:34:17.224777Z","iopub.status.idle":"2021-07-16T22:34:17.231959Z","shell.execute_reply.started":"2021-07-16T22:34:17.224749Z","shell.execute_reply":"2021-07-16T22:34:17.230599Z"},"trusted":true},"execution_count":null,"outputs":[]}]}