{"cells":[{"metadata":{"_cell_guid":"811e9767-7869-4268-9e5d-faec65daf3c9","_uuid":"20794fc0cc59377d7e2fafe3326fdae10d20decb"},"cell_type":"markdown","source":"This notebook follows the Kaggle Data Cleaning Challenges; the code and tasks are taken and inspired from the corresponding daily challenges."},{"metadata":{"_cell_guid":"90c2ef2d-b508-4448-8fbb-49fdffa51446","_uuid":"7fdc0f8f35b746c27544844f890fbd908944ebc6"},"cell_type":"markdown","source":"## Day 1 : Handling Missing Values"},{"metadata":{"_cell_guid":"2ec7d39a-71fd-464d-9c7f-65b7a7d18c86","_uuid":"5bb9ada6c2dcba5d5aa1969fc5db8c099d1f8cf7"},"cell_type":"markdown","source":"See challenge [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values?utm_medium=email&utm_source=mailchimp&utm_campaign=5DDC-data-cleaning). I will make use of the San Francisco Building Permits dataset, which can be found [here](https://www.kaggle.com/aparnashastry/building-permit-applications-data/data).  This dataset contains various information about structural building permits in SF from 1 January 2013 until 25 February 2018. "},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\", low_memory=False)\nnp.random.seed(0)","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"sf_permits.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dfefa202-8fb2-4466-a178-e76d25f4971a","_uuid":"5bc030c469b751576bb12b28c4c48fa329a01400"},"cell_type":"markdown","source":"I will now look at the number of missing values. Since the dataset contains a quite large number of columns, I will only look at the number of missing values for the first 10 columns:"},{"metadata":{"collapsed":true,"_cell_guid":"5ac2daa8-f181-4e71-b7eb-db49179c2af0","_uuid":"73671fc57230a898b5b8fc412a4d38bf4b126390","trusted":false},"cell_type":"code","source":"sf_permits.isnull().sum()[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f041587c-d50e-4890-bbdd-dcaf19599d80","_uuid":"657186f066affac51c7420f430bd1cbf49f9a022"},"cell_type":"markdown","source":"We can also compute the percentage of missing values.. I will do this by computing the total number of missing values divided by the total number of values:"},{"metadata":{"collapsed":true,"_cell_guid":"0f25f67f-91b7-4fd2-ae01-b39b50277f56","_uuid":"a9eeaacb2759d1722a463d5ea1e7e4767a5a55ee","trusted":false},"cell_type":"code","source":"sf_permits_prop_missing = (sf_permits.isnull().sum().sum()/np.product(sf_permits.shape))*100\nprint('Percentage of missing values in sf_permits: {}'.format(sf_permits_prop_missing))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81a54a81-a7d0-46c4-8413-b5e41ecf2930","_uuid":"3ce5be7c6665827b8845c8f2d91a1a1966c365c7"},"cell_type":"markdown","source":"As noted in the referenced notebook, some values can be missing because they don't exist and other can be missing because they were not recorded. For example, $\\texttt{Street Number Suffix}$ (e.g. 1A Smith St.) column has over 98% missing values, and most likely because most adresses don't have suffixes; every address has a zipcode, hence the column $\\texttt{Zipcode}$ has missing values because they were not recorded."},{"metadata":{"collapsed":true,"_cell_guid":"db1e1517-6f41-4fc4-998b-f2aac0b2825a","_uuid":"bb3116fa90348ac6021fb1900984cf75b35db9a3","trusted":false},"cell_type":"code","source":"print('Number of columns with no missing values: {}'.format(sf_permits.dropna(axis=1, inplace=False).shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d63bc76-ca9c-4095-9906-74a6b3df0724","_uuid":"c4d51ddcdc092fd587b00f83f91ed5869e7f2c4a"},"cell_type":"markdown","source":"Consider a sample. I will fill in the missing values with the whatever values come next in the corresponding columns:"},{"metadata":{"collapsed":true,"_cell_guid":"acb508ff-207b-4b72-a4f2-6844602bb613","_uuid":"92c25aa4e9a6c211d8da0e6f72a15fd9d06e9802","trusted":false},"cell_type":"code","source":"sf_permits_sample = sf_permits.sample(n=7)\nsf_permits_sample.fillna(method='bfill', axis=0).fillna(0)  #fill remaining missing values with 0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22573051-659e-45fd-af21-d15c7837cddb","_uuid":"48b330bbdee5953678b346227d61063791179c44"},"cell_type":"markdown","source":"## Day 2 : Scaling and Normalization"},{"metadata":{"collapsed":true,"_cell_guid":"ef35ed17-87c3-45ab-8bbd-fd9a26b45073","_uuid":"fc74a89c5279fb77ea49fc33f551fa37de2d83c3"},"cell_type":"markdown","source":"See challenge [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data). In this section I will be looking at the Kickstarter dataset:"},{"metadata":{"collapsed":true,"_cell_guid":"d923e850-e5ac-4338-adac-cf09c2e470af","_uuid":"fc9b77232a3864265784e47be66e5d47d932f1cc","trusted":false},"cell_type":"code","source":"ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv')\nks.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52632e92-798e-4096-ba11-d01a7f9459bd","_uuid":"96097b22d542a58a6b52b3811cdfcfc138a4d680"},"cell_type":"markdown","source":"I will now scale the $\\texttt{goal}$ columns as follows:"},{"metadata":{"_cell_guid":"cc7cfe63-6bbb-4893-a614-d1b00238114c","_uuid":"bbc022514d0638481820b291e4a5bfe931cf6f24"},"cell_type":"markdown","source":"$$ x \\mapsto \\frac{x-x_{min}}{x_{max}-x_{min}}$$"},{"metadata":{"collapsed":true,"_cell_guid":"31bfa591-cb63-4f38-8cf1-9b993fee21e9","_uuid":"b18ab992a110dc7aa53fdfd4a38617752b4399d1","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ngoal_original = ks['goal']\ngoal_scaled = (goal_original - goal_original.min())/(goal_original.max() - goal_original.min())\nfig, ax = plt.subplots(1,2, figsize=(12,5))\nax[0].hist(goal_original, ec='black')\nax[0].set_title('Original Data')\nax[1].hist(goal_scaled, ec='black')\nax[1].set_title('Scaled Data')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7d185c3a-116c-4ec0-9884-da45db743ca7","_uuid":"b6952923ec7f9c4173061bed9e7fc0f30f8dcb2b"},"cell_type":"markdown","source":"I will now normalize the $\\texttt{pledged}$ columns using the Box-Cox power transformation. I will use the parameter $\\lambda$ which maximizes the log-likelihood function:"},{"metadata":{"_cell_guid":"fbe45a8c-638b-4cc4-9b49-d827c746e92b","_uuid":"30652267b694a95da0ff904db3180a6f5b38c9ec"},"cell_type":"markdown","source":"$$ y_{i}^{(\\lambda)} = \n\\begin{cases}\n\\frac{y_{i}^{\\lambda}-1}{\\lambda}, & \\text{if}\\ \\lambda \\neq 0 \\\\\n\\text{ln}(y_{i}), & \\text{otherwise}\n\\end{cases}$$"},{"metadata":{"_cell_guid":"d2705896-0886-4a3a-8918-8140b3ee521f","_uuid":"9d4cb43e906f59e6dbe886e03345b43252a78bf9"},"cell_type":"markdown","source":"for $y_{i}$ > 0 for all $i$."},{"metadata":{"collapsed":true,"_cell_guid":"75257837-c3b1-4ae1-991a-819870e5dcaa","_uuid":"082f46b2b83b89e7b0a1081507e3bf0b2ad7a67e","trusted":false},"cell_type":"code","source":"from scipy.stats import boxcox\nimport seaborn as sns\n\nmsk = ks.pledged>0\npositive_pledges = ks[msk].pledged\nnormalized_pledges = boxcox(x=positive_pledges)[0]\nfig, ax = plt.subplots(1,2, figsize=(12,5))\nsns.distplot(a=positive_pledges, hist=True, kde=True, ax=ax[0])\nax[0].set_title('Original Positive Pledges')\nsns.distplot(a=normalized_pledges, hist=True, kde=True, ax=ax[1])\nax[1].set_title('Normalized Positive Pledges')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"324f3e2d-dc68-4b20-a856-77670fe34320","_uuid":"94bfbdade8bb0f2d09e52571c2e2a4cd12014856"},"cell_type":"markdown","source":"## Day 3 : Parsing Dates"},{"metadata":{"_cell_guid":"8843829d-fbdd-48c3-b8a3-e6a0b3b043d5","_uuid":"21ddab0e6fbb86d64b02411dcb5f192393a5cd38"},"cell_type":"markdown","source":"See challenge [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/notebook)"},{"metadata":{"collapsed":true,"_cell_guid":"46d04254-6626-4fb5-b820-b6efbbcfc2f9","_uuid":"a03fdf84da31f9ac4508988654ccb15fc153c7cc","trusted":false},"cell_type":"code","source":"quakes = pd.read_csv('../input/earthquake-database/database.csv')\n\n# Check type of date column\nquakes['Date'].dtype","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b48e9ed-7df7-40b4-9e81-c21857546173","_uuid":"220a16145112695dbcd4936629eb63914fceccd2"},"cell_type":"markdown","source":"This means the type of the dates columns is \"object\", hence Python does not know these numbers represent dates. Therefore, convert the entries to $\\texttt{datetime64}$."},{"metadata":{"collapsed":true,"_cell_guid":"28f4872b-2924-4ce1-9996-90b24954d1e5","_uuid":"0403c87982b689dfb5029c46e4ffd9b70c35a534","trusted":false},"cell_type":"code","source":"quakes.Date.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"195e93f6-d4f5-4ac9-a605-8f5f8d21436e","_uuid":"54189db8e2c360ada13fe61deed0cf6f2c579d4c"},"cell_type":"markdown","source":"I will use $\\texttt{infer_datetime_format=True}$ since not all date values are consistent, for example:"},{"metadata":{"collapsed":true,"_cell_guid":"19d57dee-c88f-40bd-8091-65134c8be4ec","_uuid":"60331d3ccf7cbdedeedac779349cd25c0df73a46","trusted":false},"cell_type":"code","source":"quakes.loc[3378,'Date']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"06a9c016-7b2b-4d11-ab1d-4343244e72d6","_uuid":"9809c5b1c30f5d1948e9a79e0d35b534d1b05413","trusted":false},"cell_type":"code","source":"quakes['date_parsed'] = pd.to_datetime(quakes.Date, infer_datetime_format=True) \nquakes.date_parsed.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1f7186b8-9f5e-4d17-a9e7-c0f4b6be2f7e","_uuid":"3f85120628251015ad72ba553a0b791722c0f718","trusted":false},"cell_type":"code","source":"day_of_month = quakes['date_parsed'].dt.day\n\n# Plot the days\nplt.hist(day_of_month, bins=31, ec='black')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34d451b6-c815-423a-a7d6-955183872f6f","_uuid":"ae39a97a9921773899fff2003d6385c2113269a6"},"cell_type":"markdown","source":"## Day 4 : Character Encodings"},{"metadata":{"_cell_guid":"4ade2291-d493-4d51-8f6a-be5bda03579a","_uuid":"9461b5a33280e3033f3eeddbe157e2b5ae75f994"},"cell_type":"markdown","source":"See challenge [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/). As described there, data can be lost if we use the wrong encoding from string to bytes. As an example, consider a Python string, with UTF-8 encoding:"},{"metadata":{"collapsed":true,"_cell_guid":"945db3d5-1651-47a1-bf01-487497493f7f","_uuid":"8e90cb96ddb99e60b6e4cf9c1a5a7be1d92f08ef","trusted":false},"cell_type":"code","source":"before = \"This is an interesting text: 你好\"\n\nafter = before.encode(encoding='UTF-8', errors='replace')\nafter.decode('UTF-8') # No issue here","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8c456cfd-60b0-4b81-8346-f2254ad936c2","_uuid":"5b3b7175a9f6b958ce4cd00d25f727387a8793e2","trusted":false},"cell_type":"code","source":"after = before.encode(encoding='ASCII', errors='replace')\nafter.decode('ASCII') # Lose information","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"727eb45d-b703-495a-a682-a24f31b6b674","_uuid":"78b631c288f4c82a8172aeaa68bfa5eacc425c43"},"cell_type":"markdown","source":"I will now use the $\\texttt{chardet}$ module to detect the encoding of the Police Killings dataset, as it is not UTF-8 encoded:"},{"metadata":{"_cell_guid":"be283434-1208-4b3f-bc95-4b75c0ec8e96","_uuid":"e7b16ee776925136e9c45dbcc88965042a691fc6","trusted":true},"cell_type":"code","source":"import chardet\n\nwith open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))  # Read first 10000 bytes.\nresult","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"5fda7d64-41ff-48f0-8f68-8f2407aaf7a1","_uuid":"ad45d8a2b9869f9bb6a1dd64eb97d67843094091","trusted":false,"collapsed":true},"cell_type":"code","source":"# See if this is the right encoding\npolice_killings = pd.read_csv('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', encoding='ascii')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22197ebd-8beb-4e2a-a37f-624a0ea47f9b","_uuid":"4ea1ab542ea41b855ebe86c3e589cb8d8791e5e3"},"cell_type":"markdown","source":"Does not work. Read the first 100 thousand bytes:"},{"metadata":{"_cell_guid":"e6490a94-0d53-43fe-a499-32b322c6f026","_uuid":"f1c5a7ff651ea3fe243788038100e87d2d6462b0","trusted":false,"collapsed":true},"cell_type":"code","source":"with open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nresult","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96fae590-146a-4f52-b842-cd60b5e5909c","_uuid":"f5e23e7ebd6faed132d400258c962f4614a2c167","collapsed":true,"trusted":false},"cell_type":"code","source":"police_killings = pd.read_csv('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', encoding='Windows-1252')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d47d38fc-f3fd-448f-89d3-280591d954d0","_uuid":"0fcb3c31f77f05b32a7b2ea87cc34cdc2e78a7e4"},"cell_type":"markdown","source":"Now it works!"},{"metadata":{"_cell_guid":"420ab3ab-9816-4e96-95e2-d5e55ca026b4","_uuid":"4dd9a43b8ebc50eb5bf5a9d30d5aeb78bd61f089"},"cell_type":"markdown","source":"## Day 5 : Inconsistent Data Entries"},{"metadata":{"_cell_guid":"8493e8dc-e6ba-4ab7-bbf6-3b62f97bcaee","_uuid":"45a9fa717ac48ff0ae7ba18ff3c74fd696964fb9","collapsed":true,"trusted":false},"cell_type":"markdown","source":"![](http://)See challenge [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/).  In this part I'm going to do some text pre-processing on the $\\texttt{PakistanSuicideAttacks Ver 11 (30-November-2017).csv}$ dataset (more information [here](https://www.kaggle.com/zusmani/pakistansuicideattacks)). As noted in the day 5 notebook, the dataset is $\\texttt{Windows-1252}$ encoded. I will start by looking at inconsistencies in the City and Province columns:"},{"metadata":{"trusted":true,"_uuid":"6ae82db85f0a7bb8380ffec74aabdca403e824a8"},"cell_type":"code","source":"suicide_attacks = pd.read_csv('../input/pakistansuicideattacks/PakistanSuicideAttacks Ver 11 (30-November-2017).csv', encoding='Windows-1252')\ncities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"23447c365e69cbfe628eca14d6b5722a6857318b"},"cell_type":"markdown","source":"We can see inconsistencies in the city names, e.g. 'karachi' & 'karachi ', 'South Waziristan' & 'South waziristan'. As suggested in the notebook, convert all names to lower case and strip all white spaces:"},{"metadata":{"trusted":true,"_uuid":"3242af31c19dd938310c72761b1bd60fe931d2c1"},"cell_type":"code","source":"suicide_attacks['City'] = suicide_attacks['City'].str.lower()\nsuicide_attacks['City'] = suicide_attacks['City'].str.strip()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"ab4e55047f781ebf2ed7935c74caddd12c522d87"},"cell_type":"markdown","source":"I will do the same thing for the Province column:"},{"metadata":{"trusted":true,"_uuid":"4c712542238aa866f4067c442206131785e491df"},"cell_type":"code","source":"provinces = suicide_attacks['Province'].unique()\nprovinces.sort()\nprovinces","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c5774276c49c327205d62559c241ccf31fbc9d2"},"cell_type":"code","source":"suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()\nsuicide_attacks['Province'] = suicide_attacks['Province'].str.strip()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"34828ede6d4b62ba036b58e1590f9df33c47cc29"},"cell_type":"markdown","source":"There are still text inconsistencies in the City column, e.g. 'd.i khan ' & 'd. i khan', 'kuram agency' & 'kurram agency'.  I will use the [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) module to search for similar strings. As described in the notebook:"},{"metadata":{"_uuid":"85e4d55d9cfc5b4e90a97d9549a5fead8b85cfc0"},"cell_type":"markdown","source":">$\\textbf{Fuzzy matching}$: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n\n>Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"d.i khan\"."},{"metadata":{"_uuid":"0ceda817f00e583913d9285fe8bc654430c9f8e9"},"cell_type":"markdown","source":"Below is a function defined in the day 5 notebook. It takes an input string and replaces all row entries with fuzzy matching ratio of >90 (by default). "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bec76adac0feee2d63a4c7ab9cbc56a4a6620aa7"},"cell_type":"code","source":"import fuzzywuzzy\nfrom fuzzywuzzy import process\n\n# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"370f3a8017dc3c9aa61ee7149d4e7794c9b0ba83"},"cell_type":"markdown","source":"I will apply this function to the City column to match entries similar to 'd. i khan' and, separately, to 'kuram agency'. "},{"metadata":{"trusted":true,"_uuid":"bef307daf6f530a13e8755bd8334ae39e4592ac6"},"cell_type":"code","source":"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f2a6c3cd6ccbde7cfb31eac15a7bf1b309b388c"},"cell_type":"code","source":"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24766e8f8dce288840ce22d6b53c4e0665a67994"},"cell_type":"code","source":"cities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":22,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}