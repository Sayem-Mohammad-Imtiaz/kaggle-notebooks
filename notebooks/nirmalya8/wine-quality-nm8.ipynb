{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will import the data and look at the first few rows and some information about the data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, first we will try to gain some insights from the data. \nHere are few of the insights I gained from this dataset - \n1. There are 1599 data points across 12 different columns.\n2. There are no missing values. \n3. The column we have to predict, 'quality' has minimum value 3 and maximum value 8. \nWe will find out more information about the data with Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS"},{"metadata":{},"cell_type":"markdown","source":"Exploratory Data Analysis on this dataset becomes a bit difficult because all columns contain continuous data except our target column, which is quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Every Variable against each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (16,10))\nsns.heatmap(data.corr(),cmap = 'Dark2',annot = True,linewidths=1.0,linecolor='black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()['quality'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(data.corr()['quality']).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at the heatmap of the correlations closely, we observe :-\n1. alcohol has the highest correlation with quality.\n2. volatile acidity has the highest negative correlation with quality.\n3. sulphates, citric acid are the next highly correlated columns with quality."},{"metadata":{},"cell_type":"markdown","source":"First, we will look at the column we have to predict, which is quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As said before, the quality column consists of classes from 3 to 8. Most of the wine is of quality 5 and 6. "},{"metadata":{},"cell_type":"markdown","source":"Now, we will look at the fixed acidity column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist((data['fixed acidity']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('quality','fixed acidity',data=data) #you can try sin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"fixed acidity\", y=\"quality\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram, the boxplot and the difference between the mean and median, we can say that this data is slightly skewed. \nNow, looking at the above regression plot, we can infer that there is a slight trend, which isn't very clear but, as the fixed acidity increases, the quality slightly increases."},{"metadata":{},"cell_type":"markdown","source":"Now we will look at volatile acidity"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['volatile acidity'].hist(bins = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('quality','volatile acidity',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"volatile acidity\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"volatile acidity\", y=\"quality\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean and the median are quite close. The boxplot shows the presence of some outliers, but the the statistical summary might allow those outliers to be present . Here, the trend between quality and volatile acidity is ,much more pronounced. And, this fact is backed up by the number -0.390558 , which is the correlation between the columns. They are negatively correlated but there is some relation between them. To be precise, as the volatile acidity increases, quality seems to decrease.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['citric acid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('quality','citric acid',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['citric acid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"citric acid\", y=\"quality\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean and the median are quite close. So,there aren't really outliers which could affect the predictions. There is, again, an upward trend but the trend isn't very clear"},{"metadata":{},"cell_type":"markdown","source":"Now, we will look at the alcohol column. It has the highest correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['alcohol'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('quality','alcohol',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"alcohol\", y=\"quality\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(data, col='quality')\ng.map(plt.hist, 'alcohol', bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the above regression plot, we can infer that there is a clear trend,as the alcohol increases, the quality increases."},{"metadata":{},"cell_type":"markdown","source":"We will look at the sulphates column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['sulphates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('quality','sulphates',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"sulphates\", y=\"quality\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, there is a slight trend for sulphates and quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(data, col='quality')\ng.map(plt.hist, 'sulphates', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Looking at the correlations, we can say that all the colummns with higher correlation. So, we can do more Exploratory Data Analysis, but it would not be of much importance. So, next we will move forward to modelling."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# MODEL CREATION"},{"metadata":{},"cell_type":"markdown","source":"First we will import all the necessary libraries and functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression   \nfrom sklearn.model_selection import KFold \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code is a classification model creator, which returns a trained model and its accuracy and cross validation score."},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_model(model, data, predictors, outcome):  \n    #Fit the model:  \n    model.fit(data[predictors],data[outcome])    \n    #Make predictions on training set:  \n    predictions = model.predict(data[predictors])    \n    #Print accuracy  \n    accuracy = metrics.accuracy_score(predictions,data[outcome])  \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n    #Perform k-fold cross-validation with 5 folds  \n    kf = KFold(5,shuffle=True)  \n    error = []  \n    for train, test in kf.split(data):\n        # Filter training data    \n        train_predictors = (data[predictors].iloc[train,:])        \n        # The target we're using to train the algorithm.    \n        train_target = data[outcome].iloc[train]        \n        # Training the algorithm using the predictors and target.    \n        model.fit(train_predictors, train_target)\n        #Record error from each cross-validation run    \n        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n     \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error))) \n    # %s is placeholder for data from format, next % is used to conert it into percentage\n    #.3% is no. of decimals\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"classification_model2 is a function, which is a slightly tweaked version of the above classification_model function. It basically trains on a splitted dataset, and then tests on test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_model2(model, x_train,p,y_train ):#, outcome):  \n    #Fit the model:  \n    model.fit(x_train[p],y_train)    \n    #Make predictions on training set:  \n    predictions = model.predict(x_train[p])    \n    #Print accuracy  \n    accuracy = metrics.accuracy_score(predictions,y_train)  \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n    #Perform k-fold cross-validation with 5 folds  \n    kf = KFold(5,shuffle=True)  \n    error = []  \n    for train, test in kf.split(x_train):\n        # Filter training data    \n        train_predictors = (x_train[p].iloc[train,:])        \n        # The target we're using to train the algorithm.    \n        train_target = y_train.iloc[train]        \n        # Training the algorithm using the predictors and target.    \n        model.fit(train_predictors, train_target)\n        #Record error from each cross-validation run    \n        error.append(model.score(x_train[p].iloc[test,:], y_train.iloc[test]))\n     \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error))) \n    # %s is placeholder for data from format, next % is used to conert it into percentage\n    #.3% is no. of decimals\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = (data.iloc[:,0:11])\ny = (data['quality'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we will train Logistic Regression models."},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates','citric acid','residual sugar','pH']\nlr = LogisticRegression(max_iter=10000,fit_intercept=False, C=10000)\nprint(\"Logistic Regression(1)\")\nlr = classification_model(lr,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates','citric acid','residual sugar','pH']\nlr2 = LogisticRegression(max_iter=10000,fit_intercept=False, C=10000)\nX_train[predict]\nprint(\"Logistic Regression with Train Test Split\")\nlr = classification_model2(lr2,X_train,predict,y_train)\npredictions = lr2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is a K-Nearest Neighbors Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity']#,'citric acid','sulphates']#,'sulphates']#volatile acidity, sulphates\nknn = KNeighborsClassifier(weights='distance', n_neighbors=200)\nprint(\"K-Nearest Neighbors(1)\")\nknn = classification_model(knn,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))\n# 68.6 , 54.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity']#,'sulphates','citric acid','residual sugar','pH']\nknn2 = KNeighborsClassifier(weights='distance', n_neighbors=150)\nprint(\"KNN with Train Test Split\")\nknn2 = classification_model2(knn2,X_train,predict,y_train)\npredictions = knn2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next up, we will create a Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['volatile acidity','sulphates', 'alcohol']#,'citric acid'] #['alcohol',\ndtree = DecisionTreeClassifier(random_state=40,max_depth=20,max_leaf_nodes=100)#,max_features='sqrt')#random_state=40,max_depth=20,max_features='sqrt',max_leaf_nodes=700)\nprint(\"DecisionTree\")\ndtree = classification_model(dtree,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))\n#99,54","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','volatile acidity','sulphates']#'citric acid','residual sugar','pH']\ndtree2 = DecisionTreeClassifier(random_state=40)#,max_features='sqrt')\nprint(\"Decision Tree 2.\")\ndtree2 = classification_model2(dtree2,X_train,predict,y_train)\npredictions = dtree2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also train a random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['volatile acidity','alcohol','sulphates'] #['alcohol',\nrf = RandomForestClassifier(n_estimators=10000,max_depth=5,bootstrap=False)\nprint(\"Random Forest\")\ndtree = classification_model(dtree,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\noutput = 'quality'\npredict = ['alcohol','sulphates','citric acid', 'volatile acidity']\nnb= GaussianNB()\nprint(\"Naive Bayes\")\nnb = classification_model(nb,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will try some ensemble learning models."},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nestimators = [('lr',lr),('knn',knn),('tree',dtree)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nsoft_vote = VotingClassifier(estimators=estimators , voting= 'soft')\nprint(\"soft voting \")\nsoft_vote=classification_model(soft_vote,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\npredict=[\"alcohol\",\"sulphates\", \"volatile acidity\"]\nestimators = [('lr',lr),('tree',dtree),('knn',knn)]#,('knn2',knn2)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nsoft_vote2 = VotingClassifier(estimators=estimators , voting= 'soft')\nprint(\"Soft Vote using Train Test Split\")\nsoft_vote2 = classification_model2(soft_vote2,X_train,predict,y_train)\npredictions = soft_vote2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nprint(\"Hard Voting\")\nhard_vote = VotingClassifier(estimators=estimators , voting= 'hard')\nhard_vote = classification_model(hard_vote,data,predict,output)\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nestimators = [('lr',lr),('tree',dtree),('knn',knn)]#,('tree',dtree2)]#,('rf',rf)] #,('support',svc)('nb',nb)('tree',dtree),\nhard_vote2 = VotingClassifier(estimators=estimators , voting= 'hard')\nprint(\"Hard Voting using Train Test Split\")\nhard_vote2 = classification_model2(hard_vote2,X_train,predict,y_train)\npredictions = hard_vote2.predict(X_test[predict])    \n    #Print accuracy  \naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nprint(\"Stacking using KNN as Meta\")\nmeta = KNeighborsClassifier(weights='distance', n_neighbors=100)\nstack = StackingClassifier(estimators = [('lr2',lr2),('knn',knn2),('tree',dtree2),('hard2',hard_vote2)])#('rf',rf)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model2(stack,X_train,predict,y_train) #('hard',hard_vote),\np = stack.predict(X_test[predict])\naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on test data = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nmeta = LogisticRegression()#max_iter=10000,fit_intercept=False, C=10000)\nstack = StackingClassifier(estimators = [('knn',knn),('tree',dtree),('soft',soft_vote),('rf',rf)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model(stack,data,predict,output)#('hard',hard_vote),,\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=time.time()\nprint(\"Using Logistic Regression as meta\")\nmeta = LogisticRegression(max_iter = 1000)#max_iter=10000,fit_intercept=False, C=10000)\nstack = StackingClassifier(estimators = [('knn2',knn2),('tree',dtree),('soft2',soft_vote2),('hard2',hard_vote2)],final_estimator=meta) #('hard',hard_vote),\nstack = classification_model2(stack,X_train,predict,y_train) #('hard',hard_vote),\np = stack.predict(X_test[predict])\naccuracy = metrics.accuracy_score(predictions,y_test)\nprint(\"Accuracy on Test Set = {}\".format(accuracy))\nprint(\"Time = {}\".format(time.time()-s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COMMENTS"},{"metadata":{},"cell_type":"markdown","source":"In all the models we have trained and tested, we have seen that many of the models tend to overfit the dataset, hence they have  high accuracy but their cross validation score is low. \n\nWhen I tried to reduce this difference, the accuracy decreased. The test set accuracy didn't reach 0.7 . \n\nCan anyone suggest any other way to increase the cross validation score and the accuracy on the test set?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}