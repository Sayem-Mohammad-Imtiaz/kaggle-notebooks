{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam Detection using Multinomial Naive Bayes Model & Pipeline\n\n> - The `SMS Spam Collection` is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of `5,574 messages`, tagged acording being ham (legitimate) or spam.\n> - A collection of `425` SMS spam messages was manually extracted from the `Grumbletext Web site`. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. \n> - A subset of `3,375` SMS randomly chosen ham messages of the `NUS SMS Corpus (NSC)`, which is a dataset of about `10,000` legitimate messages collected for research at the `Department of Computer Science at the National University of Singapore`. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. \n> - A list of `450` SMS ham messages collected from `Caroline Tag's PhD Thesis`.\n> - Finally, we have incorporated the `SMS Spam Corpus v.0.1 Big`. It has 1,002 SMS ham messages and 322 spam messages.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>Table of Contents</h1></div><a class=\"anchor\" id=\"0.1\"></a>\n\n1. [Importing Libraries](#1)\n2. [Importing Dataset](#2)\n3. [Exploratory Data Analysis](#3)\n4. [Text Processing](#4)\n5. [Spliting Dataset to Train and Test](#5)\n6. [CountVectorizer](#6)\n7. [TF/IDF Vectorization](#7)\n8. [Multinomial Naive Bayes Model](#8)\n9. [Pipeline](#9)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>1. Importing Libraries</h1></div><a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Esentials\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"darkgrid\")\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Limiting floats output to 2 decimal points\npd.set_option('display.float_format', lambda x: '{:.2f}'.format(x)) \n\n# Text Analysis\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Modelling Library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\n\n\n#print(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>2. Importing Dataset</h1></div><a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sms = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin1')[['v1', 'v2']]\nsms.columns = ['Label','Message']\nprint('Dataset Dimension:', sms.shape)\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>3. Exploratory Data Analysis</h1></div><a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset grouped as per Label\nprint(sns.countplot(data=sms, x='Label'))\nplt.title('Spam/ham Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = pd.value_counts(sms[\"Label\"], sort= True)\ncount.plot(kind='pie', figsize=(15,5), autopct='%1.0f%%')\nplt.title('Spam/ham Distribution')\nplt.ylabel('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of the messages are calculated and plotted\nsms['Length'] = sms.Message.apply(len)\nsms.hist(column='Length',by='Label',bins=50, figsize=(15,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.groupby('Label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# longest message in the dataset\nsms[sms.Length == 910].Message.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of spam message\nsms[sms.Length == 157].Message.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation</h1></div> </h1></div><a class=\"anchor\"></a>\n\n> - There are `4825` ham messages and `747` spam messages\n> - `87%` of the messages are ham while `13%` are spam\n> - Most of the ham messages have message length of approx `100` while spam messages have around `130-150`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>4. Text Processing</h1></div><a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(text):\n    '''\n    What will be covered:\n    1. Remove punctuation\n    2. Remove stopwords\n    3. Return list of clean text words\n    '''\n    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', '16' ,'im', 'dont', 'doin', 'ure']\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in text if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n\n\nsms['clean_msg'] = sms.Message.apply(process_text)\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the process_text function:\nprocess_text('Hi. My name is Rhea Das, I am a Data Scientist. It\\'s amazing!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the most common words occuring in Ham\n\nham_count = Counter(\" \".join(sms[sms['Label'] == 'ham']['clean_msg']).split()).most_common(20)\nham_count = pd.DataFrame.from_dict(ham_count)\nham_count = ham_count.rename(columns={0: \"words in non-spam\", 1 : \"count\"})\n\nham_count.plot.bar(legend=False, figsize=(12,5),color = 'black')\ny_pos = np.arange(len(ham_count[\"words in non-spam\"]))\nplt.xticks(y_pos, ham_count[\"words in non-spam\"])\nplt.title('More frequent words in non-spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the most common words occuring in Spam\n\nspam_count = Counter(\" \".join(sms[sms['Label'] == 'spam']['clean_msg']).split()).most_common(20)\nspam_count = pd.DataFrame.from_dict(spam_count)\nspam_count = spam_count.rename(columns={0: \"words in spam\", 1 : \"count\"})\n\nspam_count.plot.bar(legend=False, figsize=(12,5),color = 'blue')\ny_pos1 = np.arange(len(spam_count[\"words in spam\"]))\nplt.xticks(y_pos1, spam_count[\"words in spam\"])\nplt.title('More frequent words in Spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation</h1></div> </h1></div><a class=\"anchor\"></a>\n\n> - We can see that the majority of frequent words in both classes are stop words such as `'to', 'a', 'or' and so on`. <br>\n> - With `stop words` we refer to the most common words in a language. <br>\n> - Removing the common words, we have visualized the top 20 most freq words in spam list as well as ham list","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>5. Spliting Dataset to Train and Test</h1></div><a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into X and Y\n\nX = sms['clean_msg']\nY = sms['Label'].replace({'ham':0,'spam':1})\nprint(\"X Dimension\", X.shape)\nprint(\"Y Dimension\", Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting X & Y into train and test\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=1)\nprint('X_train Dimension:', x_train.shape)\nprint('X_test Dimension:', x_test.shape)\nprint('Y_train Dimension:', y_train.shape)\nprint('Y_test Dimension:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-comment\">\n<h1>Word of the wise</h1></div> </h1></div><a class=\"anchor\"></a>\n\n- From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> - Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**. <br>\n\n-  We will use `CountVectorizer` to convert text into a matrix of token counts. In this scheme, features and samples are defined as follows:\n\n> - Each individual token occurrence frequency (normalized or not) is treated as a `feature`.\n> - The vector of all the token frequencies for a given document is considered a `multivariate sample`.\n\n> - A `corpus of documents` can thus be represented by a matrix with **one row per document** and **one column per token** occurring in the corpus.\n> - We call `vectorization` the general process of turning a collection of text documents into numerical feature vectors. <br>\n> - This specific strategy (tokenization, counting and normalization) is called the `Bag of Words` or `Bag of n-grams` representation. <br>\n> - Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\n### What we should do with this dataset - Vectorization\n\n> - Currently, we have the messages as lists of tokens (also known as [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n> - Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n\n> **We'll do that in three steps using the bag-of-words model:**\n1. Count how many times does a word occur in each message (Known as term frequency)\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\n**Summary:**\n\n1. `vect.fit(train)` **learns the vocabulary** of the training data\n\n2. `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n\n3. `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>6. CountVectorizacer</h1></div><a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" # Initiating Vector\nvect = CountVectorizer()\n\n# Fitting the training dataset\nvect.fit(x_train)\n\n# learn training data vocabulary, then use it to create a document-term matrix(dtm)\nx_train_dtm = vect.transform(x_train) \n\n# Combine fit and transform \nx_train_dtm = vect.fit_transform(x_train) \n\n# Transform test dataset into a document-term matrix(dtm)\nx_test_dtm = vect.transform(x_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>7. TF/IDF Vectorization</h1></div><a class=\"anchor\" id=\"7\"></a>\n\n> - `Term Frequency - Inverse Document Frequency` is a numerical statistics that is intended to reflect how important a word is to a document . It is used as a weighing factor in information retrieval and text mining. \n> - TF/IDF value increases proportionally to a no. of times a word appears in a document but is offset by the frequency of the word in corpus.\n<br><center>where, `TF-IDF(t)= Term Frequency (TF) * Inverse Document Frequency (IDF)`</center>\n\n> - `Term Frequency (TF)` is a measure of how frequent a term occurs in a document.\n<br><center>`TF(t)= Number of times term t appears in document (p) / Total number of terms in that document`</center>\n> - `Inverse Document Frequency (IDF)` is measure of how important term is. For TF, all terms are equally treated. But, in IDF, for words that occur frequently like 'is' 'the' 'of' are assigned less weight. While terms that occur rarely that can easily help identify class of input features will be weighted high.\n<br><center>`IDF(t)= log<sub><i>e</i></sub>(Total number of documents / Number of documents with term t in it)`</center>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" # Initiating Model\ntfidf_transformer = TfidfTransformer()\n\n# Fitting the training dataset\ntfidf_transformer.fit(x_train_dtm)\n\n# Transforming the test dataset\ntfidf_transformer.transform(x_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>8. Multinomial Naive Bayes Model</h1></div><a class=\"anchor\" id=\"8\"></a>\n\n> - The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). \n> - The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiating Model\nnb = MultinomialNB()\n\n# Train the model using X_train_dtm \nnb.fit(x_train_dtm, y_train)\n\n# Make class predictions for X_test_dtm\ny_pred_class = nb.predict(x_test_dtm)\n\n##  Calculating accuracy of the class predictions:\nprint('Accuracy of Multinomial Naive-Bayes Model:',round(metrics.accuracy_score(y_test, y_pred_class)*100,2))\n\n## Print confusion Metrics\nprint(\"\\nConfusion Metrics\\n\", metrics.confusion_matrix(y_test, y_pred_class))\n\n## calculation ROC/AUC\nprint('\\nROC/AUC:',round(metrics.roc_auc_score(y_test, y_pred_class)*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the false positive predictions - The messages which actually HAM but model is predicting SPAM (#7)\n\nx_test[y_pred_class > y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the false negetive predictions - The messages which actually SPAM but model is predicting HAM (#16)\n\nx_test[y_pred_class < y_test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation</h1></div> </h1></div><a class=\"anchor\"></a>\n\n> - The goal of the algorithm is to predict if a new sms is a HAM or SPAM and there are 2 possible situation:\n> -  `False Positive Prediction`: The messages which actually SPAM but model is predicting HAM \n<br><center> **OUTCOME: I probably do not read it!!** </center>  \n> -  `False Negative Prediction`: The messages which actually HAM but model is predicting SPAM\n<br><center> **OUTCOME: I delete it!!** </center>\n    \n> - The second option is preferable!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1>9. Pipeline</h1></div><a class=\"anchor\" id=\"9\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiating Model\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfid', TfidfTransformer()),  \n                 ('model', MultinomialNB())])\n\n# Fitting the training dataset\npipe.fit(x_train, y_train)\n\n# Predicting on test dataset\ny_pred_pipe = pipe.predict(x_test)\n\n##  Calculating accuracy of the class predictions:\nprint('Accuracy of Pipeline:',round(metrics.accuracy_score(y_test, y_pred_pipe)*100,2))\n\n## Print confusion Metrics\nprint(\"\\nConfusion Metrics\\n\", metrics.confusion_matrix(y_test, y_pred_pipe))\n\n## calculation ROC/AUC\nprint('\\nROC/AUC:',round(metrics.roc_auc_score(y_test, y_pred_pipe)*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_spam(s):\n    return pipe.predict([s])[0]\ndetect_spam('Hi, this is Rhea.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<h1>Observation</h1></div> </h1></div><a class=\"anchor\"></a>\n\n> Pipeline is predicting the spam messages with 96% accuracy with lowset False Positives.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"___","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}