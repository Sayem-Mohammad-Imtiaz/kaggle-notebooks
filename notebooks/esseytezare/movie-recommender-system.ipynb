{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. ML Project 2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.1 Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The objective of this project is to recommend users and movies","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.2 Agenda","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Data Set Selection\n2. EDA\n3. Demographic filtering\n4. Content-based recommenders\n5. Collaborative filtering \n6. AutoML","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.3 Team Members","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Eden Zere\n2. Essey Abraham Tezare\n3. Mario Arismendi Matos","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2 1. Data Set Selection And EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1 1.1 Import libraries","execution_count":null},{"metadata":{"trusted":true,"_uuid":"c1fdd129c1cbab68ae3e6bf2062575f01f80b87c"},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ast import literal_eval\n%matplotlib inline\n\nfrom datetime import datetime\nimport datetime\nimport wordcloud as wc\nimport plotly\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        saving=False\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 1.2 Reading the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_credits.csv')\ndf2=pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_movies.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"402a28d17c13bba3f2060d72c2ff75f5377a9f01"},"cell_type":"markdown","source":"The first dataset contains the following features:-\n\n* movie_id - A unique identifier for each movie.\n* cast - The name of lead and supporting actors.\n* crew - The name of Director, Editor, Composer, Writer etc.\n\nThe second dataset has the following features:- \n\n* budget - The budget in which the movie was made.\n* genre - The genre of the movie, Action, Comedy ,Thriller etc.\n* homepage - A link to the homepage of the movie.\n* id - This is infact the movie_id as in the first dataset.\n* keywords - The keywords or tags related to the movie.\n* original_language - The language in which the movie was made.\n* original_title - The title of the movie before translation or adaptation.\n* overview - A brief description of the movie.\n* popularity - A numeric quantity specifying the movie popularity.\n* production_companies - The production house of the movie.\n* production_countries - The country in which it was produced.\n* release_date - The date on which it was released.\n* revenue - The worldwide revenue generated by the movie.\n* runtime - The running time of the movie in minutes.\n* status - \"Released\" or \"Rumored\".\n* tagline - Movie's tagline.\n* title - Title of the movie.\n* vote_average -  average ratings the movie recieved.\n* vote_count - the count of votes recieved.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's join the two dataset on the 'id' column","execution_count":null},{"metadata":{"trusted":true,"_uuid":"c87bda9d56a936be126d03eda0bc743ee35be461"},"cell_type":"code","source":"df1=df1.rename({'movie_id': 'id'},axis=1)\ndf1.columns = ['id','title2','cast','crew']\ndf2= df2.merge(df1,on='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 1.3 Training Data Info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e88ed16f798ec3a094fce8aaf8a971f7b2aae83e"},"cell_type":"markdown","source":"Just a peak at our data.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"71d266ed92947c51acf07189d3b42379134ef6e7"},"cell_type":"code","source":"df2.drop(columns=['title2'],inplace=True)\ndf2.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 1.4 Checking for Null Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df2.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see here there are a lot of nullable values in the homepage and the tagline columns ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.3 1.5 Distict values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.select_dtypes('object').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 1.6  Checking Distribution ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,6))\n\n\nplt.subplot(2, 3, 1)\nsns.distplot(df2['revenue'])\n\nplt.subplot(2, 3, 2)\nsns.distplot(df2['vote_count'])\n\nplt.subplot(2, 3, 3)\nsns.distplot(df2['budget'])\n\nplt.subplot(2, 3, 4)\nsns.distplot(df2['vote_average'].fillna(0).astype(int))\n\nplt.subplot(2, 3, 5)\nsns.distplot(df2['runtime'].fillna(0).astype(int))\n\nplt.subplot(2, 3, 6)\nsns.distplot(df2['popularity'].fillna(0).astype(int))\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5 1.7 Revenue vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pop= df2.sort_values('revenue', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(6),pop['revenue'].head(6), align='center',\n        color='skyblue')\nplt.gca().invert_yaxis()\nplt.xlabel(\"revenue\")\nplt.title(\"revenue Movies\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as above we can tell that Avatar has the highest revenue","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.6 1.8 Popularity vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pop= df2.sort_values('popularity', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(6),pop['popularity'].head(6), align='center',\n        color='skyblue')\nplt.gca().invert_yaxis()\nplt.xlabel(\"Popularity\")\nplt.title(\"Popular Movies\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As above we can see that Minions is the most popular movie ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.7 1.9 Languages vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = df2\nmovies['spoken_languages'] = movies['spoken_languages'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n\n\ns = movies.apply(lambda x: pd.Series(x['spoken_languages']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'spoken_languages_count'\ncon_df = movies.drop('spoken_languages', axis=1).join(s)\ncon_df = pd.DataFrame(con_df['spoken_languages_count'].value_counts())\ncon_df['spoken_language'] = con_df.index\ncon_df.columns = ['num_spoken_language', 'spoken_language']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_df = con_df.reset_index().drop('index', axis=1)\ncon_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the spoken language the highest is English with 4485","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"con_df = con_df[:5]\n\nfig = plt.figure(figsize=(12,7))\nsns.barplot(data = con_df, x='spoken_language', y = 'num_spoken_language')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart is telling us the highest language is English","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.8 2.0 Countries Vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = df2\nmovies['production_countries'] = movies['production_countries'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n\ns = movies.apply(lambda x: pd.Series(x['production_countries']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'countries'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_df = movies.drop('production_countries', axis=1).join(s)\ncon_df = pd.DataFrame(con_df['countries'].value_counts())\ncon_df['country'] = con_df.index\ncon_df.columns = ['num_movies', 'country']\ncon_df = con_df.reset_index().drop('index', axis=1)\ncon_df.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table shows us most movies are made in USA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"con_df.loc[con_df.country == 'United States of America', 'num_movies'] = 700\ncon_df.head(20)\ncon_df.to_csv('mycsvfile.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [ dict(\n        type = 'choropleth',\n        locations = con_df['country'],\n        locationmode = 'country names',\n        z = con_df['num_movies'],\n        text = con_df['country'],\n        colorscale = [[0,'rgb(255, 255, 255)'],[1,'rgb(255, 0,255)']],\n        autocolorscale = False,\n        reversescale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(0,0,0)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick = False,\n            tickprefix = '',\n            title = 'Production Countries'),\n      ) ]\n\nlayout = dict(\n    title = 'Production Countries for the Movies (USA is being 700+ to be apple to watch other countries)',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\n\nfig = dict( data=data, layout=layout )\npy.iplot( fig, validate=False, filename='d3-world-map' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.9 2.1 Years Vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper functions to deal with multi-hot features\ndef group_indices(series,index=\"id\"):\n    d={}\n    for i in range(series.size):\n        l=eval(series.iloc[i])\n        for x in l:\n            d.setdefault(x[index],[])\n            d[x[index]].append(i)\n    return d\n\ndef get_groups(series,index=\"name\"):\n    s=set()\n    for i in range(series.size):\n        l=eval(series.iloc[i])\n        for x in l:s.add(x[index])\n    return list(s)\n\ndef multi_count(series,index=\"id\"):\n    return {k:len(v) for (k,v) in group_indices(series,index).items()}\n\ndef expand_multi_feature(df,column,index=\"id\"):\n    groups=group_indices(df[column],index=index)\n    result=pd.DataFrame()\n    for name,indices in groups.items():\n        rows=df.iloc[indices].copy()\n        rows[column]=name\n        result=result.append(rows)\n    return result\n\ndef multi_groupby(df1,column,index=\"id\"):\n    return expand_multi_feature(df,column,index).groupby(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numbers of movies released in each decade\ndef count_pie(series,filename):\n    counts=series.value_counts()\n    counts=counts/counts.sum()\n    labels=['' if num<0.01 else str(year) for (year,num) in counts.items()]\n    f, ax = plt.subplots(figsize=(8, 8))\n    explode = [0.02 if counts.iloc[i] < 100 else 0.001 for i in range(counts.size)]\n    plt.pie(counts,labels=labels,autopct=lambda x:'{:1.0f}%'.format(x) if x > 1 else '',explode=explode)\n    if saving:plt.savefig(filename,dpi=150)\n    plt.show()\n\ndef count_decade_pie(df,filename):\n    count_pie(df2.release_date.dropna().apply(lambda x:str(int(x[:4])//10*10)+'s'),filename)\n    \ncount_decade_pie(df2,filename=\"pie_decade.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pie chart tells us that in 2000s there were alot of movies released","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1.1 2.2 Genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud of genres and keywords\ndef multi_wordcloud(series,filename):\n    w=wc.WordCloud(background_color=\"white\",margin=20,width=800,height=600,prefer_horizontal=0.7,max_words=50,scale=2)\n    count=multi_count(series,\"name\")\n    w.generate_from_frequencies(count)\n    if saving:w.to_file(filename)\n    f, ax = plt.subplots(figsize=(16, 8))\n    plt.axis('off')\n    plt.imshow(w)\n    plt.show()\n\nmulti_wordcloud(df2.genres,filename=\"wordcloud_genres.png\")\nmulti_wordcloud(df2.keywords,filename=\"wordcloud_genres2.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above tells us that most movies are Drama,Comedy and thriller and from the keywords we have most repeated key is independent","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1.2 2.3 Popularity vs Genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of popularity and runtime groupby genres\ndef plotby_box(df,x,y,filename,yscale=\"linear\"):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    sns.boxenplot(data=expand_multi_feature(df,x,\"name\"),x=x,y=y)\n    plt.yscale(yscale)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \nplotby_box(df2,\"genres\",\"popularity\",yscale=\"log\",filename=\"genres_popularity.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1.3 2.4 Vote_Average vs Genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotby_bar(df,x,y,filename):\n    sns.set(style=\"whitegrid\")\n    df=df.replace(0,np.nan).copy()\n    f,ax=plt.subplots(figsize=(20, 10))\n    sns.barplot(data=expand_multi_feature(df,x,\"name\"),x=x,y=y)\n    plt.yticks(fontsize=20)\n    plt.xticks(rotation=55,fontsize=20)\n    plt.xlabel(x,fontsize=30)\n    plt.ylabel(y,fontsize=30)\n    if saving:plt.savefig(filename,bbox_inches=\"tight\",dpi=150)\n    plt.show()\n    \nplotby_bar(df2,\"genres\",\"vote_average\",filename=\"genres_vote.png\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest average vote for the genres are war, history and documentary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1.4 2.5 Rate vs Movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter only votes to movies in movies metadata\nratings = pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings_df = ratings.merge(df2[['id']], left_on=['movieId'], right_on=['id'], how='inner')\n# add a new feature, time_dt, to ratings_df by converting timestamp to date\nratings_df['time_dt'] = ratings_df['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n# split the time_dt to year features\nratings_df['year'] = ratings_df['time_dt'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = ratings_df.groupby(['year'])['rating'].mean().reset_index()\nfig, (ax) = plt.subplots(ncols=1, figsize=(12,5))\nplt.plot(dt['year'],dt['rating']);\nplt.xlabel('Year');\nplt.ylabel('Average ratings');\nplt.title('Average ratings per year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can tell that the movies most rated are in 2015","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1.3 2.4 Correlation Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.title('Correlation Matrix')\n# mask = np.triu(np.ones_like(md.corr(), dtype=np.bool))\nsns.heatmap(df2.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is showing us that popularity and vote_count, and revenue and vote_count have the highest which is 0.78","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.1 Demographic Filtering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The most basic form of a recommendation engine would be where the engine recommends the most popular items to all the users. That would be generalized as everyone would be getting similar recommendations as we didn’t personalize the recommendations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()\ndf2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C = df2['vote_average'].mean()\nm = df2['vote_count'].quantile(0.9)\nC, m","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means the vote_average mean is 6.09 ,and we will only consider movies that have a minimum vote count of 1838.4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter out movies that don't have 90 % of vote count\nq_movies = df2.copy().loc[df2['vote_count'] >= m]\nq_movies.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 481 movies which are less than 90% of vote count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v/(v+m) * R) + (m/(m+v) * C)\n\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)\nq_movies = q_movies.sort_values('score', ascending=False)\n\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop = df2.sort_values('popularity', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['title'].head(6), pop['popularity'].head(6), align='center') \nplt.gca().invert_yaxis()\nplt.xlabel('Popularity')\nplt.title('Popular Movies')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.1 Content Based","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this technique, the users are recommended the similar content which they have used/watched/liked the most before.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"TfidVectorizer it helps us to put each word in a column and stop the common english word like example 'the' word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['overview'].head(5)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntfidf = TfidfVectorizer(stop_words='english')\n\ndf2['overview'] = df2['overview'].fillna('')\n\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\n\ntfidf_matrix.shape\n\ncosine_sim = linear_kernel(tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying cosine similarity\n\ndocuments = [\n    'alpine snow winter boots.',\n    'snow winter jacket.',\n    'active swimming briefs',\n    'active running shorts',\n    'alpine winter gloves'\n]\n\ncntvt = CountVectorizer(stop_words='english')\n\ntfidf_matrix = cntvt.fit_transform(documents)\ncntvt.get_feature_names()\ntfidf_matrix.todense()\n\ncos_sim = cosine_similarity(tfidf_matrix)\ncos_sim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cosine_similarity tells us using some equation how similar they are,so as above we can see that 'alpine snow winter' boots is similar with 'snow winter jacket' because they both have 'snow winter' content in the sentence so that gave us result of 0.577","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()\n\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]\n    movie_indices = [i[0] for i in sim_scores]\n    return df2['title'].iloc[movie_indices]\n\nidx = indices[\"The Dark Knight Rises\"]\ndf2['title'].iloc[[i[0] for i in (sorted(list(enumerate(cosine_sim[idx])), key=lambda x: x[1], reverse=True)[1:11])]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using cosine_similarity with overview for the movie 'The Dark Knight Rises' we got this result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Avengers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# literal_eval is a python function to evaluate correctness of string data. It\n# will also create python objects for you \nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\n\ndf2['cast'][0]\ndf2['crew'][0]\ndf2['keywords'][0]\ndf2['genres'][0]\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(literal_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return top 3\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        if len(names) > 3:\n            names = names[:3]\n        return names\n    \n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['director'] = df2['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    df2[feature] = df2[feature].apply(get_list)\n    \ndf2[['title', 'cast', 'director', 'keywords', 'genres']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning and prepa\n\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    df2[feature] = df2[feature].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Joining the keywords, cast, director and genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create \"soup\" for the vectorization used to compute the cosine similarity matrix\n\ndef create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\ndf2['soup'] = df2.apply(create_soup, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(df2['soup'])\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)\ndf2 = df2.reset_index()\nindices = pd.Series(df2.index, index=df2['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using cosine_similarity with keywords, genres, director and cast we get movie recommendation for 'The Dark Knight Rises' as above which is better result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1 Collaborative Filtering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In collaborative filtering, two entities collaborate to deduce recommendations on the basis of certain similarities between them. These filtering techniques are broadly of two types:\n\n1.User Based Collaborative Filtering: In user based collaborative filtering, we find out the similarity score between the two users. On the basis of similarity score, we recommend the items bought/liked by one user to other user assuming that he might like these items on the basis of similarity. This will be more clear when we go ahead and implement this. Major online streaming service, Netflix have their recommendation engine based on user based collaborative filtering.\n\n2.Item Based Collaborative Filtering: In item based collaborative filtering, the similarity of an item is calculated with the existing item being consumed by the existing users. Then on the basis of amount of similarity, we can say that if user X likes item A and a new item P is most similar to item A then it highly makes sense for us to recommend item P to user X.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# User-User, Item-Item Collaborative filtering\nfrom surprise import Reader, Dataset, SVD #, cross_validate #evaluate\nfrom surprise.model_selection import cross_validate, KFold\nreader=Reader(rating_scale=(1,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"read user rating file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the user rating file (subset file to improve processing time)\nratings=pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\nratings.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used SVD(Singular Value Decompostion)\nthe SVD is used as a collaborative filtering technique. It uses a matrix structure where each row represents a user, and each column represents an item. The elements of this matrix are the ratings that are given to items by users.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Root-Mean-Square Error (RMSE) was used for evaluation and is\ndefined as follows:\n\nRMSE <- function(true_ratings, predicted_ratings){\n    sqrt(mean((true_ratings - predicted_ratings)^2))\n}\n\nRMSE was the metric used to judge entries in the Netflix challenge. The lower the RMSE was on Netflix’s quiz set between the submitted rating predictions and the actual ratings, the better the method was.\nso we are using RMSE on our recommendation system","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n# data.split(n_folds=5)\n\nsvd= SVD()\n# evaluate(svd, data, measures=['RMSE','MAE'])\n# cross_validate(NormalPredictor(), data, cv=5)\n\n# Run 5-fold cross-validation and print results\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we got RMSE of mean of 0.89 which is good","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a training set for svd\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting all userId =1 with the rattings\nratings[ratings['userId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(svd.predict(1, 302).est)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For movie with Id 302, we get a prediction of estimated rate 2.85 out 5 with user Id 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6.1 AutoMl","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:  # SciPy >= 0.19\n    from scipy.special import comb, logsumexp\nexcept ImportError:\n    from scipy.misc import comb, logsumexp  # noqa \n!pip install auto-sklearn\n!apt-get remove swig \n!apt-get install swig3.0 build-essential -y\n!ln -s /usr/bin/swig3.0 /usr/bin/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n# !pip install sklearn\n\nimport autosklearn.classification\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# from scipy.special import comb\n# import sklearn\n# import sklearn.model_selection\nmovies = df2.dropna(subset=['vote_average', 'budget', 'revenue'], how='all')\nX = movies[['budget', 'revenue']]\ny = movies['vote_average']\n\n \n\nX = X.iloc[:, :].values\ny = y.iloc[:].values\ny = y.astype(int)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nX_train, X_test, y_train, y_test = \\\n    sklearn.model_selection.train_test_split(X, y, random_state=1)\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='/tmp/autosklearn_cv_example_tto2',\n    output_folder='/tmp/autosklearn_cv_example_oto22',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n \n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='movie_recommendation')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\n \n\nprint(automl.show_models())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = automl.predict(X_test)\nprint(\"Accuracy as per AutoML: \", sklearn.metrics.accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.1 References","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* https://www.kaggle.com/sjj118/movie-visualization-recommendation-prediction\n* https://www.kaggle.com/rounakbanik/movie-recommender-systems\n* https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system\n* https://surprise.readthedocs.io/en/stable/getting_started.html\n* https://medium.com/@gracy.f/automl-for-python-on-windows-314ca8ea6955","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}