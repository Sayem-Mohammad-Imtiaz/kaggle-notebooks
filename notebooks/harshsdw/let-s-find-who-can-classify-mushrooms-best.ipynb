{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Hello there, today we will finding out which model is best for classifying mushroomsüçÑ as poisionus ‚ò†!!**"},{"metadata":{},"cell_type":"markdown","source":"**So let's first read the dataframe!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see, this dataset doesn't have  missing values and all the columns are categorical!!**"},{"metadata":{},"cell_type":"markdown","source":"**This makes our work simpler!**"},{"metadata":{},"cell_type":"markdown","source":"**Lets seperate the label and data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['class']\nX = df.drop(columns=['class'],axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As all the columns are categorical, let's just one-hot encode them all**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next, as the label is also categorical we will convert it to numerical as well. Although this isn't required as almost all the algorithms do it if we miss it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.replace({'e':0,'p':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As the data is quite balanced, we won't be required to assign class weights or upsample or downsample the data. Nice!**"},{"metadata":{},"cell_type":"markdown","source":"# Let's split the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape , X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**That is enough data to build a good model!**"},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"# **Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression().fit(X_train,y_train)\nlog.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = log.predict(X_test)\nlog.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression gives a very impressive result!! Sometimes less complicated algorithms produces the best results!**"},{"metadata":{},"cell_type":"markdown","source":"# **Linear SVC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(C=0.005,kernel='linear' ).fit(X_train , y_train)\nsvc.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svc.predict(X_test)\nsvc.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Linear SVC also give great results, with higher value for 'C' we can get higher accuracy as well!**"},{"metadata":{},"cell_type":"markdown","source":"# **Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndec = DecisionTreeClassifier(max_depth=4 , min_samples_leaf=3).fit(X_train,y_train)\ndec.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = dec.predict(X_test)\ndec.score(X_test , y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree gives accuracy similar to Linear SVC!!"},{"metadata":{},"cell_type":"markdown","source":"# **Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrand = RandomForestClassifier(max_depth=8 , n_estimators=100 , min_samples_leaf=3).fit(X_train,y_train)\nrand.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rand.predict(X_test)\nrand.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As Expected, Random Forest gives almost perfect result! **"},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB().fit(X_train,y_train)\nnb.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nb.predict(X_test)\nnb.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compared to other models, Gaussian Naive Bayes gives decent result without any parameter tuning**"},{"metadata":{},"cell_type":"markdown","source":"# **Stochastic Gradient Descent**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(max_iter=100,shuffle=True,random_state=69).fit(X_train, y_train)\nsgd.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = sgd.predict(X_test) \nsgd.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SGD gives us perfect score!!**"},{"metadata":{},"cell_type":"markdown","source":"# **K-Nearest Neighbour**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2,leaf_size=20, algorithm='kd_tree',p=1).fit(X_train,y_train)\nknn.score(X_train, y_train)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict(X_test)\nknn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similar to SGD, KNN also gives perfect score for the test and train set!**"},{"metadata":{},"cell_type":"markdown","source":"# As per my observation while training all these models, most of them will be able to give 100% accuracy by proper hyperparameter training"},{"metadata":{},"cell_type":"markdown","source":"# So, final verdict - Because of the simplicity of the dataset almost all the models give perfect score!"},{"metadata":{},"cell_type":"markdown","source":"# If you like my work, an upvote would be great!"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}