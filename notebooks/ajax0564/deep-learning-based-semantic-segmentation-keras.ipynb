{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After generating the segmentation images, place them in the training/testing folder. Make separate folders for input images and the segmentation images. The file name of the input image and the corresponding segmentation image should be the same. For this tutorial we would be using a data-set which is already prepared. You can download it from here ([Aerial Semantic Segmentation Drone Dataset](https://www.kaggle.com/bulentsiyah/semantic-drone-dataset)).","metadata":{}},{"cell_type":"markdown","source":"## [Aerial Semantic Segmentation Drone Dataset](https://www.kaggle.com/bulentsiyah/semantic-drone-dataset)","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\noriginal_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/001.jpg\"\nlabel_image_semantic = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/001.png\"\n\nfig, axs = plt.subplots(1,2, figsize=(16, 8), constrained_layout=True)\n\naxs[0].imshow( Image.open(original_image))\n# axs.grid(False)\n\nlabel_image_semantic = Image.open(label_image_semantic)\naxs[1].imshow( Image.open(original_image))\n# label_image_semantic = np.asarray(label_image_semantic)\naxs[1].imshow(label_image_semantic,alpha = 0.6)\n# axs[1].grid(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model_name\tBase Model\tSegmentation Model\nfcn_8\tVanilla CNN\tFCN8\nfcn_32\tVanilla CNN\tFCN8\nfcn_8_vgg\tVGG 16\tFCN8\nfcn_32_vgg\tVGG 16\tFCN32\nfcn_8_resnet50\tResnet-50\tFCN32\nfcn_32_resnet50\tResnet-50\tFCN32\nfcn_8_mobilenet\tMobileNet\tFCN32\nfcn_32_mobilenet\tMobileNet\tFCN32\npspnet\tVanilla CNN\tPSPNet\nvgg_pspnet\tVGG 16\tPSPNet\nresnet50_pspnet\tResnet-50\tPSPNet\nunet_mini\tVanilla Mini CNN\tU-Net\nunet\tVanilla CNN\tU-Net\nvgg_unet\tVGG 16\tU-Net\nresnet50_unet\tResnet-50\tU-Net\nmobilenet_unet\tMobileNet\tU-Net\nsegnet\tVanilla CNN\tSegnet\nvgg_segnet\tVGG 16\tSegnet\nresnet50_segnet\tResnet-50\tSegnet\nmobilenet_segnet\tMobileNet\tSegnet","metadata":{}},{"cell_type":"code","source":"!pip install keras-segmentation","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_segmentation.models.unet import resnet50_unet\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\nmodel = resnet50_unet(n_classes=n_classes ,  input_height=416, input_width=608  )\nepochs = 10\nmodel.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"resnet50_unet\" , epochs=epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"import time\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nstart = time.time()\n\ninput_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/001.jpg\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('Pridiction')\naxs[0].grid(False)\naxs[0].imshow(out,alpha = 0.6)\n\n\nvalidation_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/001.png\"\n\naxs[1].imshow(img_orig)\naxs[1].imshow( Image.open(validation_image),alpha = 0.6)\naxs[1].set_title('Original')\naxs[1].grid(False)\n\ndone = time.time()\nelapsed = done - start","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(elapsed)\nprint(out)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}}]}