{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In this project, sentiments in tweets are analyzed. In brief, the goal is to classify the tweets based on positive sentiments using words (such as love, happy) and negative sentiments using words (racist, hate). The tasks performed are:\n1. Exploratory Data Analysis\n2. Plot WordCloud\n3. Data cleaning (Removing Punctuations)\n4. Data cleaning (Removing Stopwords)\n5. Countvectorization (Tocknization)\n6. Create pipeline to perform Task 3,4,5\n7. Train Naive Bayes Classifier\n8. Annalyze Model Performance","metadata":{}},{"cell_type":"code","source":"tweets_test_df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')\ntweets_train_df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df['tweet']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.drop(['id'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring Dataset","metadata":{}},{"cell_type":"code","source":"sns.heatmap(tweets_train_df.isnull(), yticklabels = False, cbar = False, cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.hist(bins= 30, figsize = (12,5), color = 'b')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These plots clearly shows that its a complete unbalanced data.\nsns.countplot(x=tweets_train_df['label'] ,data=tweets_train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df['lengths'] = tweets_train_df['tweet'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df['lengths'] = tweets_train_df['tweet'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution of tweets\ntweets_train_df['lengths'].plot(bins=100, kind = 'hist')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Min length is 11, so let's see it\ntweets_train_df[tweets_train_df['lengths']==11]['tweet'].iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets viwe the meesage with average length\ntweets_train_df[tweets_train_df['lengths']==85]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now separting positive and negative tweets\npositive = tweets_train_df[tweets_train_df['label']==0]\npositive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative = tweets_train_df[tweets_train_df['label']==1]\nnegative","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the word cloud\nfrom wordcloud import WordCloud","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = tweets_train_df['tweet'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#All tweets has been converted to a list\n#sentences","metadata":{}},{"cell_type":"code","source":"len(sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Joining sentences (combining all the sentences that we have)\nsentences_as_single_string = \" \".join(sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" As we have a complete unbalanced data, with almost 30K positive and 2.5K negative tweets. Hence,we can see that positive words are more often used in this string.","metadata":{}},{"cell_type":"code","source":"# Lets plot wordcloud of negative words.\nnegative_sentences = negative['tweet'].tolist()\nnegative_string = \" \".join(negative_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(negative_string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning (Remove punctuations from text)","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = 'Hi! everyone :) ; enjoy learning real world example of NLP !.....'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_punc_removed = [char   for char in sample if char not in string.punctuation]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_punc_removed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now join again\ntest_punc_removed_string = ''.join(sample_punc_removed)\ntest_punc_removed_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second and efficient method\nout = sample.translate(str.maketrans('', '', string.punctuation))\nout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Third and basic method\npunc_removed = []\nfor char in sample:\n    if char not in string.punctuation:\n        punc_removed.append(char)\n        \npunc_removed_join = ''.join(punc_removed)\npunc_removed_join","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning (Remove StopWords from text)","metadata":{}},{"cell_type":"code","source":"# The Question is what are stopwords, so lets download and plot them using Natural languae toolkit\nimport nltk #Natural language toolkit\nnltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets import stopword and see the common words stored there. These are words that don't convey any specific information\nfrom nltk.corpus import stopwords\nstopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets remove common words and retain only unique words\ntest_punc_removed_string_clean = [word for word in test_punc_removed_string.split() if word.lower() not in stopwords.words('english')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_punc_removed_string_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets try Pipeline approach to accomplish removal of punctuation and stopwords\ntest_sample = 'A sample to learn,; that how can we remove punctuations and stopwords in a pipeline fashion!!!'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_punc_removed_cleaned = [char for char in test_sample if char not in string.punctuation]\npipe_punc_removed_cleaned = ''.join(pipe_punc_removed_cleaned)\npipe_punc_removed_cleaned = [word for word in pipe_punc_removed_cleaned.split() if word.lower() not in stopwords.words('english')]\npipe_punc_removed_cleaned","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count Vectorization (Tokenization)","metadata":{}},{"cell_type":"code","source":"# This will take unique words utilized in text as features, and then count that how many time each word is utilized in that sentence. \nfrom sklearn.feature_extraction.text import CountVectorizer\nsample_new = ['This is first method.', 'This method is the second method.', 'This new one is the third one.' ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(sample_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets see the extracted feature names (unique words)\nprint (vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We can see that in first sentence, only four features (unique words) are present there (first three and last feature).\n#In second sentence of sample_new, word method is repeated two times, so we can see 2 at corresponding feature position\nprint(X.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see with following example that Countvectroizer always convert each character to lower case before transforming.\nsecond_sample = ['Hello World.', 'Hello Hello World', 'Hello World world world']\nXX = vectorizer.fit_transform(second_sample)\nprint(XX.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we will perform all operations in a pipeline, (1) Remove punctuations (2) Remove stopwords (3) Tockenization","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    remv_punc = [char for char in text.lower() if char not in string.punctuation]\n    remv_punc_join = ''.join(remv_punc)\n    remv_punc_clean = [word for word in remv_punc_join.split() if word.lower() not in stopwords.words('english')]\n    return remv_punc_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets visualize the newly created function\ntweets_df_clean = tweets_train_df['tweet'].apply(text_cleaning)\nprint(tweets_df_clean[5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual version of selected tweet, we can see that we have removed all punctuations and stopwords using a single user defined function\ntweets_train_df['tweet'][5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will use \"analyser\" to apply countvectorization. \n#In other words, analyzer is an preprocess step before applying countVectorization step.\nvectorizer_analyzer = CountVectorizer(analyzer = text_cleaning)\ncountvectorizer_tweets = CountVectorizer(analyzer= text_cleaning, dtype= 'uint8').fit_transform(tweets_train_df['tweet']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countvectorizer_tweets.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_features = countvectorizer_tweets\ny_label = tweets_train_df['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we will train a Naive Bayes Classifier Model","metadata":{}},{"cell_type":"code","source":"X_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_label.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size = 0.2, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing the model performance","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix\ncm = confusion_matrix(y_test, y_pred_test)\nsns.heatmap(cm, annot= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}