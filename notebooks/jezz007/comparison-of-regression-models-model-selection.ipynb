{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Data description\nThe Boston data frame has 506 rows and 14 columns.\n\nThis data frame contains the following columns:\n\n#### crim\nper capita crime rate by town.\n\n#### zn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n#### indus\nproportion of non-retail business acres per town.\n\n#### chas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n#### nox\nnitrogen oxides concentration (parts per 10 million).\n\n#### rm\naverage number of rooms per dwelling.\n\n#### age\nproportion of owner-occupied units built prior to 1940.\n\n#### dis\nweighted mean of distances to five Boston employment centres.\n\n#### rad\nindex of accessibility to radial highways.\n\n#### tax\nfull-value property-tax rate per $10,000.\n\n#### ptratio\npupil-teacher ratio by town.\n\n#### black\n1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\n#### lstat\nlower status of the population (percent).\n\n#### medv\nmedian value of owner-occupied homes in $1000s."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/housing.csv\",\"r\") as f:\n    data=f.readlines()\n\nhousing_data=[]\nfor line in data:\n    samples=[np.float32(x) for x in line.split()]\n    housing_data.append(samples)\n\nhousing_data=np.asarray(housing_data)\nboston=pd.DataFrame(housing_data,columns=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LTSTAT\",\"MEDV\"])\nprint(boston.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatter Plot "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(boston,figsize=(16,16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncor=boston.corr()\nfig=plt.figure(figsize=(12,12))\nfig=sns.heatmap(cor,annot=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### When we take a threshold of |0.4| we get 6 important features\n\n- seperating dataset to features and target sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=boston.loc[:,[\"RM\",\"PTRATIO\",\"LTSTAT\",\"INDUS\",\"NOX\",\"TAX\"]]\n\ny=boston[\"MEDV\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X\\n\",X1.head(),\"\\n\\nY\\n\",y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X1.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we take some regressors and fit and test the  model.\n\nwe are going to use LR,lasso,elasticnet,svr,knr,gaussian,decisiontree\n\nWe will cover ensemble methods seperately.\n\nwe are not removing outliers for now. we will check the efficiency by removing outliers later."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"setting the seed so that we will get same splits for every case.\n\nPerfomance of each models can be easily compared then."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=35\nkfold=KFold(n_splits=10,random_state=seed)\nscoring=\"r2\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"splitting the dataset to train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(X1,y,test_size=.3,random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use gridsearch to get the best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[]\nmodels.append([\"LR\",LinearRegression()])\nmodels.append([\"ENet\",ElasticNet()])\n\nmodels.append([\"SVR\",SVR(gamma=\"scale\")])\nmodels.append([\"KNR\",KNeighborsRegressor()])\nmodels.append([\"GPR\",GaussianProcessRegressor(normalize_y=True)])\nmodels.append([\"CART\",DecisionTreeRegressor()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grids=[]\nLR_param_grid={}\nparam_grids.append(LR_param_grid)\nENet_param_grid={}\nENet_param_grid[\"alpha\"]=[.001,.01,.1,.3,.5]\nENet_param_grid[\"l1_ratio\"]=[0,.2,.4,.5,.7,1]\nparam_grids.append(ENet_param_grid)\nsvr_param_grid={}\nsvr_param_grid[\"kernel\"]=[\"poly\",\"linear\",\"rbf\"]\nsvr_param_grid[\"degree\"]=[1,2,3,4]\nsvr_param_grid[\"C\"]=[.001,0.1,.3,.5,1,2,3]\nparam_grids.append(svr_param_grid)\nknr_param_grid={}\nknr_param_grid[\"n_neighbors\"]=[3,5,7,11]\nknr_param_grid[\"weights\"]=[\"uniform\",\"distance\"]\nparam_grids.append(knr_param_grid)\ngpr_param_grid={}\nparam_grids.append(gpr_param_grid)\ncart_param_grid={}\ncart_param_grid[\"max_depth\"]=[1,2,3,4]\nparam_grids.append(cart_param_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running the below cell takes some time and kaggle kernel, while commiting usually stops. \nSo i have stored the results i got seperately and have used in the next cell\n\n### for all the cells running GridSearch, I have done the same. Stored the results i got seperately and commented out the cells running GridSearch\n\n### you can try running these seperately, if you want"},{"metadata":{},"cell_type":"markdown","source":"'''\nresults=[]\nfor model,params in zip(models,param_grids):\n    gcv=GridSearchCV(estimator=model[1],param_grid=params,cv=kfold,scoring=scoring,iid=False)\n    gcv.fit(x_train,y_train)\n    results.append([model[0],gcv.best_params_,gcv.best_score_])\n''' "},{"metadata":{"trusted":true},"cell_type":"code","source":"results=[['LR', {}, 0.6743397010174477], ['ENet', {'alpha': 0.1, 'l1_ratio': 0.7}, 0.6758633980611956], ['SVR', {'C': 0.3, 'degree': 1, 'kernel': 'linear'}, 0.6646899021487152], ['KNR', {'n_neighbors': 3, 'weights': 'distance'}, 0.7612886361653691], ['GPR', {}, -13.56991654534516], ['CART', {'max_depth': 3}, 0.7346860566317643]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see how these estimators performs on the test set.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_scores=[]\nfor model,result in zip(models,results):\n    clf=model[1]\n    clf.set_params(**result[1])\n    clf.fit(x_train,y_train)\n    score=clf.score(x_test,y_test)\n    test_scores.append([model[0],result[1],score])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model,param,score in test_scores:\n    print(\"%s : %0.4f\"%(model,score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNearestRegressor has performed the best on test set.\n#### Now we can explore Ensemble Methods"},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Methods\n\nWe will now use the family of ensemble methods\n\nThere exists mainly two classes of ensemble methods\n#### Averaging  and Boosting \n\n- I am only covering Averaging class of ensemble methods for now. Boosting methods may be added later.\n"},{"metadata":{},"cell_type":"markdown","source":"In averaging methods, we will be trying Bagging Regressor with KNR, RandomForest and ExtraTrees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\nbgr_param_dict={\"n_estimators\":list(np.arange(1,100,5)),\"max_samples\":list(np.linspace(.1,1,10)),\"random_state\":[seed]}\nknn=KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\nbg=BaggingRegressor(base_estimator=knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'''\ngcv=GridSearchCV(estimator=bg,param_grid=bgr_param_dict,iid=False,cv=kfold,scoring=scoring)\n\ngcv.fit(x_train,y_train)\n'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_best_params = {'max_samples': 1.0, 'n_estimators': 76, 'random_state': 35}\nbagging_best_score=0.764816","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"best training score : %0.6f\\nbest params : %r\"%(bagging_best_score,bagging_best_params))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bg.set_params(**bagging_best_params)\nbg.fit(x_train,y_train)\nprint(\"Bagging Test score : %0.6f\"%bg.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNR Ensemble vs single estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(x_train,y_train)\nprint(knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here BaggingRegressor performed worse than base estimator.\n#### We can explore the reasons later.\n#### Feel free to comment the answers"},{"metadata":{},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use GridSearchCV to find the best parametets"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_dict={}\nparams_dict[\"n_estimators\"]=list(np.arange(1,100,5))\nparams_dict[\"max_depth\"]=[None,2,3,4,5,6,7,8,9,10]\nparams_dict[\"max_features\"]=[.2,.6,.8,1.0]\nparams_dict[\"bootstrap\"]=[True]\nparams_dict[\"random_state\"]=[seed]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'''\ngcv=GridSearchCV(estimator=RandomForestRegressor(bootstrap=True,random_state=seed),param_grid=params_dict,cv=kfold,scoring=scoring,iid=False)\ngcv.fit(x_train,y_train)\n'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gcv.best_params_)\nrfc_best_params={'bootstrap': True,\n 'max_depth': 9,\n 'max_features': 0.2,\n 'n_estimators': 96,\n 'random_state': 35}\nprint(rfc_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gcv.best_score_)\nrfc_best_score=0.8607639890718424\nprint(rfc_best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestRegressor()\nrfc.set_params(**rfc_best_params)\nrfc.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'''\ngcv=GridSearchCV(estimator=ExtraTreesRegressor(),param_grid=params_dict,cv=kfold,scoring=scoring,iid=False)\ngcv.fit(x_train,y_train)\n'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gcv.best_params_)\net_best_params={'bootstrap': True,\n 'max_depth': None,\n 'max_features': 0.6,\n 'n_estimators': 71,\n 'random_state': 35}\nprint(et_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gcv.best_score_)\net_best_score=0.870845214813861\nprint(et_best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et=ExtraTreesRegressor()\net.set_params(**et_best_params)\net.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(et.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This is the best score we got from the models we used."},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluated_models=['LR', 'ENet', 'SVR', 'KNR', 'GPR', 'CART', 'BAGGING', 'RForest','ETrees']\nevaluated_test_scores= [0.6077, 0.6012,0.587,0.6843,-0.0222,0.6223,0.6673,0.8172,0.8490]   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(evaluated_models,evaluated_test_scores,marker=\"o\",linestyle=\"--\",color=\"r\")\nplt.ylim(-0.5,1)\nplt.title(\"Model Evaluation Plot\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"R-Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We haven't done data cleaning for this dataset. \n### Data cleaning and scaling will improve the model performance.\n### I will cover the impacts of that in another kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}