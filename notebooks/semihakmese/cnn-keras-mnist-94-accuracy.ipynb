{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix\nfrom glob import glob\nfrom keras.utils.np_utils import to_categorical #Converting to one hot encoding \nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train shape : %s  \\nTest: Shape : %s\"%(train.shape,test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.nunique() # We got 10 Classes(Labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.label.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.label.value_counts())\nplt.figure(figsize = (12,10))\nsns.countplot(train.label, palette =\"cubehelix\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing with PieChart \nlabelsx = train.label.value_counts().index\ncolors = [\"grey\",\"red\",\"blue\",\"yellow\",\"brown\",\"orange\",\"pink\",\"green\",\"purple\",\"indigo\"]\nexplode = [0,0,0,0,0,0,0,0,0,0]\nsizes = train.label.value_counts().values\n\nplt.figure(figsize = (9,9))\nplt.pie(sizes,explode = explode, labels = labelsx, colors = colors, autopct =\"%1.1f%%\")\nplt.title(\"Label Counting by using PieChart (Seaborn)\",color = \"violet\",fontsize = 15, fontstyle =\"oblique\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.label.value_counts())\nplt.figure(figsize = (12,10))\nsns.countplot(test.label, palette = \"icefire\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seperating Label from the Datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Dataset\nY_train = train.label\nX_train = train.drop(labels = [\"label\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Dataset\nY_test = test.label\nX_test = test.drop(labels = [\"label\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Some of the Examples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(3,2,1)\nimg1 = X_train.iloc[0].to_numpy().reshape((28,28))\nplt.imshow(img1,cmap='gray')\nplt.subplot(3,2,2)\nimg2 = X_train.iloc[10].to_numpy().reshape((28,28))\nplt.imshow(img2,cmap='gray')\nplt.subplot(3,2,3)\nimg3 = X_train.iloc[98].to_numpy().reshape((28,28))\nplt.imshow(img3,cmap='gray')\nplt.subplot(3,2,4)\nimg4 = X_train.iloc[25].to_numpy().reshape((28,28))\nplt.imshow(img4,cmap='gray')\nplt.subplot(3,2,5)\nimg5 = X_train.iloc[120].to_numpy().reshape((28,28))\nplt.imshow(img5,cmap='gray')\nplt.subplot(3,2,6)\nimg6 = X_train.iloc[264].to_numpy().reshape((28,28))\nplt.imshow(img6,cmap='gray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization - Reshaping and Label Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalization\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\nX_train = X_train / 255.0\nX_test = X_test / 255.0\nprint(\"X_train Shape : %s \\nX_Test Shape :%s\"%(X_train.shape,X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reshaping \nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nprint(\"X_train shape : \",X_train.shape)\nprint(\"X_Test shape : \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding - IF there more Labels we could use Glob Function\nfrom keras.utils.np_utils import to_categorical \nY_train = to_categorical(Y_train,num_classes = 10)\nY_test = to_categorical(Y_test,num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train and Test(Validation) Split \n- %15 Validation \n- %85 Train ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size = 0.15,random_state = 42)\nprint(\"X_train shape\",X_train.shape)\nprint(\"X_val shape\",X_val.shape)\nprint(\"Y_train shape\",Y_train.shape)\nprint(\"Y_val shape\",Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN Architecture \n<a href=\"https://ibb.co/sKHpTLb\"><img src=\"https://i.ibb.co/8rcnyFN/gec2.jpg\" alt=\"9\" border=\"0\"></a>\n\n- Create Model >> Conv - Max Pooling - Dropout - Conv - Max Pool Dropout - Fully Connected\n- Dropout is a technique where randomly selected neurons are ignored during training - We apply this technique to avoid overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools \n\nfrom keras.utils.np_utils import to_categorical #Converting to one hot encoding \nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\n\nepochs = 75\nbatch_size = 240\nnum_of_classes = 10\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding =\"same\",\n                 activation =\"relu\", input_shape =(28,28,1)))\nmodel.add(MaxPooling2D(pool_size =(3,3)))\n\nmodel.add(Conv2D(64,3,3))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(3,3))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))  #Hidden layer1\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(num_of_classes)) # Output layer size must equal to number of classes (labels)\nmodel.add(Activation(\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***CALLBACK*** - Learning Rate Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_optimizer = ReduceLROnPlateau(monitor = \"val_accuracy\",\n                                           patience = 2, verbose = 1,\n                                           factor = 0.5, min_lr = 0.000001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Compiling Model***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = RMSprop()\nmodel.compile(optimizer = optimizer, loss  =\"categorical_crossentropy\", metrics =[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We need to use categorical_crossentropy as loss function because we used softmax as a last activation func and that's one of the multiclasses act function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Augmentation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator( \n        shear_range = 0.2,\n        zoom_range = 0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip = True,\n        vertical_flip = True)\n\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the Model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(datagen.flow(X_train,Y_train, \n                                batch_size = batch_size), \n                                epochs = epochs,\n                                validation_data = (X_val,Y_val),\n                                steps_per_epoch = X_train.shape[0]//batch_size,\n                                callbacks = [learning_rate_optimizer])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Data Results ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test, verbose = 0)\nprint(\"Test Loss : %f \\nTest Accuracy : %f \"%(score[0],score[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())\nplt.plot(history.history[\"loss\"], label =\"Train Loss\")\nplt.plot(history.history[\"val_loss\"], label =\"Test Loss\")\nplt.legend()\nplt.show()\n\n#-----------------------------------------------------------------------\n\nprint(history.history.keys())\nplt.plot(history.history[\"accuracy\"], label =\"Train Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label =\"Test Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(15,15))\nsns.heatmap(confusion_mtx, annot=True, cmap=\"cubehelix\", linewidths=0.01,linecolor=\"green\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}