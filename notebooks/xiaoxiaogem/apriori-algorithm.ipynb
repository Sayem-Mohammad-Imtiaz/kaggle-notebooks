{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Apriori Algorithm\n\nThis is the first homework of EE448. In this work, you should be familiar with the Apriori algorithm and complete its implementation.\n\nThe specific points involved are, \n* candidate generation : self-joining\n* candidate generation : pruning\n* association rule mining: calculation of confidence"},{"metadata":{},"cell_type":"markdown","source":"** Step 0：Environment Setup**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 1: Data Preparation**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/supermarket/GroceryStoreDataSet.csv', header=None)\ndata.head()\ntransactions = []\nfor i in range(len(data)):\n    transactions.append(data.values[i, 0].split(','))\nprint(transactions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2: Inplementation of Apriori algorithm**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Apriori:\n    \n    def __init__(self, transactions, min_support, min_confidence):\n        self.transactions = transactions\n        self.min_support = min_support # The minimum support.\n        self.min_confidence = min_confidence # The minimum confidence.\n        self.support_data = {} # A dictionary. The key is frequent itemset and the value is support.      \n        \n    def create_C1(self):\n        \"\"\"\n        create frequent candidate 1-itemset C1 by scaning data set.\n        Input:\n            None\n        Output:\n            C1: A set which contains all frequent candidate 1-itemsets\n        \"\"\"\n        C1 = set()\n        for transaction in self.transactions:\n            for item in transaction:\n                C1.add(frozenset([item]))\n        return C1\n    \n  \n\n    def create_Ck(self, Lksub1, k):\n        \"\"\"\n        Create Ck.\n        Input:\n            Lksub1: Lk-1, a set which contains all frequent candidate (k-1)-itemsets.\n            k: the item number of a frequent itemset.\n        Output:\n            Ck: A set which contains all all frequent candidate k-itemsets.\n        \"\"\"\n        \n        Ck = set()\n        len_Lksub1 = len(Lksub1)\n        list_Lksub1 = list(Lksub1)\n        for i in range(len_Lksub1):\n            for j in range(1, len_Lksub1):\n                l1 = list(list_Lksub1[i])\n                l2 = list(list_Lksub1[j])\n                l1.sort()\n                l2.sort()\n                if l1[0:k-2] == l2[0:k-2]:\n                    Ck_item = list_Lksub1[i] | list_Lksub1[j]\n                    for item in Ck_item:\n                        sub_Ck = Ck_item - frozenset([item])\n                        if sub_Ck in Lksub1:\n                            Ck.add(Ck_item)\n        return Ck\n    \n    def generate_Lk_from_Ck(self, Ck):\n        \"\"\"\n        Generate Lk by executing a delete policy from Ck.\n        Input:\n            Ck: A set which contains all all frequent candidate k-itemsets.\n        Output:\n            Lk: A set which contains all all frequent k-itemsets.\n        \"\"\"\n        \n        Lk = set()\n        item_count = {}\n        for transaction in self.transactions:\n            for item in Ck:\n                if item.issubset(transaction):\n                    if item not in item_count:\n                        item_count[item] = 1\n                    else:\n                        item_count[item] += 1\n        t_num = float(len(self.transactions))\n        for item in item_count:\n            support = item_count[item] / t_num\n            if support >= self.min_support:\n                Lk.add(item)\n                self.support_data[item] = support\n        return Lk\n        \n    def generate_L(self):\n        \"\"\"\n        Generate all frequent item sets..\n        Input:\n            None\n        Output:\n            L: The list of Lk.\n        \"\"\"        \n        self.support_data = {}\n        \n        C1 = self.create_C1()\n        L1 = self.generate_Lk_from_Ck(C1)\n        Lksub1 = L1.copy()\n        L = []\n        L.append(Lksub1)\n        i = 2\n        while True:\n            Ci = self.create_Ck(Lksub1, i)\n            Li = self.generate_Lk_from_Ck(Ci)\n            if Li:\n                Lksub1 = Li.copy()\n                L.append(Lksub1)\n                i += 1\n            else:\n                break\n        return L\n        \n        \n    def generate_rules(self):\n        \"\"\"\n        Generate association rules from frequent itemsets.\n        Input:\n            None\n        Output:\n            big_rule_list: A list which contains all big rules. Each big rule is represented\n                       as a 3-tuple.\n        \"\"\"\n        L = self.generate_L()\n        \n        big_rule_list = []\n        sub_set_list = []\n        for i in range(0, len(L)):\n            for freq_set in L[i]:\n                for sub_set in sub_set_list:\n                    if sub_set.issubset(freq_set):\n                        # TODO : compute the confidence\n                        conf = self.support_data[freq_set]*100 / self.support_data[freq_set - sub_set]\n                        conf = conf/100\n                        big_rule = (freq_set - sub_set, sub_set, conf)\n                        if conf >= self.min_confidence and big_rule not in big_rule_list:\n                            big_rule_list.append(big_rule)\n                sub_set_list.append(freq_set)\n        return big_rule_list\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 3： Test Algorithm**"},{"metadata":{},"cell_type":"markdown","source":"    1. Model construction"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"model = Apriori(transactions, min_support=0.1, min_confidence=0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    2. Frequent item set mining"},{"metadata":{"trusted":true},"cell_type":"code","source":"L = model.generate_L()\n\nfor Lk in L:\n    print('frequent {}-itemsets：\\n'.format(len(list(Lk)[0])))\n\n    for freq_set in Lk:\n        print(freq_set, 'support:', model.support_data[freq_set])\n    \n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    3. Association rule mining"},{"metadata":{"trusted":true},"cell_type":"code","source":"rule_list = model.generate_rules()\n\nfor item in rule_list:\n    print(item[0], \"=>\", item[1], \"confidence: \", item[2])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}