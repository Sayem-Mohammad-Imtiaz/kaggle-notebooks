{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import lib","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.style.use('ggplot')\n\ndevice = (\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T11:15:02.774823Z","iopub.execute_input":"2021-06-19T11:15:02.775373Z","iopub.status.idle":"2021-06-19T11:15:04.411014Z","shell.execute_reply.started":"2021-06-19T11:15:02.775288Z","shell.execute_reply":"2021-06-19T11:15:04.409863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\nLoad the csv files and view some of the data","metadata":{}},{"cell_type":"code","source":"train_file = os.path.join('../input/gtsrb-german-traffic-sign/Train.csv')\ntest_file = os.path.join('../input/gtsrb-german-traffic-sign/Test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.413741Z","iopub.execute_input":"2021-06-19T11:15:04.414025Z","iopub.status.idle":"2021-06-19T11:15:04.423206Z","shell.execute_reply.started":"2021-06-19T11:15:04.413996Z","shell.execute_reply":"2021-06-19T11:15:04.422042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(train_file)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.428568Z","iopub.execute_input":"2021-06-19T11:15:04.428888Z","iopub.status.idle":"2021-06-19T11:15:04.536985Z","shell.execute_reply.started":"2021-06-19T11:15:04.42886Z","shell.execute_reply":"2021-06-19T11:15:04.535952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(test_file)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.538939Z","iopub.execute_input":"2021-06-19T11:15:04.539431Z","iopub.status.idle":"2021-06-19T11:15:04.578501Z","shell.execute_reply.started":"2021-06-19T11:15:04.539372Z","shell.execute_reply":"2021-06-19T11:15:04.577229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train csv shape: {df_train.shape} \\nTest csv shape: {df_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.580257Z","iopub.execute_input":"2021-06-19T11:15:04.580672Z","iopub.status.idle":"2021-06-19T11:15:04.587589Z","shell.execute_reply.started":"2021-06-19T11:15:04.58063Z","shell.execute_reply":"2021-06-19T11:15:04.586077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data distribution\nDisplaying a bar chart which shows the sample of class occurences within the dataset.","metadata":{}},{"cell_type":"code","source":"c = df_train['ClassId'].nunique()\nx = df_train['ClassId'].value_counts()\n\nplt.bar(x=x.index.sort_values(), height=x, color='#0066ff')\nplt.title('Numbers of sample in each class', color='black')\nplt.xlabel(\"Classes\", color='black')\nplt.ylabel(\"Samples\", color='black')\nplt.tick_params(colors='black')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.589671Z","iopub.execute_input":"2021-06-19T11:15:04.590211Z","iopub.status.idle":"2021-06-19T11:15:04.877293Z","shell.execute_reply.started":"2021-06-19T11:15:04.59008Z","shell.execute_reply":"2021-06-19T11:15:04.87592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Dataset\nCreate a custom dataset class with the proper methods for importing the data","metadata":{}},{"cell_type":"code","source":"class GTSRBDataset(Dataset):\n    def __init__(self, image_info, target_shape=(32, 32)):\n        self.target_height = target_shape[0]\n        self.target_width = target_shape[1]\n\n        # read data \n        self.images_path, self.labels, self.nSample = self.read_label_data(image_info)\n\n    def read_label_data(self, image_info):\n        # load labels data\n        images_path = []\n        labels = []\n        number_data = 0\n        \n        # read label data from csv file\n        image_data = pd.read_csv(image_info)\n        \n        for index, data in image_data.iterrows():\n            images_path.append('../input/gtsrb-german-traffic-sign/' + data['Path'])\n            labels.append(data['ClassId'])\n            number_data += 1\n        \n        return images_path, labels, number_data\n\n    def read_image(self, img_path):\n        \n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (self.target_width, self.target_height), cv2.INTER_CUBIC)\n        img = img.transpose(2, 0, 1)\n        img = img / 255.0\n\n        return img\n\n    def __getitem__(self, idx):\n        img_path = self.images_path[idx]\n        label = self.labels[idx]\n        img = self.read_image(img_path)\n\n        return (img, label)\n\n    def __len__(self):\n        return self.nSample\n\n    def visualize_random_images(self, nb_row=2, nb_col=3):\n        fig, axes = plt.subplots(nb_row,nb_col, figsize=(18, 18))\n\n        for i,ax in enumerate(axes.flat):\n            r = np.random.randint(self.nSample)\n            img = cv2.imread(self.images_path[r])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            ax.imshow(img)\n            ax.grid(False)\n            ax.axis('off')\n            ax.set_title('Label: '+ str(self.labels[r]))\n\n\nclass Collator(object):\n    def __call__(self, batch):\n        images = []\n        labels = []\n\n        for sample in batch:\n            img, label = sample\n            \n            if img is None:\n                continue\n            images.append(img)\n            labels.append(label)\n\n        return torch.FloatTensor(images), torch.LongTensor(labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.879231Z","iopub.execute_input":"2021-06-19T11:15:04.879684Z","iopub.status.idle":"2021-06-19T11:15:04.898368Z","shell.execute_reply.started":"2021-06-19T11:15:04.879633Z","shell.execute_reply":"2021-06-19T11:15:04.896507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Dataset","metadata":{}},{"cell_type":"code","source":"dataset = GTSRBDataset(image_info='../input/gtsrb-german-traffic-sign/Train.csv', target_shape=(32, 32))\nnb_classes = len(np.unique(dataset.labels))\nprint('The number of data: {} \\nThe number of classes: {}'.format(len(dataset), nb_classes))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:04.9032Z","iopub.execute_input":"2021-06-19T11:15:04.903641Z","iopub.status.idle":"2021-06-19T11:15:09.298666Z","shell.execute_reply.started":"2021-06-19T11:15:04.903597Z","shell.execute_reply":"2021-06-19T11:15:09.297483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split train and val dataloader","metadata":{}},{"cell_type":"code","source":"split_ratio = 0.8\nn_train = int(len(dataset) * split_ratio)\nn_val = len(dataset) - n_train\ntrain_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n\nprint(\"The number of train data: \", len(train_dataset))\nprint(\"The number of val data: \", len(val_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:09.301363Z","iopub.execute_input":"2021-06-19T11:15:09.301803Z","iopub.status.idle":"2021-06-19T11:15:09.334303Z","shell.execute_reply.started":"2021-06-19T11:15:09.301757Z","shell.execute_reply":"2021-06-19T11:15:09.333158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create model","metadata":{}},{"cell_type":"code","source":"# Create se_block\nclass SEBlock(nn.Module):\n\n    def __init__(self, input_channels, internal_neurons):\n        super(SEBlock, self).__init__()\n        self.down = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons, kernel_size=1, stride=1, bias=True)\n        self.up = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels, kernel_size=1, stride=1, bias=True)\n        self.input_channels = input_channels\n\n    def forward(self, inputs):\n        x = F.avg_pool2d(inputs, kernel_size=inputs.size(3))\n        x = self.down(x)\n        x = F.relu(x)\n        x = self.up(x)\n        x = torch.sigmoid(x)\n        x = x.view(-1, self.input_channels, 1, 1)\n        \n        return inputs * x","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:09.335907Z","iopub.execute_input":"2021-06-19T11:15:09.336409Z","iopub.status.idle":"2021-06-19T11:15:09.347149Z","shell.execute_reply.started":"2021-06-19T11:15:09.336369Z","shell.execute_reply":"2021-06-19T11:15:09.344191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create repvgg_block\ndef conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n    result = nn.Sequential()\n    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n    return result\n\nclass RepVGGBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):\n        super(RepVGGBlock, self).__init__()\n        self.deploy = deploy\n        self.groups = groups\n        self.in_channels = in_channels\n\n        assert kernel_size == 3\n        assert padding == 1\n\n        padding_11 = padding - kernel_size // 2\n\n        self.nonlinearity = nn.ReLU()\n\n        if use_se:\n            self.se = SEBlock(out_channels, internal_neurons=out_channels // 16)\n        else:\n            self.se = nn.Identity()\n\n        if deploy:\n            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n\n        else:\n            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n            # print('RepVGG Block, identity = ', self.rbr_identity)\n\n\n    def forward(self, inputs):\n        if hasattr(self, 'rbr_reparam'):\n            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n\n        if self.rbr_identity is None:\n            id_out = 0\n        else:\n            id_out = self.rbr_identity(inputs)\n\n        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n\n    def get_equivalent_kernel_bias(self):\n        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n\n    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n        if kernel1x1 is None:\n            return 0\n        else:\n            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n\n\n    def _fuse_bn_tensor(self, branch):\n        if branch is None:\n            return 0, 0\n\n        if isinstance(branch, nn.Sequential):\n            kernel = branch.conv.weight\n            running_mean = branch.bn.running_mean\n            running_var = branch.bn.running_var\n            gamma = branch.bn.weight\n            beta = branch.bn.bias\n            eps = branch.bn.eps\n        else:\n            assert isinstance(branch, nn.BatchNorm2d)\n            if not hasattr(self, 'id_tensor'):\n                input_dim = self.in_channels // self.groups\n                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n                for i in range(self.in_channels):\n                    kernel_value[i, i % input_dim, 1, 1] = 1\n                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n            kernel = self.id_tensor\n            running_mean = branch.running_mean\n            running_var = branch.running_var\n            gamma = branch.weight\n            beta = branch.bias\n            eps = branch.eps\n            \n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape(-1, 1, 1, 1)\n        return kernel * t, beta - running_mean * gamma / std\n\n    def switch_to_deploy(self):\n        if hasattr(self, 'rbr_reparam'):\n            return\n        kernel, bias = self.get_equivalent_kernel_bias()\n        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,\n                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n        self.rbr_reparam.weight.data = kernel\n        self.rbr_reparam.bias.data = bias\n        for para in self.parameters():\n            para.detach_()\n        self.__delattr__('rbr_dense')\n        self.__delattr__('rbr_1x1')\n        if hasattr(self, 'rbr_identity'):\n            self.__delattr__('rbr_identity')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:09.34927Z","iopub.execute_input":"2021-06-19T11:15:09.349751Z","iopub.status.idle":"2021-06-19T11:15:09.379874Z","shell.execute_reply.started":"2021-06-19T11:15:09.349707Z","shell.execute_reply":"2021-06-19T11:15:09.378424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create repvgg model\nclass RepVGG(nn.Module):\n\n    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False, use_se=False):\n        super(RepVGG, self).__init__()\n\n        assert len(width_multiplier) == 4\n\n        self.deploy = deploy\n        self.override_groups_map = override_groups_map or dict()\n        self.use_se = use_se\n\n        assert 0 not in self.override_groups_map\n\n        self.in_planes = min(64, int(64 * width_multiplier[0]))\n\n        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy, use_se=self.use_se)\n        self.cur_layer_idx = 1\n        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n\n\n    def _make_stage(self, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        blocks = []\n        for stride in strides:\n            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy, use_se=self.use_se))\n            self.in_planes = planes\n            self.cur_layer_idx += 1\n        return nn.Sequential(*blocks)\n\n    def forward(self, x):\n        out = self.stage0(x)\n        out = self.stage1(out)\n        out = self.stage2(out)\n        out = self.stage3(out)\n        out = self.stage4(out)\n        out = self.gap(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n\n        return out\n\ndef create_RepVGG_A0(num_classes, deploy=False):\n    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes, width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:09.382Z","iopub.execute_input":"2021-06-19T11:15:09.382951Z","iopub.status.idle":"2021-06-19T11:15:09.402913Z","shell.execute_reply.started":"2021-06-19T11:15:09.382822Z","shell.execute_reply":"2021-06-19T11:15:09.401897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repvgg_model = create_RepVGG_A0(num_classes=nb_classes)\nrepvgg_model = repvgg_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:09.406765Z","iopub.execute_input":"2021-06-19T11:15:09.40715Z","iopub.status.idle":"2021-06-19T11:15:14.874155Z","shell.execute_reply.started":"2021-06-19T11:15:09.407101Z","shell.execute_reply":"2021-06-19T11:15:14.873014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define config","metadata":{}},{"cell_type":"code","source":"batch_size = 128\nvalid_every = 2000\nprint_every = 500\nlr = 0.001\nnum_iters = 12000","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:14.875851Z","iopub.execute_input":"2021-06-19T11:15:14.876261Z","iopub.status.idle":"2021-06-19T11:15:14.883092Z","shell.execute_reply.started":"2021-06-19T11:15:14.876219Z","shell.execute_reply":"2021-06-19T11:15:14.881769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create dataloader for loading data","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=Collator(), shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=Collator(), shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\ndata_iter = iter(train_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:14.885062Z","iopub.execute_input":"2021-06-19T11:15:14.885969Z","iopub.status.idle":"2021-06-19T11:15:14.956867Z","shell.execute_reply.started":"2021-06-19T11:15:14.88592Z","shell.execute_reply":"2021-06-19T11:15:14.955199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a loss function and optimizer","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = AdamW(repvgg_model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-09)\nscheduler = OneCycleLR(optimizer, max_lr=lr, total_steps=num_iters, pct_start=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:14.960015Z","iopub.execute_input":"2021-06-19T11:15:14.96049Z","iopub.status.idle":"2021-06-19T11:15:14.977526Z","shell.execute_reply.started":"2021-06-19T11:15:14.960437Z","shell.execute_reply":"2021-06-19T11:15:14.976218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"def batch_to_device(images, gts):\n    images = images.to(device, non_blocking=True)\n    gts = gts.to(device, non_blocking=True)\n    \n    return images, gts","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:14.979524Z","iopub.execute_input":"2021-06-19T11:15:14.980975Z","iopub.status.idle":"2021-06-19T11:15:14.988213Z","shell.execute_reply.started":"2021-06-19T11:15:14.980928Z","shell.execute_reply":"2021-06-19T11:15:14.986586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_acc(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:14.990489Z","iopub.execute_input":"2021-06-19T11:15:14.991434Z","iopub.status.idle":"2021-06-19T11:15:15.000307Z","shell.execute_reply.started":"2021-06-19T11:15:14.991327Z","shell.execute_reply":"2021-06-19T11:15:14.998508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate():\n    repvgg_model.eval()\n    total_loss = []\n    total_acc = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            images, gts = batch\n            images, gts = batch_to_device(images, gts)\n            outputs = repvgg_model(images)\n            loss = criterion(outputs, gts)\n            acc = cal_acc(outputs, gts)\n            \n            total_loss.append(loss.item())\n            total_acc.append(acc)\n            \n            del outputs\n            del loss\n            \n    val_loss = np.mean(total_loss)\n    val_acc = np.mean(total_acc)\n    repvgg_model.train()\n    \n    return val_loss, val_acc","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:15.002809Z","iopub.execute_input":"2021-06-19T11:15:15.004091Z","iopub.status.idle":"2021-06-19T11:15:15.015726Z","shell.execute_reply.started":"2021-06-19T11:15:15.004Z","shell.execute_reply":"2021-06-19T11:15:15.014492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(batch):\n    # get the inputs\n    images, gts = batch\n    images, gts = batch_to_device(images, gts)\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # forward + backward + optimize + scheduler\n    outputs = repvgg_model(images)\n    loss = criterion(outputs, gts)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(repvgg_model.parameters(), 1) \n    optimizer.step()\n    scheduler.step()\n\n    loss_item = loss.item()\n    \n    return loss_item","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:15.017902Z","iopub.execute_input":"2021-06-19T11:15:15.018547Z","iopub.status.idle":"2021-06-19T11:15:15.030517Z","shell.execute_reply.started":"2021-06-19T11:15:15.018505Z","shell.execute_reply":"2021-06-19T11:15:15.02873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_loss = 0\nbest_acc = 0\nglobal_step = 0\nweight_path = 'repvgg.pth.tar'\n\ntorch.backends.cudnn.benchmark = True\n\nfor i in range(num_iters):\n    repvgg_model.train()\n    \n    try:\n        batch = next(data_iter)\n    except StopIteration:\n        data_iter = iter(train_loader)\n        batch = next(data_iter)\n        \n    global_step += 1\n    loss = train_step(batch)\n    total_loss += loss\n\n    if global_step % print_every == 0:\n        print('step: {:06d}, train_loss: {:.4f}'.format(global_step, total_loss / print_every))\n        total_loss = 0\n        \n    if global_step % valid_every == 0:\n        # validate \n        val_loss, val_acc = validate()\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(repvgg_model.state_dict(), weight_path)\n            \n        print(\"==============================================================================\")\n        print(\"val_loss: {:.4f}, val_acc: {:.4f}\".format(val_loss, val_acc))\n        print(\"==============================================================================\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:15:15.03233Z","iopub.execute_input":"2021-06-19T11:15:15.032983Z","iopub.status.idle":"2021-06-19T12:00:11.64998Z","shell.execute_reply.started":"2021-06-19T11:15:15.03294Z","shell.execute_reply":"2021-06-19T12:00:11.648779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert the training-time models into inference-time","metadata":{}},{"cell_type":"code","source":"def repvgg_model_convert(model:torch.nn.Module, save_path=None, do_copy=True):\n    if do_copy:\n        model = copy.deepcopy(model)\n    for module in model.modules():\n        if hasattr(module, 'switch_to_deploy'):\n            module.switch_to_deploy()\n    if save_path is not None:\n        torch.save(model.state_dict(), save_path)\n    return model\n    \n    \n# weight path\nweight_path = 'repvgg.pth.tar'\nconvert_weight_path = 'convert_weight_path.pth.tar'\n\n# create model\nrepvgg_model = create_RepVGG_A0(num_classes=43)\nrepvgg_model.load_state_dict(torch.load(weight_path, map_location=device), strict=False)\n\n# convert multi branch model to single branch model\nconvert_model = repvgg_model_convert(repvgg_model, save_path=convert_weight_path)\nconvert_model = convert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T12:00:11.652575Z","iopub.execute_input":"2021-06-19T12:00:11.653066Z","iopub.status.idle":"2021-06-19T12:00:12.156016Z","shell.execute_reply.started":"2021-06-19T12:00:11.653015Z","shell.execute_reply":"2021-06-19T12:00:12.154908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check model in test dataset","metadata":{}},{"cell_type":"code","source":"def predict(model, images, device):\n    images = images.to(device, non_blocking=True)\n    outputs = model(images)\n    _, preds = torch.max(outputs, dim=1)\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-06-19T12:04:19.171433Z","iopub.execute_input":"2021-06-19T12:04:19.171772Z","iopub.status.idle":"2021-06-19T12:04:19.178198Z","shell.execute_reply.started":"2021-06-19T12:04:19.171743Z","shell.execute_reply":"2021-06-19T12:04:19.176443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of image predict true\nlst_wrong_img = []\ntp = 0\n\nfor index, data in df_test.iterrows():\n    img = cv2.imread('../input/gtsrb-german-traffic-sign/' + data['Path'])\n    \n    preprocess_img = cv2.resize(img, (32, 32), cv2.INTER_AREA)\n    preprocess_img = preprocess_img.transpose(2, 0, 1)\n    preprocess_img = preprocess_img / 255.0\n    preprocess_img = np.expand_dims(preprocess_img, axis=0)\n    preprocess_img = torch.FloatTensor(preprocess_img)\n    \n    output = predict(convert_model, preprocess_img, device)\n    output = output.cpu().detach().numpy()\n    \n    if(int(output[0]) == data['ClassId']):\n        tp += 1\n    else:\n        lst_wrong_img.append([data['Path'], data['ClassId'], int(output[0])])\n\ndf = pd.DataFrame(lst_wrong_img, columns = ['Path', 'ClassId', 'Predict'])\ndf.to_csv('list_wrong_imgs.csv', index=False)\n\nprint(\"Accuracy in test dataset: \", float(tp/df_test.shape[0])*100)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T12:04:57.228768Z","iopub.execute_input":"2021-06-19T12:04:57.229101Z","iopub.status.idle":"2021-06-19T12:08:14.780738Z","shell.execute_reply.started":"2021-06-19T12:04:57.229073Z","shell.execute_reply":"2021-06-19T12:08:14.779651Z"},"trusted":true},"execution_count":null,"outputs":[]}]}