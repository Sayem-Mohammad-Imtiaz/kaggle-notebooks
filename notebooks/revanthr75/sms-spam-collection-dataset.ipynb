{"cells":[{"metadata":{"_uuid":"54f4d31239a0647338b76c7d35f9eb115c8ab383","trusted":true},"cell_type":"code","source":"# Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Styles\nplt.style.use('ggplot')\nsns.set_style('whitegrid')\n\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 10\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 10\nplt.rcParams['figure.titlesize'] = 12\nplt.rcParams['patch.force_edgecolor'] = True\n\n# Text Preprocessing\nimport nltk\n# nltk.download(\"all\")\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import word_tokenize\n\nimport spacy\nnlp = spacy.load(\"en\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ed1ededa351f22ba7d8e53c405cc67ebb5dfb8c","trusted":true},"cell_type":"code","source":"messages = pd.read_csv(\"./../input/spam.csv\", encoding = 'latin-1')\n\n# Drop the extra columns and rename columns\n\nmessages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\nmessages.columns = [\"category\", \"text\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42bdc935918fe49044e615c0dfbac06e642da12a","trusted":true},"cell_type":"code","source":"display(messages.head(n = 10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b85a7790c16cb92c0dc5af97389e8e745a76eef3","trusted":true},"cell_type":"code","source":"# Lets look at the dataset info to see if everything is alright\n\nmessages.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c38ff46dfb45305e6bcb4ef9613d35fb81e6dd9","trusted":true},"cell_type":"code","source":"messages[\"category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53f900816f9d4908099a6edd660ef38e11ec24f8","trusted":true},"cell_type":"code","source":"topMessages = messages.groupby(\"text\")[\"category\"].agg([len, np.max]).sort_values(by = \"len\", ascending = False).head(n = 10)\ndisplay(topMessages)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7107711acdddde2d5d974141563d61ad017a9632","trusted":true},"cell_type":"code","source":"spam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\nham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\n\nspam_words = []\nham_words = []\n\n# Since this is just classifying the message as spam or ham, we can use isalpha(). \n# This will also remove the not word in something like can't etc. \n# In a sentiment analysis setting, its better to use \n# sentence.translate(string.maketrans(\"\", \"\", ), chars_to_remove)\n\ndef extractSpamWords(spamMessages):\n    global spam_words\n    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    spam_words = spam_words + words\n    \ndef extractHamWords(hamMessages):\n    global ham_words\n    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n    ham_words = ham_words + words\n\nspam_messages.apply(extractSpamWords)\nham_messages.apply(extractHamWords)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ec079b4b6ca4486f8e033a3046821b5497115e0","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6024d94d524500cac3398c35cd2e606a785887","trusted":true},"cell_type":"code","source":"#Spam Word cloud\n\nspam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(spam_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e54fcc30698393716f19c016866312b5ad7d32ac","trusted":true},"cell_type":"code","source":"#Ham word cloud\n\nham_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(ham_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74025e5ea690679aef56752c7e8ecfcf9e9816f2","trusted":true},"cell_type":"code","source":"# Top 10 spam words\n\nspam_words = np.array(spam_words)\nprint(\"Top 10 Spam words are :\\n\")\npd.Series(spam_words).value_counts().head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17de5246259a3b3978aeafeb9d5d2af743b6264","trusted":true},"cell_type":"code","source":"# Top 10 Ham words\n\nham_words = np.array(ham_words)\nprint(\"Top 10 Ham words are :\\n\")\npd.Series(ham_words).value_counts().head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0b15b50fe828f9af19f5a5287f33bdab5af71f1","trusted":true},"cell_type":"code","source":"messages[\"messageLength\"] = messages[\"text\"].apply(len)\nmessages[\"messageLength\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89f11851732828c5fd43d91aa5feffabafe6f749","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == \"spam\"][\"messageLength\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word Length\")\n\nsns.distplot(messages[messages[\"category\"] == \"ham\"][\"messageLength\"], bins = 20, ax = ax[1])\nax[0].set_xlabel(\"Ham Message Word Length\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5075668a58e1a6ac3df8ed39fb6cb2608ff91c0b","trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\ndef cleanText(message):\n    \n    message = message.translate(str.maketrans('', '', string.punctuation))\n    words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n    \n    return \" \".join(words)\n\nmessages[\"text\"] = messages[\"text\"].apply(cleanText)\nmessages.head(n = 10)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6df4f9e7df701b1eee1e2adf212fe457f09e9159","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\nfeatures = vec.fit_transform(messages[\"text\"])\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"250ef549d64b975b0723a42ef3bad50e809daca9","trusted":true},"cell_type":"code","source":"def encodeCategory(cat):\n    if cat == \"spam\":\n        return 1\n    else:\n        return 0\n        \nmessages[\"category\"] = messages[\"category\"].apply(encodeCategory)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, messages[\"category\"], stratify = messages[\"category\"], test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b41983d5b4f3ac006f3a001d4f45b156fb2452d","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import fbeta_score\n\nfrom sklearn.naive_bayes import MultinomialNB\ngaussianNb = MultinomialNB()\ngaussianNb.fit(X_train, y_train)\n\ny_pred = gaussianNb.predict(X_test)\n\nprint(fbeta_score(y_test, y_pred, beta = 0.5))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}