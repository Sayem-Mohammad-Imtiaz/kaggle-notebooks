{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Getting json files and loading their body text for filtering about target topic","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\nimport pickle\nfrom gensim import corpora, models\nimport re\nfrom gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\nfrom nltk.corpus import stopwords\nimport pickle\nimport en_core_web_sm\nimport csv\nimport json\n\nfrom sklearn.preprocessing import Binarizer\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom scipy.spatial.distance import cosine, cdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/CORD-19-research-challenge/'\nmeta_df = pd.read_csv(path + 'metadata.csv') \nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cols names: {}\".format(meta_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(meta_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_for_merge = meta_df[['title', 'abstract','pdf_json_files','url']]\n#meta_abstracts = meta_abstracts.rename({'title':'Study', 'abstract':'Abstract'}, axis=1)\nmeta_for_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_df = pd.read_csv(path +'Kaggle/target_tables/8_risk_factors/' + 'Smoking Status.csv', index_col= 'Unnamed: 0') \nsmoke_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cols names: {}\".format(smoke_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(smoke_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_to_merge = smoke_df[['Date', 'Study', 'Journal']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_target = smoke_to_merge.merge(meta_for_merge,how='inner', left_on='Study', right_on='title')\nsmoke_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(smoke_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsmoke_target.isna().sum().plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_target.drop(columns=['title'], inplace=True)\nsmoke_target.drop_duplicates(subset='Study', keep=\"first\", inplace=True)\nlen(smoke_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_target.dropna(axis=0, inplace=True)\nsmoke_target.reset_index(drop=True, inplace=True)\nsmoke_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(smoke_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Beginning loading json files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# example with the second file from the target table\nwith open(path + smoke_target.pdf_json_files[1], 'r') as myfile:\n    data=myfile.read()\n\n# parse file\nobj = json.loads(data)\n# convert json body_text into a text list\nbody = obj['body_text']\njust_text2 = [body[d]['text'] for d in range(len(body))]\nprint(len(just_text2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform all data to lower-case\nclean_body = [text.lower() for text in just_text2]\nclean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\nclean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\nclean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\nclean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\nclean_body = [strip_short(text) for text in clean_body] #removing words with length lesser than 3 \nprint(clean_body[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filtering for smoking topic ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing the parsing loop for smoking\natt = ['he smokes a lot', 'i dont like smoking', 'who is smoking', 'what is to smoke', 'a test here it is smok']\nfor text in att:\n    if 'smok' in text:\n        print(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if some part of the body is related to smoking and print the index and the part\nfor i in range(len(clean_body)):\n    if 'smok' in clean_body[i]:\n        print(i)\n        print(clean_body[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final function for getting the relevant documents from json files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nimport nltk\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_target['original']=''\nsmoke_target['body']=''\nsmoke_target['sent']=''\nsmoke_target['sent2']=''\n  \ndef countOccurences(str, word): \n      \n    # split the string by spaces in a \n    a = str.split(\" \") \n  \n    # search for pattern in a \n    count = 0\n    for i in range(0, len(a)): \n          \n        # if match found increase count  \n        if (word == a[i]): \n           count = count + 1\n             \n    return count        \ny=''  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_docs(dataframe):\n    for i in range(len(smoke_target)):\n        try:\n            # open json file\n            with open(path + smoke_target.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n        except:\n            ValueError\n        body = obj['body_text']\n        body = [body[d]['text'] for d in range(len(body))]\n        y=sent_tokenize(str(body))\n        df=pd.DataFrame(index=np.arange(len(y)))\n        df['content']='none'\n        df['original']=df['content']\n      \n        for j in range(len(y)):\n            df.original[j]=y[j] \n            df.content[j]=y[j]\n          \n        for i in range(len(df.content)):\n            df.content[i] =  strip_numeric(df.content[i]) #Remove digits\n            df.content[i] =  strip_punctuation(df.content[i])  #Remove punctuation\n            df.content[i] =  strip_multiple_whitespaces(df.content[i]) #Remove multiple whitespaces\n            df.content[i] =  df.content[i].lower()\n            df.content[i] =  remove_stopwords(df.content[i])\n            df.content[i] =  strip_short(df.content[i])\n            df.content[i] =  stem_text(df.content[i])\n        df['index']=df.index\n        df['count']= 0\n        for i in range(len(df.content)):\n            str = df.content[i]\n            word =\"smoke\"\n            df['count'][i]=float((countOccurences(str, word))*1000/len(df.content[i]))\n        x=df.loc[df['count'].idxmax()]\n        summary=df.original[x['index']]\n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_smoke_relevant_docs = summarize_docs(smoke_target)\n#for i in range(len(final_smoke_relevant_docs)):\n #   print(final_smoke_relevant_docs[i])\nsummarize_docs(smoke_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_target.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_docs(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    \n    At the end it will print the number of relevant docs and \n    return a list with:\n        [1] = index from table, \n        [2] = indexes from the body list, \n        [3] = the preprocessed body text list\n    \n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    for i in range(len(dataframe)):\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n        except:\n            ValueError\n        # convert json body_text into a text list\n        smoke_target['body_dirty']=''\n        body = obj['body_text']\n        just_text = sent_tokenize([body[d]['text'] for d in range(len(body))])\n        smoke_target['body_dirty']= sent_tokenize([body[d]['text'])\n        clean_body = [text.lower() for text in just_text]\n        clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n        clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n        clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n        clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n        clean_body = [strip_short(text) for text in clean_body]\n        y=sent_tokenize(body)\n        relevant_parts = []\n        # check if the doc is related with smoking\n        for t in range(len(clean_body)):\n            if target in clean_body[t]:\n                # save the index of relevant parts\n                relevant_parts.append(t)\n                # save the docs in a list that has: target_index, clean_body_index, clean_json_body \n        if len(relevant_parts) != 0:\n            related_docs.append([i, relevant_parts, clean_body])\n\n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relevant_docs(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    \n    At the end it will print the number of relevant docs and \n    return a list with:\n        [1] = index from table, \n        [2] = indexes from the body list, \n        [3] = the preprocessed body text list\n    \n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    for i in range(len(dataframe)):\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n        except:\n            ValueError\n        # convert json body_text into a text list\n        body = obj['body_text']\n        just_text = [body[d]['text'] for d in range(len(body))]\n        clean_body = [text.lower() for text in just_text]\n        clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n        clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n        clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n        clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n        clean_body = [strip_short(text) for text in clean_body]\n\n        relevant_parts = []\n        # check if the doc is related with smoking\n        for t in range(len(clean_body)):\n            if target in clean_body[t]:\n                # save the index of relevant parts\n                relevant_parts.append(t)\n                # save the docs in a list that has: target_index, clean_body_index, clean_json_body \n        if len(relevant_parts) != 0:\n            related_docs.append([i, relevant_parts, clean_body])\n\n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_smoke_relevant_docs = get_relevant_docs(smoke_target, 'smok')\nprint(final_smoke_relevant_docs[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parsing through all the docs in the target table\n\nsmoke_related_docs = [['target_index', 'relevant_index', 'clean_json_body']] # the first object is the description of the list content\nfor i in range(len(smoke_target)):\n    try:\n        # open json file\n        with open(path + smoke_target.pdf_json_files[i], 'r') as myfile:\n            data=myfile.read()\n        # parse file\n        obj = json.loads(data)\n    except:\n        ValueError\n    # convert json body_text into a text list\n    body = obj['body_text']\n    just_text = [body[d]['text'] for d in range(len(body))]\n    clean_body = [text.lower() for text in just_text]\n    \n    relevant_parts = []\n    # check if the doc is related with smoking\n    for t in range(len(clean_body)):\n        if 'smok' in clean_body[t]:\n            # save the index of relevant parts\n            relevant_parts.append(t)\n            # save the docs in a list that has: target_index, clean_body_index, clean_json_body \n    if len(relevant_parts) != 0:\n        smoke_related_docs.append([i, relevant_parts, clean_body])\n\nprint(len(smoke_related_docs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(smoke_related_docs[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it seems like the smoke_target.pdf_json_files[2] has two json files related to it\nfor i in range(len(smoke_target)):\n    print(smoke_target.pdf_json_files[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for examination purpose we take just one\nsmoke_target.pdf_json_files[2] = 'document_parses/pdf_json/7a4a40db618de7c1be7eda37e30ad1eb03c16c21.json'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming\nsmoke_stem=[stem_text(abstract) for abstract in smoke_data]\nprint(smoke_stem[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemmed corpus storage\npickle.dump(smoke_stem, open(\"smoke_stemmed.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}