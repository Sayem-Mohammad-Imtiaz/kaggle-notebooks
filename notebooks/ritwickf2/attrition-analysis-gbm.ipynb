{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Attrition Analysis\n\nThe dataset contains a total of 35 fields and 1470 observations. This analysis is based on the 'Attrition' field. This notebook attempts to identify some key features which can help explain 'Attrition' in the organisation. Gradient Boosting has been used to identify the important features to predict 'Attrition'.\n\n### Key Steps\n\nThe following key steps have been followed:\n1. Exploratory Data Analysis\n2. Feature Engineering\n3. Model Fitting\n4. Visualisation and Identification of Important Features\n\n## Step 1 - Exploratory Data Analysis (EDA)\n\nThis includes looking at the distribution of the target variable as well as identifying any missing values among the features.\n\nFirst, all the required libraries are imported."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, log_loss, confusion_matrix)\n#Suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a next step, the Excel file has been imported using Pandas into the dataframe named 'df'. We can see that there are 26 numeric and 9 categorical fields."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Importing  the Dataset\nprint('Importing the CSV file.')\ndf = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint('File imported successfully!')\n\n#Datatypes in the dataset\nprint('Imported Dataframe Structure : \\n', df.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the head(), we can see a small snapshot of the data. The target field 'Attrition' is a categorical field with 'Yes' and 'No'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the 'Attrition' field is highly skewed with just over 200 'Yes' out of a total of 1470 observations. This skewness can result in a prediction which is highly geared towards predicting 'No'."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the number of 'Yes' and 'No' in 'Attrition'\nax = sns.catplot(x=\"Attrition\", kind=\"count\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Attrition', ylabel = 'Number of Employees')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I check if there are any missing values in the dataframe. As can be seen from the output, there are no missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identifying columns with missing information\nmissing_col = df.columns[df.isnull().any()].values\nprint('The missing columns in the dataset are: ',missing_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2 - Feature Engineering\n\nThe numeric and categorical fields need to be treated separately and the target field needs to be separated from the training dataset. The following few steps separate the numeric and categorical fields and drops the target field 'Attrition' from the feature set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting the Numeric and Categorical features\ndf_num = pd.DataFrame(data = df.select_dtypes(include = ['int64']))\ndf_cat = pd.DataFrame(data = df.select_dtypes(include = ['object']))\nprint(\"Shape of Numeric: \",df_num.shape)\nprint(\"Shape of Categorical: \",df_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Encoding Categorical Fields\n\nThe categorical fields have been encoded using the get_dummies() function of Pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping 'Attrition' from df_cat before encoding\ndf_cat = df_cat.drop(['Attrition'], axis=1) \n\n#Encoding using Pandas' get_dummies\ndf_cat_encoded = pd.get_dummies(df_cat)\ndf_cat_encoded.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Scaling Numeric Fields\n\nThe numeric fields have been scaled next for best results. StandardScaler() has been used for the same. Post scaling of the numeric features, they are merged with the categorical ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using StandardScaler to scale the numeric features\nstandard_scaler = StandardScaler()\ndf_num_scaled = standard_scaler.fit_transform(df_num)\ndf_num_scaled = pd.DataFrame(data = df_num_scaled, columns = df_num.columns, index = df_num.index)\nprint(\"Shape of Numeric After Scaling: \",df_num_scaled.shape)\nprint(\"Shape of categorical after Encoding: \",df_cat_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining the Categorical and Numeric features\ndf_transformed_final = pd.concat([df_num_scaled,df_cat_encoded], axis = 1)\nprint(\"Shape of final dataframe: \",df_transformed_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting the target variable - 'Attrition'\ntarget = df['Attrition']\n\n#Mapping 'Yes' to 1 and 'No' to 0\nmap = {'Yes':1, 'No':0}\ntarget = target.apply(lambda x: map[x])\n\nprint(\"Shape of target: \",target.shape)\n\n#Copying into commonly used fields for simplicity\nX = df_transformed_final #Features\ny = target #Target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Train and Test Split\n\nThe data is next split into training and test dataset using the train_test_split functionality of sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting into Train and Test dataset in 90-10 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 0.8, random_state = 0, stratify = y)\nprint(\"Shape of X Train: \",X_train.shape)\nprint(\"Shape of X Test: \",X_test.shape)\nprint(\"Shape of y Train: \",y_train.shape)\nprint(\"Shape of y Test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3 - Model Fitting\n\nHere, I am using the Gradient Boosting model for the decision trees to identify the importance of the features. 'Attrition' is the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Gradient Boosting to predict 'Attrition' and create the Trees to identify important features\ngbm = GradientBoostingClassifier(n_estimators = 200, max_features = 0.7, learning_rate = 0.3, max_depth = 5, random_state = 0, verbose = 0)\nprint('Training Gradient Boosting Model')\n\n#Fitting Model\ngbm.fit(X_train, y_train)\nprint('Model Fitting Completed')\n\n#Predicting\nprint('Starting Predictions!')\ny_pred = gbm.predict(X_test)\nprint('Prediction Completed!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of the model is:  ',accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can obtain the confusion matrix to see how the model has performed. From the confusion matrix, we can see that the model has predicted 125 observations correctly and 22 observations incorrectly."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('The confusion Matrix : \\n',cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{},"cell_type":"markdown","source":"## Step 4 - Visualisation and Identification of Important Features\n\nHere, I have used the 'feature_importances_' array of the Gradient Boosting Model to ascertain the most important features for the prediction of 'Attrition'.\n\nFrom the plot below, we can clearly see that thet following features hold a lot of weightage:\n1. Overtime\n2. StockOptionLevel\n3. JobSatisfaction\n4. JobLevel\n5. EnvironmentSatisfaction\n6. TotalWorkingYears\n\nWe can next plot these individually alongside Attrition to better understand the importance of each."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot \ntrace = go.Scatter(\n    y = gbm.feature_importances_,\n    x = df_transformed_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 10,\n        color = gbm.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = df_transformed_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Model Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Overtime\n\nFrom below, we can clearly see that Attrition is a bigger percentage of employees who did Overtime."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting Seaborn font-size\nsns.set(font_scale = 1)\n\n#Attrition based on Overtime\nax = sns.catplot(x=\"OverTime\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Overtime', ylabel = 'Number of Employees', title = 'Overtime')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Stock Option Level\n\nFrom below, we can clearly see that Attrition is higher for employees who dont have stock options. Employees with stock options are less likely to leave the organisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stock Option Level\nax = sns.catplot(x=\"StockOptionLevel\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Stock Option Level', ylabel = 'Number of Employees', title = 'Stock Option Level')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Job Satisfaction\n\nWhile the number of Attrition is similar for all groupings, for Job Satisfaction of level 1, Attrition as percentage of employees in that group is higher than the others. So, employees experiencing lower satisfaction are more likely to leave."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Job Satisfaction\nax = sns.catplot(x=\"JobSatisfaction\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Job Satisfaction', ylabel = 'Number of Employees', title = 'Job Satisfaction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Job Level\n\nWe can see that the employees at lower levels are more likely to leave the organisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#JobLevel\nax = sns.catplot(x=\"JobLevel\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Job Level', ylabel = 'Number of Employees', title = 'Job Level')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Environment Satisfaction\n\nWhile the number of Attrition is similar for all groupings, for Environment Satisfaction of level 1 and 2, Attrition as percentage of employees in those groups is higher than the others. So, employees experiencing lower satisfaction are more likely to leave."},{"metadata":{"trusted":true},"cell_type":"code","source":"#EnvironmentSatisfaction\nax = sns.catplot(x=\"EnvironmentSatisfaction\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Environment Satisfaction', ylabel = 'Number of Employees', title = 'Environment Satisfaction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 Years With Current Manager\n\nHere we can see that employees with low number of years with current manager are more likely to leave. Attrition is high when the employee has spent not even 1 year with the current manager."},{"metadata":{"trusted":true},"cell_type":"code","source":"#YearsWithCurrManager\nax = sns.catplot(x=\"TotalWorkingYears\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Total Working Years', ylabel = 'Number of Employees', title = 'Total Working Years')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}