{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction and uploading data**"},{"metadata":{},"cell_type":"markdown","source":"![](https://img.webmd.com/dtmcms/live/webmd/consumer_assets/site_images/articles/health_tools/did_you_know_this_could_lead_to_heart_disease_slideshow/493ss_thinkstock_rf_heart_illustration.jpg)<br>**Hi, everyone! That's my analysis and classification for Heart Disease UCI.** Here you can find general analysis, comparison between different variables and investigation of features importance. If you find my notebook interesting and helpful, please **UPVOTE.** Enjoy the analysis :)"},{"metadata":{},"cell_type":"markdown","source":"**Import packages**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style='ticks', rc={'figure.figsize':(15, 10)})\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Acquire data**"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA)**"},{"metadata":{},"cell_type":"markdown","source":"**Let's learn some info about our data.** For that I create a function which can show us missing ratio, distincts, skewness, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detailed_analysis(df):\n  obs = df.shape[0]\n  types = df.dtypes\n  counts = df.apply(lambda x: x.count())\n  nulls = df.apply(lambda x: x.isnull().sum())\n  distincts = df.apply(lambda x: x.unique().shape[0])\n  missing_ratio = (df.isnull().sum() / obs) * 100\n  uniques = df.apply(lambda x: [x.unique()])\n  skewness = df.skew()\n  kurtosis = df.kurt()\n  print('Data shape:', df.shape)\n\n  cols = ['types', 'counts', 'nulls', 'distincts', 'missing ratio', 'uniques', 'skewness', 'kurtosis']\n  details = pd.concat([types, counts, nulls, distincts, missing_ratio, uniques, skewness, kurtosis], axis=1)\n\n  details.columns = cols \n  dtypes = details.types.value_counts()\n  print('________________________\\nData types:\\n', dtypes)\n  print('________________________')\n\n  return details","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"details = detailed_analysis(data)\ndetails","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, our data is totally clear, so we can visualize some things"},{"metadata":{},"cell_type":"markdown","source":"**Target value distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"values = data.target.value_counts()\nindexes = values.index\n\nsns.barplot(indexes, values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pair plot between all variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=data, vars=data.columns.values[:-1], hue='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of different chest types and their influence to the target value**\n<br>Types of pain:\n- Value 0: typical angina\n- Value 1: atypical angina\n- Value 2: non-anginal pain\n- Value 3: asymptomatic"},{"metadata":{},"cell_type":"markdown","source":"Here we can see that people with the same chest pain have almost the same age regardless of the sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='cp', y='age', data=data, hue='sex', ci=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship between chest pain and different variables separated by target value.**\n1. Here we can find out that fbs has significantly various values which are dependent on the chest pain \n2. Resting ecg results with normal values mean that patient hasn't heart diseases (exception: asymptomatic chest pain, value 3)\n3. If exang is 1 a patient must be healthy (exception: asymptomatic chest pain, value 3)\n4. If oldpeak is high a patient must be healthy (exception: asymptomatic chest pain, value 3)\n5. It's better if slope has low value (again asymptomatic chest pain as an exception)\n6. High number of ca (major vessels) is always great\n7. It's good when thal nearly equals 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 25))\npalettes = ['Greens', 'Purples', 'YlOrRd', 'RdBu', 'BrBG', 'cubehelix'] * 2\n\nfor x in range(10):\n    fig1 = fig.add_subplot(5, 2, x+1)\n    sns.barplot(x='cp', y=data.columns.values[x+3], data=data, hue='target', ci=None, palette=palettes[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation heatmap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = data.corr()\n\nfig = plt.figure(figsize=(12, 10))\nsns.heatmap(correlation, annot=True, center=1, cmap='RdBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship between slope and oldpeak**\n<br><br>This plot confirms our statement that lower slope is better. According to the jointplot lower slope values have higher oldpeak values which mean our patient is healthy"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='slope', y='oldpeak', data=data, height=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Violin plots for all variables**\n<br><br>Here we can investigate things about features importance too. If plots for 0 and 1 are the same it means that correlation is low. Moreover we can see smooth values distribution for each variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 25))\npalettes = ['Greens', 'Purples', 'YlOrRd', 'RdBu', 'BrBG', 'cubehelix'] * 2\n\nfor x in range(12):\n    fig1 = fig.add_subplot(6, 2, x+1)\n    sns.violinplot(x='target', y=data.columns.values[x], data=data, palette=palettes[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SelectKBest**\n<br><br>Finally for EDA we're gonna check the best features using SelectKBest"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('target', axis=1)\ny = data.target\n\nselector = SelectKBest(score_func=chi2, k=5)\nfitted = selector.fit(X, y)\nfeatures_scores = pd.DataFrame(fitted.scores_)\nfeatures_columns = pd.DataFrame(X.columns)\n\nbest_features = pd.concat([features_columns, features_scores], axis=1)\nbest_features.columns = ['Feature', 'Score']\nbest_features.sort_values(by='Score', ascending=False, inplace=True)\nbest_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{},"cell_type":"markdown","source":"**Let's split our data to test and train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint('Input train shape', X_train.shape)\nprint('Output train shape', y_train.shape)\nprint('Input test shape', X_test.shape)\nprint('Output test shape', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we're gonna scale our data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\ntype(X_train), type(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So, we can test some classification algorithms on our data. Also we create a DataFrame to collect scores**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = pd.DataFrame(columns=['Model', 'Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Also we define a function to show additional metrics (Confusion Matrix and ROC Curve)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_metrics():\n    fig = plt.figure(figsize=(25, 10))\n\n    # Confusion matrix\n    fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n\n    # ROC Curve\n    fig.add_subplot(122)\n\n    ns_probs = [0 for _ in range(len(y_test))]\n    p_probs = model.predict_proba(X_test)[:, 1]\n\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, p_probs)\n\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(lr_fpr, lr_tpr, marker='o', label='Logistic')\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Logistic Regression', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Classifier (SVC)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(probability=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'SVC', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [50, 100, 200, 300], 'max_depth': [2, 3, 4, 5]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100, model.best_params_)\nscores = scores.append({'Model': 'Random Forest', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradien Boosting Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Gradient Boosting', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extra Trees Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid={'n_estimators': [50, 100, 200, 300], 'max_depth': [2, 3, 4, 5]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Extra Trees', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Neighbors Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [1, 2, 3]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'K-Neighbors', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Gaussian NB', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Decision Tree', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'XGB Classifier', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally, let's review our scores**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='Model', y='Score', data=scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top-3 are Random Forest, K-Neighbors and Extra Trees.**"},{"metadata":{},"cell_type":"markdown","source":"# Tuning and Ensemble Stacking"},{"metadata":{},"cell_type":"markdown","source":"**Ok, now let's tune XGBoost Classifier and try to get better score.** We select our params and model. We'll tune it gradually to save time. **At first we tune max_depth and min_child_weight**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'max_depth': range(2, 8, 2),\n  'min_child_weight': range(1, 8, 2)\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=6, min_child_weight=1, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's go deeper now"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'max_depth': [3, 4, 5],\n  'min_child_weight': [4, 5, 6]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=4, min_child_weight=5, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'min_child_weight': [1, 4, 6, 7, 8, 10, 12]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's tune gamma**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tune subsample and colsample_bytree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"  params = {\n    'subsample': [0.6, 0.7, 0.8, 0.9],\n    'colsample_bytree': [0.6, 0.7, 0.8, 0.9]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'subsample': [0.55, 0.6, 0.65],\n    'colsample_bytree': [0.65, 0.7, 0.75]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=0.6, colsample_bytree=0.7, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tune regularization parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n  'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=0.6, colsample_bytree=0.7, scale_pos_weight=1, seed=228, reg_alpha=0)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_, grid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's perform our new model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n              learning_rate=0.001, max_delta_step=0, max_depth=3,\n              min_child_weight=6, missing=None, n_estimators=5000, n_jobs=1,\n              nthread=4, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=228,\n              silent=True, subsample=0.6, tree_method='gpu_hist', verbosity=1)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'XGB Classifier', 'Score': accuracy}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Actually, I don't know why the tuned model performs worse than the standart one** even though we have better score while tuning. So, I'm probably gonna fix this later, but now we're gonna do **ensemble stacking**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here soon","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}