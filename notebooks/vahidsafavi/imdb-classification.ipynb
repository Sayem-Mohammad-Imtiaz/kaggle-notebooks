{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndataset.head()\ndataset.info\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Data Visualization Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(dataset['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization Wordclouds Plot"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# word cloud on positve reviews\npos_rev = ' '.join(df[df['sentiment']=='positive']['review'].to_list()[:100])\nwc = WordCloud(width = 600, height = 400, \n                    background_color ='white', \n                    stopwords = sw, \n                    min_font_size = 10, colormap='GnBu').generate(pos_rev)\nplt.imshow(wc)"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Cleaning the texts\nimport re\nimport nltk\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom bs4 import BeautifulSoup #To remove HTML tags\ncorpus = []\n\nfor i in range(0, 49999):\n    tweet = re.sub('[^a-zA-Z]', ' ', dataset.review[i])\n    tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)\n    tweet = re.sub(r\"that's\",\"that is\",tweet)\n    tweet = re.sub(r\"there's\",\"there is\",tweet)\n    tweet = re.sub(r\"what's\",\"what is\",tweet)\n    tweet = re.sub(r\"where's\",\"where is\",tweet)\n    tweet = re.sub(r\"it's\",\"it is\",tweet)\n    tweet = re.sub(r\"who's\",\"who is\",tweet)\n    tweet = re.sub(r\"i'm\",\"i am\",tweet)\n    tweet = re.sub(r\"she's\",\"she is\",tweet)\n    tweet = re.sub(r\"he's\",\"he is\",tweet)\n    tweet = re.sub(r\"they're\",\"they are\",tweet)\n    tweet = re.sub(r\"who're\",\"who are\",tweet)\n    tweet = re.sub(r\"ain't\",\"am not\",tweet)\n    tweet = re.sub(r\"wouldn't\",\"would not\",tweet)\n    tweet = re.sub(r\"shouldn't\",\"should not\",tweet)\n    tweet = re.sub(r\"can't\",\"can not\",tweet)\n    tweet = re.sub(r\"couldn't\",\"could not\",tweet)\n    tweet = re.sub(r\"won't\",\"will not\",tweet)\n    tweet = re.sub(r\"\\W\",\" \",tweet)\n    tweet = re.sub(r\"\\d\",\" \",tweet)\n    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet)\n    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet)\n    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet)\n    tweet = re.sub(r\"\\s+\",\" \",tweet)\n    tweet = tweet.lower()\n#   tweet = tweet.split()\n    corpus.append(tweet)\n    dataset.review[i] = tweet\n#dataset.review[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stopwords\nnltk.download('stopwords')\n# Stopword list\nstop_words = nltk.corpus.stopwords.words('english')\nspecific_wc = ['br', 'movie', 'film']\nsw = stop_words + specific_wc\n#print(sw)\n#print(len(sw))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.toktok import ToktokTokenizer\n\n#Tokenization of text\ntokenizer=ToktokTokenizer()\n#Setting English stopwords\nstopword_list=nltk.corpus.stopwords.words('english')\n#set stopwords to english\nspecific_wc = ['br', 'movie', 'film']\nstop = stopword_list + specific_wc\n#print(stop)\n#print(len(stop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n#Apply function on review column\ndataset['review']=dataset['review'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\nlabels = []\nfor ind, row in dataset.iterrows():\n    labels.append(row['sentiment'])\n    sentences.append(row[ 'review']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding labels \n\nenc = LabelEncoder()\nencoded_labels = enc.fit_transform(labels)\nprint(enc.classes_)\nprint(labels[:5])\nprint(encoded_labels[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(dataset['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model parameters\n\nvocab_size = 1000\nembedding_dim = 16\nmax_length = 1000\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_portion = .8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\n# ---------------\n\n# proportion of training dataset\ntrain_size = int(len(sentences) * training_portion)\n\n# training dataset\ntrain_sentences = sentences[:train_size]\ntrain_labels = encoded_labels[:train_size]\n\n# validation dataset\nvalidation_sentences = sentences[train_size:]\nvalidation_labels = encoded_labels[train_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing, sequencing, padding features\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_padded.shape)\nprint(validation_padded.shape)\nprint(train_labels.shape)\nprint(validation_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With LSTM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model fit\nnum_epochs = 1\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=1, \n                    validation_data=(validation_padded, validation_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy and loss\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model\nmodel.save(\"IMDB_model_ LSTM.h5\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib.dump(Tokenizer,'IMDB_scaler.pkl')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}