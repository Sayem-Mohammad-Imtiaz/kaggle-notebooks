{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **How to build a real estate price prediction website**  \n\nThis data science project series walks through step by step process of how to build a real estate price prediction website.  \n\n#### **Step-1:**  \nWe will first build a model using sklearn and linear regression using banglore home prices dataset from kaggle.  \n[Bengaluru House Price Dataset](https://www.kaggle.com/amitabhajoy/bengaluru-house-price-data)\n#### **Step-2:**  \nSecond step would be to write a python flask server that uses the saved model to serve http requests.  \n#### **Step-3:**\nThird component is the website built in html, css and javascript that allows user to enter home square ft area, bedrooms etc and it will call python flask server to retrieve the predicted price. \n\n#### **Summary**\nDuring model building we will cover almost all data science concepts such as data load and cleaning, outlier detection and removal, feature engineering, dimensionality reduction, gridsearchcv for hyperparameter tunning, k fold cross validation etc.  \nTechnology and tools wise this project covers;  \n*  Python\n*  Numpy and Pandas for data cleaning\n*  Matplotlib for data visualization\n*  Sklearn for model building\n*  Jupyter notebook, visual studio code and pycharm as IDE\n*  Python flask for http server\n*  HTML/CSS/Javascript for UI  ","metadata":{}},{"cell_type":"markdown","source":"## **Building House Price Prediction Model**","metadata":{}},{"cell_type":"markdown","source":"### **TO find path of dataset**","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T18:34:57.089002Z","iopub.execute_input":"2021-06-04T18:34:57.089427Z","iopub.status.idle":"2021-06-04T18:34:57.111153Z","shell.execute_reply.started":"2021-06-04T18:34:57.089342Z","shell.execute_reply":"2021-06-04T18:34:57.11012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Importing required libraries and loading data**","metadata":{}},{"cell_type":"code","source":"# important python libraries for machine learning\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt # visualizing data\nimport seaborn as sns # visualizing data with stunning default theme\nimport sklearn # contain algorithms\nplt.rcParams[\"figure.figsize\"] = (20,10)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# load dataset from input directory\ndf = pd.read_csv(\"../input/bengaluru-house-price-data/Bengaluru_House_Data.csv\") \ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:57.132113Z","iopub.execute_input":"2021-06-04T18:34:57.132501Z","iopub.status.idle":"2021-06-04T18:34:58.125723Z","shell.execute_reply.started":"2021-06-04T18:34:57.132468Z","shell.execute_reply":"2021-06-04T18:34:58.124591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.127521Z","iopub.execute_input":"2021-06-04T18:34:58.127965Z","iopub.status.idle":"2021-06-04T18:34:58.134477Z","shell.execute_reply.started":"2021-06-04T18:34:58.127919Z","shell.execute_reply":"2021-06-04T18:34:58.13336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To check categories in a are_type column**  \nIt is helpful to analyze the dataset for categorical balance of data.","metadata":{}},{"cell_type":"code","source":"df.groupby('area_type')['area_type'].agg('count')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.136641Z","iopub.execute_input":"2021-06-04T18:34:58.137061Z","iopub.status.idle":"2021-06-04T18:34:58.159149Z","shell.execute_reply.started":"2021-06-04T18:34:58.137019Z","shell.execute_reply":"2021-06-04T18:34:58.158117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To remove columns**  \nTo make the project simple for beginners, we assume *areatype, society, balcony, availability* columns unuseful and remove these columns.","metadata":{}},{"cell_type":"code","source":"#dropping some columns\ndf2 = df.drop(['area_type','society','balcony','availability'],axis='columns')\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.161045Z","iopub.execute_input":"2021-06-04T18:34:58.161352Z","iopub.status.idle":"2021-06-04T18:34:58.177575Z","shell.execute_reply.started":"2021-06-04T18:34:58.161324Z","shell.execute_reply":"2021-06-04T18:34:58.17644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Cleaning**  \nIn this process, we handle the Null/missing values and duplicate values.","metadata":{}},{"cell_type":"code","source":"#before dropping null value, lets check it column-wise\ndf2.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.179298Z","iopub.execute_input":"2021-06-04T18:34:58.17962Z","iopub.status.idle":"2021-06-04T18:34:58.192838Z","shell.execute_reply.started":"2021-06-04T18:34:58.179588Z","shell.execute_reply":"2021-06-04T18:34:58.191443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can fill the missing-values using median but\n# here the missing values are less compare to dataset size, so we are dropping\ndf3 = df2.dropna()\ndf3.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.19432Z","iopub.execute_input":"2021-06-04T18:34:58.194654Z","iopub.status.idle":"2021-06-04T18:34:58.236741Z","shell.execute_reply.started":"2021-06-04T18:34:58.194616Z","shell.execute_reply":"2021-06-04T18:34:58.235771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to drop duplicate values\ndf4 = df3.drop_duplicates()\nprint(\"Dataset size before dropping duplicate values: {} and after {}\".format(df3.shape, df4.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.237932Z","iopub.execute_input":"2021-06-04T18:34:58.23822Z","iopub.status.idle":"2021-06-04T18:34:58.252484Z","shell.execute_reply.started":"2021-06-04T18:34:58.238187Z","shell.execute_reply":"2021-06-04T18:34:58.251379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To transform the column into appropriate datatype or category**","metadata":{}},{"cell_type":"code","source":"#lets check size column\ndf4['size'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.256302Z","iopub.execute_input":"2021-06-04T18:34:58.256631Z","iopub.status.idle":"2021-06-04T18:34:58.264141Z","shell.execute_reply.started":"2021-06-04T18:34:58.256601Z","shell.execute_reply":"2021-06-04T18:34:58.263251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from above analysis, we found the datatype inappropriate for ml-model\n#4-Bedroom and 4 BHK are same and so on. We create new column with integer type and \n# convert the given size-column. We don't drop size column for later use.\ndf4['bhk'] = df4['size'].apply(lambda x: int(x.split(' ')[0]))\ndf4.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.265824Z","iopub.execute_input":"2021-06-04T18:34:58.26635Z","iopub.status.idle":"2021-06-04T18:34:58.302651Z","shell.execute_reply.started":"2021-06-04T18:34:58.266304Z","shell.execute_reply":"2021-06-04T18:34:58.301697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4['bhk'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.303979Z","iopub.execute_input":"2021-06-04T18:34:58.304272Z","iopub.status.idle":"2021-06-04T18:34:58.312615Z","shell.execute_reply.started":"2021-06-04T18:34:58.304243Z","shell.execute_reply":"2021-06-04T18:34:58.311459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To check total_sqft column**","metadata":{}},{"cell_type":"code","source":"df4.total_sqft.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.314423Z","iopub.execute_input":"2021-06-04T18:34:58.314782Z","iopub.status.idle":"2021-06-04T18:34:58.327284Z","shell.execute_reply.started":"2021-06-04T18:34:58.314695Z","shell.execute_reply":"2021-06-04T18:34:58.32613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the above analysis shows an inappropriate data in total_sqft column\ndef is_float(x):\n    try:\n        float(x)\n    except:\n        return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.328449Z","iopub.execute_input":"2021-06-04T18:34:58.328752Z","iopub.status.idle":"2021-06-04T18:34:58.33918Z","shell.execute_reply.started":"2021-06-04T18:34:58.328701Z","shell.execute_reply":"2021-06-04T18:34:58.338378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we use ~ negative opperator to show the inappropriate data\ndf4[~df4['total_sqft'].apply(is_float)].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.34031Z","iopub.execute_input":"2021-06-04T18:34:58.340573Z","iopub.status.idle":"2021-06-04T18:34:58.371032Z","shell.execute_reply.started":"2021-06-04T18:34:58.340548Z","shell.execute_reply":"2021-06-04T18:34:58.369929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_sqft_to_num(x):\n    tokens = x.split('-')\n    if len(tokens)==2:\n        return (float(tokens[0]) + float(tokens[1]))/2\n    try:\n        return float(x)\n    except:\n        return None","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.3724Z","iopub.execute_input":"2021-06-04T18:34:58.372695Z","iopub.status.idle":"2021-06-04T18:34:58.383808Z","shell.execute_reply.started":"2021-06-04T18:34:58.372667Z","shell.execute_reply":"2021-06-04T18:34:58.382889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5 = df4.copy()\ndf5['total_sqft'] = df5['total_sqft'].apply(convert_sqft_to_num)\ndf5.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.384863Z","iopub.execute_input":"2021-06-04T18:34:58.385276Z","iopub.status.idle":"2021-06-04T18:34:58.422612Z","shell.execute_reply.started":"2021-06-04T18:34:58.385229Z","shell.execute_reply":"2021-06-04T18:34:58.421796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.423752Z","iopub.execute_input":"2021-06-04T18:34:58.424206Z","iopub.status.idle":"2021-06-04T18:34:58.434872Z","shell.execute_reply.started":"2021-06-04T18:34:58.424161Z","shell.execute_reply":"2021-06-04T18:34:58.433841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got other missing values on applying our function because we put None for values with units like meters etc and just convert simple value or range value(x1-x2) into float","metadata":{}},{"cell_type":"code","source":"# dropping the created missing values with our convert_sqft_to_num\ndf5=df5.dropna()\ndf5.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.436238Z","iopub.execute_input":"2021-06-04T18:34:58.436598Z","iopub.status.idle":"2021-06-04T18:34:58.458115Z","shell.execute_reply.started":"2021-06-04T18:34:58.436568Z","shell.execute_reply":"2021-06-04T18:34:58.457073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Feature Engineering**  \nIn this step, we add new feature which will be helpful for outlier detection and removal later-on.\nWe will also refine categorical data in location column for one hot-encoding later-on.","metadata":{}},{"cell_type":"code","source":"df6 = df5.copy()\n# the given price is in lac unit and we are converting it into rupees\ndf6[\"price_per_sqft\"] = df6['price']*100000/df6['total_sqft']\ndf6.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.45976Z","iopub.execute_input":"2021-06-04T18:34:58.460162Z","iopub.status.idle":"2021-06-04T18:34:58.481666Z","shell.execute_reply.started":"2021-06-04T18:34:58.460119Z","shell.execute_reply":"2021-06-04T18:34:58.480571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To check the number of unique categories in location column**","metadata":{}},{"cell_type":"code","source":"len(df6.location.unique())","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.483164Z","iopub.execute_input":"2021-06-04T18:34:58.483567Z","iopub.status.idle":"2021-06-04T18:34:58.491945Z","shell.execute_reply.started":"2021-06-04T18:34:58.483526Z","shell.execute_reply":"2021-06-04T18:34:58.49082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1298 is high dimensionality problem. if we apply one hot-encoding on this,\n# we will get high number of feature.\n# we will check the number of rows for each category and will make some threshold\n# for keeping the category. Obviously categories with less rows(samples)\n# will be placed in 'other' category.\ndf6.location = df6.location.apply(lambda x: x.strip()) # remove leading or end spaces\nlocation_stats = df6.groupby('location')['location'].agg('count').sort_values(ascending=False)\nlocation_stats","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.495377Z","iopub.execute_input":"2021-06-04T18:34:58.495702Z","iopub.status.idle":"2021-06-04T18:34:58.518637Z","shell.execute_reply.started":"2021-06-04T18:34:58.495662Z","shell.execute_reply":"2021-06-04T18:34:58.517743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_stats[location_stats <= 10])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.519982Z","iopub.execute_input":"2021-06-04T18:34:58.520446Z","iopub.status.idle":"2021-06-04T18:34:58.527681Z","shell.execute_reply.started":"2021-06-04T18:34:58.520403Z","shell.execute_reply":"2021-06-04T18:34:58.52665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we add all these 1057 unique categories with less or equal to 10 rows into 'other' category\nlocation_stats_less_than_ten = location_stats[location_stats<=10]\ndf6.location = df6.location.apply(lambda x: 'other' if x in location_stats_less_than_ten else x)\ndf6.location.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.529013Z","iopub.execute_input":"2021-06-04T18:34:58.529539Z","iopub.status.idle":"2021-06-04T18:34:58.56465Z","shell.execute_reply.started":"2021-06-04T18:34:58.529491Z","shell.execute_reply":"2021-06-04T18:34:58.563547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Outlier Detection and Removal**  \nOutliers are the data points which are data errors but some time they represent extrem variation.\nWe can use techniques like:\n* Standard deviation\n* Domain knowledge","metadata":{}},{"cell_type":"markdown","source":"#### **Let, we have been told for a room per sqft threshold as 300**  \nAs a data-scientist we will check our dataset for outliers which includes rooms with less then the \ngiven threshold. We will remove such samples from data considering as inappropiate.","metadata":{}},{"cell_type":"code","source":"df6[df6.total_sqft/df6.bhk <300].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.568661Z","iopub.execute_input":"2021-06-04T18:34:58.569001Z","iopub.status.idle":"2021-06-04T18:34:58.588791Z","shell.execute_reply.started":"2021-06-04T18:34:58.568969Z","shell.execute_reply":"2021-06-04T18:34:58.587845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to remove the outiers\ndf7 = df6[~(df6.total_sqft/df6.bhk <300)]\nprint(\"Data Size before outlier removal: {} and after: {}\".format(df6.shape, df7.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.590468Z","iopub.execute_input":"2021-06-04T18:34:58.590797Z","iopub.status.idle":"2021-06-04T18:34:58.601574Z","shell.execute_reply.started":"2021-06-04T18:34:58.590754Z","shell.execute_reply":"2021-06-04T18:34:58.600526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Now we check price per sqft if that is feasible or not**","metadata":{}},{"cell_type":"code","source":"df7.price_per_sqft.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.603052Z","iopub.execute_input":"2021-06-04T18:34:58.6034Z","iopub.status.idle":"2021-06-04T18:34:58.617723Z","shell.execute_reply.started":"2021-06-04T18:34:58.603361Z","shell.execute_reply":"2021-06-04T18:34:58.616599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above detail shows that according to the selected areas there isn't any area with such min or max area per sqft price. It clearly shows the variation in the given data.","metadata":{}},{"cell_type":"code","source":"def remove_pps_outliers(df):\n    df_out = pd.DataFrame()\n    for key, subdf in df.groupby('location'):\n        m = np.mean(subdf.price_per_sqft)\n        st = np.std(subdf.price_per_sqft)\n        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft <=(m+st))]\n        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.619459Z","iopub.execute_input":"2021-06-04T18:34:58.619758Z","iopub.status.idle":"2021-06-04T18:34:58.6265Z","shell.execute_reply.started":"2021-06-04T18:34:58.61973Z","shell.execute_reply":"2021-06-04T18:34:58.6253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df8 = remove_pps_outliers(df7)\ndf8.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:58.627559Z","iopub.execute_input":"2021-06-04T18:34:58.627883Z","iopub.status.idle":"2021-06-04T18:34:59.310705Z","shell.execute_reply.started":"2021-06-04T18:34:58.627853Z","shell.execute_reply":"2021-06-04T18:34:59.309704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **To check the price for 2 and 3 bedroom in same location and equal area**  \nIn this step, we can learn how to think about cleaning the dataset with outliers and wrong data.\nSo we will check that if the area is same but 2-bedroom cost is high than 3-bedroom, it means there are still false data in our dataset.","metadata":{}},{"cell_type":"code","source":"def plot_scatter_chart(df,location):\n    bhk2 = df[(df.location == location) & (df.bhk==2)]\n    bhk3 = df[(df.location == location) & (df.bhk==3)]\n    plt.rcParams['figure.figsize'] = (15,10)\n    plt.scatter(bhk2.total_sqft,bhk2.price, color='blue', label='2 BHK', s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='green',label='3 BHK', s=50)\n    plt.xlabel(\"Total Square Feet Area\")\n    plt.ylabel(\"Price\")\n    plt.title(location)\n    plt.legend()\n\n# we can check for different locations\nplot_scatter_chart(df8,'Rajaji Nagar')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:59.311927Z","iopub.execute_input":"2021-06-04T18:34:59.31224Z","iopub.status.idle":"2021-06-04T18:34:59.587228Z","shell.execute_reply.started":"2021-06-04T18:34:59.312185Z","shell.execute_reply":"2021-06-04T18:34:59.586169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we remove those bedroom appertments whose price_per_sqft is less then the mean of 1-less bedroom appartments.","metadata":{}},{"cell_type":"code","source":"def remove_bhk_outliers(df):\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft < (stats['mean'])].index.values)\n    return df.drop(exclude_indices, axis='index')\n\ndf9=remove_bhk_outliers(df8)\ndf9.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:34:59.588773Z","iopub.execute_input":"2021-06-04T18:34:59.589195Z","iopub.status.idle":"2021-06-04T18:35:00.505806Z","shell.execute_reply.started":"2021-06-04T18:34:59.589151Z","shell.execute_reply":"2021-06-04T18:35:00.505057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let re-check the price using our defined scatter plot function\nplot_scatter_chart(df9,'Rajaji Nagar')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:35:00.506803Z","iopub.execute_input":"2021-06-04T18:35:00.507194Z","iopub.status.idle":"2021-06-04T18:35:00.730929Z","shell.execute_reply.started":"2021-06-04T18:35:00.507166Z","shell.execute_reply":"2021-06-04T18:35:00.730148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can compare the plot_scatter_chart function for our df8 and new dataframe df9 as shown above. We have removed all the samples with false information.","metadata":{}},{"cell_type":"markdown","source":"### **Use Histogram to find the distribution of data w.r.t price_per_sqft**","metadata":{}},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (20,10)\nplt.hist(df9.price_per_sqft,rwidth=0.8)\nplt.xlabel(\"Price Per Square Feet\")\nplt.ylabel(\"Count\")","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:35:00.731905Z","iopub.execute_input":"2021-06-04T18:35:00.732318Z","iopub.status.idle":"2021-06-04T18:35:00.925069Z","shell.execute_reply.started":"2021-06-04T18:35:00.732289Z","shell.execute_reply":"2021-06-04T18:35:00.924133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **We check number of bathroom in the dataset for outliers**\nLets we finalize in team meeting to remove the samples(rows) with \nnumber_of_bathrooms > number_of_bedrooms + 2","metadata":{}},{"cell_type":"code","source":"# lets first check if there is any such case\ndf9[df9.bath>df9.bhk+2]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:35:00.926264Z","iopub.execute_input":"2021-06-04T18:35:00.926569Z","iopub.status.idle":"2021-06-04T18:35:00.94235Z","shell.execute_reply.started":"2021-06-04T18:35:00.926542Z","shell.execute_reply":"2021-06-04T18:35:00.941209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets remove the rows that doesn't satisfy the threshold of bathrooms\ndata = df9[df9.bath<df9.bhk+2]\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:35:00.943938Z","iopub.execute_input":"2021-06-04T18:35:00.94433Z","iopub.status.idle":"2021-06-04T18:35:00.961106Z","shell.execute_reply.started":"2021-06-04T18:35:00.94429Z","shell.execute_reply":"2021-06-04T18:35:00.960361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Now our data is much clean**  \nWe will remove the features(columns) that are unnecessary for machine learning model.\nSuch as 'size' and price_per_sqft. We need these for outlier detection but aren't useful for ml-model. ","metadata":{}},{"cell_type":"code","source":"data = data.drop(['size','price_per_sqft'], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:35:00.962201Z","iopub.execute_input":"2021-06-04T18:35:00.962662Z","iopub.status.idle":"2021-06-04T18:35:00.989444Z","shell.execute_reply.started":"2021-06-04T18:35:00.962629Z","shell.execute_reply":"2021-06-04T18:35:00.988344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Building Machine-Learning Model**  \nYet, we have sting for location column which cannot be interpreted through machine-learning model. To convert the text into numeric values, we use one hot-encoding technique using pandas dummy.","metadata":{}},{"cell_type":"code","source":"dummies = pd.get_dummies(data.location)\ndummies.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:45:46.171433Z","iopub.execute_input":"2021-06-04T18:45:46.172047Z","iopub.status.idle":"2021-06-04T18:45:46.198481Z","shell.execute_reply.started":"2021-06-04T18:45:46.171996Z","shell.execute_reply":"2021-06-04T18:45:46.197427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to avoid dummy trap, we will drop one column from dummy data \n# and consider 0 value instead of that column value\ndata2 = pd.concat([data,dummies.drop('other',axis='columns')], axis='columns')\ndata2.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:46:01.958459Z","iopub.execute_input":"2021-06-04T18:46:01.959114Z","iopub.status.idle":"2021-06-04T18:46:01.996133Z","shell.execute_reply.started":"2021-06-04T18:46:01.959058Z","shell.execute_reply":"2021-06-04T18:46:01.995048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can drop location column because that is converted into numeric\ndata3 = data2.drop('location',axis='columns')\ndata3.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:46:28.179219Z","iopub.execute_input":"2021-06-04T18:46:28.179649Z","iopub.status.idle":"2021-06-04T18:46:28.20862Z","shell.execute_reply.started":"2021-06-04T18:46:28.179611Z","shell.execute_reply":"2021-06-04T18:46:28.207786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Separating the dependent and independent features**","metadata":{}},{"cell_type":"code","source":"X = data3.drop('price',axis=\"columns\")\nY = data3.price\nprint(\"Depented Features: {}    Independent Feature: {}\".format(X.shape, Y.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T18:52:48.24962Z","iopub.execute_input":"2021-06-04T18:52:48.250095Z","iopub.status.idle":"2021-06-04T18:52:48.258192Z","shell.execute_reply.started":"2021-06-04T18:52:48.250054Z","shell.execute_reply":"2021-06-04T18:52:48.257193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Spliting the data for training and testing**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:09:28.914616Z","iopub.execute_input":"2021-06-04T19:09:28.915241Z","iopub.status.idle":"2021-06-04T19:09:29.054327Z","shell.execute_reply.started":"2021-06-04T19:09:28.9152Z","shell.execute_reply":"2021-06-04T19:09:29.053118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training and testing Linear Regression model**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n#build model\nlr_clf = LinearRegression()\n# training our model\nlr_clf.fit(X_train,Y_train)\n#testing our model\nlr_clf.score(X_test, Y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:51:08.722695Z","iopub.execute_input":"2021-06-04T19:51:08.723146Z","iopub.status.idle":"2021-06-04T19:51:08.846051Z","shell.execute_reply.started":"2021-06-04T19:51:08.723106Z","shell.execute_reply":"2021-06-04T19:51:08.844284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **evaluate machine learning model using k-fold cross-validation**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\ncross_val_score(LinearRegression(), X, Y, cv=cv)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:51:15.795844Z","iopub.execute_input":"2021-06-04T19:51:15.796254Z","iopub.status.idle":"2021-06-04T19:51:17.876217Z","shell.execute_reply.started":"2021-06-04T19:51:15.796221Z","shell.execute_reply":"2021-06-04T19:51:17.874948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Using GridSearchCV method to find best algorithm for our model**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef find_best_model_using_gridsearchcv(X,Y):\n    algos = {\n        'linear_regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n                }\n             },\n             'lasso':{\n                 'model': Lasso(),\n                 'params': {\n                     'alpha': [1,2],\n                     'selection': ['random', 'cyclic']\n                     }\n                  },\n              'decision_tree': {\n                  'model': DecisionTreeRegressor(),\n                  'params': {\n                      'criterion': ['mse', 'friedman_mse'],\n                      'splitter': ['best', 'random']\n                      }\n                  }\n              }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    for algo_name, config in algos.items():\n        gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score = False)\n        gs.fit(X,Y)\n        scores.append({\n            'model': algo_name,\n            'best_score': gs.best_score_,\n            'best_params': gs.best_params_\n            })\n            \n    return pd.DataFrame(scores, columns=['model','best_score', 'best_params'])\n    \nfind_best_model_using_gridsearchcv(X,Y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:51:33.022555Z","iopub.execute_input":"2021-06-04T19:51:33.023006Z","iopub.status.idle":"2021-06-04T19:51:39.485227Z","shell.execute_reply.started":"2021-06-04T19:51:33.022963Z","shell.execute_reply":"2021-06-04T19:51:39.484255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is found that Linear Regression model performs well and should be selected for price prediction","metadata":{}},{"cell_type":"code","source":"# lr_cfr is already trained\n# we write a function to predict prices for some data\ndef predict_price(location,area,bathroom, bedroom):\n    loc_index = np.where(X.columns==location)[0][0]\n    \n    x=np.zeros(len(X.columns))\n    x[0] = area\n    x[1] = bathroom\n    x[2] = bedroom\n    if loc_index >=0:\n        x[loc_index] = 1\n    return lr_clf.predict([x])[0]\n\npredict_price('1st Phase JP Nagar',1000,3,3)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:43:35.011396Z","iopub.execute_input":"2021-06-04T19:43:35.011896Z","iopub.status.idle":"2021-06-04T19:43:35.022727Z","shell.execute_reply.started":"2021-06-04T19:43:35.011855Z","shell.execute_reply":"2021-06-04T19:43:35.021218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **To export(save) the model into pickle file to use in website for prediction**","metadata":{}},{"cell_type":"code","source":"import pickle\nmodel_file = \"banglore_home_prices_model.pickle\"\nwith open(model_file,'wb') as f:\n    pickle.dump(lr_clf,f)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T20:11:09.34857Z","iopub.execute_input":"2021-06-04T20:11:09.348998Z","iopub.status.idle":"2021-06-04T20:11:09.354135Z","shell.execute_reply.started":"2021-06-04T20:11:09.348962Z","shell.execute_reply":"2021-06-04T20:11:09.353266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **We also store the columns name for later use in website**\n","metadata":{}},{"cell_type":"code","source":"import json\ncolumns = {\n    'data_columns' : [col.lower() for col in X.columns]\n    }\nwith open('columns.json', 'w') as f:\n    f.write(json.dumps(columns))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T19:53:01.060745Z","iopub.execute_input":"2021-06-04T19:53:01.061169Z","iopub.status.idle":"2021-06-04T19:53:01.067866Z","shell.execute_reply.started":"2021-06-04T19:53:01.061134Z","shell.execute_reply":"2021-06-04T19:53:01.06676Z"},"trusted":true},"execution_count":null,"outputs":[]}]}