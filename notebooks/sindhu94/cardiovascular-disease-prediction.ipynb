{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv',sep=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outlier Detection\n#Systolic BP(ap_hi) not greater than 370 and not less than 70\n#Dialstolic BP(ap_lo) not greater than 360 and not less than 50\noutlier = (data['ap_hi'] >= 370)| (data['ap_hi'] <= 70)|(data['ap_lo']>= 360)|(data['ap_lo']<=50)\ndata[outlier].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[~outlier]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BMI is an important feature in predicting the Cardio vascular disease. As this feature is not available, we can calculate \n# it using Height and Weight.\n#First convert Height in metres and square it\ndef bmi_conversion(x):\n    x = (x/100)**2\n    return x\ndata['height1'] = data['height'].apply(lambda x: bmi_conversion(x))\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['BMI'] = data['weight']/data['height1']\ndata['age'] = data['age'].apply(lambda x: round(x/365))\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data['cardio'],pd.to_numeric(data['ap_hi']),label = 'Systolic')\nplt.scatter(data['cardio'],data['BMI'],label = 'BMI')            \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cardio = data['cardio']\ncardio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping columns ID and Height1 as they are not important and cardio as it is dependent variable\ndata.drop(['id','height1','cardio'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data into training and testing sets\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr,X_ts,Y_tr,Y_ts = train_test_split(data,cardio,test_size = 0.2,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_tr,Y_tr)\nY_pred = lr.predict(X_ts)\nfpr1,tpr1,thresholds = roc_curve(Y_ts,lr.predict_proba(X_ts)[:,1])\nlr_a = auc(fpr1,tpr1)\nlr_acc = lr.score(X_ts,Y_ts)\nprint('AUC of Logistic Regression:',lr_a)\nprint('Accuracy of Logistic Regression:',lr_acc)\nplt.figure()\nplt.plot(fpr1,tpr1)\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nY_pred = lr.predict(X_ts)\nlr_cm = confusion_matrix(Y_ts,Y_pred)\nlr_cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sensitivity calculation\nlr_sen = lr_cm[0,0]/(lr_cm[0,0]+lr_cm[0,1])\nprint('Sensitivity of Logistic regression:',lr_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GridSearch cross validation for logistic regresssion\nfrom sklearn.model_selection import GridSearchCV\nlog_reg = LogisticRegression(solver=\"liblinear\")\ngrid = {\"penalty\" : [\"l1\",\"l2\"], \"C\" : np.arange(10,50,5)}\nlog_reg_cv = GridSearchCV(log_reg, grid, cv = 3)\nlog_reg_cv.fit(X_tr,Y_tr)\nprint(\"Tuned hyperparameter: {}\".format(log_reg_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best model\nlogreg_best = LogisticRegression(C = 25, penalty = \"l1\",solver = \"liblinear\")\nlogreg_best.fit(X_tr,Y_tr)\nprobs = logreg_best.predict_proba(X_ts)[:,1]\nfpr2,tpr2,thresholds = roc_curve(Y_ts,probs)\nlr_best_a = auc(fpr2,tpr2)\nlr_best_acc = logreg_best.score(X_ts,Y_ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC of Logistic Regression(tuned):',lr_best_a)\nprint('Accuracy of Logistic Regression(tuned):',lr_best_acc)\nplt.figure()\nplt.plot(fpr2,tpr2,label = 'With Tuning')\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix for best result and sentivity calculation\nfrom sklearn.metrics import confusion_matrix\nY_pred = logreg_best.predict(X_ts)\nlog_cm = confusion_matrix(Y_ts,Y_pred)\nlog_cm\nlog_sen = log_cm[0,0]/(log_cm[0,0]+log_cm[0,1])\nprint('Sensitivity of Logistic regression:',log_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calibration\nfrom sklearn import metrics\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nlr = LogisticRegression(solver = 'liblinear',C=30)\nlr.fit(X_tr,Y_tr)\nprobs_tr = lr.predict_proba(X_tr)[:,1]\nprobs_ts = lr.predict_proba(X_ts)[:,1]\ncur = calibration_curve(Y_ts,probs_ts,n_bins=10)\ncur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.plot(cur[1],cur[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_tr,Y_tr)\nprobs = rf.predict_proba(X_ts)[:,1]\nfpr1,tpr1,thresholds = roc_curve(Y_ts,probs)\nrf_a = auc(fpr1,tpr1)\nrf_acc = rf.score(X_ts,Y_ts)\nprint('AUC of Random Forest:',rf_a)\nprint('Accuracy of Random Forest:',rf_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr1,tpr1)\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - Random Forest')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame()\nres['columns'] = data.columns.tolist()\nres['vals'] = rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Order of features which are contributing most to the prediction\nres = res.sort_values('vals',ascending = False)\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the AUC on increasing the variables according to their importance. Considering number of trees as 100\nr1 = []\nfor i in range(1,len(res.index)):\n    c = list(res[:i]['columns'])\n    print(c)\n    rf = RandomForestClassifier(n_estimators = 100)\n    rf.fit(X_tr[c],Y_tr)\n    probs = rf.predict_proba(X_ts[c])[:,1]\n    fpr,tpr,thresholds = roc_curve(Y_ts,probs)\n    a=auc(fpr,tpr)\n    print(a)\n    r1.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(list(range(len(r1))),r1)\nplt.xlabel('Num of Variables')\nplt.ylabel('AUC')\nplt.title('Increase in Area Under Curve on addition of variables')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AUC for train set on changing the number of trees\ntr_results = []\nts_results = []\nfor i in range(10,200,10):\n    rf = RandomForestClassifier(n_estimators = i,n_jobs = -1)\n    rf.fit(X_tr,Y_tr)\n    train_pred = rf.predict(X_tr)\n    fpr,tpr,thresholds = roc_curve(Y_tr,train_pred)\n    roc_auc = auc(fpr,tpr)\n    tr_results.append(roc_auc)\n#AUC for test set on changing number of trees    \n    Y_proba = rf.predict_proba(X_ts)[:,1]\n    fpr,tpr,thresholds = roc_curve(Y_ts,Y_proba)\n    roc_auc = auc(fpr,tpr)\n    ts_results.append(roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line1 = plt.plot(list(range(10,200,10)),tr_results,'b',label = \"Train AUC\")\nline2 = plt.plot(list(range(10,200,10)),ts_results,'r',label = \"Test AUC\")\nplt.xlabel('n-estimators')\nplt.ylabel('AUC')\nplt.title('AUC for Random Forest on Train and Test set')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#ROC on changing the size of train data\nr = []\nfor i in range(1,1000,10):\n    s = int(i/1000*len(X_tr.index))\n    print(s)\n    rf = RandomForestClassifier(n_estimators = 30)\n    rf.fit(X_tr[:s],Y_tr[:s])\n    probs = rf.predict_proba(X_ts)[:,1]\n    fpr,tpr,thresholds = roc_curve(Y_ts,probs)\n    a=auc(fpr,tpr)\n    print(a)\n    r.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(list(range(1,1000,10)),r)\nplt.xlabel('Length of train data')\nplt.ylabel('AUC')\nplt.title('AUC on increasing the size of train data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter_optimizationR={'criterion':('gini','entropy'),\n                       'max_depth':(1,2,3,4,5,6), 'max_features':('auto','log2'),'n_estimators':(10,20,30,50,70)}\nrandomforest_gridcv=GridSearchCV(RandomForestClassifier(),parameter_optimizationR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest_gridcv.fit(X_tr,Y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest_gridcv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best = RandomForestClassifier(n_estimators = 10,max_depth = 6,max_features = 'auto000',criterion = 'entropy')\nrf_best.fit(X_tr,Y_tr)\nprobs = rf_best.predict_proba(X_ts)[:,1]\nfpr2,tpr2,thresholds = roc_curve(Y_ts,probs)\nrf_best_a = auc(fpr2,tpr2)\nrf_best_acc = rf_best.score(X_ts,Y_ts)\nprint('AUC of Random Forest(tuned):',rf_best_a)\nprint('Accuracy of Random Forest(tuned):',rf_best_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr1,tpr1,label = 'Without tuning')\nplt.plot(fpr2,tpr2,label = 'With tuning')\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - Random Forest')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred=rf_best.predict(X_ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\ncr = classification_report(Y_ts,Y_pred)\nrf_cm = confusion_matrix(Y_ts,Y_pred)\nrf_sen = rf_cm[0,0]/(rf_cm[0,0]+rf_cm[0,1])\nprint('Sensitivity of Random Forest:',rf_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy for best model:\",rf_best.score(X_ts,Y_ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame()\nres['columns'] = data.columns.tolist()\nres['vals'] = rf_best.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Order of features which are contributing most to the prediction\nres = res.sort_values('vals',ascending = False)\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = []\naccuracy = []\nfor i in range(1,len(res.index)):\n    c = list(res[:i]['columns'])\n    print(c)\n    rf_best.fit(X_tr[c],Y_tr)\n    probs = rf_best.predict_proba(X_ts[c])[:,1]\n    fpr,tpr,thresholds = roc_curve(Y_ts,probs)\n    a=auc(fpr,tpr)\n    acc = rf_best.score(X_ts[c],Y_ts)\n    print(a,acc)\n    r2.append(a)\n    accuracy.append(acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(list(range(len(r1))),r1,label = 'Without tuning')\nplt.plot(list(range(len(r2))),r2,label = 'With tuning')\nplt.plot(list(range(len(r2))),accuracy,label = 'Accuracy')\nplt.xlabel('Num of Variables')\nplt.ylabel('AUC')\nplt.title('Increase in Area Under Curve on addition of variables')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_tr,Y_tr)\nprobs = nb.predict_proba(X_ts)[:,1]\nfpr,tpr,thresholds = roc_curve(Y_ts,probs)\nnb_a = auc(fpr,tpr)\nnb_acc = nb.score(X_ts,Y_ts)\nprint('AUC of Naive Bayes:',nb_a)\nprint('Accuracy of Naive Bayes:',nb_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - Naive Bayes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = nb.predict(X_ts)\ncr = classification_report(Y_ts,Y_pred)\nnb_cm = confusion_matrix(Y_ts,Y_pred)\nnb_sen = nb_cm[0,0]/(nb_cm[0,0]+nb_cm[0,1])\nprint('Sensitivity of Naive Bayes:',nb_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = []\naccuracy = []\nfor i in range(1,len(res.index)):\n    c = list(res[:i]['columns'])\n    print(c)\n    nb.fit(X_tr[c],Y_tr)\n    probs = nb.predict_proba(X_ts[c])[:,1]\n    fpr,tpr,thresholds = roc_curve(Y_ts,probs)\n    a=auc(fpr,tpr)\n    acc = nb.score(X_ts[c],Y_ts)\n    print(a,acc)\n    r.append(a)\n    accuracy.append(acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(list(range(len(r))),r,label = 'AUC')\nplt.plot(list(range(len(r))),accuracy,label = 'Accuracy')\nplt.xlabel('Num of Variables')\nplt.ylabel('AUC')\nplt.title('Naive Bayes - change in AUC and Accuracy on addition of variables')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_tr,Y_tr)\nprobs = dt.predict_proba(X_ts)[:,1]\nfpr1,tpr1,thresholds = roc_curve(Y_ts,probs)\ndt_a = auc(fpr1,tpr1)\ndt_acc = dt.score(X_ts,Y_ts)\nprint('AUC of Decision Tree:',dt_a)\nprint('Accuracy of Decision Tree:',dt_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr1,tpr1,label = 'Without tuning')\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - Decision Tree')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters_optimization={'criterion':('gini','entropy'),'max_depth':(1,2,4,5,6,7),\n                       'max_features':(1,2,3,4,5,6),'max_leaf_nodes':(2,3,4,5,6)}\ndt_gridsearch=GridSearchCV(DecisionTreeClassifier(),parameters_optimization)\ndt_gridsearch.fit(X_tr,Y_tr)\ndt_gridsearch.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_best = DecisionTreeClassifier(criterion = 'entropy',max_depth = 5,max_features = 6,max_leaf_nodes = 6)\ndt_best.fit(X_tr,Y_tr)\nprobs = dt_best.predict_proba(X_ts)[:,1]\nfpr2,tpr2,thresholds = roc_curve(Y_ts,probs)\ndt_best_a = auc(fpr2,tpr2)\ndt_best_acc = dt_best.score(X_ts,Y_ts)\nprint('AUC of Decision Tree(Tuned):',dt_best_a)\nprint('Accuracy of Decision Tree(Tuned):',dt_best_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr1,tpr1,label = 'Without tuning')\nplt.plot(fpr2,tpr2,label = 'With tuning')\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - Decision Tree')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = dt_best.predict(X_ts)\ncr = classification_report(Y_ts,Y_pred)\ndt_cm = confusion_matrix(Y_ts,Y_pred)\ndt_sen = dt_cm[0,0]/(dt_cm[0,0]+dt_cm[0,1])\nprint('Sensitivity of Decision Tree:',dt_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoosting\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_tr,Y_tr)\nprobs = xgb.predict_proba(X_ts)[:,1]\nfpr1,tpr1,thresholds = roc_curve(Y_ts,probs)\nxgb_a = auc(fpr1,tpr1)\nxgb_acc = xgb.score(X_ts,Y_ts)\nprint('AUC of XGBoost:',xgb_a)\nprint('Accuracy of XGBoost:',xgb_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nlearning_rate = [0.0001,0.001,0.01,0.1,0.2,0.3]\nn_estimators = [100,200,300,400,500]\nparam_grid = dict(learning_rate = learning_rate, n_estimators = n_estimators)\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(XGBClassifier(),param_grid,scoring = \"neg_log_loss\",n_jobs = -1,cv = kfold)\ngrid_search.fit(X_tr,Y_tr)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the best Learning rate and n_estimators \nfrom sklearn.model_selection import StratifiedKFold\nlearning_rate= [0.01]\nn_estimators = [500]\nmax_depth = np.arange(3,10)\ngamma = [0]\nparam_grid = dict(learning_rate = learning_rate, n_estimators = n_estimators, max_depth = max_depth,gamma=gamma)\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(XGBClassifier(),param_grid,scoring = \"neg_log_loss\",n_jobs = -1,cv = kfold)\ngrid_search.fit(X_tr,Y_tr)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best model\nxgb_best = XGBClassifier(learning_rate = 0.01,n_estimators = 500,max_depth = 5,gamma = 0)\nxgb_best.fit(X_tr,Y_tr)\nprobs = xgb_best.predict_proba(X_ts)[:,1]\nfpr2,tpr2,thresholds = roc_curve(Y_ts,probs)\nxgb_best_a = auc(fpr2,tpr2)\nxgb_best_acc = xgb_best.score(X_ts,Y_ts)\nprint('AUC of XGBoost(tuned):',xgb_best_a)\nprint('Accuracy of XGBoost(tuned):',xgb_best_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(fpr1,tpr1,label = 'Without tuning')\nplt.plot(fpr2,tpr2,label = 'With tuning')\nplt.plot([0,1],[0,1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Curve - XGBClassifier')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = xgb_best.predict(X_ts)\ncr = classification_report(Y_ts,Y_pred)\nxgb_cm = confusion_matrix(Y_ts,Y_pred)\nxgb_sen = xgb_cm[0,0]/(xgb_cm[0,0]+xgb_cm[0,1])\nprint('Sensitivity of XGBoost:',xgb_sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({'Model':['Logistic Regression','Random Forest','Naive Bayes','Decision Tree','XGBoost'],\n                       'AUC(Without tuning)':[lr_a, rf_a, nb_a,dt_a,xgb_a],\n                       'AUC(With tuning)':[lr_best_a,rf_best_a,nb_a,dt_best_a,xgb_best_a],\n                       'Accuracy(without tuning)':[lr_acc, rf_acc, nb_acc,dt_acc,xgb_acc],\n                       'Accuracy(with tuning)':[lr_best_acc,rf_best_acc,nb_acc,dt_best_acc,xgb_best_acc],\n                       'Sensitivity':[log_sen,rf_sen,nb_sen,dt_sen,xgb_sen]})\nmodels.sort_values(by=['Accuracy(with tuning)'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}