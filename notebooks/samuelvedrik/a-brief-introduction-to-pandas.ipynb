{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A brief introduction to pandas\n\n\nPandas is a powerful library that is used to manipulate and store your data. Most of the time, it reads data in from a file (usually csv, although it supports many different formats such as xlsx, json, etc) and converts it into a DataFrame - essentially, a table - so that you can explore, summarize and clean your data. Let's jump right in and see exactly what it can do!\n\nFor this tutorial, we are going to be using the [Used Cars Dataset](https://www.kaggle.com/austinreese/craigslist-carstrucks-data). \n\n\nIn your jupyter notebook / python file, make sure to import the following: \n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n```\n\nThe other two libraries are there for additional utility that we might need. Matplotlib is a graphing library that is useful to create graphs, and numpy is a mathematical library that allows us to perform scientific computing in python."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nFILE_PATH = \"../input/craigslist-carstrucks-data/vehicles.csv\" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (13, 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the file\n\nFirst things first, we need to be able to load our data in. This is where the `read_csv` function comes in handy. It takes in your file path, and it returns a dataframe ready for us to use. There are other read functions available, such as `read_excel` that suit many different file types.\n\nOptionally, there are many different parameters that you can pass in that might be useful. One common parameter is `nrows`, which allow you to specify how many rows of the csv you want to read in. This is useful when the dataset is too large, and we only need a portion of it. \n\nIn the example, our csv is quite large, so we are going to utilize `nrows` to tell pandas to only take the first 10,000 rows. \n\n```python\ndf = pd.read_csv(FILE_PATH, nrows=10000)\n```\n\nMake sure to set `FILE_PATH` to the actual path to the csv file! \n\n\nWith that, we have our dataframe in our `df` object, and we're ready to explore our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(FILE_PATH, nrows=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick look into the data\n\nBefore we work with our data, we first have to see what our data looks like! We can do that with `df.head()` to get the first few rows. By default, it returns the first 5 rows. We can change this by passing in an integer. For example, `df.head(10)` returns the first 10 rows.\n\n\nAlternatively, we can do `df.tail()` to get the last few rows, or `df.sample()` to get a few random rows. Like `df.head()`, the default is 5 rows, and we can change it by passing in an integer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we checked out a few rows in the dataframe, it's time to get some summary statistics. It's always a good idea to get some idea of what we're working with! There are two ways to get an idea of what we're working with: \n\n- `df.info()` returns information about columns, number of Non-Null cells in each column, and the data type of each column. This is useful to know how much data is missing from the dataset. \n\n- `df.describe()` returns the count, mean, standard deviation, min, 25th percentile, median, 75th percentile, and the max of all numerical columns. This is useful to have a quick idea of what the data might look like. However, be careful of relying on these too much, as they might not give you the full picture. Read more about [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) if you're interested in seeing an example!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Indexing\n\nNow that we're able to view some of the dataset, let's now take a look on how to view specific rows. There are two different ways to index into the dataframe: `iloc` and `loc`. We will take a closer look at an example of how they differ in a later section. \n\n`iloc` indexes based on the order of the rows. For example, if we want the 10th row, we would do `df.iloc[9]`. (Remember, we start indexing from 0!) \n\n`loc` indexes based on the index of the rows. For example, if we want the row that has the index 9, we would do `df.loc[9]`. \n\nIn some cases, `df.loc[9]` and `df.iloc[9]` might return the same row. This occurs when the 9th row happens to have the index 9. We'll see an example of when this is not true in a later section."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiple rows\n\nSimilarly to how we can `:` notation to get a slice from a python list, we can use the `:` notation to get a slice from a dataframe using `iloc`. \n\nLikewise, if we have a list of the indexes for rows that we want, we can get them using `loc`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[[1, 12, 40]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[[1, 12, 40]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting Columns\n\nSometimes we only want to see the values for a certain row. We can do that in two different ways: \n\n`df.column_name` and `df[\"column_name\"]`. Whichever you use is up to you! However, be careful when your column name is not a valid name for a python variable (for example, \"Column Name\" is not a valid python variable since it contains a space). In that case, you have to use the square bracket notation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting Multiple Columns\n\nSometimes, we want to see more than one column. Easy! Just pass in the list of column names you want. Note that the order of the columns will be in the same order as the list you pass in. \n\nIn the example below, although the region column occurs after the id column, the dataframe returned has the region column before the id column since it follows the order of the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"region\", \"id\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boolean Indexing\n\nSometimes we don't know the exact indexes of the rows we want, but we do know a specific condition that they must satisfy. Don't worry, pandas has you covered! Through the power of boolean indexing, we can achieve this. First, let's take a look at what a boolean series looks like. In this example, `df[\"year\"] < 2010` means that we're checking the truth value of the predicate `x < 2010` for every row in the `year` column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"year\"] < 2010","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how every row corresponds to either `True` or `False`. With boolean indexing, we only take the rows that are `True`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"year\"] < 2010].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how the rows with index 0 and 1 are ommited. This is because they are manufactured after (and including) 2010!"},{"metadata":{},"cell_type":"markdown","source":"We can also multiple conditions at the same time. We're going to need [bitwise operators](https://realpython.com/python-bitwise-operators/) in order to do this. The ones that we often use are `& | ~`, corresponding to `and`, `or`, and `not` respectively.\n\nIn the example below, it would translate in plain english to \"Cars that are manufactured by BMW before 2010.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"cond1 = df[\"year\"] < 2010\ncond2 = df[\"manufacturer\"] == \"bmw\"\n\ndf[cond1 & cond2].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Difference between iloc and loc\n\nRemember how I mentioned we were going to look at a closer example? This is a case where `iloc` and `loc` are going to differ. This occurs pretty often, so make you understand what's happening here."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_old = df[df[\"year\"] < 2010]\ndf_old.iloc[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_old.loc[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why does `df_old.iloc[2]` and `df_old.loc[2]` return different rows? In the first case, we are looking for the 3rd row of df_old, which happens to have index 5. In the second case, we are looking for the row with index 2, which is in fact our first row in the dataframe!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_old.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping Columns\n\nSometimes we want to drop columns that does not contain any meaningful information, or because they contain too many missing (NaN) values. We can drop specific columns as such:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=[\"Unnamed: 0\", \"size\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping rows\n\nSuppose that a column has some missing values. However, most rows do not have a missing value in that column. How do we remove rows that have a missing value in that column? We can do so with boolean indexing. However, since it is a common occurence, pandas also provides us with a method to do so easily. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(subset=[\"year\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The example aboves drops rows that contain a missing value in the \"year\" column. If we wish to drop all rows with any missing values, we can simply call `df.dropna()` without any parameters."},{"metadata":{},"cell_type":"markdown","source":"## Creating new columns\n\nOften, we wish to create new columns from existing columns. We can do so quite easily! There are two ways to do so:"},{"metadata":{},"cell_type":"markdown","source":"When the operation can be vectorized, we can simply use existing columns to create new information. In the example below, we want to create a new column called \"full_model\", which combines the manufacturer and car model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"full_model\"] = df[\"manufacturer\"] + \" \" + df[\"model\"]\ndf[\"full_model\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sometimes however, we want to map a function over each rows to create a new value. We can make use of a powerful method in pandas called `apply`.\n\nSuppose that we want the manufacturer + model + year for car models that are manufactured before 2000, and manufacturer + model for car models that are manufactured after 2000. Althought there are many different ways we can do so, we are going to make use of the apply function to make it straightforward.\n\nNote the `axis=1` parameter. That is what tells pandas that we want to map the function across the rows. We can alternatively set `axis=0` to map functions across columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x):\n    result = f\"{x['manufacturer']} {x['model']}\"\n    if x[\"year\"] < 2000:\n        return result + f\" {(int(x['year']))}\"\n    else:\n        return result\n\ndf.apply(f, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistics\n\nPandas also gives us quick and easy ways to get summary statistics from each column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Column uniqueness\n\nOne of the most common questions we want to ask is \"How many unique values are there for each column?\". Pandas gives us a painless way to answer that question quickly. There are 3 methods introduced here: \n\n`.unique()`, which gives us all the unique values.\n\n`nunique()`, which gives us the number of unique values. \n\n`value_counts()`, which gives us the number of rows with each unique value."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"manufacturer\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"manufacturer\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"manufacturer\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Going one step further, we can easily visualize value counts with a bar chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"manufacturer\"].value_counts().plot.barh()\nplt.title(\"Distribution of manufacturers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Case study\n\nSupposed that we are a resident of Auburn, and we decide to start looking for a new car. We don't have a lot of budget, so we decide to go for relatively cheap cars. More specifically, we define \"cheap\" cars as cars have a price of 0.75 * the average price of cars in the Auburn region.  However, we don't want poor cars, and so we decide to only  include cars in \"good\" or \"excellent\" condition. Among all these cars, which are the most expensive? \n\nFirst, we want to only select listings in the Auburn region.\n\n```python \ndf_case_study = df[df[\"region\"] == \"auburn\"]\n```\n\nNext, we want the average price of the cars in the Auburn region, \n\n```python \nauburn_mean = df_case_study[\"price\"].mean()\n```\n\nWe can use boolean indexing to act as a \"filter\". For now, we store this boolean array. \n\n```python\ncond_cheap = df_case_study[\"price\"] <= 0.75*auburn_mean\n```\n\nWe also want to select cars with \"good\" or \"excellent\" condition. We can make use of `|` here.\n```python\ncond_good = df_case_study[\"condition\"] == \"good\"\ncond_excellent = df_case_study[\"condition\"] == \"excellent\"\ncond_condition = cond_good | cond_excellent\n```\n\nFinally, we put everything together, and get the most expensive amongst them!\n\n```python \ndf_case_study = df_case_study[cond_cheap & cond_condition]\ndf_case_study.iloc[df_case_study[\"price\"].argmax()]\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_case_study = df[df[\"region\"] == \"auburn\"]\n# df_case_study = df_case_study.drop(columns=[\"Unnamed: 0\"])\n# df_case_study = df_case_study.dropna(subset=[\"manufacturer\"])\nauburn_mean = df_case_study[\"price\"].mean()\ncond_cheap = df_case_study[\"price\"] <= 0.75*auburn_mean\ncond_good = df_case_study[\"condition\"] == \"good\"\ncond_excellent = df_case_study[\"condition\"] == \"excellent\"\ncond_condition = cond_good | cond_excellent\n\ndf_case_study = df_case_study[cond_cheap & cond_condition]\ndf_case_study.iloc[df_case_study[\"price\"].argmax()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Slightly harder: \n\nCars are cheap if they are lower than the average of the price of that manufacturer."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_case_study = df_case_study.dropna(subset=[\"manufacturer\"])\nmanu_to_mean = df_case_study.groupby(\"manufacturer\")[\"price\"].mean().to_dict()\nis_cheap = df_case_study.apply(lambda x: manu_to_mean[x[\"manufacturer\"]] >= x[\"price\"], axis=1)\ndf_case_study[is_cheap]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}