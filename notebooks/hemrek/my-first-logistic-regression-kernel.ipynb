{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/candy-data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93162c49dd597151c1a594d15590d64d087f95c5"},"cell_type":"markdown","source":"## EDA (Exploratory Data Analysis)"},{"metadata":{"trusted":true,"_uuid":"4019c89e9d72108c5792b37f8627a4673830a918"},"cell_type":"code","source":"data.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eccf21e987b4e9f6ec02b8f649a29e0729fb74b"},"cell_type":"code","source":"# I do not need \"competitorname\" so I drop it\ndata.drop(\"competitorname\", inplace = True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa94c0dbf8d780863f90052a5366c4cb3e012145"},"cell_type":"markdown","source":"In this kernel we try to predict if a candy is chocolate based or not, based on its others features\nif its chocolate based, result will be 1 else it will be 0"},{"metadata":{"trusted":true,"_uuid":"f25ece36198f01bae60fa50cbb240902633fbc8d"},"cell_type":"code","source":"# initialize x and y\ny = data.chocolate.values\nx_data = data.drop([\"chocolate\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"590713f506a927a23327dccc7ad422c41f183aff"},"cell_type":"markdown","source":"## Normalization"},{"metadata":{"trusted":true,"_uuid":"7215f836fd3c5ef3bdd6a68683c81fb4ac8b62ea"},"cell_type":"code","source":"x = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1b84818ce42c6386853c52323463d7bed1a3ac5"},"cell_type":"markdown","source":"## Train-Test-Split"},{"metadata":{"trusted":true,"_uuid":"e5e4bff2c85200e16c427bc9dacd0f7040866a1d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train shape\", x_train.shape)\nprint(\"x_test shape\", x_test.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"y_test shape\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"817e06d67b74cf10261d606f0d604b4dc6fd4975"},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    \"\"\"\n    Input\n    dimension => number of train_data's features\n    \n    Output\n    w => weights\n    b => bias\n    \"\"\"\n    w = np.full((dimension,1), 0.01)\n    b = 0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8172ceb2de70ae73901565d28aa761f3804a83ec"},"cell_type":"code","source":"def forward_backward_propagation(w, b, x_train, y_train):\n    \"\"\"\n    Input\n    w => weights\n    b => bias\n    x_train => x of the data we want the train\n    y_train => y of the data we want the train\n    \n    Output\n    cost => loss of function\n    gradients => derivative of weights and bias\n    \"\"\"\n    \n    # forward propagation\n    z = (np.dot(w.T, x_train)+b)\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-((1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss)/x_train.shape[1]\n    \n    # backward propagation\n    \n    derivative_weights = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weights\":derivative_weights, \"derivative_bias\":derivative_bias}\n    \n    return cost, gradients\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b95f919bd00d08bd920f1f967bd2841c33a15f4"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate, num_iteration):\n    \"\"\"\n    Input\n    w => weights\n    b => bias\n    x_train => x of the data we want the train\n    y_train => y of the data we want the train\n    learning_rate => learn speed (if speed is too big, function can not be run properly)\n    num_iteration => how many times we want to run forward_backward_propagation()\n    \n    Output\n    paremeter => last values of weights and bias\n    cost_list => all cost(loss) values we got\n    \"\"\"\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(num_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate*gradients[\"derivative_weights\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n    \n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n    \n    parameters = {\"weights\":w, \"bias\":b}        \n    plt.plot(index, cost_list2)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d413a8033056382870c787b8d1ab83b1b4bf2b04"},"cell_type":"code","source":"def predict(w, b, x_test):\n    \"\"\"\n    Input\n    w => last values of weights\n    b => last value of bias\n    x_test => x of the data we want to test\n    \n    Output\n    y_predict => prediction of the test data\n    \"\"\"\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    y_predict = np.zeros((1, x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n    return y_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e375a6075979856f0110a62a0fca4ef24914f64"},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iteration):\n    w, b = initialize_weights_and_bias(x_train.shape[0])\n    \n    parameter, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iteration)\n    \n    y_predict = predict(parameter[\"weights\"], parameter[\"bias\"], x_test)\n    print(\"accuracy: {}\".format(100 - np.mean(np.abs(y_predict - y_test))))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, num_iteration = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bac3653fa0cfa878287f61b70eaffb6c629da6a4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nlr.fit(x_train.T, y_train.T)\n\nprint(\"Test accuracy: {}\".format(lr.score(x_test.T, y_test.T)))\nprint(\"Train accuracy: {}\".format(lr.score(x_train.T, y_train.T)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1f8a7655d6e137f399ecc779bb45210de1163db"},"cell_type":"markdown","source":"## Conclusion\n\nIf you see my wrong spelling please ignore them :)\n\nIf you like it, please upvote :)\n\nIf you have any question, I will be appreciate to hear it."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}