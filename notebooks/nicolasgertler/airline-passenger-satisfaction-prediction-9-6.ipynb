{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Airline Passenger Satisfication Prediction Table of Contents:\n\n## [1. Data Preprocessing / Exploratory Data Analysis](#1.)\n\n#### [1.1. Data Familiarization](#1.1.)\n\n#### [1.2. Data Cleaning](#1.2.)\n- [1.2.1. NaN Values](#1.2.1.)\n- [1.2.2. Duplicate Values](#1.2.2.)\n- [1.2.3. Outliers](#1.2.3.)\n\n#### [1.3. Final Data Preparation](#1.3.)\n\n#### [1.4. Data Visualization](#1.4.)\n- [1.4.1. Correlation Matrix](#1.4.1.)\n- [1.4.2. Features Visualization](#1.4.2.)\n\n\n## [2. Machine Learning: Prediction](#2.)\n\n#### [2.1. KNN, Decision Tree, Random Forest](#2.1.)\n\n- [2.1.1. Random Forest: Hyperparameter tuning](#2.1.1.)\n\n#### [2.2. XGBoost](#2.2.)\n- [2.2.1. XGBoost: Hyperparameter tuning](#2.2.1.)\n\n#### [2.3. Confusion Matrix](#2.3.)\n\n#### [2.4. Classification Report](#2.4.)\n\n\n## [3. Conclusion](#3.)","metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"#### Data Description -> below","metadata":{}},{"cell_type":"markdown","source":"Gender: Gender of the passengers (Female, Male)\n\nCustomer Type: The customer type (Loyal customer, disloyal customer)\n\nAge: The actual age of the passengers\n\nType of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)\n\nClass: Travel class in the plane of the passengers (Business, Eco, Eco Plus)\n\nFlight distance: The flight distance of this journey\n\nInflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)\n\nDeparture/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient\n\nEase of Online booking: Satisfaction level of online booking\n\nGate location: Satisfaction level of Gate location\n\nFood and drink: Satisfaction level of Food and drink\n\nOnline boarding: Satisfaction level of online boarding\n\nSeat comfort: Satisfaction level of Seat comfort\n\nInflight entertainment: Satisfaction level of inflight entertainment\n\nOn-board service: Satisfaction level of On-board service\n\nLeg room service: Satisfaction level of Leg room service\n\nBaggage handling: Satisfaction level of baggage handling\n\nCheck-in service: Satisfaction level of Check-in service\n\nInflight service: Satisfaction level of inflight service\n\nCleanliness: Satisfaction level of Cleanliness\n\nDeparture Delay in Minutes: Minutes delayed when departure\n\nArrival Delay in Minutes: Minutes delayed when Arrival\n\nSatisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# 1. Data Preprocessing / Exploratory Data Analysis\n<a id=\"1.\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 1.1. Data Familiarization\n<a id=\"1.1.\"></a>","metadata":{}},{"cell_type":"code","source":"# Import necessary modules\n\nimport numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T18:56:45.228994Z","iopub.execute_input":"2021-08-12T18:56:45.229372Z","iopub.status.idle":"2021-08-12T18:56:48.23475Z","shell.execute_reply.started":"2021-08-12T18:56:45.229343Z","shell.execute_reply":"2021-08-12T18:56:48.233698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in dataframe\ndf = pd.read_csv('/kaggle/input/airline-passenger-satisfaction/train.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:56:48.236552Z","iopub.execute_input":"2021-08-12T18:56:48.236972Z","iopub.status.idle":"2021-08-12T18:56:48.69454Z","shell.execute_reply.started":"2021-08-12T18:56:48.236926Z","shell.execute_reply":"2021-08-12T18:56:48.693518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unneeded row and sort by ascending ID's\ndf = df.drop('Unnamed: 0', axis=1)\ndf = df.sort_values('id', ascending=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:56:53.083565Z","iopub.execute_input":"2021-08-12T18:56:53.083919Z","iopub.status.idle":"2021-08-12T18:56:53.140474Z","shell.execute_reply.started":"2021-08-12T18:56:53.083888Z","shell.execute_reply":"2021-08-12T18:56:53.139429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:57:20.693733Z","iopub.execute_input":"2021-08-11T22:57:20.694141Z","iopub.status.idle":"2021-08-11T22:57:20.722167Z","shell.execute_reply.started":"2021-08-11T22:57:20.694103Z","shell.execute_reply":"2021-08-11T22:57:20.721128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.588851Z","iopub.execute_input":"2021-08-11T22:44:36.589848Z","iopub.status.idle":"2021-08-11T22:44:36.597894Z","shell.execute_reply.started":"2021-08-11T22:44:36.589792Z","shell.execute_reply":"2021-08-11T22:44:36.5966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.599627Z","iopub.execute_input":"2021-08-11T22:44:36.600077Z","iopub.status.idle":"2021-08-11T22:44:36.649406Z","shell.execute_reply.started":"2021-08-11T22:44:36.600036Z","shell.execute_reply":"2021-08-11T22:44:36.648054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()[:10].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.651017Z","iopub.execute_input":"2021-08-11T22:44:36.651447Z","iopub.status.idle":"2021-08-11T22:44:36.759767Z","shell.execute_reply.started":"2021-08-11T22:44:36.651403Z","shell.execute_reply":"2021-08-11T22:44:36.758746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Data Cleaning\n <a id=\"1.2.\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.1. NaN Values\n<a id=\"1.2.1.\"></a>","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.760941Z","iopub.execute_input":"2021-08-11T22:44:36.761219Z","iopub.status.idle":"2021-08-11T22:44:36.798358Z","shell.execute_reply.started":"2021-08-11T22:44:36.761191Z","shell.execute_reply":"2021-08-11T22:44:36.796985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we see that we have 310 missing values in the Arrival Delay in Minutes row. At this point, we could either impute these missing values or remove them. We're going to drop the rows with NaN values because it's not worth the possibility of somewhat skewing the data by imputing the mean when we have over 100k observations (rows).","metadata":{}},{"cell_type":"code","source":"# Dropping rows with NaN values\n\ndf = df.dropna().copy()\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.802281Z","iopub.execute_input":"2021-08-11T22:44:36.802744Z","iopub.status.idle":"2021-08-11T22:44:36.863098Z","shell.execute_reply.started":"2021-08-11T22:44:36.802697Z","shell.execute_reply":"2021-08-11T22:44:36.862004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, dropping the rows with NaN values had a very small impact on the data as a whole.","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2. Duplicate Values\n<a id=\"1.2.2.\"></a>","metadata":{}},{"cell_type":"code","source":"df.duplicated().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.865982Z","iopub.execute_input":"2021-08-11T22:44:36.866955Z","iopub.status.idle":"2021-08-11T22:44:36.964186Z","shell.execute_reply.started":"2021-08-11T22:44:36.86691Z","shell.execute_reply":"2021-08-11T22:44:36.96309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check to see if there are any duplicate values; luckily there are no duplicates values. Now, let's move on to handling outliers in our dataset.","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.3. Outliers\n<a id=\"1.2.3.\"></a>","metadata":{}},{"cell_type":"markdown","source":"First, we'll look at the key numbers of the datset (mean, std, etc) to see if we can detect any anomalies; we'll use the describe function.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:36.966224Z","iopub.execute_input":"2021-08-11T22:44:36.967249Z","iopub.status.idle":"2021-08-11T22:44:37.109301Z","shell.execute_reply.started":"2021-08-11T22:44:36.967193Z","shell.execute_reply":"2021-08-11T22:44:37.107963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The max values of Departure Delay in Minutes and Arrival Delay in Minutes look to be extremely large: 1592 and 1584. We'll make a boxplot to visualize this.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=df['Departure Delay in Minutes'])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:37.111062Z","iopub.execute_input":"2021-08-11T22:44:37.111533Z","iopub.status.idle":"2021-08-11T22:44:37.327577Z","shell.execute_reply.started":"2021-08-11T22:44:37.111484Z","shell.execute_reply":"2021-08-11T22:44:37.326412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df['Arrival Delay in Minutes'])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:37.329689Z","iopub.execute_input":"2021-08-11T22:44:37.330233Z","iopub.status.idle":"2021-08-11T22:44:37.545835Z","shell.execute_reply.started":"2021-08-11T22:44:37.330167Z","shell.execute_reply":"2021-08-11T22:44:37.54437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df['Departure Delay in Minutes'] > 1300]\n\ndf.loc[df['Arrival Delay in Minutes'] > 1250]","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:37.547757Z","iopub.execute_input":"2021-08-11T22:44:37.548119Z","iopub.status.idle":"2021-08-11T22:44:37.584968Z","shell.execute_reply.started":"2021-08-11T22:44:37.548079Z","shell.execute_reply":"2021-08-11T22:44:37.583379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These visualizations confirm that these two max's are significantly larger than their counterparts. Furthermore, there is another anomaly shown above (around 1300). Overall, these observations seem to be natural, however, they are definetly outliers. Although these observations seem to be natural, our dataset is extremely large, and we do not want them to significantly skew our data (especially when we perform Machine Learning later in this project). Therefore, we will remove these 2 data points.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:37.586958Z","iopub.execute_input":"2021-08-11T22:44:37.58743Z","iopub.status.idle":"2021-08-11T22:44:37.595998Z","shell.execute_reply.started":"2021-08-11T22:44:37.587365Z","shell.execute_reply":"2021-08-11T22:44:37.594957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = df[df['Arrival Delay in Minutes'] > 1250].index\ndf.drop(outliers, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:37.597797Z","iopub.execute_input":"2021-08-11T22:44:37.598711Z","iopub.status.idle":"2021-08-11T22:44:37.641469Z","shell.execute_reply.started":"2021-08-11T22:44:37.598663Z","shell.execute_reply":"2021-08-11T22:44:37.638091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we sucessfully removed the 2 rows that contained outliers. Now if we look at a boxplot of these 2 columns again, we can see the data is more concentrated than before.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=df['Departure Delay in Minutes'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-11T22:44:37.643576Z","iopub.execute_input":"2021-08-11T22:44:37.644081Z","iopub.status.idle":"2021-08-11T22:44:37.854222Z","shell.execute_reply.started":"2021-08-11T22:44:37.64403Z","shell.execute_reply":"2021-08-11T22:44:37.852577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df['Arrival Delay in Minutes'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-11T22:44:37.855708Z","iopub.execute_input":"2021-08-11T22:44:37.856023Z","iopub.status.idle":"2021-08-11T22:44:38.04995Z","shell.execute_reply.started":"2021-08-11T22:44:37.855987Z","shell.execute_reply":"2021-08-11T22:44:38.048633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Final Data Preparation\n<a id=\"1.3.\"></a>","metadata":{}},{"cell_type":"code","source":"df['satisfaction'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:38.051693Z","iopub.execute_input":"2021-08-11T22:44:38.051997Z","iopub.status.idle":"2021-08-11T22:44:38.076187Z","shell.execute_reply.started":"2021-08-11T22:44:38.051967Z","shell.execute_reply":"2021-08-11T22:44:38.074892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we'll convert our y-variable's 2 categorical values (neutral or dissatisfied and satisfied) into 0 and 1 in order for our machine learning models to be able to classify the data","metadata":{}},{"cell_type":"code","source":"df['satisfaction'] = pd.get_dummies(df['satisfaction'])\ndf['satisfaction']\n\n# 0 = neutral or dissatisfied\n# 1 = satisfied","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:38.077893Z","iopub.execute_input":"2021-08-11T22:44:38.078342Z","iopub.status.idle":"2021-08-11T22:44:38.108444Z","shell.execute_reply.started":"2021-08-11T22:44:38.078286Z","shell.execute_reply":"2021-08-11T22:44:38.107319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:56:58.099619Z","iopub.execute_input":"2021-08-12T18:56:58.099988Z","iopub.status.idle":"2021-08-12T18:56:58.114909Z","shell.execute_reply.started":"2021-08-12T18:56:58.099958Z","shell.execute_reply":"2021-08-12T18:56:58.113596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following features: Gender, Customer Type, Type of Travel, and Class are all currently categorical data (dtype: \"object\"). However, we need to convert it to numerical data in order for our Machine Learning model(s) to be able understand the data. Therefore, we'll do just that using the get_dummies function from pandas. ","metadata":{}},{"cell_type":"code","source":"df['Gender'] = pd.get_dummies(df['Gender'])\ndf['Customer Type'] = pd.get_dummies(df['Customer Type'])\ndf['Type of Travel'] = pd.get_dummies(df['Type of Travel'])\ndf['Class'] = pd.get_dummies(df['Class'])\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-12T19:06:01.09398Z","iopub.execute_input":"2021-08-12T19:06:01.09453Z","iopub.status.idle":"2021-08-12T19:06:01.154461Z","shell.execute_reply.started":"2021-08-12T19:06:01.094497Z","shell.execute_reply":"2021-08-12T19:06:01.153415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4. Data Visualization\n<a id=\"1.4.\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### 1.4.1. Correlation Matrix\n<a id=\"1.4.1.\"></a>","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\n\nnp.fill_diagonal(corr.values, 0)\n\ncorr.replace(0, np.nan, inplace=True)\n\ncorr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-12T18:58:36.029186Z","iopub.execute_input":"2021-08-12T18:58:36.029738Z","iopub.status.idle":"2021-08-12T18:58:36.17761Z","shell.execute_reply.started":"2021-08-12T18:58:36.029693Z","shell.execute_reply":"2021-08-12T18:58:36.176421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's visualize our correlation matrix using a heatmap.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (18,9))\nsns.heatmap(corr, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:58:38.935499Z","iopub.execute_input":"2021-08-12T18:58:38.936007Z","iopub.status.idle":"2021-08-12T18:58:41.009115Z","shell.execute_reply.started":"2021-08-12T18:58:38.935975Z","shell.execute_reply":"2021-08-12T18:58:41.007977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll print out the strongest correlations between all of our variables.","metadata":{}},{"cell_type":"code","source":"corr.unstack().sort_values(kind='quicksort', na_position='first').drop_duplicates(keep='first')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-11T22:44:39.979849Z","iopub.status.idle":"2021-08-11T22:44:39.980809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll print out the highest correlated variables to our y-variable (satisfaction).","metadata":{}},{"cell_type":"code","source":"# We call the absolute value func. because whether the variables are positively or negatively correlated to our y-variable is irrelevant, as they're still highly correlated\n\ndf.corr().abs()['satisfaction'].sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:50:04.207522Z","iopub.execute_input":"2021-08-11T22:50:04.207931Z","iopub.status.idle":"2021-08-11T22:50:04.390149Z","shell.execute_reply.started":"2021-08-11T22:50:04.207899Z","shell.execute_reply":"2021-08-11T22:50:04.388728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the variables most highly correlated to our y-variable are: \n- Class (~50%)\n- Online boarding (~50%)\n- Type of Travel (~44%)\n- Inflight entertainment (~40%)\n- Seat comfort (~35%)\n- On-board service (~32%)\n- Leg room service (~31%)\n- Cleanliness (~31%)\n- Flight distance (~30%)","metadata":{}},{"cell_type":"markdown","source":"We will use these insights to guide our data visualization.","metadata":{}},{"cell_type":"markdown","source":"### 1.4.2. Features Visualization\n<a id=\"1.4.2.\"></a>","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='Inflight wifi service', y='Online boarding', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T23:05:43.709098Z","iopub.execute_input":"2021-08-11T23:05:43.70954Z","iopub.status.idle":"2021-08-11T23:05:43.999935Z","shell.execute_reply.started":"2021-08-11T23:05:43.7095Z","shell.execute_reply":"2021-08-11T23:05:43.999076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"People who get better Inflight wifi service likely had a better online boarding experience","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='satisfaction', y='Online boarding', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T23:07:40.672572Z","iopub.execute_input":"2021-08-11T23:07:40.67321Z","iopub.status.idle":"2021-08-11T23:07:40.883847Z","shell.execute_reply.started":"2021-08-11T23:07:40.673157Z","shell.execute_reply":"2021-08-11T23:07:40.882657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For some people, even though they had a good online boarding experience, they weren't satisified. On the other hand, some people who only had a decent online boarding experience turned out to be satisified with their overall flying experience. Very interesting!","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='Arrival Delay in Minutes', y='Departure Delay in Minutes', \n                hue='satisfaction', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T19:01:40.668604Z","iopub.execute_input":"2021-08-12T19:01:40.669049Z","iopub.status.idle":"2021-08-12T19:01:48.673843Z","shell.execute_reply.started":"2021-08-12T19:01:40.669017Z","shell.execute_reply":"2021-08-12T19:01:48.672918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, we see the strong correlation between Departure Delay and Arrival Delay; we can also see whether the passenger was satisified or not","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x='Inflight wifi service', y='Ease of Online booking', \n                hue='satisfaction', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:39.987833Z","iopub.status.idle":"2021-08-11T22:44:39.988814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the above variables in the graph above have a ~70% correlation. When we think of correlation this graph is not what we expect. So why does this graph look so weird?","metadata":{}},{"cell_type":"code","source":"df['Inflight wifi service'].value_counts()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-11T22:44:39.990555Z","iopub.status.idle":"2021-08-11T22:44:39.991676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Ease of Online booking'].value_counts()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-11T22:44:39.993318Z","iopub.status.idle":"2021-08-11T22:44:39.994329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is because the data points for both variables are evaluated on a scale of 0-5; this is why the graph looks odd. Rest assured, though, that it is valid.","metadata":{}},{"cell_type":"markdown","source":"Next, we'll use plotly to see some 3d visualizations on how 3 variables all highly correlated to a passenger's satisfaction affect a passenger's overall airline satisfaction.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter_3d(df.head(1000), x='On-board service', y='Leg room service', z='Cleanliness', \n                   color='satisfaction')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T19:17:27.983858Z","iopub.execute_input":"2021-08-12T19:17:27.984237Z","iopub.status.idle":"2021-08-12T19:17:28.065694Z","shell.execute_reply.started":"2021-08-12T19:17:27.984205Z","shell.execute_reply":"2021-08-12T19:17:28.064926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see how on-board service, leg room service, and cleanliness affect a passenger's satisifaction. Looking at the features in the plotly graph above, we see that the higher ranking a passenger gives to each category the more likely they are to be satisified with their overall airline experience. Specificially, we see the highest importance of a passenger's satisfaction with their on-board service experience.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter_3d(df.head(1000), x='Online boarding', y='Inflight entertainment', z='Seat comfort', \n                   color='satisfaction')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T19:17:56.689716Z","iopub.execute_input":"2021-08-12T19:17:56.690392Z","iopub.status.idle":"2021-08-12T19:17:56.769567Z","shell.execute_reply.started":"2021-08-12T19:17:56.690339Z","shell.execute_reply":"2021-08-12T19:17:56.768391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see how online boarding, inflight entertainment, and seat comfort all contribute to a passenger's airline satisfaction. Overall, the insights from this plot are similar to the first plotly graph above.","metadata":{}},{"cell_type":"markdown","source":"# 2. Machine Learning: Prediction\n<a id=\"2.\"></a>","metadata":{}},{"cell_type":"markdown","source":"First, we'll create our X and y variables","metadata":{}},{"cell_type":"code","source":"X = df.drop('satisfaction', axis=1)\ny = df['satisfaction']","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:51:48.772842Z","iopub.execute_input":"2021-08-11T22:51:48.773758Z","iopub.status.idle":"2021-08-11T22:51:48.787843Z","shell.execute_reply.started":"2021-08-11T22:51:48.773714Z","shell.execute_reply":"2021-08-11T22:51:48.78657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's perform a train-test-split on our data to split our data into train and test sets.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.003687Z","iopub.status.idle":"2021-08-11T22:44:40.004405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. KNN, Decision Tree, Random Forest\n<a id=\"2.1.\"></a>","metadata":{}},{"cell_type":"code","source":"models = {'KNN': KNeighborsClassifier(),\n          'Decision Tree' : DecisionTreeClassifier(),\n         'Random Forest': RandomForestClassifier()}\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        \n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.005937Z","iopub.status.idle":"2021-08-11T22:44:40.006932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_scores = fit_and_score(models=models, \n                             X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","metadata":{"execution":{"iopub.status.busy":"2021-08-12T19:30:18.562555Z","iopub.execute_input":"2021-08-12T19:30:18.562931Z","iopub.status.idle":"2021-08-12T19:30:18.651547Z","shell.execute_reply.started":"2021-08-12T19:30:18.5629Z","shell.execute_reply":"2021-08-12T19:30:18.650163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the scores of our models we see that our Decision Tree and Random Forest models both perform very well! However, our KNN model doesn't perform well; therefore, we'll discard it. Now, let's visualize our results!","metadata":{}},{"cell_type":"code","source":"model_comp = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_comp.T.plot.bar();","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.010797Z","iopub.status.idle":"2021-08-11T22:44:40.011597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now take our best performing model (Random Forest) and tune its hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1. Random Forest: Hyperparameter tuning\n<a id=\"2.1.1.\"></a>","metadata":{}},{"cell_type":"markdown","source":"*Not going to run Randomized Search CV on final project as it takes up a lot of computational power and therefore takes a very long time to complete. However, rest assured, that after having run it a few times, it does not increase the accuracy of the Random Forest model*","metadata":{}},{"cell_type":"code","source":"# rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n#           \"max_depth\": [None, 3, 5, 10],\n#           \"min_samples_split\": np.arange(2, 20, 2),\n#           \"min_samples_leaf\": np.arange(1, 20, 2)}\n\n# rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n#                           param_distributions=rf_grid,\n#                           cv=5,\n#                           verbose=True)\n\n# rs_rf.fit(X_train, y_train);\n\n# rs_rf.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.013345Z","iopub.status.idle":"2021-08-11T22:44:40.014041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rf = RandomForestClassifier(n_estimators=210, min_samples_split=14, min_samples_leaf=15, max_depth=None)\n# rf.fit(X_train, y_train)\n# rf_pred = rf.predict(X_test)\n# rf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.015338Z","iopub.status.idle":"2021-08-11T22:44:40.016136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tuning our Random Forest model's hyperparameters doesn't make much of a difference in the accuracy score of the model.\n\nNow, let's use **XGBoost** to see how well it scores!","metadata":{}},{"cell_type":"markdown","source":"### 2.2. XGBoost\n<a id=\"2.2.\"></a>","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\nxgb.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.017289Z","iopub.status.idle":"2021-08-11T22:44:40.017895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like our XGBoost model performs the best out of all our models -- with an +96% score! Now, let's tune its hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.1. XGBoost: Hyperparameter tuning\n<a id=\"2.2.1.\"></a>","metadata":{}},{"cell_type":"code","source":"params_xgb = {'n_estimators': [50,100,250,400,600,800,1000], 'learning_rate': [0.2,0.5,0.8,1]}\nrs_xgb =  RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)\nrs_xgb.fit(X_train, y_train)\nxgb_pred_2 = rs_xgb.predict(X_test)\nrs_xgb.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:40.019155Z","iopub.status.idle":"2021-08-11T22:44:40.019792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once again, we see that tuning our model's hyperparameters doesn't make much of a difference.","metadata":{}},{"cell_type":"markdown","source":"**Overall, we see that our best performing model is our XGBoost model.** We'll now evaluate this model using other metrics. ","metadata":{}},{"cell_type":"markdown","source":"### 2.3. Confusion Matrix\n<a id=\"2.3.\"></a>","metadata":{}},{"cell_type":"code","source":"print(confusion_matrix(y_test, xgb_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:45:06.249902Z","iopub.execute_input":"2021-08-11T22:45:06.250291Z","iopub.status.idle":"2021-08-11T22:45:06.360046Z","shell.execute_reply.started":"2021-08-11T22:45:06.250256Z","shell.execute_reply":"2021-08-11T22:45:06.355003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, xgb_pred):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, xgb_pred),\n                     annot=True, # Annotate the boxes\n                     cbar=False,\n                    fmt='g', # don't use scientic notation\n                    cmap='Blues')\n    \n    plt.xlabel(\"true label\", weight='bold')\n    plt.ylabel(\"predicted label\", weight='bold')\n    \nplot_conf_mat(y_test, xgb_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:44:56.275028Z","iopub.execute_input":"2021-08-11T22:44:56.275437Z","iopub.status.idle":"2021-08-11T22:44:56.560815Z","shell.execute_reply.started":"2021-08-11T22:44:56.275397Z","shell.execute_reply":"2021-08-11T22:44:56.559908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Classification Report\n<a id=\"2.4.\"></a>","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, xgb_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T22:45:33.462638Z","iopub.execute_input":"2021-08-11T22:45:33.46305Z","iopub.status.idle":"2021-08-11T22:45:33.54216Z","shell.execute_reply.started":"2021-08-11T22:45:33.463018Z","shell.execute_reply":"2021-08-11T22:45:33.540804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Conclusion\n<a id=\"3.\"></a>","metadata":{}},{"cell_type":"markdown","source":"Overall, our XGBoost model performs extremely well across all metrics beyond just the accuracy score. This makes sense because it is commonly known among the DS/ML community that XGBoost is the best performing model with tabular data.\n\nBeyond our model, however, this data shows that there are many important factors that go into if a passenger is satisified with their experience flying with an airline or not. The most important features of which are: **Class, Online boarding, Type of Travel, Inflight entertainment, Seat comfort, On-board service, Leg room service, Cleanliness, and Flight distance.** We saw that by using these features, along with other flying-experience metrics, we can predict with a very high level of confidence **(+96%)** whether or not a passenger is satisified with their airline flying experience.","metadata":{}},{"cell_type":"markdown","source":"# **Thanks for reading my notebook! My objective for this notebook was to clearly explain my thought process to help guide readers. Feel free to upvote this notebook and leave feedback via the comment section!**","metadata":{}}]}