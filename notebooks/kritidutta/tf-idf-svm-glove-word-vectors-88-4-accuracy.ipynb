{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import all libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport re\nfrom fuzzywuzzy import fuzz\nfrom tqdm import tqdm\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem.snowball import SnowballStemmer\n\nimport xgboost as xgb\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None) \n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_punct(text):\n    text = text.replace(\"><\",\",\")\n    text = text.replace(\"<\",\"\")\n    text = text.replace(\">\",\"\")\n    \n    return text\n\ndata['Tags_cleaned'] = data['Tags'].apply(replace_punct)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis Of Tags"},{"metadata":{},"cell_type":"markdown","source":"Total number of unique tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"vector = CountVectorizer(tokenizer=lambda x:x.split(\",\"))\ntag_trans = vector.fit_transform(data['Tags_cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of tags are {}\".format(tag_trans.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = vector.get_feature_names()\nprint(\"some of tags are {}\".format(tags[40:100]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of times tag appears**"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = tag_trans.sum(axis=0).A1\nfreq_dict=dict(zip(tags,freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_dict['.net']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_freq_df = pd.DataFrame.from_dict(freq_dict, orient='index', columns=['Count']).reset_index(drop=False)\ntag_df_sorted = tag_freq_df.sort_values(['Count'], ascending=False).reset_index(drop=True)\ntag_df_sorted.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_df_sorted.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_counts = tag_df_sorted['Count'].values\nplt.plot(tag_counts[0:150])\nplt.title(\"Distribution of frequency of Tags Appeared\")\nplt.grid()\nplt.ylabel(\"Number of times tag appeared\")\nplt.xlabel(\"Tag Number\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_final = tag_df_sorted[tag_df_sorted.Count>3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_tags = list(tags_final['index'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_tags[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tag_remove(text):\n    text_list = text.split(\",\")\n    text_list = \",\".join(list(set(text_list) & set(final_tags)))\n    return text_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Tags_final'] = data['Tags_cleaned'].apply(tag_remove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Tags_cleaned'].apply(lambda x:len(x.split(\",\"))).equals(data['Tags_final'].apply(lambda x:len(x.split(\",\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Tags', 'CreationDate','Tags_cleaned'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[10,'Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def striphtml(data):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(data))\n    return cleantext\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['code'] = data['Body'].apply(lambda x: re.findall(r'<code>(.*?)</code>', x, flags=re.DOTALL))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[3, 'Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['question'] = data['Body'].apply(lambda x:re.sub('<code>(.*?)</code>', '', x, flags=re.MULTILINE|re.DOTALL))\ndata['question'] = data['question'].apply(lambda x: striphtml(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[10,'code'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['Title'] = data['Title'].apply(lambda x:re.findall(r'b(.*?)', x, flags=re.DOTALL))\n#data['Title'] = data['Title'].apply(lambda x:x.encode('utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['question'] = data['Title'].astype(str) + data['question'].astype(str)\ndata['question'] = data['question'].apply(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[31,'Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[31,'question']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[31,'Title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english')) \n\ndef remove_stopword(words):\n    list_clean = [w for w in words.split(' ') if not w in stop_words]\n    \n    return ' '.join(list_clean)\n\ndef remove_next_line(words):\n    words = words.split('\\n')\n    \n    return \" \".join(words)\n\ndef remove_r_char(words):\n    words = words.split('\\r')\n    \n    return \"\".join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['question'] = data['question'].apply(remove_stopword)\ndata['question'] = data['question'].apply(remove_next_line)\ndata['question'] = data['question'].apply(remove_r_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution = data.groupby('Y')['Body'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making basic Features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Num_words_body'] = data['Body'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndata['Num_words_title'] = data['Title'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ndata['difference_in_words'] = abs(data['Num_words_body'] - data['Num_words_title']) #Difference in Number of words text and Selected Text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Num_char_body'] = data['Body'].apply(lambda x:len(\"\".join(set(str(x).replace(\" \",\"\"))))) \ndata['Num_char_title'] = data['Title'].apply(lambda x:len(\"\".join(set(str(x).replace(\" \",\"\")))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['len_common_words'] = data.apply(lambda x:len(set(str(x['Title']).split()).intersection(set(str(x['Body']).split()))),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Make Fuzzy features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['fuzz_qratio'] = data.apply(lambda x:fuzz.QRatio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_Wratio'] = data.apply(lambda x:fuzz.WRatio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_ratio'] = data.apply(lambda x:fuzz.partial_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_token_set_ratio'] = data.apply(lambda x:fuzz.partial_token_set_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_token_sort_ratio'] = data.apply(lambda x:fuzz.partial_token_sort_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_token_set_ratio'] = data.apply(lambda x:fuzz.token_set_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_token_sort_ratio'] = data.apply(lambda x:fuzz.token_sort_ratio(str(x['Title']),str(x['Body'])), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split train test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Body_with_title'] = data['Title'] + \" \" + data['Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(data.drop(['Id','Title','Body','Y'],axis=1).values, data['Y'].values, \n                                                  stratify=data['Y'].values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yvalid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(clf, predictions, yvalid):\n    return np.mean(predictions == yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Tf-idf features"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv.fit(list(xtrain[:,-1]))\nxtrain_tfv =  tfv.transform(xtrain[:,-1]) \nxvalid_tfv = tfv.transform(xvalid[:,-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count vectorizer Model for Comparison with TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(xtrain[:,-1])\nxtrain_ctv =  ctv.transform(xtrain[:,-1]) \nxvalid_ctv = ctv.transform(xvalid[:,-1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit a simple Logistic regression Model on tf-idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_ctv = LogisticRegression()\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict_proba(xvalid_ctv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix\nmultilabel_confusion_matrix(yvalid, predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel_confusion_matrix(yvalid, predictions_ctv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit an Xgboost on tf-idf features"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel_confusion_matrix(yvalid, predictions","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"clf_ctv = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict(xvalid_ctv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit a Naive bayes Model on tf-idf only"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_ctv = MultinomialNB()\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict(xvalid_ctv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit an SVD on tf-idf features only"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=180)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC(C=1.0) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(xvalid_svd_scl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting Xgboost on tf-idf-SVD feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"mll_scorer = metrics.make_scorer(get_accuracy, greater_is_better=True, needs_proba=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nxg_model = xgb.XGBClassifier()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('xg', xg_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'svd__n_components' : [120, 150, 180],\n              'xg__max_depth':[5,7,10],\n              'xg__learning_rate':[0.1,0.01,0.5]}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Word vector features(Still improving)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_glove_vecs(glove_file):\n    #input: file\n    #output: word to 200d vector mapping output\n    with open(glove_file, 'r') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n    return word_to_vec_map\n#word_to_vec_map = read_glove_vecs('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\nword_to_vec_map = read_glove_vecs('../input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_sequence(ds, word_to_vec_map):\n    #input: Series, and word_to_vec_map of size(vocab_size,200)\n    #output: returns shape of (len(ds), 200)\n    traintest_X = []\n    for sentence in tqdm(ds):\n        sequence_words = np.zeros((word_to_vec_map['cucumber'].shape))\n        for word in sentence.split():\n            if word in word_to_vec_map.keys():\n                temp_X = word_to_vec_map[word]\n            else:\n                temp_X = word_to_vec_map['#']\n            #print(temp_X)\n            sequence_words+=(temp_X)/len(sentence)\n            #print(sequence_words)\n        traintest_X.append(sequence_words)\n    return np.array(traintest_X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_sequence(xtrain[:,-1][0], word_to_vec_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenate all sequences for training and testing set\ntrain_w2v = prepare_sequence(xtrain[:,-1], word_to_vec_map)\nvalid_w2v = prepare_sequence(xvalid[:,-1], word_to_vec_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\nclf.fit(train_w2v, ytrain)\npredictions = clf.predict(valid_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=15, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(train_w2v, ytrain)\npredictions = clf.predict(valid_w2v)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = xgb.XGBClassifier(max_depth=10, n_estimators=150, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf2.fit(train_w2v, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf2.predict(valid_w2v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf2, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_xtrain = np.concatenate((xtrain[:,:-1],train_w2v), axis=1)\nfinal_xvalid = np.concatenate((xvalid[:,:-1],valid_w2v),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\nclf.fit(final_xtrain, ytrain)\npredictions = clf.predict(final_xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(final_xtrain, ytrain)\npredictions = clf.predict(final_xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(clf, predictions, yvalid)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}