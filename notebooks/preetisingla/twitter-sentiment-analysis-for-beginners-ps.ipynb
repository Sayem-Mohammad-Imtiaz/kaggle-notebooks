{"cells":[{"metadata":{"id":"PUGFNIWGDLIH"},"cell_type":"markdown","source":"1. Understand the Problem Statement\n2. Tweets Preprocessing and Cleaning\n  1. Data Inspection\n  2. Data Cleaning\n3. Story Generation and Visualization from Tweets\n4. Extracting Features from Cleaned Tweets\n  1. Bag-of-Words\n  2. TF-IDF\n  3. Word Embeddings\n5. Model Building: Sentiment Analysis\n  1. Logistic Regression\n  2. Support Vector Machine\n  3. RandomForest\n  4. XGBoost\n6. Model Fine-tuning\n7. Summary","execution_count":null},{"metadata":{"id":"FngvzKD-Dvt2","outputId":"69910e70-aff4-4f50-e4f8-59a97a7bb2b6","trusted":true},"cell_type":"code","source":"# import libraries\nimport re\nimport nltk\nimport string\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_colwidth', 200)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1YXJPDF9EXzf","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"_4UHtYHdEzFA"},"cell_type":"markdown","source":"## Data Inspection","execution_count":null},{"metadata":{"id":"jPxkBEqZEpap","outputId":"ef85825e-be6c-42ff-ab87-4e3c3a0027e9","trusted":true},"cell_type":"code","source":"train[train['label'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"xhS_JJYgE52a","outputId":"af314861-2bf2-479f-b4e2-ab063dbaf280","trusted":true},"cell_type":"code","source":"train[train['label'] == 1].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ViM7gQ4NFLPI","outputId":"8eedac90-98b6-4bec-8d2c-aedd5f9e7fd8","trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"boQcNI9_FWNv","outputId":"0676b607-c587-482d-ff58-50f457cc09dd","trusted":true},"cell_type":"code","source":"train.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"jL0QKlUiFbhF","outputId":"b311942c-2ada-45c8-b83b-898bf2e1cfc8","trusted":true},"cell_type":"code","source":"length_train = train['tweet'].str.len()\nlenght_test = test['tweet'].str.len()\nplt.hist(length_train, bins= 20, label = 'Train_tweets')\nplt.hist(lenght_test, bins = 20, label = 'test_tweets')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"kk01DABXGWTR","outputId":"9c15c1ab-051a-47e1-9831-6ccc24d6ca74","trusted":true},"cell_type":"code","source":"# combine the data (train + test)\ndf = train.append(test, ignore_index=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"U2-LP0BXHAXH"},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{"id":"3ZXJ6DN6G8Xr","trusted":true},"cell_type":"code","source":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, \"\", input_txt)\n    return input_txt","execution_count":null,"outputs":[]},{"metadata":{"id":"QgC1dP70yriy"},"cell_type":"markdown","source":"### Removing twitter Handles  \nNote that we have passed “@[]*” as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ‘@’.","execution_count":null},{"metadata":{"id":"WjYfgTR0IDdf","outputId":"88a134e3-d864-43de-f3bd-19752493f8c0","trusted":true},"cell_type":"code","source":"df['tidy_tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Zaxv1jf2mH0_"},"cell_type":"markdown","source":"### Removing Punctuations, Numbers, and Special Characters  \nHere we will replace everything except characters and hashtags with spaces. The regular expression “[^a-zA-Z#]” means anything except alphabets and ‘#’.","execution_count":null},{"metadata":{"id":"ifqAiOYPRWp9","outputId":"d861b873-8984-49a7-d575-0d3aa58acb29","trusted":true},"cell_type":"code","source":"df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"99WW2ZAfoLhW"},"cell_type":"markdown","source":"### Removing short words  \nWe have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.\n","execution_count":null},{"metadata":{"id":"1gKBtZpGmnjZ","outputId":"091482fc-caeb-42d4-870a-ec8ffa7a4786","trusted":true},"cell_type":"code","source":"df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"o78lsIbesmHC"},"cell_type":"markdown","source":"### Text Normalization  \nHere we will use nltk’s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","execution_count":null},{"metadata":{"id":"9BPokc2VsTqt","outputId":"71eea1d4-e41d-44f8-b072-9c590f9d4ff9","trusted":true},"cell_type":"code","source":"tokenized_tweet = df['tidy_tweet'].apply(lambda x: x.split()) \ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Kk2avUOhs-Qn","trusted":true},"cell_type":"code","source":"from nltk.stem.porter import *\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"id":"9WnN8zuttM-d","trusted":true},"cell_type":"code","source":"# Normalize the tokenized tweets\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])","execution_count":null,"outputs":[]},{"metadata":{"id":"ZuxcUQTPt-KI","trusted":true},"cell_type":"code","source":"# stitch these tokens back together, using nltk's MosesDetokenizer function\nfor i in range(len(tokenized_tweet)):\n  tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\ndf['tidy_tweet'] = tokenized_tweet","execution_count":null,"outputs":[]},{"metadata":{"id":"So96wfzdQLTO"},"cell_type":"markdown","source":"## 3. Story Generation and Visualization from Tweets","execution_count":null},{"metadata":{"id":"SNl6fbsMSwpU"},"cell_type":"markdown","source":"### A) Understanding the common words used in the tweets: WordCloud  \nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.","execution_count":null},{"metadata":{"id":"ubQGbhulTUrE","outputId":"953578b8-d05f-4643-98c1-df63f24a4dc1","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"HzQvV8p7NzHj","trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in df['tidy_tweet']])","execution_count":null,"outputs":[]},{"metadata":{"id":"QqTkA4REUjPU","outputId":"46f3d2ea-e4a0-4e7f-8a17-5220fcd2b39e","trusted":true},"cell_type":"code","source":"all_words","execution_count":null,"outputs":[]},{"metadata":{"id":"JleU4wQ5PtOS","outputId":"a8ed7f54-60c5-420d-96eb-07faae0a2977","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"gOloW2l9U8fT"},"cell_type":"markdown","source":"### B) Words in non racist tweets","execution_count":null},{"metadata":{"id":"A2mErwWWUUCr","outputId":"0953f643-2d3c-4eee-e8ea-6bb8890d66d0","trusted":true},"cell_type":"code","source":"normal_words = \" \".join([text for text in df['tidy_tweet'][df['label'] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, \n                      max_font_size = 110).generate(normal_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"8Aghaqs_Vn5a"},"cell_type":"markdown","source":"### C) Words in racist tweets","execution_count":null},{"metadata":{"id":"WyHYVy1oVgiN","outputId":"48bdbf27-6a15-493f-fcd1-df18b8ea3b81","trusted":true},"cell_type":"code","source":"negative_words = \" \".join([text for text in df['tidy_tweet'][df['label'] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, random_state = 21, \n                      max_font_size = 110).generate(negative_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"P8vf_XyDWGhQ"},"cell_type":"markdown","source":"### D) Understanding the impact of Hashtags on tweets sentiment","execution_count":null},{"metadata":{"id":"6yB8VeTsV76o","trusted":true},"cell_type":"code","source":"# function to collect hashtags \ndef hashtag_extract(x):\n    hashtags = []\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n    return hashtags","execution_count":null,"outputs":[]},{"metadata":{"id":"nBmNlCcfXW1P","trusted":true},"cell_type":"code","source":"# extracting hashtags from non racist tweets\nHT_regular = hashtag_extract(df['tidy_tweet'][df['label'] == 0])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GGju-3MWXxQq","trusted":true},"cell_type":"code","source":"# extracting hashtags from racist tweets\nHT_negative = hashtag_extract(df['tidy_tweet'][df['label'] ==1])","execution_count":null,"outputs":[]},{"metadata":{"id":"YAdIs-ZOYMGM","trusted":true},"cell_type":"code","source":"# unnesting list \nHT_regular = sum(HT_regular, [])\nHT_negative = sum(HT_negative, [])","execution_count":null,"outputs":[]},{"metadata":{"id":"8w-LbUj8YpKB","trusted":true},"cell_type":"code","source":"# plot the top n hashtags\na = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})","execution_count":null,"outputs":[]},{"metadata":{"id":"Swz_-wdLU3Xb"},"cell_type":"markdown","source":"**Non-Racist Tweets**","execution_count":null},{"metadata":{"id":"DGWohGgdaWVC","outputId":"6cffdd3c-f8fd-4f1e-9813-219cdbf37e78","trusted":true},"cell_type":"code","source":"# selecting top 20 most frequent hashtags\nd = d.nlargest(columns='Count', n = 20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = d, x = 'Hashtag', y= 'Count')\nax.set(ylabel = 'Count')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"id":"HbIIfj37VKeF"},"cell_type":"markdown","source":"**Racist Tweets**","execution_count":null},{"metadata":{"id":"WI5RMqEhaXrN","outputId":"bfcbf066-6062-4700-d0e4-d2c268c564c3","trusted":true},"cell_type":"code","source":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()),\n                  'Count': list(b.values())})\ne = e.nlargest(columns='Count', n = 20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = e, x = 'Hashtag', y= 'Count')\nax.set(ylabel = 'Count')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"id":"jsNvJ3NTasGM"},"cell_type":"markdown","source":"## 4. Extracting Features from Cleaned Tweets","execution_count":null},{"metadata":{"id":"aLsE9K3lavuG"},"cell_type":"markdown","source":"### 1. Bag-of-words Features","execution_count":null},{"metadata":{"id":"ALQs8-dQVrP_","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"id":"5tKY3zBncLqE","outputId":"429d4604-7bd8-4af0-dfb0-ba7401bb7e8e","trusted":true},"cell_type":"code","source":"bow_vectorizer = CountVectorizer(max_df= 0.90, min_df= 2,max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(df['tidy_tweet'])\nbow.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"5vBCh0OcykzX"},"cell_type":"markdown","source":"### 2. TF-IDF Features\nTF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n\nLet’s have a look at the important terms related to TF-IDF:\n\n* TF = (Number of times term t appears in a document)/(Number of terms in the document)\n\n* IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n\n* TF-IDF = TF*IDF","execution_count":null},{"metadata":{"id":"LOQhVxe7cxNq","outputId":"35c7356d-2387-456e-de21-5fb93d0ce632","trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df['tidy_tweet'])\ntfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Ve9d2XHJzp1R"},"cell_type":"markdown","source":"### 3. Word2Vec Features  \nWord embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like King -man +woman = Queen, which is mind-blowing.   \nThe advantages of using word embeddings over BOW or TF-IDF are:\n\n* Dimensionality reduction - significant reduction in the no. of features required to build a model.\n\n* It capture meanings of the words, semantic relationships and the different types of contexts they are used in.\n","execution_count":null},{"metadata":{"id":"9okpg2k0zioY","outputId":"c0f3e065-c46a-43b0-bc9b-6ec65693d91d","trusted":true},"cell_type":"code","source":"tokenized_tweet = df['tidy_tweet'].apply(lambda x: x.split())\nmodel_w2v = gensim.models.Word2Vec(tokenized_tweet, \n                                   size = 200, \n                                   window = 5, \n                                   min_count = 2, \n                                   sg = 1, \n                                   hs = 0, \n                                   negative = 10, \n                                   workers = 2, \n                                   seed = 34)\nmodel_w2v.train(tokenized_tweet, total_examples = len(df['tidy_tweet']), epochs =20)","execution_count":null,"outputs":[]},{"metadata":{"id":"BY1R6OtB9MUn","outputId":"20f8b453-4859-4b8f-c67a-b3a0fe4eb9d1","trusted":true},"cell_type":"code","source":"# We will specify a word and the model will pull out the most similar words from the corpus.\nmodel_w2v.wv.most_similar(positive = 'dinner')","execution_count":null,"outputs":[]},{"metadata":{"id":"L8mcBh5c-Flq"},"cell_type":"markdown","source":"we can see that our word2vec model does a good job of finding the most similar words for a given word. But how is it able to do so? That’s because it has learned vectors for every unique word in our data and it uses cosine similarity to find out the most similar vectors (words).","execution_count":null},{"metadata":{"id":"SFNCOmkL-N-Y"},"cell_type":"markdown","source":"**Preparing Vectors for Tweets**  \nSince our data contains tweets and not just words, we’ll have to figure out a way to use the word vectors from word2vec model to create vector representation for an entire tweet. There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. The length of the resultant vector will be the same, i.e. 200. We will repeat the same process for all the tweets in our data and obtain their vectors. Now we have 200 word2vec features for our data.\n\nWe will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet.","execution_count":null},{"metadata":{"id":"aemnDYqF94Sl","trusted":true},"cell_type":"code","source":"def word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1,size))\n    count = 0\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n        except KeyError:\n            continue\n    if count != 0: \n        vec /= count\n    return vec\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1hqJ6604_P2f","outputId":"918d6571-8d19-4179-e058-3a4cf566d98c","trusted":true},"cell_type":"code","source":"wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Qaix7I0GAJRc"},"cell_type":"markdown","source":"Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features.","execution_count":null},{"metadata":{"id":"hBiBDuksC1ku"},"cell_type":"markdown","source":"### 4. Doc2Vec Embedding\n Doc2Vec model is an unsupervised algorithm to generate vectors for sentence/paragraphs/documents. This approach is an extension of the word2vec. The major difference between the two is that doc2vec provides an additional context which is unique for every document in the corpus. This additional context is nothing but another feature vector for the whole document. This document vector is trained along with the word vectors.","execution_count":null},{"metadata":{"id":"DgTiki2S_9ax","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas(desc = 'progress-bar')\nfrom gensim.models.doc2vec import LabeledSentence","execution_count":null,"outputs":[]},{"metadata":{"id":"Fi42pkpMDelF"},"cell_type":"markdown","source":"To implement doc2vec, we have to labelise or tag each tokenised tweet with unique IDs. We can do so by using Gensim’s LabeledSentence() function.","execution_count":null},{"metadata":{"id":"HbtFvnNJDdyt","trusted":true},"cell_type":"code","source":"def add_label(twt):\n    output = []\n    for i, s in zip(twt.index, twt):\n        output.append(LabeledSentence(s, ['tweet_' + str(i)]))\n    return output\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fHBVIsQwD3sI","trusted":true},"cell_type":"code","source":"labeled_tweets = add_label(tokenized_tweet)","execution_count":null,"outputs":[]},{"metadata":{"id":"GKVqtOftD8Cv","outputId":"d95dd6f5-0df0-4337-f8ec-69640fceb031","trusted":true},"cell_type":"code","source":"labeled_tweets[:6]","execution_count":null,"outputs":[]},{"metadata":{"id":"cHsWukYMD871","outputId":"1f05c578-657c-4466-85fc-a7aa1572b5e9","trusted":true},"cell_type":"code","source":"# Now let's train a doc2vec model\nmodel_d2v = gensim.models.Doc2Vec(dm=1, \n                                  dm_mean = 1, \n                                  size = 200, \n                                  window = 5, \n                                  negative = 7, \n                                  min_counts = 5, \n                                  workers = 3, \n                                  alpha = 0.1, \n                                  seed = 23)\nmodel_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\nmodel_d2v.train(labeled_tweets, total_examples = len(df['tidy_tweet']), epochs = 15)","execution_count":null,"outputs":[]},{"metadata":{"id":"A3azu7eHIe55","outputId":"5cb4745f-3223-4b6b-83a2-bfa0a131783e","trusted":true},"cell_type":"code","source":"# Preparing doc2vec feature set\ndocvec_arrays = np.zeros((len(tokenized_tweet), 200))\nfor i in range(len(df)):\n    docvec_arrays[i, :] = model_d2v.docvecs[i].reshape((1, 200))\ndocvec_df = pd.DataFrame(docvec_arrays)\ndocvec_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Z1nBWxoKJkMT"},"cell_type":"markdown","source":"## 5. Model Building: Sentiment Analysis","execution_count":null},{"metadata":{"id":"ab7av0I_JvJf"},"cell_type":"markdown","source":"### 1. Logistic Regression","execution_count":null},{"metadata":{"id":"iBonFDxhJVxr","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"id":"uJUyvbyQKaAE"},"cell_type":"markdown","source":"#### Bag-of-words features","execution_count":null},{"metadata":{"id":"-2SdqCWwKMBM","trusted":true},"cell_type":"code","source":"train_bow = bow[:31962, :]\ntest_bow = bow[31962:, :]\n\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],\n                                                          random_state=42,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"id":"t2HsduY1KuZZ","outputId":"38a33218-b8fb-4c5f-a0af-17a134c3e972","trusted":true},"cell_type":"code","source":"lreg = LogisticRegression()\nlreg.fit(xtrain_bow, ytrain)\nprediction = lreg.predict_proba(xvalid_bow)\nprediction_int = prediction[:, 1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"lV_J3vMTQl9D","trusted":true},"cell_type":"code","source":"# Now let's make predictions for the test dataset and create a submission file\n\ntest_pred = lreg.predict_proba(test_bow)\ntest_pred_int = test_pred[:, 1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['label'] = test_pred_int\nsubmission = test[['id', 'label']]\nsubmission.to_csv('sub_lreg_bow.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"9KyygVKnR027"},"cell_type":"markdown","source":"#### TF-IDF features","execution_count":null},{"metadata":{"id":"VSPCw5YgRiSN","outputId":"9e8175c8-fa30-4239-ef2c-c36e56c1e412","trusted":true},"cell_type":"code","source":"train_tfidf = tfidf[:31962, :]\ntest_tfidf = tfidf[31962:, :]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"jikdqJDjqlpK"},"cell_type":"markdown","source":"#### Word2Vec Features","execution_count":null},{"metadata":{"id":"T-_M1EIZrNF6","outputId":"04159cf2-3272-449f-f7bc-805fb554920b","trusted":true},"cell_type":"code","source":"ytrain.count() , yvalid.count() ","execution_count":null,"outputs":[]},{"metadata":{"id":"z6wXlWL6ssdk","outputId":"458eba33-206c-4078-a989-5a4f6b45824f","trusted":true},"cell_type":"code","source":"wordvec_df.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rowm12oUqdWu","outputId":"26ec87a1-560f-4aa6-d0e1-215613d75f41","trusted":true},"cell_type":"code","source":"train_w2v = wordvec_df.iloc[:31962, :]\ntest_w2v = wordvec_df.iloc[31962:, :]\n\nxtrain_w2v = train_w2v.iloc[ytrain.index, :]\nxvalid_w2v = train_w2v.iloc[yvalid.index, :]\n\nlreg.fit(xtrain_w2v, ytrain)\nprediction = lreg.predict_proba(xvalid_w2v)\nprediction_int = prediction[:,1] >=0.3\nprediction_int= prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"qlN63D3SuSiY"},"cell_type":"markdown","source":"#### Doc2Vec features","execution_count":null},{"metadata":{"id":"iJ1G6KYIuRrF","outputId":"717594d7-80ed-4069-d7b2-fa5d85c1ec72","trusted":true},"cell_type":"code","source":"train_d2v = docvec_df.iloc[:31962, :]\ntest_d2v = docvec_df.iloc[31962:, :]\n\nxtrain_d2v = train_d2v.iloc[ytrain.index, :]\nxvalid_d2v = train_d2v.iloc[yvalid.index, :]\n\nlreg.fit(xtrain_d2v, ytrain)\nprediction = lreg.predict_proba(xvalid_d2v)\nprediction_int = prediction[:, 1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"o0uqcZyYxF1N"},"cell_type":"markdown","source":"### 2. Support Vector Machine (SVM)","execution_count":null},{"metadata":{"id":"zRlSEkuQsPO6","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(kernel = 'linear', C = 1, probability = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"gSgDK2Lty0rx"},"cell_type":"markdown","source":"#### Bag-of-words features","execution_count":null},{"metadata":{"id":"XGJ9n8Ray2pF","outputId":"f6c5c3fe-ab6f-4d5e-d9d2-dc24174a2bca","trusted":true},"cell_type":"code","source":"svc.fit(xtrain_bow, ytrain)\n\nprediction = svc.predict_proba(xvalid_bow)\nprediction_int = prediction[:, 1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"CsacRGvezNRD","trusted":true},"cell_type":"code","source":"# Creating submission file\n\ntest_pred = svc.predict_proba(test_bow)\ntest_pred_int = test_pred[:, 1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\n\ntest['label'] = test_pred_int\nsubmission = test[['id', 'label']]\nsubmission.to_csv('sub_svm_bow.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"lRN-H3mW5wn2"},"cell_type":"markdown","source":"General function for all the model implementation","execution_count":null},{"metadata":{"id":"SnFC8LUE4a6n","trusted":true},"cell_type":"code","source":"def model_apply(model, training_data, validation_data):\n    model.fit(training_data, ytrain)\n    prediction = model.predict_proba(validation_data)\n    prediction_int = prediction[:, 1] >= 0.3\n    prediction_int = prediction_int.astype(np.int)\n    f1_scor = f1_score(yvalid, prediction_int)\n    print(f1_scor)","execution_count":null,"outputs":[]},{"metadata":{"id":"9KtPf0CP3ogN"},"cell_type":"markdown","source":"#### TF-IDF features","execution_count":null},{"metadata":{"id":"U9MKeDg-5Lfg","outputId":"648a4e86-b8d5-4e6b-8fd7-d26504122791","trusted":true},"cell_type":"code","source":"model_apply(svc, xtrain_tfidf, xvalid_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"id":"X1eHMl9v4KUx"},"cell_type":"markdown","source":"#### Word2Vec Features","execution_count":null},{"metadata":{"id":"Gb8owsyk5VXg","outputId":"5bace9a0-706e-4d38-8b46-50ed447efed0","trusted":true},"cell_type":"code","source":"model_apply(svc, xtrain_w2v, xvalid_w2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"grL4MB3Q6IQP"},"cell_type":"markdown","source":"#### Doc2Vec features","execution_count":null},{"metadata":{"id":"gUv8rRZi6EE4","outputId":"aa3ee273-63d3-412f-8376-1fc0a847dbc1","trusted":true},"cell_type":"code","source":"model_apply(svc, xtrain_d2v, xvalid_d2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"kMVKLwuZ6V7J"},"cell_type":"markdown","source":"### 3. Random Forest","execution_count":null},{"metadata":{"id":"7hCt00n26Yfg","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 400, random_state = 11)","execution_count":null,"outputs":[]},{"metadata":{"id":"8t_e6aMU6-Kb","trusted":true},"cell_type":"code","source":"# General function\ndef model_imp(model, training_data, validation_data):\n    model.fit(training_data, ytrain)\n    prediction = model.predict(validation_data)\n    print(f1_score(yvalid, prediction))","execution_count":null,"outputs":[]},{"metadata":{"id":"3gYZta_m3o76"},"cell_type":"markdown","source":"#### Bag-of-words features","execution_count":null},{"metadata":{"id":"DW8RP02S3bwX","outputId":"ddfd86f5-9f58-43bb-fbd5-9efc30cc28ab","trusted":true},"cell_type":"code","source":"model_imp(rf, xtrain_bow, xvalid_bow)","execution_count":null,"outputs":[]},{"metadata":{"id":"bUIpsUih4Hys","trusted":true},"cell_type":"code","source":"# submission file\ntest_pred = rf.predict(test_bow)\ntest['label'] = test_pred\nsubmission = test[['id', 'label']]\nsubmission.to_csv('sub_rf_bow.csv', index =False)","execution_count":null,"outputs":[]},{"metadata":{"id":"xH7pJxTl38Mv"},"cell_type":"markdown","source":"#### TF-IDF features","execution_count":null},{"metadata":{"id":"ikJQUqg73vq_","outputId":"d38d4022-cd39-4c2a-d3c8-4fa5d3a2a2ce","trusted":true},"cell_type":"code","source":"model_imp(rf, xtrain_tfidf, xvalid_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"id":"P8ABJ81V4uvZ"},"cell_type":"markdown","source":"#### Word2Vec Features","execution_count":null},{"metadata":{"id":"RQCkn-_w4uCu","outputId":"7ce032b2-c5b8-44fe-c4aa-fec12c7b792a","trusted":true},"cell_type":"code","source":"model_imp(rf, xtrain_w2v, xvalid_w2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"H2Yr1tbn45N2"},"cell_type":"markdown","source":"#### Doc2Vec Features","execution_count":null},{"metadata":{"id":"zwxaquOp5AVJ","outputId":"6b82b35f-abeb-4d07-8e6b-d4893095505d","trusted":true},"cell_type":"code","source":"model_imp(rf, xtrain_d2v, xvalid_d2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"ofud_Gyq5Hzg"},"cell_type":"markdown","source":"### 3. XGBoost","execution_count":null},{"metadata":{"id":"r_ThI0cY5cKo","trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth = 6, n_estimators = 1000)","execution_count":null,"outputs":[]},{"metadata":{"id":"aSWV6YIN5WgJ"},"cell_type":"markdown","source":"#### Bag-of-words features","execution_count":null},{"metadata":{"id":"H0-VaFQG5LOB","outputId":"2d76d7fc-2c93-4833-dcee-9b446c19ba25","trusted":true},"cell_type":"code","source":"model_imp(xgb, xtrain_bow, xvalid_bow)","execution_count":null,"outputs":[]},{"metadata":{"id":"Du3cubzp56NY","trusted":true},"cell_type":"code","source":"# submission file\ntest_pred = xgb.predict(test_bow)\ntest['label'] = test_pred\nsubmission = test[['id', 'label']]\nsubmission.to_csv('sub_xgb_bow.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"xVTsLE5W6WH_"},"cell_type":"markdown","source":"#### TF-IDF features","execution_count":null},{"metadata":{"id":"taPdvuvD6QNZ","outputId":"358e9618-2330-4072-f55f-06528ffc2f9e","trusted":true},"cell_type":"code","source":"model_imp(xgb, xtrain_tfidf, xvalid_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"id":"qfhB6cS06ZGp"},"cell_type":"markdown","source":"#### Word2Vec features","execution_count":null},{"metadata":{"id":"THieEllo6ViH","outputId":"f27117a0-4a3f-4abf-f974-be7d988a35f1","trusted":true},"cell_type":"code","source":"model_imp(xgb, xtrain_w2v, xvalid_w2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"ciukNVuX-GFj"},"cell_type":"markdown","source":"Best performance till now","execution_count":null},{"metadata":{"id":"Hqh94hdS6nMb"},"cell_type":"markdown","source":"#### Doc2vec features","execution_count":null},{"metadata":{"id":"9fnkCa4g6hxG","outputId":"590f8baf-4399-40ae-cfc7-0eb823b0032c","trusted":true},"cell_type":"code","source":"model_imp(xgb, xtrain_d2v, xvalid_d2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"y4FGyiSt-MPT"},"cell_type":"markdown","source":"## Fine Tuning XGBoost + Word2Vec","execution_count":null},{"metadata":{"id":"pdlqM-aX96r3","trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"id":"xoSkxrs4-gOO"},"cell_type":"markdown","source":"A DMatrix can contain both the features and the target","execution_count":null},{"metadata":{"id":"iVXn2o1Q-aDP","trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(xtrain_w2v, label = ytrain)\ndvalid = xgb.DMatrix(xvalid_w2v, label = yvalid)\ndtest = xgb.DMatrix(test_w2v)","execution_count":null,"outputs":[]},{"metadata":{"id":"BJKNzyuu-4qG","trusted":true},"cell_type":"code","source":"# Parameters that we are going to tune\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 6,\n    'min_child_weight': 1, \n    'eta': 0.3,\n    'subsample': 1, \n    'colsample_bytree': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"id":"Pv7h43-z_SpX","trusted":true},"cell_type":"code","source":"# custom evaluation metric to calculate f1 score\n\ndef custom_eval(preds, dtrain):\n    labels = dtrain.get_label().astype(np.int)\n    preds = (preds >= 0.3).astype(np.int)\n    return[('f1_score', f1_score(labels, preds))]","execution_count":null,"outputs":[]},{"metadata":{"id":"0vDwrGQSROAQ"},"cell_type":"markdown","source":"We will follow the steps below to tune the parameters.  \n\n* Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n\n* Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n\n* Tune the learning rate.\n\n* Finally tune gamma to avoid overfitting.","execution_count":null},{"metadata":{"id":"CuQtjYHiRUgQ","outputId":"88b2d02a-887b-40e1-fa1e-ea8127342779","trusted":true},"cell_type":"code","source":"# Tuning max_depth and min_child_weight\ngridsearch_params = [\n                     (max_depth, min_child_weight)\n                     for max_depth in range(6,10)\n                     for min_child_weight in range(5,8)]\n\nmax_f1 = 0\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print('CV with max_depth = {}, min_child_weight ={}'.format(\n      max_depth, min_child_weight))\n  \n\n\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n\n#Cross-validation\n    cv_results = xgb.cv(params, dtrain, feval = custom_eval, \n                    num_boost_round = 200, \n                    maximize = True, \n                    seed = 16, \n                    nfold =5, \n                    early_stopping_rounds = 10)\n\n# Finding best F1 Score\n\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1  \n        best_params = (max_depth, min_child_weight)\nprint('Best params: {}, {}, F1 Score: {}'.format(best_params[0], best_params[1], max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"fd5SD6QEbx_V","trusted":true},"cell_type":"code","source":"params['max_depth'] = 8\nparams['min_child_weight'] = 6","execution_count":null,"outputs":[]},{"metadata":{"id":"x1pSCN-XZvrC","outputId":"d9b4f9fb-460a-4d26-a17d-f60e22772311","trusted":true,"collapsed":true},"cell_type":"code","source":"# Tuning subsample and colsample\ngridsearch_params = [\n                     (subsample, colsample)\n                     for subsample in [i/10 for i in range(5,10)]\n                     for colsample in [i/10 for i in range(5,10)]\n]\nmax_f1 = 0\nbest_params = None\nfor subsample, colsample in gridsearch_params:\n    print(\"CV with subsample = {}, colsample = {}\".format(\n        subsample, colsample\n  ))\n  # Update our parameters\n    params['colsample'] = colsample\n    params['subsample'] = subsample\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=200,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=10\n    )\n     # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = (subsample, colsample) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"Xt2-_pSJc5IH","outputId":"e3e6efe4-0298-4006-dbf5-08bbdc4d2b26","trusted":true},"cell_type":"code","source":"# Tuning the learning rate\n\nmax_f1 = 0. \nbest_params = None \nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n     # Update ETA\n    params['eta'] = eta\n\n     # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=1000,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=20\n    )\n\n     # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = eta \nprint(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"BXj_iMGqdNtS","trusted":true},"cell_type":"code","source":"# finally tuned parameters\nparams = { \n 'colsample_bytree': 0.5, 'eta': 0.1,\n 'max_depth': 8, 'min_child_weight': 7,\n 'objective': 'binary:logistic',\n 'subsample': 0.9}\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"S28zRw6Dd2bb","outputId":"3dd4423c-d20a-44bc-a294-718424c2d548","trusted":true},"cell_type":"code","source":"\nxgb_model = xgb.train(\n    params,\n    dtrain,\n    feval= custom_eval,\n    num_boost_round= 1000,\n    maximize=True,\n    evals=[(dvalid, \"Validation\")],\n    early_stopping_rounds=10\n )","execution_count":null,"outputs":[]},{"metadata":{"id":"EkO--knUd9Cj","trusted":true},"cell_type":"code","source":"# Final submission file\ntest_pred = xgb_model.predict(dtest)\ntest['label'] = (test_pred >= 0.3).astype(np.int)\nsubmission = test[['id', 'label']]\nsubmission.to_csv('sub_xgb_w2v_fintuned.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"8Hd-YpN-g2Fd","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}