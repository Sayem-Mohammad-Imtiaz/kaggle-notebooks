{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change the class(e,p) to (0,1). That means 0 is eatable and 1 is poisonous\ndf['class'].replace(['e','p'],[0,1],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get dummy variables\ndf = pd.get_dummies(df,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['class'],axis=1)\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import different algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Run cross validation to get accuracy score and confusion matrix\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare models\nmodels=[]\nmodels.append(('LR',LogisticRegression()))\nmodels.append(('RFC',RandomForestClassifier()))\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('NB',GaussianNB()))\nmodels.append(('SVC',svm.SVC()))\nmodels.append(('GBC',GradientBoostingClassifier()))\nmodels.append(('XGC',XGBClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\nfor name, model in models:\n    model.fit(X_train,y_train)\n    score = model.score(X_test,y_test)\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    ax = sns.heatmap(cm,fmt='d',annot=True,xticklabels=['Predicted_0','Predicted_1'],yticklabels=['True_0','True_1'])\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    plt.show()\n    accuracy = \"%s: %f\" % (name, score)\n    print(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyper-parameters of logisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nc_n=np.logspace(-3,3,7)\ncross_val_scores=[]\nfor i in c_n:\n    logmodel = LogisticRegression(C=i,solver='liblinear')\n    scores=cross_val_score(logmodel, X_train, y_train, cv=10,scoring='accuracy')\n    cross_val_scores.append(np.mean(scores))\nprint(\"best cross-validation score: {:.3f}\".format(np.max(cross_val_scores)))\nbest_c_n=c_n[np.argmax(cross_val_scores)]\nprint(\"best c_n: {}\".format(best_c_n))\n\nlogmodel=LogisticRegression(C=best_c_n,solver='liblinear')\nlogmodel.fit(X_train, y_train)\nprint(\"test score: {:.3f}\".format(logmodel.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams1 = {'n_estimators':range(30, 200,5)}\ngrid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid=params1, cv=10, scoring='accuracy', return_train_score=True)\ngrid.fit(X_train, y_train)\n\nprint(grid.best_estimator_)\nprint(grid.best_params_)\nprint(grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tuning hyper-parameter of XGBClassifier\nimport time\nparams2={'max_depth':np.arange(3,7,1)}\ngrid = GridSearchCV(XGBClassifier(), param_grid=params2, scoring='accuracy', cv=10 )\n\nstart=time.time()\ngrid.fit(X_train, y_train)\nend=time.time()\nprint(end-start)\n\nprint(grid.best_estimator_)\nprint(grid.best_score_)\nprint(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find top 10 important features for RFC\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\nimportance = model.feature_importances_\nindices = np.argsort(importance)[::-1][0:10]\nlabels = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize top 10 important features\nplt.title('Feature Importance')\nplt.bar(range(10),importance[indices],color='lightblue',align='center')\nplt.xticks(range(10),labels[indices],rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}