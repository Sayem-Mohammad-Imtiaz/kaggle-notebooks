{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, roc_auc_score, roc_curve, accuracy_score\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    The source of the data is UC Irvine Machine Learning. So, this dataset is from a university that provides reliable data for students to practice with machine learning techniques. In this data, there are 11 columns of measured indicators that influence overall quality, which is the 12th column. The quality is a continuous varible in the range of [3-8].\n    I chose this dataset because I am interested in what indicators have an influence over the quality of the wine. You always hear about wine critics and how that profession sometimes is not the best, to put it lightly. However, is there some science to the idea that someone can taste wine and understand its quality. Furthermore, can a computer be trained to predict the wine quality, so you can efficiently make the best wine possible. ","metadata":{}},{"cell_type":"markdown","source":"# Explore the Data","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby('quality').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the range of quality is [3,8], the cut off for 'good' wine will be 6.5","metadata":{}},{"cell_type":"code","source":"data.groupby('quality').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that a linear regression exists with quality and volatile acidity, citric acid, chlorides, sulphates. These will be important in how the algorithms consider quality.","metadata":{}},{"cell_type":"markdown","source":"# Transform the Quality Column and Visualize the Data","metadata":{}},{"cell_type":"code","source":"data['QualityB']=data['quality'] >= 6.5\ndata['QualityB']=data['QualityB'].astype(int)\ndataset=data.drop('quality',axis=1)\ndataset.groupby('QualityB').count()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(dataset.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the heatmap and correlation, when comparing quality,alcohol is strongest correlation, along with  volatile acid,citric acid and sulphates being close behind.","metadata":{}},{"cell_type":"code","source":"sns.catplot(x=\"quality\",y=\"alcohol\",hue=\"QualityB\",data=data,kind=\"violin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"quality\",y=\"volatile acidity\",hue=\"QualityB\",data=data,kind=\"violin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"quality\",y=\"citric acid\",hue=\"QualityB\",data=data,kind=\"violin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"quality\",y=\"sulphates\",hue=\"QualityB\",data=data,kind=\"violin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data.drop(\"quality\",axis=1),hue=\"QualityB\",vars=[\"alcohol\",\"volatile acidity\",\"citric acid\",\"sulphates\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The four columns visualized above deliver an insight to where the quality is good (QualityB = 1) or bad (QualityB = 0). These have the strongest correlation, so they help understand what factors into making a quality wine.","metadata":{}},{"cell_type":"markdown","source":"# Split the dataset into train and test sets","metadata":{}},{"cell_type":"code","source":"X=dataset.iloc[:,0:-1]\ny=dataset.QualityB\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logsitic Regression:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogReg = LogisticRegression()\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nLogReg.fit(X_train,y_train)\ny_pred=LogReg.predict(X_test)\nprint(\"Accuracy:\",LogReg.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classification:","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion='entropy', random_state=99)\nclf.fit(X_train,y_train)\ny_pred= clf.predict(X_test)\naccuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25, 15))\ntree.plot_tree(decision_tree=clf, max_depth= 3,fontsize=12);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier:","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_cl = RandomForestClassifier(n_estimators=100,\n                            random_state=99)\nrf_cl.fit(X_train, y_train)\nrf_pred= rf_cl.predict(X_test)\naccuracy = float(np.sum(rf_pred==y_test))/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Classification:","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_cl = xgb.XGBClassifier(objective='binary:logistic',verbosity = 0,use_label_encoder=False, max_depth=10, n_estimators=100, seed=99) \nxg_cl.fit(X_train, y_train)\npreds = xg_cl.predict(X_test) \nprint(\"accuracy: %f\" % (float(np.sum(preds==y_test))/y_test.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(xg_cl,importance_type='weight',grid=False,height=.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network with Keras:","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Dense(8,input_shape=(11,),activation='softmax'))\nmodel.add(Dense(4,activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(2,activation='softmax'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1,activation='softmax'))\nmodel.compile(optimizer='sgd',loss='binary_crossentropy')\nmodel.fit(X_train,y_train,epochs=5,validation_split=0.2)\npreds=model.predict(X_test)\nmodel.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network with Pytorch:","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class nnData(Dataset):\n    def __init__(self):\n        xy=dataset.to_numpy(dtype=np.float32)\n        self.x=torch.from_numpy(xy[:,:-1])\n        self.y=torch.from_numpy(xy[:,[-1]])\n        self.n_samples=xy.shape[0]\n    def __getitem__(self,index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return self.n_samples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self,n_inputs, hidden_size):\n        super(Net,self).__init__()\n        self.inputs= n_inputs\n        self.hidden= hidden_size\n        \n        self.fc1=nn.Linear(self.inputs,self.hidden)\n        self.fc2=nn.Linear(self.hidden,1)\n    \n    def forward(self,x):\n        out=self.fc1(x)\n        out=F.relu(out)\n        out=self.fc2(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_Data=nnData()\ntrainloader=DataLoader(dataset=nn_Data,batch_size=32,num_workers=1)\ntestloader=DataLoader(dataset=nn_Data,num_workers=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net=Net(n_inputs=11,hidden_size=10)\ncriterion= nn.CrossEntropyLoss()\noptimizer= optim.Adam(params=net.parameters(),lr=3e-4)\nfor epoch in range(1):\n    for inputs,labels in trainloader:\n        outputs=net.forward(inputs)\n        labels = np.argmax(labels,axis=1)\n        loss=criterion(outputs,labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct, total=0,0\npredictions=[]\nnet.eval()\nfor inputs,labels in testloader:\n    outputs=net(inputs)\n    _, predicted= torch.max(outputs.data,1)\n    predictions.append(outputs)\n    total +=labels.size(0)\n    correct +=(predicted==labels).sum().item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(correct/total)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison and Discussion ","metadata":{}},{"cell_type":"markdown","source":"Through all these techniques, the most accurate model is Random Forrest Cluster at 0.9125. This makes sense, as it uses many decision trees and 'votes'. Since the Decision Tree model was the third most accurate at .8825, using many of them and finding the mode of what they find should show promising results. However, it was somewhat suprising the XGBoost was not the most accurate at 0.907. It was my belief that it would be, based on less knowledge of the topic and my belief that a tree would not be the strongest predictor to use for the features. Then, the logistic regression model was the fourth most accurate at 0.88. The fifth most acurate model was the Pytorch Neural Network at 0.864. Then finally the Keras Neural Network model at 0.466. I am not suprised that the neural networks were the weakest models. I found that the choice of the optimizers and activation functions made the overall models weak since I was not sure exactly of them to use. Possibly neural networks could be the strongest models, as 0.9125 is not the most accurate, however using different functions and a better understanding in how to utilze them would be needed. ","metadata":{}},{"cell_type":"markdown","source":"Furthermore, something I would like to pose as a question, maybe I can get the answer in my feedback for this project, I am confused as to why my Keras Neural Network gives me a different accuracy score every time. All the other models return the same scores, so my intuitioin that it was how the data is split doesn't hold. It is probably easily explained, I am just curious as to why and how can it become consitent. ","metadata":{}}]}