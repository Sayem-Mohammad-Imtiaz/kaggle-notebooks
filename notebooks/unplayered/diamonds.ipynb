{"cells":[{"metadata":{},"cell_type":"markdown","source":"Problem Description: For those who love collecting rare diamonds, you might be curious: how much is the diamond actually worth? If it is worth millions of dollars, you might want to sell it in auction to make yourself rich. Hence, given some attributes of the diamond, you wish to write a machine learning algorithm to predict how much the diamond is worth, using the Diamonds dataset (https://www.kaggle.com/shivam2503/diamonds) containing detailed information of about 54000 diamonds.\n\nTeam members: Adriel, Chee Heng, Bohan\n\nObjective: Given the weight of the diamond, quality of the cut, diamond colour, clarity, length, valuewidth, depth of diamond and other attributes in the Diamonds dataset (other than the price), predict the price of the diamond in US dollars.\n\nMachine learning category: It is clear that this is a regression problem since the price of the diamond is a continuous variable. We are going to do supervised learning in this case. Unsupervised regression does not make sense since we need to know some data points before any regression can be done.\n"},{"metadata":{},"cell_type":"markdown","source":"These are the libraries required for the tutorial. The Python 3 interpreter does not have access to these libraries by default. We need to import them so that the Python 3 interpreter knows where to find the libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math, time # builtin libraries from the Python 3 system, but still requires importing\n\n# Scikit learn libraries\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, we need to read the Diamonds dataset. The dataset can be downloaded from https://www.kaggle.com/shivam2503/diamonds. For the code to work, the diamonds.csv directory must be the same as the notebook directory.\n\nThe diamonds.csv file is now read in the form of a Pandas Dataframe. By usual convention, the Dataframe is named df, even though any other name would work just fine, just that it would be very hard for users to read and understand the code.\n\nNow, let's take a look at the first 5 rows of the Dataframe. df.head(5) allows you to see the first 5 data points inside our dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/diamonds/diamonds.csv')\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the dimensions of the dataset using df.shape. As you can see, there are 53940 rows (indicating 53940 diamonds) and 11 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the columns contains an index, which is just called column 0. To remove a column, we use the drop method in the dataframe, as follows. axis=1 represents deleting a column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(df.columns[0],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might have noticed that some of the columns have data that are categorical, such as cut, which ranges from ‘Fair’ to ‘Ideal’. As a Ridge algorithm will not be able to handle such data (a runtime error will result), we will have to convert it to something that it can handle. \n\nNormally, one would convert the feature into a set of boolean features, for instance, the column ‘cut’ would be split into five columns, for example: ‘cut_Fair’ ‘cut_Good’, ‘cut_Very Good’, ‘cut_Premium’ and ‘cut_Ideal’. This particular encoding is called One Hot Encoding. The disadvantage is that memory usage is very high. We could cut the number of columns to four, as having zeroes for four of these categories implies the fifth category. However, the memory savings is insignificant when the number of columns is large. \n\nThe good news is that we can do better in memory usage while making more sense of the data at the same time since the data is ordinal (there exists an order, although we are unable to quantify the distance between each category). For instance, in the case of the ‘cut’ column, we know ‘Ideal’ is the best, followed by ‘Premium’, ‘Very Good’, ‘Good’ and finally ‘Fair’. Hence, we assign numbers to each category, in the order they come in. For instance, here ‘Fair’ is assigned 0, ‘Ideal’ is assigned 4, and everything else in between taking the remaining values.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cut_categories = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\nclarity_categories = ['I3','I2','I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF','FL']\ncolor_categories = ['J', 'I', 'H', 'G', 'F', 'E', 'D']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then convert the categorical values to the corresponding numbers as shown in the code below. The first 5 rows of the transformed Dataframe has been shown to help you visualize the difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This chunk of code is supposed to check whether the Dataframe has \n# already been converted into numeric indices.\n# If the Dataframe has already been converted into numeric indices, \n# the numeric indices will be converted to -1 without the check\nalready_converted = True\ntry:\n    float(df['cut'][0])\nexcept ValueError:\n    already_converted = False\n\nif already_converted == False:\n    cut_enc = pd.Categorical(df.cut, categories=cut_categories, ordered=True)\n    cut_labels, cut_unique = pd.factorize(cut_enc, sort=True)\n    df.cut = cut_labels\n\n    clarity_enc = pd.Categorical(df.clarity, categories=clarity_categories, ordered=True)\n    clarity_labels, clarity_unique = pd.factorize(clarity_enc, sort=True)\n    df.clarity = clarity_labels\n\n    color_enc = pd.Categorical(df.color, categories=color_categories, ordered=True)\n    color_labels, color_unique = pd.factorize(color_enc, sort=True)\n    df.color = color_labels\n\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have completed the data preprocessing. Now, the data is ready to be used for training our model! Firstly, we need to split the data into a training set and a test set, so as to prevent the possibility that the data has been overfitted to the training data by testing it against never-before-seen data. \n\nWe do this using train_test_split() from sklearn.model_selection. In this case, we use a test_size of 0.25, which means that 25% of the data points are used for the test set. (Chee Heng believes that Adriel previously forgot to include test_size parameter)\n\nWe used the current time as the random state to introduce some run-to-run variability for you to see the effect of the performance of the model when another test set has been chosen at random. However, when reproducible results are desired (e.g. for easy debugging), it is advisable to choose a fixed random state.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# You would want to drop the price column so that the machine learning program \n# does not predict the prices by peeking at the prices.\nX_vals = df.drop('price', axis=1) \n\ny_vals = df.price\nX_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, test_size=0.25, random_state=int(time.time()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the data has been split into the training set and the test set and we are ready to train our model! However, the Ridge algorithm has a hyperparameter called ‘alpha’. As we do not know what value of alpha to use, it is ideal for us to try multiple values of alpha and find the best one based on our chosen performance metric, namely the r^2 value of the best-fit regression line). We do this using GridSearchCV. The best parameters can be found using the best_params_ attribute while the best score can be found using best_score_ attribute.\n\n5-fold cross-validation was used so as to reduce the risk of overfitting by measuring the performance of the model against 5 non-overlapping unseen test sets, hence also testing the generalizability of the data set to unseen data, which are the diamonds in the training sets in this case.\n\nThe code could take seconds to run, please be patient here.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rdge = Ridge()\n\n# logspace is used because we do not really know the order of magnitude of logspace\nparam_grid = {'alpha':np.logspace(-6,6,100)} \n\ngscv = GridSearchCV(rdge,param_grid,cv=5)\ngscv.fit(X_train,y_train)\nprint(\"Best alpha value: {}\".format(gscv.best_params_['alpha']))\nprint(\"R-Squared value for training set: {}\".format(gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"R-Squared value for test set: {}\".format(gscv.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = gscv.predict(X_test)\nave_diff = 0\nave_pred = 0\nave_value = 0\nfor i in range(len(y_test)):\n    ave_diff += abs(y_test.iloc[i]-y_predict[i])\n    ave_pred += y_predict[i]\n    ave_value += y_test.iloc[i]\n    #print(\"Y=%s, Predicted=%s\" % (y_test.iloc[i], y_predict[i]))\nprint(\"average diffence: %s\" %(ave_diff/(len(y_test))))\nprint(\"average predicted value: %s\" %(ave_pred/(len(y_test))))\nprint(\"average actual value: %s\" %(ave_value/(len(y_test))))\nprint(\"error: %s\" %((ave_diff/(len(y_test)))/(ave_value/(len(y_test)))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# Any results you write to the current directory are saved as output.\n#print(int(time.time()))\ndf = df.drop(df.columns[0],axis=1)\nprint(df.columns)\n#print(set(df['cut'])) #Note: Fair < Good < Very Good < Premium < Ideal\n#print(set(df['color'])) #D ~ J\ncuttonum = {'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4}\nclaritytonum = {'I3':0,'I2':1,'I1':2,'SI2':3,'SI1':4,'VS2':5,'VS1':6,'VVS2':7,'VVS1':8,'IF':9,'FL':10}\n#'''\nfor count in range(len(df['cut'])):\n    df.loc[count,'cut'] = cuttonum[df.loc[count,'cut']]\n    df.loc[count,'clarity'] = claritytonum[df.loc[count,'clarity']]\n    df.loc[count,'color'] = ord(df.loc[count,'color'])-ord('D')\n    #df.loc[count,'price'] = math.log(df.loc[count,'price'])\n    if(count % 1000 == 0):\n        print(count)\n#'''\n#print(df.head())\nprint('Done!\\n')\ny_vals = df.price\nX_train, X_test, y_train, y_test = train_test_split(df.drop('price', axis=1), y_vals, random_state=int(time.time()))\n#print(X_train)\n#print(y_train)\n#print(y_train[12615])\n#for each in y_train[:500]:\n#    print(each)\nrdge = Ridge()\n#rdge.fit(X_train, y_train)\n#y_pred = rdge.predict(X_test)\nparam_grid = {'alpha':np.logspace(-6,6,50)}\ngscv = GridSearchCV(rdge,param_grid,cv=5)\ngscv.fit(X_train,y_train)\nprint(\"Best alpha value: {}\".format(gscv.best_params_['alpha']))\nprint(\"R-Squared value: {}\".format(gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdge = Ridge()\n#rdge.fit(X_train, y_train)\n#y_pred = rdge.predict(X_test)\nparam_grid = {'alpha':np.logspace(-6,6,200)}\ngscv = GridSearchCV(rdge,param_grid,cv=5)\ngscv.fit(X_train,y_train)\nprint(\"Best alpha value: {}\".format(gscv.best_params_['alpha']))\nprint(\"R-Squared value: {}\".format(gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"gscv.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gscv.predict(X_test))\n#print(X_test.head())\nprint(predictVals(X_test))\n#print(df.columns)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}