{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"* [Statistical Estimators](#Statistical Estimators)\n    - [Central Tendency](#Central Tendency)\n    - [Mean](#Mean)\n    - [Median](#Median)\n    - [Mode](#Mode)\n    - [Variance](#Variance)\n    - [Standard deviation](#Standard deviation)\n    - [Covariance](#Covariance)\n    - [Correlation / Pearson's Correlation](#Correlation / Pearson's Correlation)\n    - [Standard Error](#Standard Error)     \n* [Probability Distributions](#Probability Distributions)\n    - [Gausian / Normal distribution](#Gausian / Normal distribution)  \n         - [Standard Normal distribution](#Standard Normal distribution)\n    - [Student‚Äôs T-Distribution](#Student‚Äôs T-Distribution) \n    - [Binomial Distribution](#Binomial Distribution) \n    - [Bernoulli‚Äôs Distribution](#Bernoulli‚Äôs Distribution) \n    - [Uniform Distribution](#Uniform Distribution)\n    - [Exponential Distribution](#Exponential Distribution)\n    - [Poisson Distribution](#Poisson Distribution)\n    - [Probability Density Function and Probability Mass Function](#Probability Density Function and Probability Mass Function)\n* [P-value](#P-value)\n* [Degrees of Freedom](#Degrees of Freedom)\n* [Parametric vs. Non-Parametric Data](#Parametric vs. Non-Parametric Data) \n     - [Parametric Data](#Parametric Data)\n     - [Non-Parametric Data](#Non-Parametric Data) \n* [Univariate Statistical Analysis](#Univariate Statistical Analysis)\n     - [Normality Tests](#Normality Tests)\n         - [Shapiro-Wilk Test](#Shapiro-Wilk Test) \n         - [D‚ÄôAgostino‚Äôs K^2 Test](#D‚ÄôAgostino‚Äôs K^2 Test) \n         - [Anderson-Darling Test](#Anderson-Darling Test)\n     - [Correlation Tests](#Correlation Tests)  \n         - [Covariance Analysis](#Covariance Analysis) \n         - [Pearson‚Äôs Correlation Coefficient](#Pearson‚Äôs Correlation Coefficient) \n         - [Spearman‚Äôs Rank Correlation](#Spearman‚Äôs Rank Correlation) \n         - [Chi-Squared Test](#Chi-Squared Test)\n     - [Parametric Statistical Hypothesis Tests](#Parametric Statistical Hypothesis Tests)\n          - [Student‚Äôs t-test](#Student‚Äôs t-test) \n          - [Analysis of Variance Test (ANOVA)](#Analysis of Variance Test)\n     - [Nonparametric Statistical Hypothesis Tests](#Nonparametric Statistical Hypothesis Tests)\n          - [Mann-Whitney U Test](#Mann-Whitney U Test) \n          - [Wilcoxon Signed-Rank Test](#Wilcoxon Signed-Rank Test) \n          - [Kruskal-Wallis H Test](#Kruskal-Wallis H Test) \n          - [Friedman Test](#Friedman Test)\n* [Multivariate Statistical Analysis](#Multivariate Statistical Analysis)  \n    - [Covariance and Correlation Statistical Analysis](#Covariance and Correlation Statistical Analysis)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Statistical Estimators\"></a>\n# <u> Statistical Estimators </u>\n\n<a id=\"Central Tendency\"></a>\n## <u> Central Tendency </u>\n\nA measure of central tendency is a single value that describes the way in which a group of data cluster around a central value. It is a way to describe the center of a data set. There are three measures of central tendency: the mean, the median, and the mode.\n\n<a id=\"Mean\"></a>\n## <u> Mean </u>\n\nAdd up the values in the data set and then divide by the number of values.\n\n$$\\bar{x} = \\frac{\\sum{x_i}}{n}$$\n\n<a id=\"Median\"></a>\n## <u> Median </u>\n\nList the values of the data set in numerical order (ascending/ descending) and identify which value appears in the middle of the list.\n\n$$\\text{Median} = \\frac{n + 1}{2}$$\n\n<a id=\"Mode\"></a>\n## <u> Mode </u>\n\nValue in the data set which occurs most often.\n\n\n<a id=\"Variance\"></a>\n## <u> Variance </u>\n\nVariance measures how far a set of data is spread out. Variance of zero indicates that all of the data values are identical. It is the average of the squared distances from each point to the mean.\n\n$$\\sigma^2 = \\frac{\\sum (x_i - \\bar{x})^2}{N}$$\n\n<a id=\"Standard deviation\"></a>\n## <u> Standard deviation </u>\n\nStandard Deviation measures the absolute variability of a datasets‚Äô distribution. It is the square root of variance. It is used more often than variance because the unit in which it is measured is the same as that of mean, a measure of central tendency.\n\n$$\\sigma = \\sqrt{\\sigma^2}$$\n\n<a id=\"Covariance\"></a>\n## <u> Covariance </u>\n\nCovariance is a measure of how change in one variable is associated with change in a second variable. Covariance measures the degree to which two variables are linearly associated. The sign of the covariance shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easily interpreted.\n\n$$Cov(ùëã, ùëå ) = \\frac{\\sum(x_i - \\bar{x}) (y_i - \\bar{y})}{N-1}$$\n\n$$Cov(ùëã, ùëã) = Var(ùëã)$$\n$$Cov(ùëã, ùëå ) = Cov(ùëå, ùëã)$$\n\nThe use of the mean in the calculation suggests the need for each data sample to have a Gaussian or Gaussian-like distribution. Term in numerator is called the sum of cross products. Term in denominator (N-1) indicates the degrees of freedom.\n\nThe value of covariance is affected by the change in scale of the variables. If all the values of the given variable are multiplied by a constant and all the values of another variable are multiplied, by a similar or different constant, then the value of covariance also changes.\nCovariance can take any value between -‚àû and +‚àû.\n\n<a id=\"Correlation / Pearson's Correlation\"></a>\n## <u> Correlation / Pearson's Correlation</u>\n\nBoth Covariance and Correlation measures the relationship and the dependency between two variables. ‚ÄúCovariance‚Äù indicates the direction of the linear relationship between variables whereas ‚ÄúCorrelation‚Äù measures both the magnitude and direction of the linear relationship between two variables. Correlation values are standardized whereas, covariance values are not. When we divide the covariance values by the standard deviation, it scales the value down to a limited range of -1 to +1 that represents the limits of correlation from a full negative correlation to a full positive correlation. A value of 0 means no correlation. The value below -0.5 or above 0.5 indicates a notable correlation, and values below those values suggests a less notable correlation.\n\n$$Corr_{xy} = \\frac{Cov(ùëã, ùëå )}{\\sigma_x \\sigma_y}$$\n\nTypes of correlations:\n- Positive Correlation: both variables change in the same direction.\n- Neutral Correlation: No relationship in the change of the variables.\n- Negative Correlation: variables change in opposite directions.\n\nThe value of correlation is not influenced by the change in scale of the values. \n\nThe performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity. An example is Logistic Regression or Linear Regression. Decision trees and boosted trees algorithms are immune to multicollinearity by nature.\n\n<b>Quick Tip</b> : Multicollinearity can be handled by: 1 -  Delete one of the perfectly correlated features. 2 -  Use a dimension reduction algorithm such as Principle Component Analysis (PCA).\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Standard Error\"></a>\n## <u> Standard Error</u>\n\nThe standard error of the mean, also called the standard deviation of the mean tells us how accurate the mean of any given sample from that population is likely to be compared to the true population mean. When the standard error increases, i.e. the means are more spread out, it becomes more likely that any given mean is an inaccurate representation of the true population mean.\n\n- Draw \"n\" samples from a population.\n- Calaulate the mean of every sample.\n- Calculate the standard deviation of all the means.\n- Divide by the number of samples observed.\n\n$$SE(\\bar{x}) = \\frac{\\sigma_x}{\\sqrt{n}}$$"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Confidence Intervals\"></a>\n## <u> Confidence Intervals</u>\n\nA Confidence Interval is a range of values we are fairly sure our true value (eg. mean) lies in.\n\n$$\\text{Confidence interval} =\\bar{X} \\pm Z\\frac{\\sigma}{\\sqrt{n}}$$\n\nWhere $\\bar{X}$ is the mean, $Z$ is the z-value(calculated from a table) for a confidence interval (eg. 80%, 85%), $\\sigma$ is the standard deviation, $n$ is the number of items in a sample.\n\n<b> Example: Average Height </b>\n\nWe measure the heights of 40 randomly chosen men, and get a mean height of 175cm, We also know the standard deviation of men's heights is 20cm. The 95% Confidence Interval is:  175cm ¬± 6.2cm.\n\nThis says the true mean (95% of times) of ALL men (population) is likely to be between 168.8cm and 181.2cm."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"Probability Distributions\"></a>\n# <u> Probability Distributions</u>\n\nIn statistics when we use the term Distribution it usually means Probability distribution.\n\nA Distribution is a function that shows the possible values for a variable and how often they occur.\n\nOr A Probability Distribution is a mathematical function that can be thought of as providing the probabilities of occurrence of different possible outcomes in an experiment.\n\n\n<a id=\"Gausian / Normal distribution\"></a>\n## <u> Gausian / Normal distribution </u> \n\nNormal Distribution is useful because of the <b> Central Limit Theorem (CLT) </b> which states that when a large number of simple random samples are selected from the population and the mean is calculated for each then the distribution of these sample means will assume the normal probability distribution. For example, the distribution of sum of heights of all men in a region tends towards a Gaussian probability distribution.\n\n- 68% of the data falls within the first standard deviation from the mean.\n- 95% fall within two standard deviations.\n- 99.7% fall within three standard deviations."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n%matplotlib inline\n\nmu = 0 # mean\nvariance = 2 #variance\nsigma = np.sqrt(variance) #standard deviation\",\nx = np.linspace(mu - 3*variance, mu + 3*variance, 100)\nplt.plot(x, norm.pdf(x, mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"Standard Normal distribution\"></a>\n### <u> Standard Normal distribution </u> \n\nWhen we standardize a random variable, its ‚Äòmean‚Äô becomes 0 and its standard deviation becomes 1. When a Normal Distribution is standardized, the result is called a Standard Normal Distribution.  \n\n$$\\text{Z} = \\frac{x - \\mu}{\\sigma}$$\n\nWe use the letter Z to denote standardization. The standardized value i.e., Z is known as the z-score.\n\nThese Z scores are important because they tell us how far a value is from the mean. If the Z score of x is zero, then the value of x is equal to the mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n%matplotlib inline\n\nmu = 0 # mean\nvariance = 1 #variance\nsigma = np.sqrt(variance) #standard deviation\",\nx = np.linspace(mu - 2*variance, mu + 2*variance, 100)\nplt.plot(x, norm.pdf(x, mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"QQ-Plot\"></a>\n### <u> QQ-Plot </u>\n\nA QQ-Plot is used to visually determine how close a sample is to the normal distribution. The QQ-Plot orders the z-scores from low to high, and plots each value‚Äôs z-score on the y-axis; the x-axis is the corresponding quantile of a normal distribution for that value‚Äôs rank. Since the data is normalized, the units correspond to the number of standard deviations away of the data from the mean. If the points roughly fall on the diagonal line, then the sample distribution can be considered close to normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport statsmodels.api as sm\nimport pylab\n\nsm.qqplot(x, loc = 0, scale = 1)\npylab.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Student‚Äôs T-Distribution\"></a>\n## <u> Student‚Äôs T-Distribution </u>\n\nT Distribution or Student‚Äôs T Distribution is a probability distribution that is used to estimate population parameters when the sample size is small(~30). It is the distribution of the difference between an estimated parameter and its true (or assumed) value divided by the standard deviation of the estimated parameter (standard error).\n\nStudent‚Äôs T distribution looks much like a Normal distribution but generally has fatter tails. Fatter tails, allow for a higher dispersion of variables, as there is more uncertainty. The distribution follows Normal Distribution when the sample size is sufficiently large.\n\nThe t-distribution has been used as a reference for the distribution of a sample mean, the difference between two sample means, regression parameters, and other statistics.\n\nAs the z-statistic is related to the standard Normal distribution, the t-statistic is related to the Student‚Äôs T distribution. \n\n$$t = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}$$\n\n$ \\bar{x}$ =  Sample Mean, $ \\mu$ = Population Mean, $\\sigma$ = standard deviation, n  = sample size"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n%matplotlib inline\n\nmu = 0 # mean\nvariance = 20 #variance\nsigma = np.sqrt(variance) #standard deviation\",\nx = np.linspace(mu - 3*variance, mu + 3*variance, 100)\nplt.plot(x, norm.pdf(x, mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Binomial Distribution\"></a>\n## <u> Binomial Distribution </u> \n\nThis type of distribution is used when there are exactly two outcomes of a trial. These outcomes are labeled as ‚ÄúSuccess‚Äù and ‚ÄúFailure‚Äù. The random variable X can take value 1 with the probability of success, p, and the value 0 with the probability of failure, q or 1‚àíp.\n \nThe probabilities of success and failure need not be equally likely.\n\nSame Probability: Probability of head in a toss(0.5/0.5)\n\nDifferent Probability: Probability of me winning with superman(0.1/0.9)\n\nEach trial is independent since the outcome of the previous toss doesn‚Äôt determine or affect the outcome of the current toss. An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are N and P where N is the total number of trials and P is the probability of success in each trial."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import binom\nimport seaborn as sns\n\ndata_binom = binom.rvs(n=10,p=0.8,size=10000) # size = number of times to repeat the trials.\nax = sns.distplot(data_binom, kde=True)\nax.set(xlabel='Binomial Distribution', ylabel='Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the probability(0.8) of success is greater than 0.5 the distribution is skewed towards the right side."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Bernoulli‚Äôs Distribution\"></a>\n## <u> Bernoulli‚Äôs Distribution </u> \n\nBernoulli Distribution is a special case of Binomial Distribution with a single trial."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import bernoulli\n\ndata_bern = bernoulli.rvs(size=10000,p=0.6)\nax= sns.distplot(data_bern, kde=False)\nax.set(xlabel='Bernoulli Distribution', ylabel='Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Uniform Distribution\"></a>\n## <u> Uniform Distribution </u> \n\nThere are two kinds of uniform random variables: discrete and continuous ones.\n\nA discrete uniform distribution will take a (finite) set of values S, and assign a probability of 1/n to each of them, where n is the amount of elements in S.\n\nThis way, if for instance a variable Y is uniform in {1,2,3}, there‚Äôd be 33% chance for each of those values to be assigned to Y.\n\nIn a continuous uniform distribution, probability of Y taking a value in an interval (from c to d) is proportional to its size versus the size of the whole interval (b-a)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import uniform distribution\nfrom scipy.stats import uniform\nimport seaborn as sns\n\ndata_uniform = uniform.rvs(size=1000, loc = 10, scale=20)\nax = sns.distplot(data_uniform, bins=100, kde=True)\nax.set(xlabel='Uniform Distribution ', ylabel='Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Exponential Distribution\"></a>\n## <u> Exponential Distribution </u>\n\nExponential Distribution measures the distribution of the time between events.\n\n<u> Example </u>\n1. Length of time beteeen metro arrivals,\n2. Length of time between arrivals at a gas station.\n3. The life of an Air Conditioner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decreasing Exponential Function\n\nimport numpy as np \nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt \n  \ndistribution = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\n  \nplot = plt.plot(distribution, expon.pdf(distribution))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import expon\ndata_expon = expon.rvs(scale=1,loc=0,size=1000)\nax = sns.distplot(data_expon, kde=True, bins=100)\nax.set(xlabel='Exponential Distribution', ylabel='Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Poisson Distribution\"></a>\n## <u> Poisson Distribution </u>\n\nPoisson random variable is typically used to model the distribution of events per unit of time or space.\n\n<u> Example </u>\n- The number of emergency calls recorded at a hospital in a day.\n- The number of thefts reported in an area on a day.\n- The number of customers arriving at a salon in an hour.\n- The number of suicides reported in a particular city.\n- The number of printing errors at each page of the book.\n\nA distribution is called Poisson distribution when the following assumptions are valid:\n\n1. Any successful event should not influence the outcome of another successful event.\n2. The probability of success over a short interval must equal the probability of success over a longer interval.\n3. The probability of success in an interval approaches zero as the interval becomes smaller."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import poisson\n\n# \"mu\" = \"The average number of events in an interval.\ndata_poisson = poisson.rvs(mu=3, size=10000)\nax = sns.distplot(data_poisson, bins=30, kde=True)\nax.set(xlabel='Poisson Distribution', ylabel='Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Probability Density Function and Probability Mass Function\"></a>\n## <u> Probability Density Function and Probability Mass Function </u> \n\nProbability density function and Probability mass function is a statistical expression that defines a Probability Distribution for a random variable.\n\n<b> Probability density function(PDF) </b> is used to determine the probability distribution for a Continuous Random Variable. When the PDF is graphically plotted the area under the curve indicates the interval in which the variable will fall.\n\nWhereas the <b> Probability Mass Function(PMF) </b> is used to determine the probability distribution for a Discrete Random Variable."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"P-value\"></a>\n# <u> P-value </u>\n\n<a id=\"Hypothesis testing\"></a>\n## <u> Hypothesis testing </u>\nHypothesis testing is a statistical method that is used in making statistical decisions using experimental data. It is an assumption that we make about the population parameter. A hypothesis test evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data.\n\n<b>Null hypothesis </b>: A null hypothesis is a general statement or default position.\n\nExample : a company production is = 50 unit/per day.\n\n<b>Alternative hypothesis </b>: The alternative hypothesis is the hypothesis that is contrary to the null hypothesis.\n\nExample : a company production is !=50 unit/per day etc.\n\n<b>Level of significance </b>: Refers to the degree of significance in which we accept or reject the null-hypothesis. 100% accuracy is not possible for accepting or rejecting a hypothesis, so we therefore select a level of significance that is usually 5% which means the output should be 95% confident to give similar kind of result in each sample.\n\n\nIn a Probability Distribution graph, we have the range of values on the x-axis and the frequency of occurrences of different values on the y-axis. If we pick any random value from this distribution, the probability that we will pick values close to the mean is highest as it has the highest peak (due to high occurrence values in that region). If we move away from the peak, the occurrence of the values decreases rapidly and so does the corresponding probability, towards a very small value close to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# In a coin toss suppose Head comes 60/100 times, N = 100, P(Head) = 60/100\n\nimport scipy.stats\nimport matplotlib.pyplot as plt\n\n# succes = np.linspace(0, 100, 101)\nsucces = np.linspace(30, 70, 41)\nplt.plot(succes, scipy.stats.binom.pmf(succes, 100, 0.5), 'b-', label=\"Binomial(100, 0.5)\", color = \"blue\")\nupper_succes_tvalues = succes[succes > 60]\nplt.fill_between(upper_succes_tvalues, 0, scipy.stats.binom.pmf(upper_succes_tvalues, 100,0.5), alpha=.3, label=\"p-value\")\nplt.axvline(x=55, ymin=0, ymax=0.8, color = \"orange\")\nplt.axvline(x=58, ymin=0, ymax=0.5, color = \"red\")\nplt.axvline(x=upper_succes_tvalues[0], ymin=0, ymax=0.2, color = \"green\")\nplt.axvline(x=upper_succes_tvalues[3], ymin=0, ymax=0.15, color = \"pink\")\n_ = plt.legend()\npval = 1 - scipy.stats.binom.cdf(60, 100, 0.5)\nprint(pval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u> p-value is the cumulative probability (area under the curve) of the values, right of the green line in the figure above.</u>\n\nOr,\n\n<u> p-value corresponding to the green point(point of intersection of green line and the curve) tells us about the ‚Äòtotal probability‚Äô of getting any value to the right hand side of the green line, when the values are picked randomly from the population distribution. </u>\n\nA large p-value implies that sample scores are more aligned or similar to the population score."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Alpha value / Significance Level\"></a>\n## <u> Alpha value / Significance Level</u>\n\nAlpha value / Significance Level (generally 0.05 or 5%) is a threshold p-value, which we decide before conducting a test of similarity or significance ( Z-test or a T-test).\n\nConsider the above normal distribution. The red point in this distribution represents the alpha value or the threshold p-value. Now, let‚Äôs say that the orange and pink points represent different sample results obtained after an experiment.\n\n### p-value > alpha:\nThe orange point has a p-value greater than the alpha. As a result, these values can be obtained with fairly high probability. It also means that the results are in favor of the null hypothesis and therefore we fail to reject it. This result is often against the alternate hypothesis (obtained results are from another distribution).\n\n### p-value < alpha:\nThe pink point has a p-value less than the alpha value (red). As a result, the sample results are a rare outcome. Therefore, they are significantly different from the population.\n\nThe smaller the value of alpha we consider, the harder it is to consider the results as significant.\n\n\n<a id=\"Example\"></a>\n## <u> Example</u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv(\"../input/50-startups/50_Startups.csv\")\nX = dataset[['R&D Spend', 'Administration', 'Marketing Spend', 'State']] \ny = dataset[['Profit']] \nX = pd.get_dummies(X, columns = ['State'])\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that \"Administration\" has a p-value over 0.50.\n\n#### Hypothesis:\n\n- Null Hypothesis: The independent variable has no significant effect over the target variable.\n- Alternate Hypothesis: The independent variables have a significant effect on the target variable.\n\nThe above results show that \"Administration\" has no significant effect over the ‚ÄúProfit‚Äù earned by the startups. Let‚Äôs remove this variable from the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(['Administration'], axis = 1, inplace = True) \nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important thing to note in this model summary is that although we have reduced two independent variables, the value of the adjusted R-Square is increased.\n\nWith the help of p-value, we not only made a simpler model with fewer variables,  but we also improved the model‚Äôs performance."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Remarks\"></a>\n## <u> Remarks </u>\n\n- Although a low p-value promotes the rejection of the null hypothesis, it addresses nothing about the probability of rejecting it.\n- We choose the significance level before we perform the experiment. If the p-value satisfies our level of significance (p < alpha), only then can we make conclusions.\n- To talk about a null hypothesis being true using a p-value is impossible. A high p-value means that our data is highly consistent with our null hypothesis, nothing more."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Degrees of Freedom\"></a>\n# <u> Degrees of Freedom </u>\n\nTo explain what degrees of freedom are, let us just take an example. In a set of 3 numbers with the mean as 10 and two out of three variables as 5 and 15, there is only one possibility of the value that the third number can take up i.e. 10. With any set of 3 numbers with the same mean, for example: 12,8 and 10 or say 9,10 and 11, there is only one value for any 2 given values in the set. You can basically change the two values here and the third value fixes itself. The degree of freedom here is 2. Essentially, degrees of freedom is the number of independent data points that went into calculating the estimate."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Parametric vs. Non-Parametric Data\"></a>\n# <u> Parametric vs. Non-Parametric Data </u>\n\n<a id=\"Parametric Data\"></a>\n## <u> Parametric Data </u>\n\nParametric data is a sample of data drawn from a known data distribution.\n\nThis means that we already know the distribution and we know its parameters. Often, parametric is shorthand for real-valued data drawn from a Gaussian distribution. This is a useful shorthand, but strictly this is not entirely accurate.\n\nIf we have parametric data, we can use parametric methods and can harness the entire suite of statistical methods developed for data assuming a Gaussian distribution, such as:\n\n- Summary statistics.\n- Correlation between variables.\n- Significance tests for comparing means.\n\nIn general, we prefer to work with parametric data, and even go so far as to use data preparation methods that make data parametric, such as data transforms, so that we can harness these well-understood statistical methods.\n\n<a id=\"Non-Parametric Data\"></a>\n## <u> Non-Parametric Data </u>\n\nData that does not fit a known or well-understood distribution is referred to as nonparametric data.\n\nData could be non-parametric for many reasons, such as:\n\n- Data is not real-valued, but instead is ordinal, intervals, or some other form.\n- Data is real-valued but does not fit a well understood shape.\n- Data is almost parametric but contains outliers, multiple peaks, a shift, or some other feature.\n\nMost parametric methods have an equivalent nonparametric version.\n\nIn general, the findings from nonparametric methods are less powerful than their parametric counterparts, namely because they must be generalized to work for all types of data.\n\nIn the case of real-valued data(that does not fit the familiar Gaussian distribution) or ordinal data or interval data nonparametric statistics are the only type of statistics that can be used.\n\n<a id=\"Data Ranking\"></a>\n### <u> Data Ranking </u>\n\nBefore a nonparametric statistical method can be applied, the data must be converted into a rank format.\n\nData Ranking is exactly as its name suggests. The procedure is as follows:\n\n- Sort all data in the sample in ascending order.\n- Assign an integer rank from 1 to N for each unique value in the data sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import rand\nfrom numpy.random import seed\nfrom scipy.stats import rankdata\n\ndata = rand(100)\n# rank data\nranked = rankdata(data)\n# review first 10 ranked samples\nranked[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Univariate Statistical Analysis\"></a>\n# <u> Univariate Statistical Analysis </u>\n\n<a id=\"Normality Tests\"></a>\n## <u> Normality Tests </u>\n\nStatistical tests to check if the data has a Gaussian distribution.\n\n<a id=\"Shapiro-Wilk Test\"></a>\n### <u> Shapiro-Wilk Test </u>\n\nTest the null hypothesis that the data is drawn from a normal distribution.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nx = stats.norm.rvs(loc=5, scale=3, size=100)\nstat, p = stats.shapiro(x)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"D‚ÄôAgostino‚Äôs K^2 Test\"></a>\n### <u> D‚ÄôAgostino‚Äôs $K^2$ Test </u>\n\nTest the null hypothesis that the data is drawn from a normal distribution.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nx = stats.norm.rvs(loc=5, scale=3, size=100)\nstat, p = stats.normaltest(x)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Anderson-Darling Test\"></a>\n### <u> Anderson-Darling Test </u> \n\nTest the null hypothesis that the data is drawn from a particular distribution.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import anderson\n\ndata_expon = expon.rvs(scale=1,loc=0,size=1000)\n\nstatistic, critical_values, significance_level = anderson(data_expon, dist='expon')\nprint(statistic)\n\nfor i in range(len(critical_values)):\n    sl, cv = significance_level[i], critical_values[i]\n    if statistic < cv:\n        print('Probably exponential at the %.1f%% level' % (sl))\n    else:\n        print('Probably not exponential at the %.1f%% level' % (sl))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Correlation Tests\"></a>\n## <u> Correlation Tests </u>\n\n<a id=\"Covariance Analysis\"></a>\n### <u> Covariance Analysis </u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom numpy.random import randn\nfrom numpy.random import seed\nfrom matplotlib import pyplot\n\n#  Random numbers drawn from a Gaussian distribution with mean = 100 and standard deviation = 20.\ndata1 = 20 * randn(1000) + 100\n# Gaussian noise added with a mean of a 50 and a standard deviation of 10\ndata2 = data1 + (10 * randn(1000) + 50)\n# summarize\nprint('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))\nprint('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))\n# plot\npyplot.scatter(data1, data2)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import cov\ncovariance = cov(data1, data2)\ncovariance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The covariance between the two variables is 389.75. We can see that it is positive, suggesting the variables change in the same direction as we expect."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Pearson‚Äôs Correlation Coefficient\"></a>\n### <u> Pearson‚Äôs Correlation Coefficient </u>\n\nPearson Correlation Coefficient can be used with\n - Continuous variables that have a linear relationship. (draw a scatterplot first to check a linear relationship).\n \n Pearson Correlation Coefficient is not robust to outliers\n  - Presence of outliers will always impace Pearson Correlation Coefficient value."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\n\ncorr, _ = pearsonr(data1, data2)\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two variables are positively correlated and the correlation is 0.8. This suggests a high level of correlation."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Spearman‚Äôs Rank Correlation\"></a>\n### <u> Spearman‚Äôs Rank Correlation </u>\n\nSpearman‚Äôs Correlation is used when: \n- Two variables are/may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables.\n- Two variables being considered may have a non-Gaussian distribution.\n\nSpearman‚Äôs Correlation can also be used if there is a linear relationship between the variables, but may result in lower coefficient scores.\n\nAs with the Pearson correlation coefficient, the scores are between -1 and 1 for perfectly negatively correlated variables and perfectly positively correlated respectively.\n\nInstead of calculating the coefficient using covariance and standard deviations on the samples themselves, Spearman‚Äôs Correlation is calculated from the relative rank of values on each sample.\n\n$$\\text{Spearman‚Äôs Correlation Coefficient} = \\frac{Covariance(rank(x), rank(y))}{stdv(rank(x)) * stdv(rank(y))}$$ "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import spearmanr\n\ncorr, _ = spearmanr(data1, data2)\nprint('Spearmans correlation: %.3f' % corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Chi-Squared Test\"></a>\n### <u> Chi-Squared Test </u>\n\nChi-Square Test is used in statistics to test whether two non-negative features(boolean, category, non negative numbers) are related or independent. Given the data of two variables, we can get observed count O and expected count E. Chi-Square measures how expected count E and observed count O deviates each other.\n\n$$\\text{Chi-Square} = \\sum{\\frac{(O_i - E_i)^2}{E_i}} $$\n$$\\text{O = Observed values, E = Expected Values}$$\n\nIn feature selection, we aim to select the features which are highly dependent on the response(y). When two features are independent, the observed count is close to the expected count, thus we will have smaller Chi-Square value. A higher Chi-Square value indicates that the feature is more dependent on the response and can be selected for model training.\n\n<u> Example </u>:\n\nConsider a data-set where we have to determine why customers are leaving the bank. Gender of a customer with values as Male/Female as the predictor(X) and Exited describes whether a customer is leaving the bank with values Yes/No as the response(y). In this test we will check is there any relationship between Gender and Exited.\n\nSteps to perform the Chi-Square Test:\n- Define Hypothesis.\n- Build a Contingency table.\n- Find the expected values.\n- Calculate the Chi-Square statistic.\n- Accept or Reject the Null Hypothesis.\n\n### <u> Define Hypothesis </u>:\n- Null Hypothesis (H0): Two variables are independent.\n- Alternate Hypothesis (H1): Two variables are not independent.\n\n### <u> Contingency table </u>:\n\n| Gender/Exited | Yes | No | Total |\n| --- | --- | --- |\n| Male | 38 | 178 | 216 |\n| Female | 44 | 140 | 184 |\n| Total | 82 | 318 | 400 |\n\nDegrees of freedom for contingency table is given as (r-1) * (c-1) where r,c are rows and columns. Here df = (2‚Äì1) * (2‚Äì1) = 1.\n\nIn the above table we have figured out all observed values and our next steps is to find expected values, get the Chi-Square value and check for relationship.\n\n### <u> Find the Expected Value </u>:\n\nBased on the null hypothesis that the two variables are independent. We can say if A, B are two independent events\n\n$$P(A \\land B) = P(A) \\text{ * } P(B)$$\n\nLet‚Äôs calculate the expected value for the first cell that is those who are Males and are Exited from the bank.\n\n$$E_1 = N \\text{ * } P$$\n$$P = P(yes) \\text{ * } P(male)$$\n$$P = \\frac{82}{400} \\text{ * } \\frac{216}{400}$$\n$$P = 0.1107$$\n$$E_1 = 400 \\text{ * } 0.1107 = 44$$\n\nWe also calculated E2, E3, E4 and get the following results.\n\n| Gender/Exited | Yes | No |\n| --- | --- |\n| Male | 44 | 172 |\n| Female | 38 | 146 |\n\n### <u> Calculate Chi-Square value </u>:\n\n| Gender/Exited | O | E | $\\frac{(O - E)^2}{E}$ |\n| --- | --- | --- |\n| Male, Yes | 38 | 44 | 0.8181 |\n| Male, No | 178 | 172 | 0.2093 |\n| Female, Yes | 44 | 38 | 0.9473 |\n| Female, No | 140 | 146 | 0.2465 |\n| Chi-Square Value |  |  | 2.2214 |\n\n### <u> Accept or Reject the Null Hypothesis </u>:\nThe Chi-Square value is greater than alpha(0.05), we accept the null hypothesis."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import chi2\n\nChurn_Modelling = pd.read_csv(\"../input/bank-customer-churn-modeling/Churn_Modelling.csv\")\n# Select all Categorical variables, and dependent variable\nChurn_Modelling = Churn_Modelling[['Surname', 'Geography', 'Gender', 'Exited']] \nChurn_Modelling = Churn_Modelling.apply(LabelEncoder().fit_transform)\nX = Churn_Modelling[['Surname', 'Geography', 'Gender']] \ny = Churn_Modelling['Exited']\nchi_scores = chi2(X,y)\nchi_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first array represents chi square values and second array represnts p-values."},{"metadata":{"trusted":true},"cell_type":"code","source":"p_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since \"Geography\" has higher the p-value, it says that the variable is independent of the repsone and may not be considered for model training."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Stationary Tests\"></a>\n## <u> Stationary Tests </u>\n<a id=\"Augmented Dickey-Fuller\"></a>\n- ### <u> Augmented Dickey-Fuller </u>\n<a id=\"Kwiatkowski-Phillips-Schmidt-Shin\"></a>\n- ### <u> Kwiatkowski-Phillips-Schmidt-Shin </u>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Parametric Statistical Hypothesis Tests\"></a>\n## <u> Parametric Statistical Hypothesis Tests </u>\n<a id=\"Student‚Äôs t-test\"></a>\n### <u> Student‚Äôs t-test </u>\n\nA common question about two or more datasets is whether they are same/different. Specifically, whether the simplarity/difference between their central tendency (e.g. mean or median) is statistically significant. Student‚Äôs t-test compares the means of two independent groups in order to determine if the two population means are equal. It assume that the two random variables are normally distributed.  The variances of the two samples may be assumed to be equal or unequal.\n\nApplications:\n- compare the performance of two workers of by checking the average sales done by each of them.\n- compare the performance of a worker by comparing the average sales done by him with the standard value."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom scipy import stats\n\nblood_pressure = pd.read_csv(\"../input/blood-pressure-dataset/blood_pressure.csv\")\n# Use \"ttest_rel\" when the data samples may represent two independent measures or evaluations of the same object. These data samples are repeated or dependent and are referred to as paired samples or repeated measures.\nttest,pval = stats.ttest_rel(blood_pressure['bp_before'], blood_pressure['bp_after'])\n# ttest is the calculated difference of the population mean represented in units of standard error.\n# The greater the magnitude of T, the greater the evidence against the null hypothesis. The closer T is to 0, the more likely there isn't a significant difference.\nttest,pval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the [scipy documents](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html), Null Hypothesis for \"ttest_rel\" is : That 2 related or repeated samples have identical average (expected) values.\n\nAs pval < 0.05, we can discard the null hypothesis."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Analysis of Variance Test\"></a>\n### <u> Analysis of Variance Test (ANOVA) </u>\n\nANOVA is a statistical test that assumes that the mean across 2 or more groups are equal. If the evidence suggests that this is not the case, the null hypothesis is rejected and at least one data sample has a different distribution. It assume that the two(or more) random variables are normally distributed.\n\nImportantly, the test can only comment on whether all samples are the same or not; it cannot quantify which samples differ or by how much."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import f_oneway\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = f_oneway(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Nonparametric Statistical Hypothesis Tests\"></a>\n## <u> Nonparametric Statistical Hypothesis Tests </u>\n\nThe null hypothesis of these tests is often the assumption that both samples were drawn from a population with the same distribution, and therefore the same population parameters, such as mean or median.\n\n<a id=\"Test Dataset\"></a>\n### <u> Test Dataset </u>\nGenerate two samples drawn from different distributions. We will draw the samples from Gaussian distributions for simplicity, although, as noted, the tests we review are for data samples where we do not know or assume any specific distribution.\n\nWe expect the statistical tests to discover that the samples were drawn from differing distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate gaussian data samples\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import mean\nfrom numpy import std\n\n# generate two sets of univariate observations\ndata1 = 5 * randn(100) + 50\ndata2 = 5 * randn(100) + 51\ndata3 = 5 * randn(100) + 52\n# summarize\nprint('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))\nprint('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))\nprint('data3: mean=%.3f stdv=%.3f' % (mean(data3), std(data3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Mann-Whitney U Test\"></a>\n### <u> Mann-Whitney U Test </u>\n\nIn Mann-Whitney U Test, the two samples are combined and rank ordered together. The strategy is to determine if the values from the two samples are randomly mixed in the rank ordering or if they are clustered at opposite ends when combined. A random rank order would mean that the two samples are not different, while a cluster of one sample values would indicate a difference between them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mannwhitneyu\n\nstat, p = mannwhitneyu(data1, data2)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Same distribution (fail to reject H0)')\nelse:\n    print('Different distribution (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Wilcoxon Signed-Rank Test\"></a>\n### <u> Wilcoxon Signed-Rank Test </u>\n\nIf the the data samples are paired in some way or represent two measurements of the same technique or each sample is independent, but comes from the same population, we use Wilcoxon Signed-Rank Test. It is the equivalent of the paired Student T-test, but for ranked data instead of real valued data with a Gaussian distribution.\n\nExamples of paired samples in machine learning might be the same algorithm evaluated on different datasets or different algorithms evaluated on exactly the same training and test data.\n\nThe samples are not independent, therefore the Mann-Whitney U test cannot be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import wilcoxon\n\nstat, p = wilcoxon(data1, data2)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Same distribution (fail to reject H0)')\nelse:\n    print('Different distribution (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Kruskal-Wallis H Test\"></a>\n- ### <u> Kruskal-Wallis H Test </u>\n\nGeneralization of Mann-Whitney U test.\n\nThe Kruskal-Wallis test is a nonparametric version of the one-way analysis of ANOVA."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import kruskal\n\nstat, p = kruskal(data1, data2, data3)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Friedman Test\"></a>\n- ### <u> Friedman Test </u>\n\nGeneralization of the Kruskal-Wallis H Test to more than two samples.\n\nThe Friedman test is the nonparametric version of the repeated measures analysis of variance test, or repeated measures ANOVA."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import friedmanchisquare\n\nstat, p = friedmanchisquare(data1, data2, data3)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Multivariate Statistical Analysis\"></a>\n# <u> Multivariate Statistical Analysis </u>\n\nMultivariate statistics includes all statistical techniques for analyzing samples made of two or more variables.\n\n<a id=\"Covariance and Correlation Statistical Analysis\"></a>\n## <u> Covariance and Correlation Statistical Analysis </u>\n\n### <u> Covariance Matrix </u>\n\nThe Covariance matrix of $X_{NP}$ is a \"P * P\" matrix where the covariance of any two features \"P1\", \"P2\" is calculated by the formula -\n\n$$Cov(P1, P2 ) = \\frac{\\sum({P1}_i - \\bar{P1}) ({P2}_i - \\bar{P2})}{N}$$\n\nFor diagonal elements, $Cov(P1, P1) = Var(P1)$\n\n### <u> Correlation Matrix </u>\n\nThe Correlation matrix of $X_{NP}$ is also a \"P * P\" matrix where the correlation of any two features \"P1\", \"P2\" is calculated by the formula -\n\n$$Corr(P1, P2 ) = \\frac{Cov(P1, P2 )}{\\sigma_{P1} \\sigma_{P2}}$$\n\nThe correlation of an element with itself $ Corr(P1, P1) = 1$, or the highest value possible.\n\n### <u>Covariance and Correlation Statistical Analysis in PCA </u>:\n\nIn PCA analysis, use the covariance matrix when the variable are on similar scales and the correlation matrix when the scales of the variables differ.\n\nWe will see(R code) how Covariance and Correlation Matrix can be used to explain which features contributes to maximum variance in PCA analysis. We will also see how PCA results differ when computed with the correlation matrix and the covariance matrix respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# rpy2 runs embedded R in a Python process.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conda install -c r rpy2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext rpy2.ipython","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nmtcars = pd.read_csv(\"../input/mtcars/mtcars.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\nhead(mtcars, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <u>PCA with covariance matrix </u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\n# We will use the prcomp() function from the ‚Äòstats‚Äô package.\n# Setting the scale=FALSE option will use the covariance matrix to get the PCs\ncars.pca = prcomp(mtcars[,-11], scale=FALSE)\nstr(cars.pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prcomp() returns 5 key measures: sdev, rotation, center, scale and x.\n\n- The <b>center </b> and <b> scale </b> provide the respective means and standard deviation of the variables that were used for normalization before implementing PCA.\n- <b> sdev </b> shows the standard deviation of principal components. In other words, it shows the square roots of the eigenvalues.\n- Each column of the <b>rotation matrix </b> contains the principal component loading vector. The component loading can be represented as the correlation of a particular variable on the respective PC(principal component). It can assume both positive or negative. Higher the loading value, higher is the correlation. With this information, we can easily interpret the ‚Äòkey variable‚Äô in the PC.\n- The matrix <b> x </b>has the principal component score vectors."},{"metadata":{},"cell_type":"markdown","source":"The scale measure is set as \"FALSE\" as specified by in the input arguments. Let us now look at the principal component loading vectors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\ncars.pca$rotation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conda install -c bioconda r-ggbiplot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To help with the interpretation, let us plot these results."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\n\nlibrary(ggbiplot)\nggbiplot(cars.pca, labels=rownames(mtcars))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To read this chart, one has to look at the extreme ends (top, down, left and right). The first principal component here (on the left and right) corresponds to the measure of ‚Äòdisp‚Äô and ‚Äòhp‚Äô. The second principal component (PC2) does not seem to have a strong measure.\n\nWe can finish this analysis with a summary of the PCA with the covariance matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\nsummary(cars.pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this table, we see that the maximum contribution to variation caused is caused by PC1 (~92.7%) and all other principal components have progressively lower contribution. In simpler terms, it means that almost 93% of the variance in the data-set can be explained using the first principal component with measures of ‚Äòdisp‚Äô and ‚Äòhp‚Äô.\n\nAs a conclusion, not a lot of significant insights can be driven from the Principal Component Analysis on the basis of the covariance matrix."},{"metadata":{},"cell_type":"markdown","source":"#### <u> PCA with correlation matrix </u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\n# Setting the scale=FALSE option will use the correlation matrix to get the PCs\ncars.pca = prcomp(mtcars[,-11], scale=TRUE)\n# obtain the rootation matrix\ncars.pca$rotation\n# obtain the biplot\nggbiplot(cars.pca, labels=rownames(mtcars))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot looks more informative. It says that variables like ‚Äòdisp‚Äô, ‚Äòcyl‚Äô, ‚Äòhp‚Äô, ‚Äòwt‚Äô, ‚Äòmpg‚Äô, ‚Äòdrat‚Äô and ‚Äòvs‚Äô contribute significantly to PC1 (first principal component). ‚Äòqsec, ‚Äògear‚Äô and ‚Äòam‚Äô have a significant contribution to PC2.\n\nLet us try to look at the summary of this analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%R\nsummary(cars.pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One significant change we see is the drop in the contribution of PC1 to the total variation. It has dropped from 92.7% to 63.5%. On the other hand, the contribution of PC2 has increased from 7% to 22%. Furthermore, the component loading values show that the relationship between the variables in the data-set is way more structured and distributed. Another significant difference can be observed if we look at the standard deviation values in both the results above. The values from PCA done using the correlation matrix are closer to each other and more uniform as compared to the analysis done using the covariance matrix.\n\nThis analysis with the correlation matrix definitely, uncovers some better structure in the data and relationships between variables. The above example can be used to conclude that the results significantly differ when one tries to define variable relationships using covariance and correlation. This in turn, affects the importance of the variables computed for any further analyses. Selection of predictors and independent variables is one prominent application of such exercises."},{"metadata":{},"cell_type":"markdown","source":"### <u>Correlation Matrix for to compare feature correlations </u>:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n# correlation matrix:\n# method : {‚Äòpearson‚Äô, ‚Äòkendall‚Äô, ‚Äòspearman‚Äô} \ncorrMatrix = mtcars.corr(method ='pearson')\n\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(corrMatrix, annot=True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q&A\n\nQ: What is the difference between type I vs type II error?\n\nA: Type I error:  False Positive, when a girl is not pregnant but the doctor has stated that she is pregnant. i.e. the null hypothesis is true but is rejected. Type II error is opposite of Type I error."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}