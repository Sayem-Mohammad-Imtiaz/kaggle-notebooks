{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pygal","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:13:51.701336Z","iopub.execute_input":"2021-09-16T12:13:51.701625Z","iopub.status.idle":"2021-09-16T12:13:59.92684Z","shell.execute_reply.started":"2021-09-16T12:13:51.701598Z","shell.execute_reply":"2021-09-16T12:13:59.925643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\nimport pygal as py\nimport squarify as sq\nimport matplotlib \nplt.rcParams[\"figure.figsize\"] = (20,15)\nmatplotlib.rc('xtick', labelsize=7) \nmatplotlib.rc('ytick', labelsize=7) \n\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 5}\n\nmatplotlib.rc('font', **font)\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n%matplotlib inline ","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:13:59.929104Z","iopub.execute_input":"2021-09-16T12:13:59.929369Z","iopub.status.idle":"2021-09-16T12:13:59.943203Z","shell.execute_reply.started":"2021-09-16T12:13:59.929338Z","shell.execute_reply":"2021-09-16T12:13:59.942185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/sentiment-analysis-for-financial-news/all-data.csv\",engine=\"python\",encoding=\"ISO-8859-1\")\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:13:59.944912Z","iopub.execute_input":"2021-09-16T12:13:59.945255Z","iopub.status.idle":"2021-09-16T12:14:00.013199Z","shell.execute_reply.started":"2021-09-16T12:13:59.945212Z","shell.execute_reply":"2021-09-16T12:14:00.012086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col1=df.keys()[0]\ncol2=df.keys()[1]\ncol2","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.016366Z","iopub.execute_input":"2021-09-16T12:14:00.016627Z","iopub.status.idle":"2021-09-16T12:14:00.023969Z","shell.execute_reply.started":"2021-09-16T12:14:00.016599Z","shell.execute_reply":"2021-09-16T12:14:00.023064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=pd.DataFrame([[col1, col2]], columns=list([col1,col2]), index=[4845])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.026384Z","iopub.execute_input":"2021-09-16T12:14:00.026613Z","iopub.status.idle":"2021-09-16T12:14:00.036374Z","shell.execute_reply.started":"2021-09-16T12:14:00.026586Z","shell.execute_reply":"2021-09-16T12:14:00.03543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.append(df2, ignore_index=True).set_axis(['sentiment', 'news'], axis=1, inplace=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.038843Z","iopub.execute_input":"2021-09-16T12:14:00.039094Z","iopub.status.idle":"2021-09-16T12:14:00.050132Z","shell.execute_reply.started":"2021-09-16T12:14:00.039066Z","shell.execute_reply":"2021-09-16T12:14:00.049111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.051555Z","iopub.execute_input":"2021-09-16T12:14:00.051868Z","iopub.status.idle":"2021-09-16T12:14:00.07041Z","shell.execute_reply.started":"2021-09-16T12:14:00.05184Z","shell.execute_reply":"2021-09-16T12:14:00.069519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y=\"sentiment\",data=df)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.071984Z","iopub.execute_input":"2021-09-16T12:14:00.072215Z","iopub.status.idle":"2021-09-16T12:14:00.319681Z","shell.execute_reply.started":"2021-09-16T12:14:00.072182Z","shell.execute_reply":"2021-09-16T12:14:00.318782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.320968Z","iopub.execute_input":"2021-09-16T12:14:00.321911Z","iopub.status.idle":"2021-09-16T12:14:00.332303Z","shell.execute_reply.started":"2021-09-16T12:14:00.321881Z","shell.execute_reply":"2021-09-16T12:14:00.33124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.336421Z","iopub.execute_input":"2021-09-16T12:14:00.337443Z","iopub.status.idle":"2021-09-16T12:14:00.940647Z","shell.execute_reply.started":"2021-09-16T12:14:00.337399Z","shell.execute_reply":"2021-09-16T12:14:00.939686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(ReviewText):\n    ReviewText = ReviewText.str.replace(\"(<br/>)\", \"\")\n    ReviewText = ReviewText.str.replace('(<a).*(>).*(</a>)', '')\n    ReviewText = ReviewText.str.replace('(&amp)', '')\n    ReviewText = ReviewText.str.replace('(&gt)', '')\n    ReviewText = ReviewText.str.replace('(&lt)', '')\n    ReviewText = ReviewText.str.replace('(\\xa0)', ' ')  \n    return ReviewText\ndf['Review Text'] = preprocess(df['news'])\n\ndf['polarity'] = df['news'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf['news_len'] = df['news'].astype(str).apply(len)\ndf['word_count'] = df['news'].apply(lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:00.942172Z","iopub.execute_input":"2021-09-16T12:14:00.942487Z","iopub.status.idle":"2021-09-16T12:14:02.904179Z","shell.execute_reply.started":"2021-09-16T12:14:00.942448Z","shell.execute_reply":"2021-09-16T12:14:02.903334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:02.905616Z","iopub.execute_input":"2021-09-16T12:14:02.906333Z","iopub.status.idle":"2021-09-16T12:14:02.928Z","shell.execute_reply.started":"2021-09-16T12:14:02.9063Z","shell.execute_reply":"2021-09-16T12:14:02.926927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('top 4 random reviews with the highest positive sentiment polarity: \\n')\n\ndf1=df.drop_duplicates(subset=['Review Text'])\n\ncl = df1.loc[df1.polarity == 1, ['Review Text']].sample(4).values\nfor c in cl:\n    print(c[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:02.930029Z","iopub.execute_input":"2021-09-16T12:14:02.930397Z","iopub.status.idle":"2021-09-16T12:14:02.951178Z","shell.execute_reply.started":"2021-09-16T12:14:02.930353Z","shell.execute_reply":"2021-09-16T12:14:02.949868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('5 random reviews with the most neutral sentiment(zero) polarity: \\n')\ncl1 = df.loc[df.polarity == 0, ['Review Text']].sample(5).values\nfor c in cl1:\n    print(c[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:02.952828Z","iopub.execute_input":"2021-09-16T12:14:02.953442Z","iopub.status.idle":"2021-09-16T12:14:02.967031Z","shell.execute_reply.started":"2021-09-16T12:14:02.953397Z","shell.execute_reply":"2021-09-16T12:14:02.966114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('5 reviews with the most negative polarity having polarity lesser than -0.80: \\n')\ncl3 = df.loc[df.polarity <= -0.80, ['Review Text']].sample(5).values\nfor c in cl3:\n    print(c[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:02.968795Z","iopub.execute_input":"2021-09-16T12:14:02.969676Z","iopub.status.idle":"2021-09-16T12:14:02.982222Z","shell.execute_reply.started":"2021-09-16T12:14:02.969633Z","shell.execute_reply":"2021-09-16T12:14:02.980988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(df[\"polarity\"],palette=\"rainbow\",data=df)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:02.98398Z","iopub.execute_input":"2021-09-16T12:14:02.984402Z","iopub.status.idle":"2021-09-16T12:14:03.19561Z","shell.execute_reply.started":"2021-09-16T12:14:02.984356Z","shell.execute_reply":"2021-09-16T12:14:03.195006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['polarity'].plot(\n    kind='hist',\n    bins=50,\n    color=\"peru\",\n    title='Sentiment Polarity Distribution');plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:03.196837Z","iopub.execute_input":"2021-09-16T12:14:03.197162Z","iopub.status.idle":"2021-09-16T12:14:03.513023Z","shell.execute_reply.started":"2021-09-16T12:14:03.197135Z","shell.execute_reply":"2021-09-16T12:14:03.512322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_s=df[df[\"polarity\"]>0].count()[\"sentiment\"]\nneu_s=df[df[\"polarity\"]==0].count()[\"sentiment\"]\nneg_s=df[df[\"polarity\"]<0].count()[\"sentiment\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:03.514262Z","iopub.execute_input":"2021-09-16T12:14:03.514596Z","iopub.status.idle":"2021-09-16T12:14:03.531372Z","shell.execute_reply.started":"2021-09-16T12:14:03.514568Z","shell.execute_reply":"2021-09-16T12:14:03.530359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting labels for items in Chart\nsentiment = ['positive_sentiment',\"neutral_sentiment\",\"negative_sentiment\"]\n  \n# Setting size in Chart based on \n# given values\nvalues = [p_s,neu_s,neg_s]\n  \n# colors\ncolors = ['#FF0000', 'olive', '#FFFF00']\n# explosion\nexplode = (0.05, 0.05, 0.05)\n  \n# Pie Chart\nplt.pie(values, colors=colors, labels=sentiment,\n        autopct='%1.1f%%', pctdistance=0.85,\n        explode=explode)\n  \n# draw circle\ncentre_circle = plt.Circle((0, 0), 0.70, fc='white')\nfig = plt.gcf()\n  \n# Adding Circle in Pie chart\nfig.gca().add_artist(centre_circle)\n  \n# Adding Title of chart\nplt.title('count of polarity as per sentiment')\n  \n# Displaing Chart\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:03.53285Z","iopub.execute_input":"2021-09-16T12:14:03.533067Z","iopub.status.idle":"2021-09-16T12:14:03.660926Z","shell.execute_reply.started":"2021-09-16T12:14:03.533042Z","shell.execute_reply":"2021-09-16T12:14:03.66029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.plot.box(y=[\"word_count\"],color=\"hotpink\")","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:03.661991Z","iopub.execute_input":"2021-09-16T12:14:03.663192Z","iopub.status.idle":"2021-09-16T12:14:03.873115Z","shell.execute_reply.started":"2021-09-16T12:14:03.66312Z","shell.execute_reply":"2021-09-16T12:14:03.872452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['word_count'].plot(\n    kind='hist',\n    bins=100,\n    color=\"orange\",\n    title='Review Text Word Count Distribution');plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:03.874491Z","iopub.execute_input":"2021-09-16T12:14:03.875508Z","iopub.status.idle":"2021-09-16T12:14:04.265648Z","shell.execute_reply.started":"2021-09-16T12:14:03.875475Z","shell.execute_reply":"2021-09-16T12:14:04.264818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxenplot(x=\"news_len\",data=df);plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:04.266952Z","iopub.execute_input":"2021-09-16T12:14:04.267195Z","iopub.status.idle":"2021-09-16T12:14:04.443594Z","shell.execute_reply.started":"2021-09-16T12:14:04.267165Z","shell.execute_reply":"2021-09-16T12:14:04.442912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['news_len'].plot(\n    kind='hist',\n    bins=50,\n    color=\"lightblue\",\n    title='Review Text Word Count Distribution');plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:04.445109Z","iopub.execute_input":"2021-09-16T12:14:04.445575Z","iopub.status.idle":"2021-09-16T12:14:04.73654Z","shell.execute_reply.started":"2021-09-16T12:14:04.445543Z","shell.execute_reply":"2021-09-16T12:14:04.735901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(df, x=\"news_len\", y=\"word_count\", color=\"sentiment\", \n                 marginal_x=\"box\", marginal_y=\"violin\",\n                  title=\"Click on the legend items!\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:04.7377Z","iopub.execute_input":"2021-09-16T12:14:04.738098Z","iopub.status.idle":"2021-09-16T12:14:05.931937Z","shell.execute_reply.started":"2021-09-16T12:14:04.738068Z","shell.execute_reply":"2021-09-16T12:14:05.931069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf1.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar',title='Top 20 words in review before removing stop words')\ndf1","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:05.933371Z","iopub.execute_input":"2021-09-16T12:14:05.934348Z","iopub.status.idle":"2021-09-16T12:14:06.620491Z","shell.execute_reply.started":"2021-09-16T12:14:05.934301Z","shell.execute_reply":"2021-09-16T12:14:06.619684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf2.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Top 20 words in review after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:06.621864Z","iopub.execute_input":"2021-09-16T12:14:06.622103Z","iopub.status.idle":"2021-09-16T12:14:07.273114Z","shell.execute_reply.started":"2021-09-16T12:14:06.622074Z","shell.execute_reply":"2021-09-16T12:14:07.27205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf3 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf3.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar',title='Top 20 bigrams in review before removing stop words')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:07.274487Z","iopub.execute_input":"2021-09-16T12:14:07.274745Z","iopub.status.idle":"2021-09-16T12:14:08.596808Z","shell.execute_reply.started":"2021-09-16T12:14:07.274702Z","shell.execute_reply":"2021-09-16T12:14:08.595985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', title='Top 20 bigrams in review after removing stop words')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:08.600803Z","iopub.execute_input":"2021-09-16T12:14:08.60108Z","iopub.status.idle":"2021-09-16T12:14:09.537544Z","shell.execute_reply.started":"2021-09-16T12:14:08.601038Z","shell.execute_reply":"2021-09-16T12:14:09.536246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf5 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf5.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', title='Top 20 trigrams in review before removing stop words')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:09.53928Z","iopub.execute_input":"2021-09-16T12:14:09.53972Z","iopub.status.idle":"2021-09-16T12:14:10.762976Z","shell.execute_reply.started":"2021-09-16T12:14:09.539662Z","shell.execute_reply":"2021-09-16T12:14:10.761917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(df['Review Text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\ndf6 = pd.DataFrame(common_words, columns = ['ReviewText' ,'count'])\ndf6.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', title='Top 20 trigrams in review after removing stop words')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:10.764322Z","iopub.execute_input":"2021-09-16T12:14:10.764581Z","iopub.status.idle":"2021-09-16T12:14:11.743467Z","shell.execute_reply.started":"2021-09-16T12:14:10.76455Z","shell.execute_reply":"2021-09-16T12:14:11.742689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nblob = TextBlob(str(df['Review Text']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.plot(\n    kind='bar',\n    title='Top 20 Part-of-speech tagging for review corpus')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:11.744667Z","iopub.execute_input":"2021-09-16T12:14:11.744925Z","iopub.status.idle":"2021-09-16T12:14:12.183462Z","shell.execute_reply.started":"2021-09-16T12:14:11.744897Z","shell.execute_reply":"2021-09-16T12:14:12.182549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y0 = df.loc[df['sentiment'] == 'positive']['polarity']\ny1 = df.loc[df['sentiment'] == 'negative']['polarity']\ny2 = df.loc[df['sentiment'] == 'neutral']['polarity']\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'positive',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'negative',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\ntrace2 = go.Box(\n    y=y2,\n    name = 'neutral',\n    marker = dict(\n        color = 'rgb(10, 140, 208)',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"Polarity Boxplot according to sentiment\"\n)\n\ngo.Figure(data=data,layout=layout)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:12.18486Z","iopub.execute_input":"2021-09-16T12:14:12.185982Z","iopub.status.idle":"2021-09-16T12:14:12.219104Z","shell.execute_reply.started":"2021-09-16T12:14:12.18592Z","shell.execute_reply":"2021-09-16T12:14:12.218295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y0 = df.loc[df['sentiment'] == 'positive']['news_len']\ny1 = df.loc[df['sentiment'] == 'negative']['news_len']\ny2 = df.loc[df['sentiment'] == 'neutral']['news_len']\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'positive',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'negative',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\ntrace2 = go.Box(\n    y=y2,\n    name = 'neutral',\n    marker = dict(\n        color = 'rgb(10, 140, 208)',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"news length Boxplot by sentiment\"\n)\ngo.Figure(data=data,layout=layout)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:12.220439Z","iopub.execute_input":"2021-09-16T12:14:12.220662Z","iopub.status.idle":"2021-09-16T12:14:12.24878Z","shell.execute_reply.started":"2021-09-16T12:14:12.220635Z","shell.execute_reply":"2021-09-16T12:14:12.247821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xp = df.loc[df['sentiment'] == \"positive\", 'polarity']\nxneu = df.loc[df['sentiment'] == \"neutral\", 'polarity']\nxneg= df.loc[df['sentiment'] == \"negative\", 'polarity']\n\ntrace1 = go.Histogram(\n    x=xp, name='positive',\n    opacity=0.75\n)\ntrace2 = go.Histogram(\n    x=xneu, name = 'neutral',\n    opacity=0.75\n)\ntrace3 = go.Histogram(\n    x=xneg, name = 'negative',\n    opacity=0.75\n)\ndata = [trace1, trace2,trace3]\nlayout = go.Layout(barmode='overlay', title='Distribution of Sentiment polarity')\ngo.Figure(data=data, layout=layout)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:12.250088Z","iopub.execute_input":"2021-09-16T12:14:12.250382Z","iopub.status.idle":"2021-09-16T12:14:12.279302Z","shell.execute_reply.started":"2021-09-16T12:14:12.250352Z","shell.execute_reply":"2021-09-16T12:14:12.27878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Scatter(\n    x=df['polarity'], y=df['news_len'], mode='markers', name='points',\n    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n)\ntrace2 = go.Histogram2dContour(\n    x=df['polarity'], y=df['news_len'], name='density', ncontours=50,\n    colorscale='Hot', reversescale=True, showscale=False\n)\ntrace3 = go.Histogram(\n    x=df['polarity'], name='Sentiment polarity density',\n    marker=dict(color='rgb(102,0,0)'),\n    yaxis='y2'\n)\ntrace4 = go.Histogram(\n    y=df['news_len'], name='news length density', marker=dict(color='rgb(102,0,0)'),\n    xaxis='x2'\n)\ndata = [trace1, trace2, trace3, trace4]\n\nlayout = go.Layout(\n    showlegend=False,\n    autosize=False,\n    width=600,\n    height=550,\n    xaxis=dict(\n        domain=[0, 0.85],\n        showgrid=False,\n        zeroline=False\n    ),\n    yaxis=dict(\n        domain=[0, 0.85],\n        showgrid=False,\n        zeroline=False\n    ),\n    margin=dict(\n        t=50\n    ),\n    hovermode='x unified',\n    bargap=0,\n    xaxis2=dict(\n        domain=[0.85, 1],\n        showgrid=False,\n        zeroline=False\n    ),\n    yaxis2=dict(\n        domain=[0.85, 1],\n        showgrid=False,\n        zeroline=False\n    )\n)\n\ngo.Figure(data=data, layout=layout)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:12.280504Z","iopub.execute_input":"2021-09-16T12:14:12.280843Z","iopub.status.idle":"2021-09-16T12:14:12.34187Z","shell.execute_reply.started":"2021-09-16T12:14:12.280815Z","shell.execute_reply":"2021-09-16T12:14:12.340833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Scatter(\n    x=df['polarity'], y=df['word_count'], mode='markers', name='points',\n    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n)\ntrace2 = go.Histogram2dContour(\n    x=df['polarity'], y=df['word_count'], name='density', ncontours=20,\n    colorscale='Hot', reversescale=True, showscale=False\n)\ntrace3 = go.Histogram(\n    x=df['polarity'], name='Sentiment polarity density',\n    marker=dict(color='rgb(102,0,0)'),\n    yaxis='y2'\n)\ntrace4 = go.Histogram(\n    y=df['word_count'], name='word count density', marker=dict(color='rgb(112,0,0)'),\n    xaxis='x2'\n)\ndata = [trace1, trace2, trace3, trace4]\n\nlayout = go.Layout(\n    showlegend=False,\n    autosize=False,\n    width=600,\n    height=550,\n    xaxis=dict(\n        domain=[0, 0.85],\n        showgrid=False,\n        zeroline=False\n    ),\n    yaxis=dict(\n        domain=[0, 0.85],\n        showgrid=False,\n        zeroline=False\n    ),\n    margin=dict(\n        t=50\n    ),\n    hovermode='closest',\n    bargap=0,\n    xaxis2=dict(\n        domain=[0.85, 1],\n        showgrid=False,\n        zeroline=False\n    ),\n    yaxis2=dict(\n        domain=[0.85, 1],\n        showgrid=False,\n        zeroline=False\n    )\n)\n\ngo.Figure(data=data, layout=layout)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:14:12.343113Z","iopub.execute_input":"2021-09-16T12:14:12.343365Z","iopub.status.idle":"2021-09-16T12:14:12.385821Z","shell.execute_reply.started":"2021-09-16T12:14:12.343338Z","shell.execute_reply":"2021-09-16T12:14:12.384882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install scattertext","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:15:23.318559Z","iopub.execute_input":"2021-09-16T12:15:23.318921Z","iopub.status.idle":"2021-09-16T12:15:31.631515Z","shell.execute_reply.started":"2021-09-16T12:15:23.318888Z","shell.execute_reply":"2021-09-16T12:15:31.630166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install spacy","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:15:55.170056Z","iopub.execute_input":"2021-09-16T12:15:55.170376Z","iopub.status.idle":"2021-09-16T12:16:03.356836Z","shell.execute_reply.started":"2021-09-16T12:15:55.170343Z","shell.execute_reply":"2021-09-16T12:16:03.355638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scattertext as st\nimport spacy\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\ncorpus = st.CorpusFromPandas(df, category_col='sentiment', text_col='Review Text', nlp=nlp).build()\nprint(list(corpus.get_scaled_f_scores_vs_background().index[:20]))","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:17.549191Z","iopub.execute_input":"2021-09-16T12:17:17.549734Z","iopub.status.idle":"2021-09-16T12:17:23.223738Z","shell.execute_reply.started":"2021-09-16T12:17:17.54968Z","shell.execute_reply":"2021-09-16T12:17:23.223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"term_freq_df = corpus.get_term_freq_df()\nterm_freq_df['positive_sentiment'] = corpus.get_scaled_f_scores('positive')\nlist(term_freq_df.sort_values(by='positive_sentiment', ascending=False).index[:20])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:26.289071Z","iopub.execute_input":"2021-09-16T12:17:26.290351Z","iopub.status.idle":"2021-09-16T12:17:26.341759Z","shell.execute_reply.started":"2021-09-16T12:17:26.290308Z","shell.execute_reply":"2021-09-16T12:17:26.341136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"term_freq_df['neutral_sentiment'] = corpus.get_scaled_f_scores('neutral')\nlist(term_freq_df.sort_values(by='neutral_sentiment', ascending=False).index[:20])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:31.716476Z","iopub.execute_input":"2021-09-16T12:17:31.716777Z","iopub.status.idle":"2021-09-16T12:17:31.755161Z","shell.execute_reply.started":"2021-09-16T12:17:31.716748Z","shell.execute_reply":"2021-09-16T12:17:31.754164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"term_freq_df['negative_sentiment'] = corpus.get_scaled_f_scores('negative')\nlist(term_freq_df.sort_values(by='negative_sentiment', ascending=False).index[:20])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:35.368765Z","iopub.execute_input":"2021-09-16T12:17:35.3691Z","iopub.status.idle":"2021-09-16T12:17:35.409803Z","shell.execute_reply.started":"2021-09-16T12:17:35.369068Z","shell.execute_reply":"2021-09-16T12:17:35.40886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom collections import Counter\n\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\nreindexed_data = df['Review Text'].values\ndocument_term_matrix = tfidf_vectorizer.fit_transform(reindexed_data)\nn_topics = 10\nlsa_model = TruncatedSVD(n_components=n_topics)\nlsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)\n    \nlsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n\ndef get_top_n_words(n, keys, document_term_matrix, tfidf_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words\n    \ntop_lsa=get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:42.805837Z","iopub.execute_input":"2021-09-16T12:17:42.806147Z","iopub.status.idle":"2021-09-16T12:17:44.664923Z","shell.execute_reply.started":"2021-09-16T12:17:42.806116Z","shell.execute_reply":"2021-09-16T12:17:44.663649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(top_lsa)):\n    print(\"Topic {}: \".format(i+1), top_lsa[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:49.136629Z","iopub.execute_input":"2021-09-16T12:17:49.137772Z","iopub.status.idle":"2021-09-16T12:17:49.147193Z","shell.execute_reply.started":"2021-09-16T12:17:49.137727Z","shell.execute_reply":"2021-09-16T12:17:49.146103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_3_words = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)\nlabels = ['Topic {}: \\n'.format(i+1) + top_3_words[i] for i in lsa_categories]\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lsa_categories, lsa_counts,color=\"skyblue\");\nax.set_xticks(lsa_categories,);\nax.set_xticklabels(labels, rotation=45, rotation_mode='default',color=\"olive\");\nax.set_ylabel('Number of review text on topics');\nax.set_title('Count of LSA topics');\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:17:58.189284Z","iopub.execute_input":"2021-09-16T12:17:58.189612Z","iopub.status.idle":"2021-09-16T12:17:59.954393Z","shell.execute_reply.started":"2021-09-16T12:17:58.18958Z","shell.execute_reply":"2021-09-16T12:17:59.953307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}