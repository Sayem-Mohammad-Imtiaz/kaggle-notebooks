{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in \\$1000's\n\n"},{"metadata":{"_cell_guid":"d5b0617d-81ab-424f-a1ee-6674925f971e","_uuid":"b08753971c228268b0d2fba0a6978dfcfe9943f4","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\nfrom pandas import read_csv\n#Lets load the dataset and sample some\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n# Importation du dataset\ndf = read_csv('../input/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fec83140-b3c5-4938-a031-6a34c2ebc45e","_uuid":"79d8bdae5e138554cb4626c784e0cfd43c40606d","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il n'y a pas de valeur non-numérique, on peut donc continuer.  \nOn vérifie si des valeurs sont manquantes (NaN):"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Toutes les colonnes sont à 506, il ne manque donc aucune valeur."},{"metadata":{},"cell_type":"markdown","source":"# Recherche de corrélations"},{"metadata":{},"cell_type":"markdown","source":"On va utiliser la fonction \"corr\" pour calculer systématiquement le degré de corrélation entre deux paramètres :"},{"metadata":{"trusted":true},"cell_type":"code","source":"tabcorr = df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilisation d'une \"heatmap\" pour visualiser l'ensemble des corrélations :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(abs(tabcorr), cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regroupement des paramètres par cluster classé par proximité:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(abs(tabcorr), cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque 3 clusters principaux:\n- RM, LSTAT et MEDV\n- CRIL, RAD et TAX\n- NOX, INDUS, DIS et AGE"},{"metadata":{},"cell_type":"markdown","source":"On peu visiualiser différement ces cluster à l'aide d'un dendrogramme :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc\n\ncorr = 1 - df.corr()\ncorr_condensed = hc.distance.squareform(corr)\nlink = hc.linkage(corr_condensed, method='ward')\nplt.figure(figsize=(12,12))\nden = hc.dendrogram(link, labels=df.columns, orientation='left', leaf_font_size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**On va s'intéresser maintenant et pour la suite** à la valeur médiane des logements (MEDV):"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = tabcorr.MEDV\n# on retire la ligne MEDV qui sera forcement à 1.00\ncorrelations = correlations.drop(['MEDV'],axis=0)\n# Affichage dans l'ordre décroissant ET on met tout en absolue car une valeur négative est aussi significative\nprint(abs(correlations).sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque que ce qui a le plus corrélé sont LSTAT (% lower status of the population) et RM (average number of rooms per dwelling) avec ~69-74% comme ce qui avait pu être constaté avec le regroupement par cluster."},{"metadata":{},"cell_type":"markdown","source":"# Méthode #1: Régression linéaire multiple"},{"metadata":{},"cell_type":"markdown","source":"Il faut dissocier les caractèristiques discrètes du reste, mais ici il n'y en a pas donc cela n'est pas nécessaire."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#On crée la cible y (colonne 'MEVD')\ny = df['MEDV']\n#On crée les caractéristiques X\nX = df.drop(['MEDV'], axis=1)\n\n#Importation de la méthode\nfrom sklearn.model_selection import train_test_split\n#On sépare les ensembles d'apprentissage et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On utilise la fonction de régression linéaire multiple de sklearn :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation de la méthode\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n# apprentissage\nlm.fit(X_train, y_train)\n# prédiction\ny_pred = lm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On trace un nuage de points pour comparer la prédiction et les résultats attendus :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_pred)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution de l'erreur\nsns.distplot(y_test-y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque que le résultat possède une erreur entre -10 et +10 comparé à la valeur réelle du MEDV.  \n  \nOn peut calculer une erreur moyenne à l'aide des moindres carrés :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation de la méthode \nfrom sklearn.metrics import mean_squared_error, r2_score\n# Calcul\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sur les prédictions il y a une erreur moyenne de ~ 4.53 .\nOn va maintenant déterminer le score de précision R2 :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nscoreR2 = r2_score(y_test, y_pred)\nprint(scoreR2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La prédiction possède une précision de 77.86% ."},{"metadata":{},"cell_type":"markdown","source":"# Méthode #2: Régression par forêts aléatoires"},{"metadata":{},"cell_type":"markdown","source":"Nous allons essayer d'obtenir un meilleur score à l'aide de la méthode cette méthode. Pour cela nous allons utiliser les forêts aléatoires pour la régression."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importation de la méthode\nfrom sklearn import ensemble\n#Entrainement\nrf = ensemble.RandomForestRegressor()\nrf.fit(X_train, y_train)\n# Prédiction\ny_rf = rf.predict(X_test)\n#Calcul du score\nprint(rf.score(X_test,y_test))\n# Importation de la méthode \nfrom sklearn.metrics import mean_squared_error, r2_score\n# Calcul\nprint(np.sqrt(mean_squared_error(y_test, y_rf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On obtient désormais un meilleur score (aléatoire mais en moyenne supérieur) avec une précision de moyenne 90% et un erreur moyenne comprise entre 2.4 et 3.2 pour la valeur de MEDV.  \n  \nOn trace un nuage de points pour comparer la prédiction et les résultats attendus :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_rf)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ce nuage de points montre une plus petite disparité des points contrairement à la méthode précédente.\n\n**Conclusion:** Cette méthode est bien meilleur que la précédente."},{"metadata":{},"cell_type":"markdown","source":"# Méthode #3: Gradient Boosting Regressor"},{"metadata":{},"cell_type":"markdown","source":"Nous allons essayer d'obtenir un meilleur score à l'aide de la méthode cette méthode. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Importation de la méthode\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Entrainement\nreg = GradientBoostingRegressor(random_state=0)\nreg.fit(X_train, y_train)\n# Prédiction\ny_reg = reg.predict(X_test)\n# Calcul du score\nprint(reg.score(X_test,y_test))\n# Calcul de l'erreur moyenne\nprint(np.sqrt(mean_squared_error(y_test, y_reg)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque une nouvelle amélioration comparé aux méthodes précédentes avec un score de précision de 93.21% et une erreur moyenne de 2.5 pour la valeur de MEDV."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_reg)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Prediction de prix\")\nplt.title(\"Prix reels vs predictions\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** Cette méthode est bien meilleur que les 2 précédentes comme l'on peut le voir avec le score de précision, l'erreur moyenne et la disparité des points."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}