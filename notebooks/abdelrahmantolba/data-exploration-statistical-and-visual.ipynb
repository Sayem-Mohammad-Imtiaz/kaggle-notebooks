{"cells":[{"metadata":{},"cell_type":"markdown","source":"***Data Exploration***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First things first what is data explorationn and why is it important ? \ndata exploration is the precoss of reviewing the data to uncover initial patterns, characteristics and points of interest . and it is important to help us to know our data , understand it and make some assumptions and maybe test these assumptions . further more it helps to reveal some fatal errors , or in another words data corruption . like huge amount of missing data or unrealstic records or irrelevant data feaures etc .. and with some good data exploration and data cleaning , we are almost done with machine learning task unless you are willing to implement the model from the scratch . this will be a different story . now we only are intersted in data exploration and data cleaning will be out of our scope but we will beak and have a taste about what data cleaning is . enough talking and lets start .\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we jump into coding we will see a list of the content covered here . \n* Exploring the data in a statistical way \n* Seeing some data visulization and graphs \n* Inroducing missing data \n* Optional section ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So at the beginig we will import the important libraries that we are going to need throw our tutorial ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we import our data and read it using pandas lib . actually there is a variety of built in functions for reading data in pandas like read_csv() , read_json() , read_xlsx() , and more and u choose what ever u like accordind to the format of ur data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can import our data from our local host or as raw data from a wep page\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pokemon/Pokemon.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For starters we will examine our data statistically ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will take a look at our data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the first look the data is huge to examine at once . first we will se the size of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Know the size of the data uaing .shape function \nprint('The size of the data is',data.shape[0],'rows and ', data.shape[1],'columns and the shape is ',data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK , we will take a look at these columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will see the description of the data , the mean std and count of the columns etc .. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok it is time to look at the actual records of our data set , we will start with the first few rows ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can aslo add parameter to the head(x) to get the first x rows ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now will see the least 7 rows ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we may need to take a look in the middle at a specific row or a group of rows ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see a specific row \ndata.iloc[150]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examinig punch of rows \ndata.iloc[50:55]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may also use the function loc instead of iloc to include the last row which in the previous case is 55 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we may need to look at random sample from the data to spot any irruglarity and also to feel more confedint ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# five random rows from the the data  \ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am curious to know how many pocemon is legandary ! what about you ? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Legendary'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now if we want to make it in a fancy way we would use series to store the values with indector index ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = data['Legendary'].value_counts()\nseries = pd.Series(\n    np.array([result[1],result[0]]),\n    index = ['Legandary', 'NonLegendary']\n                )\nprint(series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am curuios to know which among them has the highst attack value ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# first we know what is the maximum attack value \ndata['Attack'].max()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we query the whole record using the data we had from the previous \ndata.query(\"Attack == 190\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we used query function here wich takes a valid query as input , any valid query ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# display all the legendary pocemons \ndata.query('Legendary == True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will see some specifics of a specific feature ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum value of HP')\nprint(data['HP'].max())\nprint('----------------------')\nprint('Mimimum value of HP')\nprint(data['HP'].min())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# or u can easily use describe() function that we used before on that specific feature \ndata['Attack'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remeber the describe function we used earlier ? we can envoke it in a specific column as well ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can easily use describe() function that we used before on that specific feature \ndata['Attack'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The aggregate function \"groupby\" enables us to ctegorise your data and deal with it as chunks instead of individual records as follows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"frame = data.groupby('Generation').mean()\nframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now time to see some Graphs and visualizations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will plot the distripution of the attack feature using seaborn library \nsns.distplot(data['Attack']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now will see some statistics about that graph","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Skewness** is a measure of the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. so if skewness is zero means that the data is perfectly semetric higher or lower values other than zero means that te data is skewwed right or left respectivly .\n\n**Kurtosis** is a measure of whether the data are heavy tailed or light tailed relative to a normal distribution . \nthe standard normal distribution has a kurtosis of 0 positive kurtosis indicates a \"heavy-tailed\" distribution and negative kurtosis indicates a \"light tailed\" distribution.\nif You still confused you can take a look here it is a 5 minutes read \nhttps://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the values of skewness and kurtosis of the prevoius graph \nprint(\"Skewness: %f\" % data['Attack'].skew())\nprint(\"Kurtosis: %f\" % data['Attack'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['HP']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: %f\" % data['HP'].skew())\nprint(\"Kurtosis: %f\" % data['HP'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSo far we saw thae distripution of the features alone next will see the relations between the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var2 = 'Attack'\nvar1 = 'Total'\npd.concat([data[var2], data[var1]], axis=1)\ndata.plot.scatter(x=var1, y=var2, ylim=(0,200));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The privious plot shows the the relation between the Total which represents the X axis and Attack which represents the Y aixis  \nand we can see that the line x=y will just fit in the middle so its almost a linear relationship with some outliers . outliers ??!! hang on a moment , will discuss it in a bit","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now will see another form of visualizing the relations between the features which is box blot \n\nbriefly  50% of the data will be in the colored box and 25% on the above and 25 are below \n\nfor more explanation about box blots ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var1 = 'Generation'\nvar2 = 'Total'\npd.concat([data[var1], data[var2]], axis=1)\nfig = sns.boxplot(x=var1, y=var2, data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the relation between the generation and the Total score is almost linear and the means nor the out liers changed alot ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var1 = 'Speed'\nvar2 = 'Total'\npd.concat([data[var1], data[var2]], axis=1)\nfig = sns.boxplot(x=var1, y=var2, data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here also you can see the aproximate linear relation between the speed and the total and the average speed per total alos encoded on the graph represented by the box plot ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Visualizing the realtions bettween the features does give us a good intuition about waht is going on between them but we are not yet confedint , we dont have a specific value to rely on . right ? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = data.corr()\ncorrmat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the exact numbers of how the columns are correlated , as we can see in our data its relatively easy to read the above matrix but in real world it can get missey and you will be lost between the numbers so here is a visual aid ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good , intuitive but again no numbers !!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True,annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes , thats exactly what we are looking for both statistical and visual indecation at the same outbut","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\ncols = [ 'Total', 'HP', 'Attack', 'Defense','Sp. Atk', 'Sp. Def', 'Speed' ,'Generation']\nsns.pairplot(data[cols], size = 1.5) \nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that was an over all view that gives you more confedince and makes it more easy for you to see the relationships,\nand in the diagonal we can see a histogram representaion of distripution of that feature ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"While i was exploring the dataset with you i noticed something wierd !! the column # feels like its here by mistake .. no strong corelation with other feaures , the name itself is not an indecator of any thing . what does it do here ?!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I dont know whether u have noticed that or not but that column with label # is looks like its an index that starts with 1 . is it ? lets find out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# see the count of every record in that col \ndata['#'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well it turned out that this col has some repititve values but the majority is unique , but that tells us that it is not an index .. maybe it is an index and the records are repeated so lets find out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# see the count of every record in the name columne \ndata['Name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no repetetive names , # is not an index maybe it has smth to do with the other columns , has realtions with them or tells us any extra information ?? will find that out in a bit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['#'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'Attack'\nvar0 = '#'\npd.concat([data[var0], data[var]], axis=1)\ndata.plot.scatter(x=var, y=var0, ylim=(0,800),xlim=(0,180));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'HP'\nvar0 = '#'\npd.concat([data[var0], data[var]], axis=1)\ndata.plot.scatter(x=var, y=var0, ylim=(0,800),xlim=(0,150));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as it turned out that specific feature does not have any relationshib between the other ones as the plots shows random noise , almost constant ngeligable correlation between other features , so we will delete that column and will continue our exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('#',axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that # does not exist any more","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can not say that we explored the dataset very well and know everything boutit without lookin at the missing data and the outliers in you dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Breif Intro to missing data and outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First things first -> missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# see how many null value in every column \nAll = data.isnull().sum().sort_values(ascending=False)\nAll","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 386 missing values in the column Type 2 an nothing else is misssing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\npercentage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here you can the percentage of the missing values in each coloumn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = pd.concat([All, percentage], axis=1, keys=['All', 'percentage'])\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dealing with missing data is out of our scope but will take a prief look of what to be done here ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"You can drop the entire column that has a large amount of missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"New_data = data.drop('Type 2' , axis = 1)\nNew_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maybe you dont want to drop the entire column and need only to drop the rows from ur dataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting all the rows with at least one null value \nt1 = data.dropna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"All = t1.isnull().sum().sort_values(ascending=False)\nAll","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data and type 2 col still exists","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#deleting all the rows that has all the values in them null  \nt2 = data.dropna(how = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"All = t2.isnull().sum().sort_values(ascending=False)\nAll","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It did not delete the missing data from type 2 because the entire record is not null","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"dropna() function has many other usages but we are not covering them all because they are all alike so quik look at the function documentation will give us every thing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There is also a lot of other function to deal with missing data \nlike fillna","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling the missing valuse in type 2 col witth a value of the previous row \ntemp  = data['Type 2'].fillna(method='ffill')\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = temp.isnull().sum()\nmissing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see no missing values were found in the temp , we can pass a constant also to the method fillna and will but that constant in every null value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"another way is to use interpolate() method and pass the interpolation methed like linear for example and the method interpolates the missing values for us , we can also add limit to our interpolation etc .. for more information -----> https://pandas.pydata.org/pandas-docs/version/0.16.2/generated/pandas.DataFrame.interpolate.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we can discuss outliers , outliers is the record that does not belong to the crowd , in other words when we see the distripution of a data we can see the outliers at the very left and right side of the grapgh . and it is really dangerous if we did not handle them and deal with the properly for example : if we have a fature Age in you data set and it varies from 5 to 1000 you should but a huge quiestion mark here as it affects the mean and the skewness , etc .. and that will affect the result of any thing you do with your data , as they always say garpage data in ---> garpage result out . we can deal with them by butting a limit for a specific feature like the age for example could be only from 0 to 100 or the gender can only be male or female etc ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['HP']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see that most of the vlues are between 0 and 150 but the graph continues to 250 so that could be some kind of outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now we can read your data and see any portion ywe like to inspect and also see some visualization that help us to understad our data in better way**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Extra knowledge","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Normality** - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this . In this exercise we'll just check univariate normality for 'Defense' , but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems so that's the main reason why we are doing this analysis.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Defense'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['Defense'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see that the destripution of the data is not that bad and it can really be dealt with this way but will apply the log transformation to normaliaze our data as follows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying log transformation\ndata['Defense'] = np.log(data['Defense'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Defense'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['Defense'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that its fitted in a better way after applying the log transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thats is the end of our tutorial. peice","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}