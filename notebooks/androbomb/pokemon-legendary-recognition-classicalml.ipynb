{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"###############################################################\n# NB: shift + tab HOLD FOR 2 SECONDS!\n###############################################################\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nprint('\\n ')\nprint('Getting traing dataset...')\ndata = pd.read_csv('../input/pokemon/Pokemon.csv')\nprint('Traing data set obtained. \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the dataset. \n\nWe will see that the columns 'type 1' and 'type 2' have string entries; we will need thus a function that associates a numerical value to each type. We follow the type ordering of PokemonCentralWiki. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def type_numbering(string) : \n    number = 0\n    if string == 'Normal' :\n        number = 1\n    elif string == 'Fire' :\n        number = 2\n    elif string == 'Fighting' :\n        number = 3\n    elif string == 'Water' :\n        number = 4\n    elif string == 'Flying' :\n        number = 5\n    elif string == 'Grass' :\n        number = 6\n    elif string == 'Poison' :\n        number = 7\n    elif string == 'Electric' :\n        number = 8\n    elif string == 'Ground' :\n        number = 9\n    elif string == 'Psychic' :\n        number = 10\n    elif string == 'Rock' :\n        number = 11\n    elif string == 'Ice' :\n        number = 12\n    elif string == 'Bug' :\n        number = 13\n    elif string == 'Dragon' :\n        number = 14\n    elif string == 'Ghost' :\n        number = 15\n    elif string == 'Dark' :\n        number = 16\n    elif string == 'Steel' :\n        number = 17\n    elif string == 'Fairy' :\n        number = 18\n    else :\n        number = 0\n    \n    return number;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree or Random Forest classifier\n\nWe now define a function that eats the pandas dataframe and then gives a prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def DT_RF_classifier(data, kind='DT', test_size=0.3, max_depth=None):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report,confusion_matrix\n    \n    print('Splitting data...')\n    df = data\n    df['Type 1'] = data['Type 1'].apply(type_numbering)\n    df['Type 2'] = data['Type 2'].apply(type_numbering)\n    X = df.drop('Legendary',axis=1).drop('Name', axis=1)\n    y = df['Legendary']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    print('Splitting done. \\n')\n\n    print('Initializing classifier...')\n    if (kind == 'DT'):\n        from sklearn.tree import DecisionTreeClassifier\n        \n        print('Classifier type: Decision Tree ')\n        print('Fitting classifier...')\n        clf = DecisionTreeClassifier(max_depth=max_depth)\n        clf.fit(X_train,y_train)\n        \n        predictions = clf.predict(X_test)\n        print('Fit done. \\n')\n    \n    else :\n        from sklearn.ensemble import RandomForestClassifier\n        \n        print('Classifier type: Random Forest ')\n        print('Fitting classifier...')\n        clf = RandomForestClassifier(max_depth=max_depth, n_estimators=100)\n        clf.fit(X_train,y_train)\n        \n        predictions = clf.predict(X_test)\n        print('Fit done. \\n')\n\n    print('Evaluating the model...')\n    \n    print(classification_report(y_test,predictions))\n    print('The score is: ', clf.score(X_test, y_test))\n    print('\\n')\n    cm = confusion_matrix(y_test,predictions)\n    print(cm)\n    df_cm = pd.DataFrame(cm, index = ['Non-Legendary', 'Legendary'], columns = ['Non-Legendary', 'Legendary'])\n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n    plt.xlabel(\"Predicted Class\", fontsize=18)\n    plt.ylabel(\"True Class\", fontsize=18)\n    \n    if (kind == 'DT'):\n        from sklearn.externals.six import StringIO  \n        from IPython.display import Image  \n        from sklearn.tree import export_graphviz\n        \n        \n        dot_data = StringIO()\n        \n        export_graphviz(clf, out_file='tree.dot',\n                        feature_names = X.columns.values, \n                        class_names = ['Non-Legendary', 'Legendary'], \n                        filled=True, rounded=True, proportion=False, precision=2)\n        \n    else : \n        from sklearn.externals.six import StringIO  \n        from IPython.display import Image  \n        from sklearn.tree import export_graphviz\n        \n        # Extract single tree\n        estimator = clf.estimators_[5]\n        \n        # Export as dot file\n        export_graphviz(estimator, out_file='tree.dot', \n                feature_names = X.columns.values,\n                class_names = ['Non-Legendary', 'Legendary'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n        \n    # Convert to png\n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n        \n    # Display in python\n    plt.figure(figsize = (14, 18))\n    plt.imshow(plt.imread('tree.png'))\n    plt.axis('off');\n        \n        \n    plt.show()\n    print('\\n ')\n    print('Process ended. ')\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree example"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_RF_classifier(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We haven't set the max_depth for the DT; the tree is more precise but the graph is less readable. Let us try once with a max_depth of 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_RF_classifier(data, max_depth=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest example"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_RF_classifier(data, kind='RF', max_depth=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nWe now want to use Logisti Regression to perform the same analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def LR_classifier(data, test_size=0.3):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report,confusion_matrix\n    \n    print('Splitting data...')\n    df = data\n    df['Type 1'] = data['Type 1'].apply(type_numbering)\n    df['Type 2'] = data['Type 2'].apply(type_numbering)\n    X = df.drop('Legendary',axis=1).drop('Name', axis=1)\n    y = df['Legendary']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    print('Splitting done. \\n')\n\n    print('Initializing classifier...')\n    from sklearn.linear_model import LogisticRegression\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train) # We fit the Logistic Regression Classifier\n    predictions = clf.predict(X_test) # We compute the predictions\n    \n    print('Evaluating the model...')\n    print(classification_report(y_test,predictions))\n    print('The score is: ', clf.score(X_test, y_test))\n    print('\\n')\n    cm = confusion_matrix(y_test,predictions)\n    print(cm)\n    df_cm = pd.DataFrame(cm, index = ['Non-Legendary', 'Legendary'], columns = ['Non-Legendary', 'Legendary'])\n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n    plt.xlabel(\"Predicted Class\", fontsize=18)\n    plt.ylabel(\"True Class\", fontsize=18)\n    \n    \n    print('\\n ')\n    print('Process ended. ')\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_classifier(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines (SVM)\n\nSupport Vector Machine algorithm are based on hyperplane separation of classified data-points in the feature-space; mathematically, if $x\\in\\mathcal{M}_d$ is a point in feature space (i.e. it contains d features), we can classify two sets of points $\\{ x_i\\}_{i=0, \\ldots \\# F-1}$, with $F$ the ensamble of all the points, i.e. all the Pokemon in this example, by introducing a co-dimension 1 surface $\\Sigma_{d-1} \\subset \\mathcal{M}_d$ that defines the border of two dimension-d regions $U_1$ and $U_2$, i.e. $\\partial U_i = \\Sigma_{d-1}$. Then, the classification is given by \n$$x_i \\in \\mathrm{Class}_i \\iff x_i \\in U_i$$\n\nSVM can be tricky, so we usually need to pick the right parameters entering in the SVM (we will choose a standard hyperplane as $\\Sigma_{d-1}$ for sake of simplicity), usually denoted as 'C',  'gamma', and 'kernel'. We will thus resort to a GridSearch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def SVM_classifier(data, test_size=0.3):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report,confusion_matrix\n    \n    print('Splitting data...')\n    df = data\n    df['Type 1'] = data['Type 1'].apply(type_numbering)\n    df['Type 2'] = data['Type 2'].apply(type_numbering)\n    X = df.drop('Legendary',axis=1).drop('Name', axis=1)\n    y = df['Legendary']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    print('Splitting done. \\n')\n\n    print('Initializing classifier...')\n    from sklearn.svm import SVC\n    clf = SVC()\n    clf.fit(X_train,y_train)\n    \n    # Starting the GridSearch\n    param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n    from sklearn.model_selection import GridSearchCV\n    grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n    grid.fit(X_train,y_train)\n    grid_predictions = grid.predict(X_test)\n    print('\\n ')\n    print('\\n ')\n    print('Best set of parameters found by GridSearch: ', grid.best_params_)\n    print('\\n ')\n    print('Initialization done. \\n')\n    \n    \n    \n    print('Evaluating the model...')\n    print(classification_report(y_test,grid_predictions))\n    cm=confusion_matrix(y_test,grid_predictions)\n    print(cm)\n    df_cm = pd.DataFrame(cm, index = ['Non-Legendary', 'Legendary'], columns = ['Non-Legendary', 'Legendary'])\n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n    plt.xlabel(\"Predicted Class\", fontsize=18)\n    plt.ylabel(\"True Class\", fontsize=18)\n    \n    \n    print('\\n ')\n    print('Process ended. ')\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM_classifier(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}