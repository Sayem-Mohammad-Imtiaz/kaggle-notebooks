{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam or Ham ?\n* We will see two approaches to vectorize text - word embedding and TF-IDF\n* We will transfer learning from the GloVe word embeddings dataset.\n* We use recall to evaluate the model because the dataset is highly unbalanced.\n* We also see the False Positives because we do not want genuine SMS's to be classified as spam.\n* TF-IDF works supremely better than word embeddings because the texts have a lot of shorthand and misspelt words."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\n\nimport re\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()\n\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',delimiter=',',encoding='latin-1')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\nle = LabelEncoder()\nraw_data.v1 = le.fit_transform(raw_data.v1)\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"We have an unbalanced dataset. Only 13.41% of the data is spam. So, let's use recall as a metric to evaluate our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_spam = raw_data.v1.sum()\nnum_ham = len(raw_data) - num_spam\n\nplt.pie([num_ham, num_spam],labels=[\"Ham\", \"Spam\"],explode=(0,0.2),autopct='%1.2f%%',startangle=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to try lemmatization and stopword removal. However, conventional processing techniques are not going to work well with a SMS corpus. The texts have a lot of shortened words and abbreviations. Ideally, we have to implement a customized normalization of text."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_stopwords = set([word.replace(\"'\",'') for word in stopwords.words('english')])\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.replace(\"'\",'')\n    text = re.sub('[^a-zA-Z]',' ',text)\n    words = text.split()\n    words = [lemma.lemmatize(word) for word in words if (word not in total_stopwords) and (len(word)>1)] # Remove stop words\n    text = \" \".join(words)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.v2 = raw_data.v2.apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at the most common words in spam SMS. We also observe a lot of shorthand which is going to mislead our classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(height=2000, width=2000, stopwords=set(stopwords.words('english')), background_color='white')\nwordcloud = wordcloud.generate(' '.join(raw_data[raw_data.v1==1].v2))\nplt.imshow(wordcloud)\nplt.title(\"Most common words in spam SMS\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(raw_data.v2, raw_data.v1, test_size=0.15, stratify=raw_data.v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embedding approach\nWe use a pre-trained GloVe embedding because we have a very small training set. This will serve as a starting point for our trainable embedding layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = x_train.apply(lambda str: len(str.split())).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok = Tokenizer()\ntok.fit_on_texts(x_train)\nsequences = tok.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences, maxlen=max_words, padding='post')\nvocab_size = len(tok.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in tok.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size, 50, input_length=max_words, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.fit(sequences_matrix, y_train, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tok.texts_to_sequences(x_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_glove.predict(test_sequences_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr, rc, thresholds = precision_recall_curve(y_test, y_pred)\nplt.plot(thresholds, pr[1:])\nplt.plot(thresholds, rc[1:])\nplt.show()\ncrossover_index = np.max(np.where(pr <= rc))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_recall = rc[crossover_index]\n\nprint(classification_report(y_test, y_pred > crossover_cutoff))\n\nm_confusion_test = confusion_matrix(y_test, y_pred > crossover_cutoff)\ndisplay(pd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1']))\n\nprint(\"Terrible! This model misses all the spam SMS.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nvectorizer.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_vec = vectorizer.transform(x_train).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_tdif = svm.SVC(gamma='scale')\nmodel_tdif.fit(x_train_vec, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_vec = vectorizer.transform(x_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_tdif = model_tdif.predict(x_test_vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_tdif))\n\nm_confusion_test = confusion_matrix(y_test, y_pred_tdif)\ndisplay(pd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1']))\n\nprint(\"This model misclassifies {c} genuine SMS as spam and misses only {d} SPAM.\".format(c = m_confusion_test[0,1], d = m_confusion_test[1,0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a significantly higher recall value and fewer false positives, the TF-IDF based vectorization is a more superior choice as compared to word embeddings. Since the corpus has a lot of shorthand and mispelt words, the word embeddings rendered do not hold any analogical meanings (the main reason why we use embeddings)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}