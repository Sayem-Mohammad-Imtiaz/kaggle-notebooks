{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset\ndataset = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndataset.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of data\ndataset['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is evenly distributed. YAY!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the text\nfrom nltk.tokenize.toktok import ToktokTokenizer\ntokenizer = ToktokTokenizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to clean the data thoroughly because we need to remove some HTML code snippets, stopwords, etc.\n\nLet's start by removing the HTML code snippets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import BeautifulSoup which helps us act with html text\nfrom bs4 import BeautifulSoup\ndef html_remove(review):\n    return BeautifulSoup(review, 'html.parser').get_text()\n\ndataset['review'] = dataset['review'].apply(html_remove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should now remove punctuations and other special characters that do not contribute to the sentiment. Non alpha-numeric characters will be removed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required library\nimport re\ndef non_alpha_numeric_remove(review):\n    return re.sub(pattern = '[^a-zA-Z0-9]', repl = ' ', string = review)\ndataset['review'] = dataset['review'].apply(non_alpha_numeric_remove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should now stem each word. Stemming words is nothing but converting each word into its root form.\nFor example, walking is changed to walk. Loved is changed to love. Humanity is changed to man, etc.\n\nThis makes life easier for us since we do not have to deal with many forms of the same word. We shall be using the Porter Stemmer class from the nltk library to achieve this","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the required library\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\ndef stemming(review):\n    rev = ' '.join([ps.stem(word) for word in review.split()])\n    return rev\ndataset['review'] = dataset['review'].apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now remove stopwords. Stopwords are words in the English language that do not contribute to the overall sentiment of a review. A list of stopwords follow. To help, let us first change everything to lower case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lower(review):\n    return review.lower()\ndataset['review'] = dataset['review'].apply(to_lower)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing stopwords from nltk\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))\ndef remove_stopwords(review):\n    tokens = tokenizer.tokenize(review)\n    tokens = [token.strip() for token in tokens]\n    new_tokens = [token for token in tokens if token not in stopwords.words('english')]\n    new_review = ' '.join(new_tokens)\n    return new_review\ndataset['review'] = dataset['review'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_csv('Cleaned dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now split the dataset into training and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dataset.iloc[:40000, 0].values\nX_test = dataset.iloc[40000:, 0].values\ny_train = dataset.iloc[:40000, 1].values\ny_test = dataset.iloc[40000:, 1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now create the bag of words model to train our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(binary = False, ngram_range = (1,3))\nX_train = cv.fit_transform(X_train)\nX_test = cv.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since our dataset has only two possible values for sentiment, we can use label encoder without having to worry too much. However, if multiple possibilites were present, we should have used one hot encoding or multilabel classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try out various classification models to fit to the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us create a Logistic Regression model first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training our model using Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 500, random_state = 0)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now predict the test values using our Logistic Regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction using our model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nlr_predictions = lr.predict(X_test)\nprint('Accuracy of Logistic Regression is: ', accuracy_score(y_test, lr_predictions) * 100)\nprint(classification_report(y_test,lr_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(9,9))\nsns.heatmap(confusion_matrix(y_test, lr_predictions), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'YlGnBu');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, lr_predictions) * 100)\nplt.title(all_sample_title, size = 15);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now try to create a multinomial Naive Bayes model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training our model using Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now predict the test values using our Multinomial Naive Bayes model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction using our model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nmnb_predictions = mnb.predict(X_test)\nprint('Accuracy of Multinomial Naive Bayes is: ', accuracy_score(y_test, mnb_predictions) * 100)\nprint(classification_report(y_test, mnb_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(9,9))\nsns.heatmap(confusion_matrix(y_test, mnb_predictions), annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'YlGnBu');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, mnb_predictions) * 100)\nplt.title(all_sample_title, size = 15);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}