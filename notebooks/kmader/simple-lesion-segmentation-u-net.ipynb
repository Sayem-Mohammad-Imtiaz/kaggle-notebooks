{"cells":[{"metadata":{"_uuid":"68e9b0d74db8df5421496fe3b337dae765b1ccdf"},"cell_type":"markdown","source":"# Simple U-Net\nThe notebook here just shows how to go about building a simple U-Net model and training it on some of the lesions. Given the subset of the data and looking at only one slice of a number of different lesions we don't expect fantastic performance, but it could serve as a simple baseline for comparing more complicated, pre-trained, object detection models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\nfrom glob import glob\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport zipfile as zf\nimport numpy as np\nimport h5py\nfrom keras.utils.io_utils import HDF5Matrix\nbase_h5_dir = '../input/deeplesion-overview/'\nbase_img_dir = '../input/nih-deeplesion-subset/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bade713866a22b518875348a9a129b0968b305a3"},"cell_type":"markdown","source":"## Load prepackaged image data\nHere we load the packaged image data from the hdf5 file and split it into training and testing groups"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"h5_path = os.path.join(base_h5_dir, 'deeplesion.h5')\nwith h5py.File(h5_path, 'r') as h:\n    print(list(h.keys()))\n    for k in ['image', 'mask']:\n        print(k, h[k].shape, h[k].dtype)\n    base_shape = h['image'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d8aa40fd9ab3523557c233c4631e5b1198d7808"},"cell_type":"code","source":"ct_window_func = lambda x: np.clip((x+175.0)/275, -1, 1)\nget_xyf = lambda s, e: (HDF5Matrix(h5_path, 'image', start=s, end=e, normalizer=ct_window_func), \n                       HDF5Matrix(h5_path, 'mask', start=s, end=e),\n                       HDF5Matrix(h5_path, 'file_name', start=s, end=e)\n                      )\ntrain_split = 0.7\ncut_val = int(base_shape[0]*train_split)\ntrain_x, train_y, train_paths = get_xyf(0, cut_val)\ntest_x, test_y, test_paths = get_xyf(cut_val, None)\nprint(train_x.shape, test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb3bc6bbe0a183c650f57217d9930fc164414888"},"cell_type":"code","source":"from skimage.util.montage import montage2d as montage\nt_x, t_y = train_x[:8], train_y[:8]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap = 'gray')\nax2.imshow(montage(t_y[:, :, :, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9175d225342709af87db01c12eb8830e4a992439","collapsed":true},"cell_type":"code","source":"# we dont need full resolution images so we can just use a downsampled version\nclass DownsampledHDF5Matrix(HDF5Matrix):\n    def __init__(self, datapath, dataset, downscale_factor, start=0, end=None, normalizer=None):\n        ds_func = lambda x: x[:, ::downscale_factor, ::downscale_factor, :]\n        ds_norm = ds_func if normalizer is None else lambda x: ds_func(normalizer(x))\n        self.downscale_factor = downscale_factor\n        super(DownsampledHDF5Matrix, self).__init__(datapath, dataset, start=start, end=end, normalizer=ds_norm)\n        t_val = self[0:1]\n        self._base_shape = t_val.shape[1:]\n        self._base_dtype = t_val.dtype\n    \n    @property\n    def shape(self):\n        \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n        # Returns\n            A numpy-style shape tuple. (self.data.shape[1]//self.downscale_factor, self.data.shape[2]//self.downscale_factor, self.data.shape[3])\n        \"\"\"\n        return (self.end - self.start,) + self._base_shape\n    \n    @property\n    def dtype(self):\n        \"\"\"Gets the datatype of the dataset.\n        # Returns\n            A numpy dtype string.\n        \"\"\"\n        return self._base_dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2d3afaab99fd46b342c5ad3160bfb5c2b36719d"},"cell_type":"code","source":"ct_window_func = lambda x: np.clip((x+500.0)/600, -1, 1)\nget_xyf = lambda s, e: (HDF5Matrix(h5_path, 'image', start=s, end=e, normalizer=ct_window_func), \n                       DownsampledHDF5Matrix(h5_path, 'mask', downscale_factor=4, start=s, end=e),\n                       [x.decode() for x in HDF5Matrix(h5_path, 'file_name', start=s, end=e)[:]]\n                      )\ntrain_split = 0.7\ncut_val = int(base_shape[0]*train_split)\ntrain_x, train_y, train_paths = get_xyf(0, cut_val)\ntest_x, test_y, test_paths = get_xyf(cut_val, None)\nprint(test_x.shape, test_y.shape)\nt_x, t_y = train_x[:8], train_y[:8]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap = 'gray')\nax2.imshow(montage(t_y[:, :, :, 0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dec038e209007c3caf4597da91937746d8291873"},"cell_type":"markdown","source":"# Load the Patient Data\nSo we can reference the images later"},{"metadata":{"trusted":true,"_uuid":"4890394aa638847afea5900e9c8e5f0a7b159d2e"},"cell_type":"code","source":"patient_df = pd.read_csv(os.path.join(base_img_dir, 'DL_info.csv'))\npatient_df = patient_df[patient_df['File_name'].isin(train_paths+test_paths)]\npatient_df['training'] = patient_df['File_name'].isin(train_paths)\npatient_df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f1077742ec09d48dc60cd4bf034d519bbbb24b2"},"cell_type":"code","source":"from keras.layers import Input, Activation, Conv2D, MaxPool2D, UpSampling2D, Dropout, concatenate, BatchNormalization, Cropping2D, ZeroPadding2D, SpatialDropout2D\nfrom keras.layers import Conv2DTranspose, Dropout, GaussianNoise\nfrom keras.models import Model\nfrom keras import backend as K\n\ndef up_scale(in_layer):\n    filt_count = in_layer._keras_shape[-1]\n    return Conv2DTranspose(filt_count//2+2, kernel_size = (2,2), strides = (2,2), padding = 'same')(in_layer)\ndef up_scale_fancy(in_layer):\n    return UpSampling2D(size=(2,2))(in_layer)\n\ninput_layer = Input(shape=train_x.shape[1:])\nsp_layer = GaussianNoise(0.1)(input_layer)\nbn_layer = BatchNormalization()(sp_layer)\nc1 = Conv2D(filters=8, kernel_size=(5,5), activation='relu', padding='same')(bn_layer)\nl = MaxPool2D(strides=(2,2))(c1)\nc2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = MaxPool2D(strides=(2,2))(c2)\nc3 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\n\nl = MaxPool2D(strides=(2,2))(c3)\nc4 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\n\nl = SpatialDropout2D(0.25)(c4)\ndil_layers = [l]\nfor i in [2, 4, 6, 8, 12, 18, 24]:\n    dil_layers += [Conv2D(16,\n                          kernel_size = (3, 3), \n                          dilation_rate = (i, i), \n                          padding = 'same',\n                         activation = 'relu')(l)]\nl = concatenate(dil_layers)\n\nl = SpatialDropout2D(0.2)(concatenate([up_scale(l), c3], axis=-1))\nl = Conv2D(filters=128, kernel_size=(2,2), activation='linear', padding='same')(l)\nl = BatchNormalization()(l)\nl = Activation('relu')(l)\nl = Conv2D(filters=96, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = Conv2D(filters=32, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = Conv2D(filters=16, kernel_size=(2,2), activation='linear', padding='same')(l)\nl = Cropping2D((4,4))(l)\nl = BatchNormalization()(l)\nl = Activation('relu')(l)\n\nl = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid')(l)\noutput_layer = ZeroPadding2D((4,4))(l)\n\nseg_model = Model(input_layer, output_layer)\nseg_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba12d3f2d6e53eaf00885c3e01658cb544c4325a"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\ndef dice_p_bce(in_gt, in_pred):\n    return 0.0*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\nseg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7065d5c62e9288a92f5f9c2c8956520815547439"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a50b5e046c8e87178114f2493954ecaee83b611"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 7.5, \n                  width_shift_range = 0.02, \n                  height_shift_range = 0.02, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  brightness_range = [0.9, 1.1],\n                  horizontal_flip = False, \n                  vertical_flip = False,\n                  fill_mode = 'nearest',\n                   data_format = 'channels_last')\n\nimage_gen = ImageDataGenerator(**dg_args)\ndg_args.pop('brightness_range')\nlabel_gen = ImageDataGenerator(**dg_args)\ndef train_gen(batch_size = 16, seed = None):\n    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n    while True:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        batch_count = train_x.shape[0]//batch_size\n        batch_id = np.random.permutation(range(0, train_x.shape[0]-batch_size, batch_size))\n        for c_idx in batch_id:\n            g_x = image_gen.flow(train_x[c_idx:(c_idx+batch_size)], batch_size = batch_size, seed = seed, shuffle=True)\n            g_y = label_gen.flow(train_y[c_idx:(c_idx+batch_size)], batch_size = batch_size, seed = seed, shuffle=True)\n            yield next(g_x)/255.0, next(g_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcdf50cc2d18bcdfd377a7cb71ccb4e04dfef34d"},"cell_type":"code","source":"cur_gen = train_gen(8)\nt_x, t_y = next(cur_gen)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap='gray')\nax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e99ead721bd6f6d40a2c5e2fb47e53a8ffbe453"},"cell_type":"code","source":"batch_size = 16\nloss_history = [seg_model.fit_generator(train_gen(batch_size), \n                             steps_per_epoch=train_x.shape[0]//batch_size, \n                             epochs=40, \n                             validation_data=(test_x, test_y),\n                             callbacks=callbacks_list,\n                            workers=2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90659a6a2cde5568514b9fbfa2b532d9909a320c","collapsed":true},"cell_type":"code","source":"def show_loss(loss_history):\n    epich = np.cumsum(np.concatenate(\n        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n    _ = ax1.plot(epich,\n                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n                 'b-',\n                 epich, np.concatenate(\n            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('Loss')\n\n    _ = ax2.plot(epich, np.concatenate(\n        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n                     'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n    \n    _ = ax3.plot(epich, np.concatenate(\n        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n                     'r-')\n    ax3.legend(['Training', 'Validation'])\n    ax3.set_title('Binary Accuracy (%)')\n    \n    _ = ax4.plot(epich, np.concatenate(\n        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_dice_coef'] for mh in loss_history]),\n                     'r-')\n    ax4.legend(['Training', 'Validation'])\n    ax4.set_title('DICE')\n\nshow_loss(loss_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d964e26b8091691608564855615aa7c131b10893"},"cell_type":"code","source":"seg_model.load_weights(weight_path)\nseg_model.save('seg_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf1a299d8f1cd130905b6f31a0474f869e7c95c"},"cell_type":"code","source":"fig, m_axs = plt.subplots(4,3, figsize = (20, 20))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor idx, (ax1, ax2, ax3) in enumerate(m_axs):\n    ix = test_x[idx:idx+1]\n    iy = test_y[idx:idx+idx+1]\n    p_image = seg_model.predict(ix)\n    ax1.imshow(ix[0,:,:,0], cmap = 'gray')\n    ax1.set_title('Input Image')\n    ax2.imshow(iy[0,:,:,0], vmin = 0, vmax = 1, cmap = 'bone_r' )\n    ax2.set_title('Ground Truth')\n    ax3.imshow(p_image[0,:,:,0], vmin = 0, vmax = 1, cmap = 'viridis' )\n    ax3.set_title('Prediction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff8bf5e373c14b160260047c763c923d9742c7b3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}