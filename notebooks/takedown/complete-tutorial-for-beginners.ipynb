{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hello Fellow kagglers !\n## As the title says this tutorial is for  beginners every part of this kernel summarizes how to get started in ML competitions field and IRL data problems , hope you enjoy this kernel.\n### Let's start !!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading our data , usually done with **Pandas** lib"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"housing = pd.read_csv('../input/housing.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The count for each value in ocean_proximity column."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.ocean_proximity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nhousing.isna().sum().sort_values(ascending=True).plot(kind='barh',figsize=(10,7))#Quick peak into the missing columns values\n#Let's deal with that later on the cleaning part with various methods !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins=50,figsize=(20,15))#The bins parameter is used to custom the number of bins shown on the plots.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_, test_ = train_test_split(housing,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # EDA Time to have a look on our Data\n    One good practice is to do EDA on the full data and creating a copy of it for not harming our test and training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotter = housing.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(10,8))#Figure size\nplt.scatter('longitude','latitude',data=plotter)\nplt.ylabel('Latitudes')\nplt.xlabel('Longitudes')\nplt.title('Geographical plot of Lats/Lons')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### The plot above look like california RIGHT ?![img](https://california.azureedge.net/cdt/CAgovPortal/images/Uploads/menu-living.jpg)"},{"metadata":{},"cell_type":"markdown","source":"> But we don't have a **informative** look on the plot since we need to know the density for each point, let's do a simple modification.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(10,8))#Figure size\nplt.scatter('longitude','latitude',data=plotter,alpha=0.1)\nplt.ylabel('Latitudes')\nplt.xlabel('Longitudes')\nplt.title('Geographical plot of Lats/Lons')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now it's much better , and if we're familiar with Californias map we can see clearly that the high-density areas , namely the Bay Area and all around Los Angeles & San diego\nMore generally our brains can spot patterns visually , but we always need to play around with the vizualisations to make the patterns stands out."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplotter.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n        s=plotter[\"population\"]/100, label=\"population\", figsize=(15,8),\n        c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True,\n    )\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now we can say that the house price is a bit related to the location (e.g close to ocean) and to the density of the population."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix=plotter.corr()\ncorr_matrix.median_house_value.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Checking the correlation between the main features with the Pandas function (Scatter_matrix) wich shows linear correlations between the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import scatter_matrix\nsns.set()\nfeat = ['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(plotter[feat],figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.scatter('median_income','median_house_value',data=plotter,alpha=0.1)\nplt.xlabel('Median income')\nplt.ylabel('Median house value')\nplt.title('Linear correlation Median income/Median House value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **NB:** \nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotter['rooms_per_household']= plotter.total_rooms/housing.households","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotter.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix1=plotter.corr()\ncorr=corr_matrix1.median_house_value.sort_values(ascending=False)\nd= pd.DataFrame({'Column':corr.index,\n                 'Correlation with median_house_value':corr.values})\nd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Not bad haha ! The number of rooms per household is now more informative than the total number of rooms in a district"},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"> Most Machine Learning algorithms cannot work with **missing features**, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n1. Get rid of the corresponding districts.\n2. Get rid of the whole attribute.\n3. Set the values to some value (zero, the mean, the median, etc.)\n\nSince we don't have a lot of data the first option won't be the best , the second one too because we need that feature , the wisest choice could be the median , we can't affect the mean because we have some outliers this will affect our training model."},{"metadata":{},"cell_type":"markdown","source":"> I'm commenting those options just to show you how to do them i won't use them in this tutorial"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotter.dropna(subset=[\"total_bedrooms\"]) # option 1 \n#plotter.drop(\"total_bedrooms\", axis=1) # option 2 \n#median = plotter[\"total_bedrooms\"].median() # option 3 \n#plotter[\"total_bedrooms\"].fillna(median, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Scikit learn have a handy class to compute median , mean... strategies.\n We'll use that !"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer =SimpleImputer(strategy='median')#In this case its better to use the median to replace missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> If we run the code ( imputer.fit(data) ) we'll have an error since the imputer doesn't work on objects, and as shown at the very beginning we have a categorical attribute which is **\"Ocean_proximity\"** so we need to drop that."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_data = plotter.drop('ocean_proximity',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(ft_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.statistics_ #Here's the median of every attribute in our data !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_data.total_bedrooms.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now you can use this “trained” imputer to transform the training set by replacing missing values by the learned medians:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = imputer.transform(ft_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The result is a plain NumPy array containing the transformed features. We want to\nput it back into a Pandas DataFrame, it’s simple:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_transformed = pd.DataFrame(X,columns=ft_data.columns)\nft_transformed.tail() #The missing values in total_bedrooms were replaced by the median value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's handle our categorical data issue"},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_cols = housing.dtypes\nobj_cols[obj_cols=='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(palette='Set2')\nhousing.ocean_proximity.value_counts().sort_values(ascending=True).plot(kind='barh',figsize=(10,7))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In this case i will one hot encode the labels, we got various encoders for categorical objects, label encoding, ordinal encoder..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nlab_encoder = OneHotEncoder()\ncat_house = housing[['ocean_proximity']]\ncat_enc = lab_encoder.fit_transform(cat_house)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> One of the most important transformation step to apply to your data is **Feature Scaling**\n    >Because with some few exceptions, Machine learning algorithm won't perform well since we have different attributes scales, so what we want to do is to scale them , **note that target attribute doesn't have to be scaled**"},{"metadata":{},"cell_type":"markdown","source":"> We have two common ways to get all the attributes to have the same scale\n1. Min-Max Scaling.\n    Many people call it Normalization and its quite simple , values are shifted and rescaled to be in a range of 0 and 1"},{"metadata":{},"cell_type":"markdown","source":"![iz](https://i.imgur.com/FH9LCE6.png)"},{"metadata":{},"cell_type":"markdown","source":"2. Standardization is a bit different, first it substracts the mean value so standardized values always have a zero mean, then it divides by the standard deviation so that the resulting distribution has unit variance, this is how we calculate standard deviation ( Écart Type )"},{"metadata":{},"cell_type":"markdown","source":"![img](https://i.imgur.com/EFlEx48.png)"},{"metadata":{},"cell_type":"markdown","source":"> N is the number of our samples, We sum the Squared difference from mean which means (X(i) - X̅) squared then we have our standard deviation, but dont worry we have a lot of ways compute all this, but it's always good to know what your computing.\nTo compute STD ( standard deviation ) we use numpy , exemple : to compute the STD for the median_income we only have to do this --> np.std(data['median_income'])"},{"metadata":{},"cell_type":"markdown","source":"> ### Anyway as we showed we need a lot of transformations but thanks to scikit learn that provides a **Pipeline** class to help with such transformations link here : [Pipeline doc'](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"},{"metadata":{},"cell_type":"markdown","source":"> # NOW you're ready to go and start training your model on the train set and test it's accuracy on the test set that we created with the train_test_split function !"},{"metadata":{},"cell_type":"markdown","source":"## Thank you for reading !\n    If you found this helpful an upvote would be very much appreciated "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}