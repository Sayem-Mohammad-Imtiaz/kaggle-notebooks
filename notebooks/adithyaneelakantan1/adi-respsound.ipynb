{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T05:36:53.326734Z","iopub.execute_input":"2021-07-27T05:36:53.327207Z","iopub.status.idle":"2021-07-27T05:36:56.352211Z","shell.execute_reply.started":"2021-07-27T05:36:53.327172Z","shell.execute_reply":"2021-07-27T05:36:56.350883Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#libraries\n\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\n\nimport soundfile as sf\nimport librosa.display\nfrom os import listdir\nfrom os.path import isfile, join\nfrom tensorflow.keras.utils import plot_model,to_categorical\nfrom IPython.display import Audio\nfrom scipy.io import wavfile\nfrom pydub import AudioSegment\n\nimport IPython\nfrom IPython.display import Audio, Javascript\nfrom scipy.io import wavfile\nfrom base64 import b64decode\nfrom pydub import AudioSegment","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:36:56.354186Z","iopub.execute_input":"2021-07-27T05:36:56.354482Z","iopub.status.idle":"2021-07-27T05:37:04.807668Z","shell.execute_reply.started":"2021-07-27T05:36:56.354455Z","shell.execute_reply":"2021-07-27T05:37:04.80619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diagnosis_df = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv', names=['Patient ID', 'Diagnosis'])\ndiagnosis_df['basicDiagnosis'] = diagnosis_df['Diagnosis'].apply(lambda x: 'Healthy' if x =='Healthy'  else 'Unhealthy')\ndiagnosis_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:04.809973Z","iopub.execute_input":"2021-07-27T05:37:04.810342Z","iopub.status.idle":"2021-07-27T05:37:04.864435Z","shell.execute_reply.started":"2021-07-27T05:37:04.810302Z","shell.execute_reply":"2021-07-27T05:37:04.863385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_diagnosis = pd.read_csv('../input/respiratory-sound-database/demographic_info.txt', names = \n                 ['Patient ID', 'Age', 'Gender' , 'BMI (kg/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n                 delimiter = ' ')\ndf_no_diagnosis.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:04.86675Z","iopub.execute_input":"2021-07-27T05:37:04.867113Z","iopub.status.idle":"2021-07-27T05:37:04.894489Z","shell.execute_reply.started":"2021-07-27T05:37:04.867076Z","shell.execute_reply":"2021-07-27T05:37:04.893638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  df_no_diagnosis.join(diagnosis_df.set_index('Patient ID'), on = 'Patient ID', how = 'left')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:04.89608Z","iopub.execute_input":"2021-07-27T05:37:04.896673Z","iopub.status.idle":"2021-07-27T05:37:04.929715Z","shell.execute_reply.started":"2021-07-27T05:37:04.896631Z","shell.execute_reply":"2021-07-27T05:37:04.928698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root= '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\n\nfilenames=[s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\n\n#getting data based on Tr,Al,Ar...\ndef extract_annotation_data(file_name,root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns=['Patient ID', 'Index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names=['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)\n\ni_list=[]\nrec_annotations=[]\nrec_annotations_dict={}\n\nfor s in filenames:\n    \n    (i,a)=extract_annotation_data(s,root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s]=a\n    \nrecording_info=pd.concat(i_list,axis=0)\nrecording_info.head(10) #default is 5 for some reason ???","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:04.931137Z","iopub.execute_input":"2021-07-27T05:37:04.931476Z","iopub.status.idle":"2021-07-27T05:37:12.646729Z","shell.execute_reply.started":"2021-07-27T05:37:04.931441Z","shell.execute_reply":"2021-07-27T05:37:12.645468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(rec_annotations_dict) #????? is it number of words in the annotations thing?","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.648201Z","iopub.execute_input":"2021-07-27T05:37:12.648531Z","iopub.status.idle":"2021-07-27T05:37:12.655776Z","shell.execute_reply.started":"2021-07-27T05:37:12.648499Z","shell.execute_reply":"2021-07-27T05:37:12.654405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def slice_audio(audiodata,samplerate,start,end):\n     start = samplerate * start \n     end   = samplerate * end\n     return audiodata[start:end]\n    \n#didnt give o/p because it just does it behind scenes--","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.658782Z","iopub.execute_input":"2021-07-27T05:37:12.659122Z","iopub.status.idle":"2021-07-27T05:37:12.666699Z","shell.execute_reply.started":"2021-07-27T05:37:12.659093Z","shell.execute_reply":"2021-07-27T05:37:12.665831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Diagnosis():\n    def __init__ (self, id, diagnosis, file_path):\n        self.id = id\n        self.diagnosis = diagnosis \n        self.file_path = file_path\n        \n#this doesnt give o/p either--","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.668234Z","iopub.execute_input":"2021-07-27T05:37:12.668731Z","iopub.status.idle":"2021-07-27T05:37:12.681341Z","shell.execute_reply.started":"2021-07-27T05:37:12.668696Z","shell.execute_reply":"2021-07-27T05:37:12.679826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_wav_files():\n    audio_path = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\n    files = [f for f in listdir(audio_path) if isfile(join(audio_path, f))]  \n            #Gets all files in directory\n    wav_files = [f for f in files if f.endswith('.wav')]  \n            #to get .wav files it searches under this thing\n    wav_files = sorted(wav_files)\n    return wav_files, audio_path","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.683015Z","iopub.execute_input":"2021-07-27T05:37:12.683346Z","iopub.status.idle":"2021-07-27T05:37:12.69651Z","shell.execute_reply.started":"2021-07-27T05:37:12.68331Z","shell.execute_reply":"2021-07-27T05:37:12.695351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def diagnosis_data():\n    diagnosis = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv')\n    wav_files, audio_path = get_wav_files()\n    diag_dict = { 101 : \"URTI\"}  \n    diagnosis_list = []\n  \n    for index , row in diagnosis.iterrows():\n        diag_dict[row[0]] = row[1]     \n\n    c = 0\n    audio_data_list=[]\n    for f in wav_files:\n        diagnosis_list.append(Diagnosis(c, diag_dict[int(f[:3])], audio_path+f))\n        #wav,s_rate = librosa.load(audio_path+f)\n        \n        \n        if diag_dict[int(f[:3])] == \"Healthy\":\n            \n            binary_classification_label=\"Healthy\"\n            \n        else:\n            \n            binary_classification_label=\"UnHealthy\"\n            \n        \n        audio_data_list.append({'id':c,'diagnosis':diag_dict[int(f[:3])],'binary_diagnosis':binary_classification_label,'filename':f,'file_path':audio_path+f})\n        c+=1  \n\n    return diagnosis_list,pd.DataFrame(audio_data_list)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.697952Z","iopub.execute_input":"2021-07-27T05:37:12.698271Z","iopub.status.idle":"2021-07-27T05:37:12.711186Z","shell.execute_reply.started":"2021-07-27T05:37:12.69824Z","shell.execute_reply":"2021-07-27T05:37:12.710269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_obj,dataset_df=diagnosis_data()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:12.712271Z","iopub.execute_input":"2021-07-27T05:37:12.712704Z","iopub.status.idle":"2021-07-27T05:37:13.145826Z","shell.execute_reply.started":"2021-07-27T05:37:12.712673Z","shell.execute_reply":"2021-07-27T05:37:13.144608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset_df['diagnosis'].unique())\nplt.figure(figsize=(10,5))\nsns.countplot(dataset_df['diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.147482Z","iopub.execute_input":"2021-07-27T05:37:13.147949Z","iopub.status.idle":"2021-07-27T05:37:13.390351Z","shell.execute_reply.started":"2021-07-27T05:37:13.147901Z","shell.execute_reply":"2021-07-27T05:37:13.389555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df.to_csv(\"DIagnosis.csv\",index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.391788Z","iopub.execute_input":"2021-07-27T05:37:13.392328Z","iopub.status.idle":"2021-07-27T05:37:13.411801Z","shell.execute_reply.started":"2021-07-27T05:37:13.392282Z","shell.execute_reply":"2021-07-27T05:37:13.410537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_df = dataset_df[ (dataset_df['diagnosis'] !='Asthma') &  (dataset_df['diagnosis'] !='LRTI') ]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.413191Z","iopub.execute_input":"2021-07-27T05:37:13.413483Z","iopub.status.idle":"2021-07-27T05:37:13.426117Z","shell.execute_reply.started":"2021-07-27T05:37:13.413453Z","shell.execute_reply":"2021-07-27T05:37:13.424496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(processed_dataset_df['diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.428059Z","iopub.execute_input":"2021-07-27T05:37:13.428442Z","iopub.status.idle":"2021-07-27T05:37:13.61427Z","shell.execute_reply.started":"2021-07-27T05:37:13.428404Z","shell.execute_reply":"2021-07-27T05:37:13.6131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_dataset_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.615827Z","iopub.execute_input":"2021-07-27T05:37:13.616171Z","iopub.status.idle":"2021-07-27T05:37:13.633059Z","shell.execute_reply.started":"2021-07-27T05:37:13.616134Z","shell.execute_reply":"2021-07-27T05:37:13.631715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_file, samplerate = librosa.core.load(processed_dataset_df.file_path[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:13.636619Z","iopub.execute_input":"2021-07-27T05:37:13.636952Z","iopub.status.idle":"2021-07-27T05:37:15.657099Z","shell.execute_reply.started":"2021-07-27T05:37:13.636921Z","shell.execute_reply":"2021-07-27T05:37:15.655923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for the training part----------\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import wavfile\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nimport argparse\nimport warnings\nimport wavio\nfrom librosa.core import resample, to_mono\n\n# Dependencies(?>)\nimport numpy as np \nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nfrom tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning basic stuff\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Input, LSTM, Dense, TimeDistributed, Activation, BatchNormalization, Dropout, Bidirectional\nfrom keras.models import Sequential\nfrom keras.utils import Sequence\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:15.658997Z","iopub.execute_input":"2021-07-27T05:37:15.659327Z","iopub.status.idle":"2021-07-27T05:37:15.821206Z","shell.execute_reply.started":"2021-07-27T05:37:15.659295Z","shell.execute_reply":"2021-07-27T05:37:15.820228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing parameters\n\nsr = 44100 # Sampling rate\nduration = 10\nhop_length = 347 # To make time steps 128\nfmin = 20\n\nfmax = sr // 2\nn_mels = 128\n\nn_fft = n_mels * 20\nsamples = sr * duration","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:15.822374Z","iopub.execute_input":"2021-07-27T05:37:15.822821Z","iopub.status.idle":"2021-07-27T05:37:15.827509Z","shell.execute_reply.started":"2021-07-27T05:37:15.822788Z","shell.execute_reply":"2021-07-27T05:37:15.826465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample_mono(path, sr):\n    \n    obj = wavio.read(path)\n    wav = obj.data.astype(np.float32, order='F')\n    rate = obj.rate\n    \n    try:\n        channel = wav.shape[1]\n        if channel == 2:\n            wav = to_mono(wav.T)\n        elif channel == 1:\n            wav = to_mono(wav.reshape(-1))\n    except IndexError:\n        wav = to_mono(wav.reshape(-1))\n        pass\n    except Exception as exc:\n        raise exc\n    wav = resample(wav, rate, sr)\n    wav = wav.astype(np.int16)\n    return sr, wav\n\ndef read_audio(path):\n    y, sr = librosa.core.load(path, sr=16000,duration=duration)\n\n    return y\n\ndef audio_to_melspectrogram(audio,s_r,n_mel_val):\n    #to convert to melspectrogram after audio is read in\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr = s_r,\n                                                 n_mels = n_mel_val,\n                                                 hop_length = hop_length,\n                                                 n_fft = n_mel_val*20,\n                                                 fmin = 20,\n                                                 fmax = s_r//2)\n    #dont really get the librosa thing, just get the feature.melspectrogram thing\n    \n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram(path):\n    #to convert audio into a melspectrogram so we can use machine learning\n    mels = audio_to_melspectrogram(read_audio(path))\n    return mels\n\n\ndef audio_to_melspectrogram2(audio,s_r,n_mel_val):\n    #to convert to melspectrogram after audio is read by the librosa thing ??????\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=s_r,\n                                                 n_mels=n_mel_val,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_mel_val*20,\n                                                 fmin=20,\n                                                 fmax=s_r//2)\n    \n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram2(path):\n    #to convert audio into a melspectrogram so we can use ml\n    mels = audio_to_melspectrogram2(read_audio(path))\n    return mels\n\ndef convert_wav_to_image(df):\n    \n    X_mel_spec = []\n    X_mel_spec2 = []\n    for _,row in tqdm(df.iterrows()):\n        x_mel1 = read_as_melspectrogram(row.file_path)\n        x_mel2 = read_as_melspectrogram2(row.file_path)\n        X_mel_spec.append(x_mel1.transpose(),)\n        X_mel_spec2.append(x_mel2.transpose())\n\n        \n    return X_mel_spec,X_mel_spec2\n\n\ndef convert_wav_to_mfcc(df):\n\n    X_mfcc = []\n    \n    for _,row in tqdm(df.iterrows()):\n\n        x_mfcc =generate_mfcc_feature(row.file_path)\n        X_mfcc.append(x_mfcc.transpose())\n        \n    return X_mfcc\n\ndef normalize(img):\n    #to normalize an array (subtract mean and divide by sd-- sigma)\n    eps = 0.001\n    if np.std(img) != 0:\n        img = (img - np.mean(img)) / np.std(img)\n    else:\n        img = (img - np.mean(img)) / eps\n    return img\n\ndef normalize_dataset(X):\n    #Normalizes list of arrays (subtract mean and divide by sd)\n    normalized_dataset = []\n    for img in X:\n        normalized = normalize(img)\n        normalized_dataset.append(normalized)\n    return normalized_dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:15.829007Z","iopub.execute_input":"2021-07-27T05:37:15.829424Z","iopub.status.idle":"2021-07-27T05:37:15.849538Z","shell.execute_reply.started":"2021-07-27T05:37:15.829393Z","shell.execute_reply":"2021-07-27T05:37:15.848462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nfrom typing import Tuple\nimport librosa\nimport numpy as np\n\ndef get_audio(file_path: str) -> Tuple[np.ndarray, int]:\n    audio_data, sr = librosa.core.load(file_path)\n    length = len(audio_data) / sr\n\n    return audio_data, sr, length\n\n#all the class stuff here\n\nclass AudioMovingWindowPreProcessor():\n\n    def __init__(self) -> None:\n        pass\n\n    def get_audio_windows(self, audio: np.ndarray, sr: int, length: int, window_size: int, stride: int) -> np.ndarray:\n        \n        #to generate audio frames using  sliding window with stride -memory safe\n        \n        no_frames = int((length - window_size) / stride) + 1\n        window_size = window_size * sr\n        stride = stride * sr\n\n        audio_frames = []\n\n        for index in range(no_frames):\n\n            if (stride * index + window_size) < len(audio):\n                frame = audio[stride * index:(stride * index + window_size)]\n                audio_frames.append(frame)\n\n            else:\n                break\n\n        return np.array(audio_frames)\n\n    def get_audio_windows_numpy_vectorized(self, audio: np.ndarray, sr: int, length: int, window_size: int,\n                                           stride: int) -> np.ndarray:\n        \n        #to generate audio frames using sliding window with stride Numpy - non-memory safe\n        \n        no_frames = int((length - window_size) / stride) + 1\n        audio_frames = as_strided(audio, shape=(no_frames, window_size * sr), strides=(stride * sr, 1))\n\n        audio_frames = audio_frames[:-2]\n\n        return audio_frames","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:15.850834Z","iopub.execute_input":"2021-07-27T05:37:15.85132Z","iopub.status.idle":"2021-07-27T05:37:15.8721Z","shell.execute_reply.started":"2021-07-27T05:37:15.851285Z","shell.execute_reply":"2021-07-27T05:37:15.871026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\n\nfrom skimage.restoration import denoise_wavelet\n\n#for wavelet ( denoising )\n\nclass Wavelet_Filter:\n\n    def wavelet_filter(self, filteredSignal, samplerate):\n        x_den = denoise_wavelet(filteredSignal, method='VisuShrink', mode='soft', wavelet_levels=5, wavelet='coif2',\n                                rescale_sigma='True')\n\n        return x_den, samplerate\n\n#to pass through high_pass\nclass Filter_BW_HP:\n\n    def __init__(self, high_pass):\n        self.high_pass = high_pass\n\n    def BW_highpass(self, newdata, samplerate):\n        b, a = signal.butter(4, 100 / (22050 / 2), btype='highpass')\n\n        filteredSignal = signal.lfilter(b, a, newdata)\n\n        return filteredSignal, samplerate\n\n#to pass through low_pass\nclass FIlter_BW_LP:\n\n    def __init__(self, low_pass):\n        self.low_pass = low_pass\n\n    def BW_lowpass(self, filteredSignal, samplerate):\n        c, d = signal.butter(4, 2000 / (22050 / 2), btype='lowpass')\n        newFilteredSignal = signal.lfilter(c, d, filteredSignal)\n\n        return newFilteredSignal, samplerate\n\n\nclass FilterPipeline():\n\n    def __init__(self, low_pass, high_pass):\n\n        self.low_pass = low_pass\n        self.high_pass = high_pass\n\n        self.lp_filter = FIlter_BW_LP(low_pass)\n        self.hp_filter = Filter_BW_HP(high_pass)\n        self.wavelet = Wavelet_Filter()\n\n    def filters(self, audio_signal, sample_rate):\n        filtered_output, sr = self.lp_filter.BW_lowpass(audio_signal, sample_rate)\n        filtered_output, sr = self.hp_filter.BW_highpass(filtered_output, sr)\n        filtered_output, sr = self.wavelet.wavelet_filter(filtered_output, sr)\n\n        return filtered_output, sr","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:15.876648Z","iopub.execute_input":"2021-07-27T05:37:15.877172Z","iopub.status.idle":"2021-07-27T05:37:16.639463Z","shell.execute_reply.started":"2021-07-27T05:37:15.877136Z","shell.execute_reply":"2021-07-27T05:37:16.638422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\n\nimport librosa\nimport numpy as np\nfrom typing import Tuple\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Filter Configurations\nLOW_PASS_FREQUENCY = 100\nHIGH_PASS_FREQUENCY = 2000\n\n# Mel-Spectral Configurations\nHOG_LENGTH = 347\n\n# sizes (in sec)\nMOVING_WINDOW_SIZE = 5\nAUDIO_STRIDE_SIZE = 5\n\n\nclass AudioPreProcessor(AudioMovingWindowPreProcessor, FilterPipeline):\n\n    def __init__(self):\n        AudioMovingWindowPreProcessor.__init__(self)\n        FilterPipeline.__init__(self, LOW_PASS_FREQUENCY, HIGH_PASS_FREQUENCY)\n        super().__init__()\n\n    def pre_process_audio(self, audio_path: str, sample_rate: int) -> np.ndarray:\n        \"\"\"\n\n        for this function to generate audio frames using moving window\n\n        just use Parameters\n        ----------\n        audio_path\n        sample_rate\n\n        Returns audio_frames\n        -------\n\n        \"\"\"\n        audio_data, sample_rate, length = get_audio(audio_path, sample_rate)\n        filtered_audio, sample_rate = self.filters(audio_data, sample_rate)\n        audio_frames = self.get_audio_windows(filtered_audio, sample_rate, length, MOVING_WINDOW_SIZE,\n                                              AUDIO_STRIDE_SIZE)\n\n        return audio_frames\n\n    def audio_to_mel_spectrogram(self, audio_data: np.ndarray, s_r: int, n_mel_val: int) -> np.ndarray:\n        \"\"\"\n        to convert audio_frames into mel-spectrogram\n\n        Parameters\n        ----------\n        s_r\n        n_mel_val\n\n        Returns mel-spectralgrams\n        -------\n\n        \"\"\"\n        spectrogram = librosa.feature.melspectrogram(audio_data,\n                                                     sr=s_r,\n                                                     n_mels=n_mel_val,\n                                                     hop_length=HOG_LENGTH,\n                                                     n_fft=n_mel_val * 20,\n                                                     fmin=20,\n                                                     fmax=s_r // 2)\n\n        return librosa.power_to_db(spectrogram).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:16.641244Z","iopub.execute_input":"2021-07-27T05:37:16.641725Z","iopub.status.idle":"2021-07-27T05:37:16.653455Z","shell.execute_reply.started":"2021-07-27T05:37:16.641689Z","shell.execute_reply":"2021-07-27T05:37:16.652268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sizes (in sec)\nMOVING_WINDOW_SIZE = 5\nAUDIO_STRIDE_SIZE = 4\nSAMPLE_RATE=16000\n\nfrom tqdm import tqdm\n# !mkdir breathcycles (dont really get this but sure)\n\nX_mel_spec = []\nX_mel_spec2 = []\nY_labels = []\n\ndurations_spec=[]\n\nfor  indx, df_row in tqdm( processed_dataset_df.iterrows() ):\n    \n    \n     head, audio_file = os.path.split(df_row.file_path)\n     audio_file_name = audio_file.split('.wav')[0]   \n     diagnosis_type =df_row.diagnosis\n    \n     breathing_cycles_df = rec_annotations_dict.get(audio_file_name)\n\n     audio_data, sample_rate,length = get_audio(df_row.file_path)\n\n        \n        \n     no_frames = int((length - MOVING_WINDOW_SIZE) / AUDIO_STRIDE_SIZE) + 1\n     window_size = MOVING_WINDOW_SIZE * sample_rate\n     stride = AUDIO_STRIDE_SIZE * sample_rate\n\n     audio_frames = []\n\n     for index in range(no_frames):\n\n            if (stride * index + window_size) < len(audio_data):\n                \n               frame = audio_data[stride * index:(stride * index + window_size)]\n               x_mel1 = audio_to_melspectrogram(frame,s_r=sample_rate,n_mel_val=128)\n               x_mel2 = audio_to_melspectrogram2(frame,s_r=sample_rate,n_mel_val=64)\n               X_mel_spec.append(x_mel1.transpose(),)\n               X_mel_spec2.append(x_mel2.transpose())\n            \n               Y_labels.append(str(diagnosis_type))\n\n            else:\n                break\n#has output,, shows [time, time/iteration]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:37:16.654907Z","iopub.execute_input":"2021-07-27T05:37:16.655394Z","iopub.status.idle":"2021-07-27T05:54:12.9466Z","shell.execute_reply.started":"2021-07-27T05:37:16.655359Z","shell.execute_reply":"2021-07-27T05:54:12.945018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_mel1 = np.array(X_mel_spec)\nX_mel2 = np.array(X_mel_spec2)\n\n#X = normalize_dataset(X)\nX_array_mel1=np.array(X_mel1)\nX_array_mel2=np.array(X_mel2)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:12.953638Z","iopub.execute_input":"2021-07-27T05:54:12.9569Z","iopub.status.idle":"2021-07-27T05:54:14.623109Z","shell.execute_reply.started":"2021-07-27T05:54:12.956815Z","shell.execute_reply":"2021-07-27T05:54:14.622119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_spae=[]\nfor x in X_mel1:\n    #print(x.shape[0])-------\n    val_spae.append(x.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:14.62458Z","iopub.execute_input":"2021-07-27T05:54:14.625099Z","iopub.status.idle":"2021-07-27T05:54:14.632021Z","shell.execute_reply.started":"2021-07-27T05:54:14.625065Z","shell.execute_reply":"2021-07-27T05:54:14.631174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Average(lst):\n    return sum(lst) / len(lst)\n  \n# Driver Code\n\naverage = Average(val_spae)\n# to print average of the list\nprint(\"Average of the required list =\", round(average, 2))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:14.633389Z","iopub.execute_input":"2021-07-27T05:54:14.633903Z","iopub.status.idle":"2021-07-27T05:54:14.647943Z","shell.execute_reply.started":"2021-07-27T05:54:14.633856Z","shell.execute_reply":"2021-07-27T05:54:14.6466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(val_spae)\n#works till here","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:14.649488Z","iopub.execute_input":"2021-07-27T05:54:14.649833Z","iopub.status.idle":"2021-07-27T05:54:14.960728Z","shell.execute_reply.started":"2021-07-27T05:54:14.649803Z","shell.execute_reply":"2021-07-27T05:54:14.959752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_audio_lenght = max([len(i) for i in X_array_mel1])\n\nmax_audio_lenght2 = max([len(i) for i in X_array_mel2])\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#raw data processing-padding\nX_array_mel1=pad_sequences(X_array_mel1, padding=\"post\", dtype='float32')\nX_array_mel2=pad_sequences(X_array_mel2, padding=\"post\", dtype='float32')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:14.96211Z","iopub.execute_input":"2021-07-27T05:54:14.962579Z","iopub.status.idle":"2021-07-27T05:54:15.815211Z","shell.execute_reply.started":"2021-07-27T05:54:14.962502Z","shell.execute_reply":"2021-07-27T05:54:15.813978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_array_mel1[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:15.817043Z","iopub.execute_input":"2021-07-27T05:54:15.817532Z","iopub.status.idle":"2021-07-27T05:54:15.824117Z","shell.execute_reply.started":"2021-07-27T05:54:15.817486Z","shell.execute_reply":"2021-07-27T05:54:15.823117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize an melspectogram example\nplt.figure(figsize=(15,10))\nplt.title('Visualization of audio file', weight='bold')\nplt.imshow(X_array_mel1[20]);","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:15.825515Z","iopub.execute_input":"2021-07-27T05:54:15.826014Z","iopub.status.idle":"2021-07-27T05:54:16.059117Z","shell.execute_reply.started":"2021-07-27T05:54:15.825972Z","shell.execute_reply":"2021-07-27T05:54:16.057987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_array_mel1.shape[0]\n#am i supposed to get a muchhh lower value? ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:16.060677Z","iopub.execute_input":"2021-07-27T05:54:16.06098Z","iopub.status.idle":"2021-07-27T05:54:16.066963Z","shell.execute_reply.started":"2021-07-27T05:54:16.060947Z","shell.execute_reply":"2021-07-27T05:54:16.066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape_lenght=X_array_mel1.shape[0]\nnew_Xtrain = np.empty((shape_lenght,2), dtype=np.object)\nfor idx in range(shape_lenght):\n    \n    new_Xtrain[idx,]=([X_array_mel1[idx],X_array_mel2[idx]])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:16.068343Z","iopub.execute_input":"2021-07-27T05:54:16.068746Z","iopub.status.idle":"2021-07-27T05:54:16.092128Z","shell.execute_reply.started":"2021-07-27T05:54:16.068702Z","shell.execute_reply":"2021-07-27T05:54:16.090823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras \nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.layers import LSTM, Dense,TimeDistributed, LayerNormalization,Masking\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n#!pip install kapre\n#import kapre\n#from kapre.composed import get_melspectrogram_layer\nimport tensorflow as tf\nimport os\n\n!pip install -U tensorflow-addons\n!pip install -q \"tqdm>=4.36.1\"\n\nimport tensorflow_addons as tfa\nfrom tqdm import tqdm\nfrom tqdm.keras import TqdmCallback","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:54:16.093476Z","iopub.execute_input":"2021-07-27T05:54:16.094006Z","iopub.status.idle":"2021-07-27T05:57:19.380032Z","shell.execute_reply.started":"2021-07-27T05:54:16.093952Z","shell.execute_reply":"2021-07-27T05:57:19.378922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\nle = LabelEncoder()\ndiagnosis_classes= list(set(Y_labels))\nle.fit(diagnosis_classes)\ny = le.transform(Y_labels)\nY=to_categorical(y, num_classes=len(diagnosis_classes))\nnp.save('diagnosis_classes.npy', le.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:57:19.381438Z","iopub.execute_input":"2021-07-27T05:57:19.381752Z","iopub.status.idle":"2021-07-27T05:57:19.397336Z","shell.execute_reply.started":"2021-07-27T05:57:19.381713Z","shell.execute_reply":"2021-07-27T05:57:19.395976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_time_span=new_Xtrain[0][0].shape[0]\nno_mel_sp_features =new_Xtrain[0][0].shape[1]\nNO_CLASSES=6 # Binary classification -> 2","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:57:19.398925Z","iopub.execute_input":"2021-07-27T05:57:19.399263Z","iopub.status.idle":"2021-07-27T05:57:19.403934Z","shell.execute_reply.started":"2021-07-27T05:57:19.399229Z","shell.execute_reply":"2021-07-27T05:57:19.403061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LSTM_Model_Dual(N_CLASSES,max_audio_length,no_features):\n    \n    i1=Input(shape=(max_audio_length,no_features), dtype=\"float32\",name='mel-spectral-input')\n    x1 = LayerNormalization(axis=2, name='batch_norm')(i1)\n    x1 = TimeDistributed(layers.Reshape((-1,)), name='reshape')(x1)\n    s1 = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh')(x1)\n    \n    #masking\n    x1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True),name='LSTM_1-128')(s1)\n    x1 = layers.Bidirectional(layers.LSTM(256, return_sequences=True),name='LSTM_1-256')(x1)\n    x1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True),name='LSTM_1-64')(x1)\n    #x1 = layers.Bidirectional(layers.LSTM(64),name='LSTM_2-64')(x1)\n    \n    \n    x1 = layers.concatenate([s1, x1], axis=2, name='skip_connection')\n    x1 = layers.Dense(64, activation='relu', name='dense_1_relu')(x1)\n    x1 = layers.MaxPooling1D(name='max_pool_1d')(x1)\n    x1 = layers.Dense(32, activation='relu', name='dense_2_relu')(x1)\n    x1 = layers.Flatten(name='flatten')(x1)\n    x1 = layers.Dropout(rate=0.2, name='dropout')(x1)\n    x1 = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu')(x1)\n    \n    \n    i2=Input(shape=(max_audio_length,64), dtype=\"float32\",name='mel-spectral2-input')\n    x2 = LayerNormalization(axis=2, name='batch_norm_mel-spectral2')(i2)\n    x2 = TimeDistributed(layers.Reshape((-1,)), name='reshape_mel-spectral2')(x2)\n    s2 = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh_mel-spectral2')(x2)\n    \n    #masking\n    x2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True),name='mel-2-LSTM_1-128')(s2)\n    x2 = layers.Bidirectional(layers.LSTM(256, return_sequences=True),name='mel-2-LSTM_1-256')(x2)\n    x2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True),name='mel-2-LSTM_1-64')(x2)\n    \n    x2 = layers.concatenate([s2, x2], axis=2, name='skip_connection_mel-spectral2')\n    x2 = layers.Dense(64, activation='relu', name='dense_1_relu_mel-spectral2')(x2)\n    x2 = layers.MaxPooling1D(name='max_pool_1d_mfcc')(x2)\n    x2 = layers.Dense(32, activation='relu', name='dense_2_relu_mel-spectral2')(x2)\n    x2 = layers.Flatten(name='flatten_mel-spectral2')(x2)\n    x2 = layers.Dropout(rate=0.2, name='dropout_mel-spectral2')(x2)\n    x2 = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu_mel-spectral2')(x2)\n    \n    \n    x = layers.concatenate([x1, x2], name='mel-mfcc_connection')\n    \n    o = layers.Dense(N_CLASSES, activation='softmax', name='softmax')(x)\n    model = Model(inputs=[i1,i2], outputs=o, name='dual_convolutional_long_short_term_memory')\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = LSTM_Model_Dual(NO_CLASSES,audio_time_span,no_mel_sp_features)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:57:19.405012Z","iopub.execute_input":"2021-07-27T05:57:19.405296Z","iopub.status.idle":"2021-07-27T05:57:23.815057Z","shell.execute_reply.started":"2021-07-27T05:57:19.405267Z","shell.execute_reply":"2021-07-27T05:57:23.813917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LSTM_model_V2(N_CLASSES,max_audio_length,no_features):\n    \n    i=Input(shape=(max_audio_length,no_features), dtype=\"float32\")\n    x = LayerNormalization(axis=2, name='batch_norm')(i)\n    x = TimeDistributed(layers.Reshape((-1,)), name='reshape')(x)\n    s = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh')(x)\n    #masking\n    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True),\n                             name='bidirectional_lstm')(s)\n    x = layers.concatenate([s, x], axis=2, name='skip_connection')\n    x = layers.Dense(64, activation='relu', name='dense_1_relu')(x)\n    x = layers.MaxPooling1D(name='max_pool_1d')(x)\n    x = layers.Dense(32, activation='relu', name='dense_2_relu')(x)\n    x = layers.Flatten(name='flatten')(x)\n    x = layers.Dropout(rate=0.2, name='dropout')(x)\n    x = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu')(x)\n    o = layers.Dense(N_CLASSES, activation='softmax', name='softmax')(x)\n    model = Model(inputs=i, outputs=o, name='long_short_term_memory')\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = LSTM_model_V2(NO_CLASSES,audio_time_span,no_mel_sp_features)\nmodel.summary()\n\n#kinda wobbly on this whole thing","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:57:23.816674Z","iopub.execute_input":"2021-07-27T05:57:23.817005Z","iopub.status.idle":"2021-07-27T05:57:24.435771Z","shell.execute_reply.started":"2021-07-27T05:57:23.816972Z","shell.execute_reply":"2021-07-27T05:57:24.433771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize tqdm callback with default parameters\ntqdm_callback = tfa.callbacks.TQDMProgressBar()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:57:24.438791Z","iopub.execute_input":"2021-07-27T05:57:24.439166Z","iopub.status.idle":"2021-07-27T05:57:24.444413Z","shell.execute_reply.started":"2021-07-27T05:57:24.439132Z","shell.execute_reply":"2021-07-27T05:57:24.443137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold,StratifiedKFold\nnum_folds = 3\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\n#inputs = np.concatenate((X_train, X_val), axis=0)\n#targets = np.concatenate((Y_train, Y_val), axis=0)\n\n# Define the K-fold Cross Validator\nStrftkfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n\nk_fold_history=[]\n\n#Custom callback\ncheckpoint_filepath = 'training_lstm_v2/cp.ckpt'\ncheckpoint_filepath = os.path.dirname(checkpoint_filepath)\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    verbose=0,\n    mode='max',\n    save_best_only=True)\n\nprogressbar=keras.callbacks.ProgbarLogger(count_mode=\"samples\", stateful_metrics=['acc','val_loss'])\n\n#Keras callback\nkeras_callbacks = [model_checkpoint_callback,tqdm_callback]\n\n#Setup Model\n#model = build_lstm_model()\nmodel = LSTM_Model_Dual(NO_CLASSES,audio_time_span,no_mel_sp_features)\nclass_labels = np.argmax(Y, axis=1)\n\n# Loop through the indices the split() method returns\nfor index, (train_indices, val_indices) in enumerate(Strftkfold.split(new_Xtrain, class_labels)):\n\n    # Generate batches from indices\n    xtrain, xval = new_Xtrain[train_indices], new_Xtrain[val_indices]\n    ytrain, yval = Y[train_indices], Y[val_indices]\n    \n    \n    x_mel1= np.empty( ( int(xtrain.shape[0]) ,int(xtrain[0][0].shape[0]) , int(xtrain[0][0].shape[1]) ), dtype=np.float32)\n    x_mel2= np.empty( ( int(xtrain.shape[0]) ,int(xtrain[0][1].shape[0]) , int(xtrain[0][1].shape[1]) ), dtype=np.float32)\n    \n    x_mel1_val= np.empty( ( int(xval.shape[0]) ,int(xval[0][0].shape[0]) , int(xval[0][0].shape[1]) ), dtype=np.float32)\n    x_mel2_val= np.empty( ( int(xval.shape[0]) ,int(xval[0][1].shape[0]) , int(xval[0][1].shape[1]) ), dtype=np.float32)\n    \n    # Extracting 2 features into Numpy array- Training set\n    for i,x in enumerate(xtrain):\n        \n        x_mel1[i,]=x[0]\n        x_mel2[i,]=x[1]\n\n   # Extracting 2 features into Numpy array validation set\n    for i,x in enumerate(xval):\n        \n        x_mel1_val[i,]=x[0]\n        x_mel2_val[i,]=x[1]\n    \n    \n    #create a batch\n\n    #train the model \n    history = model.fit({'mel-spectral-input':x_mel1,'mel-spectral2-input':x_mel2}, ytrain,batch_size=32,epochs=20,verbose=0,callbacks=keras_callbacks,validation_split=0.2)\n    k_fold_history.append(history)\n    \n    # Generate generalization metrics\n    scores = model.evaluate({'mel-spectral-input':x_mel1_val,'mel-spectral2-input':x_mel2_val}, yval, verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%' )\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average scores ==\nprint('------------------------------------------------------------------------')\nprint('Validation Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Val Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Val Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(num_folds, 2,sharex=True,figsize=(30,20))\nfig.suptitle('Cross-Validation Learning Curve')\n\nindex_sub_plot=1\n\nfor history in k_fold_history:\n    \n    axes[index_sub_plot-1,0].set_title('K-fold: {} Acc'.format(index_sub_plot))\n    axes[index_sub_plot-1,0].plot(history.history['accuracy'])\n    axes[index_sub_plot-1,0].plot(history.history['val_accuracy'])\n    axes[index_sub_plot-1,0].set_ylabel('accuracy')\n    axes[index_sub_plot-1,0].set_xlabel('epoch')\n    axes[index_sub_plot-1,0].legend(['train', 'test'], loc='upper left')\n    \n    axes[index_sub_plot-1,1].set_title('K-fold: {} Loss'.format(index_sub_plot))\n    axes[index_sub_plot-1,1].plot(history.history['loss'])\n    axes[index_sub_plot-1,1].plot(history.history['val_loss'])\n    axes[index_sub_plot-1,1].set_ylabel('loss')\n    axes[index_sub_plot-1,1].set_xlabel('epoch')\n    axes[index_sub_plot-1,1].legend(['train', 'test'], loc='upper left')\n    \n    index_sub_plot+=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {checkpoint_filepath}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = 'training_lstm_v2/saved_model.pb' \ncheckpoint_dir = os.path.dirname(checkpoint_path)\noptimal_model = tf.keras.models.load_model(checkpoint_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_model.save(\"my_model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\noptimal_model.save(\"my_h5_model.h5\")\n!commit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nmatrix_index = le.classes_\n\npreds = optimal_model.predict({'mel-spectral-input':x_mel1_val,'mel-spectral2-input':x_mel2_val})\n#preds = optimal_model.predict(x_mel1_val)\nclasspreds = np.argmax(preds, axis=1) # predicted classes \ny_testclass = np.argmax(yval, axis=1) # true classes\n\ncm = confusion_matrix(y_testclass, classpreds)\nprint(classification_report(y_testclass, classpreds, target_names=matrix_index))\n\n# Get percentage value for each element of the matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n# Display confusion matrix \ndf_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nfig, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(df_cm, annot=annot, fmt='')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.classes_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest = pd.DataFrame(preds, columns = le.classes_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftest.to_csv(\"Test_Predictions.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]}]}