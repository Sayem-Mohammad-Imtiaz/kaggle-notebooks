{"cells":[{"metadata":{},"cell_type":"markdown","source":"Regis MSDS Deep Learning Project\n\nThis project is to create a deep learning model using the 100-bird-species data set. In this experiment I try two different methods for analyzing the picture data in an attempt to achieve 80% or greater accuracy.\n\nThe Data:\n\nThe data for this assignment is a living set of image data that is being updated regularly. There are many different images of a wide variety of birds but some of the species only have a few pictures. At the time of completing this project there are 190 species in the dataset. \n\nSummary of Methods:\n\nTo account for some of the bird species not having many pictures I used ImageDataGenerator to create more data for the model. Then I used the ImageDataGenerator to create train, validation, and test data. When compiling the model I used adam optimizer and learning rate optimization. I plotted the accuracy/loss vs epochs.\n\nIn the second model I used transfer learning from the MobileNetV2 model to import weights. I left out the Adam learning rate optimizer but still plotted the accuracy/loss vs epochs. \n\nSummary of Models:\n\nIn the first experiment I created a model using 2d convnets, max pooling and batch normalization layers. I used padding to ensure that the image data did not get to small as it was fed through the network. The data then is flattened then run through a Dense layer, a BatchNormalization, a dropout layer and then the output layer.\n\nNext, I created a transfer learning model using the MobileNetV2. I kept the last four layers of the MobileNetV2 trainable and froze the rest and added some convolutional layers along with BatchNormalization layers.\n\nFinally, I tried a functional API model but it took so long to process the data that kaggle ended up timing out\n\nAnalysis of results:\n\nThe model I created did pretty well with an accuracy of 85.9% but the MobileNetV2 only had an accuracy of 75.2%. I can only guess that this was because I did not include padding in this transfer learning model and the image data was reduced to a size to small for interpretation.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n    \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will import the necessary Librarys to complete the project"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import backend, models, layers, optimizers, regularizers\nfrom tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, SeparableConv2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom IPython.display import display  \nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\nimport os, shutil \nimport random\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.applications import MobileNetV2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will set the seed to 42 for reproducibility\n# Next we bring in the base directory for the data and define the paths\n# we define a training Validation and a test path\n# since this is a living data set we are working with\n## the birdtypes is used so that if the set is updated the program should still work\nnp.random.seed(42)\nbase_dir = '/kaggle/input/100-bird-species'\n\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'valid')\ntest_dir = os.path.join(base_dir, 'test')\nbirdtypes = os.listdir(train_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next we use ImageDataGenerator to create more data from the images we have\n# we create a separate Data Generator for the test\n## data because we do not want to augment this\ntrain_datagen = ImageDataGenerator(rescale=1. / 255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_datagen.flow_from_directory(\n    train_dir,  \n    target_size=(224, 224),\n    batch_size=64,  \n    class_mode= 'categorical')  \n\nvalidation_data = train_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical')\n\ntest_data = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def birds(DIR, dataset):\n    label = []\n    image = []\n    j=0\n    for i in range (0,30):\n        j = random.randint(0, len(dataset.filenames))\n        label.append(dataset.filenames[j].split('/')[0])\n        image.append(DIR + '/' + dataset.filenames[j])\n    return [label,image]\n\ny,x = birds(train_dir, train_data)\n\nfor i in range(0, 16):\n    X = load_img(x[i])\n    plt.subplot(4,4,+1+i)\n    plt.axis(False)\n    plt.title(y[i], fontsize=8)\n    plt.imshow(X)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_epochs', len(train_data)) # might be good to determine steps_per_epoch\nprint('valid_steps', len(validation_data)) # help determine Validation steps\nprint('test_data', len(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backend.clear_session()\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3 ,3), padding='same', activation ='relu', input_shape=(224, 224, 3)))\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Conv2D(32, (3,3), activation ='relu'))\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Conv2D(32, (3,3), padding='same', activation ='relu'))\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Conv2D(64, (3 ,3), activation ='relu'))\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Conv2D(64, (3,3), padding='same', activation ='relu'))\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Conv2D(64, (3,3), padding='same', activation ='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(birdtypes), activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile (optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n               loss='categorical_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model.fit_generator(train_data,\n                    epochs = 40,\n                    steps_per_epoch = 404,\n                    validation_steps= 30,\n                    validation_data = (validation_data),\n                    verbose = 1,\n                    callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)])\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) +1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\ntest_loss, test_acc = model.evaluate_generator(test_data, steps=30)\n\nprint('model_test_acc:', test_acc)\nprint('Doesnt seem to work as well')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see this model has achieved an accuracy of 85.9%. Next, I imported the MobileNetV2 to try a transfer learning experiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"backend.clear_session()\nmobmodel = MobileNetV2(weights='imagenet',\n                         include_top=False,\n                         input_shape=(224, 224, 3))\n\nfor layer in mobmodel.layers[:-4]:\n    layer.trainable = False\nfor layer in mobmodel.layers:\n    print(layer, layer.trainable)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel2 = models.Sequential()\nmodel2.add(mobmodel)\nmodel2.add(layers.Conv2D(64, kernel_size=3, activation ='relu'))\nmodel2.add(BatchNormalization())\n\nmodel2.add(layers.Conv2D(64, kernel_size=3, activation ='relu'))\nmodel2.add(BatchNormalization())\n\nmodel2.add(layers.Conv2D(64, kernel_size=3, activation ='relu'))\nmodel2.add(BatchNormalization())\n\nmodel2.add(layers.Flatten())\nmodel2.add(layers.Dense(512, activation ='relu'))\nmodel2.add(layers.Dense(len(birdtypes), activation='softmax'))\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile (optimizer='adam',\n               loss='categorical_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model2.fit_generator(train_data,\n                    epochs = 40,\n                    steps_per_epoch = 404,\n                    validation_steps= 30,\n                    validation_data = (validation_data),\n                    verbose = 1,\n                    callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)])\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) +1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\ntest_loss, test_acc = model2.evaluate_generator(test_data, steps=30)\n\nprint('model_test_acc:', test_acc)\nprint('Doesnt seem to work as well')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backend.clear_session()\n\n#Visible layer\nvislayer = Input(shape=(224,224,3))\n\n#conv1\nconv11 = SeparableConv2D(32, 3, padding='same', activation='relu')(vislayer)\nconv12 = SeparableConv2D(64, 3,strides=2, padding='same', activation='relu')(conv11)\nconv13 = MaxPooling2D((2,2), padding='same')(conv12)\n\n#Conv2\nconv21 = SeparableConv2D(32, 3, padding='same', activation='relu')(vislayer)\nconv22 = SeparableConv2D(64, 3, padding='same', activation='relu', strides=2)(conv21)\nconv23 = MaxPooling2D((2,2), padding='same')(conv22)\n\n#Conv3\nconv31 = SeparableConv2D(32, 3, activation='relu', padding='same')(vislayer)\nconv32 = SeparableConv2D(64, 3,activation='relu', padding='same', strides=2)(conv31)\nconv33 = MaxPooling2D((2,2), padding='same')(conv32)\n\n#Conv4\nconv42 = SeparableConv2D(32, 3, padding='same', activation='relu')(vislayer)\nconv43 = SeparableConv2D(64, 3, padding='same', activation='relu', strides=2)(conv42)\nconv44 = MaxPooling2D((2,2), padding='same')(conv43)\n\nmerged = Concatenate(axis=-1)([conv13, conv23, conv33, conv44])\nlvl2norm = BatchNormalization()(merged)\n\n#conv5\nconv51 = SeparableConv2D(32, 3, padding='same', activation='relu')(lvl2norm)\nconv52 = SeparableConv2D(64, 3,strides=2, padding='same', activation='relu')(conv51)\nconv53 = MaxPooling2D((2,2), padding='same')(conv52)\n\n#Conv6\nconv61 = SeparableConv2D(32, 3, padding='same', activation='relu')(lvl2norm)\nconv62 = SeparableConv2D(64, 3, padding='same', activation='relu', strides=2)(conv61)\nconv63 = MaxPooling2D((2,2), padding='same')(conv62)\n\n#Conv7\nconv71 = SeparableConv2D(32, 3, activation='relu', padding='same')(lvl2norm)\nconv72 = SeparableConv2D(64, 3,activation='relu', padding='same', strides=2)(conv71)\nconv73 = MaxPooling2D((2,2), padding='same')(conv72)\n\n#Conv8\nconv82 = SeparableConv2D(32, 3, padding='same', activation='relu')(lvl2norm)\nconv83 = SeparableConv2D(64, 3, padding='same', activation='relu', strides=2)(conv82)\nconv84 = MaxPooling2D((2,2), padding='same')(conv83)\n\n#concat\nmerged2 = Concatenate(axis=-1)([conv53, conv63, conv73, conv84])\n\n\n#bnorm = BatchNormalization()(merged)\nconv5 = Conv2D(32, 1, padding='same', activation='relu')(merged2)\nconv6 = Conv2D(64, 3, padding='same', activation='relu')(conv5)\nbnorm2 = BatchNormalization()(conv6)\nflat = Flatten()(bnorm2)\nhidden = Dense(256, activation='relu')(flat)\ndrop = Dropout(0.5)(hidden)\noutput = Dense(len(birdtypes), activation='softmax')(drop)\n\n#Model\nmodel3 = Model(inputs=vislayer, outputs=output)\n\nplot_model(model3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Functional API summary\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.compile (optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n               loss='categorical_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model3.fit_generator(train_data,\n                    epochs = 40,\n                    steps_per_epoch = 404,\n                    validation_steps= 30,\n                    validation_data = (validation_data),\n                    verbose = 1,\n                    callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)])\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) +1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\ntest_loss, test_acc = model3.evaluate_generator(test_data, steps=30)\n\nprint('model_test_acc:', test_acc)\nprint('Doesnt seem to work as well')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}