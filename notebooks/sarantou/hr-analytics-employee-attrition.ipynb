{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://cdn-images-1.medium.com/max/1200/1*Yb0EWWZd1mZUTkdJZOJS-Q.png)"},{"metadata":{},"cell_type":"markdown","source":"# HR Analytics Employee Attrition & Performance\n\n### This kernel features a project for the class \"Special Topics in Information Systems Î™I\" of the Business Administration department, University of Macedonia.* \n### Its purpose is creating a reliable model that predicts **employee attrition**.\n### Knowing the reasons why your company's employees stay or leave, can prepare you in order to adapt to their preferences. This can help you predict their actions and make them stay in the company longer.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Packages\n### We first import all the packages we will need for our kernel"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Importing Model Packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\n\n#Importing Metrics\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nimport gc                         \ngc.enable()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Our Dataset\n### We then take a look into our dataset by importing it and visualizing some of its features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We import the file and view its first 10 rows as a dataframe\n\ndata = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#We then check the kind of information our dataset contains\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We print the number of rows and columns\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Creating a box plot for the age allocation of our employees\nvis1= sns.boxplot(data=data, x='Age', color='Purple')\nplt.xlabel('Employee Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Getting a chart of the number of employees for each gender\nGender= data.groupby('Gender')[['Gender']].count()\nGender.columns= ['Num_of_Empl']\nvis3= sns.barplot(data=Gender, x=Gender.index, y='Num_of_Empl', palette='Set1')\nplt.xticks(range(3),rotation='vertical')\nplt.title('Number of Employees of each Gender ')\nplt.xlabel('Gender')\nplt.ylabel('Number of Employees')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking a look at the distribution among the years in our employees' current roles\nplt.hist(data.YearsInCurrentRole, bins=100)\nplt.title('Years of Employees per Current Role ')\nplt.xlabel('Years In Current Role')\nplt.ylabel('Number of Employees')\nplt.show()\n\nsns.set_style('darkgrid')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Viewing the fields our employees are specialised in\nEducationFields= data.groupby('EducationField')[['EducationField']].count()\nEducationFields.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#The data of our highest paid employees\nhighest_paid= data.sort_values(by='MonthlyIncome', ascending=False)\nhighest_paid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a box plot for the Monthly Income of our employees\nvis2= sns.boxplot(data=data, x='MonthlyIncome', color='Green')\nplt.xlabel('Employee Income')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#We print the unique values of the \"Department\" column\nprint(data.Department.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting a chart of the number of employees for each department\nDepartment= data.groupby('Department')[['Department']].count()\nDepartment.columns= ['Num_of_Empl']\nvis3= sns.barplot(data=Department, x=Department.index, y='Num_of_Empl', palette='Blues')\nplt.xticks(range(3),rotation='vertical')\nplt.title('Number of Employees for each Department ')\nplt.xlabel('Departments')\nplt.ylabel('Number of Employees')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#The amount of employees that stayed and left (No=stayed, Yes=left)\nAttrition_num= data.Attrition.value_counts()\nAttrition_num.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#The percentages of the amounts above\nround(Attrition_num/data.Attrition.count()*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Categorical data transformation\n### We need to transform our string data into numeric figures, in order to help our model understand their value.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We first change the type of the variables below into categorical\n\ndata.Attrition= data.Attrition.astype('category')\ndata.BusinessTravel= data.BusinessTravel.astype('category')\ndata.Gender= data.Gender.astype('category')\ndata.EducationField= data.EducationField.astype('category')\ndata.JobRole= data.JobRole.astype('category')\ndata.MaritalStatus= data.MaritalStatus.astype('category')\ndata.OverTime= data.OverTime.astype('category')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 We then transform our categorical variables using \"pd.get_dummies\".\n#### This creates a separate dataframe containing the unique values of the variable as its columns\n#### The value/column that describes the employee/row takes the number 1 while the rest take the number 0.\n#### We then drop one of the columns in order to avoid a \"dummy trap\" which comes from having too many variables with high relevance between them\n#### Finally, we drop the original column and join the created dataframe to our dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Transforming the Department column\ndepartments= pd.get_dummies(data.Department)\ndepartments.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"demartments= departments.drop('Human Resources', axis=1)\ndata= data.drop(\"Department\", axis=1)\ndata = data.join(departments)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Transforming the EducationField column\neducation_fields= pd.get_dummies(data.EducationField)\neducation_fields= education_fields.drop('Human Resources', axis=1)\ndata= data.drop(\"EducationField\", axis=1)\ndata = data.join(education_fields)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Transforming the JobRole column\njob_roles= pd.get_dummies(data.JobRole)\njobe_roles= job_roles.drop(\"Human Resources\", axis=1)\ndata= data.drop(\"JobRole\", axis=1)\ndata = data.join(jobe_roles)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Transforming the MaritalStatus column\nmarital_status= pd.get_dummies(data.MaritalStatus)\nmarital_status= marital_status.drop(\"Single\", axis=1)\ndata= data.drop(\"MaritalStatus\", axis=1)\ndata= data.join(marital_status)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the Gender column\ngenders= pd.get_dummies(data.Gender)\ngenders= genders.drop(\"Female\", axis=1)\ndata= data.drop(\"Gender\", axis=1)\ngenders.columns= ['Gender']\ndata= data.join(genders)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the OverTime column\novertime= pd.get_dummies(data.OverTime)\novertime= overtime.drop(\"Yes\", axis=1)\ndata= data.drop(\"OverTime\", axis=1)\novertime.columns= ['OverTime']\ndata= data.join(overtime)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the BusinessTravel column\nbusinesstravel= pd.get_dummies(data.BusinessTravel)\nbusinesstravel= businesstravel.drop(\"Travel_Rarely\", axis=1)\ndata= data.drop(\"BusinessTravel\", axis=1)\ndata= data.join(businesstravel)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We drop these 3 following columns as they are of no value to our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the \"Over18\" , \"StandardHours\" and \"EmployeeCount\" columns since all employees share the same values.\ndata= data.drop(\"Over18\", axis=1)\ndata= data.drop(\"StandardHours\", axis=1)\ndata= data.drop(\"EmployeeCount\", axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We transform our target column (y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the Attrition column\nattrition= pd.get_dummies(data.Attrition)\nattrition= attrition.drop(\"Yes\", axis=1)\ndata= data.drop(\"Attrition\", axis=1)\nattrition.columns= ['Attrition']\ndata= data.join(attrition)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Creating predictors set\n### We separate our predictors from our target (y) column and split the data to prepare them for model training."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Setting the target (dependent values) and features (independent values)\n\n# 1. Setting the column \"Attrition\" as target\ntarget = data.Attrition\n\n# 2. Setting evrything else as features\nfeatures = data.drop(\"Attrition\",axis=1)\n\nfeatures.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We decided to focus on the stayers, therefore the employees who left are the 0s and the ones who stayed are the 1s"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The amount of employees that stayed and left (1=stayed, 0=left)\nAttrition_num= data.Attrition.value_counts()\nAttrition_num.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will split the above (target, features) into train and test sets with 70%/30% ratio, respectively\n\ntarget_train, target_test, features_train, features_test = train_test_split(target,features,test_size=0.3,\n                                                                            random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. We begin training our model.\n### The model we will use is the Decision Tree Classifier and we tried to figure out different ways and uses of it, in order to maximize our goal.\n### Since we chose to focus on the stayers, we wish our model to have a high recall score, although our top priority will be the precision score."},{"metadata":{},"cell_type":"markdown","source":"![](https://www.researchgate.net/profile/Fernando_Crespo/publication/292304919/figure/fig13/AS:341406267789324@1458409005603/Confusion-matrix-for-a-two-class-problem-TP-is-the-number-of-correct-predictions-that-an.png)"},{"metadata":{},"cell_type":"markdown","source":"### Recall Score = TP/(TP + FN)\n### Precision Score= TP/(TP + FP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize it and call model by specifying the random_state parameter\nmodel = DecisionTreeClassifier(random_state=42)\n\n# Apply a decision tree model to fit features to the target\nmodel.fit(features_train, target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Check the accuracy score of the prediction for the training set\nmodel.score(features_train,target_train)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the accuracy score of the prediction for the test set\nprint (\"The accuracy is\" ,model.score(features_test,target_test))\n\n# Use the initial model to predict churn\nprediction = model.predict(features_test)\n\n# Calculate recall score by comparing target_test with the prediction\nprint(\"The recall score is\", recall_score(target_test, prediction))\n\n#Print the precision score of the model predictions\nprint( \"The precision score is\", precision_score(target_test, prediction))\n\n# Calculate the f1_score\ny_true= target_test\ny_pred= model.predict(features_test)\nprint(\"The f1_score is\" , f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In order to avoid overfitting (check the training set's accuracy above) we experiment by limiting the Decision Tree's depth.\n#### This can lower our accuracy, but it gives us a more realistic result."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the DecisionTreeClassifier while limiting the depth of the tree to 5\nmodel_depth_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n\n# Fit the model\nmodel_depth_5.fit(features_train,target_train)\n\n# Print the accuracy of the prediction for the training set\nprint(model_depth_5.score(features_train,target_train)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the initial model to predict churn\nprediction_5 = model_depth_5.predict(features_test)\n\n# Print the accuracy of the prediction for the test set\nprint(\"The accuracy is\", model_depth_5.score(features_test,target_test))\n\n# Calculate recall score by comparing target_test with the prediction\nprint(\"The recall score is\", recall_score(target_test, prediction_5))\n\n#Print the precision score of the model predictions\nprint( \"The precision score is\", precision_score(target_test, prediction_5))\n\n# Calculate the f1_score\ny_true= target_test\ny_pred= prediction_5\nprint(\"The f1_score is\" , f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We notice that our f1 score has improved, however our recall score is higher than our precision score. We keep experimenting in order to maximize our precision while keeping a relatively high recall score"},{"metadata":{},"cell_type":"markdown","source":"#### We try to modify the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least 100 training samples in each of the left and right branches. This may have the effect of smoothing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the DecisionTreeClassifier while limiting the sample size in leaves to 100\nmodel_sample_100 = DecisionTreeClassifier(min_samples_leaf= 100, random_state=42)\n\n# Fit the model\nmodel_sample_100.fit(features_train,target_train)\n\n# Print the accuracy of the prediction (in percentage points) for the training set\nprint(model_sample_100.score(features_train,target_train)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the model to predict churn\nprediction_100 = model_sample_100.predict(features_test)\n\n# Print the accuracy of the prediction for the test set\nprint(\"The accuracy is\" , model_sample_100.score(features_test,target_test))\n\n# Calculate recall score by comparing target_test with the prediction\nprint(\"The recall score is\", recall_score(target_test, prediction_100))\n\n#Print the precision score of the model predictions\nprint( \"The precision score is\", precision_score(target_test, prediction_100))\n\n# Calculate the f1_score\ny_true= target_test\ny_pred= prediction_100\nprint(\"The f1_score is\" , f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For classification problems, not just decision trees, it isn't uncommon for unbalanced classes to give overly optimistic accuracy scores. Here's a way to handle this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the model\nmodel_depth_7_b = DecisionTreeClassifier(max_depth=7 ,class_weight=\"balanced\", random_state=42)\n# Fit it to the training component\nmodel_depth_7_b.fit(features_train,target_train)\n# Make prediction using test component\nprediction_b = model_depth_7_b.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the accuracy of the prediction for the test set\nprint(\"The accuracy is\" , model_depth_7_b.score(features_test, target_test))\n\n# Calculate recall score by comparing target_test with the prediction\nprint(\"The recall score is\", recall_score(target_test, prediction_b))\n\n#Print the precision score of the model predictions\nprint( \"The precision score is\", precision_score(target_test, prediction_b))\n\n# Calculate the f1_score\ny_true= target_test\ny_pred= prediction_b\nprint(\"The f1_score is\" , f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We try the cross_val_score model to give us accuracy scores for different data folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the function for implementing cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# Use that function to print the cross validation score for 10 folds\nprint(cross_val_score(model,features,target,cv=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Parameter tuning\n### Since we are not 100% certain of what kind of Decision Tree we need to use, we find the best possible parameters for our model using the GridSearchCV function."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Generate values for maximum depth\ndepth = [i for i in range(5,21,1)]\n\n# Generate values for minimum sample size\nsamples = [i for i in range(25,500,25)]\n\n# Create the dictionary with parameters to be checked\nparameters = dict(max_depth=depth, min_samples_leaf=samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the GridSearchCV function\nfrom sklearn.model_selection import GridSearchCV\n\n# set up parameters: done\nparameters = dict(max_depth=depth, min_samples_leaf=samples)\n\n# initialize the param_search function using the GridSearchCV function, initial model and parameters above\nparam_search = GridSearchCV(model, parameters)\n\n# fit the param_search to the training dataset\nparam_search.fit(features_train, target_train)\n\n# print the best parameters found\nprint(param_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We then calculate the feature importances for each Decision Tree variation we previously used."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate feature importances\nfeature_importances = model.feature_importances_\n\n# Create a list of features: done\nfeature_list = list(features)\n\n# Save the results inside a DataFrame using feature_list as an indnex\nrelative_importances = pd.DataFrame(index=feature_list, data=feature_importances, columns=[\"importance\"])\n\n# Sort values to learn most important features\nrelative_importances.sort_values(by=\"importance\", ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate feature importances\nfeature_importances_5 = model_depth_5.feature_importances_\n\n# Create a list of features: done\nfeature_list = list(features)\n\n# Save the results inside a DataFrame using feature_list as an indnex\nrelative_importances_5 = pd.DataFrame(index=feature_list, data=feature_importances_5, columns=[\"importance\"])\n\n# Sort values to learn most important features\nrelative_importances_5.sort_values(by=\"importance\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate feature importances\nfeature_importances_7 = model_depth_7_b.feature_importances_\n\n# Create a list of features: done\nfeature_list = list(features)\n\n# Save the results inside a DataFrame using feature_list as an indnex\nrelative_importances_7 = pd.DataFrame(index=feature_list, data=feature_importances_7, columns=[\"importance\"])\n\n# Sort values to learn most important features\nrelative_importances_7.sort_values(by=\"importance\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select only features with relative importance higher than 1%\nselected_features = relative_importances[relative_importances.importance>0]\n\n# create a list from those features: done\nselected_list = selected_features.index\n\n# transform both features_train and features_test components to include only selected features\nfeatures_train_selected = features_train[selected_list]\nfeatures_test_selected = features_test[selected_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Best Model Initialization \n### And finally we use the features with value of importance higher than 0 from our original model, as well as the best parameters given by GridSearchCV, to get the best possible prediction model for our Employee Attrition problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the best model using parameters provided in description\nmodel_best = DecisionTreeClassifier(max_depth=5, min_samples_leaf=25, class_weight=\"balanced\", random_state=42)\n\n# Fit the model using only selected features from training set: done\nmodel_best.fit(features_train_selected, target_train)\n\n# Make prediction based on selected list of features from test set\nprediction_best = model_best.predict(features_test_selected)\n\n# Print the general accuracy of the model_best\nprint(\"The accuracy is\" , model_best.score(features_test_selected, target_test) * 100)\n\n# Print the recall score of the model predictions\nprint(\"The recall score is\" ,recall_score(target_test, prediction_best) * 100)\n\n#Print the precision score of the model predictions\nprint(\"The precision score is\",  precision_score(target_test, prediction_best)* 100)\n\n# Calculate the f1_score\ny_true= target_test\ny_pred= prediction_best\nprint(\"The f1_score is\" , f1_score(y_true, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. We finally use the RandomForestClassifier model and 4 of our top features/predictors to create a visualized Decision Tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model (can also use single decision tree)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100000, max_depth=3)\n\n# Train\nmodel.fit(features[['MonthlyIncome', 'YearsInCurrentRole', 'OverTime', 'StockOptionLevel']], target)\n# Extract single tree\nestimator = model.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n\nimport pydotplus\n\ndt_graphviz = tree.export_graphviz(estimator, feature_names=['MonthlyIncome', 'YearsInCurrentRole', 'OverTime', 'StockOptionLevel'], out_file = None)\n\npydot_graph = pydotplus.graph_from_dot_data(dt_graphviz)\n\nfrom IPython.display import Image\n\nImage(pydot_graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### THANK YOU!!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}