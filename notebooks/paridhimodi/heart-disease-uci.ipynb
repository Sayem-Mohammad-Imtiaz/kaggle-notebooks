{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting heart disease using machine learning\n\nThis notebook looks into various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting weather or not someone has heart disease based on there medical attributes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we are going to take following approch:\n1. problem defination\n2. data \n3. evaluation\n4. features\n5. modelling\n6. exparimantation\n\n# problem\nin a statement, \n8> given clinical parameters about a patient , can we predict weather he have heart disease or not\n\n# data\n> originol data came from the cleaveland data from UCI machine learning repository\n\n# evaluation\n> if we can reach 95% accuracy at predicting weather or not a patient has heart disease during a proof of concept , we'll persue the project\n\n# features\nto get info abpout each feature of ur data\n\n\n**Create data dictionary**\n\n* age: age in years\n* sex: sex (1 = male; 0 = female)\n* cp: chest pain type\n   1: typical angina\n   2: atypical angina\n   3: non-anginal pain\n   4: asymptomatic\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital) above 130 - 140 is a typical cause of concern\n* chol: serum cholestoral in mg/dl\n   1. serum = LDL + HDL + .2* triglycerides\n   2. above 200 is cause for concern\n*  fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n   1. 126 mg/dL signals diabetes\n* restecg: resting electrocardiographic results\n   0: normal\n   1: having ST-T wave abnormality \n     * can range from mild symptoms to severe problems\n     * signals non-normal heart beat\n    \n   2: showing probable or definite left ventricular hypertrophy \n*  thalach: maximum heart rate achieved\n*  exang: exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n    1: upsloping : better heart rate exercise ST segment\n    2: flat : minimal change (typical healthy heart)\n    3: downsloping : signs of unhealthy heart \n* ca: number of major vessels (0-3) colored by flourosopy\n   1. colored vessels means the dr. can see the blood passing through\n   2. the more blood movement the better \n*  thal: thalium stress rate\n   1. 3 = normal; \n   2. 6 = fixed defect; \n   3. 7 = reversable defect : no proper blood movement when exercising\n*  target - have heart disease or not (1= yea, 0= no )","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preparing the tools\n\nwe are going to use pandas, numpy, matplotlib for data analysis and manipulation","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# import all the tools we need\n\n# regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# modles of sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#  model evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('heart-disease.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data exploration (exploratory data analysis or EDA)\n\nthe goal here is to find more about the data and become a subject matter export on the\n\n1. what questions r u trying to solve\n2. what kind of data do we have and how do we treat different types\n3. whats missing data and do u deal with it\n4. where are the outliers and why sould u care about them\n5. how can uh add , change or remove to get more out of ur data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets calc. how many of each classes there are\ndf['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['target'].value_counts().plot(kind = 'bar', color = ['salmon', 'lightblue']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heart disease frequency according to sex","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compare target column with sex column\npd.crosstab(df.target, df.sex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind = 'bar', \n                                    figsize = (10, 6), \n                                    color =['salmon', 'lightblue'])\nplt.title('heart disease frequency for sex')\nplt.xlabel('0 = no disease , 1 = disease')\nplt.ylabel('amount')\nplt.legend(['feamale', 'male'])\nplt.xticks(rotation =0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# age vs. max heart rate for heart disease","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# creating an another figure\nplt.figure(figsize =(10, 6))\n\n# scatter with positive cases\nplt.scatter(df.age[df.target ==1],\n            df.thalach[df.target == 1],\n            c = 'salmon')\n\n# scatter for negative cases\nplt.scatter(df.age[df.target ==0],\n            df.thalach[df.target == 0],\n            c = 'blue')\n\n\nplt.title('age vs. max heart rate for heart disease')\nplt.xlabel('age')\nplt.ylabel('max heart rate')\nplt.legend(['disease', 'not disease'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check distribution of age with histagram\ndf.age.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# heart disease frequency per chest pain type\n\ncp: chest pain type \n1. typical angina : cp due to decrease blood supply to the heart\n2. atypical angina : cp not related to heart\n3. non-anginal pain : typically esthophageal spasms\n4. asymptomatic : cp not showing signs of disease","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.cp.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.target, df.cp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.cp, df.target).plot(kind = 'bar',\n                                   figsize=(10,6),\n                                   color = ['salmon', 'pink'])\n\nplt.title(\"heart disease frequency per chest pain type\")\nplt.xlabel('chest pain')\nplt.ylabel('amount')\nplt.legend(['not disease', 'disease'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# make a corelation matrix","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix,\n                 annot = True,\n                 linewidths= 0.5,\n                 fmt = '.2f',\n                 cmap = 'YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here if value is +ve i.e having positive corellation : value of 1 variable increases then other variable's value also insrease\nfor ex cp vs target as value of cp increases for target = 1 (consider previous graph)\nnd if value is -ve i.e negative corellation : value of 1 variable increases then value of other variable dicreses\nfor ex exang vs target as value of exand inreases i.e is 1 then target value dicreases i.e is 0 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. modelling","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split data into x and y\nx = df.drop('target', axis =1)\ny = df.target\n\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split data into train and test\nnp.random.seed(42)\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have got our data split ito train and test data, now it's time to build a machine learning model\n\nwe'll train it (find patterns) on train set\n\nwe'll test it (using patterns) on test set\n\nWe are going to try 3 different machine learning models:\n1. logistic regression\n2. k-nearest neighbour classifier\n3. random forest classifier","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# put models in a dictionary\nclf = { 'logistic regressor' : LogisticRegression(),\n         'k-n neighbour' : KNeighborsClassifier(),\n         'random forest' : RandomForestClassifier()}\n\n# create a function to fit and score models\ndef fit_and_score(clf, xtrain, xtest, ytrain, ytest):\n    \"\"\"\n    fits and evaluates machine learning models\n    \"\"\"\n    # set random seed\n    \n    np.random.seed(42)\n    # make dictionary to store model scores\n    model_scores = {}\n    # loop through models\n    for name, model in clf.items():\n        # fit the model to the data\n        model.fit(xtrain, ytrain)\n        #evaluate the model and append it's score into model_scores\n        model_scores[name] = model.score(xtest, ytest)\n    return model_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores = fit_and_score(clf = clf,\n                             xtrain = xtrain,\n                             xtest = xtest, \n                             ytrain = ytrain, ytest=ytest)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index =['accuracy'])\nmodel_compare.T.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have got a baseline model.... and we know a model's first predictions aren't always what we should base our next steps off. what should we do?\n\nnow let's look at the following\n* Hyperperameter tuning\n* feature importance\n* confusion matrix\n* cross-validation\n* precision\n* recall\n* f1 score\n* classification report\n* ROC curve\n* area under the curve (AUC)\n\n## hyperperameter tuning (by hands)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# create a list of different values of n_neighbours\nneighbours = range(1, 21)\n\n# set up KNN instance\nKNN = KNeighborsClassifier()\n\n# loop through different n_neighbours\nfor i in neighbours:\n    KNN.set_params(n_neighbors = i)\n\n    # fit the algorithom\n    KNN.fit(xtrain, ytrain)\n    \n    # update the training score list\n    train_scores.append(KNN.score(xtrain, ytrain))\n    \n    # update the test score list\n    test_scores.append(KNN.score(xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(neighbours, train_scores, test_scores)\nplt.xlabel('no. of neighbours')\nplt.ylabel('model score')\nplt.xticks(np.arange(1, 21, 1))\nplt.legend(['train scores', 'test scores'])\n\nprint(f'max knn score on the test data : {max(test_scores) * 100 :.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# hyperperameter tuning with RandomizedSearchCV\n\nwe're going to tune:\n* LogisticRegression\n* randomforest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a hyperperameter grid for logistic regression\nlr_grid = {'C' : np.logspace(-4, 4, 20),\n            'solver': ['liblinear']}\n\n# create a hyperperameter grid for random forest\nrf_grid = {'n_estimators' : np.arange(10, 1000, 50),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split' : np.arange(2, 20, 2),\n           'min_samples_leaf' : np.arange(1, 20, 2)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have got hyperperameters grid setup let's tune our models by using randomizedsearchcv","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n#set up random hyperperameter search for LogisticResgression\nrs_lr = RandomizedSearchCV(LogisticRegression(),\n                           param_distributions= lr_grid,\n                           cv = 5, \n                            n_iter= 20,\n                            verbose=True)\n\n#fit random hyperperameter search for LogisticRegression\nrs_lr.fit(xtrain, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_lr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_lr.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tune random forest classifier\nnp.random.seed(42)\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv = 5, \n                           n_iter = 20, \n                           verbose=True)\n\nrs_rf.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_rf.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"since logistic regression model works best we will try and improve it with gridSearchCV","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Hyper tuning with GridSearchCV","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# creating grid for LogisticResgession Model\nlr_grid= {'C' : np.logspace(-4, 4, 30),\n          'solver' :['liblinear']}\n\n# setup grid hyperperameter search for logistic regressor\nlr_gs = GridSearchCV(LogisticRegression(),\n                     param_grid= lr_grid,\n                     cv = 5, \n                     verbose=True)\n\nlr_gs.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_gs.score(xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Our tuned machine learning model , beyond accuracy\n\n* roc curve and area under the curve \n* confusion matrix \n* classification report\n* precision\n* recall\n* f1 score\n\n... and it would be great if cross - validation is used where possible\n\nto make comparisions and evaluate our trained model, first we need to make predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# let's make predictions with tuned model\ny_preds = lr_gs.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ytest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot rOC curve and calculate AUC metrics\nplot_roc_curve(lr_gs, xtest, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# confusion metrics\nprint(confusion_matrix(y_preds, ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set(font_scale = 1.5)\n\ndef plot_conf_metrics(ytest, y_preds):\n    \"\"\"\n    this function plots confusion matrics using seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (3,3))\n    ax = sns.heatmap(confusion_matrix(ytest, y_preds),\n                     annot = True,\n                     cbar = False)\n    plt.xlabel('true labels')\n    plt.ylabel('predicted labels')\n    \nplot_conf_metrics(ytest, y_preds)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" let's get a classification report as well as cross-validated precission, recall and f1 score","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(ytest, y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we're going to calc accuracy,  precision, recall, f1 using cross-validation and to do this we are going to use cross_val_score","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# check best parameters\nlr_gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create new classifier with best params\nclf = LogisticRegression(C = 0.20433597178569418, solver = 'liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross validated accuracy\ncv_acc = cross_val_score(clf, x, y, cv = 5, scoring ='accuracy')\ncv_acc = np.mean(cv_acc)\ncv_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross validated precision\ncv_pre = cross_val_score(clf, x, y, scoring = 'precision')\ncv_pre = np.mean(cv_pre)\ncv_pre","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross validated recall\ncv_rec = cross_val_score(clf, x, y, scoring = 'recall')\ncv_rec = np.mean(cv_rec)\ncv_rec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross validated f1 score\ncv_f1 = cross_val_score(clf, x, y, scoring = 'f1')\ncv_f1 = np.mean(cv_f1)\ncv_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualise cross validated matrics\ncv_matrics = pd.DataFrame({'accuracy' : cv_acc,\n              'precision': cv_pre,\n              'recall': cv_rec,\n               'f1 score': cv_f1},\n                         index = [0])\n\ncv_matrics.T.plot.bar(title ='cross validated matrics',\n                      legend = False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance\nfeature importance is another as asking , which features 'contibuted most to the outcome of the model and how did they congtibute?'\n\nfinding feature importance is different for each machine learning model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# fitting\nclf.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check coef_\nclf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index =[0])\nfeature_df.T.plot.bar(title = 'feature importance', legend = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.sex, df.target) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" it have a negative coprelation therefore as sex value increases the target vaue decreases","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.slope, df.target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it have a positive corelation as slope value increases target value also increases","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Experimation\n\nIf you haven't hit ur evaluation metrics yet... ask yourself..\n\n* could u collect more data?\n* couls u try a better model? \n* could u improve the current model(beyond what we've done so far)\n* if ur model is gud enough        (you have hit ur evaluation metrics) how would you export and shear with other?\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Save model using Pickel**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import pickle\n# save model to disk\nfilename = 'heart-disease-final-model.sav'\npickle.dump(clf, open(filename, 'wb'))\n\n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(xtest, ytest)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}