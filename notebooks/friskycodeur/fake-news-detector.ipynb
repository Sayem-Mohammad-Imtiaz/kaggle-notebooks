{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a id=\"top_section\"></a>\n\n<div align='center'><font size=\"5\" color=\"#000000\"><b>Fake News Detector<br>(~99.9% accuracy)</b></font></div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">A General Introduction</font></div>\n<hr>\n\n\n\n## Table of Contents\n\n* [Getting the Text Data Ready](#getting_the_data_ready)\n* [Visualizing the Data](#visualize_data)\n* [Cleaning the data](#clean_data)\n* [Frequent Words](#wordcloud)\n* [Tokenization](#tokenize)\n* [Building our Model](#model)\n* [Analyzing our Model](#analyze_model)\n* [Some Last Words](#sectionlst)\n    \n## Summary\n**In this kernel , I try to analyse and then build a model to predict whether the news given to us is fake or not<br>\nI have used Glove embeddings and LSTM layers to get an accuracy of 99.9% on Train data and 99.8% on test data.If you want to check out some of my other projects , here is my GITHUB link :P**<br><br>\n<a class=\"nav-link active\"  style=\"background-color:; color:Blue\"  href=\"https://github.com/friskycodeur\" role=\"tab\">Prateek Maheshwari on GITHUB</a>\n\n<br>\n<a href=\"https://ibb.co/nm4kTk1\"><img src=\"https://bsmedia.business-standard.com/media-handler.php?mediaPath=https://bsmedia.business-standard.com/_media/bs/img/article/2020-03/16/full/1584358219-7432.jpg&width=1200\" alt=\"Fake news picture\" border=\"0\" height=300 width=300></a>\n\n\n### Here are the things I will try to cover in this Notebook:\n\n- Basic EDA of the text data.\n- Data cleaning\n- Making some awesome Word clouds\n- Using Glove embedding and tokenizer\n- Building our model \n\n### I highly appreciate your feedback, there might be some areas can be fixed or improved.\n\n## If you liked my work please dont forget to Upvote!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"getting_the_data_ready\"></a>\n## Getting the Data Ready","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=01></a>\n### Importing necessary libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**First off , we will import all the necessary libraries we need. <br> Apart from the basic libraries , I am using nltk,beautifulsoup,re,string to help with our text data. <br> For our model building i will be using embedding , lstm , dropout and dense layers.I will also be using ReduceLRonPlateau as callback. <br><br>\nSo let's import all of these libraries.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('darkgrid')\n\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import STOPWORDS,WordCloud\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\n\nfrom keras.preprocessing import text,sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,Embedding\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='load_data'></a>\n### Loading the data ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Having imported all the necessary libraries , now we will go ahead and load our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news=pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\nfake_news=pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a sneak peak at our data !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now combine both of these data and add a column of 'Isfake' so that we can use all the data as once and the 'Isfake' column will also be our target column , which will determine if the news is fake or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news['Isfake']=0\nfake_news['Isfake']=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using conactenate function of pandas :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([real_news,fake_news])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So how does our data look now ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any null values?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are no null values , we are saved from the hassle of making up for the missing values. Now we will visualize the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='visualize_data'></a>\n## Visualizing the data ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"How many of the given news are fake and how many of them are real?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.Isfake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of fake and real news are almost equal. <br>\nNow let us see how many unqiue titles are there. Are any of the titles repeated?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.title.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many subjects are there ? We can see that using value_counts()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.subject.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how much of the news in different subject are fake !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nchart=sns.countplot(x='subject',hue='Isfake',data=df,palette='muted')\nchart.set_xticklabels(chart.get_xticklabels(),rotation=90,fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will place all of the required columns in one and delete all the not-so-required columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']= df['text']+ \" \" + df['title']\ndel df['title']\ndel df['subject']\ndel df['date']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with this now , we shall head towards cleaning our data!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='clean_data'></a>\n## Cleaning the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://i.pinimg.com/originals/f5/fb/5e/f5fb5efe6b9f1d5f11f19e69f67f5ccf.gif'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our data may consist URLs , HTML tags which might make it difficult for our model to predict properly. To prevent that from happening we will clean our data so as to make our model more efficient.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will be removing punctuation , stopwords,URLS, html tags from our text data. <br>\nFor this we shall use beautifulsoup and re library which we imported earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words=set(stopwords.words('english'))\npunctuation=list(string.punctuation)\nstop_words.update(punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_html(text):\n    soup=BeautifulSoup(text,\"html.parser\")\n    return soup.get_text()\n\ndef remove_square_brackets(text):\n    return re.sub('\\[[^]]*\\]','',text)\n\ndef remove_URL(text):\n    return re.sub(r'http\\S+','',text)\n\ndef remove_stopwords(text):\n    final_text=[]\n    for i in text.split():\n        if i.strip().lower() not in stop_words:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\ndef clean_text_data(text):\n    text=string_html(text)\n    text=remove_square_brackets(text)\n    text=remove_stopwords(text)\n    text=remove_URL(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have defined the cleaning functions , let us use em' on our text data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(clean_text_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are all done with cleaning and have with us cleaned text data now.Next up are some awesome wordclouds.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='wordcloud'></a>\n## Frequent Words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I wonder what words were the most used in fake news and real news and i guess you do too!<br>\nSo let's see what these frequent words are , and for that we will use wordcloud.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see the fake news texts first !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwordcloud=WordCloud(stopwords=STOPWORDS,height=600,width=1200).generate(\" \".join(df[df.Isfake==1].text))\nplt.imshow(wordcloud,interpolation='bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now what about the real news?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwordcloud=WordCloud(stopwords=STOPWORDS,height=600,width=1200).generate(\" \".join(df[df.Isfake==0].text))\nplt.imshow(wordcloud,interpolation='bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those were some nice wordclouds , and clearly Donald Trump , United States , etc were very frequent.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='tokenize'></a>\n## Tokenization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We shall now tokenize our data ,i.e convert the text data into vectors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(df.text,df.Isfake,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=10000\nmax_len=300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To tokinize our data , I am using tokenizer here. There are other ways to tokenize data , you can also try them out.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here is what is happening in the next code tab:\n- First we initialized the tokenizer with it's size being 10k.\n- Then we fit the training data on this tokenizer.\n- Then we convert the text to sequences and save it in X_train variable.\n- Lastly we add a padding layer around our sequence.\n\nHere is a example of what tokenizer does \n<img src='https://miro.medium.com/max/2414/1*UhfwmhMN9sdfcWIbO5_tGg.jpeg'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\ntokenizer_train=tokenizer.texts_to_sequences(X_train)\nX_train=sequence.pad_sequences(tokenizer_train,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_test=tokenizer.texts_to_sequences(X_test)\nX_test=sequence.pad_sequences(tokenizer_test,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will import our GLOVE file , I am using the 100d version here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_file='../input/glove-twitter/glove.twitter.27B.100d.txt'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will get the coefficients from the glove file and save it in embedding index variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr,dtype='float32')\nembeddings_index=dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(glove_file,encoding=\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What's happening in the next code tab:\n- We first take all the values of the embeddings and store it in all_embs.\n- Then we take the mean and standard deviation of all the embeddings.\n- We now take the word indices using .word_index function of tokenizer.\n- Then we will see what the length of each vector would be and save it in nb_words.\n- We make an embedding matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs=np.stack(embeddings_index.values())\nemb_mean,emb_std=all_embs.mean(),all_embs.std()\nemb_size=all_embs.shape[1]\n\nword_index=tokenizer.word_index\nnb_words=min(max_features,len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean,emb_std,(nb_words,emb_size))\n\nfor word,i in word_index.items():\n    if i>=max_features: continue\n    embedding_vector=embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i]=embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='model'></a>\n## Building our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have succesfully done the tokenization part , let's build our model now!<br>\nHere are the parameters I'm taking.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=256\nepochs=10\nemb_size=100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's initialize our callback.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"leaning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=10,factor=0.5,min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's build our model.Here are the layers I'm using:\n- Starting with an embedding layer\n- Then 3 LSTM layers\n- Then 2 Dense layers\n\nI am using Adam optimizer for our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Embedding(max_features,output_dim=emb_size,weights=[embedding_matrix],input_length=max_len,trainable=False))\nmodel.add(LSTM(units=256,return_sequences=True,recurrent_dropout=0.25,dropout=0.25))\nmodel.add(LSTM(units=128,return_sequences=True,recurrent_dropout=0.25,dropout=0.25))\nmodel.add(LSTM(units=64,recurrent_dropout=0.1,dropout=0.1))\nmodel.add(Dense(units=32,activation='relu'))\nmodel.add(Dense(1,'sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train our model now !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=batch_size,validation_data=(X_test,y_test),epochs=epochs,callbacks=[leaning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see our model in action ! ;)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(X_test)\npred[5:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='analyze_model'></a>\n## Analyzing our model ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see how the accuracy and loss graphs of our model look now !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs,train_acc,'go-',label='Training Accuracy')\nax[0].plot(epochs,val_acc,'ro-',label='Validation Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\nax[0].legend()\n\nax[1].plot(epochs,train_loss,'go-',label='Training Loss')\nax[1].plot(epochs,val_loss,'ro-',label='Validation Loss')\nax[1].set_xlabel('Loss')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now see how many of the samples were wrongly predicted using the confusion matrix. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=pd.DataFrame(cm,index=['Fake','Not Fake'],columns=['Fake','Not Fake'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap=\"Blues\",linecolor='black',linewidth=1,annot=True,fmt='',xticklabels=['Fake','Not Fake'],yticklabels=['Fake','Not Fake'])\nplt.xlabel('Actual')\nplt.ylabel('Predicted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now what is our accuracy on Test and Train set?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Accuracy of the model on Training Data is - { model.evaluate(X_train,y_train)[1]*100:.2f}')\nprint(f'Accuracy of the model on Testing Data is -  {model.evaluate(X_test,y_test)[1]*100:.2f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#top_section\" class=\"btn btn-primary\" style=\"color:white;\" >Back to Table of Content</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Some last words:\n\nThank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"3\" color=\"#000000\"><b>And again please upvote if you liked this notebook so it can reach more people, Thanks!</b></font></div>\n\n<img src='https://media.tenor.com/images/69d1d66198f1aac60ad244f6c004f372/tenor.gif'>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}