{"cells":[{"metadata":{},"cell_type":"markdown","source":"# An implementation of Spam Classification using Logistic Regression and solving the Logistic Regression manually using Gradient Descent as well as with Scikit Learn"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/spamtest/spam.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **About the Dataset**\n\n\n****Columns****\n\nEmail\n\nText\nLabel\n\n****Labels****\n\nSpam\n\nHam -> Not Spam"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a bar graph to see the number of spam and not spam emails.\nimport matplotlib.pyplot as plt\nspam = (df.loc[df['Label'] == 'spam'])['Label'].count()\nnotspam = ((df.loc[df['Label'] == 'ham'])['Label'].count())\nlabels = ['Spam', 'Not Spam']\nvalues = [spam, notspam]\nprint(values)\nplt.bar(labels,values)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Out of the given training set of 5572 training examples the following is the compostion:\n**747** Spam\n\n**4825** Not Spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk\nfrom nltk.corpus import stopwords\ndef text_preprocess(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in  stopwords.words('english')]\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the text\nRemoving punctuations and stop words."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"email=[]\nfor text in df['EmailText']:\n    email.append(text_preprocess(text))\ntype(email) # contains the list of pre processed emails","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['EmailText'] = email\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization\n Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df['EmailText'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.array(x.toarray())\nnp.shape(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the prediction vector by treating Spam as 1 and not spam as 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"y=[]\nfor pred in df['Label']:\n    if(pred == 'spam'):\n        y.append(1)\n    else:\n        y.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= np.ones((np.shape(X)[0],1))\nX = np.append(a,X,axis=1)\nnp.shape(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the test data\nfrom sklearn.model_selection import train_test_split\n(TrainX,ValuateX,Trainy,Valuatey) = train_test_split(X,y,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The data set has been split into 2 parts, one to train the model and the other to Evaluate the accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.shape(TrainX))\nprint(np.shape(Trainy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.shape(ValuateX))\nprint(np.shape(Valuatey))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solving Logistic Regression by calculating the parameters through Gradient Descent"},{"metadata":{},"cell_type":"markdown","source":"# **The Hypothesis**\n   In the case of Logistic Regression the Hypothesis is of the form, **hypothesis = **sigmoid(Transpose(theta)X)****"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef hypothesis(theta,X):\n    h = -1*(np.matmul(X,theta))\n    h = 1/(1+np.exp(h))\n    return h","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Cost Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost(theta,X,y,m):\n    t1 = np.ones(np.shape(y)[0])\n    h = hypothesis(theta,X)\n    t2 = np.ones(np.shape(h)[0])\n    c = np.sum((y*np.log(h)) +((t1+y)*np.log(t2-h) ))/m\n    return c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Gradient Descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradientDescent(theta,X,y,m,alpha,it):\n    iteration = []\n    c = []\n    for i in range(0,it):\n       # d = alpha*((hypothesis(theta,X)-y)*X)\n        d = hypothesis(theta,X)-y\n        d = np.sum((X*d[0])*alpha,axis=0)\n       # print(np.shape(d))\n        theta = theta - d\n        temp = cost(theta,X,y,m)\n        #print(temp)\n        c.append(-1*cost(theta,X,y,m))\n        iteration.append(i)\n    return(theta,iteration,c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(TrainX)[1] #no of features\ntheta = np.zeros(np.shape(TrainX)[1])\nnp.shape(TrainX)\n(theta,i,c) = gradientDescent(theta,TrainX,Trainy,np.shape(TrainX[0]),0.000001,2000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the cost function vs Iterations to check the proper functioning of Gradient descent and check the value of parameter alpha."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.plot(i,c)\nplt.xlabel('Iterations ->')\nplt.ylabel('Cost function ->')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The cost function decreases with time and approaches a constant value, therefore the computed parameters are acceptable.\n# The following value of thetas form our hypothesis in the form H=sigmoid(X.Theta)"},{"metadata":{"trusted":true},"cell_type":"code","source":"theta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Library Implementation Of Solving Logistic Regression using liblinear solver and imposing a penalty l1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nSpam_model = LogisticRegression(solver='liblinear', penalty='l1')\nSpam_model.fit(TrainX, Trainy)\npred = Spam_model.predict(ValuateX)\naccuracy_score(Valuatey,pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# An accuracy of around 98 percent is Achieved while using this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}