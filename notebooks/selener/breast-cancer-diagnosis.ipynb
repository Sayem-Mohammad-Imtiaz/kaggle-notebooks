{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Breast Cancer Diagnostic - Classification\n\n### Tutorial for beginners in Supervised Machine Learning and classification data analysis."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Table of Content\n\n* [Objectives](#obj)\n* [Importing packages and loading data](#imp)\n* [Data cleaning and data wrangling](#clean)\n* [Exploratory Data Analysis (EDA)](#eda)\n    * [Boxplot of features by diagnosis](#p1)\n    * [Violin plot of features by diagnosis](#p2)\n    * [Correlation](#corr)\n* [Statistical inference](#stat)\n    * [Hypothesis testing](#t)\n* [Machine Learning: Classification models](#ml)\n    * [Spliting the data: train and test](#sp)\n    * [k-nearest neighbors (k-NN)](#knn)\n    * [Support Vector Classification](#sv)\n    * [Logistic Regression](#log)\n    * [ExtraTree-decision](#tree)\n    * [Random-Forest Classifier](#rf)\n    * [Nueral-Networks: KERAS-Tensorflow](#nn)\n    * [Keras with normalization](kn)\n* [Feature Selection](#fs)\n* [Summary of models performance](#sum)"},{"metadata":{"_uuid":"6b3a80128ea07905d7b45f3710fc683e173f1190"},"cell_type":"markdown","source":"<a id='obj'></a>\n## Objectives:<br>\n**1)** Determine which features of data (measurements) are most important for diagnosing breast cancer.<br><br>\n**2)** Test performance of different classification models: \n* k-nearest neighbors (k-NN)\n* Suport Vector Classifier\n* Logistic Regression\n* ExtraTree-decision\n* Random-Forest\n* Keras (Deep-Learning)"},{"metadata":{"_uuid":"f7556bf3ee1d1828317467fffd2d90899b1d9866"},"cell_type":"markdown","source":"<a id='imp'></a>\n## Importing packages and loading data"},{"metadata":{"trusted":true,"_uuid":"c0ee4b5d495401edd6507563fbdec0dd9b53bd3a"},"cell_type":"code","source":"# here we will import the libraries used for machine learning\nimport sys \nimport numpy as np # linear algebra\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I/O, data manipulation \nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \n\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.model_selection import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import Imputer  # dealing with NaN\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn import svm, datasets # for Support Vector Machine\nfrom sklearn.svm import SVC\n\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f19c690b731d60d520905f6b90e081edb32b311b"},"cell_type":"code","source":"data0 = pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"645c95efc4ba3cf642f98e0fdef3622aa97ac0ab"},"cell_type":"code","source":"data0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fba8d9c1a5cc8eabcedd403bf9263f952951cc0e"},"cell_type":"code","source":"data0.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"647b6e4a153d6aff31bd29d79ada2c9785fadeed"},"cell_type":"markdown","source":"<a id='clean'></a>\n## Data cleaning and data wrangling"},{"metadata":{"trusted":true,"_uuid":"7f1f1e2c1f3aff1e2406c5c93d7ec37573dc8fb5"},"cell_type":"code","source":"data0[data0 == '?'] = np.NaN\n# Drop missing values and print shape of new DataFrame\ndata0.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"665106840853ac788ad1ae89d74c253697e5b52f"},"cell_type":"code","source":"# drop columns:  \"Unnamed: 32\" and \"ID\"\n# To keep the same name of file, write: inplace=True\n# Separating target from features (predictor variables)\n\ny = data0.diagnosis     # target= M or B \n\nlist = ['Unnamed: 32','id','diagnosis']\nfeatures = data0.drop(list,axis = 1,inplace = False)\n\nlist = ['Unnamed: 32','id']\ndata0.drop(list, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data0.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7002d8ff11a1e9ad74d477adbd8860e89b62f73"},"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true,"_uuid":"8d9cab2647063e2b70fc99cf66b09416ccc8ea38"},"cell_type":"code","source":"# The frequency of cancer stages\nB, M = data0['diagnosis'].value_counts()\nprint('Number of Malignant : ', M)\nprint('Number of Benign: ', B)\n\nplt.figure(figsize=(10,6))\nsns.set_context('notebook', font_scale=1.5)\nsns.countplot('diagnosis',data=data0, palette=\"Set1\")\nplt.annotate('Malignant = 212', xy=(-0.2, 250), xytext=(-0.2, 250), size=18, color='red')\nplt.annotate('Benign = 357', xy=(0.8, 250), xytext=(0.8, 250), size=18, color='w');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5a943f975b6cc83d5867adec1449e5436a7a7d8"},"cell_type":"markdown","source":"The target variable (response), as shown in the bar chart above, has unbalanced data. In other words, classes are not represented equally. One way to deal with this issue is resampling. However, this tutorial will not address that problem."},{"metadata":{"trusted":true,"_uuid":"c5b8b726ee3b5d30cf3ab213bcb4f2b39e73b4c9"},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"800d43e7b13a575a77482123c8f32ec52b1c990a"},"cell_type":"markdown","source":"Ten features (predictor variables) were computed for each cell nucleus: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. In addition, mean, standard error (se), and largest (“worst”) mean of the were also registered for each feature.\n\nData are distributed in a wide range, therefore, features were standardized before their visualization, so they have a mean of ‘0’ and a standard deviation of ‘1’."},{"metadata":{"trusted":true,"_uuid":"b1d1e740f4c79612ab7a9e73cf646e72ab949c27"},"cell_type":"code","source":"#data0.columns or\ndata0.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d38f9a0cd713f4fbe8590a751df3049b45d7ce"},"cell_type":"code","source":"# Standardization of features\nstdX = (features - features.mean()) / (features.std())              \ndata_st = pd.concat([y,stdX.iloc[:,:]],axis=1)\ndata_st = pd.melt(data_st,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9afeff7614396c2e3215da9b3a0cc253a41a3e8d"},"cell_type":"markdown","source":"<a id='p1'></a>\n### Boxplot of features by diagnosis"},{"metadata":{"trusted":true,"_uuid":"c5154a23ff30c9f8f83a6490bd039f0e87e83877"},"cell_type":"code","source":"plt.figure(figsize=(12,30))\nsns.set_context('notebook', font_scale=1.5)\nsns.boxplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_st, palette='Set1')\nplt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4de41684e764213ebef177d2887dab9693ea58d"},"cell_type":"markdown","source":"From this plot it is possible to identify that some features are very similar, as for example, perimeter_mean and area_mean, and perimeter_se and area_se. Furthermore, we can also observe that some features have different measures when the tumor is malignant or benign. Examples of this are area_mean, radius_mean, and concavity_mean. On the other hand, in features such as fractal_dimension_mean and texture_se, the distribution of malignant and benign tumoers seems to be similar."},{"metadata":{"_uuid":"2944a4091f5034cfc33a0357448e2414e121c7f1"},"cell_type":"markdown","source":"<a id='p2'></a>\n### Violin plot of features by diagnosis"},{"metadata":{"trusted":true,"_uuid":"d5fa3fd92b9abf2a570cf0be5a584343040ee022"},"cell_type":"code","source":"plt.figure(figsize=(12,30))\nsns.set_context('notebook', font_scale=1.5)\nsns.violinplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_st,split=True, \n               inner=\"quart\", palette='Set1')\nplt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc093c2ac6806941cba76577a3649fe835799a9c"},"cell_type":"markdown","source":"<a id='corr'></a>\n### Correlation\n\nLooking at correlations matrix, defined via Pearson function."},{"metadata":{"trusted":true,"_uuid":"f678a31ac97d8171719f9ea65ba7b6a209a976cc"},"cell_type":"code","source":"corr = data0.corr() # .corr is used to find corelation\nf,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, cbar = True,  square = True, annot = True, fmt= '.1f', \n            xticklabels= True, yticklabels= True\n            ,cmap=\"coolwarm\", linewidths=.5, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e98fccb4e93ecf413d15380f0a119eb4e8d3ffd2"},"cell_type":"markdown","source":"Finding un-correlated variables via Pearson correlation coefficient between two arrays"},{"metadata":{"trusted":true,"_uuid":"5dc461f116255610ac7581ee4196fd529bfabf2a"},"cell_type":"code","source":"def pearson_r(x, y):\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x, y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\n# Compute Pearson correlation coefficient for 'radius_mean', 'symmetry_mean'\nr1 = pearson_r(data0['radius_mean'], data0['perimeter_mean'])\nr2= pearson_r(data0['radius_mean'], data0['symmetry_mean'])\n\nname_c = []\nfor (i,j) in zip(range(1,31),range(1,31)):\n        r = pearson_r(data0.iloc[:,1], data0.iloc[:,j])\n        if abs(r) >= 0.80 and data0.columns[j]  not in name_c:\n                    name_c.append(data0.columns[j]) \nprint()\nprint('* Lenght of columns assuming r >=0.80:', len(name_c)) \nprint('name_c =',name_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"177f37dec63aa0d0bb1783c0dcd3ab17c80099ba"},"cell_type":"code","source":"name_c = []\nfor (i,j) in zip(range(1,31),range(1,31)):\n        r = pearson_r(data0.iloc[:,1], data0.iloc[:,j])\n        if abs(r) <= 0.40 and data0.columns[j]  not in name_c:\n                    name_c.append(data0.columns[j])\n                            \nprint('* Lenght of columns assuming r <=0.40:', len(name_c)) \nprint('name_c =',name_c) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4104bccda5ccbf4963adbfe39d5140a0320e25a"},"cell_type":"markdown","source":"**What do correlations mean?**\n\nLets separately fit correlated and uncorrelated data via linear regression: "},{"metadata":{"trusted":true,"_uuid":"99e2017abf2d8d2206a6ef1c589edcd36022ce4a"},"cell_type":"code","source":"sns.lmplot(x='radius_mean', y= 'symmetry_mean', data = data0, hue ='diagnosis', \n           palette='Set1')\nplt.title('Linear Regression: distinguishing between M and B', size=16)\n\n\nsns.lmplot(x='radius_mean', y= 'perimeter_mean', data = data0, hue ='diagnosis', \n           palette='Set1')\nplt.title('Linear Regression: Cannot distinguish between M and B', size=16);\n\nprint('Uncorrelated data are poentially more useful: discrimentory!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3103904be73342bb5c8fc497374931552568ab2"},"cell_type":"markdown","source":"The bloxblot and swarm plots below show that malignant and benign tumors have almost the same measures of fractal_dimension_mean, while radius_mean provides more information for classification."},{"metadata":{"trusted":true,"_uuid":"0fc2933d6cc2dec893e0375cd916cbf8c4467d4f"},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.set_context('notebook', font_scale=1.5)\nplt.subplot(2, 2, 1)\nsns.boxplot(y=\"radius_mean\", x=\"diagnosis\", data=data0, palette=\"Set1\") \nsns.swarmplot(x=\"diagnosis\", y=\"radius_mean\",data=data0, palette=\"Set3\", dodge=True)\nplt.subplot(2, 2, 2)  \nsns.boxplot(y=\"fractal_dimension_mean\", x=\"diagnosis\", data=data0, palette=\"Set1\")\nsns.swarmplot(x=\"diagnosis\", y=\"fractal_dimension_mean\",data=data0, palette=\"Set3\",\n              dodge=True)\nplt.subplots_adjust(wspace=0.4); ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11fe61eed20963948797e806da81db3cac619fc6"},"cell_type":"markdown","source":"<a id='stat'></a>\n## Statistical inference\n\nTo further investigate the properties of features, we constructed the empirical cumulative distribution of features (ECDF), fractal_dimension_mean and radius_mean. "},{"metadata":{"trusted":true,"_uuid":"c9d61c86c2bb8564522211b6472746c3b00a0854"},"cell_type":"code","source":"# CDF function\ndef ecdf(data0):\n    n=len(data0)\n    x=np.sort(data0)\n    y=np.arange(1, n+1)/n\n    return x, y \n\ndata2 = data0['radius_mean']\nMalignant = data2[data0['diagnosis']=='M']\nBenign = data2[data0['diagnosis']=='B']\n\nx1, y1 = ecdf(Malignant)\nx2, y2 = ecdf(Benign)\n\ndata3 = data0['fractal_dimension_mean']\nMalignant_f = data3[data0['diagnosis']=='M']\nBenign_f = data3[data0['diagnosis']=='B']\n\nx3, y3 = ecdf(Malignant_f)\nx4, y4 = ecdf(Benign_f)\n\nplt.figure(figsize=(15,15))\n#plt.close('all')\nplt.subplot(2, 2,  1)\nplt.subplots_adjust(wspace=0.4, hspace=2)\nplt.plot(x1, y1, marker='.',linestyle='none', color='red', label='M')\nplt.plot(x2, y2, marker='.',linestyle='none', color ='blue', label='B')\nplt.margins(0.02)\nplt.xlabel('radius_mean', size=20)\nplt.ylabel('ECDF', size=20)\nplt.title('Empirical Cumulative Distribution Function', size=20)\nplt.legend(prop={'size':20})\n#plt.show()\nplt.subplot(2, 2,  2)\nplt.subplots_adjust(wspace=0.4, hspace=2)\nplt.plot(x3, y3, marker='.',linestyle='none', color='red', label='M')\nplt.plot(x4, y4, marker='.',linestyle='none', color ='blue', label='B')\nplt.margins(0.02)\nplt.xlabel('fractal_dimension_mean', size=20)\nplt.ylabel('ECDF', size=20)\nplt.title('Empirical Cumulative Distribution Function', size=20)\nplt.legend(prop={'size':20});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9045e9cb1c5a8823647841e4c610f539e974e98c"},"cell_type":"markdown","source":"<a id='t'></a>\n### Hypothesis testing\n\nSome necessary functions were created in order to do hypothesis analysis."},{"metadata":{"trusted":true,"_uuid":"aa6c948c03054281d44d080509f6709b96c1ba30"},"cell_type":"code","source":"def permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1, data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\n\n\ndef draw_perm_reps(data_1, data_2, func, size=1):\n    \"\"\"Generate multiple permutation replicates.\"\"\"\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n\n    return perm_replicates\n\n\n\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1)-np.mean(data_2)\n\n    return diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12fbb1202fd6b94e634bd3276497202aaaa68638"},"cell_type":"code","source":"diff_of_means(Malignant, Benign)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92058a978efacb2f7938a3b5d925649155b4a4d2"},"cell_type":"code","source":"# Computing difference of mean overall acore\nempirical_diff_means = diff_of_means(Malignant, Benign)\n\n# Drawing 10,000 permutation replicates: perm_replicates\nperm_replicates = draw_perm_reps(Malignant, Benign,diff_of_means, size=10000)\n\n# Computing p-value: p\np = np.sum(perm_replicates >= empirical_diff_means)/ len(perm_replicates) \n\nprint('p-value =', p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbfcdf1ef36da1186c6353c57871ce4c2bc91bb4"},"cell_type":"markdown","source":"We failed to reject the null hypothesis. The p-value tells us that there is 0.0% chance that we would get the difference of means observed, if Malignant and Benign radius_mean were exactly the same."},{"metadata":{"_uuid":"b6bcfb403924da8e3a1c6fbd024193f652b59881"},"cell_type":"markdown","source":"> <a id='ml'></a>\n## Machine Learning: Classification models"},{"metadata":{"_uuid":"dc468066ceee669952d88a48755d606006b48ac8"},"cell_type":"markdown","source":"#### Mapping the target: categorizing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's map diagnosis column[object] to integer value:0, 1\n# later on below I show how to use LabelEncoder(): it is better way to categorize\ndata=data0.copy()\ndata['diagnosis']=data0['diagnosis'].map({'M':1,'B':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='sp'></a>\n### Spliting the data into train and test sets\n\nBefore modelling, the original data was split into train (70%) and test (30%)."},{"metadata":{"trusted":true,"_uuid":"4f2393eede25ef9c221616e2e35dc49294e2f3eb"},"cell_type":"code","source":"# Split the data into train (0.7) and test (0.3)\n\n## all data without dropping those with correlations\nX = data.drop('diagnosis', axis=1)\ny = data['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, \n                                                    stratify=y)\n\nprint(type(X))\nprint(type(y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7204b821bebdcb984277d3faed7795556be6c350"},"cell_type":"markdown","source":"<a id='knn'></a>\n### k-nearest neighbors (K-NN) \n\nA k-Nearest Neighbors (k-NN) classifier with 3 neighbors (k) was created, and fitted to the training data."},{"metadata":{"trusted":true,"_uuid":"a1fbf1b04a6e42750298fc405e677f3042e0fdf1"},"cell_type":"code","source":"# Creating a k-NN classifier with 3 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint('Accuracy KNN(1): ', knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5a919e16e9796f4f339492e9c85e5c7cad5a43b"},"cell_type":"markdown","source":"#### Learning curves: over/underfitting\n\nThe learning curves for k-NN model were constructed, varying the number of neighbors. The results can be seen bellow."},{"metadata":{"trusted":true,"_uuid":"7d840251c160020b3abbe2ad2d42e13f350f4d60"},"cell_type":"code","source":"neighbors = np.arange(1, 22)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    #Compute accuracy on the training and testing sets\n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test)\n\nplt.figure(figsize=(12,7))\nsns.set_context('notebook', font_scale=1.5)\nplt.title('Learning curves for k-NN: Varying Number of Neighbors', size=20)\nplt.plot(neighbors, test_accuracy, marker ='o', label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, marker ='o', label = 'Training Accuracy')\nplt.legend(prop={'size':15})\nplt.xlabel('Number of Neighbors (k)', size=15)\nplt.ylabel('Accuracy', size=15)\nplt.annotate('Over-fitting', xy=(0.5, 0.94), xytext=(0.3, 0.935), size=15, color='red')\nplt.annotate('Under-fitting', xy=(0.5, 0.94), xytext=(18, 0.93), size=15, color='red')\nplt.xticks(np.arange(min(neighbors), max(neighbors)+1, 1.0));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f38d7cec3af699f761cd4caa3681abbd2b5f6eb3"},"cell_type":"markdown","source":"* <font color='blue'>It is seen from above over-fitting (low k) and under-fitting (high k).  </font>\n\n* We can find the optimized values of k via GridSearchCV in scikit-learn:"},{"metadata":{"_uuid":"25ed906ca6d565e57d48393233c012b82b9afcbb"},"cell_type":"markdown","source":"#### Normalization issue\n\nFrom the plot below, and as it was shown in the EDA, it is possible to see that features are in different scales."},{"metadata":{"trusted":true,"_uuid":"4ea8b7dfe2e8a70391324fd979f432a64d3992b4"},"cell_type":"code","source":"## data are distributed in a wide range (below), need to be normalizded.\nplt.figure(figsize=(15,3))\nax= data.drop('diagnosis', axis=1).boxplot(data.columns.name, rot=90)\nplt.xticks( size=20)\nax.set_ylim([0,50]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7716b1c75cf7e4676e2460fe5ceda306182de096"},"cell_type":"markdown","source":"#### K-NN Learning Pipeline (including scaling)\nA second k-NN model was created using a standardized data."},{"metadata":{"trusted":true,"_uuid":"2b75e942bca7cbbe52c1d6d669d4c7b3674fad40"},"cell_type":"code","source":"steps = [('scaler', StandardScaler()), \n         ('knn', KNeighborsClassifier())]\n\npipeline = Pipeline(steps)\nparameters = {'knn__n_neighbors' : np.arange(1, 50)}\n\n\nk_nn = GridSearchCV(pipeline, param_grid=parameters)\nk_nn.fit(X_train, y_train)\ny_pred = k_nn.predict(X_test)\n\nprint(k_nn.best_params_)\nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is: {}\".format(k_nn.best_score_))\n\nConfMatrix = confusion_matrix(y_test,k_nn.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"af2f4de1fcd931928640c4783778d7f054e55c47"},"cell_type":"code","source":"cv_knn = cross_val_score(k_nn, X, y, cv=5, scoring='accuracy')\nprint('Average 5-Fold CV Score: ', cv_knn.mean(), ', Standard deviation: ', cv_knn.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='sv'></a>\n### Support Vector Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space: C is regularization strength while gamma controls the kernel coefficient. \nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train & test sets\n\n# Instantiate the GridSearchCV object: cv\ncv =GridSearchCV(pipeline,parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is: {}\".format(cv.best_score_))\n\nConfMatrix = confusion_matrix(y_test,cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To visualize which SVC kernel is better: "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('diagnosis', axis=1).values[:,:2]\ny = data['diagnosis'].values\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel']\nplt.figure(figsize=(10,12))\nfor j, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n# Plot the decision boundary by assigning a color to each point in the mesh \n    plt.subplot(2, 2, j + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.6)\n\n# Ploting  the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('RdBu_r'))\n    plt.xlabel('radius_mean',size=20)\n    plt.ylabel('texture_mean',size=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[j],size=20);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9545ebe5334174067d2eee40953e3c40c290fef4"},"cell_type":"markdown","source":"<a id='lr'></a>\n### Logistic Regression "},{"metadata":{"trusted":true,"_uuid":"ff32963d03853a93cfa1103ee9920af759615348"},"cell_type":"code","source":"# Setup the hyperparameter grid, (not scaled data)\nparam_grid = {'C': np.logspace(-5, 8, 15)}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid , cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n\nConfMatrix = confusion_matrix(y_test,logreg_cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03e9329da5663d6ad3541f7935b17740f844ff80"},"cell_type":"markdown","source":"<a id='tree'></a>\n### Extra Tree Classifier"},{"metadata":{"trusted":true,"_uuid":"ecfbc10a4be1adeb7aca468cc4581a2b6298f28f"},"cell_type":"code","source":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\n#tree = DecisionTreeClassifier() # ExtraTrees is better here. \ntree= ExtraTreesClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\ny_pred = tree_cv.predict(X_test)\n\n# Print the tuned parameters and score\nprint(\"Tuned Extra Tree Parameters: {}\".format(tree_cv.best_params_))\nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n# metrics.accuracy_score(y_pred,y_test) # the same as above\n\nConfMatrix = confusion_matrix(y_test,tree_cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fc3a0df06e5995703681110da50a00a1b148c9d"},"cell_type":"markdown","source":"<a id='rf'></a>\n### Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"fc2207ab3cbbb059fb2195e2565190f054a7a519"},"cell_type":"code","source":"Ran = RandomForestClassifier(n_estimators=50)\nRan.fit(X_train, y_train)\ny_pred = Ran.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Ran, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)), \n      \", Standard deviation: {}\".format(np.std(cv_scores)))\n\nConfMatrix = confusion_matrix(y_test,Ran.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adc267dcc5fe1fe72b616e2eb0f94811bde7804a"},"cell_type":"markdown","source":"### Extra Tree Classifier (reduced features)"},{"metadata":{"trusted":true,"_uuid":"4ac8bade827290bb450addaf036691aa1800ae93"},"cell_type":"code","source":"tree_2= ExtraTreesClassifier()\ntree_2.fit(X_train, y_train)\nprint('Extra-Tree score:',tree_2.score(X_test, y_test))\nprint('Shape of original data:', X_train.shape)\nprint()\ntree_2.feature_importances_\nmodel_reduced = SelectFromModel(tree_2, prefit=True)\nX_reduced = model_reduced.transform(X_train)\nprint('Shape of data with most important features:', X_reduced.shape)\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\nConfMatrix = confusion_matrix(y_test,tree_2.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d9d0c439166bf3dc31bdb4c23806efad8b10923"},"cell_type":"code","source":"cv_tree2 = cross_val_score(logreg_cv, X, y, cv=5, scoring='accuracy')\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_tree)), \n      \"Standard deviation: {}\".format(np.std(cv_tree)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='nn'></a>\n### Nueral-Networks: KERAS-Tensorflow "},{"metadata":{"trusted":true},"cell_type":"code","source":"### The data is not normalized. \n\n## method 1 \npredictors= data.drop('diagnosis', axis=1).values  # .values to conver it to array\ntarget = to_categorical(data.diagnosis.values)\nn_cols = predictors.shape[1]\n\n#np.random.seed(1337) # for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel = Sequential()\n\n# Add layers and nodes\nmodel.add(Dense(50, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy']) \n    \n\n## Fit with 0.3 splitting with early_stopping_monitor with 30 epochs\nearly_stopping_monitor =EarlyStopping(patience=2) \n# Fit the model\n\nhistory=model.fit(predictors, target, validation_split=0.3, epochs=100, batch_size=5,\n                  callbacks = [early_stopping_monitor])\n\n# 1 epoch = one forward pass and one backward pass of all the training examples\n# batch size = number of samples that going to be propagated through the network.\n# The higher the batch size, the more memory space. ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b386c389d396731b7707499cb287a8d607b8128"},"cell_type":"markdown","source":"<a id='fs'></a>\n## Feature Selection\nLet's check the relative importance of features."},{"metadata":{"trusted":true,"_uuid":"48c1c4213297525b1324f0dad7fd821232a9e470"},"cell_type":"code","source":"#RandomForest\nimpor_Forest=Ran.feature_importances_\nindices_1 = np.argsort(impor_Forest)[::-1]\n\n#ExtraTree\nimpor_Extra_tree=tree_2.feature_importances_\nindices_2= np.argsort(impor_Extra_tree)[::-1]\n\nfeatimp_1 = pd.Series(impor_Forest, index=data.columns[1:]).sort_values(ascending=False)\nfeatimp_2 = pd.Series(impor_Extra_tree, index=data.columns[1:]).sort_values(ascending=False)\n\nTable_impor= pd.DataFrame({'ExtraTree': featimp_2,'Random-Forest': featimp_1})\nTable_impor=Table_impor.sort_values('ExtraTree', ascending=False)\nprint(Table_impor)\nprint()\nprint('The six most important features:')\nprint(featimp_1[0:6])\n\nsns.set_context('notebook', font_scale=1.5)\nTable_impor.plot(kind='barh', figsize=(12,10))\nplt.title('Feature importance', size=20);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}