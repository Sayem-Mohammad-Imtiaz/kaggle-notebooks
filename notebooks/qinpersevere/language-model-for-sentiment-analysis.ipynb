{"metadata":{"kernelspec":{"name":"python"},"language_info":{"name":"python","version":"3.5.1"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_cell_guid":"aaabe55a-0433-a4a6-2dd5-22a2433548e8","_active":false,"collapsed":false},"source":"language model for sentiment analysis","execution_count":null,"cell_type":"markdown","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"586d8a6b-a26c-48d6-a1f2-f98507815df2","_active":false},"outputs":[],"source":"%matplotlib inline","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"665488d7-922b-4a92-8219-39d64424c9a2","_active":false},"outputs":[],"source":"import pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\nimport string\n\npd.options.mode.chained_assignment = None\n\nURL = '../input/Tweets.csv'\ndef load_data(url=URL):\n\treturn pd.read_csv(url)\n\ndf = load_data()\ncolumns_to_keep = [u'airline_sentiment',u'retweet_count', u'airline', u'text']\n\ndf = df[columns_to_keep]\n\ndf.loc[:,'sentiment'] = df.airline_sentiment.map({'negative':0,'neutral':2,'positive':4})\ndf = df.drop(['airline_sentiment'], axis=1)\n\ndf = df[df['retweet_count'] <= 2]","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"e0c80692-b783-435b-a1cd-4654c16c55bc","_active":false},"outputs":[],"source":"def clean_tweet(s):\n\t'''\n\t:s : string; a tweet\n\n\t:return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n\t'''\n\ts = re.sub(clean_tweet.pattern_airline, '', s, 1)\n\tremove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n\ts = s.translate(remove_punctuation_map)\n\tsents = sent_tokenize(s)\n\n\twords = [word_tokenize(s) for s in sents]\n\twords = [e for sent in words for e in sent]\n\treturn [clean_tweet.stemmer.stem(e.lower()) for e in words]\n\nclean_tweet.stemmer = PorterStemmer()\nclean_tweet.pattern_airline = re.compile(r'@\\w+')\ndf.loc[:,'text'] = df.loc[:,'text'].map(clean_tweet)\n","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"7b728f2d-758c-47c2-97cf-8494c0be3b59","_active":false},"outputs":[],"source":"def get_stop_words(s, n):\n\t'''\n\t:s : pd.Series; each element as a list of words from tokenization\n\t:n : int; n most frequent words are judged as stop words \n\n\t:return : list; a list of stop words\n\t'''\n\tfrom collections import Counter\n\tl = get_corpus(s)\n\tl = [x for x in Counter(l).most_common(n)]\n\treturn l\n\ndef get_corpus(s):\n\t'''\n\t:s : pd.Series; each element as a list of words from tokenization\n\n\t:return : list; corpus from s\n\t'''\n\tl = []\n\ts.map(lambda x: l.extend(x))\n\treturn l\n\nfreqwords = get_stop_words(df['text'],n=100)\n\nfreq = [s[1] for s in freqwords]\n\nplt.title('frequency of top 100 most frequent words')\nplt.plot(freq)\nplt.xlim([-1,100])\nplt.ylim([0,1.1*max(freq)])\nplt.ylabel('frequency')\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"cefc9c23-c51f-47cd-a7db-8a6a465b9d8f","_active":false},"outputs":[],"source":"print(freqwords[:18])","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"7446c79b-ac65-4c21-b31a-417f2adaa732","_active":false},"outputs":[],"source":"stopwords = [w[0] for w in freqwords[:18]]\nremove_stop_words = lambda x: [e for e in x if e not in stopwords]\ndf.loc[:,'text'] = df.loc[:,'text'].map(remove_stop_words)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"561462e3-a7b0-4e61-8598-23db4bcd8cc7","_active":false},"outputs":[],"source":"import numpy as np\n\nairlines = df['airline'].unique()\ndfs = [df[(df['airline'] == a)] for a in airlines]\n\ndfs = [df for df in dfs if len(df) >= 10]\ndfs = [df.reindex(np.random.permutation(df.index)) for df in dfs]\ndfs = [(df.text, df.sentiment) for df in dfs]","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"4c934c39-b187-47c3-b348-1083e264f34c","_active":false},"outputs":[],"source":"import random\nclass lm(object):\n\t\"\"\"\n\tstatistical language model based on MLE method. Both jelinek-mercer and dirichlet smoothing methods are implemented\n\t\"\"\"\n\tdef __init__(self, a=0.1, smooth_method='jelinek_mercer'):\n\t\tsuper(lm, self).__init__()\n\t\t'''\n\t\t:a : float; discount parameter; should be tuned via cross validation\n\t\t:smooth_method: function; method selected to discount the probabilities\t\n\t\t'''\n\t\tself.a = a\n\t\tsmooth_method = getattr(self, smooth_method)\n\t\tself.smooth_method = smooth_method\n\n\t\t#self.counter = 0\n\n\tdef df_to_ct(self, df):\n\t\tfrom collections import Counter\t\n\t\tl = []\n\t\tdf.map(lambda x: l.extend(x))\n\t\treturn pd.Series(dict(Counter(l)))\n\n\tdef ct_to_prob(self, d):\n\t\ttotal_occur = d.sum()\n\t\treturn d/float(total_occur)\n\n\tdef df_to_prob(self, df):\n\t\t'''\n\t\tdf: list of lists; each containing a document of words, like [[a],[b,c],...]\n\t\tout: pd.Series; the probabilities of each word, like ({a:0.3,b:0.3,...})\n\t\t'''\n\t\treturn self.ct_to_prob(self.df_to_ct(df))\t\n\n\n\tdef fit(self, X, Y):\n\t\t'''\n\t\t:X : pd.Series; features; features are actually a list of words, standing for the document.\n\t\t:Y : pd.Series; labels\n\n\t\t:return : pd.DataFrame; language model\n\t\t'''\n\t\tif len(Y) != 0  and len(X) != 0:\n\t\t\tfrom math import log\n\t\t\tcats = Y.unique()\t\n\t\t\tp_ref = self.df_to_prob(X)\n\t\t\tmodel = pd.DataFrame()\n\t\t\tmodel['unseen'] = (p_ref*self.a).map(log)\n\t\t\tfor c in cats:\n\t\t\t\tidx = Y[Y == c].index\n\t\t\t\tct = self.df_to_ct(X.loc[idx])\n\t\t\t\tp_ml = self.ct_to_prob(ct)\n\t\t\t\tmodel[c] = self.smooth_method(ct, p_ml,p_ref)\n\t\t\t\tmodel[c].fillna(model['unseen'],inplace=True)\n\t\t\tmodel.drop(['unseen'],axis=1,inplace=True)\n\t\t\tself.model = model\n\t\telse: print('input is empty')\n\n\tdef jelinek_mercer(self, ct, p_ml,p_ref,a=0.1):\n\t\tfrom math import log\n\t\tlog_p_s = (p_ml*(1-a)+p_ref.loc[p_ml.index]*a).map(log)\n\t\treturn log_p_s\n\n\tdef dirichlet(self, ct, p_ml,p_ref,a=0.1):\n\t\tfrom math import log\n\t\td = len(p_ml)\n\t\tu = a / (1+a)*d\n\t\tlog_p_s = ((ct+u*p_ref.loc[ct.index])/(d+u)).map(log)\n\t\treturn log_p_s\n\n\t\n\tdef predict_item(self, l, N):\n\t\tmodel = self.model\n\t\t# self.counter += 1\n\t\t# if self.counter % 200 == 0: print self.counter/float(N)\n\t\tin_list = [e for e in l if e in model.index]\n\t\tif not in_list: \n\t\t\treturn model.columns[random.randint(0,len(model.columns)-1)]\n\t\tselected_model =  model.loc[in_list,:]\n\t\ts = selected_model.sum(axis=0)\n\n\t\tlabel = s.loc[s==s.max()].index[0]\n\t\tword = selected_model.loc[selected_model[label] == selected_model[label].max(),:].index[0]\n\t\tself.predwords[label].append(word)\n\t\treturn label\n\n\tdef predict(self, df):\n\t\tself.predwords = dict(zip(self.model.columns,[[] for _ in range((len(self.model.columns)))])) #tricky\n\t\treturn df.map(lambda x: self.predict_item(x,len(df)))\n\n\tdef get_params(self):\n\t\treturn (self.a, self.smooth_method.__name__)\n\t\t\n\tdef get_predictive_words(self, n=3):\n\t\tfrom collections import Counter\n\t\ttotal_words = {k:len(v) for k,v in self.predwords.items()}\n\t\tmost_predictive_words = {k:Counter(v).most_common(n) for k, v in self.predwords.items()}\n\t\tmost_predictive_words = {label:{w:v/float(length) for w, v in words} for label, length, words in zip(total_words.keys(), total_words.values(), most_predictive_words.values())}\n\t\treturn most_predictive_words","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"df367a1f-b0c6-4088-bf1a-af7a62535229","_active":false},"outputs":[],"source":"def cross_validation(clf, X, Y, cv=5, avg=False):\n\t'''\n\t:clf : classifier with fit() and predict() method\n\t:X : pd.DataFrame; features\n\t:Y : pd.DataFrame(1 column) or pd.Series; labels\n\t:cv : int; cross validation folders\n\n\t:return : list of float; cross validation scores\n\t'''\n\n\tk = [int((len(X))/cv*j) for j in range(cv+1)]\n\tscore = [0.0]*cv\n\tfor i in range(cv):\t\n\t\ttrain_x, train_y = pd.concat([X[:k[i]],X[k[i+1]:]]), pd.concat([Y[:k[i]],Y[k[i+1]:]])\n\t\ttest_x, test_y = X[k[i]:k[i+1]], Y[k[i]:k[i+1]]\n\n\t\tclf.fit(X,Y)\n\t\tpred = clf.predict(test_x)\n\n\t\tscore[i] = (pred == test_y).sum()/float(len(test_y))\n\tif avg: return sum(score)/float(len(score))\n\treturn score\n\n\nmodels = [lm()]*len(dfs)\navg_score = [cross_validation(model, X, Y, avg=True, cv=2) for model, (X, Y) in zip(models, dfs)]\nprint(avg_score)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"_cell_guid":"56a02fc1-3419-49af-904e-f56ddf62ea9e","_active":false},"outputs":[],"source":"clf = lm()\n\naxarr = [None]*len(dfs)\n\nfor i in range(len(dfs)):\n\tX, Y = dfs[i]\n\tpt = int(len(X)/2)\n\tclf.fit(X[:pt],Y[:pt])\n\t_ = clf.predict(X[pt+1:])\n\tpredictive_words = clf.get_predictive_words(n=3)\n\tf, axarr[i] = plt.subplots()\n\tf.suptitle('words with top 3 predictive contributions',fontweight='bold')\n\n\tax = axarr[i]\n\tax.set_title(airlines[i])\n\n\tfor senti, words in predictive_words.items():\n\t\ttot_frac = 0.0\n\t\tcolors = ['red','blue','green']\n\t\tct = 0\n\t\tax.set_xticks([0,2,4])\n\t\tax.set_xticklabels(('negative','netural','positive'))\n\t\twidth = 1.2\n\t\t# print words\n\t\tfor w, frac in words.items():\n\t\t\ty = tot_frac + frac / 2.0\n\t\t\tx = senti\n\t\t\tax.text(x, y, w, color='white',fontweight='bold',ha='center')\n\t\t\tif tot_frac == 0.0: ax.bar(x - width/2.0, frac, color = colors[ct], alpha=0.7, width = width)\n\t\t\telse: ax.bar(x - width/2.0, frac, bottom = tot_frac, color = colors[ct], alpha=0.7, width = width)\n\t\t\tct += 1\n\t\t\ttot_frac += frac\nplt.show()","execution_state":"idle"}]}