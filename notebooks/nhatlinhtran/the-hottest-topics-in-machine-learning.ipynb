{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport wordcloud\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-10T03:42:43.4483Z","iopub.execute_input":"2021-09-10T03:42:43.448614Z","iopub.status.idle":"2021-09-10T03:42:43.504601Z","shell.execute_reply.started":"2021-09-10T03:42:43.448584Z","shell.execute_reply":"2021-09-10T03:42:43.503564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src='https://aws1.discourse-cdn.com/standard11/uploads/ainetwork/optimized/1X/b6b7a4a8e9603919a4dbace78c13e9ec92c3dc7d_2_689x492.png' />\n<h1>Project Description</h1>\nThe NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. At each NIPS conference, a large number of research papers are published. Over 50,000 PDF files were automatically downloaded and processed to obtain a dataset on various machine learning techniques. These NIPS papers are stored in datasets/papers.csv. The CSV file contains information on the different NIPS papers that were published from 1987 until 2017 (30 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods and many more.","metadata":{}},{"cell_type":"markdown","source":"# Load the dataset.\n\n- Import the pandas library.\n- Load the papers.csv file from datasets/papers.csv and assign it to the papers variable.\n- Print the first rows of the DataFrame with the head method to verify the file was loaded correctly.","metadata":{}},{"cell_type":"code","source":"papers = pd.read_csv('../input/nips-papers/papers.csv')\npapers.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:43.506465Z","iopub.execute_input":"2021-09-10T03:42:43.50688Z","iopub.status.idle":"2021-09-10T03:42:48.024318Z","shell.execute_reply.started":"2021-09-10T03:42:43.506811Z","shell.execute_reply":"2021-09-10T03:42:48.023421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.025814Z","iopub.execute_input":"2021-09-10T03:42:48.026128Z","iopub.status.idle":"2021-09-10T03:42:48.055993Z","shell.execute_reply.started":"2021-09-10T03:42:48.026089Z","shell.execute_reply":"2021-09-10T03:42:48.055133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data for analysis\n- Remove the id, event_type and pdf_name columns.\n- Print the first rows of the DataFrame with the head method.","metadata":{}},{"cell_type":"code","source":"papers.drop(['id', 'event_type', 'pdf_name'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.05797Z","iopub.execute_input":"2021-09-10T03:42:48.058194Z","iopub.status.idle":"2021-09-10T03:42:48.064718Z","shell.execute_reply.started":"2021-09-10T03:42:48.058166Z","shell.execute_reply":"2021-09-10T03:42:48.063854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.066324Z","iopub.execute_input":"2021-09-10T03:42:48.066578Z","iopub.status.idle":"2021-09-10T03:42:48.083811Z","shell.execute_reply.started":"2021-09-10T03:42:48.066551Z","shell.execute_reply":"2021-09-10T03:42:48.082916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting how machine learning has evolved over time\n- Group the papers by year.\n- Count the number of papers per group (i.e. per year).\n- Visualise these counts per year in a bar plot.","metadata":{}},{"cell_type":"code","source":"groups = papers.groupby(['year'])\ncounts = groups.size()\ncounts.plot(figsize=(10,10), kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.085132Z","iopub.execute_input":"2021-09-10T03:42:48.085917Z","iopub.status.idle":"2021-09-10T03:42:48.546293Z","shell.execute_reply.started":"2021-09-10T03:42:48.085878Z","shell.execute_reply":"2021-09-10T03:42:48.545336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing the text data\n- Load the regular expression library (re).\n- Convert the titles to lowercase using a \"map\" operation.\n- Print the processed titles to verify the results.\n","metadata":{}},{"cell_type":"code","source":"print(papers['title'].head())\n# remove punctuation\npapers['title_processed'] = papers['title'].map(lambda sentence: re.sub('[,\\.!?]','',sentence))\n#Convert title to lowercase\npapers['title_processed'] = papers['title'].map(lambda sentence: sentence.lower())\n\nprint(papers['title_processed'].head())","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.547529Z","iopub.execute_input":"2021-09-10T03:42:48.547753Z","iopub.status.idle":"2021-09-10T03:42:48.581442Z","shell.execute_reply.started":"2021-09-10T03:42:48.547727Z","shell.execute_reply":"2021-09-10T03:42:48.580528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A word cloud to visualize the preprocessed text data\nTransform the data and create a word cloud.\n- Load the wordcloud library.\n- Convert all the processed titles to a single string.\n- Create a WordCloud object.\n- Generate a word cloud.","metadata":{}},{"cell_type":"code","source":"long_string = ''.join(papers['title_processed'])\nwc = wordcloud.WordCloud(width=800, height=400)\nwc.generate(long_string)\nwc.to_image()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:48.58283Z","iopub.execute_input":"2021-09-10T03:42:48.583054Z","iopub.status.idle":"2021-09-10T03:42:49.771007Z","shell.execute_reply.started":"2021-09-10T03:42:48.583025Z","shell.execute_reply":"2021-09-10T03:42:49.770383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the text for LDA analysis\nThe main text analysis method that we will use is latent Dirichlet allocation (LDA). LDA is able to perform topic detection on large document sets, determining what the main 'topics' are in a large unlabeled set of texts. A 'topic' is a collection of words that tend to co-occur often. The hypothesis is that LDA might be able to clarify what the different topics in the research titles are. These topics can then be used as a starting point for further analysis.\n\nLDA does not work directly on text data.First, it is necessary to convert the documents into a simple vector representation. This representation will then be used by LDA to determine the topics. Each entry of a 'document vector' will correspond with the number of times a word occurred in the document. In conclusion, we will convert a list of titles into a list of vectors, all with length equal to the vocabulary. For example, 'Analyzing machine learning trends with neural networks.' would be transformed into [1, 0, 1, ..., 1, 0].\n\nWe'll then plot the 10 most common words based on the outcome of this operation (the list of document vectors). As a check, these words should also occur in the word cloud.\n- Create a CountVectorizer object with the stop_words='english' argument to remove meaningless words.\n- Fit and transform the processed titles with the fit_transform method. Save the results in the count_data variable.\n- Plot the most common words with the helper function (plot10mostcommonwords).\n","metadata":{}},{"cell_type":"code","source":"# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    word_list = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(word_list))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(word_list, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    word_list = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(word_list)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, word_list, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stopwords\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:42:49.772147Z","iopub.execute_input":"2021-09-10T03:42:49.772383Z","iopub.status.idle":"2021-09-10T03:42:50.668075Z","shell.execute_reply.started":"2021-09-10T03:42:49.772335Z","shell.execute_reply":"2021-09-10T03:42:50.667515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysing trends with LDA\n*number_topics* defines the total number of topics in the LDA model. <br>\n*number_words* is only for debugging purposes. It is the number of words that will be printed for each topic. For each topic, the most important words for the topic are selected.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 10\nnumber_words = 10\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T03:48:56.018337Z","iopub.execute_input":"2021-09-10T03:48:56.018656Z","iopub.status.idle":"2021-09-10T03:49:13.214787Z","shell.execute_reply.started":"2021-09-10T03:48:56.018625Z","shell.execute_reply":"2021-09-10T03:49:13.213744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The future of machine learning\nMachine learning has become increasingly popular over the past years. The number of NIPS conference papers has risen exponentially, and people are continuously looking for ways on how they can incorporate machine learning into their products and services.\n\nAlthough this analysis focused on analyzing machine learning trends in research, a lot of these techniques are rapidly being adopted in industry. Following the latest machine learning trends is a critical skill for a data scientist, and it is recommended to continuously keep learning by going through blogs, tutorials, and courses.","metadata":{}}]}