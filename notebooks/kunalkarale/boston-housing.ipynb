{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"#Import libraries which is required for building model\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Get data\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#convert sklearn dataset to dataframe\ndf = pd.DataFrame(boston.data)\ndf.columns = boston.feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's add our target column now\ndf['Target'] = boston.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's check correlation between target variable i.e Target with other variables\nplt.figure(figsize=(12,7))\nplt.title('Correlation Matrix')\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Get X and Y for splitting\nX = df.iloc[:,:-1]\ny = df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's split in 80-20 ratio\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 1. Will begin our model exploration with Linear regression model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nlr = LinearRegression() \nlr.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from equation y = mx +c, let's fetch m term for all attribute other than Target\nlr.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#c value from y = mx + c equation, it will vary slightly based on test_size and random_state selected while splitting\nlr.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Converting the coefficient values to a dataframe\ncoeff = pd.DataFrame([X_train.columns,lr.coef_]).T\ncoeff = coeff.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeff","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check our model on training part which is 80% of whole data\ny_pred_train = lr.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation of model on training data\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nprint('R2:', r2_score(y_train,y_pred_train))\nprint('MAE:', mean_absolute_error(y_train,y_pred_train))\nprint('MSE:', mean_squared_error(y_train,y_pred_train))\nprint('RMSE:', np.sqrt(mean_squared_error(y_train,y_pred_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot a graph for actual price vs predicted price for training data\nplt.scatter(y_train, y_pred_train, c = 'blue') \nplt.xlabel(\"Price: in $1000's\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : Linear Regression\") \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Residual Plot\nplt.scatter(y_pred_train,y_train-y_pred_train, c = 'blue')\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#From grapgh its been clearly seen that residuals are equally distibuted around zero so our choice for selecting regression for this model is good\ny_pred_test = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation of model on 20% test data\nprint('R2:', r2_score(y_test,y_pred_test))\nprint('MAE:', mean_absolute_error(y_test,y_pred_test))\nprint('MSE:', mean_squared_error(y_test,y_pred_test))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred_test)))\nlr_r2_score = r2_score(y_test,y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#R2 of training data and R2 of test data are not varying that much so our model is not overfitting\n#As linear regression is giving around 70% of accuracy, let's try with Random forest regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2. Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Predicting training data using RFR model\ny_pred_train1 = reg.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on RFR Model for training data\nprint('R2:', r2_score(y_train,y_pred_train1))\nprint('MAE:', mean_absolute_error(y_train,y_pred_train1))\nprint('MSE:', mean_squared_error(y_train,y_pred_train1))\nprint('RMSE:', np.sqrt(mean_squared_error(y_train,y_pred_train1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot a graph for actual price vs predicted price for training data\nplt.scatter(y_train, y_pred_train1, c = 'red') \nplt.xlabel(\"Price: in $1000's\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : Random Forest Regression\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Residual Plot\nplt.scatter(y_pred_train1,y_train-y_pred_train1, c = 'red')\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Prediction using RFR Model on test data\ny_pred_test1 = reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on RFR Model for test data\nprint('R2:', r2_score(y_test,y_pred_test1))\nprint('MAE:', mean_absolute_error(y_test,y_pred_test1))\nprint('MSE:', mean_squared_error(y_test,y_pred_test1))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred_test1)))\nrfr_r2_score = r2_score(y_test,y_pred_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Wow, we have beaten linear regression's accuracy of 70% and using RFR we got 85% accuracy, let's explore other models also, lets hope we can beat RFR now","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#3. Import XGBoost Regressor\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor()\n\n# Train the model using the training sets \nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Prediction using XGBoost Model on train data\ny_pred_train2 = xgb.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on XGBoost Model for train data\nprint('R2:', r2_score(y_train,y_pred_train2))\nprint('MAE:', mean_absolute_error(y_train,y_pred_train2))\nprint('MSE:', mean_squared_error(y_train,y_pred_train2))\nprint('RMSE:', np.sqrt(mean_squared_error(y_train,y_pred_train2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot a graph for actual price vs predicted price for training data\nplt.scatter(y_train, y_pred_train2, c = 'green') \nplt.xlabel(\"Price: in $1000's\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : XGBoost\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Residual Plot\nplt.scatter(y_pred_train2,y_train-y_pred_train2, c = 'green')\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Prediction using XGBoost Model on test data\ny_pred_test2 = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on XGBoost Model for test data\nprint('R2:', r2_score(y_test,y_pred_test2))\nprint('MAE:', mean_absolute_error(y_test,y_pred_test2))\nprint('MSE:', mean_squared_error(y_test,y_pred_test2))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred_test2)))\nxgb_r2_score = r2_score(y_test,y_pred_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#ohhhh, that was really very close by margin xgb failed to beat RFR, anyways now lets try with last model SVR, but this model will require feature scaling","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#4. SVR model importing and calling scaler\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\n#Prior to fitting model scale it\n\nsc = StandardScaler()\n\n#Fit only trained data, if we fit test data it will cause data leakage, transform can be done for both\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Fit model using SVR\nsvr = SVR(kernel = 'rbf')\nsvr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Prediction using SVR Model on train data\ny_pred_train3 = svr.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on SVR Model for train data\nprint('R2:', r2_score(y_train,y_pred_train3))\nprint('MAE:', mean_absolute_error(y_train,y_pred_train3))\nprint('MSE:', mean_squared_error(y_train,y_pred_train3))\nprint('RMSE:', np.sqrt(mean_squared_error(y_train,y_pred_train3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot a graph for actual price vs predicted price for training data\nplt.scatter(y_train, y_pred_train3, c = 'purple') \nplt.xlabel(\"Price: in $1000's\") \nplt.ylabel(\"Predicted value\") \nplt.title(\"True value vs predicted value : SVR\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Residual Plot\nplt.scatter(y_pred_train2,y_train-y_pred_train3, c = 'purple')\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Prediction using SVR Model on test data\ny_pred_test3 = svr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation based on SVR Model for test data\nprint('R2:', r2_score(y_test,y_pred_test3))\nprint('MAE:', mean_absolute_error(y_test,y_pred_test3))\nprint('MSE:', mean_squared_error(y_test,y_pred_test3))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred_test3)))\nsvr_r2_score = r2_score(y_test,y_pred_test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#As we can see SVR is performing poor as compared to other models with lowest accuracy of 63% amongst all other models","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Dataframe for models accuracy comparison\n\nacc_comparison = pd.DataFrame({'Model': ['Linear Regression', 'Random Forest Regressor', 'XGBoost', 'SVR'], 'R2 Score': [lr_r2_score*100, rfr_r2_score*100, xgb_r2_score*100, svr_r2_score*100]})\n\nacc_comparison","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's conclude by saying RFR fits best for this dataset, One can also opt for XGBoost model","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}