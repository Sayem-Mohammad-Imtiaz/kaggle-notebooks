{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Avocado Data Analysis (Guided Tutorial For Beginner-Intermediate)\n\nThis notebook will analyze the Avocado Prices Dataset, which analyzes how the prices of different types of avocados change in response to their handling, the date that they were sold and the total volume of what was sold. This dataset examines Hass Avocados, which are commonly used to make avocado toast in homes nationwide. Three types of Hass Avocados are shown in the dataset: PLU 4046 (extra small), PLU 4225 (small) and PLU 4770 (large).\n\nLet's begin our analysis of this dataset by importing the required libraries and getting to look at the dataset.\n\n1. `matplotlib.pyplot` allows us to graph the data and see relationships between them before we do any machine learning.\n\n\n2. `pandas` allows us to visualize our data in a Python Excel Worksheet. We can do many operations in `pandas` that allows us to manipulate the data to see any things that need changing before we undergo machine learning.\n\n\n3. `sklearn` is a machine learning library where we will use algorithms to learn relationships between the data. If we come up with a research question about our data, we can use this library to answer our question using machine learning.\n\n\n4. `tensorflow` is a deep learning library that will be used to see if deep learning can give us a better result for our research question without overfitting the data. *This may be used in an update to this notebook.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n#import tensorflow as tf\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"We begin first by getting to look at our dataset by importing it from its csv file.\n\nThis is done using the `read_csv` method from the `pandas` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/avocado-prices/avocado.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, there is a lot of columns containing both numerical data and categorical data from the dataset.\n\nBefore we can begin our analysis of the columns of the dataset, we should remove the `Unnamed: 0` column since it brings only a duplicate index to our dataset. \n\nThis can be done by dropping the column using the `df.drop` method, specifying an axis of `1` (1 = column, 0 = row). Since we don't need to keep a copy of the dataframe with this column lying around, I use the `inplace` parameter to directly change the DataFrame instead of creating a copy."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Unnamed: 0', axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To start up our data analysis, let's statisically describe our data and find the different regions where avocados are grown using the `region` column. A statistically description of the data is done with the `df.describe()` method of the dataset."},{"metadata":{},"cell_type":"markdown","source":"Notice that `df.describe()` only provides statistical analysis of the numerical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Statistical analysis of the categorical columns of the dataframe can be done by specifying the datatype of the columns to be described. For categorical data, we need to specify the `np.object` as a parameter into the keyword argument `include` in the method."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=np.object)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see the different regions where avocados are grown by isolating the `region` column from the dataframe and using the `value_counts()` method. This method will count the frequency of each region in the column and note it in a column (or specifically a `pd.Series`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['region'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that avocados in this dataset are equally recorded from each of the different cities listed above. I gather this from the frequency of 338 records in the dataset for each U.S. city. However, there are 335 records for the West Texan / New Mexican area. \n\nIf I didn't have as much of a keen eye, I could visualize some of these records in a *horizontal bar graph* as seen below."},{"metadata":{},"cell_type":"markdown","source":"### Primer To Plots"},{"metadata":{},"cell_type":"markdown","source":"Plots will be discussed in the machine learning club in later detail but there are essentially three things to know in order to create a generic plot using `matplotlib`. These steps apply to any plot except a histogram or pie plot.\n\n1. The first argument in the `plot` function must be the x-values to put into the plot.\n\n\n2. The second argument in the `plot` function must be the y-values to put into the plot.\n\n\n3. Optional Arguments can be added in the `plot` function to change the color and style of the plot. In this example, I used the `color` keyword argument to provide my own hex color to color the inside of each of the bars. The `edgecolor` argument was also used to give a black outline to each bar to emphasize their length.\n\nIn addition to these three steps, you might want to use the `plt.title`, `plt.xlabel` and `plt.ylabel` methods to give your plot a title and labels for your x and y axis. Plots are a good way to visualize your data so try to make as many informative plots as possible.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.barh(df['region'].unique()[45:54], df['region'].value_counts().values[45:54], color = '#e57373', edgecolor ='black')\nplt.xlabel('Cities')\nplt.ylabel('Number of Records')\nplt.title('Sample of Records of Avocados Sold In Different Cities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time Plots"},{"metadata":{},"cell_type":"markdown","source":"Make sure to convert the **Date** column to `datetime` using `pd.to_datetime` (converts from `object` to `datetime`) in order to get a time line plot."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"albany_df = df[df['region'] == 'Albany'].sort_values('Date')\nalbany_df['Date'] = pd.to_datetime(albany_df['Date'])\nalbany_avocado_median = [albany_df['AveragePrice'].median()] * len(albany_df['AveragePrice'])\n\nplt.plot(albany_df['Date'], albany_df['AveragePrice'])\nplt.plot(albany_df['Date'], albany_avocado_median, color=\"#e57373\")\nplt.xlabel('Month Avocado Sold')\nplt.ylabel('Average Price Sold')\nplt.title('Average Price of Avocados in Albany, NY over 40 months.')\n\nplt.gcf().autofmt_xdate()\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"albany_avocado_median = [albany_df['Total Volume'].median()] * len(albany_df['Total Volume'])\n\nplt.plot(albany_df['Date'], albany_df['Total Volume'])\nplt.plot(albany_df['Date'], albany_avocado_median, color=\"#e57373\")\nplt.xlabel('Month Avocado Sold')\nplt.ylabel('Average Price Sold')\nplt.title('Average Price of Avocados in Albany, NY over 40 months.')\nplt.gcf().autofmt_xdate()\nplt.yscale('log')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tampa_df = df[df['region'] == 'Tampa'].sort_values('Date')\ntampa_df['Date'] = pd.to_datetime(tampa_df['Date'])\ntampa_avocado_median = [tampa_df['AveragePrice'].median()] * len(tampa_df['AveragePrice'])\n\nplt.plot(tampa_df['Date'], tampa_df['AveragePrice'], color=\"#ef5350\")\nplt.plot(tampa_df['Date'], tampa_avocado_median, color=\"#64b5f6\")\nplt.xlabel('Month Avocado Sold')\nplt.ylabel('Average Price Sold')\nplt.grid()\nplt.title('Average Price of Avocados in Tampa, FL over 40 months.')\nplt.gcf().autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lv_df = df[df['region'] == 'LasVegas'].sort_values('Date')\nlv_df['Date'] = pd.to_datetime(lv_df['Date'])\nny_avocado_median = [lv_df['AveragePrice'].median()] * len(lv_df['AveragePrice'])\n\nplt.plot(lv_df['Date'], lv_df['AveragePrice'], color=\"#ff9800\")\nplt.plot(lv_df['Date'], ny_avocado_median, color=\"#4db6ac\")\nplt.xlabel('Month Avocado Sold')\nplt.ylabel('Average Price Sold')\nplt.grid()\nplt.title('Average Price of Avocados in Las Vegas, NV over 40 months.')\nplt.gcf().autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Other Data Exploration Plots"},{"metadata":{},"cell_type":"markdown","source":"This plot asks the EDA question: \"**Do organic avocados cost more than conventional avocados on average?**\""},{"metadata":{"trusted":true},"cell_type":"code","source":"conventional_price = df[df['type'] == 'conventional']['AveragePrice']\norganic_price = df[df['type'] == 'organic']['AveragePrice']\n\n\nprice_conv_mean = conventional_price.mean()\nprice_conv_std = conventional_price.std()\n\nprice_org_mean = organic_price.mean()\nprice_org_std = organic_price.std()\n\nplt.bar(['conventional', 'organic'], [price_conv_mean, price_org_mean], \n        yerr=[price_conv_std, price_org_std], capsize=3, color=[\"#fbc02d\", \"#aed581\"], edgecolor='black')\n\nplt.grid()\nplt.xlabel('Type of Avocado Grown')\nplt.ylabel('Average Price')\nplt.title('Effect of Avocado Type on Average Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems from the error bars overlapping that there is no clear way to see if the increased price for organic avocados is a statistically significant increase in price from conventional avocados. I will use a *t-test* to see if the p-value for this difference is statistically significant.\n\n1. Implement a *t-test* function and get the *t* value. \n2. Use a t-chart with the appropriate degrees of freedom to find the corresponding p-value."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import ttest_ind\n\nttest = ttest_ind(conventional_price[:5], organic_price[:5])\n\nprint(\"The p value for the t-test to find the statistical difference between conventional and organic avocados is \" + \n      \"{:7f}\".format(ttest.pvalue))\nprint(\"The t value for this t-test is {:3f}\".format(ttest.statistic))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With only a sample of 5 values from both populations, the *p-value* is already less than the most veriafiable value of statistical signficance at $\\alpha = 0.05$, the difference of the prices between organic and conventional avocados is statistically significant.\n\n**No wonder organic avocados cost more!**"},{"metadata":{},"cell_type":"markdown","source":"----\nThere was also an inspirational question when I downloaded this dataset about asking ***\"Was the Avocadopocalypse of 2017 real?***\n\nIn layman's terms, this is asking if there was a shortage in 2017 compared to other years in the dataset.\n\nTo answer this question, we need to graph the combined quantities of avocados over the different years and see if the combined quantity of avocados in 2017 was less than during 2015, 2016 and 2018."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_years = df['year'].unique()\n\n# This dict comprehension creates a dict with the structure {2015: Volume Mean For The Year}\n\ncombined_means = {y: df[df['year'] == y]['Total Volume'].mean() for y in unique_years}\ncombined_stds = {y: df[df['year'] == y]['Total Volume'].std() for y in unique_years}\n\nplt.bar(unique_years, combined_means.values(),\n        color=['#66bb6a', '#66bb6a', '#4db6ac', '#66bb6a'], edgecolor='black', yerr=combined_stds)\n\nplt.xticks(range(2015, 2019))\nplt.grid()\nplt.title('Combined Quantity of Avocados Sold Over 3 Years')\nplt.xlabel('Year')\nplt.ylabel('Quantity of Avocados Sold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, it seems that 2015 was the year with lowest number of avocados sold. The number of avocados sold in 2017 seems to match the amount sold in 2016 with the number of avocados sold increasing in 2018."},{"metadata":{},"cell_type":"markdown","source":"---\n\nNow, let's check for any null data to check if we need to impute anything."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sweet, no null values!\n\n## Feature Engineering\n\nFor machine learning, we will likely not use the `Date` column. Now, we have to do feature engineering to find the columns that will likely not add any additional data for our classifier.\n\nWe will start by trying to find possible correlations among the data.\n\n### Correlations"},{"metadata":{},"cell_type":"markdown","source":"However, we will first need to define our target variable and our independent features before we start any feature engineering. So let's square that away by isolating the **average price** from the dataframe as our target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('AveragePrice', axis = 1)\ny = df['AveragePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Heatmap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from seaborn import heatmap\n\nheatmap(df.corr(), cmap='summer', annot=True, linecolor='black', fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_X = X._get_numeric_data()\ncategorical_X = X.select_dtypes(np.object)\ncat_X = categorical_X.drop('Date', axis = 1)\n\ncat_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_threshold = 0.6\ncorr_features = []\n\nprint(numerical_X.columns)\nprint()\n\nfor col in df._get_numeric_data().columns:\n    if abs(df._get_numeric_data()['AveragePrice'].corr(df[col])) >= corr_threshold:\n        corr_features.append(col)\n\n        \n# Columns are so correlated to one another! This method might not be the best way to do things.\n# See Vishal Patel's Dimensionality Reduction lecture for more ways to do this\n        \nprint(corr_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lecture: [Press Here](https://www.slideshare.net/VishalPatel321/feature-reduction-techniques)."},{"metadata":{},"cell_type":"markdown","source":"### Variance Removal\n\nIt seems like most of the numerical data is correlated to each other *but* before removing nearly all of the numerical data because of this, let's explore other options in removing some of our features.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nvthresh = VarianceThreshold()\nquasi_vthresh = VarianceThreshold(0.1)\n\nXp = vthresh.fit_transform(numerical_X)\nXpp = quasi_vthresh.fit_transform(numerical_X)\n\nprint(f\"Original Numerical DataFrame Shape: {X.shape}\\n\", \n      f\"With Variance: {Xp.shape}\\n\", f\"With At Least 0.1 variance: {Xpp.shape}\")\n\ndel Xpp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This `VarianceThreshold` has a great effect but not as great of an effect as does `SelectKBest`. Since categorical values do not merge well with numerical variables with `VarianceThreshold`, `SelectKBest` is used in this analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The k parameter will be tuned in a grid search. For right now, k = 5 seems to be the best value to use.\nkbest = SelectKBest(f_regression, k = 5)\nXk = kbest.fit_transform(numerical_X, y)\n\nXk.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"Now, let's test the results of the feature engineering with some basic machine learning. After this, we'll add some grid search to get the best hyperparameters for the model.\n\n<u> Research Question:</u> **How can the information about avocado distribution and volume sold predict the price of avocados sold?**\n\n1. Plot the data in a scatter plot to see an expected regression.\n2. Find a model to compute regression."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.scatter(df['Total Volume'], df['AveragePrice']) \nplt.xlabel('Total Volume of Avocados Sold (hundred thousands)')\nplt.ylabel('Average Price of Avocados')\nplt.title('Total Volume vs. Average Price on Avocados')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features & ColumnSelector"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\ncat_columns = tuple(cat_X.columns)\nnum_columns = tuple(numerical_X.columns)\n\ncol_transform = make_column_transformer(\n    (OneHotEncoder(), cat_columns),\n    (make_pipeline(SelectKBest(f_regression, k = 5), StandardScaler(with_mean=False)), num_columns)\n)\nX_transform = col_transform.fit_transform(X, y)\ny_transform = y.values.reshape((-1, 1))\n\n# From tinkering (ColumnTransformer is still a bit new to me), k = 5 seems to be the best parameter for SelectKBest\n\ntrain_x, test_x, train_y, test_y = train_test_split(X_transform, y_transform)\nprint(train_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall that the target variable `y` has the price of the avocados and the feature matrix `X` has all of the other items. The `ColumnSelector` will only modify columns that are numerical or categorical. All other columns will be dropped (i.e. **Date**)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are also 2 unique values for the `type` column and 54 unique value for the `region` column that the `ColumnTransformer` must one-hot encode."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_types = X['type'].unique()\nunique_types, len(unique_types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_regions = X['region'].unique()\nunique_regions, len(unique_regions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Selection\n\nLet's start with a simple Linear Regression using only the preprocessing in the `ColumnSelector` and observe the results with the root mean squared error. The $R^2$ result is a bit murky so I will skip using that for now.\n\nThe cross validation score is used for the final testing score. \n\n#### 1. LinearRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlr = LinearRegression()\n\nlr.fit(X_transform, y_transform)\ny_pred = lr.predict(X_transform)\nprint(f'Train RMSE: {(mean_squared_error(y_transform, y_pred)) ** 0.5}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(train_x, train_y)\ny_pred = lr.predict(test_x)\n\nprint(f\"Test RMSE: {(mean_squared_error(test_y, y_pred)) ** 0.5}\")\n\ncvscore = cross_val_score(lr, X_transform, y_transform, cv=5, scoring=\"neg_mean_squared_error\")\ncv_rmse = (-1 * cvscore) ** 0.5\n\nprint(f\"Cross Validation RMSE: {cv_rmse.mean()} +/- {cv_rmse.std()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Coefficients: {lr.coef_}\")\nprint(lr.coef_.shape) # these are for all of the columns that were kept with the SelectKBest \nprint(f\"Intercepts: {lr.intercept_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the training and testing RMSE are close together, this shows that proper preprocessing was done to create a properly fit model with no overtraining."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Ridge Regression\nUsing a `LinearRegression` algorithm is not that bad with a RMSE of 0.27 but I can do better with nonlinear models. Despite the training and validation accuries being close together, let's see if regularizing the regression using RidgeRegression via `RidgeCV` will lead to any results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n#ridge = RidgeCV(alphas=np.linspace(0.01, 10, 50)) # sample from a wide variety of alpha regularizations.\n\nridge = Ridge(alpha=0.7)\nridge.fit(train_x, train_y)\ny_pred = ridge.predict(test_x)\n\nprint(f'Test RMSE Ridge: {(mean_squared_error(test_y, y_pred)) ** 0.5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perhaps using regularization was not the best case (not much variation in the RMSE for different alphas) so we have to check other regression models now such as support vector machines and decision trees."},{"metadata":{},"cell_type":"markdown","source":"#### 3. DecisionTreeRegressor\n\nLet's try a decision tree and random forest before building the full pipeline for the model in machine learning. After that, we'll try a deep learning approach before closing the notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\ndtree = DecisionTreeRegressor(max_depth = 2)\n\ndtree.fit(train_x, train_y)\ny_pred = dtree.predict(test_x)\n\nprint(f'Test RMSE: {(mean_squared_error(test_y, y_pred)) ** 0.5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After noticing a trend that increasing the depth of the tree gives lower RMSE, let's use a `GridSearch` to find the optimal depth and maximum samples for a leaf. \n\nNote that the maximum for the negative mean squared error also finds the minimum for the RMSE from calculus optimization."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n#param_grid = [{'max_depth' : np.arange(1, 11), 'max_leaf_nodes' : [50, 100, 150, 200, 250, 300]}]\nparam_grid = [{'max_depth' : np.arange(10, 20), 'max_leaf_nodes' : np.arange(500, 601, 10)}]\n\ngsearch = GridSearchCV(dtree, param_grid, cv=5, scoring='neg_mean_squared_error')\ngsearch.fit(train_x, train_y)\n\ngsearch.best_params_\n\n# Upon running the GridSearch, the max values of 10 and 300 were obtained. I need to broaden the range to find a \n# more optimal parameter if it hit the max\n\n# New optimal parameters: depth = 18, max_leaf_nodes (maximum # of leaf nodes) = 520","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now with optimal parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree = DecisionTreeRegressor(max_depth = 18, max_leaf_nodes = 520)\n\ndtree.fit(train_x, train_y)\ny_pred = dtree.predict(test_x)\n\nprint(f'Test RMSE: {(mean_squared_error(test_y, y_pred)) ** 0.5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. RandomForestRegressor\n\nLet's try it with the optimal parameters from the decision tree before optimizing the forest itself."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"rforest = RandomForestRegressor(max_depth = 18, max_leaf_nodes = 520, n_estimators=250)\n\nrforest.fit(train_x, train_y.ravel())\ny_pred = rforest.predict(test_x)\n\nprint(f'Test RMSE: {(mean_squared_error(test_y, y_pred)) ** 0.5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the **RandomForestRegressor** was the winner out of all of the regressors due to the lower RMSE of the decision tree compared to ridge and linear regression. "},{"metadata":{},"cell_type":"markdown","source":"## Thanks for Tuning In!\n\nI hope you learned something if you are a beginner. Using a deep neural network might be overkill in this scenario since the RMSE for simple machine learning solutions are low enough to be used for production processes. Of course if you add to the parameter space or number of estimators in voting classifiers like random forest classifiers, it takes longer for the machine learning algorithm to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}