{"cells":[{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":" # Predicting the Happiness of a Nation Based on it's Development\nThis project aims to explore the link between the happiness of a nation and it's economic development. "},{"metadata":{"Collapsed":"false","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport csv\nimport sqlite3 \n\nfrom tqdm import tqdm_notebook as bar \nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"print(os.listdir(\"../input/world-happiness\"))","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"# Data Cleaning and Combining\nThe data for this project comes from two data sets and thus will require cleaning for various possible errors. One dataset contains the world happiness report from 2015, the other contains the economic development indicators. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"#reading the happiness data into a data frame\nHappinessDF=pd.read_csv(\"../input/world-happiness/2015.csv\")\nprint(HappinessDF.dtypes)\nHappinessDF.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"The development data can be read from csv files but I will read it from the sqlite database to demonstrate my SQL knowledge.  \n\nThe data base has various development indicators stored in the indicators table along with country codes,year and indicator code. The countries in the happiness data may have different names from those in indicator database so it makes sense to first find what countries in the happiness data set do not appear in the indicators data set. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"connection = sqlite3.connect(\"../input/world-development-indicators/database.sqlite\")\ncursor=connection.cursor()\n\ndef DataFrameTable(query):\n    return pd.read_sql_query(query,connection)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"#initiallty need to read country names in a Data Frame \nquery=\"\"\"\n    SELECT ShortName  \n    FROM Country\n\"\"\"\nDevelCountryNames=DataFrameTable(query)\nDevelCountryNames","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"# Now list the non matching countries\nNonMatches=[]\nfor country in HappinessDF[\"Country\"]:\n    if country not in DevelCountryNames[\"ShortName\"].tolist():\n        NonMatches.append(country)\nprint(NonMatches)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Some of these countries are not entirely recognized such as **'Palestinian Territories**, **'Somaliland Region'**, **'North Cyprus'** and **'Taiwan'** but do go by some alternative names like **Palestine**, the **Turkish Republic of Northern Cyprus** and the **Republic of China**. **'Hong Kong'** does not tend to go by any other names but is now technically part of China so has likely not been included separately because of that. **'Congo (Kinshasa)'** appears to represent the **Demorcratic Republic of the Congo** while **'Congo (Brazzaville)'** represents the **Republic of the Congo**. The **'Ivory Coast'** may be go by it's French name **Côte d'Ivoire**. Kyrgyzstan's offical name is **Kyrgyz Republic**. Laos's official name is **Lao People's Democratic Republic**. Slovakia's official name is the **Slovak Republic**."},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"SearchTerms=[\"Congo\",\"Ivoire\",\"Kyrgyzstan\",\"Kyrgyz\",\"Syria\",\"Lao\",\"Slovak\",\"Palestine\",\"Somaliland\",\"Cyprus\",\"China\",\"Korea\"]\nSearchQuery=\"SELECT ShortName FROM Country WHERE\"\nfor SearchTerm in tqdm(SearchTerms):\n    SearchQuery+=\" ShortName LIKE '%\"+SearchTerm+\"%' OR\"\nSearchQuery=SearchQuery[:-3]\nprint(SearchQuery)\n    \n\nDataFrameTable(SearchQuery)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Now we can assign the countries in the happiness data that didn't directly match countries in the development data.\n\n| Happiness Data | Development Data |\n|----------------|------------------|\n| Taiwan         |      N/A         |\n| Slovakia       | Slovak Republic  |\n| South Korea    | Korea            |\n| North Cyprus   | N/A              |\n| Hong Kong      | Hong Kong SAR, China |\n| Kyrgyzstan     | Kyrgyz Republic  |\n| Somaliland region | N/A           |\n| Laos           | Lao PDR          |\n| Palestinian Territories| N/A      |\n| Congo (Kinshasa) | Dem. Rep. Congo|\n| Congo (Brazzaville) | Congo       |\n| Ivory Coast    | Côte d'Ivoire    |\n| Syria          | Syrian Arab Republic|\n\n\n\nNo we need to read the add all the matching countries to a labeled Data Frame with the development factors. The development factors are stored in the Indicators table which contains the columns CountryName, CountryCode, IndicatorName, IndicatorCode, Year, Value. The Country table allows us to the turn a CountryCode into a ShortName. Also the year needs to be reasonably close to the 2000s the The happiness data is from 2015. Must create a list of the countries to query as they appear in the ShortName column of the Country table. Find their country codes and query the indicator table to find the the indicator name and value. All this information can then be added to a table with columns of Country,Happiness,Indicators.  \n\n"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"#first create the list of countries \nCountrysList = HappinessDF[\"Country\"].tolist()\n\nCountrysToRemove=[\"Taiwan\",\"North Cyprus\",\"Somaliland region\",\"Palestinian Territories\"]\n\nCountrysToReplace={\n    \"Slovakia\":\"Slovak Republic\",\n    \"South Korea\":\"Korea\",\n    \"Hong Kong\":\"Hong Kong SAR, China\",\n    \"Kyrgyzstan\":\"Kyrgyz Republic\",\n    \"Laos\":\"Lao PDR\",\n    \"Congo (Kinshasa)\":\"Dem. Rep. Congo\",\n    \"Congo (Brazzaville)\":\"Congo\",\n    \"Ivory Coast\":\"Côte d''Ivoire\",\n    \"Syria\":\"Syrian Arab Republic\"} \n    #This can used to convert when needed\n\n\nfor Country in CountrysToRemove:\n    CountrysList.remove(Country)\n        \nfor Country in CountrysToReplace.keys():\n    CountrysList[CountrysList.index(Country)]=CountrysToReplace[Country]\n\nprint(CountrysList)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"There are 1344 indicators in total so we must pick the ones most likely to affect happiness. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"\nIndicatorQuery=\"\"\"  \nSELECT IndicatorName,max(Value),min(Value)\nFROM Indicators\nGROUP BY IndicatorName\n\"\"\" #max and min values is to investigate values\nIndicators=DataFrameTable(IndicatorQuery)\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"Indicators.head()\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"pd.set_option('max_colwidth', 120)\npd.set_option(\"max_rows\",1400)\n\n#used to pick the indecators of interest uncomment to print them all \n#print(Indicators)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Lets start with some general stats that are likely to affect happiness or ones that it will be interesting to see the effect of on happiness. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorsList=[\n\"Access to electricity (% of population)\"\n,\"Adjusted net enrolment rate, primary, both sexes (%)\"\n,\"Adolescent fertility rate (births per 1,000 women ages 15-19)\"\n,\"Adult literacy rate, population 15+ years, both sexes (%)\"\n,\"Arable land (hectares per person)\"\n,\"Average precipitation in depth (mm per year)\"\n,\"Bribery incidence (% of firms experiencing at least one bribe payment request)\"\n,\"Central government debt, total (% of GDP)\"\n,\"Community health workers (per 1,000 people)\"\n,\"Currency composition of PPG debt, U.S. dollars (%)\"\n,\"Death rate, crude (per 1,000 people)\"\n,\"Droughts, floods, extreme temperatures (% of population, average 1990-2009)\"\n,\"Emigration rate of tertiary educated (% of total tertiary educated population)\"\n,\"Expenditure on education as % of total government expenditure (%)\"\n,\"Fixed broadband subscriptions (per 100 people)\"\n,\"GDP per capita (constant 2005 US$)\"\n,\"GDP per capita growth (annual %)\"\n,\"Improved sanitation facilities (% of population with access)\"\n,\"Income share held by highest 10%\"\n,\"Income share held by highest 20%\"\n,\"Internet users (per 100 people)\"\n,\"Life expectancy at birth, total (years)\"\n,\"Long-term unemployment (% of total unemployment)\"\n,\"Mobile cellular subscriptions (per 100 people)\"\n,\"Net enrolment rate, secondary, both sexes (%)\"\n,\"Net migration\"\n,\"Percentage of students in secondary education who are female (%)\"\n,\"Population density (people per sq. km of land area)\"\n,\"Population, total\"\n,\"Poverty gap at $3.10 a day (2011 PPP) (%)\"\n,\"Refugee population by country or territory of origin\"\n,\"Tax revenue (% of GDP)\"\n,\"Urban population (% of total)\"\n]\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"To start with I will I will use the most recent data for each indicator and see how many data points are missing."},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def ListToString(List):\n    tup=\"(\"\n    for x in List:\n        tup+=\"'\"+str(x)+\"',\"\n    tup=tup[:-1]+\")\"\n    return tup\n    \n# takes a long time to run, there is no pivot function it in SQL lite so instead I will pivot with pandas\nquery = \"\"\"\nSELECT data.CountryName,data.IndicatorName,data.Year,I.Value\nFROM\n(SELECT Country.ShortName AS CountryName,Indicators.CountryCode,IndicatorName,IndicatorCode,MAX(Year) AS Year\nFROM Indicators,Country\nWHERE Indicators.CountryCode = Country.CountryCode\nAND Country.ShortName IN \"\"\"+ListToString(CountrysList)+\"\"\"\nAND Indicators.IndicatorName IN \"\"\"+ListToString(IndicatorsList)+\"\"\"\nGROUP BY Country.ShortName,Indicators.CountryCode,IndicatorName,IndicatorCode) AS data\nLEFT JOIN Indicators I ON data.CountryCode = I.CountryCode AND data.IndicatorCode = I.IndicatorCode and I.Year = data.Year\n;\n\"\"\"\n\nIndicatorData=DataFrameTable(query)\nIndicatorData.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorData=IndicatorData.pivot(\"CountryName\",\"IndicatorName\",\"Value\")\nIndicatorData.columns.name = None \nIndicatorData.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"print(\"There are\",len(IndicatorData)-len(IndicatorData.dropna()),\"countries with missing data out of\",len(IndicatorData))\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorData.describe()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Lets find the the average year for the data points if we select the most recent non-null data point for each country. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"query = \"\"\"\nSELECT AVG(data.Year) AS AverageMostRecentNonNullYearForEachFeature\nFROM\n(SELECT MAX(Year) AS Year\nFROM Indicators,Country\nWHERE Indicators.CountryCode = Country.CountryCode\nAND Country.ShortName IN \"\"\"+ListToString(CountrysList)+\"\"\"\nAND Indicators.IndicatorName IN \"\"\"+ListToString(IndicatorsList)+\"\"\"\nAND Indicators.Value IS NOT NULL\nGROUP BY Country.ShortName,Indicators.CountryCode,IndicatorName,IndicatorCode) AS data\n;\n\"\"\"\n\nAverageMostRecentNonNullYear=DataFrameTable(query)\ndisplay(AverageMostRecentNonNullYear.head())\n\nquery = \"\"\"\nSELECT Count(data.Year) AS NumberOfNonNullValues\nFROM\n(SELECT MAX(Year) AS Year\nFROM Indicators,Country\nWHERE Indicators.CountryCode = Country.CountryCode\nAND Country.ShortName IN \"\"\"+ListToString(CountrysList)+\"\"\"\nAND Indicators.IndicatorName IN \"\"\"+ListToString(IndicatorsList)+\"\"\"\nAND Indicators.Value IS NOT NULL\nGROUP BY Country.ShortName,Indicators.CountryCode,IndicatorName,IndicatorCode) AS data\n;\n\"\"\"\nNumberOfNonNulls=DataFrameTable(query)\ndisplay(NumberOfNonNulls.head())\n\nprint(\"There should be\",len(IndicatorData.index)*len(IndicatorData.columns),\"values.\")","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"These data points are recent enough on average and there isn't many missing so I will use these instead. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"query = \"\"\"\nSELECT data.CountryName,data.IndicatorName,data.Year,I.Value\nFROM\n(SELECT Country.ShortName AS CountryName,Indicators.CountryCode,IndicatorName,IndicatorCode,MAX(Year) AS Year\nFROM Indicators,Country\nWHERE Indicators.CountryCode = Country.CountryCode\nAND Country.ShortName IN \"\"\"+ListToString(CountrysList)+\"\"\"\nAND Indicators.IndicatorName IN \"\"\"+ListToString(IndicatorsList)+\"\"\"\nAND Indicators.Value IS NOT NULL\nGROUP BY Country.ShortName,Indicators.CountryCode,IndicatorName,IndicatorCode) AS data\nLEFT JOIN Indicators I ON data.CountryCode = I.CountryCode AND data.IndicatorCode = I.IndicatorCode and I.Year = data.Year\n;\n\"\"\"\n\nIndicatorData=DataFrameTable(query)\nIndicatorData=IndicatorData.pivot(\"CountryName\",\"IndicatorName\",\"Value\")\nIndicatorData.columns.name = None \ndisplay(IndicatorData.head(5))\ndisplay(IndicatorData.describe())","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorData.to_csv(\"IndicatorData.csv\")","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"![](http://)<a href=\"IndicatorData.csv\"> Download File </a>"},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"### (Can run notebook from here)"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\n\nIndicatorData=pd.read_csv(\"../input/development-data/IndicatorData.csv\")\nIndicatorData=IndicatorData.set_index([\"CountryName\"])\ndisplay(IndicatorData.head())\nIndicatorData.describe()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Lets keep the columns that have at least 147 entries. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"Data=IndicatorData[[col for col in IndicatorData.columns if IndicatorData[col].count()>=147]]\nprint(\"We now have\",len(Data.columns),\"features instead of\",len(IndicatorData.columns))\nprint(\"If we drop counries with missing data we have\",len(Data.dropna()),\"countries out of\",len(Data))","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"We don't lose too many countries if we just drop the ones with missing data so lets do that. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"Data=Data.dropna()\nIndicatorDF=Data\ndisplay(Data.columns)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Now we need to adjust some values to per capita values. After that we can begin exploratory data analysis.\n\nFactors that need adjusting to something per person are:\n* Refugee population by country or territory of origin\n\nFactors needed for modification that then need to be removed are:\n* Population, total"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorDF[\"Refugee Rate\"]=IndicatorDF[\"Refugee population by country or territory of origin\"]/IndicatorDF[\"Population, total\"]\nIndicatorDF=IndicatorDF.drop(columns=[\"Refugee population by country or territory of origin\"])","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"HappinessFileString=\"../input/world-happiness/2015.csv\"\nHappinessDF=pd.read_csv(HappinessFileString)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"IndicatorDF.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Need to raplace the country names in the index so that we can join the data sets on country names."},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"CountrysToReplace={\n    \"Slovakia\":\"Slovak Republic\",\n    \"South Korea\":\"Korea\",\n    \"Hong Kong\":\"Hong Kong SAR, China\",\n    \"Kyrgyzstan\":\"Kyrgyz Republic\",\n    \"Laos\":\"Lao PDR\",\n    \"Congo (Kinshasa)\":\"Dem. Rep. Congo\",\n    \"Congo (Brazzaville)\":\"Congo\",\n    \"Ivory Coast\":\"Côte d''Ivoire\",\n    \"Syria\":\"Syrian Arab Republic\"} \n\nIndex=HappinessDF[\"Country\"].tolist()\nIndex=[country if country not in CountrysToReplace.keys() else CountrysToReplace[country] for country in Index ]\nHappinessDF.index=Index\nHappinessDF=HappinessDF[[\"Happiness Score\"]]\nHappinessDF.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"DataDF=IndicatorDF.join(HappinessDF,how=\"inner\")\nDataDF.head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"# Exploritory Data Analysis"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"DataDF.info()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"DataDF.describe()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"DataDF.hist(figsize=(40,40),bins=50, xlabelsize=10, ylabelsize=10)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"import seaborn as sns\n\nfor i in range(0, len(DataDF.columns), 5):\n    sns.pairplot(data=DataDF,x_vars=DataDF.columns[i:i+5],y_vars=[\"Happiness Score\"],height=5)\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Most of these features appear to have either no relationship or a linear relationship with happiness, however some such as GDP per capita appear to have a higher order relationship. These non linear relationships will likely not be captured well by linear models. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"CorMatrix=DataDF.corr()\nCorMatrix.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Lots of these features are strongly correllated with one another, so it will likely be worth while to try and ridge and lasso regression. "},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"# **Model Creation**\nI will use cross validation as there is not a huge amount of data. Hyper parameters will be tuned on the same data set that is used to score the data, in reality this isn't good practice but (data leakage) but there isn't much data. \n### Linear Regression\nWe begin with a simple linear regression model. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"X=DataDF.drop(columns=[\"Happiness Score\"]).copy()\nY=DataDF[\"Happiness Score\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\n\nmodel=LinearRegression(normalize=True)\nparams={}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(X,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nimport matplotlib.pyplot as plt\n\nmodel=Lasso(normalize=True)\nparams = {\n    \"alpha\":np.logspace(-5,1)\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(X,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])\n\n\nfor param,values in params.items():\n    plt.plot( results[\"param_\"+param],results[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.ylabel(r\"$R^2$\")\n    plt.xlabel(param)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Lasso regression does not improve results, the best parameter for alpha is the one that makes it most like linear regression. "},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"### Ridge Regression "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel=Ridge(normalize=True)\nparams = {\n    \"alpha\":np.logspace(-6,1)\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(X,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])\n\nfor param,values in params.items():\n    plt.plot( results[\"param_\"+param],results[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.ylabel(r\"$R^2$\")\n    plt.xlabel(param)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Ridge regression has given a slight improvement. Lets try some non linear models to see if we can improve the results. "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\ntransformer= PolynomialFeatures(degree=2)\nPolyX = transformer.fit_transform(X.copy())","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\nmodel=LinearRegression(normalize=True)\n\nresults=pd.DataFrame(data=cross_validate(model,PolyX,Y,cv=5,return_train_score=True),index=range(1,6))\nresults.index=results.index.rename(\"Fold\")\ndisplay(results)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Clearly we are over-fitting and need to try Ridge and Lasso regression.\n\n### Polynomial Lasso Regression"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel=Lasso(normalize=True)\nparams = {\n    \"alpha\":np.logspace(-3.5,0)\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(PolyX,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])\n\n\nfor param,values in params.items():\n    plt.plot( results[\"param_\"+param],results[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.ylabel(r\"$R^2$\")\n    plt.xlabel(param)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Appears to be fitting something close to a linear model. \n\n### Polynomial Ridge Regression"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"\nmodel=Ridge(normalize=True)\nparams = {\n    \"alpha\":np.logspace(-1,2)\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(PolyX,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])\n\nfor param,values in params.items():\n    plt.plot( results[\"param_\"+param],results[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.ylabel(r\"$R^2$\")\n    plt.xlabel(param)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"These polynomial models are only marginally more accurate, if at all, than their linear counterparts. Ridge and Lasso regression appear to be forcing to zero or minimiseing most of the polynomial coefficients respectively.\n\nLets see if a Support Vector Machine or Random Forrest can better capture these relationships. "},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"# first we need to normalize the data \nfrom sklearn.preprocessing import StandardScaler\ntransformer=StandardScaler()\nNormX = transformer.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVR \n\nmodel=SVR()\nparams={\n    \"C\"       : np.logspace(-2,2),\n    \"epsilon\" : np.logspace(-2,2),\n    \"kernel\"  : [\"rbf\",\"poly\",\"sigmoid\"]\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True,verbose=1)\nclf.fit(NormX,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\n\nBestParams = results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0]\nprint(\"The models parametres are\",BestParams)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel= RandomForestRegressor()\nparams={\n    \"n_estimators\" : [int(x) for x in np.linspace(1,1000,8)],\n    \"max_depth\"    : [x for x in np.logspace(0,3,8)]+[None]\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True,verbose=1)\nclf.fit(NormX,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\n\nBestParams = results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0]\nprint(\"The models parametres are\",BestParams)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"### Linear Ridge Regression Evaluation \n\nThe best model was the Linear Ridge Regression model which will now be retrained and evaluated.  "},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nmodel=Ridge(normalize=True)\nparams = {\n    \"alpha\":np.logspace(-1.6,-0.2)\n}\n\nclf = GridSearchCV(model,params,cv=5,n_jobs=-1,iid=True)\nclf.fit(X,Y)\nresults=pd.DataFrame(data=clf.cv_results_)\nBestScore = results[results[\"mean_test_score\"]==results[\"mean_test_score\"].max()][\"mean_test_score\"].values[0]\nprint(\"The score on the training set is\",BestScore)\nprint(\"The models parametres are\",results[results[\"mean_test_score\"]==BestScore][\"params\"].values[0])\n\nfor param,values in params.items():\n    plt.plot( results[\"param_\"+param],results[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.ylabel(r\"$R^2$\")\n    plt.xlabel(param)\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"model=Ridge(normalize=True,alpha=0.11406249238513208)\nmodel.fit(X,Y)\nPredictions=X.copy()\nPredictions[\"Prediction\"]=Predictions.apply(\n    lambda row: model.predict([[(row[col]-Predictions[col].mean())/Predictions[col].std() for col in Predictions.columns]])[0],\n    axis=1\n)\n\nPredictions = Predictions.join(pd.DataFrame(Y),how=\"inner\")\npd.options.display.max_columns=200\ndisplay(Predictions[[\"Prediction\",\"Happiness Score\"]].T)\n","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Most of these predictions look reasonable however so are not even possible. For instance Afghanistan has a predicted happiness of -37 out 10. Lets round these up and down accordingly and use the leave one out method to asses the accuracy."},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\n\n# Leave one out\nfolds = KFold(n_splits=len(X))\n\npredictions=[]\ntrue=[]\nfor train,test in folds.split(NormX):\n    model=Ridge(normalize=True,alpha=0.11406249238513208)\n    X_train, X_test = NormX[train], NormX[test]\n    y_train, y_test = Y.values[train], Y.values[test]\n    model.fit(X_train,y_train)\n    predictions.append(model.predict(X_test))\n    true.append(y_test)\n    \npredictions=[x[0] if x>0 else 0 for x in predictions]\npredictions=[x if 10>x else 10 for x in predictions]\ntrue= [x[0] for x in true]\nprint(\"The new R squared value is\",r2_score(true,predictions))\n\ndisplay(pd.DataFrame({\"Predication\":predictions,\"Happiness Score\":true},index=X.index).T)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"Coeficients=[(col,coef) for col,coef in zip(X.columns,model.coef_)]\nCoeficients = sorted(Coeficients,key=lambda x:np.abs(x[1]),reverse=True)\nCoeficients={col:coef for col,coef in Coeficients}\npd.DataFrame(index=Coeficients.keys(),data={\"Coefficient\":list(Coeficients.values())}).T","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Some of these features have surprisingly high coefficients. Such as, *Average precipitation in depth (mm per year)* one might expect a country where it rains a lot to be unhappy however if there is a lot of rain it is easier to create a food surplus. It is interesting to note that *Fixed broadband subscriptions (per 100 people)* has a strong positive effect on happiness but *Mobile cellular subscriptions (per 100 people)* has the smallest effect of any feature. The feature with the strongest negative effect is unsurprisingly *Refugee Rate* as high *Refugee Rate* indicates a war in the country. Unsurprisingly *GDP per capita* has the largest positive effect on happiness. Very surprisingly *Life expectancy at birth, total (years)* has a very small negative effect on happiness.  "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}