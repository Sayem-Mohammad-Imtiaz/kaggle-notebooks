{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF_GP_OD2 = pd.read_csv(\"../input/gufhtugu-publications-dataset-challenge/GP Orders - 2.csv\",encoding=\"unicode_escape\", delimiter=',')\nDF_GP_OD4 = pd.read_csv(\"../input/gufhtugu-publications-dataset-challenge/GP Orders - 4.csv\",encoding=\"unicode_escape\", delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shape() will show the overall rows and columns available in databaset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO KNOW OVERALL RECORDS IN DATAFRAME\nDF_GP_OD2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Head() will show the frist 5 records"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO GET FIRST FIVE RECORDS\nDF_GP_OD2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rename() is used to apply proper columns names."},{"metadata":{"trusted":true},"cell_type":"code","source":"DF_GP_OD2 = DF_GP_OD2.rename(columns={'Order Number': 'Order_No',\"Order Status\":\"Order_Status\", \"Book Name\":\"Book_Name\",\"Order Date\":\"Order_Date\",\"City (Billing)\":\"Billing_City\"})\nDF_GP_OD2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe() will show overall statistics against numerical columns only"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TO KNOW OVERALL DATA STATS: It will only show numerical column stats i.e, order number\nDF_GP_OD2.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check overall missing values in dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = DF_GP_OD2.isnull().sum()\nmissing_values_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the missing values (missing_values_count), fillna() is used to fill missing values in book name and billing city feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling a null values using fillna()  \nDF_GP_OD2[\"Book_Name\"].fillna(\"NAN\", inplace = True) \nDF_GP_OD2[\"Billing_City\"].fillna(\"NAN\", inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# check order date format and type\nprint(DF_GP_OD2['Order_Date'].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will parse the order date feature to make date consistent."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column, date_parsed, with the parsed dates\nDF_GP_OD2['Order_date_parsed'] = pd.to_datetime(DF_GP_OD2['Order_Date'], format=\"%m/%d/%Y %H:%M\")\n#DF_GP_OD2['Order_date_parsed'] = pd.to_datetime(DF_GP_OD2['Order_Date'], infer_datetime_format=True)\nDF_GP_OD2['Order_date_parsed'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to get the day of the month from the date column\nday_of_month_orders = DF_GP_OD2['Order_date_parsed'].dt.day\n# plot the day of the month\nsns.distplot(day_of_month_orders, kde=False, bins=31)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to get stats based on different features available\n\nDF_GP_OD2.Order_No.describe()\nDF_GP_OD2.Billing_City.describe()\nDF_GP_OD2.Book_Name.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to get unique values based on different features available\nDF_GP_OD2.Billing_City.unique()\nDF_GP_OD2.Book_Name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is grouped based on different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# to group data\nDF_GP_OD2.Book_Name.value_counts()\nDF_GP_OD2.groupby('Book_Name').Book_Name.count()\n\n\nDF_GP_OD2.Billing_City.value_counts()\nDF_GP_OD2.groupby('Billing_City').Billing_City.count()\n\n\nDF_BOOKPERCITY = DF_GP_OD2.groupby(['Book_Name', 'Billing_City']).apply(lambda df: DF_GP_OD2.loc[DF_GP_OD2.Order_No.idxmax()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most purchased book in any city\nbook_orders_per_city = DF_GP_OD2.groupby(['Book_Name', 'Billing_City']).Book_Name.agg([len])\nbook_orders_per_city.sort_values(by='len', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most purchased book at any date\nbook_orders_at_date = DF_GP_OD2.groupby(['Book_Name', 'Order_date_parsed']).Book_Name.agg([len])\nbook_orders_at_date.sort_values(by='len', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next Phase:\n* To scale and normalize the features\n* To add feature visalization "},{"metadata":{},"cell_type":"markdown","source":"Please Upvote if you find the notebook interesting.\nThis notebook is under MIT License Feel free to copy and edit it.\n\n[Follow me](https://www.kaggle.com/kk2033/)\n\nThank you."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}