{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Solar energy - Power plant Predictions for one region - part 3-3 \n---\nIn [the previous part](https://www.kaggle.com/obrunet/exploratory-data-analysis-part-2-3) of this study we've have made an exploratory analysis of the data set. We were able to understand how solar generation is dependant of time. Now in this final step, we're going to training different types of machine learning models in order to make predictions.\n\n\nFor that, we'll only keep one region 'FR10' corresponding to the Paris Area."},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"As usual, let's start with some data preparation : first we import the needed libraries, then we import the csv file into a dataframe. After that we can reuse a previous function to add date time informations (year, month, week of the year, the day of the year, and the hour of the day):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data manipulation\nimport numpy as np\nimport pandas as pd\n\n# for plotting purposes\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# options\npd.options.display.max_columns = 300\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv(\"../input/30-years-of-european-solar-generation/EMHIRES_PVGIS_TSh_CF_n2_19862015.csv\")\n\n\ndef add_date_time(_df):\n    \"Returns a DF with two new cols : the time and hour of the day\"\n    t = pd.date_range(start='1/1/1986', periods=_df.shape[0], freq = 'H')\n    t = pd.DataFrame(t)\n    _df = pd.concat([_df, t], axis=1)\n    _df.rename(columns={ _df.columns[-1]: \"time\" }, inplace = True)\n    _df['year'] = _df['time'].dt.year\n    _df['month'] = _df['time'].dt.month\n    _df['week'] = _df['time'].dt.weekofyear\n    _df['day'] = _df['time'].dt.dayofyear    \n    _df['hour'] = _df['time'].dt.hour\n    return _df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's keep only non leap years that cause problems for time series (because of the number of days in a year varies) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = add_date_time(df)\ndf = df[~df.year.isin([1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016])]\n\n# keeping only values for one country for the predictions\ndf = df[['FR10', 'year', 'month', 'week', 'day', 'hour', 'time']]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of rows and colums kepts :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__As usual, separation of the data set into 2 disctinct sets :__\n\nIn machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.\n\nThe data used to build the final model usually comes from multiple datasets. In particular, two data sets are commonly used in different stages of the creation of the model.\n\nThe model is initially fit on a training dataset,that is a set of examples used to fit the parameters of the model. The model is trained on the training dataset using a supervised learning method.\n\nFinally, the test dataset is a dataset used to provide an unbiased evaluation of a final model fit on the training dataset. If the data in the test dataset has never been used in training , the test dataset is also called a holdout dataset.\n\nReference : [Wikipedia](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data including 10 years of records except the last month\ntrain_data = df[-24*365*10:-24*31]\n\n# test data = last month of records for year 2015\ntest_data = df[-24*31:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Model Training"},{"metadata":{},"cell_type":"markdown","source":"## Metric : RMSE\n\nThe root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSE represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSE serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSE is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.\n\nRMSE is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSE is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.\n\nRMSE is the square root of the average of squared errors. The effect of each error on RMSE is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSE. Consequently, RMSE is sensitive to outliers.\n\nAny way we've already seen in the previous part that there isn't any outlier, so we can definitively use the RMSE as a metric to compare models accuracies.\n\nReference : [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmodel_instances, model_names, rmse_train, rmse_test = [], [], [], []\n\n\ndef plot_scores():\n    \"\"\"Create three lists : models, the RMSE on the train set and on the test set, then plot them\"\"\"\n    df_score = pd.DataFrame({'model_names' : model_names,\n                             'rmse_train' : rmse_train,\n                             'rmse_test' : rmse_test})\n    df_score = pd.melt(df_score, id_vars=['model_names'], value_vars=['rmse_train', 'rmse_test'])\n\n    plt.figure(figsize=(12, 10))\n    sns.barplot(y=\"model_names\", x=\"value\", hue=\"variable\", data=df_score)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline\n\nThis simple model use the mean of 10 years at a certain day at a specific hour. It can be considered as a weak predicator. It is used to see if the other ML models are interesting and efficient or not. Let's see how the predictions of this baseline look like :"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = train_data.drop(columns=['time']), train_data['FR10']\nx_test, y_test = test_data.drop(columns=['time']), test_data['FR10']\n\n\ndef mean_df(d, h):\n    \"return the hourly mean of a specific day of the year\"\n    res = x_train[(x_train['day'] == d) & (x_train['hour'] == h)]['FR10'].mean()\n    return res\n\n    # examples \n    #df['col_3'] = df.apply(lambda x: f(x.col_1, x.col_2), axis=1)\n    # x_train[(x_train['day'] == x['day']) & (x_train['hour'] == x['hour'])]['FR10'].mean()\n    \n    \n#x_train['pred'] = x_train.apply(lambda x: mean_df(x.day, x.hour), axis=1)\nx_test['pred'] = x_test.apply(lambda x: mean_df(x.day, x.hour), axis=1)\nx_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add the result to our lists of models and RMSE for both the train & test sets :"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names.append(\"base_line\")\nrmse_train.append(np.sqrt(mean_squared_error(x_train['FR10'], x_train['FR10']))) # a modifier en pred\nrmse_test.append(np.sqrt(mean_squared_error(x_test['FR10'], x_test['pred'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can plot the variation of the real values and the predicted ones across hours in order to understand how th model behaves (predictions are in orange and ground truth in blue) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions(data):\n    plt.figure(figsize=(14, 6))\n    sns.lineplot(data=data)\n    plt.title(\"Base line predictions (orange) vs real values (blue) for the last month\")\n    plt.xlabel(\"hours of the last month (12-2015)\")\n    plt.ylabel(\"solar installation efficiency\")\n    plt.show()\n\nplot_predictions(data=x_test[['FR10', 'pred']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression models"},{"metadata":{},"cell_type":"markdown","source":"At first we are goiing to use the basic linear regression models : LinearRegression, Ridge, Lasso, ElasticNet. Note that last three use different kind of regularizations."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = train_data[['month', 'week', 'day', 'hour']], train_data['FR10']\nX_test, y_test = test_data[['month', 'week', 'day', 'hour']], test_data['FR10']\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\n\nimport xgboost as xgb\nimport lightgbm as lgbm\n\n\ndef get_rmse(reg, model_name):\n    \"\"\"Print the score for the model passed in argument and retrun scores for the train/test sets\"\"\"\n    \n    y_train_pred, y_pred = reg.predict(X_train), reg.predict(X_test)\n    rmse_train, rmse_test = np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_pred))\n    print(model_name, f'\\t - RMSE on Training  = {rmse_train:.2f} / RMSE on Test = {rmse_test:.2f}')\n    \n    return rmse_train, rmse_test\n\n\n# list of all the basic models used at first\nmodel_list = [\n    LinearRegression(), Lasso(), Ridge(), ElasticNet(),\n    RandomForestRegressor(), GradientBoostingRegressor(), ExtraTreesRegressor(),\n    xgb.XGBRegressor(), lgbm.LGBMRegressor(), KNeighborsRegressor()\n             ]\n\n# creation of list of names and scores for the train / test\nmodel_names.extend([str(m)[:str(m).index('(')] for m in model_list])\n\n\n# fit and predict all models\nfor model, name in zip(model_list, model_names):\n    model.fit(X_train, y_train)\n    sc_train, sc_test = get_rmse(model, name)\n    rmse_train.append(sc_train)\n    rmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM model"},{"metadata":{},"cell_type":"markdown","source":"the Support Vector Machine (both linear and poly kernels) aren't efficient enough :\n\nSide note : this model is CPU or GPU intensive so i've decided to print the code and results in order to not re-excute all this part but if you want, you can easily re-run the following code"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\"\"\nsvm_lin = LinearSVR()\nsvm_lin.fit(X_train, y_train)\nsc_train, sc_test = get_rmse(svm_lin, \"SVM lin.\")\nmodel_names.append(\"SVM lin.\")\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)\n\nSVM lin. \t - RMSE on Training  = 0.31 / RMSE on Test = 0.30\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\"\"\nsvm_poly = SVR(kernel='poly', degree=4, max_iter=100)\nsvm_poly.fit(X_train, y_train)\nsc_train, sc_test = get_rmse(svm_poly, \"SVM poly.\")\nmodel_names.append(\"SVM poly.\")\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)\n\nSVM poly. \t - RMSE on Training  = 0.52 / RMSE on Test = 0.56\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using polynomial feature"},{"metadata":{},"cell_type":"markdown","source":"If we use the \"polynomial features\" method of scikit learn to elevate date time infos to different powers :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n\npoly_lin_reg = Pipeline([\n    (\"poly_feat\", PolynomialFeatures(degree=4)),\n    (\"linear_reg\", LinearRegression())\n])\n\npoly_lin_reg.fit(X_train, y_train)\n\nsc_train, sc_test = get_rmse(poly_lin_reg, \"Poly Linear Reg\")\n\nmodel_names.append('Poly Linear Reg')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating date time infos as categorical features"},{"metadata":{},"cell_type":"markdown","source":"Now let's try to deal with date time infos as categorical featues, which is some times the right way to handle such data :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data for 10 years\ntrain_data_d = df[-24*365*10:][['FR10', 'month', 'week', 'day', 'hour']]\n\n# one hot encoding for categorical feature\ncat_feat = ['month', 'week', 'day', 'hour']\ntrain_data_d = pd.get_dummies(data=train_data_d, columns=cat_feat, drop_first=True)\ntrain_data_d.head()\n\n# keep last month for the test data set\ntest_data_d, train_data_d = train_data_d[-24*31:], train_data_d[:-24*31]\n\n# get_dummies or one hot encoding\nX_train_d, y_train_d = train_data_d.drop(columns=['FR10']), train_data_d['FR10']\nX_test_d, y_test_d = test_data_d.drop(columns=['FR10']), test_data_d['FR10']\n\n# verify if different shapes match\nX_train_d.shape, y_train_d.shape, X_test_d.shape, y_test_d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_linreg = LinearRegression()\ncategorical_linreg.fit(X_train, y_train)\nsc_train, sc_test = get_rmse(categorical_linreg, \"Categorical lin. reg.\")\nprint(\"Not more efficient than linear regression without get dummies\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How does the best model behave \nNow we plot the predictions from the GradientBoostingRegressor which seems to be the best model so far. As you can see, the stricly positive values corresponding to hours of sun light are in the right \"tempo\" compared to the ground truth. But the spikes are nearly all the same because there is a daily variation that can't be handled by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\ny_pred = pd.DataFrame(gbr.predict(X_test))\n\ny_test = pd.DataFrame(y_test)\ny_test['pred'] = y_pred.values\n\nplot_predictions(data=y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Prophet - a facebook lib for time series"},{"metadata":{},"cell_type":"markdown","source":"Let's try something different : a library specific for time series.\nI've decided to use prophet, but one can also give an opportunity to other libs such as ARIMA / SARIMA.\n\n_[...]Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.[...]_\n\nAs mentionned in the documentation :\n\n_[...]Prophet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods. The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast.[...]_\n\nReference : [Prophet](https://facebook.github.io/prophet/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reload the data\ndf = pd.read_csv(\"../input/30-years-of-european-solar-generation/EMHIRES_PVGIS_TSh_CF_n2_19862015.csv\")\ndf = add_date_time(df)\n#df = df[~df.year.isin([1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016])]\n\n\n# only keep what it usefull to use here\ndata = df[['time', 'FR10']]\ndata = data.rename(columns={\"time\": \"ds\", \"FR10\": \"y\"})\n\n# train set : 10 yrs. except last month\ntrain_data = data[-24*365*10:-24*31]\n\n# test set = last month of the record (2015)\ntest_data = data[-24*31:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import the prophet library, initialize model and fit it :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n\nprophet_model = Prophet()\nprophet_model.fit(train_data)\ntest_data.tail()\n#y_train_pred = prophet_model.predict(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use our trained model to make predictions :"},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = prophet_model.predict(test_data)\nforecast = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast['FR10'] = test_data['y'].values\nforecast.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- \"prophet\" : is the forecast\n- \"prophet_lower\" : corresponds to the lower forecast\n- \"prophet_upper\" : same thing but upper forecast\n\nIn the same way, it is usefull to visualize the predictions. As you can see on the plot below, the timing of predictions seems to be fine, but some negative values are forecast and the spikes are all the same once again."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names.append(\"prophet\") #.extend([\"prophet\", \"prophet_lower\", \"prophet_upper\"])\n\nrmse_train.append(0)\n#rmse_train.extend([0, 0, 0])\nrmse_test.append(np.sqrt(mean_squared_error(test_data['y'], forecast['yhat'])))\n#rmse_test.append(np.sqrt(mean_squared_error(test_data['y'], forecast['yhat_lower'])))\n#rmse_test.append(np.sqrt(mean_squared_error(test_data['y'], forecast['yhat_upper'])))\n\n\nplot_predictions(data=forecast[['FR10', 'yhat']])\n\n# if we wanted to plot aslo inf / sup\n# plt.figure(figsize=(18, 8))\n# sns.lineplot(data=forecast[['FR10', 'yhat_lower', 'yhat_upper']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Side note : prophet seems to be a powerfull library and can certainly be tuned to obtain better forecast. Here is just a first taste so please consider reading the doc and using advanced features."},{"metadata":{},"cell_type":"markdown","source":"---\n# Using deep learning R.N.N models"},{"metadata":{},"cell_type":"markdown","source":"In this final step, we're going to train deep learning models and especialy Recurrent neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/30-years-of-european-solar-generation/EMHIRES_PVGIS_TSh_CF_n2_19862015.csv\")\ndf = df[sorted([c for c in df.columns if 'FR' in c])]\n\n# keep only 4 years\ndf = df[-24*365*4:]\n\n# nb lines / cols\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"We make a prediction of the FR10 value of a specific hour, based on all the features (all other regions' efficiencies) of the past 2 days (48 hrs).\n\nBefore doing so, we need to create the (X, y) dataset.\n\nLet's consider your action Name has 100 lines.\nThe X values should contain, in each line, a table of 48 hrs and 22 features. So that the final X array will have the shape ((100-48+1), 48, 22).\n\nThe corresponding y values should be the station efficiency of the hours 49 (indeed, y can not contain the 48 first hours, since we need 2 days of X to predict before). So the final y array will have the shape ((100-48+1), 1) (or equivalently (53,))."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(data, past):\n    X = []\n    for i in range(len(data)-past-1):\n        X.append(data.iloc[i:i+past].values)\n    return np.array(X)\n\n\nlookback = 48\n\ny = df['FR10'][lookback+1:] \nX = process_data(df, lookback)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need to scale the values since they are already between 0 and 1. So let's start by building a simple RNN model. Before we need to split the data set in two parts for training and test purposes. We can use the parameter shuffle=False in the scikit learn train_test_split method so that the last 20% will correspond to the end of our hourly records and not randomly choosen lines of the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## R.N.N Recurrent neural network"},{"metadata":{},"cell_type":"markdown","source":"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. \n\nReference : [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n\nMore infos can be found here on [Towards Data Science](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)"},{"metadata":{},"cell_type":"markdown","source":"After the libraries import, we build a RNN with Tensorflow, we compile and train it :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n\n\ndef my_RNN():\n    my_rnn = Sequential()\n    my_rnn.add(SimpleRNN(units=32, return_sequences=True, input_shape=(lookback,22)))\n    my_rnn.add(SimpleRNN(units=32, return_sequences=True))\n    my_rnn.add(SimpleRNN(units=32, return_sequences=False))\n    my_rnn.add(Dense(units=1, activation='linear'))\n    return my_rnn\n\n\nrnn_model = my_RNN()\nrnn_model.compile(optimizer='adam', loss='mean_squared_error')\nrnn_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then let's use our trained RNN model to make prediction and see the RMSE not bad isn't it ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train, y_pred_test = rnn_model.predict(X_train), rnn_model.predict(X_test)\nerr_train, err_test = np.sqrt(mean_squared_error(y_train, y_pred_train)), np.sqrt(mean_squared_error(y_test, y_pred_test))\nerr_train, err_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def append_results(model_name):\n    model_names.append(model_name)\n    rmse_train.append(err_train)\n    rmse_test.append(err_test)\n\nappend_results(\"RNN\")\n\n\ndef plot_evolution():\n    plt.figure(figsize=(12, 6))\n    plt.plot(np.arange(len(X_train)), y_train, label='Train')\n    plt.plot(np.arange(len(X_train), len(X_train)+len(X_test), 1), y_test, label='Test')\n    plt.plot(np.arange(len(X_train), len(X_train)+len(X_test), 1), y_pred_test, label='Test prediction')\n    plt.legend()\n    plt.show()\n\nplot_evolution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at the predictions versus ground truth, this is much better !"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_res = pd.DataFrame(zip(list(y_test), list(np.squeeze(y_pred_test))), columns =['FR10', 'pred'])\nplot_predictions(data=rnn_res[-30*24:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GRU"},{"metadata":{},"cell_type":"markdown","source":"More infos on [wikipedia](https://en.wikipedia.org/wiki/Gated_recurrent_unit) or [Towards Data Science](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n\n[...]_Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets._\n\n_However, as shown by Gail Weiss & Yoav Goldberg & Eran Yahav, the LSTM is \"strictly stronger\" than the GRU as it can easily perform unbounded counting, while the GRU cannot. That's why the GRU fails to learn simple languages that are learnable by the LSTM._\n\n_Similarly, as shown by Denny Britz & Anna Goldie & Minh-Thang Luong & Quoc Le of Google Brain, LSTM cells consistently outperform GRU cells in \"the first large-scale analysis of architecture variations for Neural Machine Translation._[...]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import GRU\n\ndef my_GRU(input_shape):\n    my_GRU = Sequential()\n    my_GRU.add(GRU(units=32, return_sequences=True, activation='relu', input_shape=input_shape))\n    my_GRU.add(GRU(units=32, activation='relu', return_sequences=False))\n    my_GRU.add(Dense(units=1, activation='linear'))\n    return my_GRU\n\ngru_model = my_GRU(X.shape[1:])\ngru_model.compile(optimizer='adam', loss='mean_squared_error')\ngru_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train, y_pred_test = gru_model.predict(X_train), gru_model.predict(X_test)\nerr_train, err_test = np.sqrt(mean_squared_error(y_train, y_pred_train)), np.sqrt(mean_squared_error(y_test, y_pred_test))\nerr_train, err_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"append_results(\"GRU\")\nplot_evolution()\n\ngru_res = pd.DataFrame(zip(list(y_test), list(np.squeeze(y_pred_test))), columns =['FR10', 'pred'])\nplot_predictions(data=gru_res[-30*24:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import LSTM\n\ndef my_LSTM(input_shape):\n    my_LSTM = Sequential()\n    my_LSTM.add(LSTM(units=32, return_sequences=True, activation='relu', input_shape=input_shape))\n    my_LSTM.add(LSTM(units=32, activation='relu', return_sequences=False))\n    my_LSTM.add(Dense(units=1, activation='linear'))\n    return my_LSTM\n\nlstm_model = my_LSTM(X.shape[1:])\nlstm_model.compile(optimizer='adam', loss='mean_squared_error')\nlstm_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train, y_pred_test = lstm_model.predict(X_train), lstm_model.predict(X_test)\nerr_train, err_test = np.sqrt(mean_squared_error(y_train, y_pred_train)), np.sqrt(mean_squared_error(y_test, y_pred_test))\nerr_train, err_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"append_results(\"LSTM\")\nplot_evolution()\n\nlstm_res = pd.DataFrame(zip(list(y_test), list(np.squeeze(y_pred_test))), columns =['FR10', 'pred'])\nplot_predictions(data=lstm_res[-30*24:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally there isn't any huge difference between the forecasts of the 3 types of RNN. We would be intersting to change the number of neurons, of layers, the activation functions and other parameters in order to see if we can make it even more precise. But this is not the goal here."},{"metadata":{},"cell_type":"markdown","source":"---\n\n# Conclusions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(model_names), len(rmse_train), len(rmse_test)\n\nplt.style.use('fivethirtyeight')\nplot_scores()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Linear Regression are under fitting, and don't perform well. Regularizations were not needed here.\n- Using Polynomial feature allows us to get better results with no under fitting this time but this is not enough compared to the base line.\n- Finally, the boosting familly performs a little better but there isn't any real convincing difference compared to the baseline. It could have been interresting to tune hyperparameters with the RandomizedSearchCV or GridSearchCV methods, but i don't think we'll obtain a real gain.\n- Prophet is also not suited here.\n__Those models performs well when it comes to get an overall shape but aren't suited when there is a short term change due to the weather on the global tendency. That's why deep learning models are better here.__\nNever the less, it should be noted to the R.N.N use the history of the past two days, this can also explain why the last 3 models are more performant. The same history can probably used with other models in order to obtain better forecasts.\n\n\nOn a global perspective :\n- The RMSE is around 0.05 compared to values ranging from 0 to 1.\n- Anyway it is necessary to relativize because most of the values are under 0.7 and it should be put into perspective that many values are nulls. \n- Finally the RMSE with R.N.N is half the RMSE of the base line which is quite sanguine.\n- It could be interesting to tune the parameters of few models to see if we gain few percents..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}