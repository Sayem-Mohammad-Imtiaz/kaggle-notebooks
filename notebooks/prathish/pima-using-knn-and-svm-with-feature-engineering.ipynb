{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Code by PRATHISH  MURUGAN\n# 29 April 2020\n\n#Pima-Indians-Diabetes\n\n#This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.  \n#he datasets consist of several medical predictor (independent) variables and \n#one target (dependent) variable, Outcome. \n#Independent variables include the number of pregnancies the patient has had, their BMI, \n#insulin level, age, and so on.\n\n#This dataset can be found here -> https://www.kaggle.com/uciml/pima-indians-diabetes-database\n\n#Acknowledgements :-\n#Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). \n#Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. \n#In Proceedings of the Symposium on Computer Applications and \n#Medical Care (pp. 261--265).IEEE Computer Society Press.\n\n#This is a classifiaction problem and the dataset is not clean\n\n#This problem looks easy bit it gets hard because of the unclean and unclean dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt         #graphs\nimport seaborn as sns                   #visualizations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Imporing the CSV dataset\npima_main=pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\n#Making a copy of the original dataset for working\npima_work=pima_main.copy(deep=True)\n\nprint(pima_work.head())\nprint(pima_work.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npima_work.describe()   # Gives the basic mean,meadian and quantas og each columns\n#Note atleast 25% of readings for insulin and skinthickness are 0. Min readings \n#for columns like Glucose, Bloodpressure and BMI are zero too which does not seem appropriate\n\npima_main.info()\npima_work.isnull().sum()     #to check any null values in any columns\n#There are no null values in the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we have to check if there are any 0's in the set\npima_work.isin([0]).sum()\n# Seems like there are a lot of zeros and 0's in glucose,BP,BMI is not logical\n#We need to fill these 0's with some appropirate values\n\npima_work.Outcome.value_counts()\n#Seems like the Outcome is not Balanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all zeros with NANs. I do this so that when means are calculated, zeros are not counted.\npima_nan = pima_work.replace({\n            'Glucose': 0,\n            'BloodPressure' : 0,\n            'SkinThickness' : 0,\n            'BMI' : 0 ,\n            'Insulin' : 0,\n        },np.NaN)                     \n\n\npima_nan.isin([0]).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have replaced all the unwanted 0's with NaN \n#Calcutating the mean of each columns\npima_nan.mean()\npima_nan.median()\n#Now we will try to replace this NaN with mean or median\npima_nan.isnull().sum()\npima_nan=pima_nan.fillna(pima_nan.mean())\npima_nan.isnull().sum()\n#Now we have filled the NaN values with the mean of each coloums","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the dataset\nfrom sklearn.utils import shuffle\npima_nan = shuffle(pima_nan)\n\n# I want to see how the imputation process has affected these values. \n# I will follow this up with some visualizations.\n\npima_nan.groupby('Outcome').mean().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pima_corr=pima_nan.corr()\n\n#Creating a Heatmap\nplt.figure(figsize=(9,9))\nsns.heatmap(pima_corr,cmap='coolwarm',annot=True,linewidths = 0.5)\n\n#From this heatmap we can see that Glucose is having the greatest\n#effect on outcome followed by BMI which is self-explanatory.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms for imputed data.\n\nfor cols in pima_nan.columns:\n    x = pima_nan.loc[:,cols]\n    plt.hist(x)\n    plt.title('Histogram for Feature' +str(cols))\n    plt.show()\n# End of Visualizations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a test and train DF    \nX=pima_nan.drop('Outcome',axis=1)\ny=pima_nan['Outcome'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Let's split the data randomly into training and test set\n    #importing train_test_split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ML MODELS\n\n# MODEL 1 :- KNN CLASSIFICATION\n\n#Create-KNN-model\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors = 60)       #n_neighbors = K value\n\n#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = KNN, X= X_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))\n\n#The values I got for KNN at the time of coding is \n#Average Accuracies:  0.7664285714285715\n#Standard Deviation Accuracies:  0.09502140527437733","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN.fit(X_train,y_train) #learning model\nKNN.score(X_test, y_test)\n\n#The score I got while writing the score \n#Score :-  0.751219512195122","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = KNN.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(y_test,y_predict)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytest\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL 2 :- SVM\n\n#Create SVM Model\nfrom sklearn.svm import SVC\n\nSVM = SVC(random_state=42)\n\n#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = SVM, X= X_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standard Deviation Accuracies: \",np.std(accuraccies))\n\n#The values I got for SVM at the time of coding is \n#Average Accuracies:  0.6410714285714286\n#Standard Deviation Accuracies:  0.018064274887492272","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM.fit(X_train,y_train)  #learning \n#SVM Test \nprint (\"SVM Accuracy:\", SVM.score(X_test,y_test))\n\nSVMscore = SVM.score(X_test,y_test)\n# Score :-  0.6536585365853659","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\n\nyprediciton3= SVM.predict(X_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton3)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So this is a machine learning model to accurately predict whether or not the\n# patients in the dataset have diabetes or not\n\n# Hope you find it useful\n# Corections and suggestions are welcomed\n\n# BY PRATHISH MURUGAN\n# 29 - APRIL - 2020","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}