{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.utils import resample\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hr-analytics-case-study/general_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataframe shape is\",df.shape)\nprint(\"\\n\",df.head(5))\nprint(\"\\n ******* \\n\")\nprint(df.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"NumCompaniesWorked\"].fillna(df[\"NumCompaniesWorked\"].mean(),inplace=True)\ndf[\"TotalWorkingYears\"].fillna(df[\"TotalWorkingYears\"].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['EmployeeCount','EmployeeID','StandardHours','Over18'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yes = df[df[\"Attrition\"]==\"Yes\"]\nno = df[df[\"Attrition\"]==\"No\"]\n\nprint(\"Total number of examples are \\n\", len(df))\nprint(\"Number of yes:\",len(yes), \"\\t Number of no:\", len(no), \"\\t Total is:\",len(yes+no))\n\nsns.countplot(x = \"Attrition\",data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\ndf['BusinessTravel'] = labelEncoder_X.fit_transform(df['BusinessTravel'])\ndf['Department'] = labelEncoder_X.fit_transform(df['Department'])\ndf['EducationField'] = labelEncoder_X.fit_transform(df['EducationField'])\ndf['Gender'] = labelEncoder_X.fit_transform(df['Gender'])\ndf['JobRole'] = labelEncoder_X.fit_transform(df['JobRole'])\ndf['MaritalStatus'] = labelEncoder_X.fit_transform(df['MaritalStatus'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attriton is dependent var\nlabel_encoder_y = LabelEncoder()\ndf['Attrition'] = label_encoder_y.fit_transform(df['Attrition'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_majority = df[df[\"Attrition\"]==0]\ndf_minority = df[df[\"Attrition\"]==1]\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=3699,    # to match majority class\n                                 random_state=0) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\nyes = df_upsampled[df_upsampled[\"Attrition\"]==1]\nno = df[df[\"Attrition\"]==0]\n\nprint(\"Total number of examples are \\n\", len(df_upsampled))\nprint(\"Number of yes:\",len(yes), \"\\t Number of no:\", len(no), \"\\t Total is:\",len(yes+no))\n\nsns.countplot(x = \"Attrition\",data=df_upsampled)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataframe shape is\",df_upsampled.shape)\ndf_upsampled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_upsampled['Attrition']\nX = df_upsampled.drop('Attrition', axis = 1)\nprint(y.shape, X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\nX = np.array(X)\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters_deep(layer_dims, n):\n    \"\"\"\n    This function takes the numbers of layers to be used to build our model as\n    input and otputs a dictonary containing weights and biases as parameters\n    to be learned during training\n    The number in the layer_dims corresponds to number of neurons in \n    corresponding layer\n\n    @params\n\n    Input to this function is layer dimensions\n    layer_dims = List contains number of neurons in one respective layer\n                 and [len(layer_dims) - 1] gives L Layer Neural Network\n    \n    Returns:\n    \n    parameters = Dictionary containing parameters \"W1\", \"b1\", ., \"WL\", \"bL\"\n                 where Wl = Weight Matrix of shape (layer_dims[l-1],layer_dims[l])\n                       bl = Bias Vector of shape (1,layer_dims[l])\n    \"\"\"\n    # layers_dims = [19, 7, 5, 1] #  3-layer model\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # Number of layers in the network\n\n    for l in range(1, L):          # It starts with 1 hence till len(layer_dims)\n        # Initialize weights randomly according to Xavier initializer in order to avoid linear model\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1],layer_dims[l])*np.sqrt(n / layer_dims[l-1])\n        # Initialize bias vector with zeros\n        parameters['b' + str(l)] = np.zeros((1,layer_dims[l]))\n        # Making sure the shape is correct\n        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n        assert(parameters['b' + str(l)].shape == (1,layer_dims[l]))\n\n    # parameters = {\"W [key]\": npnp.random.randn(layer_dims[l-1],layer_dims[l]) [value]}\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(Z):\n    \"\"\"\n    This function takes the forward matrix Z (Output of the linear layer) as the\n    input and applies element-wise Sigmoid activation\n\n    @params\n\n    Z = numpy array of any shape\n    \n    Returns:\n\n    A = Output of sigmoid(Z), same shape as Z, for the last layer this A is the\n        output value from our model\n\n    cache = Z is cached, this is useful during backpropagation\n    \"\"\"\n    \n    A = 1/(1+np.exp(-Z)) # Using numpy apply sigmoid to Z \n    cache = Z            # Cache the matrix Z\n    \n    return A, cache\n\ndef relu(Z):\n    \"\"\"\n    This function takes the forward matrix Z as the input and applies element \n    wise Relu activation\n\n    @params\n\n    Z = Output of the linear layer, of any shape\n\n    Returns:\n\n    A = Post-activation parameter, of the same shape as Z\n    cache = Z is cached, this is useful during backpropagation\n    \"\"\"\n    \n    A = np.maximum(0,Z) # Element-wise maximum of array elements\n    # Making sure shape of A is same as shape of Z\n    assert(A.shape == Z.shape)\n    \n    cache = Z           # Cache the matrix Z\n\n    return A, cache\n\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    This function implements the backward propagation for a single Relu unit\n\n    @params\n\n    dA = post-activation gradient, of any shape\n    cache = Retrieve cached Z for computing backward propagation efficiently\n\n    Returns:\n\n    dZ = Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    dZ = np.array(dA) # Just converting dz to a correct object.\n    \n    # When z <= 0, you set dz to 0 as well, as relu sets negative values to 0 \n    dZ[Z <= 0] = 0\n    # Making sure shape of dZ is same as shape of Z\n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    This function implements the backward propagation for a single Sigmoid unit\n\n    @params\n\n    dA = post-activation gradient, of any shape\n    cache = Retrieve cached Z for computing backward propagation efficiently\n\n    Returns:\n    dZ = Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    \n    s = 1/(1+np.exp(-Z)) # Using numpy apply Sigmoid to Z \n    dZ = dA * s * (1-s)  # This is derivatie of Sigmoid function\n\n    # Making sure shape of dZ is same as shape of Z\n    assert (dZ.shape == Z.shape)\n    \n    return dZ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward(A, W, b):\n    \"\"\"\n    This function implements the forward propagation equation Z = WX + b\n\n    @params\n\n    A = Activations from previous layer (or input data),\n        shape = (number of examples, size of previous layer)\n    W = Weight matrix of shape (size of previous layer,size of current layer)\n    b = Bias vector of shape (1, size of the current layer)\n\n    Returns:\n\n    Z = The input of the activation function, also called pre-activation parameter\n        shape = (number of examples, size of current layer)\n    cache = Tuple containing \"A\", \"W\" and \"b\"; \n            stored for computing the backward pass efficiently\n    \"\"\"\n    # A = [(3528, 19)], W = [19,7], Z = [3528,7]\n    # print(A.shape, W.shape)\n    Z = A.dot(W) + b # Here b gets broadcasted \n    #print(Z)\n    # Making sure shape of Z = (number of examples, size of current layer)\n    assert(Z.shape == (A.shape[0], W.shape[1]))\n\n    cache = (A, W, b) # Cache all the three params \n    \n    return Z, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    This function implements forward propagation LINEAR -> ACTIVATION layer\n\n    @params\n\n    A_prev = Activations from previous layer (or input data), \n             shape = (number of examples, size of previous layer)\n    W = Weight matrix of shape (size of previous layer,size of current layer)\n    b = Bias vector of shape (1, size of the current layer)\n    activation = The activation to be used in this layer, \n                 stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n\n    A = The output of the activation function, also called the post-activation value \n    cache = Tuple containing \"linear_cache\" and \"activation_cache\";\n            stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\"\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\"\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    # Making sure shape of A = (number of examples, size of current layer)\n    assert (A.shape == (A_prev.shape[0],W.shape[1]))\n    cache = (linear_cache, activation_cache)\n    #print(cache)\n    return A, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_forward(X, parameters):\n    \"\"\"\n    This function implements forward propagation as following:\n    [LINEAR->RELU]*(L-1) -> LINEAR -> SIGMOID computation\n    So we apply Relu to all the hidden layers and Sigmoid to the output layer\n\n    @params\n\n    X = Data, numpy array of shape (number of examples, number of features)\n    parameters = Output of initialize_parameters_deep() function\n    \n    Returns:\n\n    AL = last post-activation value, also rferred as prediction from model\n    caches = list of caches containing:\n             every cache of linear_activation_forward() function\n             (there are L-1 of them, indexed from 0 to L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2            # Number of layers in the neural network\n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A\n        # For hidden layers use Relu activation\n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n        #print(\"A\",A.shape)\n        caches.append(cache)\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    # For output layer use Sigmoid activation\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n    #print(\"AL\",AL.shape)\n    caches.append(cache)\n    \n    # Making sure shape of AL = (number of examples, 1)\n    assert(AL.shape == (X.shape[0],1))\n            \n    return AL, caches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL, Y, parameters, lambd):\n    \"\"\"\n    This function implements the Binary Cross-Entropy Cost alng with L2 regularization\n    J = -(1/m)*(ylog(predictions)+(1−y)log(1−predictions)) + (λ/2*m)∑(W**2)\n    \n    @params\n\n    AL = Probability vector corresponding to our label predictions \n         shape =  (number of examples, 1)\n    Y  = Ground Truth/ True \"label\" vector (containing classes 0 and 1) \n         shape  = (number of examples, 1)\n    parameters = Dictionary containing parameters as follwoing:\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    lambd = Regularization parameter, int\n\n    Returns:\n\n    cost = Binary Cross-Entropy Cost with L2 Regularizaion \n    \"\"\"\n    \n    m = Y.shape[0]  # Number of training examples\n\n    # Compute loss from aL and y\n    cross_entropy_cost = -(1/m)*(np.dot(np.log(AL).T,Y) + np.dot(np.log(1-AL).T,(1-Y)))\n    #print(cost)\n    reg_cost = []\n    W = 0\n    L = len(parameters) // 2                  # number of layers in the neural network\n    for l in range(1, L+1):\n        W = parameters[\"W\" + str(l)]\n        reg_cost.append(lambd*1./(2*m)*np.sum(W**2))\n        \n    cross_entropy_cost = np.squeeze(cross_entropy_cost) # To make sure cost's is scalar (e.g. this turns [[cost]] into cost)\n    assert(cross_entropy_cost.shape == ())\n    cost = cross_entropy_cost + np.sum(reg_cost)\n\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dZ, cache, lambd):\n    \"\"\"\n    This function implements the linear portion of backward propagation for a \n    single layer (layer l)\n\n    @params\n\n    dZ = Gradient of the cost with respect to the linear output of current \n         layer l, shape = (number of examples, size of current layer)\n    cache = Tuple of values (A_prev, W, b) coming from the forward propagation \n            in the current layer\n    lambd = Regularization parameter, int\n    \n    Returns:\n\n    dA_prev = Gradient of the cost with respect to the activation of the \n              previous layer l-1, \n              same shape as A_prev(number of examples, size of previous layer)\n    dW = Gradient of the cost with respect to W of current layer l, \n         same shape as W(size of previous layer,size of current layer)\n    db = Gradient of the cost with respect to b of current layer l, \n         same shape as b(1,size of current layer)\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[0] # Number of training examples\n    \n    dW = (1/m)*np.dot(A_prev.T,dZ) + 1./m*lambd*W # Derivative wrt Weights\n    #print(\"dW\",dW.shape)\n    db = (1/m)*np.sum(dZ, axis=0, keepdims=True)  # Derivative wrt Bias\n    #print(\"db\",db.shape)\n    dA_prev = np.dot(dZ,cache[1].T)\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_backward(dA, cache, lambd, activation):\n    \"\"\"\n    This function implements backward propagation for LINEAR -> ACTIVATION layer\n    \n    @params\n\n    dA = post-activation gradient for current layer l \n    cache = tuple of values (linear_cache, activation_cache) \n            we store for computing backward propagation efficiently\n    activation = the activation to be used in this layer, \n                 stored as a text string: \"sigmoid\" or \"relu\"\n    lambd = Regularization parameter, int\n    \n    Returns:\n\n    dA_prev = Gradient of the cost with respect to the activation of the \n              previous layer l-1, \n              same shape as A_prev(number of examples, size of previous layer)\n    dW = Gradient of the cost with respect to W of current layer l, \n         same shape as W(size of previous layer,size of current layer)\n    db = Gradient of the cost with respect to b of current layer l, \n         same shape as b(1,size of current layer)\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA,activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA,activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_backward(AL, Y, caches, lambd):\n    \"\"\"\n    This function implements the backward propagation as following: \n    [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    @params\n\n    AL = probability vector, output of the L_model_forward function\n    Y = Ground Truth/ True \"label\" vector (containing classes 0 and 1) \n        shape  = (number of examples, 1)\n    caches = list of caches containing:\n             every cache of linear_activation_forward function with \"relu\" \n             (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n             the cache of linear_activation_forward function with \"sigmoid\" \n             (it's caches[L-1])\n    lambd = Regularization parameter, int\n    \n    Returns:\n\n    grads = Dictionary with the gradients\n            grads[\"dA\" + str(l)] = ... \n            grads[\"dW\" + str(l+1)] = ...\n            grads[\"db\" + str(l+1)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[0] # Number of training examples\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    # Derivative of Binary Cross Entropy function\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    # Lth layer (SIGMOID -> LINEAR) gradients. \n    # Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, lambd, activation = \"sigmoid\")\n    \n    # Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -> LINEAR) gradients\n        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, lambd, activation = \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_adam(parameters) :\n    \"\"\"\n    This function Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters\n    \n    @param\n    \n    parameters = Dictionary containing parameters as follwoing:\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    \n    v = Dictionary that will contain the exponentially weighted average of the gradient\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s = Dictionary that will contain the exponentially weighted average of the squared gradient\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n    \n    return v, s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                              beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    This function updates our model parameters using Adam\n\n    @params\n    \n    parameters = Dictionary containing our parameters:\n                  parameters['W' + str(l)] = Wl\n                  parameters['b' + str(l)] = bl\n    grads = Dictionary containing our gradients for each parameters:\n                  grads['dW' + str(l)] = dWl\n                  grads['db' + str(l)] = dbl\n    v = Adam variable, moving average of the first gradient, python dictionary\n    s = Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate = the learning rate, scalar.\n    beta1 = Exponential decay hyperparameter for the first moment estimates \n    beta2 = Exponential decay hyperparameter for the second moment estimates \n    epsilon = hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    \n    parameters = Dictionary containing our updated parameters \n    v = Adam variable, moving average of the first gradient, python dictionary\n    s = Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n\n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        v[\"dW\" + str(l+1)] = beta1 * v['dW' + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta1 * v['db' + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        v_corrected[\"dW\" + str(l+1)] = v['dW' + str(l+1)] / float(1 - beta1**t)\n        v_corrected[\"db\" + str(l+1)] = v['db' + str(l+1)] / float(1 - beta1**t)\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l+1)] = beta2 * s['dW' + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)]**2)\n        s[\"db\" + str(l+1)] = beta2 * s['db' + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)]**2)\n          ### END CODE HERE ###\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".  \n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / float(1 - beta2**t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / float(1 - beta2**t)\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n    \n    return parameters, v, s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X, Y, mini_batch_size = 64):\n    \"\"\"\n    This function creates a list of random minibatches from (X, Y)\n    \n    @params\n    \n    X = Data, numpy array of shape (number of examples, number of features)\n    Y = Ground Truth/ True \"label\" vector (containing classes 0 and 1) \n        shape = (number of examples, 1)\n    mini_batch_size = size of the mini-batches (suggested to use powers of 2)\n    \n    Returns:\n    \n    mini_batches = list of synchronous (mini_batch_X, mini_batch_Y)\n    \n    \"\"\"\n    \n    np.random.seed(0)            \n    m = X.shape[0]                  # Number of training examples\n    mini_batches = []               # List to return synchronous minibatches\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:]\n    #print(\"S_X\",shuffled_X.shape)\n    shuffled_Y = Y[permutation].reshape((m,1))\n    #print(\"S_Y\",shuffled_Y.shape)\n    \n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = (m//mini_batch_size) # number of mini batches of size mini_batch_size in our partitionning\n    for k in range(num_complete_minibatches):\n        mini_batch_X = shuffled_X[k*mini_batch_size : (k+1)*mini_batch_size,:]\n        #print(\"M_X\",mini_batch_X.shape)\n        mini_batch_Y = shuffled_Y[k*mini_batch_size : (k+1)*mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)   # Tuple for synchronous minibatches\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size :,: ]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size :,: ]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_layer_model(X, Y, layers_dims, learning_rate = 0.01, mini_batch_size = 128,n=1,lambd=0.7,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True): #lr was 0.009\n    \"\"\"\n    This function implements a L-layer neural network: \n    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n    \n    Arguments:\n    X = Data, numpy array of shape (number of examples, number of features)\n    Y = Ground Truth/ True \"label\" vector (containing classes 0 and 1) \n        shape = (number of examples, 1)\n    layers_dims = List contains number of neurons in one respective layer\n                  and [len(layer_dims) - 1] gives L Layer Neural Network\n    learning_rate = learning rate of the gradient descent update rule\n    lambd = Regularization parameter, int\n    num_epochs -- number of epochs\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    t = 0                              # Used in Adam\n    #n_b = 10\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims,n)\n    v, s = initialize_adam(parameters)\n    \n    # MiniBatch Gradient Descent\n    for i in range(num_epochs):\n        minibatches = random_mini_batches(X, Y, mini_batch_size=64)\n        for minibatch in minibatches:\n          # Select a minibatch\n          (minibatch_X, minibatch_Y) = minibatch\n          #print(minibatch_X.shape,i)\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n          AL, caches = L_model_forward(minibatch_X, parameters)\n        \n        # Compute cost\n          cost = compute_cost(AL, minibatch_Y, parameters, lambd)\n    \n        # Backward propagation\n          grads = L_model_backward(AL, minibatch_Y, caches,lambd)\n \n        # Update parameters\n          t += 1\n          parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n                \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n            \n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title((\"Learning rate = {}, Lambda = {} \".format(str(learning_rate),str(lambd))))\n    plt.show()\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network\n    \n    @params\n\n    X = Data, numpy array of shape (number of examples, number of features)\n    parameters = Parameters of trained model returned by L_layer_model function \n    \n    Returns:\n\n    p = Predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[0] # Number of training examples in Dataset\n    n = len(parameters) // 2 # Number of layers in the neural network\n    p = np.zeros((m,1))\n\n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n    \n    # Set values in p to 0/1 as per predictions and threshold\n    for i in range(probas.shape[0]):\n        # As per sigmoid, values greater than 0.5 are categorized as 1\n        # and values lesser than 0.5 as categorized as 0\n        if probas[i] > 0.5:\n            p[i] = 1\n        else:\n            p[i] = 0\n    #p = np.squeeze(p)\n    y = y.reshape(p.shape)\n    acc = np.sum((p == y)/m)*100\n    print(\"Accuracy:%.2f%%\" % acc)\n\n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learned_parameters = L_layer_model(X, y, layers_dims=[19,10,1],  mini_batch_size =128, n=1, learning_rate = 0.0001,lambd=0.01, num_epochs = 4000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict(X,y,learned_parameters)\nprint(np.unique(pred), pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y, pred))\nprint(confusion_matrix(y, pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}