{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf = pd.DataFrame(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing column wise %ge of NaN values they contains \n\nfor i in df.columns:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Since data does'nt contain any null values, we can move further"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='quality', data=df)\n\nfor p in ax.patches:\n        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here this bar graph easily shows how data is imbalanced. Less than 1% data is in class __3__. So, first, we have to balance the data in to get more precise predictions.\n> For that we are using both Under Sampling and Over sampling\n"},{"metadata":{},"cell_type":"markdown","source":"> Here we are, firstly, under sampling class 5 and 6 to the level of class 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_7 = df[df['quality'] == 7]                            # Class to which we bring other classes.\n\nclass_5 = df[df['quality'] == 5].sample(n = len(class_7))   # UnderSampling the class to make data balanced\nclass_6 = df[df['quality'] == 6].sample(n = len(class_7))   # UnderSampling the class to make data balanced\n\nnew_df = pd.concat([df[df['quality'] == 3], df[df['quality'] == 4], class_5, class_6, class_7, df[df['quality'] == 8]]).sample(frac=1)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='quality', data=new_df)\n\nfor p in ax.patches:\n        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now we are over sampling remaining classes to their level"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\noversample = RandomOverSampler()\nx, y = oversample.fit_resample(new_df.drop(['quality'], axis=1), new_df['quality'])\n\nnew_df = pd.DataFrame(x, columns=df.drop(['quality'], axis=1).columns)\nnew_df['quality'] = y\n\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='quality', data=new_df)\n\nfor p in ax.patches:\n        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here we ca see that all the classes are balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"cormap = new_df.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cormap, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_df.drop(['quality'], axis=1)\ny = new_df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data to be between -1 and 1\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\nclf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Number of features to consider at every split \nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Create the random grid\ngrid = {'max_features': max_features,\n        'max_depth': max_depth,\n        'min_samples_split': min_samples_split,\n        'min_samples_leaf': min_samples_leaf,\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GridSearchCV( estimator = DecisionTreeClassifier(),  param_grid = grid, cv = 5)\nclf = clf.fit(X_train, y_train)\nclf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GridSearchCV( estimator = DecisionTreeClassifier(),  param_grid = grid, cv = 5)\nclf = clf.fit(X_train, y_train)\nclf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nmat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(mat, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# Measure the Accuracy Score\nprint(\"Accuracy score of the predictions: {0}\".format(metrics.accuracy_score(y_pred, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}