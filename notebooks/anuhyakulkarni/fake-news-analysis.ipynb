{"cells":[{"metadata":{},"cell_type":"markdown","source":"The dataset is a sentiment analysis for classifying fake and real news ,given a set of headings like text,title,subject,date .\nThis notebook gives a solution to identify fake and real news of a large data(set) with highest possible accuracy . the vocabulary of the dataset is over 150000 words, which is tried to reduce by using stop words and stemmer functions.comments are added wherever thought necessary, detailed explaination in the readme file ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import all the essential libraries\n\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer  # for stemming","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the csv into true and false\n\ntrue = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#true.head shows the first five rows ,so the colum details can be studied,same is done with false (false.head)\ntrue.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add a column called category that assigns '0' for fake news and '1' for true news\ntrue['category'] = 1\nfake['category'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#true.news to check if the category column is added\ntrue.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenate true and false into dataframe 'df'\ndf = pd.concat([true,fake])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display df\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set category column as the target and convert to a numpy array \ntarget = df['category'].values\ntarget = np.array(target, dtype='int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import seaborn for visualising the data \nimport seaborn as sns\nsns.countplot(x='category',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is fairly distributed b/w true and false , thus contributing to a non-biased model making","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#observe which subjects/topics contribute to most news in the data\nax=sns.countplot(x='subject',data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we study which subjects/topics contribute to most fake and true news\nsns.countplot(x='category',hue='subject',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***procedure to clean the dataset :** \n* Create a function clean_word that replaces punctuation with blank spaces, and appends only numbers and alphabets to 'new', excluding stopwords, and perform steeming and append the words to'new'.\n* add all the columns to 'text' and drop the unnecessary columns\n* find the largest sentence and pad rest of the sentences with zeros to match the largest sentence's length \n*  feed it into a vector which can be trained and tested for a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import stopwords, porter stemmer(for stemming)\n\nSTOPWORDS = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n\ndef clean_word(word_list):\n    global STOPWORDS\n    global stemmer\n    new = []\n    for word in word_list:\n        word = word.replace('.', '')\n        word = word.replace(',', '')\n        word = word.replace(';', '')\n        word = word.lower()\n        if (word.isalpha() or word.isdigit()) and word not in STOPWORDS:   \n            new.append(stemmer.stem(word.strip()))\n    return new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add columns 'text', 'title' and 'subject' to text ,and leave spaces b/w so as to easily feed into the model\ndf['text'] = df['text'] + \" \" + df['title'] + \" \" + df['subject']\ndf = df.drop(columns=['title','subject','date']) #drop columns that might not contribute to the prediction of target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntext = df['text'].values\n\n# Tokenize each sentence \ntext_arr = [row.split(' ') for row in text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvocab = []\nclean_text_array = []\nfor row in text_arr:\n    clean_row = clean_word(row)\n    clean_text_array.append(clean_row)\n    vocab.extend(clean_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set and list all the words to vocabulary and print the length(to get a count of the number of words)\nvocabulary = list(set(vocab))\nlen(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvectorizer = LabelEncoder()\nvectorizer.fit(vocabulary)\nprint(1)\n\n# Create token vector using Label Encoder fit on entire vocabulary\ntoken_vector = []\ni=0\n\n# declare max_words to keep count of the longest sentence vectorized\n# we need this to pad every other vector to same length as longest vector\n\nmax_words = 0 \nfor row in clean_text_array:\n    encoded = vectorizer.transform(row).tolist()\n    size = len(encoded)\n    if size>max_words: \n        max_words=size\n    token_vector.append(encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words #print max_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad each sentence with zeros to the length of the longest sentence\npadded = []\nfor row in token_vector:\n    r = np.pad(row, (0, max_words-len(row)), 'constant')\n    padded.append(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all padded sentences to example vector\n\nex_vector = np.array(padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train and test data into 80:20, data=ex_vector, target=target\n\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(ex_vector,target, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"split the data into batches and shuffle ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_dataset = tf.data.Dataset.from_tensor_slices((xtrain, ytrain))\ntest_dataset = tf.data.Dataset.from_tensor_slices((xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nBATCH_SIZE = 64\nSHUFFLE_BUFFER_SIZE = 100\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim=16 \n\n# defining the sequential model with an Embedding layer\n# Add a Global Average Pooling 1D layer to flattent the matrix into vector\n\nmodel = keras.models.Sequential([\n  keras.layers.Embedding(130590, embedding_dim), #130590 as input based on vocabulary\n  keras.layers.GlobalAveragePooling1D(),\n  keras.layers.Dense(32, activation='relu'),\n  keras.layers.Dense(1, activation='sigmoid')\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#complie the model\nmodel.compile(optimizer=keras.optimizers.Adam(0.001),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=5                  #five iterations\nhistory = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(xtrain,ytrain)[1]*100)\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(xtest,ytest)[1]*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Accuracy\nplt.plot(range(epochs), history.history['accuracy'])\nplt.plot(range(epochs), history.history['val_accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Loss\nplt.plot(range(epochs), history.history['loss'])\nplt.plot(range(epochs), history.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(ytest,pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = pd.DataFrame(cm , index = ['Fake','Not Fake'] , columns = ['Fake','Not Fake'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Not Fake'] , yticklabels = ['Fake','Not Fake'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kkkkkkkjjjjjj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"just typing random stuff so the notebook doesnt collapse due to idleness\n1 2 3 4 5 6 lkdaldlkaldkakdkdfffff\nlkslkslakslksal\ndlllllllllhhdkdkqhkdkhdqdqk hdqdhkdhnnnnn\nsjffffffffffff\nlsljslslxsxsaxkjd\nkalkxslxljldddd\nkLSLlsssmxxxx\ncacaa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}