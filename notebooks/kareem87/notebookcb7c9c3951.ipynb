{"metadata":{"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Our aim is to build a predictive model for the churn rate in a bank\nLet's get started!","metadata":{}},{"cell_type":"code","source":"# import important libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import our dataset for that bank for our research\ndirectory_src = r'~/Desktop/AI-ML Requirments/Professional Track_ITIDA_Scholarship/Project_DSC_churn_prediction'\ndirectory_dist = 'datasets/churnd'\nfile = 'churn.csv'\n\npath = os.path.join(directory_src, file)\npath_dist = os.path.join(directory_dist, file)\ndef load_data(path=path):\n    # if the directory of our dataset doesn't exist, please create it.\n    if not os.path.isdir(directory_dist):\n       os.makedirs(directory_dist) \n    df = pd.read_csv(path)\n    df.to_csv(path_dist)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = load_data() # loading and saving are done!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data investigations for gaining insights\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's investigate the data numerical attributes\ndf.describe() # ~20.37% of the customers in our data exited.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.Exited==1] # 20.37% = 2037 rows","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### let's do some visualizations including: histograms, scatterplots for gaining insights.","metadata":{}},{"cell_type":"code","source":"df.hist(bins=25, figsize=(20, 15))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(df[['Age', 'Balance',  'CreditScore', 'EstimatedSalary', 'Exited']], figsize=(15, 15))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In my opinion, we still not able to grasp the insights and correlations with `Exited` attribute so let's try another technique.","metadata":{}},{"cell_type":"code","source":"# let's split aour dataframe into numerical and categorical attributes for going in investigations in depth\ndf_num = df[list(df.describe())]\ndf_cat = df[['Surname', 'Geography', 'Gender']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's take a copy of our numerical attributes dataframe for some playing with it/feature engineering processes for investigations.\ndf_num_ = df_num.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualize our linear correlation matrix for gaining insights\ncorr_mat = df_num_.corr()\nsns.heatmap(corr_mat, linewidths=1.5)\ncorr_mat['Exited'].sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will focus on the Age, Balance attributes for gaining insights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We replace the `Age` attribute with the `AgeBucket` instead to help our model detect the impact of each Age stratum on the churn rate","metadata":{}},{"cell_type":"code","source":"# we can see that the rate of churn for each startum of the age: has a very different impact on the churn rate.\ndf_num_['AgeBucket'] = df_num_.Age//20*(20)\nprint(df_num_[['AgeBucket', 'Exited']].groupby(['AgeBucket']).mean())\ndf_num_[['AgeBucket', 'Exited']].groupby(['AgeBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per age strata')\nplt.ylabel('churn rate')\nplt.xlabel('AgeBucket strata')\nplt.legend(['Churn rate per age stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We replace the `Balance` attribute with the `BalanceBucket` instead to help our model detect the impact of each Balance stratum on the churn rate¶","metadata":{}},{"cell_type":"code","source":"# we can see that the rate of churn for each startum of the balance: has a very different impact on the churn rate.\ndf_num_['BalanceBucket'] = df_num_.Balance//50000*(50000)\nprint(df_num_[['BalanceBucket', 'Exited']].groupby(['BalanceBucket']).mean())\ndf_num_[['BalanceBucket', 'Exited']].groupby(['BalanceBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per balance strata')\nplt.ylabel('churn rate')\nplt.xlabel('BalanceBucket strata')\nplt.legend(['Churn rate per balance stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We replace the `CreditScore` attribute with the `CrScoreBucket` instead to help our model detect the impact of each Credit Card stratum on the churn rate¶","metadata":{}},{"cell_type":"code","source":"df_num_.CreditScore.hist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can see that the rate of churn for each startum of the credit score: has a very different impact on the churn rate.\ndf_num_['CrScoreBucket'] = df_num_.CreditScore//125*(125)\nprint(df_num_[['CrScoreBucket', 'Exited']].groupby(['CrScoreBucket']).mean())\ndf_num_[['CrScoreBucket', 'Exited']].groupby(['CrScoreBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per credit score strata')\nplt.ylabel('churn rate')\nplt.xlabel('Credit scor strata')\nplt.legend(['Churn rate per Credit score stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### All customers in the bank with a credit card score `less than 406`: 23/2037 customers, Exited!","metadata":{}},{"cell_type":"code","source":"df[df.CreditScore<406]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.CreditScore<405].Exited.value_counts().index.values, df[df.CreditScore<406].Exited.value_counts().index.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We replace the `EstimatedSalary` attribute with the `SalaryBucket` instead to help our model detect the impact of each Salary stratum on the churn rate¶","metadata":{}},{"cell_type":"code","source":"df.EstimatedSalary.hist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_['SalaryBucket'] = df_num_.EstimatedSalary//35000*(35000)\nprint(df_num_[['SalaryBucket', 'Exited']].groupby(['SalaryBucket']).mean())\ndf_num_[['SalaryBucket', 'Exited']].groupby(['SalaryBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per salary strata')\nplt.ylabel('churn rate')\nplt.xlabel('salary strata')\nplt.legend(['Churn rate per salary stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.EstimatedSalary<250000]['Exited'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With respect to the numerical continous attributes, we deduced that we should replace them with their corresponding categorical versions to help our model detect the pattern in our data very well","metadata":{}},{"cell_type":"markdown","source":"#### With respect to the numerical categorical attributes, they will be with no changes as they are well prepared with no missing values","metadata":{}},{"cell_type":"markdown","source":"### let's try combining some attributes to check their impact on the target attribute `Exited`","metadata":{}},{"cell_type":"code","source":"# Balance/Tenure\ndf_num_['BalancePerTenure'] = df_num_.Balance/df_num_.Tenure","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_['TenurePerAge'] = df_num_.Tenure/df_.Age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_[['BalancePerTenure', 'Exited']].groupby(['BalancePerTenure']).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Great Job! We could detect a strong variability in the `Exited` i.e. churn rate for each strata of the Balance Per Tenure attribute","metadata":{}},{"cell_type":"code","source":"print(df_num_[['BalPerTenBucket', 'Exited']].groupby(['BalPerTenBucket']).mean())\ndf_num_[['BalPerTenBucket', 'Exited']].groupby(['BalPerTenBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per Balance per tenure strata')\nplt.ylabel('churn rate')\nplt.xlabel('Balance per tenure strata')\nplt.legend(['Churn rate per Balance per tenure stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EstimatedSalary/Tenure\ndf_num_['SalaryPerTenure'] = df_num_.EstimatedSalary/df_num_.Tenure\ndf_num_['SalPerTenBucket'] = df_num_.SalaryPerTenure//35450*(35450)\nprint(df_num_[['SalPerTenBucket', 'Exited']].groupby(['SalPerTenBucket']).mean())\ndf_num_[['SalPerTenBucket', 'Exited']].groupby(['SalPerTenBucket']).mean().plot(kind='bar')\nplt.grid()\nplt.title('churn rate per Salary per tenure strata')\nplt.ylabel('churn rate')\nplt.xlabel('Salary per tenure strata')\nplt.legend(['Churn rate per Salary per tenure stratum'])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's try the balance/salary attribute\ndf_num_['BalancePerSalary'] = df_num_.Balance/df_num_.EstimatedSalary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num_.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### let's try the combination of the credit/numOfproduts attribute","metadata":{}},{"cell_type":"code","source":"df_num_['CreditPerProducts'] = df_num_.CreditScore/df_num_.NumOfProducts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df_num)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df_cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat.Geography.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat.Gender.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.Exited==1]['Surname'].value_counts()[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As long as that the `Age` attribute has the highest impact on the `Exited` (target) attribute; I am going to split the dataset into train and test based on this attribute so that all stratas of this attrbute to be represented in the test set as the train set for a better evaluation of the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\nfor train_idx, test_idx in splitter.split(df, df_num_.AgeBucket):\n    train_df = df.loc[train_idx]\n    test_df = df.loc[test_idx]\ntrain_df.shape, test_df.shape   ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I need to evaluate that our test dataset is including the same distribution of the values within the `AgeBucket` attribute as the same as the training dataset so as not to have a `sampling bias`in the model during evaluation\n\nI am going to visualize the histogram for both the AgeBucket in both the train, test datasets","metadata":{}},{"cell_type":"code","source":"# create the AgeBucket attribute in both the train, test datasets\ntrain_df['AgeBucket'] = train_df.Age//15*(15)\ntest_df['AgeBucket'] = test_df.Age//15*(15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.AgeBucket.hist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.AgeBucket.hist() # identical histograms","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It's the time to build our transformers to preprocess our training dataset for the machine learning algorithm as we mentioned in the above cell","metadata":{}},{"cell_type":"code","source":"df_ = train_df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The basic steps for preprocessing implemented using a pipeline for automation:\n**the following is with respect to numerical (continous and discrete) attributes.**\n1. replace `Age` with `AgeBucket`\n2. replace `Balance` with `BalanceBucket`\n3. replace `CreditScore` with `CrScoreBucket`\n4. create `TenurePerAge` attribute\n5. create `BalancePerSalary` attribute\n6. create `CreditPerProducts` attribute\n7. drop `RowNumber`, `CustomerId`  attributes\n\n**the following is with respect to categorical attributes.**\n1. create `ExitedNameRatio` attribute instead of the `Surname` attribute\n2. encode `Gender`, `Geography` using the OneHotEncoder","metadata":{}},{"cell_type":"code","source":"df_.Tenure/df_.Age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_crscore, idx_age, idx_tenure, idx_balance, idx_products, idx_estsalary = 0, 1, 2, 3, 4, 7\n\nclass create_buckets(BaseEstimator, TransformerMixin):\n      def fit(self, X, y=None):\n          return self\n      def transform(self, X, y=None):\n          # convert the dataframe into 2d numpy array  \n          data_array = X.values \n            \n          # we aim to replace the Age attribute with it's bucket array  \n          ageBucket = data_array[:, idx_age]//15*(15)\n          balanceBucket = data_array[:, idx_balance]//50000*(50000)\n          crscoreBucket = data_array[:, idx_crscore]//125*(125)\n          tenureperage =  data_array[:, idx_tenure]/data_array[:, idx_age]\n          balpersal = data_array[:, idx_balance]/data_array[:, idx_estsalary]  \n          creditperproduct = data_array[:, idx_crscore]/data_array[:, idx_products]\n            \n          # replace now and add the new created attributes \n          data_array[:, idx_age] = ageBucket\n          data_array[:, idx_balance] = balanceBucket\n          data_array[:, idx_crscore] = crscoreBucket\n          return np.c_[data_array[:, :-1], tenureperage, balpersal, creditperproduct]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obj = create_buckets()\nz = obj.fit_transform(df_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z[10:20]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z.shape #Done!","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'Akubundu' in df['Surname'].values","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_.Surname","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_surname = 0\n\ndef get_ratio(series_surname, series_exited, series_not_exited):\n    ratio_exited = []\n    # for each name in the surnames of our whole data\n    for name_ in series_surname.values:\n        val_exited = 0\n        val_not_exited = 0\n        total = 0\n        if name_ in series_exited.index:\n           val_exited = series_exited[name_]\n        if name_ in series_not_exited.index:\n           val_not_exited = series_not_exited[name_]\n        total = val_exited + val_not_exited\n        ratio_exited.append(val_exited/total)  \n    return ratio_exited\n\nclass count_exited(BaseEstimator, TransformerMixin):\n      def fit(self, X, y=None):\n          # we aim to get the count of names in both cases when that name exited and didn't exit to get the ratio of exited with respect to didn't exit for that name \n          series_exited = X[X['Exited']==1]['Surname'].value_counts()\n          series_not_exited = X[X['Exited']==0]['Surname'].value_counts()\n          # we aim to get the list of counts for each exited name by get_freq(self.series_) function\n          self.ratio_exited = get_ratio(X.Surname, series_exited, series_not_exited)  \n          return self\n      def transform(self, X, y=None):\n          data_ = X.values\n          ratios_ = np.array(self.ratio_exited)\n          # we replace the surname attribute with the ratio of exited for that surname and then to drop the target attribute\n          data_[:, 0] = ratios_\n        \n          return data_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obj = count_exited()\nz = obj.fit_transform(df_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_.Surname.value_counts()['Andrews']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = df_[df.Exited==1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.Surname.value_counts()['Fanucci']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp2 = df_[df_.Exited==0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp2.Surname.value_counts()['Fanucci']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z[21:30]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_[df_.Exited==0]['Surname'].value_counts()[20:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array([90, 80, 56, 78, 90, 80, 56, 78, 90, 80, 56, 78]).reshape(3, 4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x[:, 2:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combine the output of the numerical and categorical attributes\nUsing the column transformer library.","metadata":{}},{"cell_type":"markdown","source":"idx_crscore, idx_age, idx_tenure, idx_balance, idx_products, idx_estsalary = 0, 1, 2, 3, 4, 7\nidx_surname = 0","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\nattr_list = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\nfull_pipe = ColumnTransformer([\n    ('count_excited', count_exited(), ['Surname', 'Exited']),\n    ('num_pipe', create_buckets(), attr_list),\n    ('scaler', StandardScaler(), ['EstimatedSalary']),\n    ('encode_geo_gender', OneHotEncoder(), ['Geography', 'Gender'])\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ = train_df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting the training labels\ny_train = df_.Exited.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data = full_pipe.fit_transform(df_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Great Job ! We could transform our training dataframe `sucessfully` ;).","metadata":{}},{"cell_type":"code","source":"final_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data[8000:8020] #1, 7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_.iloc[8999]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Great Transformation Job is done here... !","metadata":{}},{"cell_type":"markdown","source":"#### We aim to create our transformed dataframe on the training dataset and to save it for furture researches","metadata":{}},{"cell_type":"code","source":"list_transformed_attrs = ['SurnamePercent', 'Exited', 'CreditCardBucket', 'AgeBucket', 'Tenure', 'BalanceBucket', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'TenurePerAge', 'BalancePerSalary', 'CreditPerProduct', 'EstSalaryScaled', 'Geo.France', 'Geo.Germany', 'Geo.Spain', 'Gen.Female', 'Gen.Male']\n\nlen(list_transformed_attrs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_transformed = pd.DataFrame(final_data, columns=list_transformed_attrs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see it's structure!\ntrain_df_transformed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_transformed.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Great!**","metadata":{}},{"cell_type":"code","source":"# save this dataframe\ntrain_df_transformed.to_csv('datasets/churnd/churn_train_transformed.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's start to train our model\nI am going to try/fine-tune the following models for our Churn Rate Problem: `Binary Classification 1: Churned, 0: Didn't Churn`\n1. `Stochastic Gradient Descent Classifier`\n2. `Support Vector Machine Classifier`\n3. `Desicion Tree Classifier`\n4. `Random Forest Classifier`\n\nLet's Get Started!","metadata":{}},{"cell_type":"code","source":"# We aim here to remove the target labels away from our final training 2d-numpy array for training.\nX_train = np.c_[final_data[:, 0], final_data[:, 2:]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, final_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nmodel_SGD = SGDClassifier()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_SGD.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model prediction on the training data\nSGD_train_pred = model_SGD.predict(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[:10].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SGD_train_pred[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It looks that the stochastic gradient descent classifier is `underfitting` the training dataset with overall accuracy = 63.62%**\nWe will need to use a more powerful model for a better prediction.","metadata":{}},{"cell_type":"code","source":"# let's evaluate the overall model accuracy on the training dataset\nfrom sklearn.metrics import accuracy_score\n\ntrain_accuracy = accuracy_score(y_train, SGD_train_pred)\ntrain_accuracy*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's evaluate the model accuracy on multiple training rounds\nfrom sklearn.model_selection import cross_val_score\n\ncross_SGD_accuracy = cross_val_score(model_SGD, X_train, y_train, cv=10, scoring='accuracy')\n(cross_SGD_accuracy.mean())*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's investigate the model confusion matrix on training dataset\nfrom sklearn.metrics import confusion_matrix\nconf_SGD = confusion_matrix(y_train, SGD_train_pred)\nconf_SGD","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\ncross_SGD_pred = cross_val_predict(model_SGD, X_train, y_train, cv=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's investigate the model confusion matrix on multiple rounds of training\nfrom sklearn.metrics import confusion_matrix\nconf_SGD = confusion_matrix(y_train, cross_SGD_pred)\nconf_SGD","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the model precision/recall/f1 metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision_score(y_train, cross_SGD_pred), recall_score(y_train, cross_SGD_pred), f1_score(y_train, cross_SGD_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimal classifier is:\nconf_SGD_ = confusion_matrix(y_train, y_train)\nconf_SGD_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's investigate the SGradientDescent classifier Precision/Recall plot, ROC curve\nfrom sklearn.metrics import precision_recall_curve\n\ncross_SGD_score = cross_val_predict(model_SGD, X_train, y_train, cv=3, method='decision_function')\nprec_sgd, recall_sgd, thre_sgd = precision_recall_curve(y_train, cross_SGD_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision_recall_curve_plot(precisions, recalls, thresholds):\n    # plot() is used for continous values\n    plt.plot(thresholds, precisions[:-1], 'r--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'b-', label='recall')\n    plt.xlabel('Threshold')\n    plt.legend()\n    plt.grid()\n    plt.title('Precision - Recall Curve vs Threshold')\n    plt.figure(figsize=(25, 10))\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_recall_curve_plot(prec_sgd, recall_sgd, thre_sgd)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision_recall_direct_plot(recalls, precisions):\n    plt.plot(recalls[:-1], precisions[:-1])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid()\n    plt.legend(['Recall-Precision'])\n    plt.title('Precisin - Recall Curve Directly')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_recall_direct_plot(prec_sgd, recall_sgd)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's investigate the model roc curve\nfrom sklearn.metrics import roc_curve\nfpr_sgd, tpr_sgd, thre_sgd_roc = roc_curve(y_train, cross_SGD_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roc_curve_plot(FPR, TPR):\n    plt.plot(FPR, TPR, 'r-', label='FPR : TPR')\n    plt.plot([0, 1], [0, 1], 'b--', label='diagonal') # plotting the diagonal of the ROC curve.\n#     plt.plot([0, 0.4], [0.4, 0.98], 'b--', label='checked:)') # plotting the diagonal of the ROC curve.\n    plt.legend()\n    plt.grid()\n    plt.xlabel('False Postive Rate')\n    plt.ylabel('True Postive Rate')\n    plt.title('ROC curve of TPRate, FPRate')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_curve_plot(fpr_sgd, tpr_sgd)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the stochastic gradient descent classifier is very poor on the training dataset and is considered as a very weak model","metadata":{}},{"cell_type":"code","source":"# area under the roc curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_train, cross_SGD_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's try to train our support vector machine model\nfrom sklearn.svm import SVC\n\nmodel_SVC = SVC()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_SVC.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the SVC model on the training dataset","metadata":{}},{"cell_type":"code","source":"SVC_train_pred = model_SVC.predict(X_train)\nSGD_train_pred[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[:10].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model accuracy on training dataset\naccuracy_score(y_train, SVC_train_pred)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the confusion matrx on trainig dataset\nconfusion_matrix(y_train, SVC_train_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Much better results","metadata":{}},{"cell_type":"code","source":"# evaluate on multiple rounds of trainig\ncross_SVC_score = cross_val_score(SVC(), X_train, y_train, cv=10, scoring='accuracy')\n(cross_SVC_score.mean())*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the accuracy on multiple trainig rounds\ncross_SVC_pred = cross_val_predict(SVC(), X_train, y_train, cv=10)\naccuracy_score(y_train, cross_SVC_pred)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_train, cross_SVC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_SVC_score = cross_val_predict(SVC(), X_train, y_train, cv=10, method='decision_function')\nprec_svc, recall_svc, thre_svc = precision_recall_curve(y_train, cross_SVC_score)\nprecision_recall_curve_plot(prec_svc, recall_svc, thre_svc)\nprecision_recall_direct_plot(prec_svc, recall_svc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We got an overall poor performance as well with the support vector machine classifier**","metadata":{}},{"cell_type":"code","source":"# evaluate the roc curve\nfpr_svc, tpr_svc, thre_svc_roc = roc_curve(y_train, cross_SVC_score)\nroc_curve_plot(fpr_svc, tpr_svc)\nroc_auc_score(y_train, cross_SVC_score)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's evaluate the decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel_DTC = DecisionTreeClassifier()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_DTC.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DTC_train_pred = model_DTC.predict(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.values[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DTC_train_pred[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For sure, our decision tree classifier reached accuracy = 100% on training dataset indicating **Overfitting** problem","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_train, DTC_train_pred)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_DTC_train = cross_val_score(model_DTC, X_train, y_train, cv=10, scoring='accuracy')\n(cross_DTC_train.mean())*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_DTC_pred = cross_val_predict(model_DTC, X_train, y_train, cv=10)\ncross_DTC_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the confusion matrix of multiple training rounds\nconfusion_matrix(y_train, cross_DTC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_score(y_train, cross_DTC_pred), recall_score(y_train, cross_DTC_pred), f1_score(y_train, cross_DTC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_DTC_ = DecisionTreeClassifier(max_depth=7)\ncross_DTC_pred = cross_val_predict(model_DTC_, X_train, y_train, cv=10, method='predict_proba')\ncross_DTC_pred[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_DTC_score = cross_DTC_pred[:, 1] # labels of the +ve class only.`\nprec_dtc, recall_dtc, thre_dtc = precision_recall_curve(y_train, cross_DTC_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_recall_curve_plot(prec_dtc, recall_dtc, thre_dtc)\nprecision_recall_direct_plot(prec_dtc, recall_dtc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr_dtc, tpr_dtc, thre_dtc_roc = roc_curve(y_train, cross_DTC_score)\nroc_curve_plot(fpr_dtc, tpr_dtc)\nroc_auc_score(y_train, cross_DTC_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's try the random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_RFC = RandomForestClassifier()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_RFC.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFC_train_pred = model_RFC.predict(X_train)\nRFC_train_pred[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.values[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy on multiple training processes\ncross_RFC_train_score = cross_val_score(model_RFC, X_train, y_train, cv=10, scoring='accuracy')\n(cross_RFC_train_score.mean())*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix for Random forest classifier\ncross_RFC_pred = cross_val_predict(model_RFC, X_train, y_train, cv=5)\nconfusion_matrix(y_train, cross_RFC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_RFC_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_score(y_train, cross_RFC_pred), recall_score(y_train, cross_RFC_pred), f1_score(y_train, cross_RFC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_RFC_score = cross_val_predict(model_RFC, X_train, y_train, cv=10, method='predict_proba')\ncross_RFC_score # we got the probability for each customer to be within which class either: churned or didn't churn","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's evaluate the model performance with the precision/recall metrics\n#  get the +ve class probability only as the score\ncross_RFC_score = cross_RFC_score[:, 1]\nprec_rfc, recall_rfc, thre_rfc = precision_recall_curve(y_train, cross_RFC_score)\nprecision_recall_curve_plot(prec_rfc, recall_rfc, thre_rfc)\nprecision_recall_direct_plot(prec_rfc, recall_rfc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr_rfc, tpr_rfc, thre_rfc_roc = roc_curve(y_train, cross_RFC_score)\nroc_curve_plot(fpr_rfc, tpr_rfc)\nroc_auc_score(y_train, cross_RFC_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We concluded that the **`random forest classifier`** is the most promising model to be used.\nWith **`AUC` = 92.37%**","metadata":{}},{"cell_type":"markdown","source":"Let's `fine-tune` our model","metadata":{}},{"cell_type":"code","source":"from scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = [\n    {'n_estimators': randint(low=1, high=200), 'max_features':randint(low=1, high=17), 'max_depth': randint(low=1, high=50)}\n]\n\nmodel_RFC_randcv = RandomizedSearchCV(model_RFC, param_dist, cv=3, random_state=42, return_train_score=True, verbose=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_RFC_randcv.fit(X_train, y_train)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_RFC_randcv.best_estimator_, model_RFC_randcv.best_score_, model_RFC_randcv.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_RFC_pred = cross_val_predict(model_RFC_randcv.best_estimator_, X_train, y_train, cv=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_train, cross_RFC_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_RFC_score = cross_val_predict(model_RFC_randcv.best_estimator_, X_train, y_train, cv=10, method='predict_proba')\ncross_RFC_score = cross_RFC_score[:, 1] # the proba of the +ve class to be used as a score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's evaluate the model performance with the precision/recall metrics\n#  get the +ve class probability only as the score\nprec_rfc, recall_rfc, thre_rfc = precision_recall_curve(y_train, cross_RFC_score)\nprecision_recall_curve_plot(prec_rfc, recall_rfc, thre_rfc)\nprecision_recall_direct_plot(prec_rfc, recall_rfc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr_rfc, tpr_rfc, thre_rfc_roc = roc_curve(y_train, cross_RFC_score)\nroc_curve_plot(fpr_rfc, tpr_rfc)\nroc_auc_score(y_train, cross_RFC_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_RFC_randcv.best_estimator_ # Done!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's evaluate the model performance on the testing dataset\n","metadata":{}},{"cell_type":"code","source":"final_test_data = full_pipe.fit_transform(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_data[-10:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's drop the target labels\nX_test = np.c_[final_test_data[:, 0], final_test_data[:, 2:]]\ny_test = test_df.Exited.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_final_pred = model_RFC_randcv.best_estimator_.predict(X_test)\nconfusion_matrix(y_test, rfc_final_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_score(y_test, rfc_final_pred), recall_score(y_test, rfc_final_pred), f1_score(y_test, rfc_final_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_final_score = model_RFC_randcv.best_estimator_.predict_proba(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_final_score = rfc_final_score[:, 1] # we take the +ve class probability as the score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prec_rfc, recall_rfc, thre_rfc = precision_recall_curve(y_test, rfc_final_score)\nprecision_recall_curve_plot(prec_rfc, recall_rfc, thre_rfc)\nprecision_recall_direct_plot(prec_rfc, recall_rfc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr_rfc, tpr_rfc, thre_rfc_roc = roc_curve(y_test, rfc_final_score)\nroc_curve_plot(fpr_rfc, tpr_rfc)\nroc_auc_score(y_test, rfc_final_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, rfc_final_pred)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is the end of our classification Journey  ;)","metadata":{}},{"cell_type":"markdown","source":"Final results on the testing dataset after fine-tuning: <br>\n1. `Precision: 93.23%`<br>\n2. `Recall:    86.06%`<br>\n3. `accuracy:  95.8%`<br>","metadata":{}},{"cell_type":"markdown","source":"### Saving our final model","metadata":{}},{"cell_type":"code","source":"import joblib\njoblib.dump(model_RFC_randcv.best_estimator_, 'final_RFC_fine_tuned.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving our final transformed testing dataset","metadata":{}},{"cell_type":"code","source":"test_df_transformed = pd.DataFrame(final_test_data, columns=list_transformed_attrs)\ntest_df_transformed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_transformed.to_csv('datasets/churnd/test_df_transformed.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}