{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Predicting Medical Insurance Costs in the US**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Training a linear regression model to predict medical insurance costs in the US.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#read in csv file\ninsurance = pd.read_csv('/kaggle/input/insurance/insurance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **EXPLORING THE DATASET**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 1338 entries, and 7 variables in this dataset. There are no missing values. The variables in this dataset are:\n\n* age: age of primary beneficiary\n* sex: sex of primary beneficiary - female or male\n* bmi: body mass index (healthy bmi is considered to be between 18.5 and 24.9)\n* children: number of children covered by health insurance / number of dependents\n* smoker: is the beneficiary a smoker - yes or no\n* region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* charges: individual medical costs billed by health insurance\n\nWe will be trying to predict the medical costs based on one or more of the features listed. Our target variable is 'charges'. This is a continuous variable, and so the prediction model will be a **linear regression** model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Three of the seven variables are categorical variables -'sex', 'smoker', and 'region'. 'sex' and 'smoker' are binary in nature (male/female, yes/no). We will create dummy variables for each of the three categorical variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_vars = ['sex', 'smoker', 'region']\n\nfor var in categorical_vars:\n    dummies = pd.get_dummies(insurance[var], prefix = var)\n    insurance = pd.concat([insurance, dummies], axis=1)\n    \ninsurance.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of dropping the features we do not need, we will create a list of the features to be used in the regression model which we will use to index into the *insurance* dataset. We will drop features from the list when they are not required for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of features for model\nfeatures = insurance.columns.tolist()\n#removing categoricals 'sex', 'smoker', 'region' as we have created dummy variables for them; also remove target variable 'charges'\nfor f in ['sex','smoker','region','charges']:\n    features.remove(f)\n#check\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the basic distribution of the features which are continuous in nature and also of the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance[['age', 'bmi', 'children', 'charges']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average age of the dataset is 39, and the ages range from 18 - 64 (no elderly people in the dataset - the retirement age in the US is 62). \n\nThe average bmi is 30 which is considered to be the lower bound of the bmi obese range. The smallest bmi in the dataset is 15.96 which is severely underweight. Interestingly, three quarters of the dataset population are overweight, according to thee bmi metric.\n\nThe average number of children is 1.\n\nThere is huge variation in the medical charges. The maximum charge is at least 3.8 times larger than the charges of three quarters of the dataset. \n\nWe will note here that before we fit the regression model, we will need to scale the 'age', bmi', 'children', and 'charges' variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Strong correlation between features will lead to an inefficient model. We know that the dummy variables 'sex_male' and 'sex_female' are strongly related as one implies the other so we will drop 'sex_male' from our list of features for the model. Likewise, we will drop 'smoker_no'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in ['sex_male','smoker_no']:\n    features.remove(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pairwise correlation between features 'age','bmi','children', and the target variable 'charges'\ncorr = insurance[['age','bmi','children','charges']].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap=\"YlOrRd\", annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'age', 'bmi', and 'children' are  weakly correlated with each other and the target variable. 'children' is the feature with the weakest correlation with the target variable.\n\nLet's plot each of the above features and 'charges' to see if there are any non-linear relationships.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plots\nfig, axs = plt.subplots(3,3, figsize= (20,20))\n\nf = 0\nfor i in range(3):\n    for j in range(3):\n        axs[i,j].scatter(insurance[features[f]], insurance['charges'], marker = 'x')\n        axs[i,j].set_title('Charges and '+ features[f])\n        plt.xlabel(features[f])\n        plt.ylabel('charges')\n        f += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Things to note from the scatter plots:\n\n* There is an upward trend in charges with age. \n* Charges trend upwards somewhat with bmi, but most points are clumped together on the bottom with no discernible trend.\n* The maximum amount of charges drop with the fourth and fifth child.\n* Smokers pay higher charges, on average, than non-smokers.\n* There are no striking relationships between charges and sex or between charges and any of the regions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will first split the model into training and testing sets, and then scale each set using mean normalisation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the insurance data set into train and test datasets\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(insurance[features], insurance['charges'], test_size = 0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scale features and target\nfor f in ['age', 'bmi', 'children']:\n    train_X[f + '_scaled'] = (train_X[f]- train_X[f].mean())/(train_X[f].std())\n    test_X[f + '_scaled'] = (test_X[f]- test_X[f].mean())/(test_X[f].std())\n    features.append(f + '_scaled')\n    features.remove(f)\n    \n    \ntrain_y = (train_y - train_y.mean())/(train_y.std())\ntest_y = (test_y - test_y.mean())/(test_y.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features)\nprint(train_X.columns)\nprint(test_X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IMPLEMENTING GRADIENT DESCENT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p><font size = 3><ins>**Hypothesis function:**</ins></font></p>\n<p>$ h_\\theta(x) = \\theta_0 + \\theta_1*sex\\_female + \\theta_2*smoker\\_yes + \\theta_3*region\\_northeast + \\theta_4*region\\_northwest + \\theta_3*region\\_southeast + \\theta_6*region\\_southwest + \\theta_7*age + \\theta_8*bmi + \\theta_9*children$</p>\n <p>$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4 + \\theta_3x_5 + \\theta_6x_6 + \\theta_7x_7 + \\theta_8x_8 + \\theta_9x_9 = \\theta^Tx$</p>\n\n<p><font size = 3><ins>**Cost Function:**</ins></font></p>\n<p>$Cost(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m ( h_\\theta(x^{(i)}) - y^{(i)} )^2 $ where **m** is the number of entries</p>\n\n<p><font size = 3><ins>**Gradient Descent**</ins></font></p>\n\n<p>Minimise $Cost(\\theta)$ with respect to $\\theta$</p>\n\n<p>Repeat**{**</p>\n<p>$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}Cost(\\theta)$</p>\n<p>simulataneously update $\\theta_j$ for **j = 0,...,n** **}** where $\\alpha$ is the learning rate and\n\n$\\frac{\\delta}{\\delta\\theta_j}Cost(\\theta) = \\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$.</p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorised cost function\n\ndef CostFunction(X, y, theta):\n    \n    m = len(y)\n    cost = (1/(2*m))*np.sum((X@theta - y)**2)\n    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef GradientDescent(X,y,theta,alpha, precision):\n  \n    m = len(y)\n    cost_history = []\n    \n    previous_cost = CostFunction(X, y, theta)\n    cost_history.append(previous_cost)\n    cost = 0\n    iter = 1\n    \n    while (previous_cost - cost) > precision:\n        if iter == 1:\n            pass\n        else:\n            previous_cost = cost\n        theta_temp = np.copy(theta)\n        for j in range(len(theta)):\n            theta[j] = theta[j] - (alpha/m)*np.sum((X@theta_temp - y)*X[:,j])\n        cost = CostFunction(X,y,theta)\n        cost_history.append(cost)\n        iter +=1\n    \n    return theta, cost_history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will convert our data into numpy arrays to work with the algorithm function above, and initalise the vector theta of parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#store target variable in y as numpy array\ny = np.array(train_y)\nm = len(y)\n\n#X is matrix with features data and a columns of ones as the first column\narray_train = np.array(train_X[features])\nX = np.insert(array_train, 0, 1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a learning rate of 0.01 and stop the algorithm when there is no difference between the costs of the last two iterations. We will also time how long it took the cost function to converge. We will begin with initial weights of 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import perf_counter \n\ntime_start = perf_counter()\nopt_theta, cost_hist_1 = GradientDescent(X, y, theta = np.zeros(X.shape[1]), alpha = 0.01, precision = 0)\ntime_stop = perf_counter()\nprint('Convergence time: {:.2f} seconds'.format(time_stop - time_start))\nprint('Converged at cost = {:.4f}'.format(cost_hist_1[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The gradient function took 5 seconds to converge with a learning rate of 0.01. We will plot the cost history of the algorithm to see if it worked as intended.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9.6, 7.2))\nplt.plot(cost_hist_1)\nplt.xlabel('Number of iterations')\nplt.ylabel('Mean Squared Error')\nplt.title('Cost history with learning rate = 0.01')\nplt.show()\n\nprint('Function converged after {} iterations.'.format(len(cost_hist_1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cost has decreased with each iteration. It took over 10000 iterations for the cost function to converge. \n\nWe will now increase the learning rate to 0.1 to see if it converges faster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#learning rate increased to 0.1\ntime_start = perf_counter()\nopt_theta, cost_hist_2 = GradientDescent(X, y, theta = np.zeros(X.shape[1]), alpha = 0.1, precision = 0)\ntime_stop = perf_counter()\nprint('Convergence time: {:.2f} seconds'.format(time_stop - time_start))\nprint('Converged at cost = {:.4f}'.format(cost_hist_2[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Increasing the learning rate to 0.1 decreased the time for the function to converge by roughly 90%. We will plot the cost history to see how the function converged.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9.6, 7.2))\nplt.plot(cost_hist_2)\nplt.xlabel('Number of iterations')\nplt.ylabel('Mean Squared Error')\nplt.title('Cost history with learning rate = 0.1')\nplt.show()\n\nprint('Function converged after {} iterations.'.format(len(cost_hist_2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Increasing the learning rate from 0.01 to 0.1 reduced the number of iterations necessary for the function to converge by {:.2f}%'.format(\n    ((len(cost_hist_1) - len(cost_hist_2))/(len(cost_hist_1)))*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The costs at which the function converged with the learning rates of 0.01 and 0.1 are the same. The cost function is convex and has a global minimum, so the algorithm, if working properly, will always find the global minimum of the cost function. The cost is basically the mean squared error of the function for a given weight (theta).\n\n\nLet's now use the weights which minimised the cost function on the test data, and see how well the model performs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n#convert test_X into numpy array with initial column of ones\narray_test = np.array(test_X[features])\nX_test = np.insert(array_test, 0, 1, axis=1)\n#calculate predictions on test data\npredictions_test = X_test@opt_theta\n#mean squared error\nmse_gd = CostFunction(X_test, test_y, opt_theta)\nprint('The mean squared error of the model on test data is {:.3f}.'.format(mse_gd))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test mean squared error (0.099) is much better than the training mean squared error (13.14). Usually, the test score would be higher than the training score. This suggests that the model will generalise well to unseen data and is not over fitting. Tentatively, this could be a good model for predicting US medical insurance costs. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This dataset has been useful to practice implementing the gradient descent algorithm for linear regression. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}