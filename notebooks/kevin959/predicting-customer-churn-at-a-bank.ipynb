{"cells":[{"metadata":{"_uuid":"9eb373e261569d67a73e6f705ebfb8e206fc692e"},"cell_type":"markdown","source":"# **A Machine Learning program detecting bank churns ** "},{"metadata":{"_uuid":"4e9da85fab7fa061ee9eddccf1e5d5c205d2fd8b"},"cell_type":"markdown","source":"## **Introduction**\n\nCustomer churn is a term used to refer to customers who leave the financial institution they've been working with. It has emerged as one of the major problems for financial institutions including banks ([ukessays, 2016](http://www.ukessays.com/essays/marketing/customer-churn-management-in-banking-and-finance-marketing-essay.php)).\n\nThe following program codes contain three phases including:\n\n        1. Analyzing data and feature engineering\n\n        2. Building a Machine Learning Mode\n\n        3. Building a Demo-API detecting Bank Churn from user inputs\n      \n\n\n**Key terms used in the codes**\n1. Model: A machine learning term used to represent an algorithm that learns from the data and make predictions\n1. Features : Usually known as columns\n2. Observations: Usually known as rows\n3. Label or Target: A column which is being predicted\n4. Outlier: A value situated away form the mean \n4. Feature engineering: Checking the correlation between features and the target and removing features which doesn't support the accuracy of our model."},{"metadata":{"_uuid":"12e60198a8a19e2b6154373cecd16613f3fbf789"},"cell_type":"markdown","source":"# **Analyzing data and feature engineering**"},{"metadata":{"_uuid":"c68ff2c93fa66b7aa1f38df9f780a801590da884"},"cell_type":"markdown","source":"## Overview on the Dataset\n\n1. The dataset is a labelled dataset (has features determining the target) thus we'll use **Supervised Learning models**\n2. The target column is classifying the customer into Churned customer or not, thus we'll implement **Classification models**"},{"metadata":{"_uuid":"1e5ead9d0e9a67b3855f49b353f8d59b59fdcb5f"},"cell_type":"markdown","source":"### 1. Importing all Libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # For data manipulation\nimport pandas as pd # For data representation\nimport matplotlib.pyplot as plt # For basic visualization\nimport seaborn as sns  # For synthetic visualization\nfrom sklearn.cross_validation import train_test_split # For splitting the data into training and testing\nfrom sklearn.neighbors import KNeighborsClassifier # K neighbors classification model\nfrom sklearn.naive_bayes import GaussianNB # Gaussian Naive bayes classification model\nfrom sklearn.svm import SVC # Support Vector Classifier model\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier model\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression model\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest Classifier model\nfrom sklearn.metrics import accuracy_score # For checking the accuracy of the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e03185167ff2a1851956fa9ecdd16e9c69ea9b84"},"cell_type":"markdown","source":"### 2. Importing the dataset and grasping basic insights "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Importing the dataset\nchurn_dataset = pd.read_csv('../input/Churn_Modelling.csv')\n# Visualizing first five elements in the dataset\nchurn_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce747a2383403b3acd52449c8100dd066fc6522b"},"cell_type":"code","source":"# Checking basic information (rows, columns, missing values, datatypes of columns, etc) in our dataset\nchurn_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56af32a2acc57f8579fe8c2653bf16d230fa76e0"},"cell_type":"markdown","source":"From the above cell, we conclude that there are no missing values in our dataset (since all features have 10000 non-null values). \n\nHowever we can identify some features which have non-numerical data (usually represented as objects) and we'll need to delete them or encode them into numbers (**Because computers don't understand texts thus we transfer them into encoded numbers**). Those features are:\n1. Surname\n2. Geography\n3. Gender"},{"metadata":{"trusted":true,"_uuid":"408e7b214b031c8f11c0c0fec3a3725d53fefbc4"},"cell_type":"code","source":"# Checking statistical information in our dataset\nchurn_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f96e935ba7bb4c3f74e80c1e034f77aac073ab3"},"cell_type":"markdown","source":"**As for features with numerical datatypes, we analyze their statistical distributions** (count, mean, standard deviation, median, etc). \nThis also help us to easily detect outliers (ex: if the maximum value on the age column was 200, that would easily mark the presence of outliers in our dataset)."},{"metadata":{"_uuid":"9e225d0f2bf3d62d7ad7a6154386810cbb0b7930"},"cell_type":"markdown","source":"### 3. Feature Engineering"},{"metadata":{"_uuid":"4bc3418ecc1d34b99f4fd0324c17d0123ff3dfd1"},"cell_type":"markdown","source":"**a. Working with categorical features (Features with non-numerical datatypes)**\n\nWe'll be analyzing the likeability of encoding values in these features by checking their unique characters (we'll automatically drop the feature if it has more than 3 meaningful unique characters for better accuracy)."},{"metadata":{"trusted":true,"_uuid":"e409875ca88690a21aa7eee2dfe68afd23b70ef6"},"cell_type":"code","source":"# Checking set of unique characters in each categorical feature\nfor col in churn_dataset.columns:  # Looping over all columns \n    if churn_dataset[col].dtypes == 'object':\n        num_of_unique_cat = len(churn_dataset[col].unique()) # Checking the length of unique characters\n        print(\"feature '{col_name}' has '{unique_cat}' unique categories\".format(col_name = col, unique_cat=num_of_unique_cat))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4776e95c9466a3bafa785803a36cd1a9bb1cd37a"},"cell_type":"markdown","source":"* Since unique values in the Surname feature are more than 3, we'll consider** deleting the feature**"},{"metadata":{"trusted":true,"_uuid":"618db3b0ba6065392afeaa296724451f03620bd2"},"cell_type":"code","source":"# Deleting the Surname feature from the dataset\nchurn_dataset = churn_dataset.drop(\"Surname\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdf3ec695a9201efe05e06bdc7ce971a0ee22a8d"},"cell_type":"code","source":"# Creating a pivot table demonstrating the percentile\n# Of different genders and geographical regions in exiting the bank \nvisualization_1 = churn_dataset.pivot_table(\"Exited\", index=\"Gender\", columns=\"Geography\")\nvisualization_1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1a54e593b710cc9baf6a4284412dac38075290c"},"cell_type":"markdown","source":"From the table above, we can easily detect the following trends:\n1. Many females have exited the bank than males in all regions represented in the Dataset\n2. Germany is the country with many bank churns\n\nHowever, though these features (Geography and Gender) might correlate with the target column, it is better to drop them from our model to preserve the universality of our prediction model (not only predicting values from Germany, France and Spain)."},{"metadata":{"trusted":true,"_uuid":"1038c618dcc524a880bf4bc298456307329504ba"},"cell_type":"code","source":"# Deleting gender and geography features from the dataset\nchurn_dataset = churn_dataset.drop([\"Geography\", \"Gender\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8caaec1f852103090d613bfb96de9685e0ec92ca"},"cell_type":"markdown","source":"**b. Working with numerical features**\n\nWe'll be analyzing the correlation between these features and the target"},{"metadata":{"_uuid":"8f5cb0f92fa710cb76c7d52e50e40dc9499419b5"},"cell_type":"markdown","source":"Some features such as RowNumber and CustomerId contain personal informations which doesn't affect our model, thus they should also be removed from the dataset"},{"metadata":{"trusted":true,"_uuid":"5155ad2a1d0ad0e63450d8dba96cb359f98ceee9"},"cell_type":"code","source":"# Removing RowNumber and CustomerId features from the dataset\nchurn_dataset = churn_dataset.drop([\"RowNumber\", \"CustomerId\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6e3878b0bf9d57456b477f1e21ff76a30602c40"},"cell_type":"code","source":"correlation = churn_dataset.corr()\nsns.heatmap(correlation.T, square=True, annot=False, fmt=\"d\", cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc0f0f208986daa2da0f6aee4418aa05bf952c23"},"cell_type":"markdown","source":"The above heatmap easily depict how different features correlate among themselves (including against the target feature: \"Exited\").\n\nTrends from the heatmap visualization:\n1. All features have a weak or strong correlation with the target (Thus we are considering all of them for our model)\n1. Age, Balance, NumOfProducts, IsActiveMember, CreditScore are the features with significant correlation."},{"metadata":{"_uuid":"8f2c84b5bbf8f4cb0dc274240eedb9af74f51e23"},"cell_type":"markdown","source":"# Building a Machine Learning Model"},{"metadata":{"trusted":true,"_uuid":"28591b63950206c255f7c835a1ddb51dcca9ce8b"},"cell_type":"markdown","source":"## 1. Preparing the data \n\na. Shuffle the data (to randomize all data)\n\nb. Split feature data from the target (To easily differentiate what is being predicted from determinants)\n\nc. Split feature data and target into training and testing sets (For validation accuracy)\n\n\n"},{"metadata":{"trusted":true,"_uuid":"cf938f05e732555073efc89ad40ed97e1930aa4e"},"cell_type":"code","source":"# Shuffling the dataset\nchurn_dataset = churn_dataset.reindex(np.random.permutation(churn_dataset.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e0969a9c5f90103e9450e2c4cc17ae5ec81f18"},"cell_type":"code","source":"# Splitting feature data from the target\ndata = churn_dataset.drop(\"Exited\", axis=1)\ntarget = churn_dataset[\"Exited\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d09e33e8da8cc85c58bbf0b3624f2104f78cc52"},"cell_type":"code","source":"# Splitting feature data and target into training and testing\nX_train, X_test, y_train, y_test = train_test_split(data, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbb57d82298d6f97dca579f7f775ac633d484161"},"cell_type":"markdown","source":"## 2. Choosing the best classification model to use"},{"metadata":{"trusted":true,"_uuid":"e272a5ffb0f66c8e1b51c3ac3200a0d651fa7591"},"cell_type":"code","source":"# Creating a python list containing all defined models\nmodel = [GaussianNB(), KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=5, random_state=0), LogisticRegression()]\nmodel_names = [\"Gaussian Naive bayes\", \"K-nearest neighbors\", \"Support vector classifier\", \"Decision tree classifier\", \"Random Forest\", \"Logistic Regression\",]\nfor i in range(0, 6):\n    y_pred = model[i].fit(X_train, y_train).predict(X_test)\n    accuracy = accuracy_score(y_pred, y_test)*100\n    print(model_names[i], \":\", accuracy, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b94f3850c6e6a4ba5812b1147364725113af9e0"},"cell_type":"markdown","source":"From the above, we can easily see that Random Forest Classifier is the model with the highest accuracy, thus it is the one we are going to use."},{"metadata":{"trusted":true,"_uuid":"95d0ccc32da87450bbbbc23bf0648c324dc7c6e2"},"cell_type":"code","source":"# Working with the selected model\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 0)\ny_pred = model.fit(X_train, y_train).predict(X_test)\nprint(\"Our accuracy is:\", accuracy_score(y_pred, y_test)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12160984732d36eebd239e51a979c8c49a1221e7"},"cell_type":"markdown","source":"Having an accuracy of 86% means that our model is good enough to predict new data"},{"metadata":{"_uuid":"e04cfd6afadcbfdd53cf718d1898590502cc5dc4"},"cell_type":"markdown","source":"# A Demo-API detecting Bank Churn from user inputs\n\n\n### To run the following API, Do fork the notebook and remove comments\n\n\n## The following are inputs:\n\n1. Credit score of the client\n2. Age of the client\n3. Tenure of the client\n4. Current balance in the bank account of the client\n5. Number of product the client uses\n6. Does the client have a credit card\n7. Is the client an active member\n8. Estimated salary of the client"},{"metadata":{"trusted":true,"_uuid":"72a8b5618fbf2e12fdab8de95f1c6a5ec1d0640e"},"cell_type":"code","source":"#print(\"Enter the credit score of the client \\n\")\n#credit_score = int(input())\n#print(\"Enter the age of the client \\n\")\n#age = int(input())\n#print(\"Enter the tenure of the client \\n\")\n#tenure = int(input())\n#print(\"Enter the current balance of the client \\n\")\n#balance = float(input())\n#print(\"Enter the number of product the client use \\n\")\n#product_no = int(input())\n#print(\"Press 1 if the user has a credit card or 0 if not \\n\")\n#credit_card = int(input())\n#print(\"Press 1 if the user is an active member or 0 if not \\n\")\n#active_member = int(input())\n#print(\"Enter the estimated salary of the client \\n\")\n#salary = float(input())\n\n\n#X_user = np.array([credit_score, age, tenure, balance, product_no, credit_card, active_member, salary])\n\n#y_pred = model.predict([X_user])\n#index = y_pred  \n#if index == 1:\n#    print(\"\\n Client is not exiting the bank\")\n#elif index == 0:\n#   print(\"\\n Client is on the threshold of exiting the bank\")\n#    print(\"\\n Consider taking further steps to incentivise the client\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b75fc25609984e1145448670871466eae2b3f8"},"cell_type":"markdown","source":"# Conclusion and Recommendation\n\nIn conclusion, we have been able to get insights from our churn dataset and predicted clientelle dynamics with the Random Forest Classifier model at 86% accuracy.  \n\nHowever, this accuracy would be improved by collecting more relevant data with enough features from different individuals. Additionally, I do believe this accuracy would be improved after having a full grasp of the domain of data to use in the improved model. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}