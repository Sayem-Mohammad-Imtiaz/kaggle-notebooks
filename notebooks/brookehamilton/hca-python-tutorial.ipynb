{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"> ## HCA Data Science Summit Python Tutorial  \n\n- Brooke Hamilton, Brooke.Hamilton2@hcahealthcare.com\n- Trevor Townsend, trevor.townsend@hcahealthcare.com\n\nThis tutorial walks through the process of training a machine learning model to predict whether a diabetic patient will be readmitted within 30 days of an inpatient stay."},{"metadata":{},"cell_type":"markdown","source":"## Roadmap\n1.  Where are we?  Jupyter notebooks in Kaggle\n2.  Mock business case\n3.  Import the data\n4.  Data exploration/ Getting to know the data\n5.  Clean the data and get it ready for building a model\n6.  Train a model\n7.  Evaluate the model's performance\n8.  Conclusions (Back to the business case)"},{"metadata":{},"cell_type":"markdown","source":"## 1.  Where are we?  Jupyter notebooks in Kaggle\nKAGGLE is an online community of data scientists and machine learners, owned by Google LLC. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges. "},{"metadata":{},"cell_type":"markdown","source":"<font size=\"6\">**WARNING:  No PHI in Kaggle!  Don't upload any real HCA data here!**</font>"},{"metadata":{},"cell_type":"markdown","source":"### Intro to Jupyter Notebooks  \n\nJupyter notebooks\n- Installation: https://jupyter.org/install.html\n- Docs: https://jupyter-notebook.readthedocs.io/en/stable/\n\nGreat things about Jupyter notebooks:\n- You can run small segments of code instead of the whole script at once\n- Easy to visualize the output of the code\n- Inline graphics \n- Markdown comments\n\nCaveats:\n- Not for production; only for development\n- Hidden state\n- You can get into trouble if you run the cells out of order\n- I Don't Like Notebooks by Joel Grus:  https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/preview?slide=id.g362da58057_0_1"},{"metadata":{},"cell_type":"markdown","source":"## 2.  Mock business case\n- **Problem**:  Preventable readmissions after inpatient stays\n- **Goal:**  Predict whether a patient will be readmitted within 30 days of discharge \n- **Patient population:**  Diabetic patients currently inpatient in one of our facilities  \n- **Timeline:**  Deliver a prediction (\"score\") to the facility during the patient's stay, before discharge\n- **Intervention**:  The facility will take actions to reduce the chance of readmission for patients flagged by the model\n- **Success Metric**:  Fewer readmissions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Import Python libraries\nA python library is a collection of functions and methods that allow you to perform many actions without writing your own code."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd       ## data analysis\nimport numpy as np        ## mathematical functions\nimport sklearn            ## machine learning\nimport xgboost            ## machine learning - a particular model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.  Import the data\n\nFor this tutorial, we will use a public dataset with diabetic patients from 130 US hospitals.  This dataset is hosted on Kaggle.  \n\n`File --> Add or Upload Data --> Search for \"Diabetes\"-->   \nSelect \"Diabetes 130 US hospitals for years 1999-2008\"`"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import the dataset and name it 'data'\ndata = pd.read_csv('/kaggle/input/diabetes/diabetic_data.csv')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.  Data exploration / Getting to know the data "},{"metadata":{},"cell_type":"markdown","source":"### View the dataframe to get an idea what we're working with"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)                               ## print the shape of the dataframe (rows, columns)\nprint(f'number of rows: {data.shape[0]}')       ## print the number of rows in the dataframe \nprint(f'number of columns: {data.shape[1]}')    ## print the number of columns in the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(8)    ## view the first 8 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3).T     ## .T transposes the rows and columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Intro to lists and loops\nlists are defined by square \"[ ]\" brackets and contain any number of elements or none at all."},{"metadata":{"trusted":true},"cell_type":"code","source":"pets = ['dog','cat','bird']     ## create a list of pets\npets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loops can iterate over a list and perform a task for each item in the list"},{"metadata":{"trusted":true},"cell_type":"code","source":"for pet in pets:                ## loop through each pet in the list\n    print(f'I have a {pet}')    ## print the string 'I have a ' + pet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Look at each column's contents \nLet's create a loop to print the contents of each column and their associated counts within the column. <br />\nTo do this, we can use the `value_counts()` function."},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in data.columns:                           ## loop through each column in the dataframe \n    print(column)                                     ## print the name of the column\n    print(data[column].unique())                      ## show unique values\n    print('\\n')                                       ## print an empty line","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.  Clean the data and get it ready for building a model\n\n### Data cleaning tasks that are usually a good idea:\n* Eyeball the data and make sure it has the right number of rows and columns\n* Make sure missing values are coded correctly (usually as NaN in pandas)\n* Make sure the outcome variable is coded correctly for your project\n* Check the data type of each feature (*e.g.*, numeric, string, datetime).  Correct as necessary.\n* Drop any feature that's mostly missing data"},{"metadata":{},"cell_type":"markdown","source":"### Make sure missing values are coded correctly"},{"metadata":{"trusted":true},"cell_type":"code","source":"## We noticed up above that there were some \"?\" values in some fields\ndata[['payer_code']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas `DataFrame.replace()` function: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Anywhere in the dataframe that has a value of `?`, replace with NaN\ndata = data.replace('?', np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check that the `?` values have changed to `NaN`\ndata[['payer_code']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make sure the outcome variable is coded correctly for your project\n\nWhat are the unique values of the outcome variable (`readmitted`), and how many patients had each outcome?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['readmitted'].unique()   ## show value counts for readmitted column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['readmitted'].value_counts(normalize=True)    ## show value counts with percentage ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are three values:\n    * No readmission  (~ 54%)\n    * Readmitted after more than 30 days (~35%)\n    * Readmitted within 30 days (~11%)\n    \nHow we treat this variable would depend on the business purpose of the model.  Do we just want to predict readmissions within 30 days, or any readmission?  This would be a good question to discuss with the business owner.  For this workshop, we will assume we only want to predict readmissions **within 30 days**.  More than 30 days will be combined with the \"NO\" group."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Anywhere where `readmitted` is '<30', give a 1, otherwise give a 0\ndata['readmitted_30'] = np.where(data['readmitted'] == '<30', 1, 0)    ## Create new column\n\ndata[['readmitted', 'readmitted_30']].head(15)                         ## Compare to original","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check the data type of each feature (*e.g.*, numeric, string, datetime).  Correct as necessary  \nHow has pandas interpreted the data type of each feature?  \n\nPandas data types:\n- object\n    - Text \n    - `\"Dog\", \"Cat\", \"Frog\"`\n- int64\t\n    - Integer numbers\n    - `-12, 5, 1064`\n- float64\n    - Floating point (decimal) numbers\n    - `3.14159`\n- datetime64\n    - Date and time values\n    - `2019-10-28 11:32:04`"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes    ## print each column name and its data type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the data types look correct, except for a few features that look numeric but are actually categorical:\n\n`admission_type_id` <br />\n`discharge_disposition_id` <br />\n`admission_source_id` <br />\n<br />\nThese three columns look like numbers but they are actually categorical values. <br />\nLet's change their type to 'object'"},{"metadata":{"trusted":true},"cell_type":"code","source":"## create a list of columns\ncolumns_to_correct = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n\n## Look at those columns\ndata[columns_to_correct]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check what the current data types are\ndata[columns_to_correct].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Convert each column in the list to type \"object\"\ndata[columns_to_correct] = data[columns_to_correct].astype('object')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Look at those columns again now that we've corrected them\ndata[columns_to_correct]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[columns_to_correct].dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Patient age  \n* The `age` column is formatted as a string.  It might be more meaningful as a numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['age']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Look at the value for the first row to see that it is a string (text)\ndata.age[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we fix `age`, let's talk about how Python uses **zero-based indexing**. Each element of a sequence is assigned a number - its position or index. The first index is zero, the second index is one, and so forth."},{"metadata":{"trusted":true},"cell_type":"code","source":"pets = ['dog', 'cat', 'bird']   ## create a list of pets\npets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get the first element of `pets`\npets[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get the first element of `pets`, then get the first element of that\npets[0][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's correct the `age` column by taking a substring of each value and then converting it to a number:"},{"metadata":{"trusted":true},"cell_type":"code","source":"## As an example, look at the value of age for the fifth row of the dataframe\ndata['age'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get a substring from positions 1 and 2 \n## (python goes up to but doesn't include the last value in the range) \ndata['age'][5][1:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks like what we want. Take the substring for every value in the column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['age'] = data['age'].str[1:3]    ## take the 1-2 characters of the string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Does it look correct now?\ndata['age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This worked for every value except `[0-10)`.  We can remove the hyphen using the `np.where()` function again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Find where age = '-0' and replace it with '0'; otherwise take its existing value\ndata['age'] = np.where(data['age'] == '0-', '0', data['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Does it look right now?\ndata['age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## It looks like numbers but it's still a string, so now convert to integer\ndata['age'] = data['age'].astype('int64')    ## convert age column to int64\ndata['age'].dtype                            ## print new data type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Does the `age` column look like a number now, instead of text?\ndata['age'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that `age` is correctly coded as a number, we can look at a histogram:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['age'].hist()    ## create a histogram with the age column ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop features that have a high percentage of missing values  \nIf a feature has too many missing values, it probably won't be helpful to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Find columns with more than 30% missing values\nfor column in data.columns:\n    if sum(data[column].isnull())/len(data[column]) > 0.3:\n        print(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## show number of columns before   \nprint(f'number of columns before: {data.shape[1]}')                       \n\n## drop columns in list\ndata = data.drop(['weight','payer_code','medical_specialty'], axis = 1)    \n\n## show number of columns before\nprint(f'number of columns after: {data.shape[1]}')                            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Columns with little or no variation  \nFind and drop columns with no meaningful variation:  "},{"metadata":{"trusted":true},"cell_type":"code","source":"no_variation_cols = []                           ## initialize empty list\n\nfor column in data.columns:                      ## loop through each column in the dataframe\n    if len(data[column].unique()) == 1:          ## if only 1 unique value exists\n        no_variation_cols.append(column)         ## add the column name to the list\n\n\nno_variation_cols                                ## print the list of columns with no variation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read more about list comprehensions here: https://www.pythonforbeginners.com/basics/list-comprehensions-in-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"## A different way to do the same thing, with a list comprehension\nno_variation_cols = [i for i in list(data) if len(data[i].unique()) == 1]\nno_variation_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## show number of columns before   \nprint(f'number of columns before: {data.shape[1]}')                       \n\n## drop columns in list\ndata = data.drop(no_variation_cols, axis=1) \n\n## show number of columns before\nprint(f'number of columns after: {data.shape[1]}')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choose features for training  \nLastly, identifying the features (predictors) we want to include for modeling.  You can use any subset of features for modeling but will want to remove those features which have little or no bearing on the outcome.\n\nFor this model we can assume the patient identifiers will not be useful to predicting the likelihood of readmission.  Since we created our own 30 day readmission indictor flag from the 'readmitted' column,  we want to be sure to remove that column as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Drop unique identifiers and the old outcome variable\nprint(f'number of columns before: {data.shape[1]}')                             ## show number of columns before    \n\ndata.drop(['encounter_id','patient_nbr','readmitted'], axis=1, inplace=True)    ## drop columns\n\nprint(f'number of columns after: {data.shape[1]}')                              ## show number of columns after  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Timeliness of Features  \nWe should be careful not to train a model using features that wouldn't be available at the time of scoring, especially features that may unfairly \"peek\" at what the outcome is.  \n\nLet's look at the features again and see if any of them wouldn't be available."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`discharge_disposition_id` sticks out as a feature that probably wouldn't be available at the time of scoring, assuming we would score the patients *before* they're discharged.  We should remove this from the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('discharge_disposition_id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variable Handling \n\nSince many models can't use string (text) values, we need to convert the categorical features into numerical ones. We can do this in two ways.\n\n### Option 1:  Label Encoding\nWe can give each categorical value a number.  This allows us to keep the variable in one column.  However, encoding variables in this way may inherently imply an order to the values that does't actually exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show an example of a text column\npets = ['dog', 'cat', 'dog', 'bird', 'turtle']\npets_df = pd.DataFrame(pets, columns=['pet'])  ## display as DataFrame\npets_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\npets_transformed = le.fit_transform(pets)  ## Change each text value to a number\n\n## Show the original column side-by-side with new\npd.DataFrame(list(zip(pets, pets_transformed)), columns=['original', 'label_encoded']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Option 2:  One-Hot Encoding  \nOne-Hot Encoding makes a new column for each value of the feature, with a 0 or 1 for each value.\n\nNote:  XGBoost recommends one-hot encoding (https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n\nThe benefit of this method is there is no inherent order to the numbers. The downside is it can make your dataframe very large and unwieldy."},{"metadata":{"trusted":true},"cell_type":"code","source":"pets_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas `get_dummies()`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create a one hot encoded dataframe\npd.get_dummies(pets_df)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this tutorial, we will choose one-hot encoding for our categorical features.  However, there are three problematic columns:\n\n`diag_1`, `diag_2`, & `diag_3` each have over 700 potential options. <br /> \nWe will remove these 3 columns for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'diag_1: {len(data.diag_1.unique())}')    ## show number of distinct values\nprint(f'diag_2: {len(data.diag_2.unique())}')    ## show number of distinct values\nprint(f'diag_3: {len(data.diag_3.unique())}')    ## show number of distinct values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'number of columns before: {data.shape[1]}')                ## show number of columns before \n\ndata.drop(['diag_1', 'diag_2', 'diag_3'], axis=1, inplace=True)    ## drop columns\n\nprint(f'number of columns after: {data.shape[1]}')                 ## show number of columns after","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will proceed with one-hot encoding the rest of the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)     ## use the get_dummies function to one hot encode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape    ## get the shape of the dataframe in rows and columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Look at the dataframe to see if everything looks correct\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test/Validate Data Split  \nWe will split the data into three dataframes:\n- Training (80%):  For building the model  \n- Validation (10%):  For evaluating the trained model as we go  \n- Testing (10%):  For evaluating the *final* trained model chosen from all the models we tried"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the data into two dataframes, train_df with 90% and val_df with 10%\ntrain_df, val_df = train_test_split(data, test_size=0.10, random_state=7,\n                                     stratify=data.readmitted_30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the data again, pulling out another 10% for the testing set\ntrain_df, test_df = train_test_split(train_df, test_size=(10/90),random_state=7, \n                                    stratify=train_df.readmitted_30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_df:\", train_df.shape, \"\\n\",round(train_df.shape[0]/data.shape[0], 2), \"%\", \"\\n\")\nprint(\"val_df:\", val_df.shape, \"\\n\",round(val_df.shape[0]/data.shape[0], 2), \"%\", \"\\n\")\nprint(\"test_df:\", test_df.shape, \"\\n\",round(test_df.shape[0]/data.shape[0], 2), \"%\", \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Divide each of the dataframes into predictors (X) and the outcome (Y):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop('readmitted_30', axis=1)\nY_train = train_df[['readmitted_30']]\n\nX_val = val_df.drop('readmitted_30', axis=1)\nY_val = val_df[['readmitted_30']]\n\nX_test = test_df.drop('readmitted_30', axis=1)\nY_test = test_df[['readmitted_30']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.  Train A Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bring in the model classifier you want to use\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a model object, including hyperparameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = XGBClassifier(n_estimators=500,\n                          objective= 'binary:logistic', \n                          nthread=50,\n                          seed=27)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the object.  What are the default values for hyperparameters it chose?"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.fit(X_train, \n              Y_train.values.ravel(), \n              eval_metric=['logloss','aucpr'],\n              verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bring in sklearn metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = estimator.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Write a function to generate a nicely formatted confusion matrix\n\ndef get_cm(Y_val, val_preds):\n    cm = pd.DataFrame(confusion_matrix(Y_val, val_preds))\n    cm = cm.rename(columns={0: 'predict not readmitted', 1: 'predict readmitted'})\n    cm['Actual'] = ['not readmitted', 'readmitted']\n    cm = cm.set_index('Actual')\n    del cm.index.name\n    \n    return cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_cm(Y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Better metrics than Accuracy"},{"metadata":{},"cell_type":"markdown","source":"**Precision:** Out of all the cases we predicted would be admitted, what percent were actually readmitted?  \n`tp / (tp + fp)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(Y_val, val_preds, pos_label=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recall:**  Out of all the true readmissions, what percent did we correctly flag?  \n`tp / (tp + fn)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(Y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variable Importances\n\nInterpreting variable importance with xgboost:  https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nxgboost.plot_importance(estimator, importance_type='gain', max_num_features=15, \n                        show_values=False, height=0.6, grid=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['readmitted_30', 'number_inpatient']].groupby('readmitted_30').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now try weighting readmissions more heavily with `scale_pos_weight`:  \nhttps://xgboost.readthedocs.io/en/latest/parameter.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_weighted = XGBClassifier(n_estimators=500,\n                          objective= 'binary:logistic', \n                          nthread=50,\n                          scale_pos_weight=9,\n                          seed=27)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_weighted.fit(X_train, \n              Y_train, \n              eval_metric=['logloss','aucpr'],\n              verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds_weighted = estimator_weighted.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_cm(Y_val, val_preds_weighted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_val, val_preds_weighted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(Y_val, val_preds_weighted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(Y_val, val_preds_weighted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost.plot_importance(estimator_weighted, importance_type='gain', max_num_features=15, \n                        show_values=False, height=0.6, grid=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.  Evaluate the model's performance \n\nAlthough the first model we trained looked promising at first, when we dug deeper into the metrics, we found out that it would be of little use to our business owners.  \n\nThe second model, while it still has room for improvement, was moving in the right direction.  "},{"metadata":{},"cell_type":"markdown","source":"## 8. Conclusions (Back to the business case)\n\nReasonable next steps for this model would be:\n* Hyperparameter tuning to improve the performance of the model\n* Look for additional features\n    * Add back in diag_1, diag_2, diag_3\n* Try other model types (*e.g.*, Random forest)\n* When we land on a model we're happy with, check its performance on the final holdout dataset (the \"Test\" set) to make sure we don't have overfitting\n* Communicate progress to the business owner and get feedback on the business requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}