{"cells":[{"metadata":{"id":"AbuCZC_omK5p","colab_type":"text"},"cell_type":"markdown","source":"###  Choosing The dataSet\n\nI recently watched a documentary about the diamond's import and export from South Africa. From this, I got inspired to do some data analysis about the diamonds, so that I can gain some rich insights. Since I don't have any knowledge apart from its shiny look, I decided to search about the data and stumbled upon the dataset provided by Shivam Agarwal in kaggle. Surprisingly the dataset contained an enormous amount of detail about the properties of the diamond such as its price, shape, and other attributes. Therefore, given the vast amount of data, I decided to choose this dataset and perform exploratory data analysis.\n\n\n"},{"metadata":{"id":"7gj4ytjkpEmc","colab_type":"text"},"cell_type":"markdown","source":"### Obtaining Data\n\nThe dataset file which is available in kaggle is in CSV format, so I decided to upload the file into google drive using a module called \"drive\" and mounted it into colab.\nWhile mounting the google drive into colab the google will provide a link and text field for authorization code, clicking on the link will redirect to another web page and the authorization code will be generated which I need to copy and paste it in the text field provided for authorization code.\nFurthermore, access the file using the panda library to store the CSV file data into a variable to explore the data further. "},{"metadata":{"id":"MFHauMTtc7Qc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"\n\n\nimport pandas as pd\nfrom pandas import DataFrame,Series\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PolynomialFeatures\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso, ElasticNet,SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \nfrom sklearn.pipeline import Pipeline\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"id":"npq1tnwzdwZI","colab_type":"code","outputId":"0bfed1aa-899f-4f7a-ce36-2f817d67d2e7","colab":{"base_uri":"https://localhost:8080/","height":33},"trusted":true},"cell_type":"code","source":"# drive.mount('/content/drive')\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jsc5kmI6gQjs","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"data =pd.read_csv(\"../input/diamonds/diamonds.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tPR20kQVf8wE","colab_type":"text"},"cell_type":"markdown","source":"### Scrubbing  and Formatting"},{"metadata":{"id":"6Z4nU-qvhb2B","colab_type":"code","outputId":"7413178d-fca1-49a4-a7d5-b8a42aad013d","colab":{"base_uri":"https://localhost:8080/","height":150},"trusted":true},"cell_type":"code","source":"print(data.head())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CsGaJ0Mzha4Y","colab_type":"text"},"cell_type":"markdown","source":"We can see from the above data that there is an unnamed column that provides indexing. Since Pandas itself provides the indexing of the data, the \"unnamed\" column is unnecessary. Therefore, we will drop that column.\n\n"},{"metadata":{"id":"9hLFlqOuh4hx","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"del data[data.columns[0]]","execution_count":null,"outputs":[]},{"metadata":{"id":"qNRUDyjqgZ5Q","colab_type":"text"},"cell_type":"markdown","source":"Now let us check any null values is present in the data."},{"metadata":{"id":"j47Y6RNHf6Ud","colab_type":"code","outputId":"27843de2-6a39-4f71-8045-10530c00a109","colab":{"base_uri":"https://localhost:8080/","height":33},"trusted":true},"cell_type":"code","source":"data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"id":"z-3Bq7QxgunA","colab_type":"text"},"cell_type":"markdown","source":"Fortunately, there were no null values present in this dataset as we can see from the above IsNull() function output which returns true if any null values present in the data frame and false if there are no null values."},{"metadata":{"id":"lTwRE1Pu1msi","colab_type":"text"},"cell_type":"markdown","source":"####Diamonds"},{"metadata":{"id":"pMsMRchr1saI","colab_type":"text"},"cell_type":"markdown","source":"<p>Four main features(also known as 4 C's) which are used to access the quality of a diamond, are listed down below.<p>\n<ol>\n  <li> Carat Weight</li>\n<li> Cut</li>\n<li> Clarity</li>\n<li> Color</li>\n</ol>\n\n<h4> Carat Weight </h4>\n\n<p>Carat is a metric that is used to measure the weight of a diamond. One carat is equivalent to 200mg. Diamond prices increase with diamond carat weight, which means bigger the diamond higher the price.\nIf two diamonds weights are equal, then other features are used to determine the price.\n\n</p>\n\n<h4> Clarity </h4>\n<p>Diamonds are generated from sheer pressure and heat below the ground. Therefore, there will be some inclusion inside a diamond i.e., a mark or line pattern inside a diamond.\nAlso, there will be a mark or line in the outer layer of a diamond, which is called blemishes.\n\nBased on the amount of inclusion and blemishes, the clarity of a diamond is categorized such as FL, IF, VSS1, VSS2, VS1, VS2, SI1, SI2, I1, I2, I3.\n \n  The categories mentioned above are ordered in descending order by the amount of presence of inclusion and blemishes.\n \n </p>\n <h4> Cut</h4>\n <p>  This feature is an important thing to notice in a diamond as it measures three crucial things, such as :\n\n  <li>Brilliance: It means the brightness of a diamond by the reflection of white lights inside and outside of a diamond.</li>\n  <li> Fire: It means Scattering of white light into all the colors of the rainbow.</li>\n  <li> Scintillation: the amount of sparkle produced and the pattern of light and dark areas caused by reflection within a diamond.</li>\n</p>\n\nThe goal is to cut a diamond within an appropriate size shape, and angle such that the light entering the diamond should reflect and leave from the top surface.\n\nThe other Features of a Cut are the Depth Percentage and Table percentage.\n\n  <h5> Depth %</h5>\nDepth is the distance from a top surface i.e., table to a culet.\n  The depth percentage is calculated by dividing the diamond depth by the overall width of a diamond. \n  Lower the depth percentage the bigger the diamond looks from the below i.e., pavilion.\n\n  <h5> Table % </h5>\n\n  The table is the topmost surface of a diamond and also the most significant facet of the round diamond. An appropriate width of a table will allow the light to enter and reflect on the appropriate direction .if not most of the light will scatter off in different directions.\n  The table percentage is calculated by dividing the table width by overall diamond width.\n\n\nThe following five categories measure the quality of Cut.\n\n<li>Excellent</li>\n<li>Very Good</li>\n<li>Good</li>\n<li>Fair </li>\n<li>Poor </li>\n\n<h4>Color</h4>\n \nColor measurement in diamond measures lacks color. If the diamond color is like a drop of water that is colorless, it will have a high value. As then only it can scatter the light without observing.\n However, there are some diamonds that are in different colors will have higher prices, but here in the dataset, we are going to only look at the colorless diamonds.\n \nThe color scale is categorized from D to Z letters and ordered in ascending by the amount of presence of color in a diamond.\nFrom the K onwards to till Z, we can see a yellowish color present. \n\nD ,E,F - Colorless\nG,H,I,J - Near colorlessness\nK, L, M - Faint color\nN-R: Very Light Color\nS-Z: light color\n\n\n<h4>Length,Width, and Depth</h4>\n\nThe dimension of a diamond is measured in millimeters. Moreover, the shape of a diamond is determined by the Length to width ratio. \nFor instance, to determine the roundness of a diamond, we need to check the L/W ratio, If the ratio is between 1 and 1.05, it is a round diamond, and an oval shape diamond L/W ratio can be around 1.50 or less."},{"metadata":{"id":"gy_IgjUzzbIh","colab_type":"text"},"cell_type":"markdown","source":"After learning  about the diamonds in detail, Let's look into the dataset and check what features are available and what are their instances."},{"metadata":{"id":"11mRCBPbtRcR","colab_type":"text"},"cell_type":"markdown","source":"####Modifying the Column Name"},{"metadata":{"id":"_0U7_YGFzafd","colab_type":"code","outputId":"441eb6d5-fed8-4ac1-9a68-461990da45a1","colab":{"base_uri":"https://localhost:8080/","height":117},"trusted":true},"cell_type":"code","source":"print(data.head())","execution_count":null,"outputs":[]},{"metadata":{"id":"Zb-IiIVv05Gm","colab_type":"text"},"cell_type":"markdown","source":"From above we can see that the column names are not clear and are misguiding. Therefore, let us rename the columns So that we can have a better understanding.<br>\nThe following columns are going to be replaced.\n<hr>\n<b>Existing   &nbsp;&nbsp;&nbsp;&nbsp;     Replaced</b><br>\n<hr>\ndepth       &nbsp;&nbsp;&nbsp;&nbsp;   ->  depth %<br>\ntable        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp   ->  table %<br>\n  x             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp  ->  length<br>\n  y             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp ->  width<br>\n  z             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp ->  depth<br>"},{"metadata":{"id":"OGQYVKdt4Qxp","colab_type":"code","outputId":"edfe28fb-0197-44d9-aeea-6833ffef1c0b","colab":{"base_uri":"https://localhost:8080/","height":77},"trusted":true},"cell_type":"code","source":"data.rename(columns={\"x\": \"length\", \"y\": \"width\", \"z\" : \"depth\",\"depth\" : \"depth %\", \"table\": \"table %\"},inplace = True)\ndata.head(1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-MHPVGkd4ahI","colab_type":"text"},"cell_type":"markdown","source":"Now, we can see that the length/width ratio information is not present in the dataset. However, we have length and width information. Therefore, we will create another column called L/W ratio by dividing the length by width and see how it correlates with our other features later."},{"metadata":{"id":"bHPAsnYf5SSb","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"data['L/W'] = data['length']/data['width']\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GVbFAbxhjmKV","colab_type":"code","outputId":"743e568a-7df9-4c07-986d-e4dbd947f0c3","colab":{"base_uri":"https://localhost:8080/","height":150},"trusted":true},"cell_type":"code","source":"print(data.head())","execution_count":null,"outputs":[]},{"metadata":{"id":"dppXYeHO1Wtt","colab_type":"text"},"cell_type":"markdown","source":"Before going any further for cleaning and formatting the data let us first know about diamonds and the column values present in the dataset."},{"metadata":{"id":"yXRtPyY_cmeG","colab_type":"text"},"cell_type":"markdown","source":"###  Exploratory Data Analysis"},{"metadata":{"id":"gBW8eM6oeFqy","colab_type":"text"},"cell_type":"markdown","source":"Until now, we were formatting the data and added new features. Now let us explore the data to find out any exciting pattern or any anomaly."},{"metadata":{"id":"2agZxeJrZbVa","colab_type":"code","outputId":"57ab6315-767b-41cd-8590-b2c593884343","colab":{"base_uri":"https://localhost:8080/","height":318},"trusted":true},"cell_type":"code","source":"\nprint(data.info())\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"lAQd_CooyjfK","colab_type":"text"},"cell_type":"markdown","source":"From above we can see that there are three features which are categorical and others are continuous and also the dimension of the data 53940,11, which means there are 11 features and 53940 rows. Now let us look at the data."},{"metadata":{"id":"6fjBgKQGy62h","colab_type":"code","outputId":"564157be-0697-4367-bbf5-c4ec5023fdb6","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"print(data.head())\nprint(data.tail())","execution_count":null,"outputs":[]},{"metadata":{"id":"YsfxlO7A2ECk","colab_type":"text"},"cell_type":"markdown","source":"From looking at the above categorical data, i.e., cut, color, and clarity, we could see that this data follows AGI scaling values for each of these categories.\nTo confirm, let us check each of the unique values present in these certain features."},{"metadata":{"id":"Rr6a5lwUuBLw","colab_type":"code","outputId":"40b94065-0395-4ff2-a893-8359e51c6b7b","colab":{"base_uri":"https://localhost:8080/","height":67},"trusted":true},"cell_type":"code","source":"print(data['cut'].unique().tolist())\nprint(data['clarity'].unique().tolist())\nprint(data['color'].unique().tolist())","execution_count":null,"outputs":[]},{"metadata":{"id":"EvmLjnFc3YXG","colab_type":"text"},"cell_type":"markdown","source":"<p>\nFrom the above output, we can confirm that each of these categorical values is based on AGS scaling grade. Not the GIA. \nHowever, some of the categorical values are not present in the data such as color values from K to Z, clarity values such as I2 and I3, and also poor cut values are not there.\n  \nFurther, we will check with other continuous  feature.\n \n  \n</p>"},{"metadata":{"id":"WCRALBQC89M4","colab_type":"code","outputId":"16290e56-c17c-4e46-97e1-6c14e7b3efeb","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"ptL6WzWQ-goy","colab_type":"text"},"cell_type":"markdown","source":"We can see from the above output that the length, width, and depth and L/W ratio has 0 values, which is an inappropriate size value. Therefore let us check for how many diamonds the length,  width, depth, and L/W are 0."},{"metadata":{"id":"Y_MY3gSJvaiN","colab_type":"code","outputId":"140da817-bc2b-4b1a-9fbf-785bc545e72f","colab":{"base_uri":"https://localhost:8080/","height":418},"trusted":true},"cell_type":"code","source":"# check if any dimensions have zero value in it\nprint(data.loc[(data['length'] == 0) | (data['width'] == 0) | (data['depth'] == 0) | (data['L/W'] == 0)] )\nprint('length :',len(data.loc[(data['length'] == 0) | (data['width'] == 0) | (data['depth'] == 0) | (data['L/W'] == 0)]))","execution_count":null,"outputs":[]},{"metadata":{"id":"E-3FNlyhDUAL","colab_type":"text"},"cell_type":"markdown","source":"From the above output, we can see that 20 rows have 0 values in either of the dimensions. Therefore, we need to delete the entire row, which has 0 values in either of these features. As the data contains 53940 rows, it will not affect our analysis. \nMoreover, L/W contains NaN values. Since L/W values were calculated using length and width values, NaN would have occurred when the width was 0.\nSo, we need to delete these 20 rows.\nTo do that we have to replace all the 0 values with NaN and then drop it using pandas drop() function."},{"metadata":{"id":"IZB1WQu82oEd","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"data[['length','width','depth','L/W']]=data[['length','width','depth','L/W']].replace(0,np.NaN)\n\ndata.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"7lQ-Pns_5EM8","colab_type":"code","outputId":"29386e6b-9fac-4f37-baaf-79192ce70ac3","colab":{"base_uri":"https://localhost:8080/","height":47},"trusted":true},"cell_type":"code","source":"\ndata.loc[(data['length'] == 0) | (data['width'] == 0) | (data['depth'] == 0) | (data['L/W'] == 0) ]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"n7spDWgi5Skl","colab_type":"code","outputId":"3cfdc868-9495-4210-9272-39e7a571c8f1","colab":{"base_uri":"https://localhost:8080/","height":234},"trusted":true},"cell_type":"code","source":"print(data.isnull().sum())\nprint('shape:',data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"VpOu6L13COeK","colab_type":"text"},"cell_type":"markdown","source":"From the above output, we can see that the null values and the rows which contained 0 value are deleted. Moreover, the dimension of the data has now become  53920 rows with 11 features.\n\nNow let us look at the correlation between each feature using corr() function and plotting the values using heatmap."},{"metadata":{"id":"yYbMuREMvvq2","colab_type":"code","outputId":"60bb9f3a-e09a-469c-e8a4-42711f1728f6","colab":{"base_uri":"https://localhost:8080/","height":409},"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(19, 6))\nsns.heatmap(data.corr(), annot=True, linewidths=8, center=0,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"fAK4i0C_3br7","colab_type":"text"},"cell_type":"markdown","source":"From the above plot, We can see that carat, length, width, depth, and price are having a high correlation with each other. \n\nHowever, the categorical data is not taken into account. Since these categories are considered as essential factors for pricing the diamond, therefore, we will compare each category feature with the price."},{"metadata":{"id":"u59QappWfXMV","colab_type":"text"},"cell_type":"markdown","source":"####Color Vs Price\n"},{"metadata":{"id":"DWI6u4BpuoNu","colab_type":"code","outputId":"a9dafa27-5f30-4352-b2e6-ba1e09eb6707","colab":{"base_uri":"https://localhost:8080/","height":321},"trusted":true},"cell_type":"code","source":"\nsns.boxplot(y='price',data=data,x='color',palette ='Set1',width =0.3,order = ['D','E','F','G','H','I','J'] )\n# \n# \n\n# data.boxplot('price','cut',rot = 30,figsize=(5,6)) \n# data.boxplot('price','clarity',rot = 30,figsize=(5,6))","execution_count":null,"outputs":[]},{"metadata":{"id":"Ui91G5z-zD9b","colab_type":"text"},"cell_type":"markdown","source":"From the above plot, we can see that G, H,  I and J type color has less number of outliers compared to D and E.It suggests that the better the quality of color the higher the outliers except for G type color. Also, each category type has the same maximum and minimum price."},{"metadata":{"id":"asAu9ya5Agvi","colab_type":"text"},"cell_type":"markdown","source":"####Cut Vs Price"},{"metadata":{"id":"Hvb_Us16-MkH","colab_type":"code","outputId":"2b232d76-b3f9-445c-e816-2e5aa14a66ec","colab":{"base_uri":"https://localhost:8080/","height":321},"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price',data=data,x='cut',palette ='Set1', width =0.5,order =['Ideal' ,'Premium' ,'Very Good' ,'Good' ,'Fair'] )","execution_count":null,"outputs":[]},{"metadata":{"id":"CIH6-dzCAgLb","colab_type":"text"},"cell_type":"markdown","source":"From the above plot, we can see that the lower the quality of cut, the higher the number of outliers except for the Ideal cut type. Also, each category type has the same maximum and minimum price."},{"metadata":{"id":"z4oizgy9CleF","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hYF8fvyeHq3B","colab_type":"text"},"cell_type":"markdown","source":"####Clarity Vs Price"},{"metadata":{"id":"pePczCnKBI3M","colab_type":"code","outputId":"b1c5af39-1551-4814-9c79-398101a9d69d","colab":{"base_uri":"https://localhost:8080/","height":321},"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price',data=data,x='clarity',palette ='Set1', width =0.7,linewidth=3,order =['IF','VVS1','VVS2','VS1','VS2','SI1','SI2','I1'])","execution_count":null,"outputs":[]},{"metadata":{"id":"jw-k8fKGCIK9","colab_type":"text"},"cell_type":"markdown","source":"From the above plot, we can see that IF, VVS1 and VVS2 have a high number of outliers compared to other categories of color. Moreover VS1,VS2 are having less number of outliers compared to others.Also, each category type has the same maximum and minimum price."},{"metadata":{"id":"Q7Li9UEpO8ju","colab_type":"text"},"cell_type":"markdown","source":"#### Clarity vs cut"},{"metadata":{"id":"8VJ11FPgb8Q-","colab_type":"code","outputId":"fb9fbb08-7cc6-4a4a-f6b1-29a362ddad55","colab":{"base_uri":"https://localhost:8080/","height":673},"trusted":true},"cell_type":"code","source":"clarity_cut_table = pd.crosstab(index=data[\"clarity\"], columns=data[\"cut\"])\n\nclarity_cut_table.plot(kind=\"bar\", \n                 figsize=(10,10),\n                 stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ldh1tlwbiVsy","colab_type":"text"},"cell_type":"markdown","source":"We can see that from above that most of the people prefer to buy diamond of SI1 clarity followed by VS2, SI2, and VS1.In that, the cut they prefer is Ideal, Premium, and very good's diamond cut category. Moreover, we can infer that people are not taking the highest clarity diamonds, such as IF or VVS1 and others . and are ready to sacrifice on clarity but are more focusing on the cut of the diamonds.\n\nLet us figure it out by plotting the cut vs. clarity."},{"metadata":{"id":"9o0szwD_PEnw","colab_type":"text"},"cell_type":"markdown","source":"#### Cut vs Clarity"},{"metadata":{"id":"9jfCvQq9dvq5","colab_type":"code","outputId":"eb125c2e-5ed2-4419-9e0d-f8122fd11ae2","colab":{"base_uri":"https://localhost:8080/","height":714},"trusted":true},"cell_type":"code","source":"cut_clarity_table = pd.crosstab(index=data[\"cut\"], columns=data[\"clarity\"])\n\ncut_clarity_table.plot(kind=\"bar\", \n                 figsize=(10,10),\n                 stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"WTDRV4cRi4SW","colab_type":"text"},"cell_type":"markdown","source":"We can see that people prefer Ideal cut over any other cut diamonds followed by Premium and Very Good. It suggests that people are focusing on cut than clarity.\n\nNow let us check the priority of the color."},{"metadata":{"id":"zha8syh3PH33","colab_type":"text"},"cell_type":"markdown","source":"#### Color Vs Clarity"},{"metadata":{"id":"Sypn-V5ManhJ","colab_type":"code","outputId":"dbb8fce3-5b31-4da9-c755-3271be2fa898","colab":{"base_uri":"https://localhost:8080/","height":586},"trusted":true},"cell_type":"code","source":"color_clarity_table = pd.crosstab(index=data[\"color\"], columns=data[\"clarity\"])\n\ncolor_clarity_table.plot(kind=\"bar\", \n                 figsize=(8,9),\n                 stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ziZsoNs3L1R8","colab_type":"text"},"cell_type":"markdown","source":"We can see that from above that most of the people prefer G color followed by E, F, and H.In that the clarity they mostly prefer SI1 or SI2 category. \n\nTherefore from above all the plots, we can conclude that carat has high importance followed by cut, color, and clarity in predicting the price of a diamond.\n"},{"metadata":{"id":"JIuw32xMPjnz","colab_type":"text"},"cell_type":"markdown","source":"Now let us check all the other continuous data type features with price using a scatter plot to check the linearity."},{"metadata":{"id":"FRKdpGNMbYfB","colab_type":"code","outputId":"a9e9cce4-6f14-49c7-a41c-ee7713c169ee","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true},"cell_type":"code","source":"g= sns.pairplot(data , height=3, aspect =1, x_vars = ['carat','depth %','table %'] , y_vars =  ['price'] , kind = 'reg')","execution_count":null,"outputs":[]},{"metadata":{"id":"y2hN_MRIX0Yr","colab_type":"code","outputId":"7b5c83e2-6efe-4730-a239-efc15842e955","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true},"cell_type":"code","source":"g= sns.pairplot(data , height=3, aspect =1, x_vars = ['length','width','depth','L/W'] , y_vars =  ['price'] , kind = 'reg')","execution_count":null,"outputs":[]},{"metadata":{"id":"wl__nC0nZAr9","colab_type":"text"},"cell_type":"markdown","source":"We could see that carat, length, width, and depth are showing linearity with price with fewer outliers and table %, depth %, and L/W are showing linearity but with high outliers."},{"metadata":{"id":"bZuQspwLa8cp","colab_type":"text"},"cell_type":"markdown","source":"###  Findings"},{"metadata":{"id":"rWX_YlhkcWLa","colab_type":"text"},"cell_type":"markdown","source":"From the above analysis, we could say that carat,length, width, depth  are an essential factor in deciding the price of a diamond. However, other features also play an essential role  such as cut, clarity, and color, Length/Width, depth percentage, and table percentage. However, some of the features have a considerable number of outliers. Therefore, We have to use regression-based machine learning algorithms to determine the price of a diamond based on some of the potential features such as polynomial, Linear, and RandomForest regression algorithms to create our model.\n\n"},{"metadata":{"id":"adICtxP2HQsq","colab_type":"text"},"cell_type":"markdown","source":"### Determine a Proper Model"},{"metadata":{"id":"Mb2UXPrvBj74","colab_type":"text"},"cell_type":"markdown","source":"### Data Transformation"},{"metadata":{"id":"8iQJhyA_cosI","colab_type":"text"},"cell_type":"markdown","source":"In the previous section, we have discussed the diamond dataset and the feature present in it. Moreover, we derived our problem statement, which is \"How to determine the price of a diamond. ?\" and guessed what kind of modeling techniques we have to implement.\n\nFurther on, we will apply those modeling techniques and choose the best algorithm based on the algorithm's performance.\n\nSince our problem statement indicates that we have to predict continuous data(price) based on other features using machine learning algorithms. Hence, we have to convert the categorical data into numerical as the regression-based machine learning algorithms will predict the continuous data based on the other feature of type continuous.\n\nHence, we will convert the categorical data into numerical by assigning a weight depending upon the importance of each categorical value present in the feature."},{"metadata":{"id":"PGy2oLPXaXgw","colab_type":"code","outputId":"83144450-1dc3-4353-c27b-676301590511","colab":{"base_uri":"https://localhost:8080/","height":196},"trusted":true},"cell_type":"code","source":"data['clarity_score']=data['clarity'].replace(['IF','VVS1','VVS2','VS1','VS2','SI1','SI2','I1'],[8,7,6,5,4,3,2,1])\ndata['color_score'] = data['color'].replace(['D','E','F','G','H','I','J'],[7,6,5,4,3,2,1])\ndata['cut_score'] = data['cut'].replace(['Ideal','Premium','Very Good','Good','Fair'],[5,4,3,2,1])\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"JGU4Ypqha30A","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"data_score = data.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"meQuIkx-bD_d","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"\nX=DataFrame(data_score,columns =['carat','cut_score','clarity_score','color_score','table %','depth %','L/W','depth','length','width'])\nY=DataFrame(data_score,columns =['price'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Qd3FuvQr8TLK","colab_type":"code","outputId":"40f100c5-301c-4b3e-d61e-68a26572d008","colab":{"base_uri":"https://localhost:8080/","height":33},"trusted":true},"cell_type":"code","source":"skY=data['price'].skew()\nskY","execution_count":null,"outputs":[]},{"metadata":{"id":"wUm1EQ728tRQ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"data_score = DataFrame({'price':np.log(data_score['price'])})\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nuhVIzfM96Bw","colab_type":"code","outputId":"32a342bc-f0a9-4a3f-c11a-ad55fa0590a5","colab":{"base_uri":"https://localhost:8080/","height":33},"trusted":true},"cell_type":"code","source":"sk=data_score['price'].skew()\nsk","execution_count":null,"outputs":[]},{"metadata":{"id":"hrEjXgAI56FX","colab_type":"text"},"cell_type":"markdown","source":"A normal distribution has very few occurrences in the tails. Having more data points in any of the tails is called skew. However, Normal distribution is entirely symmetrical, which means the right and left tail has the same size i.e., the skew of the normal distribution is zero. The more the number of outliers in data, the higher the value of skew. In turn, the prediction of outliers will be challenging by a regression algorithm.\n\nFrom the below visualization, we could see that in our distribution of diamond price, the data points are skewed to the left and have a value of 1.618. Therefore, we need to transform our price data in such a way that the skew must be near to zero.we have used log transformation on the price, and the skew value became 0.115.\n\nTo conclude, The log transformation is done to reduce the skew of price data distribution, which will benefit us from getting a Higher $R^2$ value and also an accurate model."},{"metadata":{"id":"_R7kWlZD6Ymd","colab_type":"code","outputId":"66a79c37-ef0d-453d-92e2-6607dade922f","colab":{"base_uri":"https://localhost:8080/","height":372},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,5])\nplt.subplot(1,2,1)\nplt.hist(data['price'], bins=50, ec='black', color='#2196f3')\nplt.xlabel('Price in thousands')\nplt.ylabel('Nr. of Diamonds')\nplt.title(f'Before Log transformation, Skew:{round(skY,3)}')\n\nplt.subplot(1,2,2)\nY = np.log(Y['price'])\nplt.hist(Y, bins=50, ec='black', color='#2196f3')\nplt.xlabel('Price in logs')\nplt.ylabel('Nr. of Diamonds')\nplt.title(f'After Log transformation, Skew:{round(sk,3)}')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"adWR-V0QZWLz","colab_type":"text"},"cell_type":"markdown","source":"As we have selected our features, we are going to split the dataset into train and test set where 80 percent of the data will go into the training set, and the remaining 20 percent will go into the test."},{"metadata":{"id":"B0LZcGr1bNsS","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=10)\n# Y_train = DataFrame(Y_train)\n# Y_test=DataFrame(Y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","id":"iY4m8VUIpGSm","colab":{},"trusted":true},"cell_type":"code","source":"# scale = StandardScaler()\n# X_train = DataFrame(scale.fit_transform(X_train[['carat','depth %','table %','length','width','depth', 'clarity_score' ,'color_score', 'cut_score','L/W']]),columns=['carat','depth %','table %','length','width','depth', 'clarity_score' ,'color_score', 'cut_score','l/W'],index=X_train.index)\n# X_test = DataFrame(scale.transform (X_test[['carat','depth %','table %','length','width','depth', 'clarity_score' ,'color_score', 'cut_score','L/W']]),columns=['carat','depth %','table %','length','width','depth', 'clarity_score' ,'color_score', 'cut_score','l/W'],index=X_test.index)\n# print(X_train.head())\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"CoLg1nZipF4q"},"cell_type":"markdown","source":"Instead of transforming the whole dataset, we are only transforming training and test data separately. Because if we convert the whole dataset, information about the distribution of the dataset will be known by algorithms, which means it cannot process the outliers efficiently. Therefore, we want the test data to be \"new and unseen\" by our machine learning algorithm and the target value of test data only be predicted by using the distribution value of the training dataset."},{"metadata":{"id":"68dR-fPwBZtY","colab_type":"text"},"cell_type":"markdown","source":"###Regression Model"},{"metadata":{"id":"hY6TDJ_aAKTo","colab_type":"text"},"cell_type":"markdown","source":"Now before implementing the regression model. Let us understand the assumptions of the regression model as well as it is evaluation.\n\n\n<b>Assumptions of the Regression Model</b><br>\n\nThe regression model makes an assumption about the data, and based upon that; It predicts the target value. If these assumptions are not fulfilled, then the model will not predict an accurate value..\n\nEssential assumptions of regression are as follows:-\n<ol>\n  <li>There should be no multi-collinearity.</li>\n  <li> There should be no correlation between residuals. If there is a correlation, then it called Autocorrelation.</li>\n   <li>The residual value should be random i.e.; there should be no pattern. \n<br>If there is a pattern in the residuals, then there also some predictive information in the residuals and if there is predictive information in the residuals, then the predictive information is missing from the model.     </li>\n  \n  <li>Residual should be normally distributed. i.e., the mean and the skew should be equal or near to zero.</li>\n     \n  </ol>\n <b>Evaluation of a Regression Model</b><br>\nThere are several parameters to check a regression model. However, we are going to focus on three essential aspects.\n<ol>\n\n<li>Mean Squared Error (MSE) -> It is an average of the sum of the square of residuals, Which represents an absolute value in the units of the target value.</li>\n<li>R- Sqaure($R ^ 2$)  ->It represents how much variation a target value has against the features and  shows a relative measure in the range of 0 and 1.</li>\n<li>Adjusted $R ^ 2$    -> $R ^ 2$ only increases or remains constant but never decreases even if the model is less precise. However, Adjusted $R ^ 2$ only increases if the model is increasing the precision. It is calculated using the below formula.</li>\n  \n  $$1 -  \\frac{(1 - r2) *(n - 1)}{(n-p-1))}$$\n  \n  \nwhere\nR2 = R square\n\np = Number of features\n\nN = Total Number of rows\n</ol>\n<b>Ploting the Regression Model</b><br>\nLet us also understand, How to plot the regression models and validate them.<br>\nFor a single feature regression model, the best way to visualize it is by showing the best fit line between the independent feature and the target value. However, if it is a multivariate regression, We have to increase the dimensionality of the graph depending upon the number of features which can lead to visual clutter and the user cannot understand the model correctly.<br>\nHowever, to visualize the multivariate model, we can plot three types of a graph to validate the model which are discussed below.\n\n<b>1.Ploting Predicted Values Vs Actual Values</b><br>\n  \nThe rationale behind plotting actual and predicted values is to check how good our prediction is and how our prediction stack up to the actual diamond prices.\nTo check the difference between predicted and actual values in the graph we will draw a straight line as if it had the perfect predictive capabilities which mean plotting   the best-fit line as if the  r-squared value is 1.This would allow us to see how our predictions stack up to the actuals very very clearly by comparing data points with best-fit line.\n\nThe bigger the distance of a point from the line, the bigger the residual or the error.\n\n\n  \n<b>2.Ploting Predicted Values Vs Residual Values</b><br>\nIn the above plotting method, we cannot see the residuals explicitly. We can only see how far the data points are from the line.\nThis plot also will tell us if there is any pattern in the residuals that exists or not.\n  \n<b>3.Plotting the distribution of residuals</b><br>\nThis plot will tell us if the residuals assumption is satisfied or not.\n"},{"metadata":{"id":"xlR2kFlzCHti","colab_type":"text"},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"id":"ZL92g7fvCZPS","colab_type":"text"},"cell_type":"markdown","source":"Before implementing this algorithm let us understand how it works <br>\nLinear regression is used to predict a constant value or a continuous target value from one or multiple features.\nIn another term, it provides us a line equation such as below if linear regression is based on a single variable.\n\n$$y=\\theta x+c$$\n else if it is multiple regression, then it provides the below equation.\n \n$$y=\\theta_0x_0 +\\theta_1x_1+\\theta_2x_2....+\\theta_nx_n + c $$\n\nWhere y is, our target or independent value and $\\theta$ is slope or coefficient, which means the rate of change of the independent variable at a certain point. It also suggests the importance of an independent variable on predicting the target value and $c$ is constant and also called as intercept.\n\nGraphically single variable linear regression is represented by a best-fit line, which represents the difference between actual values and the predicted values i.e., also called residual..\n\nThe lesser the residual better the predicted values.\n\n\nNow let us implement our Linear regression model and evaluate our model."},{"metadata":{"id":"wZEUtP71bMqX","colab_type":"code","outputId":"829c9a8e-1af3-40e4-9c93-24268f28ab21","colab":{"base_uri":"https://localhost:8080/","height":117},"trusted":true},"cell_type":"code","source":"dlin = LinearRegression()\ndlin.fit(X_train, Y_train)\ny_pred = dlin.predict(X_test)\nprint('####### Linear Regression #######')\nprint('Score : %.4f' % dlin.score(X_test, Y_test))\nmse = mean_squared_error(Y_test,y_pred)\nr2 = r2_score(Y_test, y_pred)\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('R2     : %0.2f ' % r2)\nn=X_test.shape[0]\np=X_test.shape[1]\nadj_rsquared = 1 - (1 - r2) * ((n - 1)/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3Kmf8qwiyIgf","colab_type":"text"},"cell_type":"markdown","source":"From the above output, we can see that ($R ^ 2$)  value is 0.97  and MSE is 869465. Based on the score, we can that this model prediction is accurate. However, we need to analyze regression assumptions such as multicollinearity, autocorrelation, hetroskedacity, and normal distribution to validate our regression model.\n\nBefore checking the assumptions let us check the significance of each coefficient as these will let us determine which feature are important and which are not.<br><br>\n###<b>Feature Selection</b><br>\n<b>P-values</b><br>\np- values represent the significance of a coefficient; if the p-value is less than a threshold of 0.05, the coefficient is deemed statistically significant else not significant.\n\nHowever, To calculate the p-values, scikit-learn is not much of a help. Therefore, to look at the detailed statistics of our model, we will be using the Statsmodel module. It provides a lot of statistical methods that are useful in evaluating a regression model and also some methods which are useful in validating the features.\n"},{"metadata":{"id":"R_LSsBvSVNo_","colab_type":"text"},"cell_type":"markdown","source":"Now let us run our regression with this new module.\n\nThe thing to note is that in order to make our regression tie out with scikit-learn we are going to have\n\nto add an intercept because from the above-implemented model, we could see that an intercept was calculated from our scikit-learn regression model.\n\nSo we have to take our features from the training data set and add an intercept using Statsmodel's add_constant() function. Further which the return value from the add_contant() function will be passed to Ordinary Least Square(OLS) method which implements a linear regression model.\n\nMoreover, after passing the feature and target into the OLS method, we will generate the p-values and coefficients."},{"metadata":{"id":"PRYwDzYYmX8T","colab_type":"code","outputId":"837228d3-c059-4a97-d102-716e23cac84c","colab":{"base_uri":"https://localhost:8080/","height":375},"trusted":true},"cell_type":"code","source":"X_incl_const = sm.add_constant(X_train)\ntype(X_incl_const)\nX_incl_const.shape\nmodel = sm.OLS(Y_train,X_incl_const)\nresults = model.fit()\n\n# # #results.params\n# # #results.pvalues\n\npd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})\n","execution_count":null,"outputs":[]},{"metadata":{"id":"b3PVu7P75rOr","colab_type":"text"},"cell_type":"markdown","source":"We could see that from the above output all the feature coefficient's p-values are less than 0.05. Therefore, all the features are statistically significant in determining the price of a diamond.\n\nNow we will check if there is any multicollinearity in the model.\nMulticollinearity occurs when two or more features variables in regression are highly related to one another.\n\nIn that case, they do not provide unique or independent information to the model and can lead to\n loss of reliability in the estimates of the effects for the individual features.\n<br><br>\n<b>Variance Inflation factor(V.I.F)</b></br>\nWe can check the multicollinearity in the model using a statistical factor called V.I.F., which is also called a Variance Inflation factor(V.I.F.).\nIf V.I.F. is less than a threshold of 10 means that the feature is not co-related to other feature else multicollinearity exists in the model.\n\n"},{"metadata":{"id":"qjneWumCcV8W","colab_type":"code","outputId":"4ae3f97d-f2d3-43cc-a34c-bd8a86c9a6e9","colab":{"base_uri":"https://localhost:8080/","height":375},"trusted":true},"cell_type":"code","source":"vif = [variance_inflation_factor(exog=X_incl_const.values, \n                                 exog_idx=i) for i in range(X_incl_const.shape[1])]\n\npd.DataFrame({'coef_name': X_incl_const.columns, \n             'vif': np.around(vif, 2)})","execution_count":null,"outputs":[]},{"metadata":{"id":"XI1J6rOWdWO3","colab_type":"text"},"cell_type":"markdown","source":"From the above, we could see that the feature carat, length, width, and depth are having high collinearity with each other. As we know carat has a high correlation with predicting the price and is one of the critical features in determining the price of a diamond, let us delete the length,  width and depth column and check the VIF values to see if there are any values greater than 10 or not.\n"},{"metadata":{"id":"HjVemMNhr5FB","colab_type":"code","outputId":"1bf704ef-5dd0-40e8-96ee-789de0b9b8e9","colab":{"base_uri":"https://localhost:8080/","height":336},"trusted":true},"cell_type":"code","source":"# Reduced model #1 excluding carat\nX_incl_const = sm.add_constant(X_train)\nX_incl_const = X_incl_const.drop(['length','width','depth'], axis=1)\n\nmodel = sm.OLS(Y_train, X_incl_const)\nresults = model.fit()\n\ncoef_minus_indus = pd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})\n\nr2_lr=results.rsquared\nmse_lr =results.mse_resid\nar2_lr =results.rsquared_adj\nar2_lr = results.rsquared_adj\nprint('r-squared is', r2_lr)\nprint('MSE is', results.mse_resid)\nprint('Adjusted R -sqaure is', results.rsquared_adj)\nvif = [variance_inflation_factor(exog=X_incl_const.values, \n                                 exog_idx=i) for i in range(X_incl_const.shape[1])]\n\npd.DataFrame({'coef_name': X_incl_const.columns, \n             'vif': np.around(vif, 2)})","execution_count":null,"outputs":[]},{"metadata":{"id":"R9Z68AerxqVe","colab_type":"text"},"cell_type":"markdown","source":"From the above output, we can see that there are no features that are co-related with each other as all the feature's VIF value is less than 10.However our model's accuracy has been decreased as the R-square, and MSE value has been reduced to 0.85.\n\nNow if we look back at our first regression in which we took all the features , there were 3 features which were co-related with each other , which means there will a high amount of uncertainty and an incorrect conclusion in determining the importance of a feature and also there will be high standard errors which will lead to higher intervals or ranges. Overall, the model with all the features was incorrect.\n\nNow let us plot the graph to understand more about the model and validate the regression assumptions."},{"metadata":{"id":"7xwOOWMbr_UJ","colab_type":"code","outputId":"a2000e9c-5773-4faf-c4ad-57487fb15574","colab":{"base_uri":"https://localhost:8080/","height":393},"trusted":true},"cell_type":"code","source":"del X_train['length']\ndel X_train['width']\ndel X_train['depth']\n# X_train.head()\n\ndel X_test['length']\ndel X_test['width']\ndel X_test['depth']\n# Using Statsmodel\nX_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(Y_train, X_incl_const)\nresults = model.fit()\n\nplt.figure(figsize=[20,5])\nplt.subplot(1,3,1)\n\ncorr = round(Y_train.corr(results.fittedvalues), 5)\nplt.scatter(x=Y_train, y=results.fittedvalues, c='navy', alpha=0.6)\nplt.plot(Y_train, Y_train, color='cyan')\n\nplt.xlabel('Actual log prices')\nplt.ylabel('Prediced log prices ')\nplt.title(f'1(a) Actual vs Predicted log prices: \\n (Corr {corr})')\n\n\nplt.subplot(1,3,2)\n# Residuals vs Predicted values\n\nplt.scatter(x=results.fittedvalues, y=results.resid, c='navy', alpha=0.6)\n\nplt.xlabel('Predicted log prices')\nplt.ylabel('Residuals', fontsize=14)\nplt.title('1(b) Residuals vs Fitted Values')\n\n# Distribution of Residuals (log prices) - checking for normality\nresid_mean = round(results.resid.mean(), 3)\nresid_skew = round(results.resid.skew(), 3)\n# print(resid_mean)\n# print(resid_skew)\nplt.subplot(1,3,3)\nsns.distplot(results.resid, color='navy')\nplt.xlabel('Residuals')\n\nplt.title(f'1(c) Log price model:\\n residuals Skew({resid_skew}) Mean ({resid_mean})')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"dU83fiGN5UMi","colab_type":"text"},"cell_type":"markdown","source":"#### Plotting Linear Regression Model\n\nFrom the above <b> Figure 1(a) Predicted vs. Actual Values</b>, We could say that a lot of predicted values are very close to the line. (This line is not the best fit line for this scatter plot. The line here shows where a point would be if our prediction were perfect and residuals would be equal to zero.).\nHowever, If we look at the top right and the bottom left, we could see that for some of the data points the residuals are actually very large, and our model is performing quite poorly for those data points. These points are outliers. Therefore, we could say that the linear regression model is working poorly for the outliers.\n\nFrom the above <b> Figure 1(b)Residuals vs. Predicted values </b>, We could say that there is a pattern in the residuals which is the higher the log diamond's prices lesser the residual, again these data points are outliers in the data.\nTherefore, We could say that one of the assumptions is not being held up by this model.\n\nFrom the above  <b>Figure 1(c)Distribution of Residuals  </b>, We could say that that the residuals are asymmetrical and they are more spreading in the left direction.\nFurthermore, we notice that this distribution in contrast to a normal distribution\nhas much longer tails as well as two bigger peaks. So there are more values in the extreme left than what we would see with a normal distribution. Therefore, the residual values are not normally distributed.\n\nOverall, This model is not able to satisfy the regression assumptions and also not able to accurately predict the outliers.\n"},{"metadata":{"id":"Q9iz-1-bERk_","colab_type":"text"},"cell_type":"markdown","source":"### Polynomial Regression"},{"metadata":{"id":"MzvCiF83Zf8L","colab_type":"text"},"cell_type":"markdown","source":"\nBefore implementing this algorithm, let us understand the polynomial regression in detail.\n\nTo handle the non-linear data, we use the Polynomial Regression model. It works by adding extra independent variables that are powers of the original variables ($x$,$x^1$,$x^2$,.etc). into the model<br>\nPower of the independent variable must be greater than 1.<br>\na best-fit curve is used to represent a single feature polynomial regression in a graph.\nWe can use any order of polynomials. However the higher the order of polynomial greater the risk of overfitting the model.<br>\nOn our data, we are going to use second-order polynomial, and the model is calculated based on the below equation.\n$$y=\\theta_0 +\\theta_1x+\\theta_2x^2$$\n\nwhere y is our predicted value, x is our independent variable and $\\theta$'s are coefficients.\nThis equation is calculated for each feature in our model.\n."},{"metadata":{"id":"0X-fYI1ayzok","colab_type":"code","outputId":"92ed6b74-4943-4e04-f402-c1a930c664bd","colab":{"base_uri":"https://localhost:8080/","height":134},"trusted":true},"cell_type":"code","source":"poly_features = PolynomialFeatures(degree=2)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\n\npoly_model = LinearRegression()\npoly_model.fit(X_train_poly, Y_train)\n\ny_test_predict = poly_model.predict(X_test_poly)\n\nr2_pr =poly_model.score(X_test_poly, Y_test)\n\nprint('####### Polynomial Regression #######')\nprint('Test R2 Score : %.4f' % r2_pr)\n\n\nmse_pr = mean_squared_error(Y_test, y_test_predict)\nrmse_pr = mean_squared_error(Y_test, y_test_predict)**0.5\nr2 = r2_score(Y_test, y_test_predict)\n\nprint('')\nprint('MSE    : %0.2f ' % mse_pr)\nprint('RMSE   : %0.2f ' % rmse_pr)\nprint('Actual-Predicted R2 Score     : %0.2f ' % r2)\n\nn=X_test.shape[0]\np=X_test.shape[1]\n\n\nar2_pr = 1 - (1 - r2) * ((n - 1)/(n-p-1))\nprint('Adjusted R Squared: {}'.format(ar2_pr))\n# pd.DataFrame(data=, index=X.columns, columns=['coef'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vWviLqhMFTJR","colab_type":"text"},"cell_type":"markdown","source":"We can see that from the above output, the $R^2$ value has been increased drastically to 0.95, and the Mean squared error value also has been reduced compared to the previous model. Now let us plot the model to validate the assumptions and the predicted values."},{"metadata":{"id":"5n_ZFW-J364R","colab_type":"code","outputId":"78de3077-8a14-4369-9e2e-bcbd8bb649bd","colab":{"base_uri":"https://localhost:8080/","height":426},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[20,5])\nplt.subplot(1,3,1)\n\nplt.scatter(x=Y_test, y=y_test_predict, c='navy', alpha=0.6)\nplt.plot(Y_train, Y_train, color='cyan')\n\n\n# corr = round(Y_train.corr(y_test_predict), 2)\nplt.xlabel('Actual log prices ', fontsize=14)\nplt.ylabel('Prediced log prices ', fontsize=14)\nplt.title(f'2(a) Actual vs Predicted log prices: ')\n\nplt.subplot(1,3,2)\n\n\n# Residuals vs Predicted values\n\nplt.scatter(x=y_test_predict, y=(Y_test - y_test_predict), c='navy', alpha=0.6)\n\nplt.xlabel('Predicted log prices', fontsize=14)\nplt.ylabel('Residuals', fontsize=14)\nplt.title('2(b) Residuals vs Fitted Values', fontsize=17)\n\nplt.subplot(1,3,3)\n# Distribution of Residuals (log prices) - checking for normality\nresid_mean = round((Y_test - y_test_predict).mean(), 3)\nresid_skew = round((Y_test - y_test_predict).skew(), 3)\nprint(resid_mean)\nprint(resid_skew)\n\nsns.distplot((Y_test - y_test_predict), color='navy')\nplt.xlabel('Residuals')\nplt.title(f'2(c) Log price model:\\n residuals Skew ({resid_skew}) Mean ({resid_mean})')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"B_zmyq2PKcFm","colab_type":"text"},"cell_type":"markdown","source":"####Plotting Polynomial Regression Model\n\nFrom the above <b> Figure 2(a) Predicted vs. Actual Values</b>, We could say that a lot of predicted values are very close to the line. (This line is not the best fit line for this scatter plot. The line here shows where a point would be if our prediction were perfect and residuals would be equal to zero.).\nHowever, comparing with our linear regression model, the predicted are performing accurately with all the data points except the top-left point(an outlier).\n\n\nFrom the above <b> Figure 2(b)Residuals vs. Predicted values </b>, We could say that there is no pattern in the residuals. However, at the bottom right corner, one data point is standing alone, which is an outlier.\nTherefore, We could say that this model is somewhat holding up by the assumptions of no pattern in residuals better than the linear regression model.\n\nFrom the above  <b>Figure 2(c)Distribution of Residuals  </b>, We could say that that the residuals are relatively symmetrical and they are spreading more in the left direction than in the right.\nFurthermore, we notice that this distribution in contrast to a normal distribution\nhas much longer tails. So there are more values in the extremes than what we would see with a normal distribution. Therefore, this model is residual values equivalent to the normal distribution and also compared to the linear regression model, the residual skew value is now closer to zero.\n\nOverall, This model can satisfy the regression assumptions and also able to accurately predict the values. However, for some outliers, it is not able to predict accurately."},{"metadata":{"id":"S2oJ4OIREYXU","colab_type":"text"},"cell_type":"markdown","source":"### RandomForest Regression"},{"metadata":{"id":"XprS5Pw8-4ab","colab_type":"text"},"cell_type":"markdown","source":"Before implementing this algorithm, let us understand the Random Forest in detail.\n\nRandom Forest is made out of decision trees. Before going further, let us understand about decision trees.\n\nDecision trees are predictive algorithms that use a set of trees(based on the number of features)  to predict a target value.\n\n\nIt combines the simplicity of decision trees with flexibility\nresulting in a vast improvement in accuracy.<br>\nIt creates a bootstrapped dataset by randomly selecting samples from the original dataset, and an interesting factor here is that it can select the same sample more once.<br>\nAfter creating a bootstrapped dataset, it creates a decision based on the bootstrapped dataset but only uses a random subset of features at each step.<br>\nThis process of creating a bootstrapped dataset and building a decision tree based on a subset of the feature is repeated  'N' number of times. This result in a wide variety of trees which in turn makes the random forest more effective than individual trees.<br>\nAfter creating the trees it will predict the value or make the decision based on the aggregate of the target value which was determined by each decision trees.this process is also called as bagging.<br>\n"},{"metadata":{"colab_type":"code","outputId":"725c1e9d-0a9b-41d2-f0b3-3e3289a177dd","id":"bxzwRcxaFK93","colab":{"base_uri":"https://localhost:8080/","height":150},"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(X_train , Y_train)\n# accuracies = cross_val_score(estimator = clf_rf, X = X_train, y = Y_train, cv = 4,verbose = 1)\ny_pred = rf.predict(X_test)\nr2_rf =rf.score(X_test, Y_test)\nprint('')\nprint('###### Random Forest ######')\nprint('Testdata R2 :Score : %.4f' % r2_rf )\n# print(accuracies)\nmse_rf = mean_squared_error(Y_test, y_test_predict)\nrmse_rf = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse_rf)\nprint('RMSE   : %0.2f ' % rmse_rf)\nprint('actual-predicted R2     : %0.2f ' % r2)\n\nar2_rf = 1 - (1 - r2) * ((n - 1)/(n-p-1))\nprint('Adjusted R Squared: {}'.format(ar2_rf))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"VGQZOGULGBSz","colab_type":"text"},"cell_type":"markdown","source":"We can see that from the above output the $R^2$ value has been increased drastically to 0.99 and the Mean squared error value also has been reduced compared to the previous model and also RMSE has decreased to 0.03 compared to other models. Therefore, we can our model is predicting the values accurately compared to the other models. However, let us plot the model to validate the assumptions and the predicted values."},{"metadata":{"id":"OjkDQ68i48_z","colab_type":"code","outputId":"d9a44387-b0d5-413b-e3d0-959928661fe6","colab":{"base_uri":"https://localhost:8080/","height":446},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[25,5])\nplt.subplot(1,3,1)\n\nplt.scatter(x=Y_test, y=y_pred, c='navy', alpha=0.6)\nplt.plot(Y_train, Y_train, color='cyan')\n\nplt.xlabel('Actual log prices ', fontsize=14)\nplt.ylabel('Prediced log prices ', fontsize=14)\nplt.title(f' 3(a) Actual vs Predicted log prices')\n\nplt.subplot(1,3,2)\n\n\n# Residuals vs Predicted values\n\nplt.scatter(x=y_pred, y=(Y_test - y_pred), c='navy', alpha=0.6)\n\nplt.xlabel('Predicted log prices ',)\nplt.ylabel('Residuals', )\nplt.title(' 3(b) Residuals vs Fitted Values')\n\nplt.subplot(1,3,3)\n# Distribution of Residuals (log prices) - checking for normality\nresid_mean = round((Y_test - y_pred).mean(), 3)\nresid_skew = round((Y_test - y_pred).skew(), 3)\nprint(resid_mean)\nprint(resid_skew)\n\nsns.distplot((Y_test - y_pred), color='navy')\nplt.xlabel('Residuals')\nplt.title(f' Log price model:\\n residuals Skew ({resid_skew}) Mean ({resid_mean})')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1BrIWP8I5sEe","colab_type":"text"},"cell_type":"markdown","source":"####Plotting Random Forest Regression Model\n\nFrom the above <b> Figure 3(a) Predicted vs. Actual Values</b>, We could say that a lot of predicted values are very close to the line. (This line is not the best fit line for this scatter plot. The line here shows where a point would be if our prediction were perfect and residuals would be equal to zero.).This graph further suggests that this model, compared to other previous models, can predict the prices more accurately. However, if we look at below the line, for some data points, the residual is quite large.\n\n\n\nFrom the above <b> Figure 3(b)Residuals vs. Predicted values </b>, We could say that there is no pattern in the residuals. It shows a cloud shape where most of the residual are centered around zero, and the cloud is more or less symmetrical, and there is not any bias for low and high predicted values.\n\nFrom the above  <b>Figure 3(c)Distribution of Residuals  </b>, We could say that that the residuals are symmetrical and they are equally spreading on both of the tails.\nFurthermore, we notice that this distribution in contrast to a normal distribution has fairly a longer tails. So there are more values in the extremes than what we would see with a normal distribution.Therefore , this model's residual values are equivalent to the normal distribution and also compared to the other previous regression model, the residual skew value is now closer to zero.i.e 0.2.\n\nOverall, This model can satisfy the regression assumptions and also able to accurately predict the value. Moreover, It can predict the values better than other regression models based on our evaluation scores."},{"metadata":{"id":"hFHrg8D_WTze","colab_type":"code","outputId":"d7027b07-6ab8-44e6-96a4-bb1ee5e4e2d1","colab":{"base_uri":"https://localhost:8080/","height":137},"trusted":true},"cell_type":"code","source":"pd.DataFrame({'AdjustedR-Sqr': [ar2_lr, ar2_pr, ar2_rf],\n              'R-Squared': [r2_lr, r2_pr, r2_rf],\n             'MSE': [mse_lr, mse_pr, mse_rf], \n             'RMSE': np.sqrt([mse_lr, mse_pr, mse_rf])\n             }, \n            index=['Linear Regression', 'Polynomial Regression', 'RandomForest Regression'])","execution_count":null,"outputs":[]},{"metadata":{"id":"dCbaq2JCMOA2","colab_type":"text"},"cell_type":"markdown","source":"From the above output, we can say RandomForest Regression can predict the values better than any other model as the evaluation score are accurate compared to others, and also it satisfies the regression assumptions.\nNote:  As discussed before the MSE values are absolute and take up the unit of the dependent variable, thus, MSE and RMSE  for each model are in the units of the log."},{"metadata":{"id":"fvyBOMRtOyB9","colab_type":"text"},"cell_type":"markdown","source":"###  Discussion"},{"metadata":{"id":"0XD9TO0vuiN-","colab_type":"text"},"cell_type":"markdown","source":"In this section, we will create a function that will take a data frame consist of diamond features as an argument and will return the predicted price and the range of the predicted price."},{"metadata":{"id":"HJa8fUXQPSNn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def test(testData):\n\n  # Data Transformation\n  testData['clarity_score']=testData['clarity'].replace(['IF','VVS1','VVS2','VS1','VS2','SI1','SI2','I1'],[8,7,6,5,4,3,2,1])\n  testData['color_score'] = testData['color'].replace(['D','E','F','G','H','I','J'],[7,6,5,4,3,2,1])\n  testData['cut_score'] = testData['cut'].replace(['Ideal','Premium','Very Good','Good','Fair'],[5,4,3,2,1])\n  \n  X=DataFrame(testData,columns =['carat','cut_score','clarity_score','color_score','table %','depth %','L/W'])\n  Y= rf.predict(X)\n  \n  upper_bound = Y + rmse_rf\n  lower_bound = Y - rmse_rf\n  print(f'The price predited by our model is {round(np.e**Y[0],2)}')\n  print(f'The price predited by our model is in range between {round(np.e**lower_bound[0],2)} \\\nand {round(np.e**upper_bound[0],2)}')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3HKq2JSN05w-","colab_type":"text"},"cell_type":"markdown","source":"Now we will create two data frame which consists of diamond features.\nThe features data will be selected randomly within the range of the dataset, and we will check if it can predict the price accurately or not."},{"metadata":{"id":"Dxtl1dyfVCT7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"testData=pd.DataFrame({'carat': 1.01,\n                                      'clarity': 'IF' ,\n                                      'cut': 'Ideal', \n                                      'color': 'D',\n                                      'table %': 56 ,\n                                      'depth %' : 54.9,\n                                      'L/W' : 1.00\n                                      }, index =[0])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-EiaNZ1TY9G_","colab_type":"code","outputId":"f986c54f-c36e-404c-b0fd-8d567ca54e7d","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"test(testData)","execution_count":null,"outputs":[]},{"metadata":{"id":"vrt07GQDSjJA","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"testData1=pd.DataFrame({'carat': 2.01,\n                                    'clarity': 'IF' ,\n                                    'cut': 'Ideal', \n                                    'color': 'D',\n                                    'table %': 56,\n                                    'depth %' : 54.9,\n                                    'L/W' : 1.00\n                                    }, index =[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"6IQYUsSnTqee","colab_type":"code","outputId":"c071c9c7-d584-4074-ce84-95c4e16ef4d1","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"test(testData1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"eXL5bx0IaOJ2","colab_type":"text"},"cell_type":"markdown","source":"From the above, we can infer that this model can accurately predict the diamond price for the interpolation data. As we can see that the two data frames testData and testData1 which we created, have the same information except for the carat weight, and we found that the price predicted for testData is higher than the price predicted testData1 because the testData had a more significant carat weight than the testData1 carat weight.\n\nNow let us check the model with Extrapolation data."},{"metadata":{"id":"7W2AnSs37qL_","colab_type":"code","outputId":"10d4567f-a5ab-4e06-cb37-624a88a93441","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"testData2=pd.DataFrame({'carat': 50,\n                                    'clarity': 'IF' ,\n                                    'cut': 'Ideal', \n                                    'color': 'D',\n                                    'table %': 58,\n                                    'depth %' : 65.5,\n                                    'L/W' : 1.01\n                                    }, index =[0])\ntest(testData2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZHAaaGce8Zuw","colab_type":"text"},"cell_type":"markdown","source":"From the above, we can see that this model is not able to predict the price accurately As we have passed the same information again except the carat weight, which we have assigned a value of 50 and it predicted a price value which is lower than the previous test data where we passed the carat weight as 2.01.\n\nOverall, This model can predict the price values accurately for the data which are in the range of this dataset. However, not able to predict accurately for the extrapolate data. "},{"metadata":{"id":"zFzPBjOyjjJN","colab_type":"text"},"cell_type":"markdown","source":"###  Summary"},{"metadata":{"id":"SEqKAsFGql6C","colab_type":"text"},"cell_type":"markdown","source":"The goal of our analysis was to \"How to determine the price of a diamond,\" and after in-depth analysis, we came to know that the features that affect the price of a diamond  when applied to our model are carat, cut, color, clarity, depth %, table % and length to width ratio.\nOur model has an accuracy of 99 percent which no other model or algorithm has shown this much of precision in predicting the diamond price.\nHowever, Due to the absence of Date Feature, this model is relative to this dataset only.\n\nOverall, the prediction of our model is accurate. However, this model is not able to perform well with Extrapolation data due to insufficient data."},{"metadata":{"id":"cZ6-sV1jdtrF","colab_type":"text"},"cell_type":"markdown","source":"\n\n"}],"metadata":{"colab":{"name":"A-Shankar-Vignesh_Overall.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["68dR-fPwBZtY"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}