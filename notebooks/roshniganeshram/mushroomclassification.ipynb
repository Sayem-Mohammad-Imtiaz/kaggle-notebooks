{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mushrooms.describe()"},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\nmushrooms.describe()\n#To check the top few rows\nmushrooms.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the dimensions and shape\nmushrooms.ndim\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting different variables to see the distribution\nimport seaborn as sns\nplot1 = sns.countplot(x= 'odor', data = mushrooms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot2 = sns.countplot(x= 'class', data = mushrooms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot3 = sns.countplot(x= 'cap-surface', data = mushrooms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot3 = sns.countplot(x= 'cap-color', data = mushrooms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Variablenames Extraction\n\nvariable_labels = np.asarray(mushrooms.columns)[0:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_var = variable_labels[1:22]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace Categoricaldata with dummy variable\n#Used Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nencoder.fit(mushrooms['cap-shape'].drop_duplicates())\nmushrooms['cap-shape']=encoder.transform(mushrooms['cap-shape'])\nencoder.fit(mushrooms['cap-surface'].drop_duplicates())\nmushrooms['cap-surface']=encoder.transform(mushrooms['cap-surface'])\nencoder.fit(mushrooms['cap-color'].drop_duplicates())\nmushrooms['cap-color']=encoder.transform(mushrooms['cap-color'])\nencoder.fit(mushrooms['bruises'].drop_duplicates())\nmushrooms['bruises']=encoder.transform(mushrooms['bruises'])\nencoder.fit(mushrooms['odor'].drop_duplicates())\nmushrooms['odor']=encoder.transform(mushrooms['odor'])\nencoder.fit(mushrooms['gill-attachment'].drop_duplicates())\nmushrooms['gill-attachment']=encoder.transform(mushrooms['gill-attachment'])\nencoder.fit(mushrooms['gill-spacing'].drop_duplicates())\nmushrooms['gill-spacing']=encoder.transform(mushrooms['gill-spacing'])\nencoder.fit(mushrooms['gill-size'].drop_duplicates())\nmushrooms['gill-size']=encoder.transform(mushrooms['gill-size'])\nencoder.fit(mushrooms['gill-color'].drop_duplicates())\nmushrooms['gill-color']=encoder.transform(mushrooms['gill-color'])\nencoder.fit(mushrooms['stalk-shape'].drop_duplicates())\nmushrooms['stalk-shape']=encoder.transform(mushrooms['stalk-shape'])\nencoder.fit(mushrooms['stalk-root'].drop_duplicates())\nmushrooms['stalk-root']=encoder.transform(mushrooms['stalk-root'])\nencoder.fit(mushrooms['stalk-surface-above-ring'].drop_duplicates())\nmushrooms['stalk-surface-above-ring']=encoder.transform(mushrooms['stalk-surface-above-ring'])\nencoder.fit(mushrooms['stalk-surface-below-ring'].drop_duplicates())\nmushrooms['stalk-surface-below-ring']=encoder.transform(mushrooms['stalk-surface-below-ring'])\nencoder.fit(mushrooms['stalk-surface-below-ring'].drop_duplicates())\nmushrooms['stalk-surface-below-ring']=encoder.transform(mushrooms['stalk-surface-below-ring'])\nencoder.fit(mushrooms['stalk-color-above-ring'].drop_duplicates())\nmushrooms['stalk-color-above-ring']=encoder.transform(mushrooms['stalk-color-above-ring'])\nencoder.fit(mushrooms['stalk-color-below-ring'].drop_duplicates())\nmushrooms['stalk-color-below-ring']=encoder.transform(mushrooms['stalk-color-below-ring'])\nencoder.fit(mushrooms['veil-type'].drop_duplicates())\nmushrooms['veil-type']=encoder.transform(mushrooms['veil-type'])\nencoder.fit(mushrooms['veil-color'].drop_duplicates())\nmushrooms['veil-color']=encoder.transform(mushrooms['veil-color'])\nencoder.fit(mushrooms['ring-number'].drop_duplicates())\nmushrooms['ring-number']=encoder.transform(mushrooms['ring-number'])\nencoder.fit(mushrooms['ring-type'].drop_duplicates())\nmushrooms['ring-type']=encoder.transform(mushrooms['ring-type'])\nencoder.fit(mushrooms['spore-print-color'].drop_duplicates())\nmushrooms['spore-print-color']=encoder.transform(mushrooms['spore-print-color'])\nencoder.fit(mushrooms['population'].drop_duplicates())\nmushrooms['population']=encoder.transform(mushrooms['population'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data into training and testing\nfrom sklearn.model_selection import train_test_split\n\n#use iloc for selecting the class as y variable as this is about mushroom classification and all others as X variable\nX = mushrooms.iloc[:,1:22]\ny = mushrooms.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2 , random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Method1 - RandomForestClassification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nrf = RandomForestClassifier(n_estimators = 100)\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rf = rf.predict(X_test)\nerror_rf = metrics.accuracy_score(y_test,y_pred_rf)\nprint(np.sqrt(error_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Importance Plot\nimportance = rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize Feature Importance\nindices = np.argsort(rf.feature_importances_)[::-1]\nnames = [X_var[i] for i in indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.title('Feature Importance')\nplt.bar(range(X.shape[1]), importance[indices])\nplt.xticks(range(X.shape[1]), names, rotation=90)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Method 2 - K Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))\n#From the confusion matrix and the classification report it can be seen that the class e 849 have been predicted correctly and\n#3 are predicted wrong. Similarly, for class p all 773 are predicted correctly.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initiate the error values as a empty matrix. With the loop running from 1 to 20 the error value will get appendend accordingly\nerror = []\n\n# Calculating error for K values between 1 and 20\nfor i in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the above we can see mean error is lowest when k=1,2 and k=5,6 . Since k=1 will not classify it we can go for the next best \n#option that produces lowest error which is k=2 which is ideal in this case as 'Class' has 2 levels e and p\n\n#Accuracy\ny_pred_knn = classifier.predict(X_test)\nerror_knn = metrics.accuracy_score(y_test,y_pred_knn)\nprint(np.sqrt(error_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the above 2 methods of knn and random forest we can see that random forest has slightly better accuracy than knn\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}