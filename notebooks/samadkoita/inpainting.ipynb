{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import print_function\n%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n\n\n\nmanualSeed = 999\n\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nimagefolder = '/kaggle/input/celeba-dataset/img_align_celeba'\nprint(os.listdir(imagefolder))\n\nout_folder = \"/kaggle/working/models/celeba/\"\n\nbatch_size = 128\nimage_size = 64\nnc = 3\nnoise_dim = 100\nnum_epochs = 0\nlr = 0.0002\nbeta1 = 0.5\niter_check = 2000\nprint_check = 100\ndemo_batch_size = 5\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\nprint(device)\n\nNUM_TRAIN = 200000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dset.ImageFolder(root=imagefolder,transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\nprint(len(dataset))\nval_mask  = [i for i in range(NUM_TRAIN,len(dataset))]\nprint(val_mask[-1])\nval_set = torch.utils.data.Subset(dataset,val_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom models import Generator,Discriminator\n\npretrained = torch.load(\"../input/pretrained/celeba_epoch31.pt\")\nGen_eval = Generator().to(device)\nDis_eval = Discriminator().to(device) \n\nGen_eval.load_state_dict(pretrained[\"state_dict_G\"])\nDis_eval.load_state_dict(pretrained[\"state_dict_D\"])\n\nGen_eval.eval()\nDis_eval.eval()\n\nfor p in Gen_eval.parameters():\n    p.requires_grad=False\n    \nfor p in Dis_eval.parameters():\n    p.requires_grad=False\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nclass ImageMask(torch.utils.data.Dataset):\n  def __init__(self, dataset, image_size=(3,64,64),param=0.4):\n    self.dataset = dataset\n    self.image_size = image_size\n    self.image_shape = (image_size[1],image_size[2])\n    self.map = ['left','random','center','box','up','down']\n    self.param = param\n  \n  def __getitem__(self, index):\n    target_image = self.dataset[index][0]\n    assert(torch.is_tensor(target_image))\n    \n    maskType = self.map[index%4]\n    mask = np.ones(self.image_shape)\n    if index%8==0:\n        maskType = 'right'\n    \n\n    param = 0.6\n    if maskType == 'random':\n        mask[np.random.random(self.image_shape) < param] = 0.0\n    elif maskType == 'center':\n        centerScale = 0.3\n        sz = tuple([(int)(z * centerScale) for z in self.image_shape])\n        mask[ sz[1]:-sz[1], sz[0]:-sz[0]] = 0.0\n    elif maskType == 'left':\n        sz = np.random.randint(10,64-35,size=(2,))\n        mask[sz[0]:sz[0]+10,sz[1]:sz[1]+30] = 0.0\n    elif maskType == 'right':\n        sz = np.random.randint(10,64-35,size=(2,))\n        mask[sz[1]:sz[1]+30,sz[0]:sz[0]+10] = 0.0\n    elif maskType == 'box':\n        sz = np.random.randint(10,64-20,size=(3,2))\n        mask[sz[0][0]:sz[0][0]+10,sz[0][1]:sz[0][1]+10] = 0.0\n        mask[sz[1][0]:sz[1][0]+10,sz[1][1]:sz[1][1]+10] = 0.0\n        mask[sz[2][0]:sz[2][0]+10,sz[2][1]:sz[2][1]+10] = 0.0\n    elif maskType == 'up':\n        c = self.image_shape[0] // 2\n        mask[:,:c] = 0.0\n    elif maskType == 'down':\n        c = self.image_shape[0] // 2\n        mask[:,c:] = 0.0\n    else:\n        assert(False)\n\n    return (target_image, torch.FloatTensor(mask),maskType)\n  \n  def __len__(self):\n    return len(self.dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masked_dataset = ImageMask(val_set)\nmasked_loader = torch.utils.data.DataLoader(masked_dataset, batch_size=12,\n                                         shuffle=False,drop_last=True ,num_workers=2,pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_fun = dset.ImageFolder(root=\"../input/images-fun\",transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\nmasked_dataset_fun = ImageMask(dataset_fun)\nmasked_loader = torch.utils.data.DataLoader(masked_dataset_fun, batch_size=12,\n                                         shuffle=False,drop_last=True ,num_workers=2,pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WINDOW_SIZE = 7\nprior_loss_parameter = 0.003\nspecial_conv = torch.ones(1,1,WINDOW_SIZE,WINDOW_SIZE,device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/moodoki/semantic_image_inpainting/blob/extensions/src/inpaint.py\n#\n#\n# Original code from https://github.com/parosky/poissonblending \nimport scipy.sparse\nimport os\nimport PIL.Image\nimport torch\nfrom torchvision.transforms import ToPILImage,ToTensor\ntry:\n    import pyamg\nexcept:\n    os.system(\"pip3 install pyamg\")\nimport pyamg\n\n\ndef convert_to_np(pic):\n    npimg = None\n    if isinstance(pic, torch.FloatTensor):\n        pic = pic.mul(255).byte()\n    if isinstance(pic, torch.Tensor):\n        npimg = np.transpose(pic.numpy(), (1, 2, 0))\n        \n    assert npimg is not None\n    \n    return npimg\n\n\n# pre-process the mask array so that uint64 types from opencv.imread can be adapted\ndef unnormalise(img):\n    return img/2 + 0.5\n\ndef poissonblending_batch(og_batch,gan_batch,mask_inv_batch):\n    out = torch.zeros_like(gan_batch)\n    i = 0\n    for og,gan,mask_inv in zip(og_batch,gan_batch,mask_inv_batch):\n        out[i,:,:,:] = ToTensor()(poissonblending_fromcpu(og,gan,mask_inv))\n        i += 1\n        \n    return out\n        \n\ndef poissonblending_fromcpu(imgt, imgs, mask):\n    mask = convert_to_np(mask)\n    mask = np.squeeze(mask)\n    imgt = convert_to_np(imgt)\n    imgs = convert_to_np(imgs)\n    \n    assert imgs.shape==(64,64,3)\n    assert mask.shape==(64,64)\n    \n    assert np.min(mask)==0 \n    assert np.max(mask)==255\n    \n    return blend(imgt, imgs,mask)\n\ndef blend(img_target, img_source, img_mask, offset=(0, 0)):\n    # compute regions to be blended\n    region_source = (0,0,64,64)\n    region_target = (0,0,64,64)\n    region_size = (region_source[2]-region_source[0], region_source[3]-region_source[1])\n\n    # clip and normalize mask image\n    img_mask[img_mask==0] = False\n    img_mask[img_mask!=False] = True\n\n\n    # create coefficient matrix\n    A = scipy.sparse.identity(np.prod(region_size), format='lil')\n    for y in range(region_size[0]):\n        for x in range(region_size[1]):\n            if img_mask[y,x]:\n                index = x+y*region_size[1]\n                A[index, index] = 4\n                if index+1 < np.prod(region_size):\n                    A[index, index+1] = -1\n                if index-1 >= 0:\n                    A[index, index-1] = -1\n                if index+region_size[1] < np.prod(region_size):\n                    A[index, index+region_size[1]] = -1\n                if index-region_size[1] >= 0:\n                    A[index, index-region_size[1]] = -1\n    A = A.tocsr()\n    \n    # create poisson matrix for b\n    P = pyamg.gallery.poisson(img_mask.shape)\n\n    # for each layer (ex. RGB)\n    for num_layer in range(img_target.shape[2]):\n        # get subimages\n        t = img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer]\n        s = img_source[region_source[0]:region_source[2], region_source[1]:region_source[3],num_layer]\n        t = t.flatten()\n        s = s.flatten()\n\n        # create b\n        b = P * s\n        for y in range(region_size[0]):\n            for x in range(region_size[1]):\n                if not img_mask[y,x]:\n                    index = x+y*region_size[1]\n                    b[index] = t[index]\n\n        # solve Ax = b\n        x = pyamg.solve(A,b,verb=False,tol=1e-10)\n\n        # assign x to target image\n        x = np.reshape(x, region_size)\n        x[x>255] = 255\n        x[x<0] = 0\n        x = np.array(x, img_target.dtype)\n        img_target[region_target[0]:region_target[2],region_target[1]:region_target[3],num_layer] = x\n\n    return img_target\n\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntry:\n    import piq\nexcept:\n    os.system(\"pip3 install piq\")\n        \nfrom piq import psnr,ssim,multi_scale_ssim\nfrom typing import Union, Tuple\nimport time\ninpaint_iters=1800\ninpainted_images_gan = None\nimage_batch = None\ninpainted_images= None\nmasks = None\nssim_index = torch.zeros(len(masked_loader),12)\npsnr_index = torch.zeros(len(masked_loader),12)\n\nfor i,data in enumerate(masked_loader,0):\n    \n    b_size = data[0].size(0)\n    image_batch_cpu = unnormalise(data[0]) # unnormalise cpu tensor\n    image_batch = data[0].to(device)\n    masks_cpu = data[1].unsqueeze(1)\n    masks = data[1].to(device).unsqueeze(1)\n    masks_inv = torch.ones_like(masks) - masks\n    masks_inv_cpu = torch.ones_like(masks_cpu) - masks_cpu\n    \n    weighted_masks = (torch.nn.functional.conv2d(masks_inv,special_conv,padding=WINDOW_SIZE//2)*masks)/(WINDOW_SIZE*WINDOW_SIZE)\n\n    z_closest = torch.randn(b_size,noise_dim,1,1,device=device,requires_grad=True)\n    z_optimizer = torch.optim.Adam([z_closest])\n    \n    prior_criterior = nn.BCEWithLogitsLoss()\n    p_losses=[]\n    c_losses=[]\n    start_time = time.time()\n    for j in range(inpaint_iters):\n        z_optimizer.zero_grad()\n        fake_images = Gen_eval(z_closest)\n        d_output = Dis_eval(fake_images).view(-1)\n\n        prior_loss= 64*64*3*prior_loss_parameter*prior_criterior(d_output,torch.ones(b_size,device=device))\n\n        context_loss = torch.norm(weighted_masks*(fake_images - image_batch),p=1)\n        if (j % 50):\n            p_losses.append(prior_loss.detach().cpu())\n            c_losses.append(context_loss.detach().cpu())\n            \n        loss = context_loss + prior_loss\n        loss.backward()\n        z_optimizer.step()\n    inpainted_images_gan_cpu = unnormalise(Gen_eval(z_closest.detach()).cpu())\n    # returns a torch tensor of images\n    mid_time = time.time()\n    inpainted_images_cpu = poissonblending_batch(image_batch_cpu,inpainted_images_gan_cpu,masks_inv_cpu)\n    inpainted_images = inpainted_images_cpu.to(device)\n    ssim_index[i,:] = ssim(inpainted_images,unnormalise(image_batch),data_range=1.,size_average=False).cpu()\n    psnr_index[i,:] = psnr(inpainted_images_cpu,image_batch_cpu,data_range=1.,reduction='none')\n    end_time = time.time()\n    for img,gen_img,gen_poisson,mask in zip(image_batch_cpu,inpainted_images_gan_cpu,inpainted_images_cpu,masks_cpu):\n        og = transforms.ToPILImage(mode=\"RGB\")(img)\n        inp = transforms.ToPILImage(mode=\"RGB\")(img*mask + (1-mask)*gen_img)\n        inpp = transforms.ToPILImage(mode=\"RGB\")(gen_poisson)\n        ogm = transforms.ToPILImage(mode=\"RGB\")(img*mask)\n        f = plt.figure()\n        f.add_subplot(1,4, 1).set_title('Original')\n        plt.imshow(og)\n        plt.axis('off')\n        f.add_subplot(1,4, 2).set_title('Masked')\n        plt.imshow(ogm)\n        plt.axis('off')\n        f.add_subplot(1,4, 3).set_title('Overlay')\n        plt.imshow(inp)\n        plt.axis('off')\n        f.add_subplot(1,4, 4).set_title('Inpainted')\n        plt.imshow(inpp)\n        plt.axis('off')\n        plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(c_losses)\nplt.plot(p_losses)\nplt.legend([\"Context loss\",\"Prior Loss\"])\nplt.show()\nplt.plot(p_losses)\nplt.legend([\"Prior Loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ssim_index.size())\npsnr_sum = torch.mean(psnr_index,dim=0)\nfor i in range(4):\n    print(torch.mean(psnr_sum[i::4]))\nprint(ssim_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ssim_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntry:\n    import piq\nexcept:\n    os.system(\"pip3 install piq\")\n        \nfrom piq import psnr,ssim,multi_scale_ssim\nfrom typing import Union, Tuple\nimport time\ninpaint_iters=1800\ninpainted_images_gan = None\nimage_batch = None\ninpainted_images= None\nmasks = None\nssim_index = torch.zeros(len(masked_loader),12)\npsnr_index = torch.zeros(len(masked_loader),12)\n\nfor i,data in enumerate(masked_loader,0):\n    \n    b_size = data[0].size(0)\n    image_batch_cpu = unnormalise(data[0]) # unnormalise cpu tensor\n    image_batch = data[0].to(device)\n    masks_cpu = data[1].unsqueeze(1)\n    masks = data[1].to(device).unsqueeze(1)\n    masks_inv = torch.ones_like(masks) - masks\n    masks_inv_cpu = torch.ones_like(masks_cpu) - masks_cpu\n    \n    weighted_masks = (torch.nn.functional.conv2d(masks_inv,special_conv,padding=WINDOW_SIZE//2)*masks)/(WINDOW_SIZE*WINDOW_SIZE)\n\n    z_closest = torch.randn(b_size,noise_dim,1,1,device=device,requires_grad=True)\n    z_optimizer = torch.optim.Adam([z_closest])\n    \n    prior_criterior = nn.BCEWithLogitsLoss()\n    p_losses=[]\n    c_losses=[]\n    start_time = time.time()\n    for j in range(inpaint_iters):\n        z_optimizer.zero_grad()\n        fake_images = Gen_eval(z_closest)\n        d_output = Dis_eval(fake_images).view(-1)\n\n        prior_loss= 64*64*3*prior_loss_parameter*prior_criterior(d_output,torch.ones(b_size,device=device))\n\n        context_loss = torch.norm(weighted_masks*(fake_images - image_batch),p=1)\n        if (j % 50):\n            p_losses.append(prior_loss.detach().cpu())\n            c_losses.append(context_loss.detach().cpu())\n            \n        loss = context_loss + prior_loss\n        loss.backward()\n        z_optimizer.step()\n    inpainted_images_gan_cpu = unnormalise(Gen_eval(z_closest.detach()).cpu())\n    # returns a torch tensor of images\n    mid_time = time.time()\n    inpainted_images_cpu = poissonblending_batch(image_batch_cpu,inpainted_images_gan_cpu,masks_inv_cpu)\n    inpainted_images = inpainted_images_cpu.to(device)\n    ssim_index[i,:] = ssim(inpainted_images,unnormalise(image_batch),data_range=1.,size_average=False).cpu()\n    psnr_index[i,:] = psnr(inpainted_images_cpu,image_batch_cpu,data_range=1.,reduction='none')\n    end_time = time.time()\n    for img,gen_img,gen_poisson,mask in zip(image_batch_cpu,inpainted_images_gan_cpu,inpainted_images_cpu,masks_cpu):\n        og = transforms.ToPILImage(mode=\"RGB\")(img)\n        inp = transforms.ToPILImage(mode=\"RGB\")(img*mask + (1-mask)*gen_img)\n        inpp = transforms.ToPILImage(mode=\"RGB\")(gen_poisson)\n        ogm = transforms.ToPILImage(mode=\"RGB\")(img*mask)\n        f = plt.figure()\n        f.add_subplot(1,4, 1).set_title('Original')\n        plt.imshow(og)\n        plt.axis('off')\n        f.add_subplot(1,4, 2).set_title('Masked')\n        plt.imshow(ogm)\n        plt.axis('off')\n        f.add_subplot(1,4, 3).set_title('Overlay')\n        plt.imshow(inp)\n        plt.axis('off')\n        f.add_subplot(1,4, 4).set_title('Inpainted')\n        plt.imshow(inpp)\n        plt.axis('off')\n        plt.show()\n    break\n        \n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}