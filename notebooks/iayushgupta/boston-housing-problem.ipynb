{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\n\nfrom pandas import read_csv\n\nfrom matplotlib.animation import FuncAnimation\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the csv file\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nboston = read_csv('/kaggle/input/boston-house-prices/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nboston","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the features\nboston['AGE']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#performing basic functions\nprint(max(boston['MEDV']))\nprint(min(boston['MEDV']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#describing the data\nboston.describe().round(decimals = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation between every column in the data\n#using PEARSON CORRELATION\ncorr = boston.corr('pearson')\n\n#absolute value of the correlation\ncorrs = [abs(corr[attr]['MEDV']) for attr in list(boston)]\n\n#make a list of pair [(corr, feature)] using zip\nl = list(zip(corrs, list(boston)))\n\n#sorting the list pairs in reverse\n#with the correlation value as the key for sorting\nl.sort(key = lambda x : x[0], reverse=True)\n\n#'UNZIP' pairs to 2 lists\n#zip(*l) makes a list looking like ([a,b,c], [d,e,f], [g,h,i]) to ([a,d,g], [b,e,h], [c,f,i])\ncorrs, labels = list(zip((*l)))\n\n#plot correlation wrt MEDV variables as a bar graph\nindex = np.arange(len(labels))\nplt.figure(figsize=(15, 5))\nplt.bar(index, corrs, width=0.5)\nplt.xlabel('Attributes')\nplt.ylabel('Correlation wrt MEDV Variables')\nplt.xticks(index, labels)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setting the values\nX=boston['LSTAT'].values\nY=boston['MEDV'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#before normalisation\nprint(Y[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalising\nx_scaler = MinMaxScaler()\nX = x_scaler.fit_transform(X.reshape(-1, 1))\nX = X[:, -1]\ny_scaler = MinMaxScaler()\nY = y_scaler.fit_transform(Y.reshape(-1, 1))\nY = Y[:, -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after normalisation\nprint(Y[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mean Squared Error\ndef error(m, x, c, t):\n    N = x.size\n    e = sum(((m * x + c) - t) ** 2)\n    return e * 1/(2 * N)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#0.2 indicates that 20% of the data is randomly sampled as testing data\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#update Function\ndef update(m, x, c, t, learning_rate):\n    grad_m = sum(2 * ((m * x + c) - t) * x)\n    grad_c = sum(2 * ((m * x + c) - t))\n    m = m - grad_m * learning_rate\n    c = c - grad_c * learning_rate\n    return m, c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gradient Descent Function\ndef gradient_descent(init_m, init_c, x, t, learning_rate, iterations, error_threshold):\n    m = init_m\n    c = init_c\n    error_values = list()\n    mc_values = list()\n    for i in range(iterations):\n        e = error(m, x, c, t)\n        if e < error_threshold:\n            print('Error less than the threshold. Stopping Gradient Descent.')\n            break\n        error_values.append(e)\n        m, c = update(m, x, c, t, learning_rate)\n        mc_values.append((m, c))\n    return m, c, error_values, mc_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time   \n#time taken for computing the given number of iterations\n\ninit_m = 0.9\ninit_c = 0\nlearning_rate = 0.001\niterations = 250\nerror_threshold = 0.001\n\n\nm, c, error_value, mc_values = gradient_descent(init_m, init_c, xtrain, ytrain, learning_rate, iterations, error_threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as the number of iterations increase, the changes in the line is less noticeable\n#in order to reduce the processing time for the animation, it is advised to choose smaller values\nmc_values_anim = mc_values[0:250:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting a scatter plot of train dataset\nplt.scatter(xtrain, ytrain, color='b')\nplt.plot(xtrain, ((m * xtrain) + c), color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot of Error vs Iteration Curve\nplt.plot(np.arange(len(error_value)), error_value)\nplt.xlabel('Iterations')\nplt.ylabel('Error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating the prediction on the tset set as vectorized operation\npredicted = (m * xtest) + c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating MSE for the predicted values on the test dataset\nmean_squared_error(ytest, predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#putting xtest, ytest and predicted values into a single data frame so that we can see the \n#predicted values alongside the testing set\np = pd.DataFrame(list(zip(xtest, ytest, predicted)), columns =['X', 'Target Y', 'Predicted Y'])\np.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting a scatter plot of test dataset wrt predicted values\nplt.scatter(xtest, ytest, color='b')\nplt.plot(xtest, predicted, color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reshape to change the shape that is required by the scaler\npredicted = predicted.reshape(-1, 1)\nxtest = xtest.reshape(-1, 1)\nytest = ytest.reshape(-1, 1)\n\nxtest_scaled = x_scaler.inverse_transform(xtest)\nytest_scaled = y_scaler.inverse_transform(ytest)\npredicted_scaled = y_scaler.inverse_transform(predicted)\n\n\n#this is to remove extra dimensions\n\nxtest_scaled = xtest_scaled[:, -1]\nytest_scaled = ytest_scaled[:, -1]\npredicted_scaled = predicted_scaled[:, -1]\n\np = pd.DataFrame(list(zip(xtest_scaled, ytest_scaled, predicted_scaled)), columns =['X', 'Target Y', 'Predicted Y'])\np.round(decimals = 2)\np.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}