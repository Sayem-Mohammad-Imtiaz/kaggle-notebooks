{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"1. Use the BOW model on the text of the BBC dataset (apply data preprocessing) and do K-means clustering (don't use the category feature). See if the clusters correspond to the categories. (Choose K to be the number of categories)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nds = pd.read_csv(\"../input/bbc-fulltext-and-category/bbc-text.csv\")\nprint(ds.head())\nprint(\" \")\nprint(\"Category         Counts\")\nprint(ds[\"category\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Preprocessing\n\n# Droping the \"category\" column\n\nds2 = ds.drop(\"category\", axis=1)\n\n# Removing punctuation\n\nimport string\n\npunct = \"\\n\\r\"+string.punctuation\nds2[\"text\"] = ds2[\"text\"].str.translate(str.maketrans('', '', punct))\n\n# Removing stop words\n\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words(\"english\")\nds2 = ds2[\"text\"].str.lower().str.split()\nds2 = ds2.apply(lambda k: ([i for i in k if i not in stop]))\n\n# Stemming\n\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\na = []\ndef SW(txt):\n    for i2 in txt:\n        a.append(stemmer.stem(i2))\n    b = a[:]\n    a.clear()\n    return b\nds2 = ds2.apply(SW)\n\n# Joining back\n\ndef JB(il):\n    return \" \".join(il)  \nds2 = ds2.apply(JB)\n\n# Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef tokenize(str_input):\n    words = re.sub(r\"(?u)[^A-Za-z]\", \" \", str_input).lower().split(\" \")\n    words = [wordnet_lemmatizer.lemmatize(word) for word in words if len(word)>2]\n    return words\n\n# Vectorization\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(tokenizer = tokenize)\nvectors = vectorizer.fit_transform(ds2)\nfeature_names = vectorizer.get_feature_names()\nds2 = pd.DataFrame(vectors.toarray(), columns = feature_names)\nprint(ds2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the K-Means clustering algorithm\n\nfrom sklearn import cluster\n\nk_means = cluster.KMeans(n_clusters = 5, max_iter = 50, random_state = 1)\nk_means.fit(ds2) \nlabels = k_means.labels_\nds2 = pd.DataFrame(labels, index = ds.category , columns = [\"Cluster ID\"])\nprint(ds2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The clusters' correspondence to the categories\n\nprint(\"Article category: Sport\")\nprint(ds2.loc[\"sport\"].value_counts())\nprint(\" \")\nprint(\"Article category: Business\")\nprint(ds2.loc[\"business\"].value_counts())\nprint(\" \")\nprint(\"Article category: Politics\")\nprint(ds2.loc[\"politics\"].value_counts())\nprint(\" \")\nprint(\"Article category: Tech\")\nprint(ds2.loc[\"tech\"].value_counts())\nprint(\" \")\nprint(\"Article category: Entertainment\")\nprint(ds2.loc[\"entertainment\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results:\n\n1. The cluster with the lowest number of article in it is cluster 2 (190 articles). \n1. The cluster with the highest number of article in it is cluster 0 (953 articles) and it includes the highest concentration of business articles (501/510 business articles are found in this cluster).\n1. The highest concentration of sport articles is found in cluster 1 (489/511 articles).\n1. The highest concentrations of politics articles are found in clusters 0 (199/417 articles) and 3 (213/417 articles).\n1. The highest concentration of tech articles is found in cluster 4 (350/401 articles).\n1. The highest concentrations of entertainment articles are found in clusters 0 (194/386 articles) and 2 (184/386 articles)."},{"metadata":{},"cell_type":"markdown","source":"2. Apply K-means clustering to the customer segmentation dataset. See what does the clusters correspond to."},{"metadata":{"trusted":true},"cell_type":"code","source":"dss = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\nprint(dss.head())\nprint(\" \")\n\n# Obtaining information about the dataset to determine which features are important for the clustering analysis\n\nprint(\"Dataset Information: \")\nprint(dss.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping the unneeded columns\n\ndss = dss.drop([\"CustomerID\", \"Gender\"], axis=1)\n\n# Performing the SSE test to determine the best number of clusters\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnumClusters = [1,2,3,4,5,6]\nSSE = []\nfor k in numClusters:\n    k_means = cluster.KMeans(n_clusters=k)\n    k_means.fit(dss)\n    SSE.append(k_means.inertia_)\nplt.plot(numClusters, SSE)\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Means clustering\n\nk_means = cluster.KMeans(n_clusters = 5, max_iter = 50, random_state = 1)\nk_means.fit(dss) \nlabels = k_means.labels_\npd.DataFrame(labels, columns = [\"Cluster ID\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the results\n\nimport seaborn as sns\n\ndss[\"Labels\"] = k_means.labels_\nf = plt.figure(figsize = (24,12))\nax1 = f.add_subplot(221)\nsns.swarmplot(x = \"Labels\", y = \"Age\", hue = dss[\"Labels\"], data = dss, ax = ax1)\nax1.set_title(\"Age Labels\")\nax2 = f.add_subplot(222)\nsns.swarmplot(x = \"Labels\", y = \"Annual Income (k$)\", hue = dss[\"Labels\"], data = dss, ax = ax2)\nax2.set_title(\"Annual Income Labels\")\nax3 = f.add_subplot(223)\nsns.swarmplot(x = \"Labels\", y = \"Spending Score (1-100)\", hue = dss[\"Labels\"], data = dss, ax = ax3)\nax3.set_title(\"Scoring History Labels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results:\n\nFrom the plots above, it can be seen that the role of age is not significant in the clustering analysis (or as significant as the other features), while the annual income and scoring history do have a significant role as the clusters correspond to them, which can be seen in the plots above. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}