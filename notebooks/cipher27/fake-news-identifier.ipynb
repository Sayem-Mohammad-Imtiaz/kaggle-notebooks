{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames: \n        if filename == 'True.csv':\n            true_news = pd.read_csv(os.path.join(dirname, filename))\n        else:\n            fake_news = pd.read_csv(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' --- Real News --- ')\nprint(true_news.head(5))\nprint(' --- Fake News --- ')\nprint(fake_news.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's indicate true news as 1 and fake news as 0\ntrue_news['truth'] = len(true_news) * [1]\nfake_news['truth'] = len(fake_news) * [0]\nprint(true_news.head(3))\nprint(fake_news.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's concatenate the two and prepare to shuffle up the data\nall_news = pd.concat([true_news, fake_news], ignore_index=True) # ignore index to reset the indices\n\n# Shuffle\nall_news = all_news.sample(frac=1).reset_index(drop=True)\n\nprint(all_news.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's only keep title, text, and truth for now\nall_news = all_news.drop(['date'], axis=1)\nall_news = all_news.drop(['subject'], axis=1)\nprint(all_news.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this case, let's tokenize and then split\n# First, let's find how big the vocabulary is without truncating\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_news['text']) # tokenizing just on text should be fine\nword_index = tokenizer.word_index\nprint(len(word_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 138021 in total\ndel(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Great, now let's do two splits on this dataset: one between train/validation and test,\n# and another between train and validation\ntest_breakpoint = int(0.8 * len(all_news))\ntrain_val_data = all_news[:test_breakpoint]\ntest_data = all_news[test_breakpoint:]\n\nval_breakpoint = int(0.8 * len(train_val_data))\ntrain_data = train_val_data[:val_breakpoint]\nval_data = train_val_data[val_breakpoint:]\n\nprint(f'Length of training data: {len(train_data)}')\nprint(f'Length of validation data: {len(val_data)}')\nprint(f'Length of test data: {len(test_data)}')\nprint(f'Length of all news: {len(all_news)}')\n# Numbers add up!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's tokenize\nvocab_size = 50000\nembedding_dim = 64\nmax_length = 150\nmax_length_title = 50\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\n\n# Separate train data\ntrain_text = train_data['text']\ntrain_titles = train_data['title']\ntrain_truth = train_data['truth']\n\n# Separate validation data\nval_text = val_data['text']\nval_titles = val_data['title']\nval_truth = val_data['truth']\n\n# Separate test data\ntest_text = test_data['text']\ntest_titles = test_data['title']\ntest_truth = test_data['truth']\n\n# Get the vocabulary\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\ntokenizer.fit_on_texts(train_text)\nword_index = tokenizer.word_index\n\n# Tokenize the train texts\ntrain_text_sequences = tokenizer.texts_to_sequences(train_text)\ntrain_text_padded = np.array(pad_sequences(train_text_sequences, maxlen = max_length, \\\n                               padding=padding_type, truncating = trunc_type))\n\n# Tokenize the train titles, with lower max length\ntrain_titles_sequences = tokenizer.texts_to_sequences(train_titles)\ntrain_titles_padded = np.array(pad_sequences(train_text_sequences, maxlen = max_length_title, \\\n                               padding=padding_type, truncating = trunc_type))\nprint(train_text[:2])\nprint(train_text_padded[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize validation texts and titles\nval_text_sequences = tokenizer.texts_to_sequences(val_text)\nval_text_padded = np.array(pad_sequences(val_text_sequences, maxlen = max_length, \\\n                               padding=padding_type, truncating = trunc_type))\n\nval_titles_sequences = tokenizer.texts_to_sequences(val_titles)\nval_titles_padded = np.array(pad_sequences(val_titles_sequences, maxlen = max_length_title, \\\n                               padding=padding_type, truncating = trunc_type))\n\n# Tokenize test texts and titles\ntest_text_sequences = tokenizer.texts_to_sequences(test_text)\ntest_text_padded = np.array(pad_sequences(test_text_sequences, maxlen = max_length, \\\n                               padding=padding_type, truncating = trunc_type))\n\ntest_titles_sequences = tokenizer.texts_to_sequences(test_titles)\ntest_titles_padded = np.array(pad_sequences(test_text_sequences, maxlen = max_length_title, \\\n                               padding=padding_type, truncating = trunc_type))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val_titles_padded[0])\nprint(val_text_padded[0])\nprint(test_titles_padded[0])\nprint(test_text_padded[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We're all set! Let's make the model\nimport tensorflow.keras.layers as layers\nmodel = tf.keras.Sequential([\n    layers.Embedding(vocab_size, embedding_dim, input_length=max_length), # hyperparameters set above\n    #layers.GlobalAveragePooling1D(),\n    #layers.Dropout(0.5),\n    layers.Bidirectional(layers.LSTM(32, activation='relu')),\n    #layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's prepare the data and train\ntrain_data = (train_text_padded, train_titles_padded)\nval_data = (val_text_padded, val_titles_padded)\nnum_epochs = 10\nhistory = model.fit(train_data, train_truth, epochs=num_epochs, validation_data=(val_data, val_truth), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Some nice plots!\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(titles, texts, tokenizer=tokenizer, max_length=max_length, max_length_title=max_length_title, trunc_type=trunc_type, padding=padding_type):\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    text_padded = pad_sequences(text_sequences, maxlen = max_length, \\\n                               padding=padding, truncating = trunc_type)\n    titles_sequences = tokenizer.texts_to_sequences(titles)\n    titles_padded = pad_sequences(titles_sequences, maxlen = max_length_title, \\\n                               padding=padding, truncating = trunc_type)\n    predictions = model.predict((titles_padded, text_padded))\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(predictions, actual):\n    assert len(predictions) == len(actual), \"To compute accuracy, arrays must be same size\"\n    predictions = np.array(predictions)\n    actual = np.array(actual)\n    total_correct = 0.0\n    for i in range(len(predictions)):\n        if round(float(predictions[i])) == actual[i]:\n            total_correct += 1\n    return total_correct / len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predict(test_titles, test_text)\nprint(accuracy(predictions, test_truth))\n# nearly 80%, not too bad!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I used a Kaggle dataset for this project, you can find it at\n# https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}