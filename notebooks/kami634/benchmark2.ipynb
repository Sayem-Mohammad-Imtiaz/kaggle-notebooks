{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 転移学習のベースライン\n参考： https://www.kaggle.com/khyeh0719/pytorch-efficientnet-baseline-train-amp-aug/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# timm: 事前学習済みモデルの使用のためのライブラリ\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master') # パスを通す\nimport timm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Available Vision Transformer Models: \")\ntimm.list_models(\"vit*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport datetime\nimport random\nimport time\nfrom tqdm import tqdm\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import  log_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 設定まとめ config\nCFG = {\n    \"fold_num\": 5,\n    \"seed\": 6, # シード値\n    \"model_arch\": \"vit_small_patch16_224\", # 使用するモデル\n    \"img_size_h\": 224, # 画像の高さ\n    \"img_size_w\": 224,\n    \"epochs\": 10,\n    \"train_bs\": 64, # bs: バッチサイズ\n    \"valid_bs\": 64,\n    \"T_0\": 5,\n    \"lr\": 1e-4,\n    \"min_lr\": 1e-6, # lr: learning rate\n    \"weight_decay\": 1e-4,\n    \"num_workers\": 6,\n    \"accum_iter\": 5, # 学習をまとめて行うことでバッチサイズが大きくなるような効果\n    \"verbose_step\": 1,\n    \"device\": \"cuda:0\",\n    \"tta\": 5,  # test time augmentation の回数\n    \"used_epochs\": [\n        5,6,7,\n    ],\n    \"weights\": [\n        0.5,1,1\n        \n    ]\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存用ディレクトリの作成\nos.makedirs(\"output\", exist_ok=True)\nos.makedirs(\"save\", exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train と test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train 用 df の作成\ntrain_df = pd.DataFrame()\nbase_train_data_path = '../input/flowers-recognition/train/'\n\ntrain_data_labels = ['daisy',\n                    'dandelion',\n                    'rose',\n                    'sunflower',\n                    'tulip'\n                   ]\n\nfor one_label in train_data_labels:\n    one_label_df = pd.DataFrame()\n    one_label_paths = os.path.join(base_train_data_path, one_label)\n    one_label_df['image_path'] = [os.path.join(one_label_paths, f) for f in os.listdir(one_label_paths)]\n    one_label_df['label'] = one_label\n    train_df = pd.concat([train_df, one_label_df])\ntrain_df = train_df.reset_index(drop=True)\nprint(train_df.shape)\ndisplay(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train の label を数字にエンコードする\n\nlabel_dic = {\"daisy\":0, \"dandelion\":1, \"rose\":2,\"sunflower\":3, \"tulip\":4}\ntrain_df[\"label\"]=train_df[\"label\"].map(label_dic)\ndisplay(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test 用 df の作成\ntest_df = pd.DataFrame()\nbase_test_data_path = '../input/flowers-recognition/test/'\ntest_df['image_path'] = [os.path.join(base_test_data_path, f) for f in os.listdir('../input/flowers-recognition/test/')]\ntest_df = test_df.sort_values('image_path').reset_index(drop=True)\n\ndisplay(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 便利関数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    \"seed値を一括指定\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef get_img(path):\n    \"\"\"\n    pathからimageの配列を得る\n    \"\"\"\n    im_bgr = cv2.imread(path)\n    if im_bgr is None:\n        print(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データセットクラス"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FlowerDataset(Dataset):\n    def __init__(self, df, \n                 shape, # 追加\n                 transforms=None,\n                 output_label=True,\n                 one_hot_label=False,\n                 image_name_col = \"image_path\",\n                 label_col = \"label\"\n                ):\n\n        super().__init__()\n        self.shape = shape\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.output_label = output_label\n        self.one_hot_label = one_hot_label\n        self.image_name_col = image_name_col\n        self.label_col = label_col\n\n        if output_label == True:\n            self.labels = self.df[self.label_col].values\n            if one_hot_label is True:\n                self.labels = np.eye(self.df[self.label_col].max()+1)[self.labels]\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index: int):\n\n        if self.output_label:\n            target = self.labels[index]\n\n        img  = get_img(self.df.loc[index][self.image_name_col])\n\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n\n        if self.output_label == True:\n            return img, target\n        else:\n            return img\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 画像のスケーリング&オーグメンテーション"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom albumentations import (\n    PadIfNeeded, HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize,ToGray\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms(input_shape):\n    return Compose([\n            Resize(input_shape[0], input_shape[1]),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(scale_limit=0.0, p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),                \n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            Cutout(num_holes=20,p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_valid_transforms(input_shape):\n    return Compose([\n                Resize(input_shape[0], input_shape[1]),\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n                ToTensorV2(p=1.0),\n            ], p=1.)\n\ndef get_inference_transforms(input_shape):\n    return Compose([\n                Resize(input_shape[0], input_shape[1]),\n                HorizontalFlip(p=0.5),\n                VerticalFlip(p=0.5),\n                ShiftScaleRotate(scale_limit=0.0, p=0.5),\n                HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n                RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n                Cutout(p=0.5),\n                ToTensorV2(p=1.0),\n            ], p=1.)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データローダー作成"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataloader(df, input_shape, trn_idx, val_idx, train_bs, valid_bs, num_workers):\n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n\n    train_ds = FlowerDataset(train_, input_shape, transforms=get_train_transforms(input_shape), output_label=True, one_hot_label=False)\n    valid_ds = FlowerDataset(valid_, input_shape, transforms=get_valid_transforms(input_shape), output_label=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=train_bs,\n        pin_memory=True, # faster and use memory\n        drop_last=False,\n        shuffle=True,\n        num_workers=num_workers,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds,\n        batch_size=valid_bs,\n        num_workers=num_workers,\n        shuffle=False,\n        pin_memory=False,\n    )\n    return train_loader, val_loader\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FlowerImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, n_class)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\nclass MyCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean'):\n        super().__init__(weight=weight, reduction=reduction)\n        self.weight = weight\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        lsm = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            lsm = lsm * self.weight.unsqueeze(0)\n\n        loss = -(targets * lsm).sum(-1)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習用&推論用関数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, accum_iter, verbose_step, scheduler=None, schd_batch_update=False):\n    model.train()\n    scaler = GradScaler()\n\n    t = time.time()\n    running_loss = None\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        with autocast():\n            image_preds = model(imgs)\n            loss = loss_fn(image_preds, image_labels)\n\n            scaler.scale(loss).backward()\n\n            if running_loss is None:\n                running_loss = loss.item()\n            else:\n                running_loss = running_loss * .99 + loss.item() * .01\n\n            if ((step + 1) %  accum_iter == 0) or ((step + 1) == len(train_loader)):\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n                if scheduler is not None and schd_batch_update:\n                    scheduler.step()\n\n            if ((step + 1) % verbose_step == 0) or ((step + 1) == len(train_loader)):\n                description = f'epoch {epoch} loss: {running_loss:.4f}'\n                pbar.set_description(description)\n\n    print(\"train: \"+ description)\n    if scheduler is not None and not schd_batch_update:\n        scheduler.step()\n\ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, accum_iter, verbose_step, scheduler=None, schd_loss_update=False):\n    model.eval()\n\n    t = time.time()\n    loss_sum = 0\n    sample_num = 0\n    image_preds_all = []\n    image_targets_all = []\n\n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        image_preds = model(imgs)   \n        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n        image_targets_all += [image_labels.detach().cpu().numpy()]\n\n        loss = loss_fn(image_preds, image_labels)\n\n        loss_sum += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]\n\n        if ((step + 1) % verbose_step== 0) or ((step + 1) == len(val_loader)):\n            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n            pbar.set_description(description)\n\n    print(\"valid \"+ description)\n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n\n\n    if scheduler is not None:\n        if schd_loss_update:\n            scheduler.step(loss_sum/sample_num)\n        else:\n            scheduler.step()\n\n\ndef inference_one_epoch(model, data_loader, device):\n    model.eval()\n    image_preds_all = []\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n\n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n\n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    train = train_df\n    seed_everything(CFG['seed'])\n\n    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        \"\"\"\n        \"\"\"\n        \n        if fold > 0:\n            break\n        print(f'Training with fold {fold} started (train:{len(trn_idx)}, val:{len(val_idx)})')\n\n        train_loader, val_loader = prepare_dataloader(train, (CFG[\"img_size_h\"], CFG[\"img_size_w\"]), trn_idx, val_idx, train_bs=CFG[\"train_bs\"], valid_bs=CFG[\"valid_bs\"], num_workers=CFG[\"num_workers\"] )\n\n        device = torch.device(CFG['device'])\n\n        model = FlowerImgClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n\n        loss_tr = nn.CrossEntropyLoss().to(device)\n        loss_fn = nn.CrossEntropyLoss().to(device)\n\n        for epoch in range(CFG['epochs']):\n            train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, CFG['accum_iter'], CFG['verbose_step'],scheduler=scheduler, schd_batch_update=False)\n\n            with torch.no_grad():\n                valid_one_epoch(epoch, model, loss_fn, val_loader, device, CFG['accum_iter'], CFG['verbose_step'], scheduler=None, schd_loss_update=False)\n\n            torch.save(model.state_dict(),f'save/{CFG[\"model_arch\"]}_fold_{fold}_{epoch}')\n\n        del model, optimizer, train_loader, val_loader,  scheduler\n        torch.cuda.empty_cache()\n        print(\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 推論&提出"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef infer():\n    print(\"pred start\")\n    train = train_df\n    seed_everything(CFG['seed'])\n\n    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n\n\n    tst_preds = []\n    val_loss = []\n    val_acc = []\n\n    # 行数を揃えた空のデータフレームを作成\n    cols = ['daisy',\n            'dandelion',\n            'rose',\n            'sunflower',\n            'tulip'\n           ]\n\n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        \"\"\"\n        \"\"\"\n        if fold > 0:\n            break\n        print(' fold {} started'.format(fold))\n        input_shape=(CFG[\"img_size_h\"], CFG[\"img_size_w\"])\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = FlowerDataset(valid_, transforms=get_inference_transforms(input_shape), shape = input_shape, output_label=False)\n\n        test_ds = FlowerDataset(test_df, transforms=get_inference_transforms(input_shape), shape=input_shape, output_label=False)\n\n\n        val_loader = torch.utils.data.DataLoader(\n            valid_ds,\n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        tst_loader = torch.utils.data.DataLoader(\n            test_ds,\n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device(CFG['device'])\n        model = FlowerImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n\n        val_preds = []\n\n        #for epoch in range(CFG['epochs']-3):\n        for i, epoch in enumerate(CFG['used_epochs']):\n            model.load_state_dict(torch.load(f'save/{CFG[\"model_arch\"]}_fold_{fold}_{epoch}'))\n\n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    val_preds += [CFG['weights'][i]/sum(CFG['weights'])*inference_one_epoch(model, val_loader, device)]\n                    tst_preds += [CFG['weights'][i]/sum(CFG['weights'])*inference_one_epoch(model, tst_loader, device)]\n\n        val_preds = np.mean(val_preds, axis=0)\n        val_loss.append(log_loss(valid_.label.values, val_preds))\n        val_acc.append((valid_.label.values == np.argmax(val_preds, axis=1)).mean())\n\n    print('validation loss = {:.5f}'.format(np.mean(val_loss)))\n    print('validation accuracy = {:.5f}'.format(np.mean(val_acc)))\n    tst_preds = np.mean(tst_preds, axis=0)\n\n    del model\n    torch.cuda.empty_cache()\n    return np.argmax(tst_preds, axis=1)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tst_preds_label_all = infer()\nprint(tst_preds_label_all.shape)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 予測結果を保存\nsub = pd.read_csv(\"../input/flowers-recognition/sample_submission.csv\")\nsub['class'] = tst_preds_label_all\nlabel_dic = {0:\"daisy\", 1:\"dandelion\", 2:\"rose\",3:\"sunflower\", 4:\"tulip\"}\nsub[\"class\"] = sub[\"class\"].map(label_dic)\nprint(sub.value_counts(\"class\"))\nsub.to_csv(f'output/submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}