{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Will you get a job?\n  \nThe data set consists of MBA students' basic demographic variables, past grades before undergradute degree, whether they get a job after graduation and the salary.   \n\nTo understand which and how factors affect the recruitment, first I would conduct EDA, then build a model to figure out the impact of each variable and to make prediction. All the analysis is as follows."},{"metadata":{},"cell_type":"markdown","source":"# Just Guess\n\n*Before we start, I want to make some guess about this data set.  After that, we will check with the data.*\n* **Better Grades Better:**  \nCompany can't be less interested in candidates with better grades than those with lower grades. The better the grades are, the more posible you'll get a job. \n  \n* **The Recents Matter:**  \nThe more recent grades should be more representative of the candidates' ability at that time, as a result, grades in MBA might affect the chance of getting a job more than grades in higher secondary education or secondary education do.   \n  \n* **Popular Central Board of Education Is More Competetive:**  \nThere are some different boards of education in India, and central board is the most popular one. We believe that it is more competetive because most students want to go to the schools belonging to it, thus, the students from central board might have higher chance of getting a job.  \n  \n* **Gender Equality?**  \nLiteracy and educated ratio of female to male in India is relative low. Because the data was collected from MBA students, we could guess that number of female would be lower than male.  \nHowever, for females with such high education level, it might not necessarily be more difficult to get a job.  \n\n"},{"metadata":{},"cell_type":"markdown","source":"# A. Exploratory Data Analysis  \n  \nWe would quickly figure out that \n* The dimension of the data.\n* What variables are there in this dataset.\n* Any irrational value or missing value there.\n* The distribution of every variable."},{"metadata":{},"cell_type":"markdown","source":"## A-1. Generally  \n\nQuickly see the dimension and columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nfrom scipy import stats\nfrom statsmodels.stats import anova\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nwd = './'\nmydat = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\nprint(type(mydat))\n\n# 維度\nprint(mydat.shape)\nprint(len(mydat))\ncols = (mydat.columns)\nprint(cols.tolist())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then check if there is any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# types of columns\nimport numpy as np\n\nis_numeric = mydat.dtypes != 'object'\nnumeric_cols = mydat.columns[is_numeric]\n\n\n# missing values\nmissing_cols = mydat.columns[np.where(mydat.isna())[1]]\nprint('Missing columns:\\n', np.unique(missing_cols),'\\n')\nprint('Missing percentage:\\n', missing_cols.value_counts() / len(mydat),'\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems that there's only salary column which has missing values. Let's go deeper."},{"metadata":{"trusted":true},"cell_type":"code","source":"### salary missing deeper explore\nnot_placed = np.where(mydat['status']=='Not Placed')[0]\nsalary_missing = np.where(mydat['salary'].isna())[0]\n\nprint('Is the data points which has missing salary are also the \"not placed\" points:\\n', (not_placed == salary_missing).all(),'\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The missing salary is all due to not getting a job. It's quite rational.  \n  \nNext, let's take a look at the numeric variables."},{"metadata":{},"cell_type":"markdown","source":"## A-2. Numeric Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# numeric distribution\n### distribution\npd.set_option(\"display.max_columns\", 15)\nprint('Summary of numeric variables:\\n',mydat.describe(include=[np.number]),'\\n')\n\n### outlier detect\ndef inRangeCheck(x, left, right): # x: series, left: int, right: int; outlier: list\n    #outlier = [x[i] for i in range(len(x)) if x[i]<left] + [x[i] for i in range(len(x)) if x[i]>right]\n    outlier = pd.concat([x[x<left], x[x>right]]).sort_values()\n    return outlier\n\ndef outlierDetect(x): # x: series; outlier: list\n    x_series = pd.Series(data=x)\n    q1 = x_series.quantile(.25)\n    q3 = x_series.quantile(.75)\n    iqr = q3 - q1\n    outlier = pd.concat([x_series[x_series<(q1-1.5*iqr)], x_series[x_series>(q1+1.5*iqr)]]).sort_values()\n    return outlier\n\n\ncols = [nc for nc in numeric_cols if nc not in ['sl_no','salary']]\nout_range = mydat.loc[:,cols].apply(inRangeCheck, left=0, right=100)\nout_salary = inRangeCheck(mydat.loc[:,'salary'], left=0,right=float('inf'))\nprint('Out of range:\\n', out_range, '\\n')\nprint('Negative salary:\\n ', out_salary, '\\n')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that the variables presenting as percentage format are all in the reasonable range, (1,100).  \n\nBesides, we checked if there's any negative value in salary, and it seemed very clean. "},{"metadata":{},"cell_type":"markdown","source":"Next, we checked the outliers in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [nc for nc in numeric_cols if nc not in ['sl_no']]\noutlier_normal = []\nfor col in cols:\n    temp = outlierDetect(mydat.loc[:, col])\n    outlier_normal.append(temp)\n\noutlier_len = {}\nfor i in range(len(outlier_normal)):\n    outlier_len.update({outlier_normal[i].name :len(outlier_normal[i])})\n\nprint(\"outlier_len: \\n\", outlier_len, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, there are some outliers in every columns detected by IQR method, but according to the exploration I've done, the data is quite clean, I still keep these detected outliers in the data set.\n"},{"metadata":{},"cell_type":"markdown","source":"Also, the correlation coefficient shows that the grade in MBA and exployment test have certain correlation with the salary if you get a job."},{"metadata":{"trusted":true},"cell_type":"code","source":"### correlation\ncorr = mydat.loc[:,numeric_cols].corrwith(mydat['salary'])\nprint('Correlation with salary:\\n',corr,'\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A-3. Categorical Variables  \n  \nFirst, take a look at the distribution of each categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical distribution\ncategorical_cols = mydat.columns[mydat.dtypes == 'object']\n\nfrequency = {}\ni = 0\nfor cc in categorical_cols:\n    if i == 0:\n        print('Count Values:')\n    temp = mydat[cc].value_counts()\n    frequency.update({cc: temp})\n    print(temp,'\\n')\n    i += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Then we try to plot the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n## Single variable distribution check\n### Numeric Variables\nplot_numeric_cols = [nc for nc in numeric_cols if nc not in ['sl_no']]\nn_row = 3\nn_col = 2\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(8,7))\nplt.subplots_adjust(hspace=0.7)\ncount=0\nfor i in range(n_row):\n    for j in range(n_col):\n        sns.distplot(mydat[plot_numeric_cols[count]],\n            ax=axes[i,j])\n        axes[i,j].set_title(plot_numeric_cols[count], fontsize=15)\n        count+=1\n\nplt.show()\n\n#### box plot\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(8,7))\nplt.subplots_adjust(hspace=0.7)\n\ncount=0\nfor i in range(n_row):\n    for j in range(n_col):\n        sns.boxplot(mydat[plot_numeric_cols[count]],\n            ax=axes[i,j])\n        axes[i,j].set_title(plot_numeric_cols[count], fontsize=15)\n        count+=1\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From density plots, it seems to us that most variables look like normal, but we suggest to further check the normility assumtion of \"etest_p\" to \"status\". (We don't check \"salary\" because we will not choose it as the predictor or the dependent varible.)  \n  \nThe boxplot show some outliers of hsc_p. It is the percentage of high scholl grades. The only restriction for it is that the value must be from 0 to 100. Since all samples follow the rule, and we can't really come up with any other solid reason to explain how the outliers come, we would keep these data to further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Categorical Variables\nn_row = 4\nn_col = 2\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(10,7))\nplt.subplots_adjust(hspace=0.7, wspace=0.3)\ncount=0\nfor i in range(n_row):\n    for j in range(n_col):\n            sns.barplot(x=frequency[categorical_cols[count]].values, y=frequency[categorical_cols[count]].index,\n                ax=axes[i,j])\n            axes[i,j].set_title(categorical_cols[count], fontsize=15)\n            count+=1\n\n#plt.savefig(wd + '/categorical_eda1.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the relationship between categorical predictors and job status."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Which variable is associated with the \"status\" variable\n##### Numeric variables\nplot_numeric_cols = [nc for nc in numeric_cols if nc not in ['sl_no']]\n\nfig, axes = plt.subplots(nrows=len(plot_numeric_cols), ncols=1, figsize=(7,10))\nfor i in range(len(plot_numeric_cols)):\n    if(plot_numeric_cols[i]!='salary'):\n        sns.violinplot(data=mydat, x=plot_numeric_cols[i], y='status', #hue='status',\n            cut=0, order=['Placed','Not Placed'], scale='count', bw=.3, orient='h',\n            ax=axes[i]) # 指定畫在哪個subplots\n    else:\n        sns.violinplot(data=mydat, x=plot_numeric_cols[i], y='status', hue='status',\n            cut=0, order=['Placed','Not Placed'], scale='count', bw=.3, orient='h',\n            ax=axes[i])\n    axes[i].set_ylabel(plot_numeric_cols[i], rotation=0, fontsize=15, labelpad=27) # ax.set_ylabel\n    axes[i].set_yticks(ticks=[])\n    axes[i].set_xlabel('')\n\naxes[0].set_title('EDA of Numeric Variables', fontsize=20)\n\n#plt.savefig(wd + '/numeric_eda.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### catgorical variables\nplot_categorical_cols = categorical_cols[categorical_cols != 'status']\n#cross_dat = pd.crosstab(index=mydat['status'], columns=[mydat['workex']])\n\nn_row = 4\nn_col = 2\nfix, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(10,7))\nplt.subplots_adjust(hspace=0.7, wspace=0.3)\nfor i in range(n_row):\n    end = False\n    for j in range(n_col):\n        ind = i*2+j\n        if ind>(len(plot_categorical_cols)-1):\n            end = True\n            break\n        col = plot_categorical_cols[ind]\n        count_table = mydat.groupby(['status',col]).size().reset_index(name='counts')\n        count_table['total'] = count_table['counts'].groupby(count_table[col]).transform('sum')\n        count_table['proportion'] = (count_table['counts']/count_table['total']*100).round(2)\n\n        sns.barplot(data=count_table, x=col, y=\"proportion\", hue=\"status\", ax=axes[i,j])\n        ys = count_table['proportion'].groupby(count_table[col]).max()\n        values = count_table.loc[:,[col,'total']].drop_duplicates(subset=col).set_index(col)['total']\n\n        for x, y, value in zip(range(len(ys)), ys.values, values.values):\n            axes[i,j].text(x=x, y=y, s=str(value), fontsize=12, horizontalalignment='center')\n\n        axes[i,j].set_title(col, fontsize=14)\n        axes[i,j].set_ylabel('')\n        axes[i,j].set_xlabel('')\n\n    if end:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Brief Summary:*\n* The grades variables are quite clean.\n* We didn't remove any outlier because we believe it's reasonable enough.\n* Surprisingly, it seems that the grades of college and below are more correlated with the chance of getting a job, rather than the most recent grades of MBA and the employment test.  \n* Students from central board didn't get a job with higher percentage.\n* Males' ratio of getting a job to not is indeed higher than females'.\n* The ratio of those who have work experience before entering MBA and get hired is more than which of those who haven't worked before MBA and get hired.  \n"},{"metadata":{},"cell_type":"markdown","source":"# B. Select Predictors  \nHaving seen the distribution plot, we still want to know which variables are correlated with the probablity of getting a job through statistical tests.  \n* For categorical variables, we choose chi-square independence test\n* For those numeric, we would apply t-test ***after making sure they follow the assumptions***. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test of different mean of every numeric variable\n# #Before hypothesis test we first do data transfomation and normality test\n# #salary, etest_p seem to be more likely to be transform to normality\n# #Seems etest_p is a little right skrew\n\nlm_model = ols('etest_p~status', data=mydat).fit()\nsns.distplot(lm_model.resid)\nplt.show()\n \nmydat['etest_p_trans0'] = mydat['etest_p'] ** .1 #Make it less right-skrewed\nlm_model = ols('etest_p_trans0~status', data=mydat).fit()\nsns.distplot(lm_model.resid)\nplt.show()\n\nmydat['etest_p_trans'] = (mydat['etest_p_trans0']-min(mydat['etest_p_trans0']))*100/(max(mydat['etest_p_trans0'])-min(mydat['etest_p_trans0']))\n\nmydat.to_csv(wd + '/Placement_Data_Transformed.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After power transformaion, the residual density plot of 'etest_p' seemed to be less right-skrewed.  \n  \nThen we're going to do hypothesis test.  \n### **Null hypothesis:**  \n***Numeric variables:***  \nThe means of the value of certain variable are the same in \"Placed\" group & \"Not Placed\" group.\n\n***Categorical variables:***  \nThe certain variable is independent to the \"status\" variable.  \n\nIf the null hypothesis is rejected, it means that the value of certain variable would be different when the employment status change. which says: **The variable has correlation with employment status.** Then we could consider to put the variable in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_numeric_cols = plot_numeric_cols\ntest_numeric_cols = ['etest_p_trans' if col == 'etest_p' else col for col in test_numeric_cols]\ntest_numeric_cols.remove('salary')\n\nt_test = pd.DataFrame(columns=['coef','se','tvalue','pvalue'])\nfor col in test_numeric_cols:\n    X=np.array(mydat['status'].map({'Placed':0, 'Not Placed':1}))\n    X = sm.add_constant(X)\n    Y = np.array(mydat[col])\n    temp = sm.OLS(Y,X).fit()\n\n    df_temp = pd.DataFrame({'coef':[temp.params[1]],\n        'se':[temp.bse[1]],\n        'tvalue':[temp.tvalues[1]],\n        'pvalue':[temp.pvalues[1]]})\n    t_test = t_test.append(df_temp, ignore_index=True)\n\nt_test.index = test_numeric_cols\nt_test = t_test.apply(lambda x: round(x,2) , axis=0)\nprint(t_test)\n\n# chi-square independence test\noutcome = 'status'\ncols = categorical_cols[categorical_cols!='status']\nchiind_test = pd.DataFrame(columns=['chi2','pvalue'])\nfor i in range(len(cols)):\n    col = cols[i]\n    contingency_table = pd.crosstab(index=mydat[outcome], columns=mydat[col])\n    chi2, p, dof, expected = stats.chi2_contingency(contingency_table, correction=False)\n    chiind_test = chiind_test.append(pd.DataFrame({'chi2':[chi2], 'pvalue':[p]}))\n\nchiind_test.index = cols\nchiind_test = chiind_test.apply(lambda x: round(x,2), axis=0)\nprint(chiind_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the result of statistical tests, **ssc_p, hsc_p, degree_p, workex** and **specialisation** are the 5 variables that has significant difference in 2 different status groups. Thus, they would be the considered predictors of the model.\n  \nConsidering the plots we've made before, it seemed to us that although without significant difference, the **gender** and **grades of exployment test (etest_p)** still have some correlation with status, as a result, these two would be considered in the model.  \n"},{"metadata":{},"cell_type":"markdown","source":"# C. Building model  \nHere, we would like to build a model to explain the effect of each variable and to predict the one's probability of getting a job. There is 4 main parts in this section.  \n\n1. Data preparing\n1. Model selection\n1. Select polynomial features\n1. Hyperparameter tuning  "},{"metadata":{},"cell_type":"markdown","source":"## C-1. Data Preparing:  \n* Data would be split as training & testing sets with ratio of 4:1.  \n* Categorical variables would be one-hot coded, moreover, scale of numerical ones are all from 0 to 100.\n* Note that \"Not Placed\" status is set to be positive event."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nfrom joblib import dump, load\n\n\nwd = os.path.abspath(os.getcwd())\nmydat = pd.read_csv(wd + '/Placement_Data_Transformed.csv')\n\n\"\"\"\n1. Data Preparing\n\"\"\"\nfrom sklearn import model_selection\ncategorical_predictor = ['gender', 'specialisation', 'workex']\nnumeric_predictor = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p_trans']\nresponse = 'status'\n\nX_dummy = pd.get_dummies(mydat.loc[:, categorical_predictor], drop_first=True)\nX = pd.concat([X_dummy, mydat.loc[:, numeric_predictor]], axis=1)\nY = mydat.loc[:, response].map({'Placed':0, 'Not Placed':1})\ntrain_X, test_X, train_Y, test_Y = model_selection.train_test_split(X, Y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C-2. Model Selection  \n* In the section the following models would be compared:  \n\n  1. Logistic regression\n  2. Support Vector Machine\n  3. Naive Bayes\n  4. Decision Tree  \n  \n  Note that we tend to choose relative simple models because we hope the model would be explained.\n  \n  \n* Hyperparameters are obtained by 10-fold CV being the one with the best **accuracy** using **balanced weighted** scoring metrics. The reason we choose balanced weighted is the imbalance of the dependent variable despite it is not too much. \n\n* Select models based on metrics including:  \n  \n    1. ROC AUC\n    2. F1 score\n    3. PR curve\n    4. Sensitivity\n    5. Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n2. Model Selection\n\n- Compare models including\n    1. Logistic regression\n    2. Support Vector Machine\n    3. Naive Bayes\n    4. Decision Tree\n\n- Hyperparameters are obtained by 10-fold CV being the one\n    * with the best 'accuracy' using 'balanced' scoring metrics *\n\n- Select models based on metrics including\n    1. ROC AUC\n    2. F1 score\n    3. PR curve\n    4. Sensitivity\n    5. Accuracy\n\n\"\"\"\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn import naive_bayes\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\n\nn_cv = 10\nn_Cs = 10\nrandom_state = 101\nmax_iter = 300\nscoring = 'accuracy'\nclass_weight = 'balanced'\ncv_splitter = model_selection.KFold(n_splits=n_cv, shuffle=True, random_state=random_state)\n\n\ndef PerformanceCompare(class_true, class_predict, method, score_predict, exist_score=True):\n    if exist_score:\n        roc_auc = metrics.roc_auc_score(y_true=class_true, y_score=score_predict)\n        pr_score = metrics.average_precision_score(y_true=class_true, y_score=score_predict)\n    else:\n        roc_auc = float('nan')\n        pr_score = float('nan')\n    f1_score = metrics.f1_score(y_true=class_true, y_pred=class_predict)\n    sensitivity = metrics.recall_score(y_true=class_true, y_pred=class_predict)\n    accuracy = metrics.accuracy_score(y_true=class_true, y_pred=class_predict)\n\n    output = pd.DataFrame(np.array([roc_auc, pr_score, f1_score, sensitivity, accuracy]),\n        index=['ROC_AUC', 'PR_score', 'F1_score', 'sensitivity', 'accuracy'], columns=method)\n    return(output)\n\n\n# Logistic Regression\ndef LRPerformance(X, Y):\n    param_grid = {\n        'C': [10**uu for uu in np.linspace(-4, 4, n_Cs)]\n    }\n\n    lr_model = linear_model.LogisticRegression(random_state=random_state, class_weight=class_weight, max_iter=max_iter)\n    lr_cv = GridSearchCV(estimator=lr_model, param_grid=param_grid, cv=cv_splitter, scoring=scoring).fit(X,Y)\n    Y_predict = lr_cv.predict(X)\n    score_predict = lr_cv.predict_proba(X)[:,1]\n    best_estimator = lr_cv.best_estimator_\n\n    performance = PerformanceCompare(class_true=Y, class_predict=Y_predict, score_predict=score_predict, method=['LR'])\n    return({'performance': performance, 'best_estimator': best_estimator})\n\n\n# Support Vector Machine\ndef SVMPerformance(X, Y):\n    #penalty = 'l1'\n    #loss = 'hinge'\n    param_grid = {\n        'C' : [10**uu for uu in np.linspace(-4, 4, n_Cs)]\n    }\n    svm_model = svm.SVC(random_state=random_state, class_weight=class_weight)\n    svm_cv = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=cv_splitter, scoring=scoring).fit(X, Y)\n    Y_predict = svm_cv.predict(X)\n    score_predict = svm_cv.decision_function(X)\n    best_estimator = svm_cv.best_estimator_\n\n    performance = PerformanceCompare(class_true=Y, class_predict=Y_predict, score_predict=score_predict, method=['SVM'])\n    return({'performance': performance, 'best_estimator': best_estimator})\n\n\n# Naive Bayes\ndef NBPerformance(X, Y):\n    categorical_cols = ['gender_M', 'specialisation_Mkt&HR', 'workex_Yes']\n    numerical_cols = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p_trans']\n\n    cate_nb = naive_bayes.BernoulliNB().fit(X.loc[:, categorical_cols], Y)\n    conti_nb = naive_bayes.GaussianNB().fit(X.loc[:, numerical_cols], Y)\n    prob = cate_nb.predict_proba(X.loc[:, categorical_cols]) * conti_nb.predict_proba(X.loc[:, numerical_cols])\n    Y_predict = list(np.argmax(prob, axis=1))\n    score_predict = prob[:,1]\n\n    performance = PerformanceCompare(class_true=Y, class_predict=Y_predict, score_predict=score_predict, method=['NB'])\n    return({'performance': performance, 'best_estimator': [cate_nb, conti_nb]})\n\n\n# Decision Tree\ndef DTPerformance(X, Y):\n    param_grid = {\n        'max_leaf_nodes': range(3,15),\n        'ccp_alpha': np.linspace(0, 0.05, 5)\n    }\n    dt_model = DecisionTreeClassifier(random_state=random_state, class_weight=class_weight)\n    dt_cv = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=cv_splitter, scoring=scoring).fit(X, Y)\n    Y_predict = dt_cv.predict(X)\n    score_predict = dt_cv.predict_proba(X)[:,1]\n    best_estimator = dt_cv.best_estimator_\n\n    performance = PerformanceCompare(class_true=Y, class_predict=Y_predict, score_predict=score_predict, method=['DT'])\n    return({'performance': performance, 'best_estimator': best_estimator})\n\n\nlr_pb = LRPerformance(train_X,train_Y)\nsvm_pb = SVMPerformance(train_X, train_Y)\nnb_pb = NBPerformance(train_X, train_Y)\ndt_pb = DTPerformance(train_X, train_Y)\n\nprint(lr_pb['best_estimator'])\nperformance_table = pd.concat([lr_pb['performance'],\n    svm_pb['performance'],\n    nb_pb['performance'],\n    dt_pb['performance']], axis=1)\nperformance_table = round(performance_table, 3)\nprint(performance_table)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table we could easily find that logistic regression model excels on every metric we choosed, also, it is easy for explaining. Therefore, we would choose logistic regression model as the final decision."},{"metadata":{},"cell_type":"markdown","source":"## C-3. Select Polynomial Features  \nAfter selecting the favorite model, we generated polynomial features then did feature selection through \n1. Recursive feature elimination  \n\n    *The number of variables remaining is decided by cross validation.*  \n\n\n1. L1 penalty  \n  \n    *The variable of which the absolute value of coefficient is under 10^-5 would be abandonded*"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n3. Select Polynomial Features\n\n- CV to select best feature set from polynomial of the basic predictors.\n- L1 penalty to eliminate redundant variables.\n\"\"\"\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\n\npoly = PolynomialFeatures(2)\ntrain_poly = poly.fit_transform(train_X)\npoly_name = poly.get_feature_names(train_X.columns)\n\nstay_id= [i for i, x in enumerate(poly_name) if x not in ['gender_M^2','specialisation_Mkt&HR^2','workex_Yes^2']]\ntrain_poly = train_poly[:,stay_id]\npoly_name = [poly_name[i] for i in stay_id]\n\nmax_iter = 10000\nlr_model = linear_model.LogisticRegression(C=0.36, random_state=random_state, class_weight=class_weight, max_iter=max_iter, solver='liblinear')\nrfecv = RFECV(estimator = lr_model, step=1, cv=StratifiedKFold(5), scoring='roc_auc').fit(train_poly, train_Y)\n\nplt.figure()\nplt.xlabel('Number of features selected')\nplt.ylabel('ROC_AUC score')\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.show()\n\nrfe = RFE(estimator = lr_model, step=1, n_features_to_select=21).fit(train_poly, train_Y)\npoly_variables_rfe = [poly_name[id] for id, x in enumerate(rfe.support_) if x]\n\n# Lasso Feature Selection\nparam_grid = {\n    'C': [10**x for x in np.linspace(-4,4,n_Cs)]\n}\n\n\nlr_model = linear_model.LogisticRegression(penalty='l1', random_state=random_state, class_weight=class_weight, max_iter=max_iter, solver='liblinear')\n#lr_cv = GridSearchCV(lr_model, param_grid=param_grid, scoring='roc_auc').fit(train_poly, train_Y)\n#importance = np.abs(lr_cv.best_estimator_.coef_)[0]\n\nthreshold = 1e-5\nsfm = SelectFromModel(lr_model, threshold=threshold).fit(train_poly, train_Y)\n\n\npoly_variables_l1 = [poly_name[id] for id, x in enumerate(sfm.get_support()) if x]\n\nfeature_selection_output = pd.DataFrame({\n    'Feature Name': poly_name,\n    'RFE': rfe.support_*1,\n    'L1': sfm.get_support()*1\n})\nfeature_selection_output.to_csv(wd+'/feature_selection_output.csv', index=False)\n\nprint(feature_selection_output)\n\n# Set the final version of FEATURE SET\nstay_id = [id for id, x in enumerate(poly_name) if x in poly_variables_rfe]\ntrain_X = train_poly[:, stay_id]\n\ntest_poly = poly.transform(test_X)\nstay_id = [id for id, x in enumerate(poly.get_feature_names(test_X.columns)) if x in poly_variables_rfe]\ntest_X = test_poly[:, stay_id]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RFE:**  \nThe roc-auc score started to be stable while choosing more than 20 variables.  \n\n**L1:**  \nThe number of variables remaining is 22.\n\nThe number of variables remaining obtained by these 2 methods are close. Compare to l1-penaly method, there are less quadratic variables in RFE mothods, which is easier for people to explain and understand, thus we use the feature set obtained by RFE."},{"metadata":{},"cell_type":"markdown","source":"## C-4. Hyperparameter Tuning  \nFinally, we would grind the model and choose a best hyperparameter set."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n4. Hyperparameter Tuning\n\n- Tune to the best hyperparameter\n\"\"\"\nparam_grid = {\n    'C': np.linspace(1e-5, 1e5, 20),\n    'class_weight': ['balanced']\n}\n\nlr_model = linear_model.LogisticRegression(random_state=random_state, max_iter=max_iter, solver='liblinear')\nlr_cv = GridSearchCV(lr_model, param_grid=param_grid, scoring='roc_auc').fit(train_X, train_Y)\n\ndump(lr_cv, wd+'/final_model.joblib')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Performance\nY_predict = lr_cv.predict(test_X)\nY_score = lr_cv.predict_proba(test_X)[:,1]\n\nconfusion_matrix_train = metrics.confusion_matrix(train_Y, lr_cv.predict(train_X))\nconfusion_matrix_train = pd.DataFrame(confusion_matrix_train, index=['Neg','Pos'], columns=['Pred_Neg','Pred_Pos'])\n\nconfusion_matrix_test = metrics.confusion_matrix(y_true=test_Y, y_pred=Y_predict)\n#tn, fp, fn, tp = confusion_matrix.ravel()\nconfusion_matrix_test = pd.DataFrame(confusion_matrix_test, index=['Neg','Pos'], columns=['Pred_Neg','Pred_Pos'])\n\nroc_auc = metrics.roc_auc_score(y_true=test_Y, y_score=Y_score)\naverage_precision = metrics.average_precision_score(y_true=test_Y, y_score=Y_score)\nsensitivity = metrics.recall_score(y_true=test_Y, y_pred=Y_predict)\nspecificity = metrics.recall_score(y_true=test_Y, y_pred=Y_predict, pos_label=0)\naccuracy = metrics.accuracy_score(y_true=test_Y, y_pred=Y_predict)\nprecision = metrics.precision_score(y_true=test_Y, y_pred=Y_predict)\nprecision_neg = metrics.precision_score(y_true=test_Y, y_pred=Y_predict, pos_label=0)\n\n\n\nmetrics.plot_roc_curve(estimator=lr_cv.best_estimator_, X=test_X, y=test_Y)\nplt.title('ROC Curve')\nplt.show()\n\nmetrics.plot_precision_recall_curve(estimator=lr_cv.best_estimator_, X=test_X, y=test_Y)\nplt.title('Precision-Recall Curve')\nplt.show()\n\nprint(lr_cv.best_estimator_)\nprint(\"best roc_auc:\",lr_cv.best_score_)\nprint('confusion_matrix_train:\\n',confusion_matrix_train,'\\n')\n\nprint('confusion_matrix_test:\\n', confusion_matrix_test)\nprint(pd.Series({\n    'roc_auc': roc_auc, 'average_precision':average_precision, \n    'accuracy':accuracy, 'sensitivity':sensitivity, 'specificity':specificity,\n    'precision':precision, 'precision_neg':precision_neg\n}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Performance:  \n* C, the regulization strength parameter we choose is quite large, which means that there is almost no penalty on the size of coeffecients.\n* ROC-AUC and AP are quite good, which are near 0.85 and 0.83, showing that the model has excellent discriminant ability.\n* Accuracy of the model is about 0.81, while comparing to specificity, the sensitivity (recall) is relative small. It means that the ability of testing the real positive one is poorer than which of testing the negative one.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Impact  \npd_coef = pd.DataFrame({'Feature Name': poly_variables_rfe, 'Coefficient': lr_cv.best_estimator_.coef_[0,:]})\n\n\nfeature_selection_output = pd.merge(feature_selection_output, pd_coef, left_on='Feature Name', right_on='Feature Name', how='outer')\nprint(feature_selection_output)\n\ndefault_value = X.loc[:,['ssc_p','hsc_p','degree_p','etest_p_trans']].apply(lambda x: sum(x)/len(x), axis=0)\ndefault_value = round(default_value, 0).tolist()\n\ndef makeSample(moving_variable=None, fixed_variable=None, default_value=default_value):\n    \"\"\"\n    To generate sample with some variable values being fixed & ONE variable value moving. \n    The sample would be transformed to polynomial and be selected as the feature set we've selected.\n    \n    * moving_variable is a dict with only one element. The key is column name. The value could be a number or list containing multiple numbers.\n    * fixed_variable is a dict containing one or multiple elements. The keys are column names. The values could only be a number.\n    * The default values of the categoricals are 0, and which of the numericals are 50.\n    \"\"\"\n    \n    columns0=['gender_M','specialisation_Mkt&HR','workex_Yes','ssc_p','hsc_p','degree_p','etest_p_trans']\n    values0 = [0,0,0]+default_value\n    \n    if fixed_variable is None:\n        X = values0\n    else:\n        change_names = [x for x in fixed_variable.keys()] # would be a list even if theres only one element\n        change_values = [x for x in fixed_variable.values()] # would be a list even if theres only one element\n        change_id = [id for cn in change_names for id, x in enumerate(columns0) if x == cn]\n        \n        X = values0\n        for ind in range(len(change_names)):\n            X[change_id[ind]] = change_values[ind]\n    \n    \n    if moving_variable is None:\n        N=1\n        moving_name = [None]\n        moving_value = []\n    else: \n        moving_name = [x for x in moving_variable.keys()][0]\n        moving_value = [x for x in moving_variable.values()][0] # would have been a double list if not choosing the first element\n        N = len(moving_value)\n    \n    \n    X = np.array(X*N)\n    X = np.reshape(X, (N,7))\n    moving_id = [id for id, x in enumerate(columns0) if x in moving_name][0] # would have been a list if not choosing the first element\n    X[:,moving_id] = moving_value\n    X = pd.DataFrame(X, columns=columns0)\n    \n    X_poly = poly.fit_transform(X)\n    poly_name = poly.get_feature_names(X.columns)\n    \n    stay_id= [i for i, x in enumerate(poly_name) if x in poly_variables_rfe]\n    X_poly = X_poly[:,stay_id]\n        \n    \n    return(X_poly)\n\n\ndef makeComparisonSample(categoricals, numerical):\n    output = []\n    name = []\n    for ind1 in [0,1]:\n        name1 = \"\".join([categoricals[0], str(ind1)])\n        for ind2 in [0,1]:\n            name2 = \"\".join([categoricals[1], str(ind2)])\n            for ind3 in [0,1]:\n                name3 = \"\".join([categoricals[2], str(ind3)])\n                name_temp = \" \".join([name1, name2, name3])\n                temp = makeSample(moving_variable={numerical:range(101)}, fixed_variable={categoricals[0]:ind1, categoricals[1]:ind2, categoricals[2]:ind3})\n                output.append(temp)\n                name.append(name_temp)\n    return(output, name) # output is a list made of 4 arrays\n\n        \ndef plotComparison(labels, categoricals, suptitle):\n    \n    # Show the probablity predicted of different \n    numerical_cols = ['ssc_p','hsc_p','degree_p','etest_p_trans']\n    # gender & specialization & numericals\n    s = []\n    s_name = []\n    linestyle = ['solid']*(len(labels)+1)\n    count = 0\n    n_row = 2\n    n_col = 2\n    fig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(10,7))\n    plt.subplots_adjust(hspace=0.7, wspace=0.3)\n    for col in numerical_cols:\n        s_temp, s_name_temp = makeComparisonSample(categoricals=categoricals, numerical=col)\n\n        r_id = count//n_col\n        c_id = count%n_col\n        for ind in range(8):\n            prob_temp = lr_cv.predict_proba(s_temp[ind])[:,0]\n\n            axes[r_id, c_id].plot(range(101), prob_temp, linestyle=linestyle[ind], label=labels[ind])\n        axes[r_id, c_id].set_ylim(-0.1,1.1)\n        axes[r_id, c_id].set_yticks([0,.2,.4,.6,.8,1])\n        axes[r_id, c_id].hlines(y=0.5, xmin=0, xmax=100, linestyles='dashed', colors='gray', linewidth=1)\n        axes[r_id, c_id].set_ylabel('Probability')\n        axes[r_id, c_id].set_xlabel(col)\n        #axes[r_id, c_id].set_title('Prob. of being PLACED')\n        s.append(s_temp)\n        s_name.append(s_name_temp)\n        count+=1\n    \n    lines, labels = fig.axes[-1].get_legend_handles_labels()\n    fig.legend(lines, labels, loc = 'right')\n    fig.suptitle(suptitle)\n    return(fig, axes)\n\n\n\n# Create Labels   \nlabel_grid = {'Gender':['Female','Male'], 'Specialization':['Fin','HR'], 'WorkExperience':[\"Haven't Worked\", \"Have Worked\"]}\ngrid = model_selection.ParameterGrid(label_grid)\nlabels = []\nfor g in grid:\n    temp =\"/\".join(list(g.values()))\n    labels.append(temp)\n\nplotComparison(labels=labels, categoricals=['gender_M','specialisation_Mkt&HR', 'workex_Yes'], suptitle='Prob. of being PLACED')    \nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Impact:  \n> Note that the positive event means **Not Placed** in our analysis. The larger the coefficient is, the less posible that someone would get a job.  \nFor example, the coefficient of ssc_p is -0.23, it can be interpreted as that 1 unit increasing of ssc_p would let the log odds ratio of \"**DO getting a job**\" to \"**Not getting a job**\" **RAISE** by 0.23.\n\n> Scale of categorical variables including gender, specialization and work experience is (0,1), their effects on model output can't be larger than the coefficients themselves, while the numerical predictors could increase to the maximum 100, which allows the effects of the numericals dozen times of their own coefficients.  \nFor example, there might be 2 people with diffence of ssc_p being 60. Then their difference of log odds ratio of getting a job is 0.23\\*60=13.8. However, difference of log odds ratio of people with different gender would be only 7.3, which is relatively smaller than the value of the coefficient itself, comparing to that of the ssc_p.  \n\nHere's things the graph shows:\n  \nBasically, effects of ssc_p, hsc_p, degree_p, etest_p_trans are positive to probability of getting a job. Except for some cases including \n* Female who hasn't worked & male who hasn't worked specializes HR, on which employment test grade has negative impact.\n* On the situation that female who hasn't worked specializes HR, secondary school grade has barely no impact.  \n  \nWith same mean values of other variables, males usually have better chance to get placed than females when they get about above-average grades, which verifies the trend we've observed on EDA.\n  \nHowever, the specialization doesn't affect the chance as much as we thought. In some cases the curves are similar, in other cases people with HR specialization have higher chance than who specialize Finance.  \n  \nWork experience has really positive impact on the placed chance.  \n  \nThe effect of secondary school grade is quite consistent and influential in every situation except for one case.  \n  \nEmployment test grade seems random. There are up trend and down trend on different situations.\n  \nEffects of high school grade and college grade are similarily strong. "},{"metadata":{},"cell_type":"markdown","source":"# Reference  \n\nhttps://www.orfonline.org/research/literacy-in-india-the-gender-and-age-dimension-57150/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}