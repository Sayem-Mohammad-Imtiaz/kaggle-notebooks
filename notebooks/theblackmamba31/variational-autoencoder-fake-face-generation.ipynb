{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras \nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport keras.backend as K\nimport ipywidgets as widgets\nfrom IPython.display import display\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/age-gender-and-ethnicity-face-data-csv/age_gender.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data","metadata":{}},{"cell_type":"code","source":"data = data.pixels.apply(lambda x: np.array(x.split(\" \"),dtype = float))\narr = np.stack(data)\narr = arr / 255.0\narr = arr.astype('float32')\narr = arr.reshape(arr.shape[0],48,48,1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize our data\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(arr[i],cmap = 'gray')\n    plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Batch Size and shuffling data","metadata":{}},{"cell_type":"code","source":"batch_size = 64\ndataset = tf.data.Dataset.from_tensor_slices(arr).batch(batch_size).shuffle(132)     \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling ","metadata":{}},{"cell_type":"code","source":"class Sampling(keras.layers.Layer):\n    def call(self, inputs):\n        z_mean , z_log_var = inputs;\n        batch = tf.shape(z_mean)[0]\n        dimension = tf.shape(z_mean)[1]\n        epsilon = K.random_normal(shape = (batch, dimension))\n        return z_mean + tf.exp(z_log_var * 0.5) * epsilon","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining our encoder model","metadata":{}},{"cell_type":"code","source":"latent_dim = 3\nencoder_inputs = layers.Input(shape = (48,48,1))\nx = layers.Conv2D(filters = 32, kernel_size = (2,2), activation = 'relu', padding = 'same')(encoder_inputs)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\n\nx = layers.Flatten()(x)\nmean = layers.Dense(latent_dim)(x)\nlog_var = layers.Dense(latent_dim)(x)\nz = Sampling()([mean, log_var])\n\nencoder = tf.keras.Model(encoder_inputs, [mean,log_var,z])\n\nencoder.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Decoder Model","metadata":{}},{"cell_type":"code","source":"decoder_input = layers.Input(shape = (latent_dim,))\n\nx = layers.Dense(6*6*32, activation = 'relu')(decoder_input)\n\nx = layers.Reshape((6,6,32))(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters =128, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 1, kernel_size = (2,2), padding = 'same')(x)\n\ndecoder = tf.keras.Model(decoder_input, x)\ndecoder.summary()\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VAE Model","metadata":{}},{"cell_type":"code","source":"inp = encoder.input\nout = encoder.output\ndecoder_output = decoder(out[2])\nvae = tf.keras.Model(inp, decoder_output)\nvae.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Loss functions","metadata":{}},{"cell_type":"code","source":"def kl_loss(z_log_var,z_mean):\n    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n    return kl_loss * 0.012\n\n\nloss = tf.keras.losses.BinaryCrossentropy()\ndef reconstruction_loss(data,reconstructed):\n    return loss(data,reconstructed)\noptimizer = tf.keras.optimizers.Adam()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train steps","metadata":{}},{"cell_type":"code","source":"def train_steps(data):\n    with tf.GradientTape() as vae_tape:\n        z_mean,z_log_var,z = encoder(data)\n        reconstructed_image = decoder(z)\n        kl_ = kl_loss(z_log_var,z_mean)\n        reconstruction_ = reconstruction_loss(data, reconstructed_image)\n        total_loss = kl_ + reconstruction_\n\n    gradient2 = vae_tape.gradient(total_loss, vae.trainable_variables)\n    optimizer.apply_gradients(zip(gradient2, vae.trainable_variables))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise = tf.keras.backend.random_normal(shape = (9,latent_dim))\ndef visualize(epoch):\n        prediction = decoder(noise)\n        plt.figure(figsize = (5,5))\n        for i in range(9):\n            if(i == 2):\n                plt.title(\"Epoch: {}\".format(epoch))\n            plt.subplot(3,3,i+1)\n            plt.imshow(prediction[i],cmap = 'gray')\n            plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        for data in dataset:\n            train_steps(data)\n        print(\"Epoch: {} Time: {}\".format(epoch+1,np.round(time.time()-start),3))\n        if epoch % 3 == 0:\n            visualize(epoch+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"train(dataset,15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To use widgets for interactive visualization\nIn order to use this widgets you have to copy and run this model","metadata":{}},{"cell_type":"code","source":"def generate_image(latent1, latent2, latent3):\n    latent_values = np.array([[latent1, latent2, latent3]])\n    reconstruction = np.array(decoder(latent_values))\n    reconstruction = reconstruction.reshape(48,48,1)\n    plt.figure(figsize = (4,4))\n    plt.imshow(reconstruction,cmap = 'gray')\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\na,b,z = encoder(np.array(arr[:5000]).reshape(np.array(arr[:5000]).shape[0],48,48,1))\nlatent1_min = np.min(z[:,0])-1\nlatent1_max = np.max(z[:,0])+1\n\nlatent2_min = np.min(z[:,1])-1\nlatent2_max = np.max(z[:,1])+1\n\nlatent3_min =np.min(z[:,2])-1\nlatent3_max = np.max(z[:,2])+1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nface_image_generator = widgets.interact(\n    generate_image,\n    latent1=(latent1_min, latent1_max),\n    latent2=(latent2_min, latent2_max),\n    latent3=(latent3_min, latent3_max),\n)\n\ndisplay(generate_image)\n##  copy and run this notebook to use this widget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize our input image through VAE\ni = np.random.randint(1,2323)\nout = vae.predict(arr[i].reshape(1,48,48,1))\nplt.subplot(1,2,1)\nplt.title(\"Original Image\")\nplt.imshow(arr[i], cmap = 'gray')\nplt.subplot(1,2,2)\nplt.title(\"Reconstructed Image\")\nplt.imshow(out.reshape(48,48,1), cmap = 'gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank You \n## Any Suggestion to improve this notebook is highly appreciated\n\nref: <a href= 'https://www.kaggle.com/gcdatkin/an-introduction-to-variational-autoencoders'> here </a>","metadata":{}}]}