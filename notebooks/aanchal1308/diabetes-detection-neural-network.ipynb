{"cells":[{"metadata":{"_uuid":"c09b4031-b9de-483e-8a5b-7fe29dae0568","_cell_guid":"05f9f97a-67c3-4b2f-925a-49d7b01c68bc","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd6062a6-a896-43ae-a54e-ac402b71ec9c","_cell_guid":"43fc7b86-6009-42cc-a0e8-95ee971d0e7d","trusted":true},"cell_type":"code","source":"url = \"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\"\nnames = ['n_pregnant', 'glucose_concentration', 'blood_pressuer (mm Hg)', 'skin_thickness (mm)', 'serum_insulin (mu U/ml)',\n        'BMI', 'pedigree_function', 'age', 'class']\ndf = pd.read_csv(url, names = names)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9127e246-9fd0-4939-be48-474b1f9eadb2","_cell_guid":"13cea6b2-eff5-4597-8d82-f8289b9db909","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bd809bf-519b-4081-80dc-554bfd8675b3","_cell_guid":"c74caff1-48a9-4c40-9c21-d8d1a6609573","trusted":true},"cell_type":"code","source":"columns = ['glucose_concentration', 'blood_pressuer (mm Hg)', 'skin_thickness (mm)', 'serum_insulin (mu U/ml)', 'BMI']\nfor col in columns:\n    df[col].replace(0, np.NaN, inplace=True)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d1a5e05-7fa0-42ff-80c7-dc00fe4be905","_cell_guid":"de65f4e0-1850-4104-85ad-442d4678f23e","trusted":true},"cell_type":"code","source":"df.dropna(inplace=True) \ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53505d49-7871-417f-ae61-4efb4030d1c4","_cell_guid":"dfb16896-4185-480f-bdf4-c957929ba975","trusted":true},"cell_type":"code","source":"dataset = df.values  # Convert dataframe to numpy array\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"441f8ce7-96c9-4ffc-8e82-2855951ea31b","_cell_guid":"8eaf3801-7391-4f5f-a917-52420a192ea7","trusted":true},"cell_type":"code","source":"# Split into input (X) and output (Y)\nX = dataset[1:,0:8]   # Array slicing, select all rows and columns from 0 to 7\nY = dataset[1:,8]\nprint(X.shape)\nprint(Y.shape)\nprint(Y[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e818c47f-aae5-4c6e-aa80-82c7a3c6e3db","_cell_guid":"93fb2ce2-eb39-45fb-b042-6bb493c35ff8","trusted":true},"cell_type":"code","source":"# Standardize the data\nscaler = StandardScaler().fit(X)\nX_standardized = scaler.transform(X)\ndata = pd.DataFrame(X_standardized)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfa949e7-902b-4124-93f3-caf206bf88a7","_cell_guid":"8e10bd0a-5d02-4047-84bb-350d6e9fa891","trusted":true},"cell_type":"code","source":"# Building the Keras model\ndef create_model():\n    # create sequential model\n    model = Sequential()    \n    model.add(Dense(8, input_dim = 8, kernel_initializer = 'normal', activation = 'relu')) #rectifier(relu) activation function\n    model.add(Dense(4, input_dim = 8, kernel_initializer = 'normal', activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid')) #sigmoid function in output layer\n\n    #compile the model\n    adam = Adam(lr = 0.01)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy']) # metrics for classification problem: classification accuracy\n    return model\n\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee1107fa-11db-4d6b-bac1-d537d796c18c","_cell_guid":"08395cf7-2cc4-493b-84da-f4f29bea81a4","trusted":true},"cell_type":"markdown","source":"Optimizing network by tuning the hyperparameters"},{"metadata":{"_uuid":"9e1f4146-9e02-459b-8dcd-66b893792676","_cell_guid":"986faa40-5ea4-450c-a378-72c3b0db61dc","trusted":true},"cell_type":"code","source":"\n\n# Performing a grid search for the optimal batch size and number of epochs\nseed = 6  # Define a random seed\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model():\n    \n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    adam = Adam(lr = 0.01)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n#Create the model\nmodel = KerasClassifier(build_fn = create_model, verbose = 1)\n\n# define the grid search parameters\nbatch_size = [10,20,40]\nepochs = [10, 50, 100]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(batch_size = batch_size, epochs = epochs)\n\n# build and fit the GridSearchCV- exhaustive search over specified parameter values for an estimator\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state = seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c27db36b-4a9b-4265-adb5-f78600789c0f","_cell_guid":"0f7d4350-a037-46c5-945f-0ee2a1c3be40","trusted":true},"cell_type":"code","source":"# Summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstd = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, std, params):\n    print('{0} ({1} with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67b30529-4942-49af-989d-8f52933cbd62","_cell_guid":"50cf95a1-215b-4ab5-ac77-5992ce56edf3","trusted":true},"cell_type":"code","source":"# Do a grid search for learning rate and dropout rate\n# Reducing overfitting using dropout regularization\nseed = 6\nnp.random.seed(seed)\n\n# defining the model\ndef create_model(learn_rate, dropout_rate):    \n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    adam = Adam(lr = learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# grid search parameters\nlearn_rate = [0.001, 0.01, 0.1]\ndropout_rate = [0.0, 0.1, 0.2]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(learn_rate = learn_rate, dropout_rate = dropout_rate)\n\n# build and fit the GridSearchCV- exhaustive search over specified parameter values for an estimator\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state = seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a6fb0fb-c1f4-413e-9b00-4dc56b0796e8","_cell_guid":"10f3ff0f-7443-41f9-9c21-fd8b99ce8b9d","trusted":true},"cell_type":"code","source":"# Summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstd = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, std, params):\n    print('{0} ({1} with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6756c710-ed83-4995-8ab5-5e2a3473848b","_cell_guid":"d3f31d2c-cdf3-44ff-b6b0-2ba23c57df59","trusted":true},"cell_type":"code","source":"# Do a grid search to optimize kernel initialization and activation functions\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(activation, init):\n    \n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer= init, activation=activation))\n    model.add(Dense(4, input_dim = 8, kernel_initializer= init, activation=activation))\n    model.add(Dense(1, activation='sigmoid'))\n    # We will modify the learning rate of the Adam optimizer to 0.001, as this is the best value that we found.\n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# Create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# Define the grid search parameters\nactivation = ['softmax', 'relu', 'tanh', 'linear']\ninit = ['uniform', 'normal', 'zero']\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(activation = activation, init = init)\n\n# build and fit the GridSearchCV- exhaustive search over specified parameter values for an estimator\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state = seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a2419b1-3ce3-4d4f-9c7a-ae361f91cefb","_cell_guid":"f739d5c7-a796-4851-b25a-81e654362e12","trusted":true},"cell_type":"code","source":"# Summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstd = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, std, params):\n    print('{0} ({1} with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6498a2f4-cd27-4e7f-8d0a-a7ec98415aaa","_cell_guid":"14b8d933-cd62-4986-a061-2ad5946f8981","trusted":true},"cell_type":"code","source":"# Do a grid search to find the optimal number of neurons in each hidden layer\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(neuron1, neuron2):\n    \n    model = Sequential()\n    model.add(Dense(neuron1, input_dim = 8, kernel_initializer='uniform', activation='linear'))\n    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer='uniform', activation='linear'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# Create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n\n# Define the grid search parameters\nneuron1 = [4, 8, 16]\nneuron2 = [2, 4, 8]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(neuron1=neuron1, neuron2=neuron2)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state = seed), refit=True, verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2868910-f230-4008-adb3-c7d102dee7a2","_cell_guid":"46b7f7d6-3dea-4a31-854d-6e8ed1134c9a","trusted":true},"cell_type":"code","source":"# Summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstd = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, std, params):\n    print('{0} ({1} with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc7600b5-2c2d-47cd-a6e5-ac05f15c425c","_cell_guid":"c1d0f6e2-a72f-4212-8427-d91a9bb7818d","trusted":true},"cell_type":"code","source":"# generate predictions with optimal hyperparameters\ny_pred = grid.predict(X_standardized)\nprint(y_pred.shape)\nprint(y_pred[:5])\n\nprint(accuracy_score(Y,y_pred))\nprint(classification_report(Y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e82b5c0-47d0-4f82-94de-9e8cd4c86871","_cell_guid":"34b66ee0-133c-4b38-b8b5-e8cdf5402c49","trusted":true},"cell_type":"code","source":"#prediction for any example\nexample = df.iloc[5]\nprint(example)\nprint(\"Actual class\",Y[4])\nprediction = grid.predict(X_standardized[4].reshape(1,-1))\nprint(\"Predicted class\", prediction)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}