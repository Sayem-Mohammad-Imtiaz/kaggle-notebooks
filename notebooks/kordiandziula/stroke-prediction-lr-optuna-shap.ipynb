{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Stroke is third most common cause of death and main cause of disability or complete dependance on performing activities of daily living within adults. There are two main varieties:\n* Hemorrhagic stroke (sudden bleeding may occur due to a ruptured brain aneurysm, which damages brain structure)\n* Ischemic (usuallt caused by blockage of a blood vessel) \n\nWe can highlight several factors of a stroke:\n* Age\n* Family history of stroke\n* Hypertension\n* Heart diseases\n* Diabetes\n* Smoking status\n* Alcohol abuse\n* Amphetamine, cocaine abuse\n* Obesity\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.columns = [col.lower() for col in df.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[((df[\"age\"] < 18) & \n    (df[\"work_type\"] != \"children\"))].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations, that include a 7 or 8 yeard old running a business (or other unusuall) would require consulting with a specialist. Propable errors in collecting data.","metadata":{}},{"cell_type":"code","source":"df = df[((df[\"age\"] >= 18) |\n         (df[\"work_type\"] == \"children\"))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [\"gender\",\n                       \"hypertension\",\n                       \"heart_disease\",\n                       \"ever_married\",\n                       \"work_type\",\n                       \"residence_type\",\n                       \"smoking_status\"]\n\nnumerical_columns = [\"age\",\n                     \"avg_glucose_level\",\n                     \"bmi\"]\n\ntarget = \"stroke\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = \"whitegrid\")\n\ndef plot_cnt_prc(feature, target, data, axes):\n    sns.barplot(x = feature, y = \"id\",\n                data = data.groupby([feature, target]).count().reset_index(),\n                color = \"#d6d6f5\", hue = target, ax = axes[0])\n    \n    axes[0].set_xlabel(axes[0].get_xlabel(), size = 16)\n    axes[0].set_ylabel(\"Quantity\", size = 16)\n    \n    sns.barplot(x = feature, y = target,\n                data = (data.groupby(feature).mean() * 100).reset_index(),\n                color = \"#d6d6f5\", ax = axes[1])\n    \n    axes[1].set_xlabel(axes[1].get_xlabel(), size = 16)\n    axes[1].set_ylabel(\"Percentage\", size = 16)\n\n    \ncolumns = [\"gender\",\n           \"age\",\n           \"hypertension\",\n           \"heart_disease\",\n           \"ever_married\",\n           \"work_type\",\n           \"residence_type\",\n           \"avg_glucose_level\",\n           \"bmi\",\n           \"smoking_status\"]\n    \n\nfig, axes = plt.subplots(10, 2, figsize = (20, 70))\ndata = df.copy()\nfor ax, col in zip(axes, columns):\n    if col in numerical_columns:\n        data[col] = pd.qcut(data[col], q = 5,\n                            duplicates = \"drop\")\n    plot_cnt_prc(col, target, data, ax)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = df[target].value_counts()\nplt.figure(figsize = (12, 6))\n\nplt.pie(x = counts,\n        labels = counts.keys(),\n        autopct = \"%.1f%%\",\n        explode = (0, 0.1),\n        colors = [\"#99b3ff\", \"#4d79ff\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats as stats\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef correlation_plot(df, columns, \n                     method = \"pearson\", \n                     figsize = (12, 6)):\n    corr = df.loc[:, columns].corr(method = method)\n    mask = np.triu(np.ones_like(corr, dtype = np.bool))\n    \n    plt.figure(figsize = figsize)\n    heatmap = sns.heatmap(data = corr, mask = mask,\n                          vmin = -1, vmax = 1,\n                          annot = True, cmap = \"coolwarm\")\n    \n    heatmap.set_title(\"Correlation Heatmap\", fontdict = {\"fontsize\": 15})\n    plt.show()\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.copy()\nencoder = OrdinalEncoder()\ncolumns = categorical_columns + [target]\n\ndata = pd.DataFrame(encoder.fit_transform(data[columns]), \n                    columns = columns)\n\ncorrelation_plot(df = data,\n                 columns = columns,\n                 method = cramers_v)\ndel data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset does not include highly correlated categorical features","metadata":{}},{"cell_type":"code","source":"columns = numerical_columns + [target]\ncorrelation_plot(df = df,\n                 columns = columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset does not include highly correlated numerical features.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df.isna().sum(), \n             columns = [\"na_quantity\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assuming, that glucose measurment\n* Was performed on empty stomach\n* Is given with set of units mg/dL\n\nWe can highlight (surely this would require consulting a specialist) four categories:\n* avg_glucose_level < 70 – too low glucose level\n* 70 < avg_glucose_level < 88 – normal blood glucose level\n* 100 < avg_glucose_level < 125 – pre-diabetes\n* 126 < avg_glucose_level – diabetes\n\nWe can assign BMI values to 8 categories (this would also require consulting a specialist):\n* BMI < 16 – severely underweight\n* 16 < BMI < 16.99 - emaciation\n* 17 < BMI < 18.49 - underweight\n* 18.5 < BMI < 24.99 – normal weight\n* 25 < BMI < 29.99 - overweight\n* 30 < BMI < 34.99 – obesity class I \n* 35 < BMI < 39.99 - obesity class II\n* 40 < BMI - obesity class III\n","metadata":{}},{"cell_type":"code","source":"def glucose_level(glucose):\n    if glucose <= 70:\n        return \"TOO_LOW_GLUCOSE_LEVEL\"\n    elif glucose <= 99:\n        return \"NORMAL_BLOOD_GLUCOSE_LEVEL\"\n    elif glucose <= 125:\n        return \"PRE_DIABETES\"\n    else:\n        return \"DIABETES\"\n\ndef bmi(bmi_level):\n    if str(bmi_level) == \"nan\":\n        return \"NAN\"\n    elif bmi_level < 16:\n        return \"SEVERELY_UNDERWEIGHT\"\n    elif bmi_level < 16.99:\n        return \"EMACIATION\"\n    elif bmi_level < 18.49:\n        return \"UNDERWEIGHT\"\n    elif bmi_level < 24.99:\n        return \"NORMAL_WEIGHT\"\n    elif bmi_level < 29.99:\n        return \"OVERWEIGHT\"\n    elif bmi_level < 34.99:\n        return \"OBESITY_CLASS_I\"\n    elif bmi_level < 39.99:\n        return \"OBESITY_CLASS_II\"\n    else:\n        return \"OBESITY_CLASS_III\"\n\ndata = df.copy()\ndata[\"avg_glucose_level\"] = data[\"avg_glucose_level\"].apply(glucose_level)\ndata[\"bmi\"] = data[\"bmi\"].apply(bmi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [\"gender\",\n                       \"hypertension\",\n                       \"heart_disease\",\n                       \"ever_married\",\n                       \"work_type\",\n                       \"residence_type\",\n                       \"smoking_status\",\n                       \"avg_glucose_level\",\n                       \"bmi\"]\n\nnumerical_columns = [\"age\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OrdinalEncoder()\ncolumns = categorical_columns + [target]\n\ndata = pd.DataFrame(encoder.fit_transform(data[columns]), \n                    columns = columns)\n\ncorrelation_plot(df = data,\n                 columns = columns,\n                 method = cramers_v)\ndel data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\nclass Transformer(BaseEstimator, TransformerMixin):\n    def __bmi(self, value):\n        if str(value) == \"nan\":\n            return \"NAN\"\n        elif value < 16:\n            return \"SEVERELY_UNDERWEIGHT\"\n        elif value < 16.99:\n            return \"EMACIATION\"\n        elif value < 18.49:\n            return \"UNDERWEIGHT\"\n        elif value < 24.99:\n            return \"NORMAL_WEIGHT\"\n        elif value < 29.99:\n            return \"OVERWEIGHT\"\n        elif value < 34.99:\n            return \"OBESITY_CLASS_I\"\n        elif value < 39.99:\n            return \"OBESITY_CLASS_II\"\n        else:\n            return \"OBESITY_CLASS_III\"\n  \n    def __glucose(self, value):\n        if value <= 70:\n            return \"TOO_LOW_GLUCOSE_LEVEL\"\n        elif value <= 99:\n            return \"NORMAL_BLOOD_GLUCOSE_LEVEL\"\n        elif value <= 125:\n            return \"PRE_DIABETES\"\n        else:\n            return \"DIABETES\"\n\n    def transform(self, X, y = None):\n        X = X.copy()\n        X[\"bmi\"] = X[\"bmi\"].apply(self.__bmi)\n        X[\"avg_glucose_level\"] = X[\"avg_glucose_level\"].apply(self.__glucose)\n        return X\n\n    def fit(self, X, y = None):\n        return self","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom category_encoders import WOEEncoder\nfrom sklearn.compose import ColumnTransformer\n\nX = df[categorical_columns + numerical_columns]\nY = df[target]\n\nX_train, X_test, Y_train, Y_test =\\\n  train_test_split(X, Y, test_size = 0.2, stratify = Y, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline([(\"bmi_glucose\", Transformer()),\n                     (\"woe\", WOEEncoder(cols = categorical_columns))])\n\ntransformer = ColumnTransformer([(\"pipeline\",\n                                  pipeline,\n                                  categorical_columns),\n                                 (\"scale\",\n                                  StandardScaler(),\n                                  numerical_columns)])\n\nX_train = pd.DataFrame(transformer.fit_transform(X_train, Y_train),\n                       columns = X_train.columns,\n                       index = X_train.index)\nX_test = pd.DataFrame(transformer.transform(X_test),\n                      columns = X_test.columns,\n                      index = X_test.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nlr = LogisticRegression(class_weight = \"balanced\")\nlr.fit(X_train, Y_train)\n\nprint(classification_report(Y_test, lr.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nX_train_skf = X_train.values\nY_train_skf = Y_train.values\n\ndef objective(trial):\n    param_grid = {\n          \"class_weight\": \"balanced\",\n          \"random_state\": 1,\n          \"solver\": \"liblinear\",\n          \"C\": trial.suggest_float(\"C\", 0.01, 1),\n          \"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n      }\n  \n    skf = StratifiedKFold(n_splits = 3)\n    test_scores = []\n\n    for train_index, test_index in skf.split(X_train_skf, Y_train_skf):\n        X_train_fold, X_test_fold = X_train_skf[train_index], X_train_skf[test_index]\n        Y_train_fold, Y_test_fold = Y_train_skf[train_index], Y_train_skf[test_index]\n  \n    classifier = LogisticRegression(**param_grid)\n    classifier.fit(X_train_fold, Y_train_fold)\n    test_scores.append(roc_auc_score(Y_test_fold, classifier.predict_proba(X_test_fold)[:, 1]))\n\n    return np.asarray(test_scores).mean()\n\noptuna.logging.disable_default_handler()\nstudy = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from optuna.visualization import plot_parallel_coordinate\nplot_parallel_coordinate(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import make_scorer\n\nparam_grid = {\n      \"class_weight\": [\"balanced\"],\n      \"random_state\": [1],\n      \"solver\": [\"liblinear\"],\n      \"C\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2],\n      \"penalty\": [\"l1\", \"l2\"]\n}\n\ngrid = GridSearchCV(estimator = LogisticRegression(),\n                    param_grid = param_grid,\n                    cv = StratifiedKFold(n_splits = 3), \n                    n_jobs = -1,\n                    verbose = 10,\n                    scoring = make_scorer(roc_auc_score, needs_proba = True))\n\ngrid.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(**grid.best_params_)\nlr.fit(X_train, Y_train)\n\nprint(classification_report(Y_test, lr.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix\n\nfig, ax = plt.subplots(figsize = (10, 6))\nplot_confusion_matrix(lr, \n                      X_test, \n                      Y_test, \n                      ax = ax, \n                      values_format = '.0f')\nplt.grid(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, threshold = roc_curve(Y_test, lr.predict_proba(X_test)[:, 1])\ndata = list(zip(threshold, tpr, fpr))\ntrh = pd.DataFrame(data, \n                   columns = [\"threshold\", \n                              \"true_positive_rate\", \n                              \"false_positive_rate\"])\ntrh[\"tpr-fpr\"] = trh[\"true_positive_rate\"] - trh[\"false_positive_rate\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trh.sort_values(by = \"tpr-fpr\", ascending = False).head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = X_test.copy()\ncolumns = data.columns\ndata[\"label\"] = Y_test\ndata[\"pred_proba\"] = lr.predict_proba(data[columns])[:, 1]\n\ndata = data.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\n\nexplainer = shap.Explainer(lr.predict_proba, data.loc[:, columns])\nexplainer_output = explainer(data.loc[:, columns])\n\nexpected_values = explainer_output.base_values[:1, :].reshape(-1)\nshap_values = explainer_output.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values[:, :, 1], data.loc[:, columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sort_values(by = \"pred_proba\").head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(expected_values[1], shap_values[508].T[1], df.loc[4581, columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sort_values(by = \"pred_proba\", ascending=False).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(expected_values[1], shap_values[65].T[1], df.loc[218, columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(expected_values[1], shap_values[313].T[1], df.loc[4164, columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset includes part of observations, where the target variable is classified as 0, and simultaneously they are very similar to obesrvations reffered to as „success”. Age turned out to be the most relevant attribute out of the accessible set. Perhaps inserting additional features to the set would improvement of the results.","metadata":{}}]}