{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import TweetTokenizer\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/imdb_master.csv\", encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.groupby(['type']).count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = data[data.type == \"test\"]\ntrain_data = data[data.type == \"train\"][data.label != \"unsup\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef foo(data):\n    train_x = []\n    train_y = []\n    for i in range(len(data)):\n        train_x.append([i.lower() for i in TweetTokenizer().tokenize(data.iloc[i].review) if i.isalpha()])\n        label = data.iloc[i].label\n        if label == \"neg\":\n            train_y.append(0)\n        elif label == \"pos\":\n            train_y.append(1)\n    return train_x, train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, train_y = foo(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter1 = 0\ncounter0 = 0\nc = 0\nfor i in train_y:\n    if i == 1:\n        counter1 += 1\n    else: counter0 += 1\n    c +=1\nprint(counter0, counter1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_x = []\nval_y = []\nfor i in range(3000):\n    ind = random.randint(0, len(train_x)-1)\n    val_x.append(train_x[ind].copy())\n    val_y.append(train_y[ind])\n    del train_x[ind]\n    del train_y[ind]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c0 = 0\nc1 = 0\nfor i in val_y:\n    if i == 0:\n        c0 += 1\n    else: c1 += 1\nprint(c0, c1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x, test_y =  foo(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ncounter = Counter()\nfor i in train_x:\n    for j in i:\n        counter[j] += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counter.most_common(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counter.most_common()[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabular = {}\nfor num, i in enumerate(counter.most_common()):\n    vocabular[i[0]] = num+1\n    if num > 10000:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(vocabular))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(x):\n    train_tokenize_x = []\n    for i in x:\n        temp = []\n        for j in i:\n            tokenize = vocabular.get(j)\n            if tokenize is not None:\n                temp.append(tokenize)\n        train_tokenize_x.append(temp)\n    for i in range(len(train_tokenize_x)):\n        if len(train_tokenize_x[i]) < 200:\n            train_tokenize_x[i] += [0 for i in range(200-len(train_tokenize_x[i]))]\n        else:\n            train_tokenize_x[i] = train_tokenize_x[i][:200]\n    return train_tokenize_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokenize_x = tokenize(train_x)\nprint(train_tokenize_x[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_tokenize_x = tokenize(val_x)\nprint(val_tokenize_x[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokenize_x = tokenize(test_x)\nprint(test_tokenize_x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self, v_size):\n        super().__init__()\n        self.l1 = torch.nn.Embedding(v_size, 50, padding_idx=0)\n        self.l2 = torch.nn.LSTM(50, 500, batch_first=True)\n        self.l3 = torch.nn.Linear(500, 1)\n        \n    def forward(self, x):\n        out = self.l1(x)\n        out, (h_t, h_c) = self.l2(out)\n        out = self.l3(h_t)\n        return torch.sigmoid(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(len(vocabular)+2).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mydataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __getitem__(self, n):\n        return torch.tensor(data=self.x[n]), torch.FloatTensor(data=[self.y[n]])\n    \n    def __len__(self):\n        return len(self.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = Mydataset(train_tokenize_x, train_y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = Mydataset(val_tokenize_x, val_y)\nval_dataloader = DataLoader(val_dataset, batch_size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = Mydataset(test_tokenize_x, test_y)\ntest_dataloader = DataLoader(test_dataset, batch_size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = torch.optim.Adam(model.parameters(), lr=0.002)\ncriterion = torch.nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfor i in range(7):\n    c = 0\n    model.train()\n    for x, y in train_dataloader:\n        optim.zero_grad()\n        x, y = x.cuda(), y.cuda()\n        out = model(x)[0]\n        loss = criterion(out, y)\n        loss.backward()\n        optim.step()\n        if c % 50 == 0:\n            print(c/len(train_dataloader))\n        c += 1\n    model.eval()\n    y_labels = []\n    y_predicted = []\n    with torch.no_grad():\n        c = 0\n        for x, y in val_dataloader:\n            y_labels.extend(y.tolist())\n            x, y = x.cuda(), y.cuda()\n            out = model(x)[0]\n            out_value = []\n            for i in out.tolist():\n                if i[0] > 0.5:\n                    out_value.append(1)\n                else:\n                    out_value.append(0)\n            y_predicted.extend(out_value)\n            if c % 50 == 0:\n                print(c/len(val_dataloader))\n            c += 1\n    print(classification_report(y_labels, y_predicted))\n        \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_labels = []\ny_predicted = []\nwith torch.no_grad():\n    c = 0\n    for x, y in test_dataloader:\n        y_labels.extend(y.tolist())\n        x, y = x.cuda(), y.cuda()\n        out = model(x)[0]\n        out_value = []\n        for i in out.tolist():\n            if i[0] > 0.5:\n                out_value.append(1)\n            else:\n                out_value.append(0)\n        y_predicted.extend(out_value)\n        if c % 100 == 0:\n            print(c/len(test_dataloader))\n        c += 1\nprint(classification_report(y_labels, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ndf = pd.read_csv('../input/imdb_master.csv',encoding=\"latin-1\")\ndf = df.drop(['Unnamed: 0','file'],axis=1)\ndf.columns = ['type',\"review\",\"sentiment\"]\ndf.head()\nprint(df.head())\ndf = df[df.sentiment != 'unsup']\ndf['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})\n\ndf_train = df[df.type == 'train']\ndf_test = df[df.type == 'test']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : (1, 2),\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : 'word',\n        'min_df' : 2,\n    }\n    \n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    selector = SelectKBest(f_classif, k=min(6000, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val\n\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\nfrom sklearn import metrics\n\nnb = MultinomialNB()\nnb.fit(df_bag_train, df_train['sentiment'])\nnb_pred = nb.predict(df_bag_test)\nprint(metrics.classification_report(df_test['sentiment'], nb_pred))\ncm = metrics.confusion_matrix(nb_pred, df_test['sentiment'])\nprint('Accuracy ',metrics.accuracy_score(df_test['sentiment'], nb_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}