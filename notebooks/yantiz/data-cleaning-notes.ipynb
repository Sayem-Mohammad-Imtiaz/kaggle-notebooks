{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# read in all our data\nnfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\n\n# set seed for reproducibility\nnp.random.seed(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the first five rows of the nfl_data file.\nnfl_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows that contain a missing value\nnfl_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a small subset of the NFL dataset\nsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all NA's with 0\nsubset_nfl_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all NA's the value that comes directly after it in the same column, \n# then replace all the remaining na's with 0\nsubset_nfl_data.fillna(method='bfill', axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Scaling and Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# for Box-Cox Transformation\nfrom scipy import stats\n\n# for min_max scaling\nfrom mlxtend.preprocessing import minmax_scaling\n\n# plotting modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# set seed for reproducibility\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in **scaling**, you're changing the range of your data\n* while in **normalization**, you're changing the shape of the distribution of your data."},{"metadata":{},"cell_type":"markdown","source":"* **Scaling** means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points are, like **support vector machines (SVM)** or **k-nearest neighbors (KNN)**. With these algorithms, a change of \"1\" in any numeric feature is given the same importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate 1000 data points randomly drawn from an exponential distribution\noriginal_data = np.random.exponential(size=1000)\n\n# mix-max scale the data between 0 and 1\nscaled_data = minmax_scaling(original_data, columns=[0])\n\n# plot both together to compare\nfig, ax = plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Scaling just changes the range of your data. **Normalization** is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n\n* In general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include **linear discriminant analysis (LDA)** and **Gaussian naive Bayes**. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize the exponential data with boxcox\nnormalized_data = stats.boxcox(original_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Parsing Dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime\n\n# read in our data\nlandslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\n\n# set seed for reproducibility\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the data type of our date column\nlandslides['date'].dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* You may have to check the [numpy documentation](https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.dtype.kind.html#numpy.dtype.kind) to match the letter code to the `dtype` of the object. \"O\" is the code for \"object\"."},{"metadata":{},"cell_type":"markdown","source":"We can pandas what the format of our dates are with a guide called as [\"strftime directive\"](https://strftime.org/), which you can find more information on at this link. The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are lots of possible parts of a date, but the most common are `%d` for day, `%m` for month, `%y` for a two-digit year and `%Y` for a four digit year.\n\nSome examples:\n\n* 1/17/07 has the format \"%m/%d/%y\"\n* 17-1-2007 has the format \"%d-%m-%Y\"\n\nLooking back up at the head of the \"date\" column in the landslides dataset, we can see that it's in the format \"month/day/two-digit year\", so we can use the same syntax as the first example to parse in our dates:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column, date_parsed, with the parsed dates\nlandslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first few rows\nlandslides['date_parsed'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **What if I run into an error with multiple date formats?** While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:\n\n`landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the day of the month from the date_parsed column\nday_of_month_landslides = landslides['date_parsed'].dt.day\nday_of_month_landslides.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* If we tried to get the same information from the original \"date\" column, we would get an error: `AttributeError: Can only use .dt accessor with datetimelike values`. This is because `dt.day` doesn't know how to deal with a column with the dtype \"object\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove na's\nday_of_month_landslides = day_of_month_landslides.dropna()\n\n# plot the day of the month\nsns.distplot(day_of_month_landslides, kde=False, bins=31)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.)"},{"metadata":{},"cell_type":"markdown","source":"# Character Encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# helpful character encoding module\nimport chardet\n\n# set seed for reproducibility\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Character encodings** are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding than the one it was originally written in, you ended up with scrambled text called \"mojibake\".\n\n\n* There are lots of different character encodings, but the main one you need to know is UTF-8. UTF-8 is **the** standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble."},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# check to see what datatype it is\ntype(before)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The other data is the [bytes](https://docs.python.org/3.1/library/functions.html#bytes) data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it's in:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"utf-8\", errors=\"replace\")\n\n# check the type\ntype(after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at what the bytes look like\nafter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert it back to utf-8\nprint(after.decode(\"utf-8\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* However, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in."},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to decode our bytes with the ascii encoding\nprint(after.decode(\"ascii\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n\n\n* This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next."},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with a string\nbefore = \"This is the euro symbol: €\"\n\n# encode it to a different encoding, replacing characters that raise errors\nafter = before.encode(\"ascii\", errors=\"replace\")\n\n# convert it back to utf-8\nprint(after.decode(\"ascii\"))\n\n# We've lost the original underlying byte string! It's been \n# replaced with the underlying byte string for the unknown character :(","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to read in a file not in UTF-8\ntry:\n    kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")\nexcept Exception as e:\n    print (\"{}: {}\".format(type(e), e))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Notice that we get the same `UnicodeDecodeError` we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the `chardet` module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n\n\n* One reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the first ten thousand bytes to guess the character encoding\nwith open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\n\n# check what the character encoding might be\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inconsistent Data Entry"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# helpful modules\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nimport chardet\n\n# read in all our data\nprofessors = pd.read_csv(\"../input/pakistan-intellectual-capital/pakistan_intellectual_capital.csv\")\n\n# set seed for reproducibility\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"professors.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to lower case\nprofessors['Country'] = professors['Country'].str.lower()\n# remove trailing white spaces\nprofessors['Country'] = professors['Country'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It does look like there is another inconsistency: 'southkorea' and 'south korea' should be the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to use the [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) package to help identify which strings are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, it’s fun!)\n\n* **Fuzzy matching**: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n\nFuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"d.i khan\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the top 10 closest matches to \"south korea\"\nmatches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n# take a look at them\nmatches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [match[0] for match in matches if match[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\nreplace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}