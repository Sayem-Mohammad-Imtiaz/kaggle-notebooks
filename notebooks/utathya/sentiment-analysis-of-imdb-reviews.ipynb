{"cells":[{"outputs":[],"metadata":{"_uuid":"c0a1caea37fd4544b759ce7bd9196c4037e77130","_cell_guid":"3bef449f-3314-4900-8e56-d30cd137b0ba"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":25},{"metadata":{},"cell_type":"markdown","source":"**Library Section**"},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"################################################# import libraries ###########################################\n\nimport pandas as pd\nimport os\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport operator\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import AgglomerativeClustering","execution_count":26},{"metadata":{},"cell_type":"markdown","source":"**Modules Section**\n\nThis section contains all the functions created for the Sentiment Analysis"},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def rem_sw(df):\n    # Downloading stop words\n    stop_words = set(stopwords.words('english'))\n\n    # Removing Stop words from training data\n    count = 0\n    for sentence in df:\n        sentence = [word for word in sentence.lower().split() if word not in stop_words]\n        sentence = ' '.join(sentence)\n        df.loc[count] = sentence\n        count+=1\n    return(df)","execution_count":27},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def rem_punc(df):\n    count = 0\n    for s in df:\n        cleanr = re.compile('<.*?>')\n        s = re.sub(r'\\d+', '', s)\n        s = re.sub(cleanr, '', s)\n        s = re.sub(\"'\", '', s)\n        s = re.sub(r'\\W+', ' ', s)\n        s = s.replace('_', '')\n        df.loc[count] = s\n        count+=1\n    return(df)","execution_count":28},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def lemma(df):\n\n    lmtzr = WordNetLemmatizer()\n\n    count = 0\n    stemmed = []\n    for sentence in df:    \n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(lmtzr.lemmatize(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":29},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def stemma(df):\n\n    stemmer = SnowballStemmer(\"english\") #SnowballStemmer(\"english\", ignore_stopwords=True)\n\n    count = 0\n    stemmed = []\n    for sentence in df:\n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(stemmer.stem(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":30},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def get_feature(df, number):\n    \n    feature_list = []\n    # create an instance for tree feature selection\n    tree_clf = ExtraTreesClassifier()\n\n    # first create arrays holding input and output data\n\n    # Vectorizing Train set\n    cv = CountVectorizer(analyzer='word')\n    x_train = cv.fit_transform(df['review'])\n\n    # Creating an object for Label Encoder and fitting on target strings\n    le = LabelEncoder()\n    y = le.fit_transform(df['label'])\n\n    # fit the model\n    tree_clf.fit(x_train, y)\n    \n    # Preparing variables\n    importances = tree_clf.feature_importances_\n    feature_names = cv.get_feature_names()\n    feature_imp_dict = dict(zip(feature_names, importances))\n    sorted_features = sorted(feature_imp_dict.items(), key=operator.itemgetter(1), reverse=True)\n    indices = np.argsort(importances)[::-1]\n\n    # Create the feature list\n    for f in range(number):\n        feature_list.append(sorted_features[f][0])\n    \n    return(feature_list)","execution_count":31},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def print_feature(df):\n    \n    # create an instance for tree feature selection\n    tree_clf = ExtraTreesClassifier()\n\n    # first create arrays holding input and output data\n\n    # Vectorizing Train set\n    cv = CountVectorizer(analyzer='word')\n    x_train = cv.fit_transform(df['review'])\n\n    # Creating an object for Label Encoder and fitting on target strings\n    le = LabelEncoder()\n    y = le.fit_transform(df['label'])\n\n    # fit the model\n    tree_clf.fit(x_train, y)\n\n    # Preparing variables\n    importances = tree_clf.feature_importances_\n    feature_names = cv.get_feature_names()\n    feature_imp_dict = dict(zip(feature_names, importances))\n    sorted_features = sorted(feature_imp_dict.items(), key=operator.itemgetter(1), reverse=True)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n    for f in range(20):\n        print(\"feature %d : %s (%f)\" % (indices[f], sorted_features[f][0], sorted_features[f][1]))\n\n    # Plot the feature importances of the forest\n    plt.figure(figsize = (20,20))\n    plt.title(\"Feature importances\")\n    plt.bar(range(100), importances[indices[:100]],\n           color=\"r\", align=\"center\")\n    plt.xticks(range(100), sorted_features[:100], rotation=90)\n    plt.xlim([-1, 100])\n    plt.show()\n\n    return()","execution_count":32},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def get_bestrf(X, y):\n    parameters = [\n        {\n            \"n_estimators\":[5, 10, 20, 50, 100],\n            \"criterion\":['gini', 'entropy']\n        }\n    ]\n\n    best_clf = GridSearchCV(clf, parameters, scoring=\"accuracy\", verbose=5, n_jobs=4)\n\n    best_clf.fit(X, y)\n    \n    return(best_clf.best_estimator_.n_estimators, best_clf.best_estimator_.criterion)","execution_count":33},{"metadata":{},"cell_type":"markdown","source":"**Phase 1: Data Cleaning**\n\nFirst task is to upload and clean the data."},{"outputs":[],"metadata":{},"cell_type":"code","source":"############################## Loading Data #########################################\ndf_master = pd.read_csv(\"../input/imdb_master.csv\", encoding='latin-1', index_col = 0)\n\n##################### Seperating the data in to train and test set #############################\nimdb_train = df_master[[\"review\", \"label\"]][df_master.type.isin(['train'])].reset_index(drop=True)\nimdb_test = df_master[[\"review\", \"label\"]][df_master.type.isin(['test'])].reset_index(drop=True)\n\n##################################### Removing Stop words from training data ##################################\n\nimdb_train['review'] = rem_sw(imdb_train['review'])\n##################################### Removing Stop words from testing data ###################################\n\nimdb_test['review'] = rem_sw(imdb_test['review'])\n###################################### Removing punctuations from Train set ##################################\n\nimdb_train['review'] = rem_punc(imdb_train['review'])\n###################################### Removing punctuations from Test set ###################################\n\nimdb_test['review'] = rem_punc(imdb_test['review'])\n############################################### Stemming Train set ##########################################\n\nimdb_train['review'] = lemma(imdb_train['review'])\nimdb_train['review'] = stemma(imdb_train['review'])\n############################################### Stemming Test set ###########################################\n\nimdb_test['review'] = lemma(imdb_test['review'])\nimdb_test['review'] = stemma(imdb_test['review'])\n\n################################# Visualising the best features ################################\nprint_feature(imdb_train)","execution_count":34},{"metadata":{},"cell_type":"markdown","source":"**Phase 2: Exploration**\n\nVisualising the best features of train and test"},{"outputs":[],"metadata":{},"cell_type":"code","source":"################################# Training Set ################################\nprint_feature(imdb_train)","execution_count":35},{"outputs":[],"metadata":{},"cell_type":"code","source":"############################################# Test set #############################################\n\nprint_feature(imdb_test)","execution_count":36},{"outputs":[],"metadata":{},"cell_type":"code","source":"###################################### Negative set frequency of train and test combined ################################\n\n# Creating a frequency dataframe of stemmed train and test data set\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\n# Vectorizing negative reviews set\nvect = CountVectorizer(stop_words = 'english', analyzer='word')\nvect_pos = vect.fit_transform(df_freq[df_freq.label.isin(['neg'])].review)\n\n# Visualising the high frequency words for negative set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\ndf_freq.nlargest(10, 'frequency')","execution_count":37},{"outputs":[],"metadata":{},"cell_type":"code","source":"###################################### Positive set frequency of train and test combined ################################\n\n# Creating a frequency dataframe of stemmed train and test data set\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\n# Vectorizing pos reviews set\nvect = CountVectorizer(stop_words = 'english', analyzer='word')\nvect_pos = vect.fit_transform(df_freq[df_freq.label.isin(['pos'])].review)\n\n# Visualising the high frequency words for positive set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\ndf_freq.nlargest(10, 'frequency')","execution_count":38},{"outputs":[],"metadata":{},"cell_type":"code","source":"######################### Lowest and highest frequency words ###########################\n\n# Creating a frequency dataframe of stemmed train and test data set\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\n# Vectorizing complete review set\nvect = CountVectorizer(stop_words = 'english', analyzer='word')\nvect_pos = vect.fit_transform(df_freq.review)\n\n# Visualising the high and low frequency words for complete set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\nprint(df_freq.nlargest(1, 'frequency'), sep='\\n')\nprint(df_freq.nsmallest(1, 'frequency'), sep='\\t')","execution_count":39},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## WordCloud Positive Train & Test set ##################################\n\n# Creating a list of train and test data to analyse\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\nimdb_list = df_freq[\"review\"][df_freq.label.isin(['pos'])].unique().tolist()\nimdb_bow = \" \".join(imdb_list)\n\n# Create a word cloud for psitive words\nimdb_wordcloud = WordCloud().generate(imdb_bow)\n\n# Show the created image of word cloud\nplt.figure(figsize=(20, 20))\nplt.imshow(imdb_wordcloud)\nplt.show()","execution_count":40},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## WordCloud Negative Train & Test set ##################################\n\n# Creating a list of train and test data to analyse\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\nimdb_list = df_freq[\"review\"][df_freq.label.isin(['neg'])].unique().tolist()\nimdb_bow = \" \".join(imdb_list)\n\n# Create a word cloud for negative words\nimdb_wordcloud = WordCloud().generate(imdb_bow)\n\n# Show the created image of word cloud\nplt.figure(figsize=(20, 20))\nplt.imshow(imdb_wordcloud)\nplt.show()","execution_count":41},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## Histogram Positive Train & Test set ##################################\n\n#Combining cleaned train and test data\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\n# Creating an object for Count vectorizer and fitting it to positive dataset\nhist_cv = CountVectorizer(stop_words = 'english', analyzer='word')\nhist_pos = hist_cv.fit_transform(df_freq[df_freq.label.isin(['pos'])].review)\n\n# Visualising the histogram for positive reviews only from train and dataset\ndata = hist_pos.sum(axis=0).tolist()\nbinwidth = 2500\nplt.hist(data[0], bins=range(min(data[0]), max(data[0]) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":42},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Zooming in on below 100 frequency words\n\nzoom_data = [f for f in data[0] if f <= 100]\nbinwidth = 5\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.xlim(0, 100)\nplt.show()","execution_count":43},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at above 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f > 100]\nbinwidth = 2500\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":44},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## Histogram Negative Train & Test set ##################################\n\n#Combining cleaned train and test data\ndf_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\n# Creating an object for Count vectorizer and fitting it to positive dataset\nhist_cv = CountVectorizer(stop_words = 'english', analyzer='word')\nhist_neg = hist_cv.fit_transform(df_freq[df_freq.label.isin(['neg'])].review)\n\n# Visualising the histogram for positive reviews only from train and dataset\ndata = hist_neg.sum(axis=0).tolist()\nbinwidth = 2500\nplt.hist(data, bins=range(min(data[0]), max(data[0]) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":45},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at less than 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f <= 100]\nbinwidth = 5\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":46},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at above 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f > 100]\nbinwidth = 2500\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":47},{"metadata":{},"cell_type":"markdown","source":"**Visualising after feature selection**\n\nCreating the required dataset"},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"df_freq = pd.concat([imdb_train, imdb_test], ignore_index = True)\n\nword_list = get_feature(df_freq, 1000)\n\n# Removing non prefered words from training and test combined data\ncount = 0\nfor sentence in df_freq['review']:\n    sentence = [word for word in sentence.lower().split() if word in word_list]\n    sentence = ' '.join(sentence)\n    df_freq.loc[count, 'review'] = sentence\n    count+=1","execution_count":48},{"metadata":{},"cell_type":"markdown","source":"**Visualisation**"},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## WordCloud Positive Train & Test set post feature selection ##################################\n\n# Creating a list of train and test data to analyse\nimdb_list = df_freq[\"review\"][df_freq.label.isin(['pos'])].unique().tolist()\nimdb_bow = \" \".join(imdb_list)\n\n# Create a word cloud for psitive words\nimdb_wordcloud = WordCloud().generate(imdb_bow)\n\n# Show the created image of word cloud\nplt.figure(figsize=(20, 20))\nplt.imshow(imdb_wordcloud)\nplt.show()","execution_count":49},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## WordCloud Negative Train & Test set post feature selection ##################################\n\n# Creating a list of ham data only to analyse\nimdb_list = df_freq[\"review\"][df_freq.label.isin(['neg'])].unique().tolist()\nimdb_bow = \" \".join(imdb_list)\n\n# Create a word cloud for ham\nimdb_wordcloud = WordCloud().generate(imdb_bow)\n\n# Show the created image of word cloud\nplt.figure(figsize=(20, 20))\nplt.imshow(imdb_wordcloud)\nplt.show()","execution_count":50},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## Histogram Positive Train & Test set post feature selection ##################################\n\n# Creating an object for Count vectorizer and fitting it to positive dataset\nhist_cv = CountVectorizer(stop_words = 'english', analyzer='word')\nhist_pos = hist_cv.fit_transform(df_freq[df_freq.label.isin(['pos'])].review)\n\n# Visualising the histogram for positive reviews only from train and dataset\ndata = hist_pos.sum(axis=0).tolist()\nbinwidth = 2500\nplt.hist(data, bins=range(min(data[0]), max(data[0]) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":51},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at less than 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f <= 100]\nbinwidth = 5\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=False)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":52},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at above 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f > 100]\nbinwidth = 2500\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=False)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":53},{"outputs":[],"metadata":{},"cell_type":"code","source":"########################## Histogram Negative Train & Test set post feature selection ##################################\n\n# Creating an object for Count vectorizer and fitting it to positive dataset\nhist_cv = CountVectorizer(stop_words = 'english', analyzer='word')\nhist_pos = hist_cv.fit_transform(df_freq[df_freq.label.isin(['neg'])].review)\n\n# Visualising the histogram for positive reviews only from train and dataset\ndata = hist_pos.sum(axis=0).tolist()\nbinwidth = 2500\nplt.hist(data, bins=range(min(data[0]), max(data[0]) + binwidth, binwidth), log=True)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":54},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at less than 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f <= 100]\nbinwidth = 5\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=False)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":55},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Having a look at above 100 frequency words more closely\n\nzoom_data = [f for f in data[0] if f > 100]\nbinwidth = 2500\nplt.hist(zoom_data, bins=range(min(zoom_data), max(zoom_data) + binwidth, binwidth), log=False)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of instances\")\nplt.show()","execution_count":56},{"metadata":{},"cell_type":"markdown","source":"**Phase 3: Model Building post Hypothesis Testing (Naive Bayes for Supervised Learning and K-Means for Unsupervised Learning)**"},{"outputs":[],"metadata":{},"cell_type":"code","source":"imdb_unsup = df_master[[\"review\", \"label\"]][df_master.label.isin(['unsup'])].reset_index(drop=True)\n\n# Cleaning Unlabelled data\n\nimdb_unsup['review'] = rem_sw(imdb_unsup['review'])\nimdb_unsup['review'] = rem_punc(imdb_unsup['review'])\nimdb_unsup['review'] = lemma(imdb_unsup['review'])\nimdb_unsup['review'] = stemma(imdb_unsup['review'])\n\n# Vectorizing unlabelled reviews set\nvect = CountVectorizer(stop_words = 'english', analyzer='word')\nvect_pos = vect.fit_transform(imdb_unsup.review)\n\n# Creating a dataframe for the high frequency words for unlabelled reviews set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\n\n# Removing high frequency and low frequency data for more accuracy\nword_list = df_freq.nlargest(100, 'frequency').index\nword_list = word_list.append(df_freq.nsmallest(43750, 'frequency').index)\n\n# Removing unwanted words based on word_list from unlabelled data\ncount = 0\nfor sentence in imdb_unsup['review']:\n    sentence = [word for word in sentence.lower().split() if word not in word_list]\n    sentence = ' '.join(sentence)\n    imdb_unsup.loc[count, 'review'] = sentence\n    count+=1\n\n################################## Preparing dataframe for model ##############################\n\n# Creating df_algo dataframe which will be used for hypothesis testing\ndf_algo = pd.concat([imdb_train, imdb_test], keys=['train', 'test'])\ndf_algo = df_algo.reset_index(col_level=1).drop(['level_1'], axis=1)\n\n# Cleaning the dataset\ndf_algo['review'] = rem_sw(df_algo['review'])\ndf_algo['review'] = rem_punc(df_algo['review'])\ndf_algo['review'] = lemma(df_algo['review'])\ndf_algo['review'] = stemma(df_algo['review'])\n\n# df_algo = pd.read_csv(\"clean_algo.csv\", encoding='latin-1', index_col = 0) # Uncomment this line to load from csv\n\n################################### Removing non feature words ###############################\n\n# Creating the feature word_list\n# Selecting 14440 feature selected words based on 80-20 rule\nword_list = get_feature(df_algo[['review', 'label']], 14440)\n\n# Removing non prefered words from training and test combined data\ncount = 0\nfor sentence in df_algo['review']:\n    sentence = [word for word in sentence.lower().split() if word in word_list]\n    sentence = ' '.join(sentence)\n    df_algo.loc[count, 'review'] = sentence\n    count+=1\n\n################################## Splitting with feature selection data ###############################a\n\n# Vectorising the required data\nvect_algo = TfidfVectorizer(stop_words='english', analyzer='word')\nvect_algo.fit(df_algo.review)\nXf_train = vect_algo.transform(df_algo[df_algo['level_0'].isin(['train'])].review)\nXf_test = vect_algo.transform(df_algo[df_algo['level_0'].isin(['test'])].review)\n\n# Encoding target data\n# Creating an object and fitting on target strings\nle = LabelEncoder()\nyf_train = le.fit_transform(df_algo[df_algo['level_0'].isin(['train'])].label)\nyf_test = le.fit_transform(df_algo[df_algo['level_0'].isin(['test'])].label)\n\n########################################### Naive Bayes #########################################\n\n# Fit the Naive Bayes classifier model to the object\nclf = MultinomialNB()\nclf.fit(Xf_train, yf_train)\n\n# predict the outcome for testing data\npredictions = clf.predict(Xf_test)\n\n# check the accuracy of the model\naccuracy = accuracy_score(yf_test, predictions)\nprint(\"Observation: Naive Bayes Classification gives an accuracy of %.2f%% on the testing data\" %(accuracy*100))","execution_count":57},{"metadata":{},"cell_type":"markdown","source":"**Unsupervised Learning: Kmeans**\n\nSelecting 100 largest and 43750 lowest frequency words for Unsupervised learning. The numbers have been decided based on numerous iterations done during Hypothesis testing. These numbers gave the most distinct clusters."},{"outputs":[],"metadata":{},"cell_type":"code","source":"##################################### Using K-means to create two clusters ##################################### \n\n# Vectorizing dataset\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(imdb_unsup.review)\n \n# Creating a k-means object and fitting it to target variable\ntrue_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n \n# Visualising the 2 clusters\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind])","execution_count":58},{"outputs":[],"metadata":{},"cell_type":"code","source":"# Prediction for test set using Kmeans clusters\nY = vectorizer.transform(imdb_test.review)\nprediction = model.predict(Y)\n\n# Actual results of test sets for comparison\nle = LabelEncoder()\ny = le.fit_transform(imdb_test.label)\n\n# check the accuracy of the model\naccuracy = accuracy_score(y, prediction)\nif accuracy < 0.5:\n    accuracy = 1 - accuracy\nprint(\"Observation: The unsupervised learning gives an accuracy of %.2f%% on the testing data\" %(accuracy*100))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dividing the data into more clusters to enable finding more classes. Using Cluster divisions to derive movie genres from review."},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"imdb_unsup = df_master[[\"review\", \"label\"]][df_master.label.isin(['unsup'])].reset_index(drop=True)\n\n# Cleaning Unlabelled data\n\nimdb_unsup['review'] = rem_sw(imdb_unsup['review'])\nimdb_unsup['review'] = rem_punc(imdb_unsup['review'])\nimdb_unsup['review'] = lemma(imdb_unsup['review'])\nimdb_unsup['review'] = stemma(imdb_unsup['review'])\n\n# Vectorizing unlabelled reviews set\nvect = CountVectorizer(analyzer='word')\nvect_pos = vect.fit_transform(imdb_unsup.review)\n\n# Creating a dataframe for the high frequency words for unlabelled reviews set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\n\n# Removing high frequency and low frequency data for more accuracy\n\nword_list = df_freq.nlargest(100, 'frequency').index\nword_list = word_list.append(df_freq.nsmallest(43750, 'frequency').index)\n\n# Removing unwanted words based on word_list from unlabelled data\ncount = 0\nfor sentence in imdb_unsup['review']:\n    sentence = [word for word in sentence.lower().split() if word not in word_list]\n    sentence = ' '.join(sentence)\n    imdb_unsup.loc[count, 'review'] = sentence\n    count+=1\n    \n##################################### Using K-means to create clusters ##################################### \n\n# Vectorizing dataset\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(imdb_unsup.review)\n \n# Creating a k-means object and fitting it to target variable\ntrue_k = 9\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, random_state=13)\nmodel.fit(X)\n \n# Visualising the clusters\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind])","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### From above we get the following genres:-\n###### Cluster 0 - Comedy, Romance, Romantic Comedy\n###### Cluster 1 - Musical, Feel Good, Inspirational\n###### Cluster 2 - Television Series\n###### Cluster 3 - Cartoon, Animation, Disney\n###### Cluster 4 - Action, Thriller, Mystery\n###### Cluster 5 - Reality, Drama\n###### Cluster 6 - War, Political, History\n###### Cluster 7 -  Children, Adaptation\n###### Cluster 8 -  Horror, Thriller"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}