{"cells":[{"metadata":{"id":"Hpb0629DWOOQ","trusted":true},"cell_type":"code","source":"# Load the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\nfrom sklearn import decomposition\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import auc\nimport math\nimport joblib","execution_count":null,"outputs":[]},{"metadata":{"id":"KWPymtofWOOY","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"8lz3rAtAWOOZ","outputId":"214ac510-dc1a-4a85-f5e1-38f1fbe3fcb0","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"hEGujCGkWOOb","outputId":"1e25efaa-8be0-4b5e-d7ca-2ce31c5f8cf0","trusted":true},"cell_type":"code","source":"data['Churn'].value_counts().plot.bar()\n# We can see that the data set is imbalanced, i.e., not many people have churned.","execution_count":null,"outputs":[]},{"metadata":{"id":"zcrdPZI2WOOb","outputId":"042bed0b-eaec-432c-ad72-fcdf03a4ed16","trusted":true},"cell_type":"code","source":"# Get the percentage of values in 'Churn' column\n(data['Churn'].value_counts()/data['Churn'].count())*100","execution_count":null,"outputs":[]},{"metadata":{"id":"P36HnBgLWOOc"},"cell_type":"markdown","source":"## We can see that the dataset is a little imbalanced. \n1. Churned customers = 26.53%\n2. Existing customers = 73.46%\n\n## We shall use SMOTE when fitting models for classification, and see if it provides any improvement."},{"metadata":{"id":"hAU8FNPtWOOc"},"cell_type":"markdown","source":"# Data Pre-processing"},{"metadata":{"id":"5DWNN7fvWOOd","outputId":"8741b7fb-5771-46ff-d4ea-ee8dbe6b6839","trusted":true},"cell_type":"code","source":"# Check if data has missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in the dataset."},{"metadata":{"id":"_RDSdaQTWOOd","outputId":"eeed096f-cba8-464c-c502-93e40bc20d08","trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"e5QR_WeVWOOe"},"cell_type":"markdown","source":"### In the dataset, the 'TotalCharges' column has numerical values, but its type is 'object'. So we will convert it to float."},{"metadata":{"id":"DrHapEs9WOOe","outputId":"a138fcda-5c45-4e65-bb46-489e7c195c29","trusted":true},"cell_type":"code","source":"data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"0qFlH-UcWOOe"},"cell_type":"markdown","source":"### We see that there are 11 missing values in the TotalCharges column. When fitting models, we will:-\n1. First split this data into test and train sets\n2. Then impute the missing values in TotalCharges in the train set(if present in training set)\n3. Fill the missing values in test set(if any)"},{"metadata":{"id":"62X9ZJ5FWOOf"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"id":"2n9b1CmnWOOf","trusted":true},"cell_type":"code","source":"# Separate out the churned and existing customers\nchurned = data.loc[data['Churn']=='Yes']\nexisting = data.loc[data['Churn']=='No']","execution_count":null,"outputs":[]},{"metadata":{"id":"7-7-SodBWOOf","outputId":"f1970a46-8f93-43bf-e39d-d08ecfbd29ea","trusted":true},"cell_type":"code","source":"g = sns.catplot(x=\"gender\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8,palette='husl' );","execution_count":null,"outputs":[]},{"metadata":{"id":"_MjJQumOWOOg"},"cell_type":"markdown","source":"### The genders are uniformally distributed in both churning and existing coustomers. Hence, gender cannot be considered a factor which determines whether a customer will churn"},{"metadata":{"id":"cm1ZFbsGWOOg","outputId":"c6267fdf-a5f5-436b-c5a5-82cfb13eb4a0","trusted":true},"cell_type":"code","source":"g = sns.catplot(x=\"SeniorCitizen\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8, palette='YlOrBr');","execution_count":null,"outputs":[]},{"metadata":{"id":"_EzRvBcjWOOg","outputId":"b5014167-b086-4f36-f20b-6b7faf53bd8d","trusted":true},"cell_type":"code","source":"g = sns.catplot(x=\"Partner\", col=\"Churn\",\n                data=data, kind=\"count\",\n                height=5, aspect=0.8);","execution_count":null,"outputs":[]},{"metadata":{"id":"bFb_qH3pWOOh"},"cell_type":"markdown","source":"### Most of the customers who have churned, did not have a partner."},{"metadata":{"id":"9pT4J9z5WOOh"},"cell_type":"markdown","source":"## Analyze the numerical columns"},{"metadata":{"id":"JDelRmazWOOh","outputId":"9e00b398-8500-43f4-f4fe-6af43254366f","trusted":true},"cell_type":"code","source":"# Tenure vs Churn\nplt.figure(figsize=(9, 4))\nsns.boxplot(x=\"Churn\", y=\"tenure\",\n            hue=\"Churn\", palette=\"pastel\",\n            data=data).set_title(\"tenure vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"43oY0cK_WOOh"},"cell_type":"markdown","source":"### We can see that 75% of the customers who churned(in the right boxplot), have stayed with the company for less than 30 months. \n### This is less than the median of existing customers group, which is around 37 months.\n1. This tells us that most customers who left, did so relatively early.\n2. There are some customers who left after long time, around 70 months(5.8 years). There are even some outliers, exceeding even that mark."},{"metadata":{"id":"FkobbWnLWOOi","outputId":"4dea34a2-31e0-40af-da3c-c90b83228926","trusted":true},"cell_type":"code","source":"# MonthlyCharges vs Churn\nplt.figure(figsize=(9, 4))\nsns.boxplot(x=\"Churn\", y=\"MonthlyCharges\",\n            hue=\"Churn\", palette='Blues',\n            data=data, notch=True).set_title(\"MonthlyCharges vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"JW9CSJOlWOOi","outputId":"3f28beaa-a685-4d59-fabe-20e05caa78e2","trusted":true},"cell_type":"code","source":"# TotalCharges vs Churn\nplt.figure(figsize=(9, 6))\nsns.boxplot(x=\"Churn\", y=\"TotalCharges\",\n            hue=\"Churn\", palette=\"muted\",\n            data=data, notch=True).set_title(\"TotalCharges vs Churn\", fontsize=15)\nsns.despine(offset=30, trim=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"8Lqlt9SAWOOj"},"cell_type":"markdown","source":"### Many of the customers who left, were having high monthly charges. 109 customers are even outliers in Total Charges. \nThere is probably some service which costed more, but did not provide the quality as expected by the customers. \n"},{"metadata":{"id":"WlfyBCAJWOOj","outputId":"da0eb16c-79eb-4f83-ab7f-adaec99c93b9","trusted":true},"cell_type":"code","source":"churned_PS = churned[\"PhoneService\"].value_counts()\nexisting_PS = existing[\"PhoneService\"].value_counts()\ncolors = ['#99ff99','#ffcc99']\nexplode = (0, 0.1)\nfig1, axs = plt.subplots(1, 2)\n\naxs[0].pie(churned_PS, labels=churned_PS.index, autopct='%1.1f%%', shadow=True, startangle=100, colors=colors, explode=explode)\naxs[0].set_title('Churned customers')\n\ncolors = ['#ff9999','#66b3ff']\naxs[1].pie(existing_PS, labels=existing_PS.index, autopct='%1.1f%%', shadow=True, startangle=100, colors=colors, explode=explode)\naxs[1].set_title('Existing customers')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"w94vA245WOOj"},"cell_type":"markdown","source":"### Both the existing and churned customers have neraly equal distribution for phone service."},{"metadata":{"id":"G5urLn3MWOOj","outputId":"13f33998-dcb3-423d-c48c-48fee388fd4e","trusted":true},"cell_type":"code","source":"churned_IS = churned[\"InternetService\"].value_counts()\nexisting_IS = existing[\"InternetService\"].value_counts()\n\nfig, axs = plt.subplots(1, 2)\nfig.tight_layout()\n\naxs[0].pie(churned_IS, labels=churned_IS.index, autopct='%1.2f%%', shadow=None)\naxs[0].set_title('Churned customers')\n\naxs[1].pie(existing_IS, labels=existing_IS.index, autopct='%1.2f%%', shadow=None)\naxs[1].set_title('Existing customers')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4NYuj_ZLWOOk","outputId":"67cbefb9-8ad5-412d-c5a0-f892a9df5cb9","trusted":true},"cell_type":"code","source":"churned_IS","execution_count":null,"outputs":[]},{"metadata":{"id":"bXPj2hssWOOk"},"cell_type":"markdown","source":"1. Almost 70% of the churned customers were using fiber optics for their internet service. Could this have to do something with the reason for their leaving the company services?\n2. Also, nearly 94% customers who left, had even internet services. Could it be that they were dissatisfied by some of the services being provided over the internet?  "},{"metadata":{"id":"sUcWZMssWOOk","outputId":"fea0841d-d196-4086-b44d-e98292a017c6","trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(nrows=1,ncols=3,figsize = (14,4))\nfig.tight_layout(pad=3.0)\nsns.countplot(x ='MultipleLines', hue = \"Churn\", data = data, ax=ax[0],palette=\"rocket\")\nsns.countplot(x ='OnlineBackup', hue = \"Churn\", data = data, ax=ax[1],palette=\"rocket\")\nsns.countplot(x ='OnlineSecurity', hue = \"Churn\", data = data, ax=ax[2],palette=\"rocket\")\n\nfig, ax =plt.subplots(nrows=1,ncols=4,figsize = (14,4))\nfig.tight_layout(pad=3.0)\nsns.countplot(x ='DeviceProtection', hue = \"Churn\", data = data, ax=ax[0],palette=\"Set2\")\nsns.countplot(x ='TechSupport', hue = \"Churn\", data = data, ax=ax[1],palette=\"Set2\")\nsns.countplot(x ='StreamingTV', hue = \"Churn\", data = data, ax=ax[2],palette=\"Set2\")\nsns.countplot(x ='StreamingMovies', hue = \"Churn\", data = data, ax=ax[3],palette=\"Set2\")","execution_count":null,"outputs":[]},{"metadata":{"id":"U46WP7ZqWOOl"},"cell_type":"markdown","source":"1. Many customers who left, did not have online backup, security, even though 94% had inernet services.\n2. Device Protection and tech support was also not being used in the same proportion as the customers who have stayed."},{"metadata":{"id":"1CylLCmHXQlf","outputId":"4a189c58-5bd3-4f7e-d050-e57d3f4466b4","trusted":true},"cell_type":"code","source":"!pip install squarify","execution_count":null,"outputs":[]},{"metadata":{"id":"v8rnzLXpWOOl","outputId":"285a1a44-2c23-4097-ab7a-8adcc297f0c9","trusted":true},"cell_type":"code","source":"import squarify\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nplt.subplots_adjust(top=0.95, bottom=0.05, left=0.05, right=0.95, hspace=0.35)\n\nx1 = churned.groupby(['Contract']).MonthlyCharges.count().sort_values(ascending=False)\naxes[0].set_title('Churned customers contract lengths')\nsquarify.plot(sizes=x1.tolist()[:3], label=x1.index.tolist()[:3], alpha=0.6, ax=axes[0])\naxes[0].axis('off')\nx2 = existing.groupby(['Contract']).MonthlyCharges.count().sort_values(ascending=False)\naxes[1].set_title('Existing customers contract lengths')\nsquarify.plot(sizes=x2.tolist()[:3], label=x2.index.tolist()[:3], alpha=0.6, ax=axes[1])\naxes[1].axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"II4LZh-gWOOm"},"cell_type":"markdown","source":"### We can see that a very large portion of the customers who left were mostly using month-to-month contracts. This probably made it easier for them to leave, as they were not bound to the company with long term contracts."},{"metadata":{"scrolled":true,"id":"TN5-xHKNWOOm","outputId":"bbdcdd22-ef35-4011-e183-d6e7132de8ed","trusted":true},"cell_type":"code","source":"#fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nchurned_paymethods=churned[\"PaymentMethod\"].value_counts()\nexisting_paymethods=existing[\"PaymentMethod\"].value_counts()\n    \nnames=churned_paymethods.index.tolist()\nplt.pie(churned_paymethods, labels=names)\n#plt.show()\n\n# add a circle at the center\nmy_circle=plt.Circle((0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title(\"Churned customer payment methods\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qAMmbWOSWOOn"},"cell_type":"markdown","source":"# Model Fitting for classification"},{"metadata":{"id":"rjheWytgWOOn","outputId":"288f209b-aa8c-4684-f4e8-d431c95c6268","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"2K9OZu4BWOOn"},"cell_type":"markdown","source":"### We have 11 null values in 'TotalCharges' column. To impute them, we must first:-\n1. Split into test and training.\n2. Impute any missing values in training set.\n3. Then fit the test data accordingly."},{"metadata":{"id":"aEqlG1oFWOOn","outputId":"4ae05a8f-c304-48a2-eee6-fcd9e777de66","trusted":true},"cell_type":"code","source":"# Split into test and training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\ndata = data.drop(['customerID'], axis=1)\nx = data.drop(['Churn'], axis=1)\ny = data['Churn']\nX_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)\n\n# Replace the null values in 'TotalCharges' with mean value\ntrain_mean = X_train['TotalCharges'].mean()\nX_train['TotalCharges'].fillna(train_mean, inplace=True)\n\ntest_mean = X_test['TotalCharges'].mean()\nX_test['TotalCharges'].fillna(test_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"c9BGA0VP1ymz","outputId":"d2d902e5-f2c4-40fe-93dd-b39a215aa40d","trusted":true},"cell_type":"code","source":"corr = X_train[['TotalCharges', 'MonthlyCharges', 'tenure']].corr()\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"lM9_jbij68je"},"cell_type":"markdown","source":"1. Tenure has a very positive correlation with TotalCharges, but also has almost no correlation with MonthlyCharges."},{"metadata":{"id":"roLDDcyjWOOo"},"cell_type":"markdown","source":"## Borderline-SMOTE oversampling"},{"metadata":{"id":"3weHcWXzC8Uq"},"cell_type":"markdown","source":"Create the transformation pipeline"},{"metadata":{"id":"F2-FcCOnWOOo","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])\n\nnum_attribs = ['tenure', 'MonthlyCharges', 'TotalCharges']\ncat_attribs = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService',\n               'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies',\n               'Contract','PaperlessBilling','PaymentMethod']\n\n# handle_unknown = 'ignore' is needed to produce transformed test data with same dimensions as the transformed training data.\nfull_pipeline = ColumnTransformer([\n        ('num', num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(handle_unknown = 'ignore'), cat_attribs),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"emVCtDEbDCGk"},"cell_type":"markdown","source":"Transform the training data and test data"},{"metadata":{"id":"9c6ckEWaDCn3","trusted":true},"cell_type":"code","source":"data_prepared = full_pipeline.fit_transform(X_train)\ntest_data = full_pipeline.fit_transform(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"id":"ZZMXW7xvDRB_"},"cell_type":"markdown","source":"Perform the SMOTE oversampling"},{"metadata":{"id":"urt2X8oZWOOq","outputId":"82953c0d-e1ec-4074-d2bf-6d6bfea86cea","trusted":true},"cell_type":"code","source":"# Install imblearn\nimport sys\n!{sys.executable} -m pip install imblearn\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\n# summarize class distribution\ncounter = Counter(y_train)\nprint(counter)\n# transform the dataset\noversample = BorderlineSMOTE(random_state=123)\nX, y = oversample.fit_resample(data_prepared, y_train)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)","execution_count":null,"outputs":[]},{"metadata":{"id":"HtN8sW8hWOOq"},"cell_type":"markdown","source":"### Our training data is well balanced now, containing equal samples from both classes(churned and existing customers)"},{"metadata":{"id":"69rn8uSTWOOr"},"cell_type":"markdown","source":"## Principal Component Analysis to plot data"},{"metadata":{"id":"tmiMk6O7WOOs"},"cell_type":"markdown","source":"### Plot the original training data"},{"metadata":{"id":"rrQks9sYWOOs","outputId":"3a6cf033-9bfe-4612-8eec-ddb71c0e48f4","trusted":true},"cell_type":"code","source":"pca = decomposition.PCA(n_components=3)\npca.fit(data_prepared)\ndata_pca = pca.transform(data_prepared)\nfig, ax = plt.subplots()\ncolors = {'Yes':'red','No':'blue'}\nax.scatter(data_pca[:,0], data_pca[:,1], c=y_train.map(colors))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"F-0sPcAAWOOt"},"cell_type":"markdown","source":"1. The customer information are plotted w.r.t. to the first two principal components. We can see that there are very few customers who churned(red points).\n2. We also see 2 distinct clusters for customers of both kind. This suggests that there must be some very specific behaviors belonging to customers in the clusters. "},{"metadata":{"id":"_J3Nw-WxWOOt"},"cell_type":"markdown","source":"### Plot the oversampled data"},{"metadata":{"id":"oM3g9DfyWOOt","outputId":"e105fba9-4798-4fcd-8813-cdf28ba8b2c2","trusted":true},"cell_type":"code","source":"pca = decomposition.PCA(n_components=3)\npca.fit(X)\nX_pca = pca.transform(X)\nfig, ax = plt.subplots()\ncolors = {'Yes':'red','No':'blue'}\ny_series = pd.Series(y)\nax.scatter(X_pca[:,0], X_pca[:,1], c=y_series.map(colors))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ph4g8dyWWOOu"},"cell_type":"markdown","source":"1. We can see that the churned customers(red points) are in much larger number now.\n2. Also, notice that the red points are generated in those places where they were in much larger density in the first plot. Hence, Borderline SMOTE makes sure not to generate outlier like samples, as it will increase noise in the dataset."},{"metadata":{},"cell_type":"markdown","source":"### Loading the models that I had already trained, so that they do not have to run again on Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the models\nRF_model = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_RF_model.pkl\")\nknn_model = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_knn_model.pkl\")\nlog_model = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_logistic_model.pkl\")\nnb_model = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_naive_bayes_model.pkl\")\n\nRF_basemodel = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_RF_basemodel.pkl\")\nknn_basemodel = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_knn_basemodel.pkl\")\nlog_basemodel = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_logistic_basemodel.pkl\")\nnb_basemodel = joblib.load(\"../input/pre-trained-models-for-customer-churn/telco_naive_bayes_basemodel.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ## Since it is more important to classify the customers who might churn, we will focus more on precision and recall of the models, and select the final model based on that."},{"metadata":{"id":"bY3zQR-gWOOu"},"cell_type":"markdown","source":"## Support Vector Machine classifier"},{"metadata":{"id":"fyDstIX5WOOw","trusted":true},"cell_type":"code","source":"def tune_svm(x,y):\n  param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001],'kernel': ['linear', 'rbf']}\n  grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n  grid.fit(x,y)\n  return grid\n\nsvm_model = tune_svm(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"2uVyGoqHsWmf","outputId":"31db77ba-b797-4249-cd4b-b010e0333373","trusted":true},"cell_type":"code","source":"joblib.dump(svm_model, \"telco_svm_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"id":"S-8Qhk5muMt0","outputId":"9511a4ee-a32f-42b4-ccb1-29b787e6fad6","trusted":true},"cell_type":"code","source":"y_pred = svm_model.predict(test_data)\nprint (\"best train accuracy\", svm_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"dZPpQBP1u_xw","outputId":"e44350f1-76ff-4c8e-cb1c-c0c4046f3c57","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"fJWKzxuxxcKz"},"cell_type":"markdown","source":"### SVM without SMOTE"},{"metadata":{"id":"2g4vCXHlxfy3","outputId":"023d743f-7430-48a2-9c69-5d1dc93a210f","trusted":true},"cell_type":"code","source":"grid = tune_svm(data_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"zNUTLQPU01b9","outputId":"f28faf11-3bb0-40d6-db59-915765af91d4","trusted":true},"cell_type":"code","source":"y_pred = grid.predict(test_data)\nprint (\"best train accuracy\", grid.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"z8v84daB1T0J","outputId":"0ad04aae-a5bc-47ed-c036-bd7d43cc33fa","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"pfLyw08NCczn"},"cell_type":"markdown","source":"## K nearest neighbors"},{"metadata":{"id":"Sr2PhR4ACtlN","outputId":"920b8a10-423a-42c9-d63f-c085aa7f3a55","trusted":true},"cell_type":"code","source":"# knn = KNeighborsClassifier()\n# k_range = list(range(1, 31))\n# param_grid = dict(n_neighbors=k_range)\n# grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n# grid.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"id":"c4G9KOFkvW4K","trusted":true},"cell_type":"code","source":"# knn_model = grid\n# import joblib\n# joblib.dump(knn_model, \"telco_knn_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"id":"nJ-pvxObDBXU","outputId":"92f757a5-4d4c-4fa3-e9c7-49a359078fc5","trusted":true},"cell_type":"code","source":"knn_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"xQJTP_tMEY58","trusted":true},"cell_type":"code","source":"y_pred = knn_model.predict(test_data)\nprint (\"best train accuracy\", knn_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"iaKIq_BTGnnh","outputId":"04a87e50-f923-4563-a94d-aa49965d12f6","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"fi671c-Ek5gS"},"cell_type":"markdown","source":"### KNN Without SMOTE sampling"},{"metadata":{"id":"tvjngB0Xk-DY","outputId":"b7f3ed0e-2c52-4d07-98b4-9e012935842c","trusted":true},"cell_type":"code","source":"# knn = KNeighborsClassifier()\n# k_range = list(range(1, 31))\n# param_grid = dict(n_neighbors=k_range)\n# grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n# grid.fit(data_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"YILc3NU7v6VR","outputId":"5627db43-623c-47e6-cab4-d97aa83e3c37","trusted":true},"cell_type":"code","source":"y_pred = knn_basemodel.predict(test_data)\nprint (\"best train accuracy\", knn_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"WzMWssQfvqxy","outputId":"d8a820ef-f742-4a9d-a7c6-478d595414f7","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"ewpBVHer_Bn4"},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"id":"sYpR4bs3_KxK","trusted":true},"cell_type":"code","source":"def tune_log_reg(x,y): \n  logistic = linear_model.LogisticRegression(solver='liblinear')\n\n  # Create regularization penalty space\n  penalty = ['l1', 'l2']\n\n  # Create regularization hyperparameter space\n  C = [100, 10, 1.0, 0.1, 0.01]\n\n  # Create hyperparameter options\n  hyperparameters = dict(C=C, penalty=penalty)\n\n  clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n  best_model = clf.fit(x, y)\n  return best_model\n","execution_count":null,"outputs":[]},{"metadata":{"id":"gojAoLVmmmrF","trusted":true},"cell_type":"code","source":"#best_model = tune_log_reg(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"K2A-Y6oDES0D","outputId":"30fb5190-6f82-4a09-c262-d75df7a68734","trusted":true},"cell_type":"code","source":"# logistic_model = best_model\n# import joblib\n# joblib.dump(logistic_model, \"telco_logistic_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"id":"7zIghLbiA9ck","outputId":"e809570e-6245-4154-be7e-7da7153f9ece","trusted":true},"cell_type":"code","source":"log_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"-R-8HXIGDCP7","outputId":"8c4d950c-f8fc-47c4-e780-4f78dd8a75d3","trusted":true},"cell_type":"code","source":"y_logistic_pred = log_model.predict(test_data)\nprint (\"best train accuracy\", log_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"KhnnQ604Dsq_","outputId":"a6613e26-5783-44b0-f646-9317f383be64","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_logistic_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"_y9ssOxdmJZu"},"cell_type":"markdown","source":"### Logistic regression without SMOTE sampling"},{"metadata":{"id":"6C3FpSw7mPrO","trusted":true},"cell_type":"code","source":"#best_model = tune_log_reg(data_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"vs06J46MnJzZ","outputId":"9797ea4b-6112-4b8a-e156-6c285717ab3d","trusted":true},"cell_type":"code","source":"y_pred = log_basemodel.predict(test_data)\nprint (\"best train accuracy\", log_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"LH-MAZX0nWa2","outputId":"f197b384-aa1c-4a5c-872b-f9f0f7a4ee4d","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"eKhgQag8GNG5"},"cell_type":"markdown","source":"# Naive Bayes Classifier"},{"metadata":{"id":"zwIERzV3GMS8","outputId":"2f516630-2b9f-495a-f3b5-027037e07035","trusted":true},"cell_type":"code","source":"# #Create a Gaussian Classifier\n# model = GaussianNB()\n\n# # Train the model using the training sets\n# model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"-PCzIau_G3EB","outputId":"cddee80b-95af-402a-d06e-a5c8112e2a16","trusted":true},"cell_type":"code","source":"y_naive_pred = nb_model.predict(test_data)\naccuracy_score(y_test, y_naive_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"R5lP7OcwHX_A","outputId":"e60b1471-282f-4ea3-a521-af2dc0bd815f","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_naive_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"VsTkDVwrn_nj"},"cell_type":"markdown","source":"### Naive Bayes without SMOTE"},{"metadata":{"id":"023AwDaDnpeg","outputId":"76ebf32a-9623-41d4-d2b3-e888ca796119","trusted":true},"cell_type":"code","source":"# #Create a Gaussian Classifier\n# model = GaussianNB()\n\n# # Train the model using the training sets\n# model.fit(data_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"KqIU23HMnz41","outputId":"1c17649e-5eef-4a2a-ba24-0557909aa9d3","trusted":true},"cell_type":"code","source":"y_naive_pred = nb_basemodel.predict(test_data)\naccuracy_score(y_test, y_naive_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"2dIjhplcn56c","outputId":"b72ff174-a844-4775-c785-ce8d3ff7ca48","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_naive_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"nbyrg04tHrbo"},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"id":"5hm8JJAgHopk","trusted":true},"cell_type":"code","source":"def tune_random_forest(x,y):\n  model_params = {\n      'n_estimators': [50, 150, 250],\n      'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n      'min_samples_split': [2, 4, 6]\n  }\n\n  # create random forest classifier model\n  rf_model = RandomForestClassifier(random_state=1, oob_score = True)\n\n  # set up grid search meta-estimator\n  clf = GridSearchCV(rf_model, model_params, cv=5)\n\n  # train the grid search meta-estimator to find the best mode\n  clf.fit(x, y)\n  return clf\n\n#RF_model = tune_random_forest(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"LsIKtYanmAcT","outputId":"4e95d079-899e-4e82-8f97-3e7deafdc224","trusted":true},"cell_type":"code","source":"RF_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"tCicyMbTmDVN","outputId":"236b2e3d-68ac-4fd0-eda6-6e404b7e3171","trusted":true},"cell_type":"code","source":"y_forest_pred = RF_model.predict(test_data)\nprint (\"best train accuracy\", RF_model.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"YOTVq5DWmYqo","outputId":"87aa238d-0d80-4839-b04c-ea1a2af38be9","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_forest_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"b2HxthPkoLOG"},"cell_type":"markdown","source":"### RF without SMOTE"},{"metadata":{"id":"GcO_uGD_oO5m","trusted":true},"cell_type":"code","source":"#RF_basemodel = tune_random_forest(data_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"TlUC_qXHvp0a","outputId":"b42e3cf1-5d66-413b-e28b-6a89d8909531","trusted":true},"cell_type":"code","source":"y_forest_pred = RF_basemodel.predict(test_data)\nprint (\"best train accuracy\", RF_basemodel.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"83MfwMrZwCB_","outputId":"fb0bd983-3068-4a5f-c21f-7c10cf40e4c3","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_forest_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"ZsLhmRVR_7Zj"},"cell_type":"markdown","source":"# Neural Network Classifier"},{"metadata":{"id":"RTM8rRW8AAJz","outputId":"9b860280-6ebc-4a38-de43-cecbbbeb1f98","trusted":true},"cell_type":"code","source":"def FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes):\n    layers = []\n\n    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n    nodes = first_layer_nodes\n    for i in range(1, n_layers+1):\n        layers.append(math.ceil(nodes))\n        nodes = nodes + nodes_increment\n\n    return layers\n\ndef createmodel(n_layers, first_layer_nodes, last_layer_nodes, activation_func):\n    model = Sequential()\n    n_nodes = FindLayerNodesLinear(n_layers, first_layer_nodes, last_layer_nodes)\n    for i in range(1, n_layers):\n        if i==1:\n            model.add(Dense(first_layer_nodes, input_dim=X.shape[1], activation=activation_func))\n        else:\n            model.add(Dense(n_nodes[i-1], activation=activation_func))\n\n    #Finally, the output layer should have a single node in binary classification\n    model.add(Dense(1, activation=activation_func))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = [\"accuracy\"]) #note: metrics could also be 'mse'\n\n    return model\n\n#Wrap model into scikit-learn\nmodel =  KerasClassifier(build_fn=createmodel, verbose = False)\n\nactivation_funcs = ['sigmoid', 'relu', 'tanh']\n#loss_funcs = ['binary_crossentropy','hinge']\nparam_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4], \n                  activation_func = activation_funcs, batch_size = [100], epochs = [20,60])\nMLP = GridSearchCV(estimator = model, param_grid = param_grid)\nMLP.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"CP9lBblwMcJw","outputId":"d240471b-a648-40f2-95c0-9699888c76cb","trusted":true},"cell_type":"code","source":"MLP.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"DauyZ5cCMm6p","outputId":"b47eea73-0278-4f89-b693-71b271ee5455","trusted":true},"cell_type":"code","source":"y_mlp_pred = MLP.predict(test_data)\nprint (\"best train accuracy\", MLP.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"3vXQllkgM9nD","outputId":"c35a99d1-83f2-4051-f573-36f4af4e0154","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"mZOvwgLoohtf"},"cell_type":"markdown","source":"### Neural Network without SMOTE"},{"metadata":{"id":"oKovG0LQomNz","outputId":"c30a6720-2ee7-4794-e6e2-e51a20de2d23","trusted":true},"cell_type":"code","source":"#Wrap model into scikit-learn\nmodel =  KerasClassifier(build_fn=createmodel, verbose = False)\n\nactivation_funcs = ['sigmoid', 'relu', 'tanh']\n#loss_funcs = ['binary_crossentropy','hinge']\nparam_grid = dict(n_layers=[2,3], first_layer_nodes = [64,32,16], last_layer_nodes = [4], \n                  activation_func = activation_funcs, batch_size = [100], epochs = [20,60])\nbase_mlp = GridSearchCV(estimator = model, param_grid = param_grid)\nbase_mlp.fit(data_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"uknhqQSdrGzi","outputId":"8d22faa5-164c-420d-e50f-fc26d357c921","trusted":true},"cell_type":"code","source":"# Classification accuracy on test data\ny_pred = base_mlp.predict(test_data)\nprint (\"best train accuracy\", base_mlp.best_score_)\nprint (\"test accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"0mED7CYyQxOK"},"cell_type":"markdown","source":"# Precision Recall AUC plot"},{"metadata":{"id":"YddeOZiy9Wzu","outputId":"a02b9be0-6a96-4dc8-c476-b64aefb87b78","trusted":true},"cell_type":"code","source":"# Create the SVM model with the best parameters again, as 'probability=True' was not there while hyperparameter tuning,\n# which is needed for the function 'predict_proba'. \nprint (svm_model.best_params_)\nprint (grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"1ypuwoMUAIS1","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(kernel='rbf', C=1, gamma=1, probability=True).fit(X,y)\nsvm_base = SVC(kernel='rbf', C=10, gamma=0.01, probability=True).fit(data_prepared,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"ep---OM_9nhd","outputId":"db297034-5a78-40b5-f880-8b575d8014cc","trusted":true},"cell_type":"code","source":"y_svm_pred = svm.predict(test_data)\naccuracy_score(y_test, y_svm_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"1wdr8pc5-FCM","outputId":"6cd0ef36-9edc-4d5d-895c-3acb3b0dddac","trusted":true},"cell_type":"code","source":"y_svm_pred = svm_base.predict(test_data)\naccuracy_score(y_test, y_svm_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"08BiJypfQuXD","outputId":"42ae497e-35ed-4696-93a2-1b5d3db9a7c1","trusted":true},"cell_type":"code","source":"pred_prob1 = RF_model.predict_proba(test_data)\npred_prob2 = knn_model.predict_proba(test_data)\npred_prob3 = log_model.predict_proba(test_data)\npred_prob4 = nb_model.predict_proba(test_data)\npred_prob5 = svm.predict_proba(test_data)\npred_prob6 = MLP.predict_proba(test_data)\n\npred_baseprob1 = RF_basemodel.predict_proba(test_data)\npred_baseprob2 = knn_basemodel.predict_proba(test_data)\npred_baseprob3 = log_basemodel.predict_proba(test_data)\npred_baseprob4 = nb_basemodel.predict_proba(test_data)\npred_baseprob5 = svm_base.predict_proba(test_data)\npred_baseprob6 = base_mlp.predict_proba(test_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"7lImJzNRDs2O"},"cell_type":"markdown","source":"## Precision Recall Curves"},{"metadata":{"id":"ghObyADpefaG","outputId":"4a3f6b04-3d98-4e1c-ddf2-db57e48b41fd","trusted":true},"cell_type":"code","source":"lr_precision1, lr_recall1, _ = precision_recall_curve(y_test, pred_prob1[:,1], pos_label='Yes')\nlr_precision2, lr_recall2, _ = precision_recall_curve(y_test, pred_prob2[:,1], pos_label='Yes')\nlr_precision3, lr_recall3, _ = precision_recall_curve(y_test, pred_prob3[:,1], pos_label='Yes')\nlr_precision4, lr_recall4, _ = precision_recall_curve(y_test, pred_prob4[:,1], pos_label='Yes')\nlr_precision5, lr_recall5, _ = precision_recall_curve(y_test, pred_prob5[:,1], pos_label='Yes')\nlr_precision6, lr_recall6, _ = precision_recall_curve(y_test, pred_prob6[:,1], pos_label='Yes')\n\nlr_precision7, lr_recall7, _ = precision_recall_curve(y_test, pred_baseprob1[:,1], pos_label='Yes')\nlr_precision8, lr_recall8, _ = precision_recall_curve(y_test, pred_baseprob2[:,1], pos_label='Yes')\nlr_precision9, lr_recall9, _ = precision_recall_curve(y_test, pred_baseprob3[:,1], pos_label='Yes')\nlr_precision10, lr_recall10, _ = precision_recall_curve(y_test, pred_baseprob4[:,1], pos_label='Yes')\nlr_precision11, lr_recall11, _ = precision_recall_curve(y_test, pred_baseprob5[:,1], pos_label='Yes')\nlr_precision12, lr_recall12, _ = precision_recall_curve(y_test, pred_baseprob6[:,1], pos_label='Yes')\n\nfig, axes = plt.subplots(2, 1, figsize=(15,20))\naxes[0].plot(lr_recall1, lr_precision1, linestyle='--',color='orange', label='Random Forest')\naxes[0].plot(lr_recall2, lr_precision2, linestyle='--',color='green', label='KNN')\naxes[0].plot(lr_recall3, lr_precision3, linestyle='--',color='blue', label='Logistic Regression')\naxes[0].plot(lr_recall4, lr_precision4, linestyle='--',color='red', label='Naive Bayes')\naxes[0].plot(lr_recall5, lr_precision5, linestyle='--',color='yellow', label='SVM')\naxes[0].plot(lr_recall6, lr_precision6, linestyle='--',color='brown', label='Neural Network')\naxes[0].set_ylim([0.0, 1])\n# title\naxes[0].set_title('SMOTE PR curve')\n# x label\naxes[0].set_xlabel('Recall')\n# y label\naxes[0].set_ylabel('Precision')\naxes[0].legend(loc='best')\n\naxes[1].plot(lr_recall7, lr_precision7, linestyle='--',color='orange', label='Random Forest')\naxes[1].plot(lr_recall8, lr_precision8, linestyle='--',color='green', label='KNN')\naxes[1].plot(lr_recall9, lr_precision9, linestyle='--',color='blue', label='Logistic Regression')\naxes[1].plot(lr_recall10, lr_precision10, linestyle='--',color='red', label='Naive Bayes')\naxes[1].plot(lr_recall11, lr_precision11, linestyle='--',color='yellow', label='SVM')\naxes[1].plot(lr_recall12, lr_precision12, linestyle='--',color='brown', label='Neural Network')\n\n# title\naxes[1].set_title('PR curve')\n# x label\naxes[1].set_xlabel('Recall')\n# y label\naxes[1].set_ylabel('Precision')\naxes[1].legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"id":"tdcUNiX4xctP"},"cell_type":"markdown","source":"### We can see that the models are performing better when we are NOT doing BorderlineSMOTE sampling.\n### Hence, we will continue with the models trained on the regular dataset(basemodels)."},{"metadata":{"id":"gkkob1xu9JJ9"},"cell_type":"markdown","source":"### Also, the probabilities generated by SVM and Random forest models can be calibrated by the 'CalibratedClassifierCV' method from sklearn."},{"metadata":{"id":"8rVBm5Gh9cZf","outputId":"f2bb95ab-3a67-4820-e646-6f13676940bf","trusted":true},"cell_type":"code","source":"# reliability diagram\nfop1, mpv1 = calibration_curve(y_test, pred_baseprob1[:,1], n_bins=10, normalize=True)\nfop2, mpv2 = calibration_curve(y_test, pred_baseprob2[:,1], n_bins=10, normalize=True)\n# plot perfectly calibrated\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot model reliability\npyplot.plot(mpv1, fop1, marker='.')\npyplot.plot(mpv2, fop2, marker='.')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zSCg209uL_1f"},"cell_type":"markdown","source":"### We can see that the both curves are mostly under the curve, meaning that the models have over-forecast i.e., the probabilites are too large."},{"metadata":{"id":"NoJziN7rJ5n_","outputId":"7e7b07bd-2805-4c0e-fba8-e84630cf53a0","trusted":true},"cell_type":"code","source":"rf_cal = RF_basemodel.best_estimator_\nrf_calibrated = CalibratedClassifierCV(rf_cal, method='sigmoid', cv=5)\nrf_calibrated.fit(data_prepared, y_train)\n# predict probabilities\nprobs_rf = rf_calibrated.predict_proba(test_data)[:, 1]\n# reliability diagram\nfop3, mpv3 = calibration_curve(y_test, probs_rf, n_bins=10, normalize=True)\n\nsvm_cal = svm_base\nsvm_calibrated = CalibratedClassifierCV(svm_cal, method='sigmoid', cv=5)\nsvm_calibrated.fit(data_prepared, y_train)\n# predict probabilities\nprobs_svm = svm_calibrated.predict_proba(test_data)[:, 1]\n# reliability diagram\nfop4, mpv4 = calibration_curve(y_test, probs_svm, n_bins=10, normalize=True)\n\n# plot perfectly calibrated\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot model reliability\npyplot.plot(mpv3, fop3, marker='.')\npyplot.plot(mpv4, fop4, marker='.')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5jJoKgOzMq92"},"cell_type":"markdown","source":"### After calibration, we can see that parts of the curves are both above and below the base curve, making them much more balanced.\n### Let's make a new PR curve with the calibrated probabilities:-"},{"metadata":{"id":"evqkB5HhNEPJ","outputId":"1c0b8bc1-a932-44ee-c2f0-cacbf54a1b58","trusted":true},"cell_type":"code","source":"lr_precision7, lr_recall7, _ = precision_recall_curve(y_test, probs_rf, pos_label='Yes')\nlr_precision8, lr_recall8, _ = precision_recall_curve(y_test, pred_baseprob2[:,1], pos_label='Yes')\nlr_precision9, lr_recall9, _ = precision_recall_curve(y_test, pred_baseprob3[:,1], pos_label='Yes')\nlr_precision10, lr_recall10, _ = precision_recall_curve(y_test, pred_baseprob4[:,1], pos_label='Yes')\nlr_precision11, lr_recall11, _ = precision_recall_curve(y_test, probs_svm, pos_label='Yes')\nlr_precision12, lr_recall12, _ = precision_recall_curve(y_test, pred_baseprob6[:,1], pos_label='Yes')\n\n#fig, axes = plt.subplots(2, 1, figsize=(12, 16))\nplt.figure(figsize=(15,10))\nplt.plot(lr_recall7, lr_precision7, linestyle='--',color='orange', label='Random Forest')\nplt.plot(lr_recall8, lr_precision8, linestyle='--',color='green', label='KNN')\nplt.plot(lr_recall9, lr_precision9, linestyle='--',color='blue', label='Logistic Regression')\nplt.plot(lr_recall10, lr_precision10, linestyle='--',color='red', label='Naive Bayes')\nplt.plot(lr_recall11, lr_precision11, linestyle='--',color='yellow', label='SVM')\nplt.plot(lr_recall12, lr_precision12, linestyle='--',color='brown', label='Neural Network')\nplt.ylim([0.0, 1])\n# title\nplt.title('PR curve')\n# x label\nplt.xlabel('Recall')\n# y label\nplt.ylabel('Precision')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"id":"OwjqBKYYiMAA"},"cell_type":"markdown","source":"## Voting Classifier"},{"metadata":{"id":"c3oSd-JRyAb8"},"cell_type":"markdown","source":"### We can also employ an ensemble learning method, and compare its performance with the previous models."},{"metadata":{"id":"XctFzUrwx9kE","trusted":true},"cell_type":"code","source":"a = y_test.to_list()\npred_base1 = RF_basemodel.predict(test_data)\npred_base2 = knn_basemodel.predict(test_data)\npred_base3 = log_basemodel.predict(test_data)\npred_base4 = nb_basemodel.predict(test_data)\npred_base5 = svm_base.predict(test_data)\npred_base6 = base_mlp.predict(test_data)\n\nauc_score7 = round(auc(lr_recall7, lr_precision7),2)\nrf_f1 = round(f1_score(a, pred_base2, pos_label='Yes'),2)\n\nauc_score8 = round(auc(lr_recall8, lr_precision8),2)\nknn_f1 = round(f1_score(a, pred_base2, pos_label='Yes'),2)\n\nauc_score9 = round(auc(lr_recall9, lr_precision9),2)\nlog_reg_f1 = round(f1_score(a, pred_base3, pos_label='Yes'),2)\n\nauc_score10 = round(auc(lr_recall10, lr_precision10),2)\nnb_f1 = round(f1_score(a, pred_base4, pos_label='Yes'),2)\n\nauc_score11 = round(auc(lr_recall11, lr_precision11),2)\nsvm_f1 = round(f1_score(a, pred_base5, pos_label='Yes'),2)\n\nauc_score12 = round(auc(lr_recall12, lr_precision12),2)\nmlp_f1 = round(f1_score(a, pred_base6, pos_label='Yes'),2)","execution_count":null,"outputs":[]},{"metadata":{"id":"wZ7B8U0ViHCO","outputId":"3109d8fc-02f6-4f27-8ead-3c5462626370","trusted":true},"cell_type":"code","source":"voting_clf = VotingClassifier(estimators=[('RF', RF_model.best_estimator_), ('knn', knn_model.best_estimator_), ('LogReg', log_model.best_estimator_), ('NB', nb_model),\n                                          ('SVM', svm_base)], voting='soft')\nvoting_clf.fit(data_prepared, y_train)\npreds = voting_clf.predict(test_data)\n\na = y_test.to_list()\nb = preds.tolist()\nacc = accuracy_score(a, b)\nf1 = round(f1_score(a, b, pos_label='Yes'),2)\n\n# PR AUC score\nprobs = voting_clf.predict_proba(test_data)\nmodel_probs = probs[:, 1]\n# calculate the precision-recall auc\nprecision, recall, _ = precision_recall_curve(y_test, model_probs, pos_label='Yes')\nauc_score = round(auc(recall, precision),2)\nprint (\"Statistics for the voting classsifier:-\")\nprint(\"Accuracy is: \" + str(acc))\nprint(\"F1 Score is: \" + str(f1))\nprint('Voting classifier PR AUC: %.3f' % auc_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"o9mjuDTalh-8"},"cell_type":"markdown","source":"### Plot the F1 scores and Precision-Recall AUC value:-"},{"metadata":{"id":"jCKH3oqblhPb","outputId":"bb34a633-a5ec-4e31-b645-33409c0fe338","trusted":true},"cell_type":"code","source":"models = ['Random Forest', 'KNN', 'Log Reg', 'Naive Bayes', 'SVM', 'Neural Network', 'Ensemble model']\nF1_scores = [rf_f1, knn_f1, log_reg_f1, nb_f1, svm_f1, mlp_f1, f1]\npr_auc = [auc_score7, auc_score8, auc_score9, auc_score10, auc_score11, auc_score12, auc_score]\n\nx = np.arange(len(models))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12,8))\nrects1 = ax.bar(x - width/2, F1_scores, width, label='F1 scores')\nrects2 = ax.bar(x + width/2, pr_auc, width, label='PR AUC')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\n#fig.figure()\nax.set_ylim([0.0, 1])\nax.set_ylabel('Scores')\nax.set_title('Scores by F1 score and PR AUC')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend(loc='best')\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"RoqgIHyuyb0T"},"cell_type":"markdown","source":"## Out of all the 7 models trained, the neural network model is providing the best combination of PR AUC, and F1 scores. Hence, we can finalize that as the best algorithm for this dataset."},{"metadata":{"id":"m40NmDkiyyQ7","outputId":"6789ccb6-acd8-4ed1-cf96-17413bef8ad6","trusted":true},"cell_type":"code","source":"y_mlp_pred = base_mlp.predict(test_data)\n\nprint(\"Statistics for the neural network model:-\")\nprint(\"Accuracy is: \" + str(accuracy_score(y_test, y_mlp_pred)))\nprint(\"F1 Score is: \" + str(mlp_f1))\nprint('Voting classifier PR AUC: %.3f' % auc_score12)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}