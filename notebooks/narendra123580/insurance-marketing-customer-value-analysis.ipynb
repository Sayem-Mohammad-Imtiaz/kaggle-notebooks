{"cells":[{"metadata":{},"cell_type":"markdown","source":"### IMPORTING LIBRARIES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats import diagnostic as diag\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import normaltest,f_oneway\nfrom scipy.stats import ttest_ind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeRegressor()\net = ExtraTreeRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abr = AdaBoostRegressor()\nbr = BaggingRegressor()\netr = ExtraTreesRegressor()\ngbr = GradientBoostingRegressor()\nrfr = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### READING THE DATA SET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/ibm-watson-marketing-customer-value-data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NULL VALUE'S CHECK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MEASURES OF CENTRAL TENDENCY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Customer Lifetime Value','Income','Monthly Premium Auto','Total Claim Amount']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAPE OFTHE DATA SET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DATA VISUALIZATION AND INFERENCES\n\n#### UNIVARIATE ANALYSIS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data['Monthly Premium Auto'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data['Total Claim Amount'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see that there are outliers in the total claim amount and also in monthly premium auto , usually we remove the outliers for a better model.\n- since our dataset is related to insurance and banking industry, we must be accept the outliers,as they can be our potential customers.\n- And there are no outliers in the income.\n- Conclusion: No outlier treatment required.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Monthly Premium Auto'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Total Claim Amount'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that none of the continuous variables are normally distributed.\nSo in our case , we want to make the distributions normal, we can apply some transformations to the data and see if we can achieve a normally distributed variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### TRANSFORMATION OF THE NUMERICAL VARIABLES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Income']**2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Income']**(1/2))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that while we are trying to transform the data to make it normal,rather the distribution is getting skewed, or is having multiple peaks which again is a problem to our model, hence we just stick with the same distribution of the variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Monthly Premium Auto']**(2))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The monthly premium auto has multiple peaks,so to remove those peaks we can apply any of the power transformation (SQUARE / CUBE) but as we can see that after the square transformation the data is getting heavily skewed, so we stick with the actual distribution again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Total Claim Amount']**2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again for the total claim amount after applying the transformation's the data is getting skewed, and hence we stick to the actual distibution of the data.\n\nConclusion: No matter what power transformation we are applying to the numerical variables, it is still not getting normally distributed, and moreover the data is getting skewed, so rather we will just stick with the actual distribution ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Location Code',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average customer lifetime value of the customer who stay in different location code is the same so while creating the model we can drop this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'State',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average customer lifetime value of the customer who stay in different state is same and we can drop this also","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Response',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average customer lifetime value for both of them is same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Gender',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the average customer lifetime value is same for both male and female.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Education',y='Customer Lifetime Value',data = data)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see that education is not a significant feature for assessing the lifetime value of the customer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Number of Policies',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a pattern here, customers who have taken only 1 policy have lower customer lifetime value, and customers who have taken 3 or greater show a similar trend, so we can combine all of them into one bin, and we can also see that the customers who have taken 2 policies have very high customer lifetime value comparitively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Policy Type',y='Customer Lifetime Value',data = data)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't much difference in the customer lifetime value w.r.t what policy type he has taken, all we need is how much revenue a customer can bring to the company, so it doesnt matter what type of policy he/she has chosen.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Coverage',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customer Lifetime Value is different for different types of coverage.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = 'Number of Open Complaints',y='Customer Lifetime Value',data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of open complaints also show kind of similar trend, where people who have complaints 2 or lesser have a similar pattern but where as >3 do not show any pattern we will have to do statistical test to understand if this feature is really significant or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(y_vars='Customer Lifetime Value',x_vars=['Income','Monthly Premium Auto','Total Claim Amount'],data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can clearly see that there is a linear relationship between Customer lifetime value and monthly premium auto, but we do not see any relationship between income and the total claim amount.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data[['Customer Lifetime Value','Monthly Premium Auto','Income','Total Claim Amount']].corr(),annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAnd we can clearly see in the correlation map, that customer lifetime value has a better correlation with monthly premium auto and acceptable co relation with total claim amount, but it show's no relationship with income, so again with all the visualization's we can come to the conclusion that we can dis regard the INCOME feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### BASE MODEL USING OLS\n\n##### Using label encoding just for the purpose of looking at the base model, encoding technique's may change furthur(one-hot encoding is used)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = data.select_dtypes(object).columns\nfor i in cols:\n    data[i] = le.fit_transform(data[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Customer Lifetime Value',axis=1)\ny = data['Customer Lifetime Value']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# train data - 70% and test data - 30%\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sm = X\nX_sm = sm.add_constant(X_sm)\nlm = sm.OLS(y,X_sm).fit()\nlm.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Looking at the base model and the p-value of the feature's, we know that the Hypothesis for the feature's is\n\nH0: Feature is not significant\nHa: Feature is significant\nBut we just cant conclude the significance of the feature's just by base model and also without using any of the feature engineering technique's we have at our disposal. So we will first try to do the statistical test's of the feature for the feature selection, we can also use the forward selection and backward elimination , we will use the Variance inflation factor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ASSUMPTIONS OF LINEAR REGRESSION.\n### Linearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(x_vars=['Monthly Premium Auto','Total Claim Amount','Income'],y_vars =['Customer Lifetime Value'],data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see any linear relationship between the variables and the Y varible , which fails the first assumption of linear regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Mean Of Residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nresiduals = y_pred-y_test\nmean_of_residuals = np.mean(residuals)\nprint(f\"The mean of the residuals is {mean_of_residuals}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 2nd assumption is that the mean of the residuals must be close to zero, which again fails.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Homoscedasticity_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"name = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals,X_test)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0: Error terms are homoscedastic\n\nHa: Error terms are not homoscedastic\n\np-value < 0.05 reject null hypothesis, error terms are not homoscedastic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Test of normality of residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = sns.distplot(residuals,kde=True)\np = plt.title('Normality of error terms/residuals')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution clearly show's that the residuals are not normally distributed, and the third assumption also fails.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### No Autocorrelation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min(diag.acorr_ljungbox(residuals , lags = 40)[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ho: Autocorrelation is absent\n\nHa: Autocorrelation is present\n\nThe P-value is >0.05 ,we fail to reject the null hypothesis, autocorrelation is absent.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NO MULTI COLLINEARITY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_sm.values, i) for i in range(X_sm.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, multicollinearity exists.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### STATISTICAL ANALYSIS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Considering CLV (Customer Lifetime Value) as the target variable, we shall try to understand how each of the independent variables are contributing towards the target variable.\n\nSince our target variable is a continuous variable, we will have to perform ANOVA to understand how significant are the independent variables towards target variable.\n\nFor ANOVA,\n\nNull hypothesis is that there is no significant difference among the groups\nAlternative hypothesis is that there is at least one significant difference among the groups","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### State vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/ibm-watson-marketing-customer-value-data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"State = data.groupby('State')\nWashington = State.get_group('Washington')['Customer Lifetime Value']\nArizona = State.get_group('Arizona')['Customer Lifetime Value']\nNevada = State.get_group('Nevada')['Customer Lifetime Value']\nCalifornia = State.get_group('California')['Customer Lifetime Value']\nOregon = State.get_group('Oregon')['Customer Lifetime Value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [Washington,Arizona,Nevada,California,Oregon]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the 'States' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(Washington,Arizona,Nevada,California,Oregon)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ALL STATES HAVE SAME MEAN VALUE FOR CLV\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable which means 'State' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Customer Response to marketing calls vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Response = data[['Customer Lifetime Value','Response']].groupby('Response')\nNo = Response['Customer Lifetime Value'].get_group('No')\nYes = Response['Customer Lifetime Value'].get_group('Yes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [No,Yes]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the 'Response' follow a normal distribution. Hence, we can perform ANOVA test or test of mean for independent categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ttest_ind(No,Yes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RESPONE HAVE SAME MEAN VALUE\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable which means 'Response' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Coverage Type vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Coverage = data[['Customer Lifetime Value','Coverage']].groupby('Coverage')\nbasic = Coverage['Customer Lifetime Value'].get_group('Basic')\nextended = Coverage['Customer Lifetime Value'].get_group('Extended')\npremium = Coverage['Customer Lifetime Value'].get_group('Premium')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [basic,extended,premium]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the 'Coverage' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(basic,extended,premium)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE NOT SAME FOR COVERAGE\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Coverage' which means 'Coverage' feature can be a significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Education vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Education = data[['Customer Lifetime Value','Education']].groupby('Education')\nbachelor = Education['Customer Lifetime Value'].get_group('Bachelor')\ncollege = Education['Customer Lifetime Value'].get_group('College')\nhighschool = Education['Customer Lifetime Value'].get_group('High School or Below')\nmaster = Education['Customer Lifetime Value'].get_group('Master')\ndoctor = Education['Customer Lifetime Value'].get_group('Doctor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [basic,college,highschool,master,doctor]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the categories of 'Education' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(bachelor,college,highschool,master,doctor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE NOT SAME FOR EDUCATION\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Education' which means 'Education' feature can be a significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Employment Status vs Customer Lifetime Value ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"es = data[['Customer Lifetime Value','EmploymentStatus']].groupby('EmploymentStatus')\nemployed = es['Customer Lifetime Value'].get_group('Employed')\nunemployed = es['Customer Lifetime Value'].get_group('Unemployed')\nmedleave = es['Customer Lifetime Value'].get_group('Medical Leave')\ndisabled = es['Customer Lifetime Value'].get_group('Disabled')\nretired = es['Customer Lifetime Value'].get_group('Retired')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [employed,unemployed,medleave,disabled,retired]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the categories of 'Employment Status' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(employed,unemployed,medleave,disabled,retired)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE NOT SAME FOR Employment Status\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Employment Status' which means 'Employment Status' feature can be a significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Gender vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = data[['Customer Lifetime Value','Gender']].groupby('Gender')\nf = g['Customer Lifetime Value'].get_group('F')\nm = g['Customer Lifetime Value'].get_group('M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [f,m]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the categories of 'Gender' follow a normal distribution. Hence, we can perform ANOVA test or test of mean for independent features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ttest_ind(f,m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE SAME FOR GENDER\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Gender' which means 'Gender' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Location Code vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"location = data[['Customer Lifetime Value','Location Code']].groupby('Location Code')\nsub = location['Customer Lifetime Value'].get_group('Suburban')\nurban = location['Customer Lifetime Value'].get_group('Urban')\nrural = location['Customer Lifetime Value'].get_group('Rural')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [sub,urban,rural]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the categories of 'Location Code' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(sub,urban,rural)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE SAME FOR LOCATION CODE\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Location Code' which means 'Location Code' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Marital Status vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MaritalStatus = data[['Customer Lifetime Value','Marital Status']].groupby('Marital Status')\nMarried = MaritalStatus['Customer Lifetime Value'].get_group('Married')\nSingle = MaritalStatus['Customer Lifetime Value'].get_group('Single')\nDivorced = MaritalStatus['Customer Lifetime Value'].get_group('Divorced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [Married,Single,Divorced]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLV of all the categories of 'Location Code' follow a normal distribution. Hence, we can perform ANOVA test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(Married,Single,Divorced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE NOT SAME Marital Status\n\npvalue < 0.05 implies that there is significant difference in the mean of target variable for at least on Group of 'Marital Status' which means 'Marital Status' feature can be significant for predicting 'Customer Lifetime Value'\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Policy vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Policy  = data[['Customer Lifetime Value','Policy']].groupby('Policy')\np3 = Policy['Customer Lifetime Value'].get_group('Personal L3')\np2 = Policy['Customer Lifetime Value'].get_group('Personal L2')\np1 = Policy['Customer Lifetime Value'].get_group('Personal L1')\nc3 = Policy['Customer Lifetime Value'].get_group('Corporate L3')\nc2 = Policy['Customer Lifetime Value'].get_group('Corporate L2')\nc1 = Policy['Customer Lifetime Value'].get_group('Corporate L1')\ns3 = Policy['Customer Lifetime Value'].get_group('Special L3')\ns2 = Policy['Customer Lifetime Value'].get_group('Special L2')\ns1 = Policy['Customer Lifetime Value'].get_group('Special L1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [p3,p2,p1,c3,c2,c1,s3,s2,s1]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(p3,p2,p1,c3,c2,c1,s3,s2,s1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Renew Offer Type vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"R  = data[['Customer Lifetime Value','Renew Offer Type']].groupby('Renew Offer Type')\no1 = R['Customer Lifetime Value'].get_group('Offer1')\no2 = R['Customer Lifetime Value'].get_group('Offer2')\no3 = R['Customer Lifetime Value'].get_group('Offer3')\no4 = R['Customer Lifetime Value'].get_group('Offer4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [o1,o2,o3,o4]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(o1,o2,o3,o4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sales Channel vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Sales  = data[['Customer Lifetime Value','Sales Channel']].groupby('Sales Channel')\nagent = Sales['Customer Lifetime Value'].get_group('Agent')\nbranch = Sales['Customer Lifetime Value'].get_group('Branch')\ncall = Sales['Customer Lifetime Value'].get_group('Call Center')\nweb = Sales['Customer Lifetime Value'].get_group('Web')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [agent,branch,call,web]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(agent,branch,call,web)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vehicle Class vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VC  = data[['Customer Lifetime Value','Vehicle Class']].groupby('Vehicle Class')\nfd = VC['Customer Lifetime Value'].get_group('Four-Door Car')\ntd = VC['Customer Lifetime Value'].get_group('Two-Door Car')\nsuv = VC['Customer Lifetime Value'].get_group('SUV')\nsc = VC['Customer Lifetime Value'].get_group('Sports Car')\nls = VC['Customer Lifetime Value'].get_group('Luxury SUV')\nlc = VC['Customer Lifetime Value'].get_group('Luxury Car')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [fd,td,suv,sc,ls,lc]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(fd,td,suv,sc,ls,lc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vehicle Size vs Customer Lifetime Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VS  = data[['Customer Lifetime Value','Vehicle Size']].groupby('Vehicle Size')\nm = VS['Customer Lifetime Value'].get_group('Medsize')\ns = VS['Customer Lifetime Value'].get_group('Small')\nl = VS['Customer Lifetime Value'].get_group('Large')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [m,s,l]:\n    print(normaltest(i),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_oneway(m,s,l)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Furthur Modelling:\n\n#### So we did the EDA and also the Statistical Analysis, so now we can just dis regard the features which we are not significant  for our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['State','Customer','Response','EmploymentStatus','Gender','Location Code','Vehicle Size','Policy','Policy Type','Sales Channel','Income','Effective To Date','Education'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though the features, months since policy inception, months since last claim, number of open complaints and number of policies are all numerical, but they are discrete numbers and we will consider them as categorical features while preparing the model.\n\nFirstly, according to our EDA, we saw that the number of policies >= 3 have similar trend so we will group all of them as 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Policies'] = np.where(data['Number of Policies']>2,3,data['Number of Policies'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, when we convert the numerical features to categorical, our normal practice is label encoding for ordinal data and one hot for nominal data, but we can also use one hot encoding for ordinal data if there isn't any curse of dimensionality, so we will convert the categorical to numerical with one-hot encoding / dummification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new = pd.get_dummies(data,columns=['Coverage','Marital Status','Number of Policies','Renew Offer Type','Vehicle Class'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spliting the data into train(70) and test(30)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new.drop('Customer Lifetime Value',axis=1)\ny = new['Customer Lifetime Value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So after removing the unnessary feature's our model is giving is an accuracy of about 60%, we would like to take it 70% in the furthur models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection-Forward, Backward\n#### Forward","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sfs = SFS(lr, k_features='best', forward=True, floating=False, \n          scoring='neg_mean_squared_error', cv=20)\nmodel = sfs.fit(new.drop('Customer Lifetime Value', axis=1),new['Customer Lifetime Value'])\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Selected features:', sfs.k_feature_idx_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Backward","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sfs = SFS(lr, k_features='best', forward=False, floating=False, \n          scoring='neg_mean_squared_error', cv=20)\nmodel = sfs.fit(new.drop('Customer Lifetime Value', axis=1).values,new['Customer Lifetime Value'])\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Backward Selection (w. StdErr)')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Selected features:', sfs.k_feature_idx_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suprisingly Both the forward and backward selection gave us the same features to select for our model, so we will be sticking to the same features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = X[['Monthly Premium Auto','Number of Open Complaints','Total Claim Amount','Coverage_Premium',\n            'Marital Status_Single','Number of Policies_2','Number of Policies_3',\n            'Renew Offer Type_Offer2','Vehicle Class_SUV','Vehicle Class_Sports Car']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = []\ntest = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(test_X,y,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.append(lr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(lr.score(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can clearly see that the features removed didn't contribute to tell us the differing variance in the data, so it was a good decision to remove those features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = [r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = []\nmae = []\nmape = []\nmse = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in metrics:\n    print(i(y_test,y_pred))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred))\n    else:\n        mse.append(i(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will consider the r2_Score and the Mean absolute percentage error as the metrics we are going to use to measure the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Finding the best sample by random state for each model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algo = [abr,gbr,dt,et,etr,br,rfr]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in algo:\n    temp = 0\n    print(f\"New Model{i}\")\n    for j in range(1,300,1):\n        NXT,NXt,NYT,NYt = train_test_split(X,y,test_size=0.3,random_state=j)\n        i.fit(NXT,NYT)\n        test_score = i.score(NXt,NYt)\n        train_score = i.score(NXT,NYT)\n        if test_score>temp:\n            temp = test_score\n            print(j,train_score,temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the best sample for each model, and we can also see which model is a good fit and which model is overfitting/underfitting model.\n\n- AdaBoost is good model, but we will have to check for the metrics\n- Gradient Boosting is the best model comparitively, again we will check for the metrics\n- DecisionTree and the Extra Tree regressor models are not working better for this data set ,as the model is overfitting\n- Bagging and random forest regressor models are again overfitting model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=159)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dt = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in metrics:\n    print(i(y_test,y_pred_dt))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_dt))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_dt))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_dt))\n    else:\n        mse.append(i(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(dt.score(X_train,y_train))\ntest.append(dt.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Tree Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=69)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_et = et.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in metrics:\n    print(i(y_test,y_pred_et))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_et))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_et))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_et))\n    else:\n        mse.append(i(y_test,y_pred_et))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(et.score(X_train,y_train))\ntest.append(et.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Model':['Linear Regression','Decision Tree','Extra Tree'],'R2_Score':r2,'MAE':mae,'MAPE':mape,'MSE':mse})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the Linear Regression model is having the best r2_Score, and decision tree and extra tree regressor have no better accuracy that so we will have to build our model using ensemlle technique's, boosting and bagging.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ENSEMBLE METHODS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=83)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_abr = abr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in metrics:\n    print(i(y_test,y_pred_abr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_abr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_abr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_abr))\n    else:\n        mse.append(i(y_test,y_pred_abr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(abr.score(X_train,y_train))\ntest.append(abr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=292)\nbr.fit(X_train,y_train)\ny_pred_br = br.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_br))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_br))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_br))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_br))\n    else:\n        mse.append(i(y_test,y_pred_br))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(br.score(X_train,y_train))\ntest.append(br.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=69)\netr.fit(X_train,y_train)\ny_pred_etr = etr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_etr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_etr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_etr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_etr))\n    else:\n        mse.append(i(y_test,y_pred_etr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(etr.score(X_train,y_train))\ntest.append(etr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=181)\ngbr.fit(X_train,y_train)\ny_pred_gbr = gbr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_gbr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_gbr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_gbr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_gbr))\n    else:\n        mse.append(i(y_test,y_pred_gbr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(gbr.score(X_train,y_train))\ntest.append(gbr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=35)\nrfr.fit(X_train,y_train)\ny_pred_rfr = rfr.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_rfr))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_rfr))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_rfr))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_rfr))\n    else:\n        mse.append(i(y_test,y_pred_rfr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(rfr.score(X_train,y_train))\ntest.append(rfr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of all the models we decided to choose gradient boosting as the next model, and furthur tweak the hyper parameter's of the model and also put this boosting model into bagging regressor and check for the model accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NEXT MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_params_gbr = {'loss':['ls','lad','huber'],'learning_rate':[0.1,0.01,1],'n_estimators':[100,150]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr2 = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(gbr2,param_grid=hyper_params_gbr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=181)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the default parameters of the model so no hyper parameter tuning for this model is required, let us try and put this model into the bagging regressor.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Final Model(Boosting+Bagging)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr2 = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"br2 = BaggingRegressor(gbr2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding the best sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 0\nfor j in range(1,300,1):\n    NXT,NXt,NYT,NYt = train_test_split(X,y,test_size=0.3,random_state=j)\n    br2.fit(NXT,NYT)\n    test_score = br2.score(NXt,NYt)\n    train_score = br2.score(NXT,NYT)\n    if test_score>temp:\n        temp = test_score\n        print(j,train_score,temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=87)\nbr2.fit(X_train,y_train)\ny_pred_br2 = br2.predict(X_test)\nfor i in metrics:\n    print(i(y_test,y_pred_br2))\n    if i == r2_score:\n        r2.append(i(y_test,y_pred_br2))\n    elif i == mean_absolute_error:\n        mae.append(i(y_test,y_pred_br2))\n    elif i == mean_absolute_percentage_error:\n        mape.append(i(y_test,y_pred_br2))\n    else:\n        mse.append(i(y_test,y_pred_br2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.append(br2.score(X_train,y_train))\ntest.append(br2.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using KFold Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_scores = []\ntrain_scores = []\ncv = KFold(n_splits=10,random_state=42, shuffle=False)\nfor train_index,test_index in cv.split(X):\n    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n    br2.fit(X_train,y_train)\n    test_scores.append(br.score(X_test, y_test))\n    train_scores.append(br.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(train_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(test_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that after using KFold validation we got our average training accuracy to be 86.45 and average testing accuracy to be 86.30, OUR final model is bagging regressor with base estimator gradient boosting regressor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ALL_SCORES = pd.DataFrame({'Model':['Linear Regression','Decision Tree','Extra Tree','AdaBoost','Bagging',\n                                    'Extra Trees','GradientBoosting','Random Forest','Final_Model'],\n                           'Training_Score':train,'Testing_Score':test,'R2_Score':r2,'MAE':mae,'MAPE':mape,'MSE':mse})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALL_SCORES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}