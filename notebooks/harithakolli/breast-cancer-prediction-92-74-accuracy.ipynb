{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Breast Cancer Prediction - Using Logistic regression\n\nHello Kagglers,\n\nWelcome to my first kernel on Kaggle. In this notebook, I analyse the Breast Cancer dataset and develop a Logistic Regression model to try classifying suspected cells to either Benign or Malignant."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import pandas \nimport pandas as pd\n\n#import numpy\nimport numpy as np\n\n#import seaborn for visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\n\n#import statsmodel \nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#import sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,f1_score,roc_curve,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing to show more rows for visual analysis\npd.set_option('display.max_rows', 500)\n# Changing display format to not show scientific notation\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n# Displaying all columns\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data\ndata = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning and preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to calculate the missing value percent in DataFrame columns - As we are goining to do it frequently\ndef missingValues(df):\n   missingcontent=round(df.isnull().sum()/len(df) *100,2)\n   print(\"Total Missing Value Percentage in dataframe: \",round(missingcontent.mean(),2))\n   print(missingcontent[missingcontent>0].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#null value percentage in data\nmissingValues(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dropping 'Unnamed: 32' - since its holding null values 100% and 'id' as well - since doesn't hold any significance"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['id','Unnamed: 32'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#null value percentage in data\nmissingValues(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Diagnosis' count details\ndata['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=data['diagnosis'].value_counts(),x=data['diagnosis'].unique(),palette=\"pastel\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finding correlations using the heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),cmap='YlGnBu',annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    \n   - all the `worst` scenerios data is highly correlated with the `mean` data\n   - Ex: `radius_mean` is highly correlated with `radius_worst` \n   - similarly `radius_se` with 'perimeter_se' & 'area_se'\n   - High correlation exists between many variables\n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a pair plot with the \"mean\" columns alone\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=data[cols], hue='diagnosis', palette='RdBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    \n   - `radius_mean` is highly correlated with 'perimeter_mean', 'area_mean'\n   - `compactness_mean` is similar to 'concavity_mean' & 'concave_points_mean'"},{"metadata":{},"cell_type":"markdown","source":"Lets' drop the cols, which we have stated above as highly correlated"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols to be dropped inorder to handle the multicollinearity between the variables\ncols= ['perimeter_se', 'area_se',\n       'perimeter_mean','area_mean',\n       'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst',\n       'concavity_mean',\n       'concave points_mean',\n       'concavity_se', 'concave points_se']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=cols,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our final columns for our model\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binary map of 'M' & 'B' values in the diagnosis column \ndata['diagnosis'] = data['diagnosis'].map({'B':0,'M':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    \n   - Let's rescale the values later inorder to have cofficients of same scale"},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the data to X,y DataSets"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.pop('diagnosis')\nX= data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test and Train Dataset Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_Train Dataset: ',X_train.shape)\nprint('y_Train Dataset: ',y_train.shape)\nprint('X_Test Dataset: ',X_test.shape)\nprint('y_Test Dataset: ',y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Rescaling\n\nNormalising the numerical columns using Min Max scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data normalization using sklearn MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[cols] = scaler.fit_transform(X_train[cols])\nX_test[cols] = scaler.transform(X_test[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression model using statsmodel\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to calculate VIF values\ndef VIF_values(X_train):\n    vif = pd.DataFrame()\n    X= X_train\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    print(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VIF_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    \n   - `smoothness_mean` has high VIF indicating the multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'smoothness_mean' from the model\nX_train = X_train.drop(columns='smoothness_mean',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VIF_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    -`compactness_mean` has high VIF "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'compactness_mean' from the model\nX_train = X_train.drop(columns='compactness_mean',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature variables and their corresponding VIFs\nVIF_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n - `symmetry_mean` has high VIF of 15.07"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'symmetry_mean' from the model\nX_train = X_train.drop(columns='symmetry_mean',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - `compactness_se` has high VIF`10.16`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'compactness_se' from the model\nX_train = X_train.drop(columns='compactness_se',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - `symmetry_se` has high P-value of `0.995`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'symmetry_se' from the model\nX_train = X_train.drop(columns='symmetry_se',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n   - `smoothness_se` has P_value of `0.394`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'smoothness_se' from the model\nX_train = X_train.drop(columns='smoothness_se',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    -`texture_se` has p_value of `0.253`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the 'texture_se' from the model\nX_train = X_train.drop(columns='texture_se',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train)\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creating a dataframe to hold `diagnosis`and the predicted `diagnosis_prob` of train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Diagnosis':y_train.values, 'Diagnosis_Prob':y_train_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicted values above 0.5 is considered to be Malignant i.e 1\ny_train_pred_final.Diagnosis_Prob = y_train_pred_final.Diagnosis_Prob.map(lambda x: 1 if x>0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#build confusion matrix using confusion_matrix from sklearn.metrics\nconfusion = confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncategories = ['Beingn', 'Malignant']\nsns.heatmap(confusion,annot=True,fmt='d', cmap='Blues',linewidths=1,xticklabels=categories,yticklabels=categories,cbar=False)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy Score : ',accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob))\nprint('f1 Score : ',f1_score(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    \n- We have built a model with `Accuracy score` - `92.74%` and of `f1_score` - `89.71%`"},{"metadata":{},"cell_type":"markdown","source":"Let's see the sensitivity and specificity values "},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity : ',TP / float(TP+FN))\nprint('Specificity : ',TN / float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting Malignant when patient does have beingn\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive predictive value  and  Negative predictive value\n\nprint('positive predictive value: ',TP / float(TP+FP))\nprint('Negative predictive value: ',TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting ROC Curve"},{"metadata":{},"cell_type":"markdown","source":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#filtering out the columns based on our final model \n\nX_test = X_test[X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add constant to the X_test data\n\nX_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict the y_test values\n\ny_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forming new dataframe holding y_test and y_test_pred values\n\ny_pred_final = pd.concat([pd.DataFrame(y_test),pd.DataFrame(y_test_pred)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'diagnosis_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['diagnosis_Prob'] = y_pred_final['diagnosis_Prob'].map(lambda x: 1 if x>0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy Score : ',accuracy_score(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob))\nprint('f1 Score : ',f1_score(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#build confusion matrix using confusion_matrix from sklearn.metrics\nconfusion = confusion_matrix(y_pred_final.diagnosis, y_pred_final.diagnosis_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncategories = ['Beingn', 'Malignant']\nsns.heatmap(confusion,annot=True,fmt='d', cmap='Blues',linewidths=1,xticklabels=categories,yticklabels=categories,cbar=False)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}