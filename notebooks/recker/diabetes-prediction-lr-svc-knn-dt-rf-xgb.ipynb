{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Hi kagglers! This is my first ever kaggle notebook. Please let me know how can I improve. Have fun!","metadata":{}},{"cell_type":"markdown","source":"# **Dataset** : Pima Indians Diabetes Database\n# **Source** : National Institute of Diabetes and Digestive and Kidney Diseases\n## **Objective** :  Diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset","metadata":{}},{"cell_type":"markdown","source":"# **Features**:\n### *Pregnancies* : Number of times pregnant\n### *Glucose* : Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n### *BloodPressure* : Diastolic blood pressure (mm Hg)\n### *SkinThickness* : Triceps skin fold thickness (mm)\n### *Insulin* : 2-Hour serum insulin (mu U/ml)\n### *BMI* : Body mass index (weight in kg/(height in m)^2)\n### *DiabetesPedigreeFunction* : Diabetes pedigree function\n### *Age* : Age (years)","metadata":{}},{"cell_type":"markdown","source":"# **Target**:\n### *Outcome* : Class variable (0 or 1) 268 of 768 are 1, the others are 0","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold,StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report,roc_auc_score ,accuracy_score\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings\nimport optuna\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:45.797841Z","iopub.execute_input":"2021-09-13T16:21:45.798223Z","iopub.status.idle":"2021-09-13T16:21:45.806778Z","shell.execute_reply.started":"2021-09-13T16:21:45.798188Z","shell.execute_reply":"2021-09-13T16:21:45.805885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Reading the dataset using Pandas and checking the head(first 5) of the dataset\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:45.9158Z","iopub.execute_input":"2021-09-13T16:21:45.916095Z","iopub.status.idle":"2021-09-13T16:21:45.937425Z","shell.execute_reply.started":"2021-09-13T16:21:45.916067Z","shell.execute_reply":"2021-09-13T16:21:45.936686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the number of datapoints in each class\nprint(df['Outcome'].value_counts())\nsns.countplot(x='Outcome', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:46:12.495037Z","iopub.execute_input":"2021-09-13T16:46:12.495354Z","iopub.status.idle":"2021-09-13T16:46:12.608533Z","shell.execute_reply.started":"2021-09-13T16:46:12.495326Z","shell.execute_reply":"2021-09-13T16:46:12.607547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding missing values using seaborn heatmap\nplt.figure(figsize=(12,6))\nsns.heatmap(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:46.019601Z","iopub.execute_input":"2021-09-13T16:21:46.019858Z","iopub.status.idle":"2021-09-13T16:21:46.50874Z","shell.execute_reply.started":"2021-09-13T16:21:46.019831Z","shell.execute_reply":"2021-09-13T16:21:46.507672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.drop('Outcome', axis=1).columns:\n    print(i, df[df[i] == 0][i].count())","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:46.510541Z","iopub.execute_input":"2021-09-13T16:21:46.510988Z","iopub.status.idle":"2021-09-13T16:21:46.529384Z","shell.execute_reply.started":"2021-09-13T16:21:46.510925Z","shell.execute_reply":"2021-09-13T16:21:46.528439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation : There are some data which are 0 in the features of our dataset. But, pregnancies can be 0. So, except it we will try to fill rest features with 'Median' values respectively.","metadata":{}},{"cell_type":"code","source":"# Here we are first converting 0 with nan and then with median values. Using median instead of mean cause it is less prone to outliers.\nto_process_features = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\ndf[to_process_features] = df[to_process_features].replace(0, np.nan)\nfor i in to_process_features:\n    df[i].fillna(df[i].median(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:46.531824Z","iopub.execute_input":"2021-09-13T16:21:46.532217Z","iopub.status.idle":"2021-09-13T16:21:46.546367Z","shell.execute_reply.started":"2021-09-13T16:21:46.532181Z","shell.execute_reply":"2021-09-13T16:21:46.545316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:46.549627Z","iopub.execute_input":"2021-09-13T16:21:46.549939Z","iopub.status.idle":"2021-09-13T16:21:46.561049Z","shell.execute_reply.started":"2021-09-13T16:21:46.549912Z","shell.execute_reply":"2021-09-13T16:21:46.560212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the dataset\ndf.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:46.562504Z","iopub.execute_input":"2021-09-13T16:21:46.562894Z","iopub.status.idle":"2021-09-13T16:21:47.375841Z","shell.execute_reply.started":"2021-09-13T16:21:46.562847Z","shell.execute_reply":"2021-09-13T16:21:47.374863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ploting pairwise relationships in a dataset.\nsns.pairplot(df, hue='Outcome')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:21:47.377266Z","iopub.execute_input":"2021-09-13T16:21:47.377623Z","iopub.status.idle":"2021-09-13T16:22:06.470999Z","shell.execute_reply.started":"2021-09-13T16:21:47.377585Z","shell.execute_reply":"2021-09-13T16:22:06.469988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining our features and target\nX = df.drop('Outcome',axis=1)\ny = df['Outcome']\n# Splitting the dataset using train_test_split() in 80-20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n# Using Standard Scaler to fit and transform the training data but only transforming the test data, so that no data leakage happens\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))\n# Re-assigning the columns\nX_train_scaled.columns = X_train.columns\nX_test_scaled.columns = X_test.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:06.472395Z","iopub.execute_input":"2021-09-13T16:22:06.472717Z","iopub.status.idle":"2021-09-13T16:22:06.491795Z","shell.execute_reply.started":"2021-09-13T16:22:06.472683Z","shell.execute_reply":"2021-09-13T16:22:06.490835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the distribution of the scaled train and test data\nX_train_scaled.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:06.494307Z","iopub.execute_input":"2021-09-13T16:22:06.494627Z","iopub.status.idle":"2021-09-13T16:22:07.324494Z","shell.execute_reply.started":"2021-09-13T16:22:06.494594Z","shell.execute_reply":"2021-09-13T16:22:07.322725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_scaled.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:07.327353Z","iopub.execute_input":"2021-09-13T16:22:07.328232Z","iopub.status.idle":"2021-09-13T16:22:08.282616Z","shell.execute_reply.started":"2021-09-13T16:22:07.328166Z","shell.execute_reply":"2021-09-13T16:22:08.281773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to fit, predict and show the scores \ndef evaluation(model, X_train_scaled, y_train, X_test_scaled, y_test):\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    print(f\"Accuracy Score : {accuracy_score(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nRoc auc score : {roc_auc_score(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nConfusion Matrix : \\n {confusion_matrix(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nClassification Report : \\n {classification_report(y_test, y_pred)}\")   ","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.283928Z","iopub.execute_input":"2021-09-13T16:22:08.284475Z","iopub.status.idle":"2021-09-13T16:22:08.290556Z","shell.execute_reply.started":"2021-09-13T16:22:08.284435Z","shell.execute_reply":"2021-09-13T16:22:08.289656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning models","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression(random_state=42)\nevaluation(lr, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.291623Z","iopub.execute_input":"2021-09-13T16:22:08.292308Z","iopub.status.idle":"2021-09-13T16:22:08.32049Z","shell.execute_reply.started":"2021-09-13T16:22:08.292268Z","shell.execute_reply":"2021-09-13T16:22:08.319644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Knn\nknn = KNeighborsClassifier()\nevaluation(knn, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.323102Z","iopub.execute_input":"2021-09-13T16:22:08.323351Z","iopub.status.idle":"2021-09-13T16:22:08.351666Z","shell.execute_reply.started":"2021-09-13T16:22:08.32332Z","shell.execute_reply":"2021-09-13T16:22:08.350293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Classification\nsv = SVC()\nevaluation(sv, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.35285Z","iopub.execute_input":"2021-09-13T16:22:08.353238Z","iopub.status.idle":"2021-09-13T16:22:08.38126Z","shell.execute_reply.started":"2021-09-13T16:22:08.353199Z","shell.execute_reply":"2021-09-13T16:22:08.380352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# linear svc [‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss which \n# is used by Linear SVC]\nlrsv = LinearSVC(random_state=42)\nevaluation(lrsv, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.382573Z","iopub.execute_input":"2021-09-13T16:22:08.382912Z","iopub.status.idle":"2021-09-13T16:22:08.423374Z","shell.execute_reply.started":"2021-09-13T16:22:08.382877Z","shell.execute_reply":"2021-09-13T16:22:08.422543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree\ndt = DecisionTreeClassifier(random_state=42)\nevaluation(dt, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.424439Z","iopub.execute_input":"2021-09-13T16:22:08.424848Z","iopub.status.idle":"2021-09-13T16:22:08.445995Z","shell.execute_reply.started":"2021-09-13T16:22:08.4248Z","shell.execute_reply":"2021-09-13T16:22:08.445217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nrf = RandomForestClassifier(random_state=42)\nevaluation(rf, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.447716Z","iopub.execute_input":"2021-09-13T16:22:08.44817Z","iopub.status.idle":"2021-09-13T16:22:08.674152Z","shell.execute_reply.started":"2021-09-13T16:22:08.448136Z","shell.execute_reply":"2021-09-13T16:22:08.673205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost classifier \nxgbc = XGBClassifier(eval_metric='logloss')\nevaluation(xgbc, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:22:08.675498Z","iopub.execute_input":"2021-09-13T16:22:08.675844Z","iopub.status.idle":"2021-09-13T16:22:08.777466Z","shell.execute_reply.started":"2021-09-13T16:22:08.675807Z","shell.execute_reply":"2021-09-13T16:22:08.776713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation:\n### 1) Xgboost classifier worked best overall. \n### 2) Accuracy score: 0.7597 and ROC_AUC score: 0.7566\n### 3) It also has False Negative (Type I error) as 14 which is less than others and False Positive(Type II error) as 23 as can be seen in Confusion Matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:26:56.901422Z","iopub.execute_input":"2021-09-13T16:26:56.901758Z","iopub.status.idle":"2021-09-13T16:26:56.905734Z","shell.execute_reply.started":"2021-09-13T16:26:56.901726Z","shell.execute_reply":"2021-09-13T16:26:56.904852Z"}}},{"cell_type":"markdown","source":"# Hyperparameter Tuning of the ML models","metadata":{}},{"cell_type":"code","source":"# Function for computing fit, predict and scores after using GridSearchCV()\ndef grid_evaluation(model, X_train_scaled, y_train, X_test_scaled, y_test):\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    print(f\"Best Parameters : {model.best_params_}\")\n    print(\"*\"*50)\n    # Best Score: Mean cross-validated score of the best_estimator\n    print(f\"\\nBest Score :  {model.best_score_}\")\n    print(\"*\"*50)\n    print(f\"\\nAccuracy Score (Train set) :  {model.score(X_train_scaled,y_train)}\")\n    print(\"*\"*50)\n    print(f\"\\nAccuracy Score (Test set): {accuracy_score(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nRoc auc score : {roc_auc_score(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nConfusion Matrix : \\n {confusion_matrix(y_test, y_pred)}\")\n    print(\"*\"*50)\n    print(f\"\\nClassification Report : \\n {classification_report(y_test, y_pred)}\") ","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:29:26.402652Z","iopub.execute_input":"2021-09-13T16:29:26.403011Z","iopub.status.idle":"2021-09-13T16:29:26.409114Z","shell.execute_reply.started":"2021-09-13T16:29:26.402973Z","shell.execute_reply":"2021-09-13T16:29:26.408107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN tuned\nparam_grid = {'n_neighbors' : np.arange(1, 30, 2),\n             'metric' : ['euclidean', 'minkowski', 'manhatten']}\n\nbest_param_knn = {'metric': ['euclidean'], 'n_neighbors': [25]}\n\nknnt = KNeighborsClassifier()\ngrid_knnt = GridSearchCV(knnt, best_param_knn, scoring='accuracy', cv=10, refit=True)\ngrid_evaluation(grid_knnt, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:29:40.092465Z","iopub.execute_input":"2021-09-13T16:29:40.092782Z","iopub.status.idle":"2021-09-13T16:29:40.242058Z","shell.execute_reply.started":"2021-09-13T16:29:40.092754Z","shell.execute_reply":"2021-09-13T16:29:40.240572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  svc tuned\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100], \n              'gamma': [1,0.1,0.01,0.001],\n              'kernel': ['rbf']}\n\nbest_param_svc = [{'C': [100], 'gamma': [0.001], 'kernel': ['rbf']}]\n\nsvct = SVC()\ngrid_svct = GridSearchCV(svct, best_param_svc, scoring='accuracy', cv=10, refit=True)\ngrid_evaluation(grid_svct, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:29:48.940793Z","iopub.execute_input":"2021-09-13T16:29:48.941164Z","iopub.status.idle":"2021-09-13T16:29:49.103222Z","shell.execute_reply.started":"2021-09-13T16:29:48.941133Z","shell.execute_reply":"2021-09-13T16:29:49.102246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree tuned\nparam_grid = {\"splitter\":[\"best\",\"random\"],\n            \"max_depth\" : [1,3,5,7,9,11,12],\n            \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n            \"min_weight_fraction_leaf\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n            \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n            \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90]}\n\nbest_param_dt = {'max_depth': [5],\n                 \"max_features\":[None],\n                 \"max_leaf_nodes\":[None],\n                 'min_samples_leaf': [1], \n                 'min_weight_fraction_leaf': [0.1], \n                 'splitter': ['best']}\n\ndt = DecisionTreeClassifier(random_state=42)\ngrid_dt = GridSearchCV(dt, best_param_dt ,scoring='accuracy',cv = 10,refit = True)\ngrid_evaluation(grid_dt, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:30:09.03858Z","iopub.execute_input":"2021-09-13T16:30:09.038913Z","iopub.status.idle":"2021-09-13T16:30:09.117436Z","shell.execute_reply.started":"2021-09-13T16:30:09.03888Z","shell.execute_reply":"2021-09-13T16:30:09.116444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest tuned\nparam_grid = {'n_estimators':[200,500,1000,2000],\n              'max_depth':[2,3,4,5]\n              }\nbest_param_rf = {'max_depth': [3], 'n_estimators': [2000]}\n\nrfct = RandomForestClassifier(random_state=42, n_jobs=-1)\ngrid_rfct = GridSearchCV(rfct, best_param_rf ,scoring='accuracy',cv = 10,refit = True)\ngrid_evaluation(grid_rfct, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:30:16.68376Z","iopub.execute_input":"2021-09-13T16:30:16.684134Z","iopub.status.idle":"2021-09-13T16:31:03.949327Z","shell.execute_reply.started":"2021-09-13T16:30:16.6841Z","shell.execute_reply":"2021-09-13T16:31:03.948352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using optuna for doing hyperparameter tuning in Xgboost. You can uncomment and run to find the optimum parameters\n# # def objective(trial):\n    \n# #     learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n# #     reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n# #     reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n# #     subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n# #     colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n# #     max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n# #     n_estimators= trial.suggest_int(\"n_estimators\", 50,2000,50)\n\n# #     model = XGBClassifier(eval_metric='logloss',\n# #         random_state=42,\n# #         tree_method=\"gpu_hist\",\n# #         gpu_id=0,\n# #         predictor=\"gpu_predictor\",\n# #         n_estimators = n_estimators,\n# #         learning_rate=learning_rate,\n# #         reg_lambda=reg_lambda,\n# #         reg_alpha=reg_alpha,\n# #         subsample=subsample,\n# #         colsample_bytree=colsample_bytree,\n# #         max_depth=max_depth,\n# #     )\n# #     model.fit(X_train_scaled, y_train,verbose=1)\n# #     preds = model.predict(X_test_scaled)\n# #     pred_labels = np.rint(preds)\n# #     accuracy = accuracy_score(y_test, preds)\n# #     return accuracy\n\n# # study = optuna.create_study(direction='maximize')\n# # study.optimize(objective, n_trials=1000)\n# # print(\"Number of finished trials: \", len(study.trials))\n# # print(\"Best trial:\")\n# # trial = study.best_trial\n\n# # print(\"  Value: {}\".format(trial.value))\n# # print(\"  Params: \")\n# # for key, value in trial.params.items():\n# #     print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T15:18:30.847364Z","iopub.execute_input":"2021-09-13T15:18:30.847693Z","iopub.status.idle":"2021-09-13T15:18:30.851722Z","shell.execute_reply.started":"2021-09-13T15:18:30.847662Z","shell.execute_reply":"2021-09-13T15:18:30.85083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost tuned\nlearning_rate= 0.1135930253853376\nreg_lambda= 0.0015187772228404815\nreg_alpha= 3.1569434136364856e-08\nsubsample= 0.19543620768271805\ncolsample_bytree= 0.9783970896407462\nmax_depth= 1\nn_estimators= 100\n\nxgb = XGBClassifier(eval_metric='logloss',\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators = n_estimators,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth\n    )\n\nevaluation(xgb, X_train_scaled, y_train, X_test_scaled, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:33:38.551113Z","iopub.execute_input":"2021-09-13T16:33:38.551474Z","iopub.status.idle":"2021-09-13T16:33:38.643407Z","shell.execute_reply.started":"2021-09-13T16:33:38.551438Z","shell.execute_reply":"2021-09-13T16:33:38.642761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation:\n### 1) Xgboost works best this time also but with higher accuracy and Roc score\n### 2) Xgboost: Accuracy = 0.8052 and Roc_auc = 0.7838\n### 3) Xgboost has False Negative 16 and False Positive 14 which is good.\n### 4) Random Forest also improved. Accuracy = 0.7922 and Roc_auc = 0.7495","metadata":{}},{"cell_type":"markdown","source":"# Stacking","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:38:22.729779Z","iopub.execute_input":"2021-09-13T16:38:22.73012Z","iopub.status.idle":"2021-09-13T16:38:22.733529Z","shell.execute_reply.started":"2021-09-13T16:38:22.730092Z","shell.execute_reply":"2021-09-13T16:38:22.732593Z"}}},{"cell_type":"code","source":"# Stacking two best models RandomForestClassifier() and XGBClassifier() to see if performance increases\nestimators_list = [\n    ('rf', grid_rfct),\n    ('xgb', xgb)]\n\nstack_model = StackingClassifier(estimators = estimators_list, final_estimator=LogisticRegression())\n\nstack_model.fit(X_train_scaled, y_train)\ny_train_pred = stack_model.predict(X_train_scaled)\n\ny_test_pred = stack_model.predict(X_test_scaled)\n\nprint(f\"Accuracy Score (Train set) : {accuracy_score(y_train, y_train_pred)}\")\nprint(\"*\"*50)\nprint(f\"\\nAccuracy Score (Test set) : {accuracy_score(y_test, y_test_pred)}\")\nprint(\"*\"*50)\nprint(f\"\\nRoc auc score : {roc_auc_score(y_test, y_test_pred)}\")\nprint(\"*\"*50)\nprint(f\"\\nConfusion Matrix : \\n {confusion_matrix(y_test, y_test_pred)}\")\nprint(\"*\"*50)\nprint(f\"\\nClassification Report : \\n {classification_report(y_test, y_test_pred)}\") ","metadata":{"execution":{"iopub.status.busy":"2021-09-13T16:39:31.280861Z","iopub.execute_input":"2021-09-13T16:39:31.281232Z","iopub.status.idle":"2021-09-13T16:43:57.786482Z","shell.execute_reply.started":"2021-09-13T16:39:31.2812Z","shell.execute_reply":"2021-09-13T16:43:57.785799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation:\n### After Stacking: Accuracy = 0.7922 and Roc_auc = 0.7957 . Both are less than the tuned XGBClassifier()\n### So, Hyperparameter tuned XGBClassifier is the best model among others.","metadata":{}},{"cell_type":"markdown","source":"## Thank you. Leave an upvote if you liked my notebook. Leave a suggestion if I can improve it.","metadata":{}}]}