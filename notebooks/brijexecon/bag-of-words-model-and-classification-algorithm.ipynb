{"cells":[{"metadata":{"_uuid":"7c7f409391147476440b45481e9350133a4d5ac7"},"cell_type":"markdown","source":"This is my first kaggle contribution.\nCredit goes to best explained kernel [**Text Preprocessing and Machine Learning Modeling**](https://www.kaggle.com/futurist/text-preprocessing-and-machine-learning-modeling)\n\n**Simple binary classification problem**\n\nData preprocessing involves,\n* Removing unnecessary columns and renaming features name\n* Numericalizing categorical feature which is our label (ham or sam)\n* Genearting corpus from raw sms mesages (stopwords,lowering,stemming)\n* Creating bag of words model using CountVectorizer (You must try some other as well)\n\nUpto this point we have preprocessed our data, generated features and ready to fit into classifier algorithm.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import datasets\ndata = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a18cfc2b9caac4a9371826747dfb609d9cdce334"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"484bbef60bdb8cd1d62c391873164af7e58dacc5"},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"trusted":true,"_uuid":"a8d1d1f7c377f81890ed4eef080561b2ff06aa92"},"cell_type":"code","source":"#drop unwanted columns and name change\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70bf233d189cf8c5d53d54256e9490e75feee645"},"cell_type":"code","source":"# convert label to a numerical variable\ndata['label'] = data.label.map({'ham':0, 'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c5a563fef5e1ed66bbd439260fd19d7faad1a8"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6608e31f1007affeabbcc559fb73886f7f0bb462"},"cell_type":"code","source":"#count observations in each label\ndata.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10bfa42935d1fb0aab4f91d128eb8b9983de9e34"},"cell_type":"markdown","source":"From above, we can say datasets are imbalance hence we can perform certain sampling technique to make it balance datasets.\nI'm not doing it here to make it more simpler.\n# Feature creation i.e bags of words model from sms texts"},{"metadata":{"trusted":true,"_uuid":"58684b9b01333c20f6a39981842ff6a865017c6a"},"cell_type":"code","source":"#text transformation (stopwords,lowering,stemming) and creating bag of words model using CountVectorizer\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db7641b6dfc9c7da8da01c3fe6fbe48f0a157ea3"},"cell_type":"code","source":"corpus = []\nfor i in range(0, len(data)):\n    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69eec75ca9d8fedcc5072899a41c2366f31c1daa"},"cell_type":"code","source":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, 0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f79bbb34ccdd145f66efd1c7eb3e5ad6c6c4ac6a"},"cell_type":"code","source":"#showing first and last 20 features names\nprint(cv.get_feature_names()[0:20])\nprint(cv.get_feature_names()[-20:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"086b3577ebb29fd5ea2ddebcfadc4dbac4ca49a9"},"cell_type":"code","source":"print(X.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e90af6dc82d14485bba594bcfe6d089df2c57e44"},"cell_type":"markdown","source":"Splitting datasets into training and test data\n"},{"metadata":{"trusted":true,"_uuid":"c90ed0b5ecd423cd0a6ddf27245bd40663224898"},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ff3b7de13e6917b593dca0b4d3831eaab9133d"},"cell_type":"code","source":"print(X_train.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeaff6bea928b89d878f049bd4ee9a46a9e4231d"},"cell_type":"markdown","source":"Data visualisation just for fun "},{"metadata":{"trusted":true,"_uuid":"74bcc6fdba9adf5d2bafc25420b4d6835f858789"},"cell_type":"code","source":"#Visualisations\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f519839d09c88b3be3d94bce94b1ad1a93dc9576"},"cell_type":"code","source":"ham_words = ''\nspam_words = ''\nspam = data[data.label == 1]\nham = data[data.label ==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45ffa74ad7acae077c5c65a27958cd0438a57c83"},"cell_type":"code","source":"for val in spam.text:\n    text = re.sub('[^a-zA-Z]', ' ', val)\n    text = text.lower()\n    text = text.split()\n    ps = PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    for words in text:\n        spam_words = spam_words + words + ' '\n    \nfor val in ham.text:\n    text = re.sub('[^a-zA-Z]', ' ', val)\n    text = text.lower()\n    text = text.split()\n    ps = PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    for words in text:\n          ham_words = ham_words + words + ' '\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f90c159e605cdb67d693bdea2e593d02c70a91eb"},"cell_type":"code","source":"# Generate a word cloud image\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c8a7d0684de059f7ff89170c678a5d9da39bcf5"},"cell_type":"code","source":"#Spam Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2cb0ec2c23d53d5e5fb2a1c25d0dff5d393ec38"},"cell_type":"code","source":"#Ham word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a91bd75a5758c2fae71ca3342f7ddcc76d46c4"},"cell_type":"markdown","source":"# Machine learning models"},{"metadata":{"trusted":true,"_uuid":"6f91680fb6ceb1d25aa0bdd09aa3e1a5c1fd8865"},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set (Gaussian NB)\nprediction = dict()\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf8935e393beae2db497ff185d60a2216514a0a"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"GaussianNB\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"GaussianNB\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70bcd4a71ca35be64a32a32c570ae9912ce5fc73"},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set (Multinomial NB)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b757e09cb2b60d0161992bdc19c510e9cc971637"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"MultinomialNB\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"MultinomialNB\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe34e47db353df98008d1b9477bd8bea035fc90"},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set (Multinomial NB)\nfrom sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd56685631cfe41f98a2aa811639ef10f1cc0886"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"BernouliiNB\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"BernouliiNB\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2afcc2b494bedfc2aa278f8aeaaa9a3a9f5ba61e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b11d273b2b3a68650b3b53f587d5782782ac242"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"Logistic\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"Logistic\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d52f209ae231ace84ad7c742a43bb6fc3c703f4"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f88f93a9925ce91661a9312fdf4ff360c30e79c2"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"KNN\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"KNN\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32d98a8db5c4399b37f983c66ee5a1456ee46e6b"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ff927941a7e8af3dbd90051fd74af77655ce0cd"},"cell_type":"code","source":"# Predicting the Test set results\nprediction[\"RandomForrest\"] = classifier.predict(X_test)\naccuracy_score(y_test,prediction[\"RandomForrest\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbba09f4c0eeafdad95256a62fcf668f0831d81"},"cell_type":"markdown","source":"Model evaluation"},{"metadata":{"trusted":true,"_uuid":"6903d9de3e359f8a2aa469f5dd9b1b22aee12653"},"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, prediction['MultinomialNB'])\nconf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca774a614429566c369a5c21203f4c4203ab4169"},"cell_type":"code","source":"sns.heatmap(conf_mat_normalized)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c2e0fc28a1a79f2706a51b20428b7dcd45c012a"},"cell_type":"code","source":"print(conf_mat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba381ca5ecef4e58a8440895cdf2b19e5ea4d5b1"},"cell_type":"markdown","source":"By seeing the above confusion matrix, it is clear that 17 Ham are mis classified as Spam, and 6 Spam are misclassified as Ham. "},{"metadata":{"trusted":true,"_uuid":"cf0bdbd997a38352bcae14207764cf27083805a7"},"cell_type":"code","source":"#model evaluation\nprint(classification_report(y_test, prediction['MultinomialNB'], target_names = [\"Ham\", \"Spam\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"269a0f40285fe521d5cfe4df7cce6334db3ebf76"},"cell_type":"markdown","source":"Generally, Naive Bayes works well on text data. Multinomail Naive bayes is best suited for this classification problem."},{"metadata":{"trusted":true,"_uuid":"f0eb90bc9499c3cf22ca1c96ee01dc7790f461bc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}