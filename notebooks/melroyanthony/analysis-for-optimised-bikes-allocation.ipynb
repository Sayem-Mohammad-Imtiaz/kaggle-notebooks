{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Problem Statement**\n- 🔍 analyze the users' bike usage pattern wrt day and week, weather, and ride-distances to build a visualization.\n- 🔮 suggest the approaches to build the prediction model to optimize bikes allocation based on the visualization above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the weather ⛅️\nThe weather data would help us give insights on ***how exactly is the weather event in the particular region affects the bike usage on a daily basis***. Before diving deeper into bike usage patterns, we would be performing exploratory data analysis on weather data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather = pd.read_csv(\"/kaggle/input/sf-bay-area-bike-share/weather.csv\")\n\n# We only require date, zip_code, and events for our analysis\ndf_weather = df_weather[[\"date\", \"zip_code\", \"events\"]]\n\n# Convert date to datetime format for uniform analytic exercises\ndf_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\ndf_weather[\"dd\"] = df_weather[\"date\"].dt.day\ndf_weather[\"mm\"] = df_weather[\"date\"].dt.month\ndf_weather[\"yyyy\"] = df_weather[\"date\"].dt.year\n\n# Sort the weather dataframe by zip_code ascending and date ascending\ndf_weather.sort_values(by=[\"zip_code\", \"date\"], inplace=True)\ndf_weather = df_weather[[\"zip_code\", \"dd\", \"mm\", \"yyyy\", \"events\"]]\n\n# Filling missing values in event with Sunny\ndf_weather[\"events\"].fillna(\"Sunny\", inplace=True)\n\n# Normalizing the noise i.e. 'rain' -> 'Rain', 'Fog-Rain' -> 'Rain', 'Rain-Thunderstorm' -> 'Rain'\n# Also, creating a zip_code_str as type 'str' for visualization\ndf_weather[\"events\"].replace([\"rain\", \"Fog-Rain\", \"Rain-Thunderstorm\"], [\"Rain\", \"Rain\", \"Rain\"], inplace=True)\ndf_weather[\"zip_code_str\"] = df_weather[\"zip_code\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventually, the weather data looks somewhat like this ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weather.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we would be grouping the weather data to get insights on the number of weathers events per ZIP code for the given year ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate the weather based on number of events per year per zip_code\ndf_weather_eda = pd.DataFrame(\n    df_weather[[\"yyyy\", \"zip_code_str\", \"events\"]].value_counts()\n).sort_values(by=[\"yyyy\", \"zip_code_str\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data for visualization\nweather_data = {}\n\nfor k, v in df_weather_eda[0].items():\n    if not k[0] in weather_data:\n        weather_data[k[0]] = {}\n    if k[2] in weather_data[k[0]]:\n        weather_data[k[0]][k[2]][\"zip_code\"] += [k[1]]\n        weather_data[k[0]][k[2]][\"count\"] += [v]\n    else:\n        weather_data[k[0]][k[2]] = {\n            \"zip_code\": [k[1]],\n            \"count\": [v]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have the data prepared for visualization, we will be going forward with plotting the bar graphs depicting the yearly weather patterns (i.e. **Sunny** 🌞, **Rain** 🌧, **Fog** ☁️) for every ZIP code."},{"metadata":{"trusted":true},"cell_type":"code","source":"colour = {\"Sunny\": \"darkorange\", \"Rain\": \"mediumturquoise\", \"Fog\": \"grey\"}\nfor year, events in weather_data.items():\n    data = []\n    for event, event_data in events.items():\n        data += [\n            go.Bar(\n                x=event_data[\"zip_code\"],\n                y=event_data[\"count\"],\n                name=event,\n                marker_color=colour[event]\n            )\n        ]\n    fig = go.Figure(data=data)\n    fig.update_layout(barmode='group',\n                      title=f\"{year} Weather Forecast\",\n                      xaxis={\"title\": \"ZIP Codes\"},\n                      yaxis={\"title\": \"No. of days\"})\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the data above, we can easily conclude that...\n> \"It is always (mostly) sunny in SF bay area 🌞\""},{"metadata":{},"cell_type":"markdown","source":"## Understanding the stations 🚏\nNow that we have clearer understanding of the distribution of weather data, we can now move forward with understanding patterns in the bike station dataset. \n\n📝 **NOTE:** We would be laying our emphasis on *exploring the distribution of dock capacity per station* out here."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_station = pd.read_csv(\"/kaggle/input/sf-bay-area-bike-share/station.csv\")\n\n# Choose relevant columns\ndf_station = df_station[[\"id\", \"name\", \"city\", \"dock_count\"]]\n\n# Sort the values based on city and dock_count in ascending order\ndf_station.sort_values(by=[\"city\", \"dock_count\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once, we have performed the necessary pre-processing on station data, the station data looks something like ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_station.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dock count shows the following analogies:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_station.dock_count.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the no. of dock_count, we found the occurence of no. of stations having similar dock_counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_station.dock_count.value_counts()).set_axis([\"No. of stations\"], axis=\"columns\", inplace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following are the no. of stations present in each city."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_station.city.value_counts()).set_axis([\"No. of stations\"], axis=\"columns\", inplace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we visualize the dock capacity of each station in each and every city using the pie charts ."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data for visualization\ncities = df_station.city.unique()\n\nfor city in cities:\n    _df = df_station[df_station.city == city]\n    data = [go.Pie(labels=_df.name.to_list(), values=_df.dock_count.to_list(), hole=.4)]\n    fig = go.Figure(data=data)\n    fig.update_traces(hoverinfo='label+percent', textinfo='value')\n    fig.update_layout(title=f\"Max. dock capacity in {city.strip()}\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the trips 🛣\nNow that we understand the patterns in the station and weather data, we now proceed to understand each and every transaction of the trips data. This dataset should help us make inferences on -\n- 🚲 Patterns for total trips per bikes.\n- 🚉 Patterns for average trip duration and total trip duration per weather event for each bike.\n- 🚉 Patterns for total no. of trips, average daily trip duration and total trip duration per route.\n- ⛅️ Patterns for average daily trips and total trips per weather event per start station\n- 🚏 Patterns for popular route distribution based on number of trips and total number of trip duration\n\n📝 **NOTE:** We can generally find deeper patterns in the trips data for each and every city, but, for limiting our scope of this research, we will only be diving deeper into trips from and within San Francisco 🌉."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trip = pd.read_csv(\"/kaggle/input/sf-bay-area-bike-share/trip.csv\")\n\n# Filter the trips for only the ZIP Codes where weather data is available\nzip_code = df_weather.zip_code_str.unique()\ndf_trip = df_trip[df_trip[\"zip_code\"].isin(zip_code)]\n\n# Choose relevant columns\ndf_trip = df_trip[[\"bike_id\", \"duration\", \"start_date\", \"start_station_id\", \"end_station_id\", \"subscription_type\", \"zip_code\"]]\n\n# Normalize the duration to minutes and convert start_date to datetime type\ndf_trip[\"duration\"] = df_trip[\"duration\"] // 60\ndf_trip['duration'].quantile(0.9)\ndf_trip = df_trip[df_trip[\"duration\"] <= 60 * 24]  # Filter trips with duration of more than 24 hours\ndf_trip[\"start_date\"] = pd.to_datetime(df_trip[\"start_date\"])\n\n# Create individual values for date for efficient querying\ndf_trip[\"dd\"] = df_trip[\"start_date\"].dt.day\ndf_trip[\"mm\"] = df_trip[\"start_date\"].dt.month\ndf_trip[\"yyyy\"] = df_trip[\"start_date\"].dt.year\n\n# Drop 'start_date' column since we have already splitted it into 'dd', 'mm', 'yyyy'\ndf_trip.drop(columns=[\"start_date\"], inplace=True)\n\n# Join the weather data with the trip data to gain contextual insights on the effects of weather \ndf_trip = pd.merge(df_trip, \n                   df_weather, \n                   left_on=[\"zip_code\", \"dd\", \"mm\", \"yyyy\"], \n                   right_on=[\"zip_code_str\", \"dd\", \"mm\", \"yyyy\"], \n                   how=\"left\").drop([\"zip_code_y\"], axis=1)\n\n# Filtering trips other than San Francisco (SF)\n# NOTE: There are 95272 trips overall, out of which:\n# 1. 83857 trips originated from one of the stations in SF\n# 2. 83858 trips ended at one of the stations of SF\n# 3. 83856 trips were between stations in SF\n# 4. one trip originated from one of the stations in SF but did not end in one of the stations in SF\n# 5. two trips originated from the stations outside of SF but ended in one of the stations in SF\nsf_station_ids = df_station[df_station[\"city\"] == \"San Francisco\"][\"id\"].values\ndf_trip = df_trip[df_trip[\"start_station_id\"].isin(sf_station_ids)]\ndf_trip = df_trip[df_trip[\"end_station_id\"].isin(sf_station_ids)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventually, the following is the rough schematics of the trips data..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trip.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = df_trip[\"subscription_type\"].value_counts()\nsubs_per_event = df_trip.groupby([\"subscription_type\"])[\"events\"].value_counts()\nsubs_per_event_per_bike = df_trip.groupby([\"subscription_type\", \"events\"])[\"bike_id\"].value_counts()\n\nids = []\nlabels = []\nparents = []\nvalues = []\n\nfor k, v in subs.items():\n    ids += [k.strip()]\n    labels += [k.strip()]\n    parents += [\"\"]\n    values += [v]\n    \nfor k, v in subs_per_event.items():\n    ids += [\"|\".join(k)]\n    labels += [k[1]]\n    parents += [k[0]]\n    values += [v]\n    \nfor k, v in subs_per_event_per_bike.items():\n    ids += [\"{}|{}|{}\".format(*k)]\n    labels += [f\"B-{k[2]}\"]\n    parents += [\"|\".join(k[:2])]\n    values += [v]\n\nfig = go.Figure(\n    go.Sunburst(\n        ids=ids,\n        labels=labels,\n        parents=parents,\n        values=values,\n        branchvalues=\"total\",\n        maxdepth=3\n    )\n)\nfig.update_layout(margin=dict(t=30, l=30, r=30, b=30),\n                  title=\"Total trips per bike per event per subscription\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the distribution of data is uneven *(i.e. **91759 Subscribers** and **3513 Customers**)*. The distribution is biased for having many instances of Subscribers. Regardless of the `subscription_type`, there are higher number fo instances for the trips during `Sunny` followed by `Rain` and then at last the trips during `Fog`.\n"},{"metadata":{},"cell_type":"markdown","source":"Similar to the visualization above, we would try to explore the distribution of trips wrt total trip duration and average trip duration per bike."},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = df_trip.groupby([\"subscription_type\"])[\"duration\"].sum()\nsubs_per_event = df_trip.groupby([\"subscription_type\", \"events\"])[\"duration\"].sum()\nsubs_per_event_per_bike = df_trip.groupby([\"subscription_type\", \"events\", \"bike_id\"])[\"duration\"].sum()\n\nids = []\nlabels = []\nparents = []\nvalues = []\n\nfor k, v in subs.items():\n    ids += [k.strip()]\n    labels += [k.strip()]\n    parents += [\"\"]\n    values += [v]\n    \nfor k, v in subs_per_event.items():\n    ids += [\"|\".join(k)]\n    labels += [k[1]]\n    parents += [k[0]]\n    values += [v]\n    \nfor k, v in subs_per_event_per_bike.items():\n    ids += [\"{}|{}|{}\".format(*k)]\n    labels += [f\"B-{k[2]}\"]\n    parents += [\"|\".join(k[:2])]\n    values += [v]\n\nfig = go.Figure(\n    go.Sunburst(\n        ids=ids,\n        labels=labels,\n        parents=parents,\n        values=values,\n        branchvalues=\"total\",\n        maxdepth=3\n    )\n)\nfig.update_layout(margin=dict(t=30, l=30, r=30, b=30),\n                  title=\"Total trip duration per bike per event per subscription\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = df_trip.groupby([\"subscription_type\"])[\"duration\"].mean()\nsubs_per_event = df_trip.groupby([\"subscription_type\", \"events\"])[\"duration\"].mean()\nsubs_per_event_per_bike = df_trip.groupby([\"subscription_type\", \"events\", \"bike_id\"])[\"duration\"].mean()\n\nids = []\nlabels = []\nparents = []\nvalues = []\n\nfor k, v in subs.items():\n    ids += [k.strip()]\n    labels += [k.strip()]\n    parents += [\"\"]\n    values += [v]\n    \nfor k, v in subs_per_event.items():\n    ids += [\"|\".join(k)]\n    labels += [k[1]]\n    parents += [k[0]]\n    values += [v]\n    \nfor k, v in subs_per_event_per_bike.items():\n    ids += [\"{}|{}|{}\".format(*k)]\n    labels += [f\"B-{k[2]}\"]\n    parents += [\"|\".join(k[:2])]\n    values += [v]\n\nfig = go.Figure(\n    go.Sunburst(\n        ids=ids,\n        labels=labels,\n        parents=parents,\n        values=values,\n        maxdepth=3\n    )\n)\nfig.update_layout(margin=dict(t=30, l=30, r=30, b=30),\n                  title=\"Avg. trip duration per bike per event per subscription\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"route_duration = df_trip.groupby([\"yyyy\", \"events\", \"start_station_id\", \"end_station_id\"])[\"duration\"].sum()\n\nstations = df_station[[\"name\", \"id\"]].sort_values(by=[\"id\"])[df_station[\"id\"].isin(sf_station_ids)]\nstation_names = stations.name.values\nstation_ids = stations.id.values\n\nroute_data = {}\nfor k, v in route_duration.items():\n    if not k[0] in route_data:\n        route_data[k[0]] = {}\n    if not k[1] in route_data[k[0]]:\n        route_data[k[0]][k[1]] = None\n        \nfor year, v in route_data.items():\n    for event, data in v.items():\n        starts = []\n        for start in station_ids:\n            ends = []\n            for end in station_ids:\n                try:\n                    ends += [route_duration[(year,event,start,end)]]\n                except KeyError:\n                    ends += [None]\n            starts += [ends]\n        route_data[year][event] = np.transpose(np.array(starts))\n        \nfor year, year_data in route_data.items():\n    for event, data in year_data.items():\n        fig = go.Figure(\n            go.Heatmap(\n                z=data,\n                y=list(station_names),\n                x=list(station_names),\n                type='heatmap',\n                colorscale='Viridis'\n            )\n        )\n        fig.update_layout(\n            title=f\"{year} total trip duration per route ({event})\",\n            xaxis={\"title\": \"Start Station\"},\n            yaxis={\"title\": \"End Station\"},\n            margin=dict(t=30, r=10, b=10, l=10), \n            width=700, \n            height=600,\n            autosize=False\n        )\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"route_duration = df_trip.groupby([\"yyyy\", \"events\", \"start_station_id\", \"end_station_id\"])[\"duration\"].mean()\n\nstations = df_station[[\"name\", \"id\"]].sort_values(by=[\"id\"])[df_station[\"id\"].isin(sf_station_ids)]\nstation_names = stations.name.values\nstation_ids = stations.id.values\n\nroute_data = {}\nfor k, v in route_duration.items():\n    if not k[0] in route_data:\n        route_data[k[0]] = {}\n    if not k[1] in route_data[k[0]]:\n        route_data[k[0]][k[1]] = None\n        \nfor year, v in route_data.items():\n    for event, data in v.items():\n        starts = []\n        for start in station_ids:\n            ends = []\n            for end in station_ids:\n                try:\n                    ends += [route_duration[(year,event,start,end)]]\n                except KeyError:\n                    ends += [None]\n            starts += [ends]\n        route_data[year][event] = np.transpose(np.array(starts))\n        \nfor year, year_data in route_data.items():\n    for event, data in year_data.items():\n        fig = go.Figure(\n            go.Heatmap(\n                z=data,\n                y=station_names,\n                x=station_names,\n                type='heatmap',\n                colorscale='Viridis'\n            )\n        )\n        fig.update_layout(\n            title=f\"{year} avg. trip duration per route ({event})\",\n            xaxis={\"title\": \"Start Station\"},\n            yaxis={\"title\": \"End Station\"},\n            margin=dict(t=30, r=10, b=10, l=10), \n            width=700, \n            height=600,\n            autosize=False\n        )\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"route_count = df_trip.groupby([\"yyyy\", \"events\", \"start_station_id\"])[\"end_station_id\"].value_counts()\n\nstations = df_station[[\"name\", \"id\"]].sort_values(by=[\"id\"])[df_station[\"id\"].isin(sf_station_ids)]\nstation_names = stations.name.values\nstation_ids = stations.id.values\n\nroute_data = {}\nfor k, v in route_count.items():\n    if not k[0] in route_data:\n        route_data[k[0]] = {}\n    if not k[1] in route_data[k[0]]:\n        route_data[k[0]][k[1]] = None\n        \nfor year, v in route_data.items():\n    for event, data in v.items():\n        starts = []\n        for start in station_ids:\n            ends = []\n            for end in station_ids:\n                try:\n                    ends += [route_count[(year,event,start,end)]]\n                except KeyError:\n                    ends += [None]\n            starts += [ends]\n        route_data[year][event] = np.transpose(np.array(starts))\n        \nfor year, year_data in route_data.items():\n    for event, data in year_data.items():\n        fig = go.Figure(\n            go.Heatmap(\n                z=data,\n                y=station_names,\n                x=station_names,\n                type='heatmap',\n                colorscale='Viridis'\n            )\n        )\n        fig.update_layout(\n            title=f\"{year} trips per route ({event})\",\n            xaxis={\"title\": \"Start Station\"},\n            yaxis={\"title\": \"End Station\"},\n            margin=dict(t=30, r=10, b=10, l=10), \n            width=700, \n            height=600, \n            autosize=False\n        )\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_sstation_trips = df_trip.groupby([\"start_station_id\", \"events\", \"yyyy\", \"mm\", \"dd\"])[\"duration\"].mean()\n\nsstation_data = {}\nfor k, v in avg_sstation_trips.items():\n    if not k[0] in sstation_data:\n        sstation_data[k[0]] = {}\n    if k[1] in sstation_data[k[0]]:\n        sstation_data[k[0]][k[1]][\"date\"] += [f\"{k[4]}-{k[3]}-{k[2]}\"]\n        sstation_data[k[0]][k[1]][\"avg\"] += [v]\n    else:\n        sstation_data[k[0]][k[1]] = {\n            \"date\": [f\"{k[4]}-{k[3]}-{k[2]}\"],\n            \"avg\": [v]\n        }\n\ncolour = {\"Sunny\": \"darkorange\", \"Rain\": \"mediumturquoise\", \"Fog\": \"grey\"}\nfor sub_type, event_data in sstation_data.items():\n    fig = go.Figure()\n    category_array = []\n    for event, data in event_data.items():\n        fig.add_trace(\n            go.Scatter(\n                x=data[\"date\"],\n                y=data[\"avg\"],\n                name=f'{event}',\n                mode=\"markers\",\n                marker_color=colour[event]\n            )\n        )\n        category_array += data[\"date\"]\n    category_array.sort(key=lambda date: datetime.strptime(date, '%d-%m-%Y'))\n    fig.update_xaxes(rangeslider_visible=True, categoryorder=\"array\", categoryarray=category_array)\n    fig.update_layout(yaxis={\"title\": f\"Avg. duration of bikes used ({df_station[df_station.id == sub_type].name.values[0]})\"})\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sstation_trips = df_trip.groupby([\"start_station_id\", \"events\", \"yyyy\", \"mm\", \"dd\"])[\"duration\"].sum()\n        \nmax_sstation_data = {}\nfor k, v in max_sstation_trips.items():\n    if not k[0] in max_sstation_data:\n        max_sstation_data[k[0]] = {}\n    if k[1] in max_sstation_data[k[0]]:\n        max_sstation_data[k[0]][k[1]][\"date\"] += [f\"{k[4]}-{k[3]}-{k[2]}\"]\n        max_sstation_data[k[0]][k[1]][\"sum\"] += [v]\n    else:\n        max_sstation_data[k[0]][k[1]] = {\n            \"date\": [f\"{k[4]}-{k[3]}-{k[2]}\"],\n            \"sum\": [v]\n        }\n        \ncolour = {\"Sunny\": \"darkorange\", \"Rain\": \"mediumturquoise\", \"Fog\": \"grey\"}\nfor sub_type, event_data in max_sstation_data.items():\n    fig = go.Figure()\n    category_array = []\n    for event, data in event_data.items():\n        fig.add_trace(\n            go.Scatter(\n                x=data[\"date\"],\n                y=data[\"sum\"],\n                name=f'{event}',\n                mode=\"markers\",\n                marker_color=colour[event]\n            )\n        )\n        category_array += data[\"date\"]\n    category_array.sort(key=lambda date: datetime.strptime(date, '%d-%m-%Y'))\n    fig.update_xaxes(rangeslider_visible=True, categoryorder=\"array\", categoryarray=category_array)\n    fig.update_layout(yaxis={\"title\": f\"Total duration of bikes used ({df_station[df_station.id == sub_type].name.values[0]})\"})\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the status of bike station 🚏\nNow that eventually we have got a deeper understanding of the trip dataset. We will now try to make some more detailed inferences on bike's docking status.\n\n🚨 **ALERT:** This dataset is huge with ***~71 Million records*** since this is more like a cron polling bike station's current dock status"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_status = pd.read_csv(\"/kaggle/input/sf-bay-area-bike-share/status.csv\")\n\n# Convert the 'time' to type 'pd.datetime'\ndf_status[\"time\"] = pd.to_datetime(df_status[\"time\"])\n\n# Splitting the 'time' into 'dd', 'mm', 'yyyy', 'hh'\ndf_status[\"dd\"] = df_status[\"time\"].dt.day\ndf_status[\"mm\"] = df_status[\"time\"].dt.month\ndf_status[\"yyyy\"] = df_status[\"time\"].dt.year\ndf_status[\"hh\"] = df_status[\"time\"].dt.hour\n\n# Dropping 'time' and 'bikes_available'\ndf_status.drop(columns=[\"time\", \"bikes_available\"], inplace=True)\n\n# We take distinct records for each hourly data \n# since in the current format the dataset is recorded every minute\ndf_status.drop_duplicates(inplace=True)\ndf_status.drop_duplicates(subset=[\"station_id\", \"dd\", \"mm\", \"yyyy\", \"hh\"], inplace=True, keep=\"last\")\n\n# Filter the status for stations in SF\ndf_status = df_status[df_status.station_id.isin(sf_station_ids)]\n\n# Join the trip data and bike status data to gain contextual insights on the effects of weather on bike station status\ndf_trip = df_trip[[\"start_station_id\", \"dd\", \"mm\", \"yyyy\", \"events\"]]\ndf_trip.drop_duplicates(inplace=True)\ndf_status = pd.merge(df_status, \n                     df_trip, \n                     left_on=[\"station_id\", \"dd\", \"mm\", \"yyyy\"], \n                     right_on=[\"start_station_id\", \"dd\", \"mm\", \"yyyy\"], \n                     how=\"left\").drop([\"start_station_id\"], axis=1)\ndf_status.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_status.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_docks_vacant = df_status.groupby([\"station_id\", \"yyyy\", \"mm\", \"dd\"])[\"docks_available\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"businesses = df_status.station_id.unique()\n\nbusiness_pulse = {}\nfor business in businesses:\n    subset = df_status[df_status.station_id == business][[\"dd\", \"mm\", \"yyyy\"]]\n    business_days = list(subset.drop_duplicates().itertuples(index=False))\n    business_pulse[business] = {\n        \"xaxis\": {\n            year: [f\"{day.dd}-{day.mm}-{day.yyyy}\" for day in business_days if day.yyyy == year]\n            for year in subset.yyyy.unique()\n        },\n        \"yaxis\": {\n            year: [avg_docks_vacant[(business,day.yyyy,day.mm,day.dd)] for day in business_days if day.yyyy == year]\n            for year in subset.yyyy.unique()\n        }\n    }\n    all_dock_count = [avg_docks_vacant[(business,day.yyyy,day.mm,day.dd)] for day in business_days]\n    business_pulse[business][\"hline_y_abs\"] = sum(all_dock_count) // len(all_dock_count)\n    business_pulse[business][\"hline_y\"] = sum(all_dock_count) / len(all_dock_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for business, pulse in business_pulse.items():\n    data = [\n        go.Scatter(x=xdata,y=pulse[\"yaxis\"][year],mode='lines',name=f\"{year}\")\n        for year, xdata in pulse[\"xaxis\"].items()\n    ]\n    fig = go.Figure(data=data)\n    fig.update_xaxes(rangeslider_visible=True)\n    fig.add_hline(y=pulse[\"hline_y\"],\n                  line_dash=\"dot\",\n                  annotation_text=f\"Baseline Average = {pulse['hline_y_abs']}/Hour\", \n                  annotation_position=\"bottom right\")\n    fig.update_layout(title=f\"Avg. Hourly Active Usage (HAU) for Bikes in {df_station[df_station['id'] == business].name.values[0]}\",\n                      yaxis={\"title\": \"HAU\"})\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pulse_avg = [\n    {\n        \"station_name\": df_station[df_station.id == station_id].name.values[0],\n        \"avg_bikes_used_per_hour_per_day\": v[\"hline_y\"],\n        \"is_performing_above_avg\": v[\"hline_y\"] > (df_station[df_station.id == station_id].dock_count.values[0] / 2)\n    }\n    for station_id, v in business_pulse.items()\n]\npd.DataFrame(df_pulse_avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = []\nfor station_id, v in business_pulse.items():\n    bikes_available = []\n    \n    station_name = df_station[df_station.id == station_id].name.values[0]\n    dock_count = df_station[df_station.id == station_id].dock_count.values[0]\n    \n    for year, aba in v[\"yaxis\"].items():\n        bikes_available += aba\n    bikes_available = np.array(bikes_available) / dock_count\n    \n    min_max_scaler = MinMaxScaler()\n    norm_bikes_available = min_max_scaler.fit_transform(bikes_available.reshape(-1,1))\n    \n    bins = [0, 0.5, 0.7]\n    names = [\"Bad\", \"Normal\", \"Good\"]\n    d = dict(enumerate(names, 1))\n    \n    cat_bikes_available = np.vectorize(d.get)(np.digitize(norm_bikes_available[:,0], bins))\n    all_count = len(cat_bikes_available)\n    \n    df_data += [\n        {\n            \"station_name\": station_name,\n            \"underperformed_days\": (cat_bikes_available == \"Bad\").sum(),\n            \"normal_days\": (cat_bikes_available == \"Normal\").sum(),\n            \"optimal_days\": (cat_bikes_available == \"Good\").sum(),\n            \"underperformed_days_percent\": ((cat_bikes_available == \"Bad\").sum() / all_count) * 100,\n            \"normal_days_percent\": ((cat_bikes_available == \"Normal\").sum() / all_count) * 100,\n            \"optimal_days_percent\": ((cat_bikes_available == \"Good\").sum() / all_count) * 100\n        }\n    ]\n\npd.DataFrame(df_data).sort_values(by=[\"underperformed_days_percent\", \"normal_days_percent\"], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_data).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_status.groupby([\"events\",\"station_id\",\"dd\",\"mm\",\"yyyy\"])[\"docks_available\"].sum()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}