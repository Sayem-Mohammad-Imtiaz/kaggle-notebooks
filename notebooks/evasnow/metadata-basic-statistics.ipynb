{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport collections\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\nimport time\nimport spacy\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read-in metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"metaDataPath = \"/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv\"\nmetaData = pd.read_csv(metaDataPath, header = 0, index_col = 0)\nprint(\"The number of literatures: \" + str(metaData.shape[0]))\nmetaData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nan elements counts in all columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"metaData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Literature source distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sourceDic = collections.defaultdict(int)\nfor s in metaData[\"source_x\"][metaData[\"source_x\"].notnull()]:\n    sourceDic[s] += 1\nsizes = []\nexplode = []\nlabels = []\nfor s in sourceDic:\n    sizes.append(sourceDic[s])\n    explode.append(0)\n    labels.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['gold', 'lightskyblue', 'yellowgreen', 'lightcoral']\nplt.pie(sizes, explode=explode, labels=labels, colors = colors, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Publish year distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"yearList = []\nfor y in metaData[\"publish_time\"][metaData[\"publish_time\"].notnull()]:\n    yearList.append(int(re.split(' |-', y)[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(yearList, bins = 50)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Has full text distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"hasFullText = metaData[\"has_full_text\"][metaData[\"has_full_text\"].notnull()]\nnanCount = metaData.shape[0] - hasFullText.shape[0]\ntrueCount = sum(hasFullText)\nfalseCount = hasFullText.shape[0] - trueCount\nprint(\"The number of literatures with full text: \" + str(trueCount))\nprint(\"The number of literatures without full text: \" + str(falseCount + nanCount))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Abstract distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of literatures with abstract: \" + str(sum(metaData[\"abstract\"].notnull())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"startTime = time.time()\nabsLength = []\nword2count = {}\nfor abstract in metaData[\"abstract\"][metaData[\"abstract\"].notnull()]:\n    ## Remove web links\n    abstract = re.sub('https?://\\S+|www\\.\\S+', '', abstract) \n\n    ## Lowercase\n    abstract = abstract.lower()\n    \n    ## Remove punctuation\n    abstract = re.sub('<.*?>+', ' ', abstract)\n    abstract = re.sub('[%s]' % re.escape(string.punctuation), ' ', abstract)\n    \n    ## Tokenize\n    words = word_tokenize(abstract)\n    \n    ## Remove stop words\n    nltk_stop_words = stopwords.words('english')\n    words = [word for word in words if word not in nltk_stop_words]\n    \n    ## Stem\n    stemmer = SnowballStemmer('english')\n    words = [stemmer.stem(word) for word in words]\n    \n    ## Lematize verbs\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n    \n    ## Record length\n    absLength.append(len(words))\n    \n    ## Get word count\n    for word in words:\n        count = word2count.get(word, 0)\n        word2count[word] = count + 1\nprint(\"Time spent: \" + str(round((time.time() - startTime) / 60, 3)) + \"min.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 20 extremely long abstracts. Excluding them from the following histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sorted(absLength)[:-20], bins = 50) # There are 20 extremely long abstracts\nplt.xlabel(\"Abstract token count\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 50 mostly frequent tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_word_count = pd.DataFrame(sorted(word2count.items(), key=lambda x: x[1])[::-1])\nsns.set(rc={'figure.figsize':(12,10)})\nsns.barplot(y = df_word_count[0].values[:50], x = df_word_count[1].values[:50], color='red')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}