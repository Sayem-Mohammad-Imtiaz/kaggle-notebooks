{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Formalizzazione del problema </h1>\n\nQuando c'e' un grande quantitativo di dati da analizzare, risulta indispensabile compiere un'analisi su quelli utilizzabili piuttosto che interessanti. Un modo per svolgere tale selezione e' comprendere quali sono gli aspetti del problema, in questione, importanti e/o necessari per risolverlo. Ovvero discriminare le parti rilevanti all'esperienza da quelle che non lo sono. Per fare cio' esistono due tecniche: Feature Selection e Feature Extraction."},{"metadata":{},"cell_type":"markdown","source":"<h1> Feature Selection </h1>\n\nLa Feature Selection viene praticata di continuo dalla mente umana,  perche' non fa altro che eliminare caratteristiche ridondanti  o con informazione poco utile, riducendo lo spazio di dimensionalita' delle features. Tale pratica e' utile quando si vuole migliorare l'interpretabilita' del modello."},{"metadata":{},"cell_type":"markdown","source":"<h1>Feature Extraction </h1>\n\nLa Feature Extraction svolge una riduzione della dimensionalita' delle features, perdendone pero' in interpretabilita' del modello. Questo perche' le features originali vengono combinate tra loro, per ottenere quelle del modello.\n\nUna tecnica che implementa la Feature Extraction e' la PCA. Di seguito tratto nel dettaglio tale tecnica."},{"metadata":{},"cell_type":"markdown","source":"<h3> PCA </h3>\nUn esempio di Feature Extraction e' la PCA (Principal Component Analysis), tecnica per la semplificazione dei dati utilizzata nel campo della statistica multivariata. Il suo scopo e' quello di ridurre il numero, piu' o meno elevato, di variabili, che descrivono un dataset, a un numero inferiore di variabili latenti, limitando al massimo la perdita d'informazione.\n\nLa PCA, algoritmo non supervisionato, puo' avere una duplice visione:\n1. come algoritmo di riduzione della dimensionalita': svolge una trasformazione lineare delle variabili, proiettando quelle originali in un nuovo sistema cartesiano, in cui la nuova variabile con maggiore varianza viene proiettata sul primo asse, e la nuova variabile seconda per dimensione di varianza, nel secondo (cosi avviene la combinazione delle features) e cosi via; \n2. come strumento di visualizzazione, per il filtraggio del rumore, estrazione e ingegneria delle features, ecc.. \n\nPer studiare l'applicazione della PCA, di seguito:\n- implemento un piccolo esempio, composto da un set di dati bidimensionale di 150 campioni;\n- studio le conseguenze della PCA sul dataset `Breast Cancer Wisconsin (Diagnostic) Data Set.`"},{"metadata":{},"cell_type":"markdown","source":"<h5>Applicazione della PCA su dataset di prova (150 esempi)</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nnum = np.random.RandomState(1) # viene restituito un array 1-D riempito con i valori generati\nX = np.dot(num.rand(2, 2), num.randn(2, 150)).T\n# prodotto scalare tra una matrice 2x2 e una matrice 2x150\n# T fa la trasposta\n\nplt.rcParams['figure.figsize'] = [10, 5] # ridimensiono l'area di stampa del plot\nplt.scatter(X[:, 0], X[:, 1], c = 'purple')\nplt.axis('equal'); # imposto la scala degli assi uguale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come appare dalla figura sopra e' evidente una relazione tra gli assi x e y. Difatti essendo, questo un problema di apprendimento senza supervisione, l'obiettivo e' quello di conoscere la relazione che esiste tra i due assi, e non come farebbe la regressione, prevedere i valori di y da x.\\\nNella PCA, tale relazione viene quantificata individuando un elenco degli assi principali nei dati, e utilizzando tali assi per descrivere il set di dati in analisi.\n\nLa PCA, sul mio dataset d'esempio, la calcolo nel modo che segue:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tale processo permette di apprendere alcune quantita' di dati. Tra le quali i \"componenti\" e la \"varianza spiegata\".\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca)\nprint(\"componenti della pca: \" + str(pca.components_)) # componenti\nprint(\"varianza spiegata della pca: \" + str(pca.explained_variance_)) # varianza spiegata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Queste possono venire trasformate in un vettore, tale che:\n- componenti: e' la direzione del vettore;\n- varianza spiegata: e' la lunghezza del vettore.\n\nE permettono di definire quali sono gli assi principali dei dati (\"componenti\") e quanto e' importante ogni asse principale, in rapporto alla distribuzione dei dati (\"varianza spiegata\").\n\nPer generare suddetto vettore ho implementato il metodo `vector_build`.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def vector_build(v,w,ax):\n    ax = ax or plt.gca() # ottengo gli assi correnti\n    arrowprops = dict(arrowstyle='->', linewidth=2)\n    # s: parametro e' il testo dell'annotazione\n    # xy: parametro e' il punto (x, y) da annotare\n    # xytext: parametro facoltativo. È la posizione (x, y) in cui posizionare il testo\n    # arrowprops: parametro opzionale e contiene il tipo dict. Il suo valore predefinito e' None\n    ax.annotate('', w, v, arrowprops=arrowprops)\n\nplt.scatter(X[:, 0], X[:, 1], alpha=0.4, color=\"purple\")\n# zip accoppia gli elementi, mediante iteratore, in ordine di comparizione\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 2 * np.sqrt(length)\n    vector_build(pca.mean_, pca.mean_ + v, None)\nplt.axis('equal');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'uso della PCA, come ho gia' affermato, comporta la riduzione di dimensionalita'; cioe' l'azzerramento di uno o piu' componenti principali, risultando di una proiezione dimensionale inferiore, preservando la varianza massima dei dati. Tale trasformata appare evidente se svolgo, sul mio esempio di prova, le seguenti chiamate ai metodi propri della PCA, implementati da Scikit-Learn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=1)\npca.fit(X) \t# adatta il modello con X\nX_pca = pca.transform(X)  # applica la riduzione della dimensionalità a X\nprint(\"Dimensione originale: \", X.shape)\nprint(\"Dimensione dopo l'applicazione della pca: \",X_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ecco che da un array a 2 dimensioni mi sono ricondotta a un array in una singola dimensione.\n\nE' interessante da vedere l'effetto che ha tale riduzione sul dataset, per questo eseguo la trasformazione inversa dei dati ridotti (`X_pca_inv`) per tracciarla insieme ai dati originali (`X`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca_inv = pca.inverse_transform(X_pca) # trasforma i dati nel loro spazio originale\nplt.scatter(X[:,0], X[:,1], alpha=0.2, color=\"purple\")\nplt.scatter(X_pca_inv[:,0], X_pca_inv[:,1], alpha=0.8, color=\"purple\")\nplt.axis(\"auto\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In figura sopra, i punti piu' chiari sono i dati originali, invece quelli piu' scuri la loro versione proiettata. Questo mi permette di comprendere il significato di \"riduzione di dimensionalita'\" per la PCA: le informazioni lungo l'asse o gli assi principali meno importanti vengono rimosse, lasciando solo le componenti con varianza superiore. La frazione di varianza tagliata, proporzionale all'area dei punti attorno alla linea di distribuzione, rappresenta quanta informazione viene scartata nella riduzione di dimensionalita'.\n\nIn questo specifico set di dati sono passata da due a un'unica dimensione, cio' significa che ho avuto una riduzione di dimensionalita' del ben 50%; tuttavia rimangono evidenti quali sono le relazioni fondamentali tra i punti. Difatti la PCA, ha come punto di forza, preservare la relazione complessiva dei dati del dataset."},{"metadata":{},"cell_type":"markdown","source":"Applicazione della PCA su `Breast Cancer Wisconsin (Diagnostic) Data Set`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        df = os.path.join(dirname, filename)\n        print(df)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Caricamento del dataframe**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"breast_cancer_df =  pd.read_csv(df, usecols=[i for i in range(0, 32)],encoding='latin-1')  # costruisco il dataframe\nbreast_cancer_df.head(10) # stampo delle prime 10 righe del dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creazione del dataset**\n\nSalvo il dataset su una struttura dati array e rendo il campo `diagnosis`uniforme al resto dei dati: 0 se il il tumore e' benigno (B), altrimenti 1 se e' maligno (M)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = breast_cancer_df.values # data set\nX[X == 'B'] = 0 # benigno\nX[X == 'M'] = 1 # maligno\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Caratteristiche del dataset**"},{"metadata":{},"cell_type":"markdown","source":"Nel dataset in analisi si hanno:\n- numero di features: 32;\n- numero di valori per ogni features: 569."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimensione originale: \" + str(X.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"E il contenuto e' quanto stampato di seguito."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standaridazzazione delle features**\n\nProcedo a standardizzare le features. Per farlo ho deciso di usare il metodo di standardizzazione (`StanderScaler`).\nLa formula e':\n<center> $\\frac{X - \\mu}{\\sigma}$</center>\nove X rappresenta il vettore di dati da standardizzare, $\\mu$ la media e $\\sigma$  la devianza standard.\n\nPer svolgere una standardizzazione conforme, procedo:\n1. a calcolare $\\mu$ e $\\sigma$ sui dati di X;\n2. a standardizzare X sulla base del calcolo svolto al punto (1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\nsc = StandardScaler() # richiamo la classe StandardScaler(), inizializzo un oggetto e lo assegno alla variabile sc\nsc.fit(X) # stimo la media del campione e la deviazione standard di X\nX_std = sc.fit_transform(X) # standardizzazione di X con quanto calolato da fit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applicazione della PCA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nX_std_pca = pca.fit(X_std)\nX_pca = pca.transform(X_std)  # applica la riduzione della dimensionalità a X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con l'applicazione della PCA, come mi aspettavo, i nuovi dati (`X_std_pca`) sono stati ridotti da 32 a 2 features, invece il numero di righe sono rimaste immutate rispetto al dataset di dati originale (`X`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimensione originale: \" + str(X.shape))\nprint(\"Dimensione dopo l'applicazione della pca: \" + str(X_pca.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Di seguito mostro alcuni risultati che ho ottenuto dall'applicazione della tecnica."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"####### PCA #######\")\nprint(pca)\nprint(\"componenti: \")\nprint(pca.components_) # componenti\nprint(\"varianza spiegata: \")\nprint(pca.explained_variance_) # varianza spiegata\nprint(\"dataset ottenuto al termine dell'applicazione della tecnica: \")\nprint(X_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per confrontare la distribuzione, sulle prime due componeti principali del dataset originale con il dataset sottoposto a sandardizzazione e PCA, ho proceduto con la generazione di plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.scatter(X[:, 0], X[:, 1], alpha=0.5, color = \"purple\")\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, color = \"orange\")\nplt.legend(['valori originali','valori post-pca'], numpoints=1)\nplt.axis('tight'); # imposto limiti abbastanza grandi da mostrare tutti i dati\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Faccio seguire, per maggiore chiarezza della distribuzione, anche i due grafici distinti."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.scatter(X[:, 0], X[:, 1], alpha=0.5, color = \"purple\")\nplt.axis('tight'); # imposto limiti abbastanza grandi da mostrare tutti i dati\nplt.legend(['valori originali'], numpoints=1)\nplt.show()\n\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, color = \"orange\")\nplt.axis('tight'); # imposto limiti abbastanza grandi da mostrare tutti i dati\nplt.legend(['valori post-pca'], numpoints=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dai grafici sopra e' evidente la riduzione di dimensionalita' a seguito della PCA. \nDifatti come si puo' vedere dai punti in giallo, l'informazione viene riassunta nei primi due assi evitando situazione come:\n- pedita d'informazione;\n- un'eccessiva distribuzione dei dati in uno spazio troppio ampio e percio' complesso da scandagliare. Come si puo' vedere, infatti i punti in viola non hanno una distribuzione omogenea gia' nelle prime due features, dunque per 30 implica un lavoro con una complessita' sia in tempo che in spazio considerevole;\n-  la mancanza di somiglianza tra le features o un'eccessiva dispersione dei valori, puo' generare situazioni di cattiva interpretazione dei dati. Quest'ultimo aspetto e' particolarmente rilevante quando si devono svolgere stime, su dati, che non fanno uso di metodi di apprendimento, ma di calcoli di frequenza, e che percio' tendono a sottovalutare il valore di punti isolati."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}