{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Data Manipulation**\n\nThe code hidden below walks through my data manipulations. An overview of the steps follow.\n\n1. Load data and packages\n2. Create a stocks df\n    \n    a. Import each company data\n    \n    b. Create a return column equal to the adjusted close divded by previous adjusted close, and subtract one\n    \n    c. Create lagged columns of the data (for future use)\n    \n    d. Create step forward columns (for future use)\n    \n    e. Correct the date column formatting\n    \n    f. Save a list of trading days\n\n3. Create a tweets df\n    \n    a. Remove tweets that seem to be written by bots\n    \n    b. Change post_date to date time\n    \n    c. Join company names in\n    \n    d. Make time zone correction to tweet times (i.e. UTC to EST)\n    \n    e. Adjust post date to period which it applies (a tweet made after trading hours is moved up to the next trading day)\n    \n    f. Clean up columns names\n    \n    g. Make final adjustment to tweet date (if not in trading days move to the next closest trading day)\n    \n4. Create final df\n\n    a. Join tweet and stock price dfs\n    \n    b. Use VADER to create composite, negative, neutral, and positive scores per tweet\n    \n    c. Save final df to feather format","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from datetime import *\n# import pandas_market_calendars as mcal\n# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n# import feather\n\n\n# # In[1]:\n\n# stocks = pd.DataFrame(columns = ['Company', 'Date',\n#                                   'lag1return',\n#                                   'lag2return',\n#                                   'lag3return',\n#                                   'lag4return',\n#                                   'lag5return',\n#                                   'lag6return',\n#                                   'lag7return',\n#                                   'step1return',\n#                                   'step2return',\n#                                   'step3return',\n#                                   'step4return',\n#                                   'step5return',\n#                                   'step6return',\n#                                   'step7return'\n#                                   ])\n\n# companies = ['aapl', 'amzn', 'goog', 'googl', 'msft', 'tsla']\n\n# for co in companies:\n    \n#     csv = co.upper() + '.csv'\n#     data = pd.read_csv(csv)\n    \n#     data['Company'] = co\n#     data = data.drop(['High', 'Low', 'Open', 'Close', 'Volume'], axis = 1)\n#     data['return'] = data['Adj Close'].div(data['Adj Close'].shift(1)) - 1\n    \n#     for i in range(1, 8):\n        \n#         lag = str(i)\n        \n#         lr = 'lag' + lag + 'return'\n                \n#         temp = data.shift(periods = i, axis = 0, fill_value = 0)\n\n#         data[lr] = temp['return']\n    \n#     for i in range(1, 8):\n        \n#         step = str(i)\n        \n#         sr = 'step' + step + 'return'\n        \n#         temp = data.shift(periods = -i, axis = 0, fill_value = 0)\n        \n#         data[sr] = temp['return']\n    \n#     stocks = stocks.append(data)\n\n# stocks = stocks[['Company', 'Date', 'return', 'lag1return',\n#         'lag2return', 'lag3return', 'lag4return', 'lag5return', 'lag6return',\n#         'lag7return', 'step1return', 'step2return', 'step3return', 'step4return',\n#         'step5return', 'step6return', 'step7return']]\n\n# stocks['Date'] = stocks['Date'].astype('str')\n# stocks['Company'] = stocks['Company'].astype('str')\n\n# tradingdays = stocks['Date'].unique()\n# tradingdays = tradingdays.tolist()\n\n# # In[2]:\n\n# tweets = pd.read_csv('tweet.csv')\n\n# #thanks to Alex Kozlov/@aramacus for identifying bots\n# #https://www.kaggle.com/aramacus/bot-hunting-or-how-many-tweets-were-made-by-bots\n\n# bots = ['CBuck81TV', 'Chardenee18', 'EngkisK', 'FAJSYSTEMS', 'IncomeREIT',\n#         'Javan_Andrew', 'JoshuaCEwell', 'KingRichard128', 'MarketParse',\n#         'RealTraderGeek', 'RobinhoodPromo', 'Samantha_Lamb_', 'StockSaintsPrem',\n#         'StrategyAcad', 'bcarpano', 'byCyril', 'impeachTHEpotus', 'mehranhydary',\n#         'myforexchart', 'rival96618425', 'snowbirdie_xo', 'tweetchristo', 'uchinatravel',\n#         'williaj51', 'yinyang21198  ']\n\n# tweets = tweets[~tweets.writer.isin(bots)]\n\n# tweets['post_date'] = pd.to_datetime(tweets['post_date'], unit = 's')\n\n# co_tweet = pd.read_csv('company_tweet.csv')\n\n# tweets = pd.merge(tweets, co_tweet, on = 'tweet_id')\n\n# del co_tweet\n\n# tweets['post_dateUTC'] = tweets['post_date'].dt.tz_localize('UTC')\n\n# tweets['post_dateEST'] = tweets['post_dateUTC'].dt.tz_convert('US/Eastern')\n\n# tweets['post_datedelta'] = [int(str(x)[-4]) for x in tweets['post_dateEST']]\n\n# tweets.loc[:, 'post_date'] = pd.to_datetime(tweets['post_date']) -  pd.to_timedelta(tweets['post_datedelta'], unit='h')\n\n# tweets = tweets.drop(['tweet_id', 'post_dateUTC', 'post_dateEST', 'post_datedelta', 'tweet_id']   , axis = 1)\n\n# tweets['post_date'] = np.where(tweets['post_date'].dt.hour >= 16, tweets['post_date'] + pd.Timedelta(1, unit = 'day'), tweets['post_date'])\n\n# tweets['post_date'] = tweets['post_date'].dt.date\n\n# tweets = tweets.rename(columns = {'writer': 'Writer',\n#                         'post_date': 'Date',\n#                         'body': 'Tweet',\n#                         'comment_num': 'Comments',\n#                         'retweet_num': 'Retweet',\n#                         'like_num': 'Likes',\n#                         'ticker_symbol': 'Company'})\n\n# tweets['Company'] = tweets['Company'].str.lower()\n# tweets['Date'] = tweets['Date'].astype('str')\n# tweets['Company'] = tweets['Company'].astype('str')\n\n# def dayshift(date):\n    \n#     attempts = 0\n    \n#     while date not in tradingdays and attempts < 7:\n#         date = datetime.fromisoformat(date) + timedelta(days = 1)\n#         date = str(date.date())\n#         attempts += 1\n    \n#     return date\n\n# tweets['Date'] = tweets['Date'].apply(lambda x: dayshift(x))\n\n# In[3]:\n\n# df = pd.merge(tweets, stocks, how = 'left', on = ['Date', 'Company'])\n\n# analyzer = SentimentIntensityAnalyzer()\n\n# df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['Tweet']]\n# df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['Tweet']]\n# df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['Tweet']]\n# df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['Tweet']]\n\n\n# In[]:\n# feather.write_dataframe(df, 'masterdf.feather')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-29T00:32:16.206731Z","iopub.execute_input":"2021-06-29T00:32:16.20713Z","iopub.status.idle":"2021-06-29T00:32:16.213282Z","shell.execute_reply.started":"2021-06-29T00:32:16.207095Z","shell.execute_reply":"2021-06-29T00:32:16.212497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sentiment Indexing and Analysis**\n\nThis part aggregates the tweet sentiments by date and company and analyzes using simple linear correlation.\n\nThree caveats:\n\n* It is likely linear correlation is not the optimal correlation analysis tool here. It is only a starting point. A better analysis would analyze normal and tail correlations, as a linear assumption is likely too simplistic.\n\n* This analysis should probably be expanded to a time series approach. Autocorrelations in the tweets and stocks are likely present and not accounted for here.\n\n* The engagement index assumes an equal weight to likes, retweets, and comments. This is probably overly simplistic, as the importance of these features may not be equal.\n\nWith that out of the way, lets look at what we have.","metadata":{}},{"cell_type":"code","source":"#import packages\nimport pandas as pd\nfrom matplotlib import rcParams, pyplot as plt\nimport seaborn as sns\nimport feather\nfrom scipy. stats import pearsonr\nimport math\n\n#set pic resolution\ndpi = 96\nplt.rcParams['figure.figsize'] = (1280/dpi, 720/dpi)\n\n#read in data\ndf = pd.read_feather('../input/masterdf/masterdf.feather')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:32:16.214506Z","iopub.execute_input":"2021-06-29T00:32:16.214943Z","iopub.status.idle":"2021-06-29T00:32:20.36299Z","shell.execute_reply.started":"2021-06-29T00:32:16.214907Z","shell.execute_reply":"2021-06-29T00:32:20.361938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we will take a basic look at the composite sentiment distribution for all tweets.\n\nIt appears most score 0, or neutral, and the distribution has extreme behavior in the tails.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data = df.sample(frac = .2), x = 'compound', bins = 25)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:32:20.36463Z","iopub.execute_input":"2021-06-29T00:32:20.364923Z","iopub.status.idle":"2021-06-29T00:32:21.971454Z","shell.execute_reply.started":"2021-06-29T00:32:20.364896Z","shell.execute_reply":"2021-06-29T00:32:21.970528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our next plot is the sotck returns. Similar to the sentiment, it is cenetered around 0, and has extreme behavior in the tails.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data = df.sample(frac = .2), x = 'return', bins = 25)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:32:21.973043Z","iopub.execute_input":"2021-06-29T00:32:21.973395Z","iopub.status.idle":"2021-06-29T00:32:23.423762Z","shell.execute_reply.started":"2021-06-29T00:32:21.973361Z","shell.execute_reply":"2021-06-29T00:32:23.421736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This next section will aggregate the tweet and engagements.\n\nAn engagement index is made by fidning the weighted average sentiment by using a tweets total engagement (i.e. retweet + like + comment).\n\nThe tweet sentiment is just the unweighted average of tweets by company and date.\n\nSummary stats of total engagement before aggregation follows. We can tell most tweets recieve very little or no engagement.","metadata":{}},{"cell_type":"code","source":"df['Engagement'] = df['Likes'] + df['Retweet'] + df['Comments']\n\nprint(df['Engagement'].describe())","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:38:34.489944Z","iopub.execute_input":"2021-06-29T00:38:34.490702Z","iopub.status.idle":"2021-06-29T00:38:34.626131Z","shell.execute_reply.started":"2021-06-29T00:38:34.490645Z","shell.execute_reply":"2021-06-29T00:38:34.624876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we build the sentiment index df.","metadata":{}},{"cell_type":"code","source":"#create weighted sentiment\ndf['WeightSent'] = df['Engagement'] * df['compound']\n\n#aggregate by date/company\nSentIndex = df.groupby(['Date', 'Company']).agg({'compound': 'mean', \n                                      'WeightSent': 'sum',\n                                      'Engagement': 'sum'})\n#reset index\nSentIndex = SentIndex.reset_index()\n\n#create weighted average index\nSentIndex['EngIndex'] = SentIndex['WeightSent']/SentIndex['Engagement']\n\n#get unique set of trading days and bind the aggregated sentiment to returns for each date/company\nReturns = df[['Date', 'Company', 'return', 'lag1return', 'lag2return', 'lag3return', 'lag4return',\n       'lag5return', 'lag6return', 'lag7return', 'step1return', 'step2return','step3return', \n       'step4return', 'step5return', 'step6return', 'step7return']]\n\nReturns = Returns.drop_duplicates()\n\nSentIndex = pd.merge(SentIndex, Returns, how = 'left')\n\nSentIndex = SentIndex.drop(['WeightSent', 'Engagement'], axis = 1)\n\nSentIndex.rename(columns={'compound':'AvgTweetSent'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:34:59.478763Z","iopub.execute_input":"2021-06-29T00:34:59.479046Z","iopub.status.idle":"2021-06-29T00:35:02.34162Z","shell.execute_reply.started":"2021-06-29T00:34:59.479018Z","shell.execute_reply":"2021-06-29T00:35:02.340564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have aggregated data lets look at the correlation heatmap.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data = SentIndex[['AvgTweetSent', 'EngIndex', 'return', 'lag1return',\n       'lag2return', 'lag3return', 'lag4return', 'lag5return', 'lag6return',\n       'lag7return', 'step1return', 'step2return', 'step3return',\n       'step4return', 'step5return', 'step6return', 'step7return']].corr(), cmap = 'Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:35:02.342972Z","iopub.execute_input":"2021-06-29T00:35:02.343274Z","iopub.status.idle":"2021-06-29T00:35:02.784021Z","shell.execute_reply.started":"2021-06-29T00:35:02.343245Z","shell.execute_reply":"2021-06-29T00:35:02.782983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot makes it clear tweet sentiment and engagement are correlated.\n\nIt also suggeests that maybe the returns are correlated as well.\n\nWe will look at the two days before and after a tweet more closely.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data = SentIndex[['AvgTweetSent', 'EngIndex', 'return', 'lag1return',\n       'lag2return', 'step1return', 'step2return']].corr(), cmap = 'Blues', annot = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:35:02.786753Z","iopub.execute_input":"2021-06-29T00:35:02.787182Z","iopub.status.idle":"2021-06-29T00:35:03.301676Z","shell.execute_reply.started":"2021-06-29T00:35:02.787137Z","shell.execute_reply":"2021-06-29T00:35:03.300857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us a little closer look. We can see that the return for the proceeding trading session has a positive correlation with the tweet sentiment, and less so as you move ahead through time.\n\nWhat is more surprising is that the lag returns arent as strongly correlated. One would assume that if the stock is performing good/bad we would see that crop up in the tweets, possibly more so than in the next trading session since the people tweeting know what has transpired.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data = SentIndex.sample(frac = .1), x = 'AvgTweetSent', y = 'return', fill = True)\n\nprint(pearsonr(SentIndex['AvgTweetSent'], SentIndex['return']))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:35:03.30294Z","iopub.execute_input":"2021-06-29T00:35:03.303258Z","iopub.status.idle":"2021-06-29T00:35:04.180216Z","shell.execute_reply.started":"2021-06-29T00:35:03.303222Z","shell.execute_reply":"2021-06-29T00:35:04.179302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above kernel density plot indicates there is likely a relationship between the average tweet sentiment and the return that day.\n\nOur pearson r test also seems to suggest there is a statistically significant correlation between return and average tweet sentiment.\n\n","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data = SentIndex.sample(frac = .1), x = 'EngIndex', y = 'return', fill = True)\n\nprint(pearsonr(SentIndex['EngIndex'], SentIndex['return']))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T00:35:04.181805Z","iopub.execute_input":"2021-06-29T00:35:04.182219Z","iopub.status.idle":"2021-06-29T00:35:05.035181Z","shell.execute_reply.started":"2021-06-29T00:35:04.182176Z","shell.execute_reply":"2021-06-29T00:35:05.034174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This kernel density plot also suggests a relationship between the engagement index and the return for that day.\n\nThe correlation also seems to be significant as well.","metadata":{}}]}