{"cells":[{"metadata":{"_uuid":"33794feb-7e19-4187-a540-407eafd38533","_cell_guid":"e531613c-72e3-41e0-b56e-5210598674cc","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport json\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport string\nimport spacy\nimport math\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d29dd4a1-22eb-47c0-8be2-f9b04bb98730","_cell_guid":"d9462bf1-a4c3-4687-96e0-9da01f50b688","trusted":true},"cell_type":"code","source":"# Load the metatadata about research papers in dataset directory\n\n\ndebug = False\narticles = {}\nstat = { }\nfor dirpath, subdirs, files in os.walk('/kaggle/input'):\n    for x in files:\n        if x.endswith(\".json\"):\n            articles[x] = os.path.join(dirpath, x)        \ndf = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\n\n# A brief dataset first to test things\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"498dbc5e-5391-4d3d-9355-216ac605a418","_cell_guid":"81e04d33-ddce-4b8d-a6a6-65bc525ffdfa","trusted":true},"cell_type":"code","source":"# Only get papers related to coronavirus\n\nVIRUS_REF = ['covid-19', 'covid', 'coronavirus', 'cov-2', 'sars-cov-2', 'sars-cov', 'hcov', '2019-ncov']\ndef virus_match(text):\n    return len(re.findall(rf'({\"|\".join(VIRUS_REF)})', text, flags=re.IGNORECASE)) > 0\n\ndef list_to_string(data, attribute): \n    text = ' '.join([elem['text'] for elem in data[attribute]])\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682d8d7a-66f0-4655-83e5-0ec415f73624","_cell_guid":"6ee5e9f2-d19a-4399-b424-3554cc2ac1ef","trusted":true},"cell_type":"code","source":"# Parse all the papers json files and extract abstract and body_text\nliterature = []\nfor index, row in tqdm(df.iterrows(), total=df.shape[0]):\n    sha = str(row['sha'])\n    if sha != 'nan':\n        sha = sha + '.json';\n        try:\n            with open(articles[sha]) as f:\n                data = json.load(f)\n                paper = {'paper_id': data['paper_id'], 'title': data['metadata']['title']}\n                body_text = list_to_string(data, 'body_text')\n                abstract = list_to_string(data, 'abstract')\n                if virus_match(abstract) | virus_match(body_text):\n                    paper['body_text'] = body_text\n                    paper['abstract'] = abstract\n                    literature.append(paper)\n    \n        except KeyError:\n            pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a52694f-2bdf-47b7-9bb7-3ec05125b22d","_cell_guid":"1cda7f38-f7f4-4969-916a-19942a9497f2","trusted":true},"cell_type":"code","source":"literature_df = pd.DataFrame(literature)\nprint(len(literature_df))\nliterature_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31fee18c-ed27-4210-ae8b-49144e6e5449","_cell_guid":"cc49e48c-19ca-4f7c-be7d-90c9fc6badf7","trusted":true},"cell_type":"markdown","source":"## Group and query papers by similarity"},{"metadata":{"_uuid":"7b8ab460-67bf-431d-b75d-5cb34c4a676c","_cell_guid":"062a4878-3382-44e1-8946-ddcb13f15c8f","trusted":true},"cell_type":"markdown","source":"### Comparing articles similarity"},{"metadata":{"_uuid":"ed78fac5-dad7-47ba-a317-73497826dff5","_cell_guid":"b25526bd-260b-49fb-8bf1-83efec2898dc","trusted":true},"cell_type":"code","source":"# count vectorizer convert a collection of documents into a matrix of word frecuencies\n# max_df param is set the max treshold to ignore words wich frequency pass this value.\n# stop_worlds to remove stop_words\n# ngrams_range set the tuple of ngrams, in our case we want tuples (word1, word2)\n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nanalyzer = CountVectorizer().build_analyzer()\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(doc):\n    doc=doc.lower()\n    return str.join(\" \", [stemmer.stem(w) for w in analyzer(doc)])\n\n%time literature_df['stemmized'] = literature_df['body_text'].apply(lambda doc: stemming(doc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom scipy.spatial import distance\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\nimport ipywidgets as widgets\n\nfrom IPython.display import Image\nfrom IPython.display import display, HTML","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b97be2-8474-4442-bab4-7666a69586b0","_cell_guid":"3286d153-96d4-4186-b199-2fd87a0dc3eb","trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_df=0.95, min_df=0.01, stop_words='english')\n%time word_count = cv.fit_transform(literature_df.body_text)\ntfidf_tr = TfidfTransformer(smooth_idf=True, use_idf=True)\n%time tfidf_tr.fit_transform(word_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ccb6d2-0cdf-4068-b803-c7f9d67ca570","_cell_guid":"f375cd0c-a328-4332-aee7-6869f20e9816","trusted":true},"cell_type":"code","source":"\ndef get_word_vector(document):\n    word_vector = tfidf_tr.transform(cv.transform([document]))\n    return word_vector \n\n%time literature_df['word_vector'] = literature_df.body_text.apply(get_word_vector)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08f8fd72-5a6f-46a1-bd1b-2b6cb02b151e","_cell_guid":"ed6efca2-f035-4002-a3fc-bda466c54e7b","trusted":true},"cell_type":"code","source":"\ndef show_word_cloud(word_vector):\n    cloud = WordCloud(background_color='white',\n        width=500,\n        height=500,\n        max_words=20,\n        colormap='tab10',\n        prefer_horizontal=1.0)\n    word_frequency = dict(get_words_with_value(word_vector))\n    cloud.generate_from_frequencies(word_frequency)\n    plt.gca().imshow(cloud)\n    plt.gca().axis('off')\n\nfeature_names = cv.get_feature_names()\ndef get_words_with_value(word_vector):\n    return sorted([(feature_names[ind], val) for ind, val in zip(word_vector.indices, word_vector.data)], key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f20ecf-a735-4825-b852-f56ac15723b5","_cell_guid":"9b4411e4-ed46-40c5-bcc8-d37849cd11c1","trusted":true},"cell_type":"code","source":"\ndef calculate_distance_between_words_vectors(query_indices, search_vec, document_vector):\n    document_vec = document_vector[0, query_indices].toarray()\n    return distance.euclidean(search_vec, document_vec)\n\ndef calculate_related_documents(query, max_documents_comparison):    \n    query_vector = get_word_vector(query)\n    query_indices = query_vector.indices\n    query_vector_array = query_vector[0, query_indices].toarray()\n    distance_idx = literature_df.apply(lambda x: calculate_distance_between_words_vectors(query_indices, query_vector_array, x.word_vector), axis=1)\n    \n    relevant_indexes = distance_idx.sort_values().head(max_documents_comparison).index \n    result_columns = [\"paper_id\", \"word_vector\"]\n    \n    result = literature_df[result_columns].iloc[relevant_indexes].fillna(\"\")\n    return result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace08c24-36b9-40db-b678-6642268b0782","_cell_guid":"baa22e48-1f77-4923-a98f-5194db76c92c","trusted":true},"cell_type":"code","source":"\ndef display_word_frecuencies_distances(df_result):\n    display_columns = [\"paper_id\", \"word_vector\"]\n    display(df_result[display_columns].reset_index(drop=True))\n    rows = math.ceil(len(df_result)/3)\n    plt.rcParams[\"figure.figsize\"] = (20,15)\n    for i in range(len(df_result)):\n        row = df_result.iloc[i]\n        plt.subplot(rows, 3, i+1)\n        show_word_cloud(row.word_vector)\n        paper_id = f'{row.paper_id[:5]}...{row.paper_id[-5:]}'\n\n        plt.title(f'Paper {paper_id}', fontsize=10)\n    plt.show()\n\ndef compare_distances_and_show(questionary):\n    for question in questionary:\n        display(HTML(f\"<h3>{question}<h3>\"))\n        topic_result = calculate_related_documents(question, 6)\n        display_word_frecuencies_distances(topic_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9a8dfad-33a2-4696-ace2-79e277963c84","_cell_guid":"2b19bf3d-5acc-4251-b583-fefe58e96e7f","trusted":true},"cell_type":"code","source":"task1 = [\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery?\", \n               \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children)\", \n               \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\", \n               \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\"]\n\ncompare_distances_and_show(task1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task2 = [\"Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\",\n\"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\",\n    \"livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\",\n\" whether farmers are infected, and whether farmers could have played a role in the origin.\" ]\n\ncompare_distances_and_show(task2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5cb6057-c776-4c09-aaaa-21e6fa1a542c","_cell_guid":"18f3c109-deac-42d4-848d-22dfec1ab44f","trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nkmeans = MiniBatchKMeans(n_clusters=10,\n                          random_state=0,\n                          batch_size=100,\n                          max_iter=50)\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.4)\n%time transform = tfidf_vectorizer.fit_transform(literature_df['body_text'])\n%time kmeans.fit(transform)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c9eef42-ad37-4ab7-9286-9b96bf9ede4a","_cell_guid":"80635f0a-087a-4b6d-8a4d-41c32dd3b3c9","trusted":true},"cell_type":"code","source":"\nprint(\"Centroid of clusters: \\n\")\norder_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = tfidf_vectorizer.get_feature_names()\nfor i in range(10):\n    top_ten_words = [terms[ind] for ind in order_centroids[i, :30]]\n    print(\"Cluster {}: {} \\n\".format(i, ' '.join(top_ten_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task2 = [\"Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\",\n\"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\",\n    \"livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\",\n\" whether farmers are infected, and whether farmers could have played a role in the origin.\" ]\n\npredictions = [kmeans.predict(tfidf_vectorizer.transform([task])) for task in task2]\nfor i in range(0, len(task2)):\n    print(f'Query: {task2[i]}')\n    print(f'Prediction: {predictions[i]}')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}