{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\n\nimport re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:58:19.599061Z","iopub.execute_input":"2021-08-25T21:58:19.599407Z","iopub.status.idle":"2021-08-25T21:58:19.604752Z","shell.execute_reply.started":"2021-08-25T21:58:19.599378Z","shell.execute_reply":"2021-08-25T21:58:19.60377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define personality types\n\npersonalities = [\n    'ENFJ',\n    'ENFP',\n    'ENTJ',\n    'ENTP',\n    'ESFJ',\n    'ESFP',\n    'ESTJ',\n    'ESTP',\n    'INFJ',\n    'INFP',\n    'INTJ',\n    'INTP',\n    'ISFJ',\n    'ISFP',\n    'ISTJ',\n    'ISTP',\n    ]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:52:42.669745Z","iopub.execute_input":"2021-08-25T21:52:42.670124Z","iopub.status.idle":"2021-08-25T21:52:42.674414Z","shell.execute_reply.started":"2021-08-25T21:52:42.670084Z","shell.execute_reply":"2021-08-25T21:52:42.673767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\n\n# Number of words we want to consider in the study\nn_common_words = 50\n\n# Word Lemmatizer\nwnl = WordNetLemmatizer()\n\n# English Stopwords\nengstopwords = stopwords.words('english')\n\n# Add stopwords that doesn't contain punctuations such 'wasnt, hasnt, iam'\nengstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n                        ' '.join(engstopwords)).split()\n\n# Additional stopwords : non-effective words & some words that are common for all personality types\nwith open(\"../input/mbtiadditionalwords/additional-stopwords.json\") as file:\n    additional_stopwords = json.load(file)\n\n# Add additional stopwords\nengstopwords.extend(additional_stopwords)\n\n\n# Combine all stopwords\nengstopwords = set(engstopwords).union(set(engstopwordsV2))\n\n# Punctuations\nstr_punc = string.punctuation\n\n# Dataset filename\nfilename = '../input/mbti-type/mbti_1.csv'\n\n# Initiate count vectorizer and apply it to the text\ncv = CountVectorizer(stop_words='english')\n\n# Dots Per Inch setting\nplt.rcParams['figure.dpi'] = 300\n\n# Word cloud with options\nwc = WordCloud(stopwords=engstopwords, background_color='white',\n               colormap='Dark2', max_font_size=150, random_state=42,width=800, height=400)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:58:47.181075Z","iopub.execute_input":"2021-08-25T21:58:47.181429Z","iopub.status.idle":"2021-08-25T21:58:47.211836Z","shell.execute_reply.started":"2021-08-25T21:58:47.181393Z","shell.execute_reply":"2021-08-25T21:58:47.211062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data set\n\ndataset = pd.read_csv(filename)\n\n# Group texts by personality type to combine all\n# text samples of a single personality type in one single row\ndataset = dataset.groupby(['type'])['posts'].apply(lambda x: \\\n        '. '.join(x)).reset_index()\n\ndataset.head(16)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:58:50.500202Z","iopub.execute_input":"2021-08-25T21:58:50.500649Z","iopub.status.idle":"2021-08-25T21:58:51.966261Z","shell.execute_reply.started":"2021-08-25T21:58:50.500619Z","shell.execute_reply":"2021-08-25T21:58:51.965491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to lemmatize a word using the three types: adjective, verb, noun\ndef lemmatize_all_types(word):\n    word = wnl.lemmatize(word, 'a')\n    word = wnl.lemmatize(word, 'v')\n    word = wnl.lemmatize(word, 'n')\n    return word\n\n# Function to clean text\ndef clean(text):\n    # Remove URLs from text\n    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n    # Remove specific punctuation (usually associated with a word)\n    text = re.sub(r\"(:|;).\", \"\", text)\n    # Remove punctuations\n    text = re.sub('['+re.escape(str_punc)+']',\"\",  text)\n    # Remove parantheses, brackets\n    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', '', text)\n    # Remove string marks\n    text = re.sub('[’‘“\\.”…–]', '', text)\n    # Check that each word is not stopword, and lemmatize it\n    text = list(map(lemmatize_all_types, text.split()))\n    text = [word for word in text if (word not in engstopwords)]\n    text = \" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:59:01.288691Z","iopub.execute_input":"2021-08-25T21:59:01.289028Z","iopub.status.idle":"2021-08-25T21:59:01.297545Z","shell.execute_reply.started":"2021-08-25T21:59:01.288983Z","shell.execute_reply":"2021-08-25T21:59:01.296723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply text cleaning\ndataset.posts = pd.Series(dataset.posts.apply(clean))\n\n# Set personality types as the index\ndataset.index = dataset[\"type\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:59:05.2482Z","iopub.execute_input":"2021-08-25T21:59:05.248664Z","iopub.status.idle":"2021-08-25T22:00:51.285911Z","shell.execute_reply.started":"2021-08-25T21:59:05.248634Z","shell.execute_reply":"2021-08-25T22:00:51.285197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Count Vectorizer\n\nposts_vectorized = cv.fit_transform(dataset.posts)\n\n# convert Vectorized text DTM (Document Term Matrix)\ndtm = pd.DataFrame(data=posts_vectorized.toarray(), columns=cv.get_feature_names())\n\n# Set its index to personality types\ndtm.index = dataset['type']","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:06:17.339797Z","iopub.execute_input":"2021-08-25T22:06:17.340391Z","iopub.status.idle":"2021-08-25T22:06:22.498622Z","shell.execute_reply.started":"2021-08-25T22:06:17.340357Z","shell.execute_reply":"2021-08-25T22:06:22.497826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize Document Term Matrix\ndtm.iloc[:, -20000:-20016:-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:59:34.846653Z","iopub.execute_input":"2021-08-22T15:59:34.847311Z","iopub.status.idle":"2021-08-22T15:59:34.868695Z","shell.execute_reply.started":"2021-08-22T15:59:34.847274Z","shell.execute_reply":"2021-08-22T15:59:34.86744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transpose the Document Term Matrix, so we can sort the rows (words)\ndtmT = dtm.transpose()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:06:22.499817Z","iopub.execute_input":"2021-08-25T22:06:22.50021Z","iopub.status.idle":"2021-08-25T22:06:22.55042Z","shell.execute_reply.started":"2021-08-25T22:06:22.500183Z","shell.execute_reply":"2021-08-25T22:06:22.549662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize DTM Transposed\ndtmT.iloc[-20000:-20016:-1, :]","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:59:41.799349Z","iopub.execute_input":"2021-08-22T15:59:41.799758Z","iopub.status.idle":"2021-08-22T15:59:41.823208Z","shell.execute_reply.started":"2021-08-22T15:59:41.799723Z","shell.execute_reply":"2021-08-22T15:59:41.822302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Top 50 words\n\ncloud = []\n# cloud will store 16 dict, each one has a key (personality type)\n# and value (list of most used words by this personality type)\n\n# Loop over columns (words), sort them, select top 50, append them with a key (personality type)\nfor personality in dtmT.columns:\n    topWords = list(dtmT[personality].sort_values(ascending=False).head(n_common_words).index)\n    cloud.append({personality: topWords})","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:06:24.974557Z","iopub.execute_input":"2021-08-25T22:06:24.975048Z","iopub.status.idle":"2021-08-25T22:06:25.488263Z","shell.execute_reply.started":"2021-08-25T22:06:24.975015Z","shell.execute_reply":"2021-08-25T22:06:25.487414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize a single cloud\ncloud[11]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T21:55:12.564562Z","iopub.execute_input":"2021-08-25T21:55:12.564879Z","iopub.status.idle":"2021-08-25T21:55:12.570354Z","shell.execute_reply.started":"2021-08-25T21:55:12.564851Z","shell.execute_reply":"2021-08-25T21:55:12.569664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate WordCloud for each personality type\nfor (index, personality) in enumerate(dtmT.columns):\n    wc.generate(dataset.posts[personality])\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(personality)\n    plt.savefig(personality + '.png')","metadata":{"execution":{"iopub.status.busy":"2021-08-25T22:06:27.942959Z","iopub.execute_input":"2021-08-25T22:06:27.943341Z","iopub.status.idle":"2021-08-25T22:07:41.764095Z","shell.execute_reply.started":"2021-08-25T22:06:27.94331Z","shell.execute_reply":"2021-08-25T22:07:41.763135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find common.common words (same common words for same personality types)\n\nn_of_types = 15\nshared_words = []\n# If a word is shared between (n_of_types) or more personality types, add it to shared_words\n\n\n# Collect all words\nall_words = []\nfor index, personality in enumerate(dtmT.columns):\n     all_words.append(cloud[index][personality])\n\n        \n# Get unique words\nunique_words = np.unique(np.array(all_words))\n\n# loop over each common word, check if it appears for (n_of_types) or more\nfor word in unique_words:\n     counter = 0\n     for index, personality in enumerate(dtmT.columns):\n         if word in cloud[index][personality]:\n             counter += 1\n     if counter >= n_of_types:\n         shared_words.append(word)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T16:03:09.978806Z","iopub.execute_input":"2021-08-22T16:03:09.979354Z","iopub.status.idle":"2021-08-22T16:03:09.990657Z","shell.execute_reply.started":"2021-08-22T16:03:09.979317Z","shell.execute_reply":"2021-08-22T16:03:09.989454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shared words can be used to filter the dataset to get\n# clearer results (boundaries between types)\n# You can add any of these words to file 'additional-stopwords.json'\nshared_words","metadata":{"execution":{"iopub.status.busy":"2021-08-22T16:03:18.900203Z","iopub.execute_input":"2021-08-22T16:03:18.900594Z","iopub.status.idle":"2021-08-22T16:03:18.908314Z","shell.execute_reply.started":"2021-08-22T16:03:18.900561Z","shell.execute_reply":"2021-08-22T16:03:18.906989Z"},"trusted":true},"execution_count":null,"outputs":[]}]}