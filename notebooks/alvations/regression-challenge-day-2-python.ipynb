{"cells":[{"source":"This is the second day of the 5-Day Regression Challenge. You can find the first day's challenge [here](https://www.kaggle.com/rtatman/regression-challenge-day-1). Today, we’re going to learn how to fit a model to data and how to make sure we haven’t violated any of the underlying assumptions. First, though, you need a tiny bit of background:\n____\n\n**Regression formulas in R**\n\nIn R, regression is expressed using a specific type of object called a formula. This means that the syntax for expressing a regression relationship is the same across packages that use formula objects. The general syntax for a formula looks like this:\n\n    Output ~ input\n\nIf you think that more than one input might be affecting your output (for example that both the amount of time spent exercising and the number of calories consumed might affect changes in someone’s weight) you can represent that with this notation:\n\n\tOutput ~ input1 + input2\n    \nWe'll talk about how to know which inputs you should include later on: for now, let's just stick to picking inputs based on questions that are interesting to you. (Figuring out how to turn a quesiton into a query)","cell_type":"markdown","metadata":{"_cell_guid":"282e4969-fcc1-42fb-98a4-790310b3d6d3","_uuid":"234dc52bb9549e83e0d15134932d98a94668234a"}},{"source":"**Regression in Python**\n\nIn Python, there is no notion of using the native types/operators to \"regress\". Instead we would have to structure our input into a nested array (usually using `numpy`) and use machine learning libraries such as `sklearn` to perform regression.\n\n\nFor example, the input is a list of lists where each element in the outer list represents a data point and the inner list represents each data point's feature(s)/signal(s) that affects the output. E.g. a single signal/feature input we could do this:\n\n```python\nfrom sklearn import linear_model\nX = inputs = [[0], [1], [2]]  # Usually we use X to denote the inputs.\nY = outputs = [0, 1, 2]       # Usually we use Y to denote the outputs.\nregressor = linear_model.LinearRegression()\nregressor.fit(X, Y)\n```\n\nWe can visualize the above inputs/outputs as a table as such:\n\n\n|Feature 1 |**Output**|\n|:---:|:---:|\n| 0 | **0** |\n| 1 | **1** |\n| 2 | **2** |\n\n\nAnd if we have multiple features/signals that affects the output: \nThe inner list of `X` would include more values:\n\n```python\nfrom sklearn import linear_model\nX = inputs = [[0,0,0], [1,1,1], [2,2,2]]  # Usually we use X to denote the inputs.\nY = outputs = [0, 1, 2]       # Usually we use Y to denote the outputs.\nregressor = linear_model.LinearRegression()\nregressor.fit(X, Y)\n```\n\nAnd visualizing it as a matrix/table:\n\n\n|Feature 1| Feature 2| Feature3 |**Output**|\n|:---:|:---:|:---:|:---:|\n| 0 | 0 | 0 | **0** |\n| 1 | 1 | 1 | **1** | \n| 2 | 2 | 2 | **2** |\n\n\n","cell_type":"markdown","metadata":{"_cell_guid":"14589e8d-7106-4028-9ef9-272325921454","_uuid":"b8c0c407ab4575076a90946355c9d67f883463ab"}},{"source":"**What are these “residuals” everyone keeps talking about?**\n\nA residual is just how far off a model is for a single point. So if our model predicts that a 20 pound cantaloupe should sell for eight dollars and it actually sells for ten dollars, the residual for that data point would be two dollars. Most models will be off by at least a little bit for pretty much all points, but you want to make sure that there’s not a strong pattern in your residuals because that suggests that your model is failing to capture some underlying trend in your dataset.\n____\n\nToday, we're going to practice fitting a regression model to our data and examining the residuals to see if our model is a good representation of our data.\n\n___\n\n<center>\n[**You can check out a video that goes with this notebook by clicking here.**](https://www.youtube.com/embed/3C8SxyD8C7I)\n","cell_type":"markdown","metadata":{"_cell_guid":"69346d2b-a3f1-4197-904c-05d620bcbede","_uuid":"2fad0cd8e08756850d42a457f87fb84241b85898"}},{"source":"## Example: Kaggle data science survey\n___\n\nFor our example today, we're going to use the Kaggle we’re going to use the 2017 Kaggle ML and Data Science Survey. I’m interested in seeing if we can predict the salary of data scientists based on their age. My intuition is that older data scientists, who are probably more experienced, will have higher salaries.\n\nBecause salary is a count value (you're usually paid in integer increments of a unit of currency, and hopefully you shouldn't be being paid a negative amount), we're going to model this with a Poisson regression. \n\nBefore we train a model, however, we need to set up our environment. I'm going to read in two datasets: the Kaggle Data Science Survey for the example and the Stack Overflow Developer Survey for you to work with. \n\nIn **R**\n\n\n```\n# libraries\nlibrary(tidyverse)\nlibrary(boot) #for diagnostic plots\n\n# read in data\nkaggle <- read_csv(\"../input/kaggle-survey-2017/multipleChoiceResponses.csv\")\nstackOverflow <- read_csv(\"../input/so-survey-2017/survey_results_public.csv\")\n```\n\nIn **Python**:","cell_type":"markdown","metadata":{"_cell_guid":"58cd1596-f45b-4fb6-99cc-dd7b7152da74","_uuid":"72649db2fc382d9639f94da23c687b61726cf04b"}},{"source":"from pandas import read_csv\n\n# Note that the Kaggle data seems to be in latin-1 encoding\nkaggle = read_csv(\"../input/kaggle-survey-2017/multipleChoiceResponses.csv\", encoding='iso-8859-1')\nstackoverflow = read_csv(\"../input/so-survey-2017/survey_results_public.csv\", encoding='utf8')","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"9c296d41-7665-4318-9f3d-365b3743e35b","_kg_hide-output":false,"_uuid":"aa4ab90b6d4207aacce6673edb525439a2ecb852"},"execution_count":3},{"source":"Now that we've got our environment set up, I'm going to do a tiny bit of data cleaning. First, I only want to look at rows where we have people who have reported having compensation of more than 0 units of currency. (There are many different currencies in the dataset, but for simplicity I'm going to ignore them.)","cell_type":"markdown","metadata":{"_cell_guid":"b818a1af-cc61-4a4f-9234-44292790b3be","_uuid":"1034eee95c8e480519b16d42386745595ee8ecae"}},{"source":"In **R**:\n\n```\n# do some data cleaning\nhas_compensation <- kaggle %>%\n    filter(CompensationAmount > 0) %>% # only get salaries of > 0\n    mutate(CleanedCompensationAmount = str_replace_all(CompensationAmount,\"[[:punct:]]\", \"\")) %>%\n    mutate(CleanedCompensationAmount = as.numeric(CleanedCompensationAmount)) \n\n# the last two lines remove puncutation (some of the salaries has commas in them)\n# and make sure that salary is numeric\n```\n\nIn **Python**:","cell_type":"markdown","metadata":{"_cell_guid":"3ff70e68-0bc4-4323-b1ed-295d02887742","_uuid":"71daccaca52e978225a9a24ad870f325040c1e5f","collapsed":true}},{"source":"import re\nimport string\n\npunct = re.escape(string.punctuation)\nregex = re.compile(f'[{punct}]')\n\n# Python's numpy/panda gives NaN preferential treatment and leaves \n# them as they are if there's no explicit instructions to handle them.\n# So we have to fill them up with zeros first.\nclean_compensation = kaggle['CompensationAmount'].fillna(0)\n\n# Then we make sure that the column values are string before \n# we apply the regex to handles the the punctuations.\nclean_compensation = clean_compensation.astype(str).apply(lambda x: regex.sub('', x))\n\n# Then create a new 'CleanedCompensationAmount' column with the clean values \n# and we filter all the rows where the 'CompensationAmount' are not numerical\nkaggle['CleanedCompensationAmount'] = clean_compensation\nkaggle_clean = kaggle[kaggle['CleanedCompensationAmount'].apply(lambda x: x.isnumeric())]\n\n# Now, we can safely cast all values in the 'CleanedCompensationAmount' into integers.\nkaggle_clean['CleanedCompensationAmount'] = kaggle_clean['CleanedCompensationAmount'].astype(int)\n\n# Lastly, we filter out the rows where people don't get paid @_@\nhas_compensation = kaggle_clean[kaggle_clean['CleanedCompensationAmount']  > 0]\n\n# Additional to the R cleaning, let's also remove the rows that NaN for the 'Age' column.\nhas_compensation = has_compensation.dropna(subset=['Age'], how='all')","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"77be5261-5016-4368-850e-090a5cf7abaa","_uuid":"ca24e0c957b4f92255eaf519df28b2fb61be9d37"},"execution_count":4},{"source":"In **Python** (with fewer lines):","cell_type":"markdown","metadata":{"_cell_guid":"4ab3291d-b0eb-4f9d-955f-7198a3a9527e","_uuid":"8e9a3d9aab2696dc5f0a4ba7c89ad411de704465"}},{"source":"import re\nimport string\n\npunct = re.escape(string.punctuation)\nregex = re.compile(f'[{punct}]')\n\nkaggle['CleanedCompensationAmount'] = kaggle['CompensationAmount'].fillna(0).astype(str).apply(lambda x: regex.sub('', x))\nkaggle_clean = kaggle[kaggle['CleanedCompensationAmount'].apply(lambda x: x.isnumeric())]\nhas_compensation = kaggle_clean[kaggle_clean['CleanedCompensationAmount'].astype(int)  > 0].dropna(subset=['Age'], how='all')","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"02f1579a-c970-42d6-8a54-b0dcab55ad64","_uuid":"7c097e394ca198be0fc3a47d3aaecdeeee8d1b96","collapsed":true},"execution_count":5},{"source":"Alright, now we're ready to fit our model! To do this, we need to pass the function glm() a formula with the columns we're interested in, the name of the dataframe (so it knows where the columns are from) and the family for our model. Remember from earlier that our formula should look like this:\n\n    Output ~ input\n    \nWe're also predicting a count value, as discussed above, so we want to make sure the family is Poisson.","cell_type":"markdown","metadata":{"_cell_guid":"0a460662-1676-4ac7-8ffa-44d594cce15b","_uuid":"39dd1ea6735a51205591349271f73e9b10972b1a"}},{"source":"In **R**:\n\n```\n# poisson model to predict salary by age\nmodel <- glm(CleanedCompensationAmount ~ Age, data = has_compensation, family = poisson)\n```","cell_type":"markdown","metadata":{"_cell_guid":"839d2bd8-9148-4db8-b9d1-a09a5d9aefdc","_uuid":"ba4edf11d5c2aaeee7a1c009174068d976644269"}},{"source":"\nIn **Python** (using `sklearn` without poisson distribution):","cell_type":"markdown","metadata":{"_cell_guid":"57720279-ef32-4420-a9d8-43de027f449b","_uuid":"98f0f217e54d038c6c1b6678ecc1e9e294b068f2"}},{"source":"import numpy as np\n\n# First we must get 'Age' into the correct input nested array format.\nages_flat_array = np.array(has_compensation['Age'])  # This is a flat array of int.\nages_flat_array","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"464e8a51-4539-41e8-b78d-98f4d93da65a","_uuid":"98e6ebadbf8171b28ec519ead1a684a98d51c49c"},"execution_count":6},{"source":"# We use the `reshape()` function.\nages_nested_array = np.array(has_compensation['Age']).reshape(len(has_compensation), 1)\nages_nested_array","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c6011bce-9824-4324-9321-4f149407b695","_uuid":"db5e0d16ec8e45783290b5e5b38503ad27471a15"},"execution_count":7},{"source":"import numpy as np\n\nfrom sklearn import linear_model\nX = inputs = np.array(has_compensation['Age']).reshape((len(has_compensation), 1))\nY = outputs = has_compensation['CleanedCompensationAmount']\nregressor = linear_model.LinearRegression()\nregressor.fit(X, Y)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a1e23134-b6ae-42e0-be26-372bea2d797a","_uuid":"a748a503eaf88fd47e12c44a46c01a553cbdebc5"},"execution_count":8},{"source":"Sadly `sklearn` doesn't have yet poisson regression. Here's the documentation of the generalized linear models available in `sklear`: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\nBut fear not the `glm` library comes to the rescue.","cell_type":"markdown","metadata":{"_cell_guid":"2bad84cf-3eec-435f-998a-9c8076628dab","_uuid":"e576a38bdc18a65f5d68da64f06474fe82a18d46"}},{"source":"In **Python** (with `statsmodels`):\n\nSee https://stackoverflow.com/a/37942077/610569 for more details","cell_type":"markdown","metadata":{"_cell_guid":"d69d2780-2783-4478-9ddc-e0306cd1ef1f","_uuid":"c0369f57cc5733536267c530ffe63256eb22cd3d"}},{"source":"# Make sure that our input and output are integer\nhas_compensation['CleanedCompensationAmount'] = has_compensation['Age'].astype(int)\nhas_compensation['Age'] = has_compensation['Age'].astype(int)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"ddbb8486-698b-4bc6-b284-d87118c68521","_uuid":"c1c83541452f08978a900aab2fea0cf07663b7a8","collapsed":true},"execution_count":57},{"source":"import statsmodels.formula.api\nfrom statsmodels.genmod.families import Poisson\n\nglm = statsmodels.formula.api.gee # A wrapper to emulate the R syntax.\nmodel = glm(\"CleanedCompensationAmount ~ Age\", groups=None, \n            data=has_compensation, family=Poisson())\nresults = model.fit()\nprint(results.summary())","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b856d4b9-d586-4d8e-9776-63ddac0ad97b","_uuid":"70a0ec95b9ae1577ce0a47582731ef8bacdf4703"},"execution_count":58},{"source":"dWe'll talk about how to examine and interpret a model tomorrow. For now, we want to make sure that it's a good fit for our data and problem. To do this, let's use some diagnostic plots.  ","cell_type":"markdown","metadata":{"_cell_guid":"bc48b84d-9007-48c0-aa36-39fab1c5dded","_uuid":"558483c0acbcf974de3eba25e1a0bf4cd02fb6b2"}},{"source":"In **R**:\n    \n```\n# diagnostic plots\nglm.diag.plots(model)\n```","cell_type":"markdown","metadata":{"_cell_guid":"4e545e8c-8d12-4acb-b719-b467475c9627","_uuid":"cbad51da505f588a4721d6cdad4d4ccf1bef1931"}},{"source":"In **Python**:\n\nSadly, in Python to get diagnostic plots isn't as simple as R. There's a good article on this at https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034 \n\nWe'll try our best to replicate the R plots here:\n","cell_type":"markdown","metadata":{"_cell_guid":"48769f1c-81c9-4a18-b682-7ea596f85f2c","_uuid":"a76c5c8b6936f2f9645013597e48cf11dc0c44f1","collapsed":true}},{"source":"# The libraries we'll need for the plot.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn') # pretty matplotlib plots","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c0dce31c-8a9d-4420-8775-9bcd218457de","_uuid":"3355e5ad1f7b5f78148a1f78776332b767a635ae"},"execution_count":49},{"source":"Before we get to some actual plottings we need to extract the values that we'll be plotting.","cell_type":"markdown","metadata":{}},{"source":"# fitted values (need a constant term for intercept)\nmodel_fitted_y = results.fittedvalues\n# model residuals\nmodel_residuals = results.resid\n# normalized residuals\nmodel_norm_residuals = results.get_influence().resid_studentized_internal\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n# leverage, from statsmodels internals\nmodel_leverage = results.get_influence().hat_matrix_diag\n# cook's distance, from statsmodels internals\nmodel_cooks = results.get_influence().cooks_distance[0]","outputs":[],"cell_type":"code","metadata":{},"execution_count":66},{"source":"# Line Predictor vs Residuals.\nplot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, 'Age', \n                                  data=has_compensation, \n                                  lowess=True, \n                                  scatter_kws={'alpha': 0.5}, \n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplot_lm_1.axes[0].set_xlabel('Line Predictor')\nplot_lm_1.axes[0].set_ylabel('Residuals')","outputs":[],"cell_type":"code","metadata":{},"execution_count":60},{"source":"QQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","outputs":[],"cell_type":"code","metadata":{},"execution_count":61},{"source":"","outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null},{"source":"All of these diagnostic plots are plotting residuals, or how much our model is off for a specific prediction. Spoiler alert: all of these plots are showing us big warning signs for this model! Here's what they should look like:\n\n* **Residuals vs Linear predictor**: You want this to look like a shapeless cloud. If there are outliers it means you've gotten some things very wrong, and if there's a clear pattern it usually means you've picked the wrong type of model. (For logistic regression, you can just ignore this plot. It's checking if the residuals are normally distributed, and logistic regression doesn't assume that they will be.)\n* **Quantiles of standard normal vs. ordered deviance residuals**: For this plot you want to see the residuals lined up along the a diagonal line that goes from the bottom left to top right. If they're strongly off that line, especially in one corner, it means you have a strong skew in your data. (For logistic regression you can ignore this plot too.)\n* **Cook's distance vs. h/(1-h)**: Here, you want your data points to be clustered near zero. If you have a data point that is far from zero (on either axis) it means that it's very influential and that one point is dramatically changing your analysis.\n* **Cook's distance vs. case**: In this plot, you want your data to be mostly around zero on the y axis. The x axis just tells you what row in your dataframe the observation is taken from. Points that are outliers on the y axis are changing your model a lot and should probably be removed (unless you have a good reason to include them).\n\nBased on these diagnostic plots, we should definitely not trust this model. There are a small handful of very influential points that are drastically changing our model. Remember, we didn't convert all the currencies to the same currency, so we're probably seeing some weirdnesses due to including a currency like the Yen, which is worth roughly one one-hundredth of a dollar. \n\nWith that in mind, let's see how the plots change when we remove any salaries above 200,000. ","cell_type":"markdown","metadata":{"_cell_guid":"630aa1f2-3842-4031-87dd-1b5f4f91e5ec","_uuid":"e5ce1294d9d12611e9192e9832d8ddbb7ab473df"}},{"source":"# remove compensation values above 150,000\nhas_compensation <- has_compensation %>%\n    filter(CleanedCompensationAmount < 150000)\n\n# linear model to predict salary by age\nmodel <- glm(CleanedCompensationAmount ~ Age, data = has_compensation, family = poisson)\n\n# diagnostic plots\nglm.diag.plots(model)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"9f62edca-3acf-42b0-91cd-1601d53c030d","_uuid":"ed52f745f37da4690d2a6d43b543642a2abddc4a","collapsed":true},"execution_count":null},{"source":"Now our plots looks much better! Our residuals are more-or-less randomly distributed (which is what the first two plots tell us) and while we still have one outstanding influential point, we can tell by comparing the Cook statistics from the first and second set of plots that it's waaaaaaaayyy less influential than the outliers we got rid of. \n\nOur first model would probably not have been very informative for a new set of observations. Our second model is more likely to be helpful. \n\nAs a final step, we can fit & plot a model to our data, like we did yesterday to see if our hunch about age and salary was correct.","cell_type":"markdown","metadata":{"_cell_guid":"c266788b-2834-4abf-b750-f3e3ee5e28a8","_uuid":"65ab4917cccfb98124a7b4f61fc5cfe50aa44291"}},{"source":"# plot & add a regression line\nggplot(has_compensation, aes(x = Age, y = CleanedCompensationAmount)) + # draw a \n    geom_point() + # add points\n    geom_smooth(method = \"glm\", # plot a regression...\n    method.args = list(family = \"poisson\")) # ...from the binomial family","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"fdc78029-62f1-4045-9574-fa33629d1a5e","_uuid":"8def2ab983d01d3c2fbd517a5846e66b6bffa8c6","collapsed":true},"execution_count":null},{"source":"It looks like we were right about older data scientists making more. It does look like there are some outliers in terms of age, which we could remove with further data cleaning (which you're free to do if you like). First, however, why don't you try your hand at fitting a model and using diagnostic plots to check it out?","cell_type":"markdown","metadata":{"_cell_guid":"f3f971fa-3018-47c1-a9b7-47de2f3d3beb","_uuid":"9628c9d327406c9883ec2dd90ce250bdb964a706"}},{"source":"## Your turn!\n___\n\nNow it's your turn to come up with a model and check it out using diagnostic plots!\n\n1. Pick a question to answer to using the Stack Overflow dataset. (You may want to check out the \"survey_results_schema.csv\" file to learn more about the data.) Pick a variable to predict and one varaible to use to predict it.\n2. Fit a GLM model of the appropriate family. (Check out [yesterday's challenge](https://www.kaggle.com/rtatman/regression-challenge-day-1) if you need a refresher.\n3. Plot diagnostic plots for your model. Does it seem like your model is a good fit for your data? Are the residuals normally distributed (no patterns in the first plot and the points in the second plot are all in a line)? Are there any influential outliers?\n4. Plot your two variables & use \"geom_smooth\" and the appropriate family to fit and plot a model\n5. Optional: If you want to share your analysis with friends or to ask for help, you’ll need to make it public so that other people can see it.\n    * Publish your kernel by hitting the big blue “publish” button. (This may take a second.)\n    * Change the visibility to “public” by clicking on the blue “Make Public” text (right above the “Fork Notebook” button).\n    * Tag your notebook with 5daychallenge","cell_type":"markdown","metadata":{"_cell_guid":"4dd0fe85-1c6b-4bc5-8ed4-fb4d8e5c2c06","_uuid":"88f2044aea7313119366ee7e76b96eebcb71306e"}},{"source":"# your work goes here :)\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b199f580-a489-42d0-af72-615cf9ce3b0c","_uuid":"0b2b31fbcea4f0562377881e7ef8b8cc4329e2c5","collapsed":true},"execution_count":null},{"source":"Want more? Ready for a different dataset? [This notebook](https://www.kaggle.com/rtatman/datasets-for-regression-analysis/) has additional dataset suggestions for you to practice regression with. ","cell_type":"markdown","metadata":{"_cell_guid":"cfd54f2e-6061-4a3e-8bae-1a8feeaf4183","_uuid":"705d478da3ebb89a039bd69bc51363b853a423c3"}}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}