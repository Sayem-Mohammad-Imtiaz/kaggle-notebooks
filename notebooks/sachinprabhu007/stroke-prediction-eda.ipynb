{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df['stroke'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sp_df['stroke'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sp_df['bmi'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sp_df['avg_glucose_level'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sp_df['heart_disease'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(sp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#skewness and kurtosis\nprint(f\"Skewness: {sp_df['stroke'].skew()}\" )\nprint(f\"Kurtosis: {sp_df['stroke'].kurt()}\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ncorrmat = sp_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8,annot=True,square=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ntotal = sp_df.isnull().sum().sort_values(ascending=False)\npercent = (sp_df.isnull().sum()/sp_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_zero_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {df.index.name:'col_name', 0 : 'Missing Values', 1 : '% of Total Values'})\n        mz_table['Data_type'] = df.dtypes\n        mz_table = mz_table[\n            mz_table.iloc[:,1] != 0 ].sort_values(\n        '% of Total Values', ascending=False)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n            \"There are \" + str(mz_table.shape[0]) +\n              \" columns that have missing values.\")\n        return mz_table.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = missing_zero_values_table(sp_df)\nmissing[:20].style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling null values with the mean\nsp_df['bmi'].fillna(sp_df['bmi'].mean(), inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import seaborn as sns\n#sns.countplot(sp_df['stroke'])\n\n\n# Plot the value counts with a bar graph\nsp_df.stroke.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heart Disease Frequency according to Gender\n\nsp_df.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare target column with sex column\npd.crosstab(sp_df.stroke,sp_df.gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.gender).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\"]);\n\nplt.title(\"Stroke Frequencey based on Gender\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\",\"Other\"])\nplt.xticks(rotation=0); #  labels on the x-axis is kept vertical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df['smoking_status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.smoking_status).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\",\"orange\"]);\nplt.title(\"Stroke Frequencey based on Smoking Status\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df['work_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.work_type).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\",\"crimson\",\"orange\",\"blue\"]);\nplt.title(\"Stroke Frequencey based on Work Type\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df[\"hypertension\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.hypertension).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Hyper Tension\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"HyperTension\",\"No HyperTension\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df[\"heart_disease\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.hypertension).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Heart Disease\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"HeartDisease\",\"No HeartDisease\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df[\"Residence_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.Residence_type).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Residence type\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df[\"ever_married\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot\npd.crosstab(sp_df.stroke, sp_df.ever_married).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"crimson\",]);\nplt.title(\"Stroke Frequencey based on Martial Status\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"Married\",\"Not Married\"])\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling Categorical Variables\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nsp_df['gender'] = label.fit_transform(sp_df['gender'])\nsp_df['ever_married'] = label.fit_transform(sp_df['ever_married'])\nsp_df['work_type']= label.fit_transform(sp_df['work_type'])\nsp_df['Residence_type']= label.fit_transform(sp_df['Residence_type'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df['smoking_status'] = label.fit_transform(sp_df['smoking_status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardizing the dataset with Standard Scaler\nfrom sklearn.preprocessing import StandardScaler \n  \nscalar = StandardScaler() \n  \nscalar.fit(sp_df) \nscaled_data = scalar.transform(sp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelling\n# Reference : https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression\n\nfrom statsmodels.tools import add_constant as add_constant\n\nstroke_df_constant = add_constant(sp_df)\nstroke_df_constant.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport scipy.stats as st\n\nst.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols = stroke_df_constant.columns[:-1]\nmodel = sm.Logit(sp_df.stroke,stroke_df_constant[cols])\nresult = model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection: Backward elemination (P-value approach)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def back_feature_elem (data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(stroke_df_constant,sp_df.stroke,cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = sp_df['stroke']\nX = sp_df.drop(['stroke'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation \n#### Model Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy of the model is 94 %"},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion Matrix shows 967 correct predictions and 52+3=55 incorrect ones\n\n### True Positives : 0\n### True Negatives : 967\n### False Positives: 3 (Type 1 Error)\n### False Negatives: 52 (Type 2 Error)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP/float(TP+FN)\nspecificity=TN/float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation - Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) = ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP/(TP+FN) = ',TP/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN/(TN+FP) = ',TN/float(TN+FP),'\\n',\n\n'Positive Predictive value = TP/(TP+FP) = ',TP/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN/(TN+FN) = ',TN/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above statistics it is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of no Stroke(0)','Prob of Stroke (1)'])\ny_pred_prob_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lower the threshold"},{"metadata":{},"cell_type":"raw","source":"Since the model is predicting Stroke disease too many type II errors is not advisable. A False Negative ( ignoring the probability of disease when there actualy is one) is more dangerous than a False Positive in this case. Hence inorder to increase the sensitivity, threshold can be lowered."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Stroke disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Area under the curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmetrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nknn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    score=cross_val_score(knn_classifier,X,y,cv=10)\n    knn_scores.append(score.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nknn_classifier = KNeighborsClassifier(n_neighbors = 12)\nscore=cross_val_score(knn_classifier,X,y,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest_classifier= RandomForestClassifier(n_estimators=10)\n\nscore=cross_val_score(randomforest_classifier,X,y,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reference \n\n1. https://www.kaggle.com/salmaeng/statistical-analysis-eda\n2. https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n3. https://www.kaggle.com/swatis1/stroke-prediction\n4. https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression    \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}