{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the dataset\ndf=pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\nprint(df.head(5))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#explore the dataset\ndf.shape\ndf.info()\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check missing values\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#no missing value in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data visualization\n\n#countplot for gender where female =0, male=1\nsns.countplot(x='sex',data=df)\nplt.title(\"Sex\")\nplt.show()\n\n#countplot for exercise induced angina where 1=yes ,0=No\nsns.countplot(x='exng',data=df)\nplt.title('Exercise induced angina (exng)')\nplt.show()\n\n#countplot for chest pain type\nsns.countplot(x='cp',data=df)\nplt.title('Chest pain type')\nplt.show()\n\n#countplot for resting electrocardiographic results\nsns.countplot(x='restecg',data=df)\nplt.title('Resting electrocardiographic results')\nplt.show()\n\n#countplot for the target variable\nsns.countplot(x='output',data=df)\nplt.title('Having heart attack')\nplt.show()\n\n#histogram of age\nsns.histplot(x='age',data=df)\nplt.title('Distribution of age')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#scale the needed scaling variables\nscaler=StandardScaler()\ndf[['trtbps','chol','thalachh']]=scaler.fit_transform(df[['trtbps','chol','thalachh']])\n\n#assign target variable to y and features to X\ny=df['output']\nX=df.drop('output',axis=1)\nX.shape,y.shape\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\n#divide dataset to training and test sets\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n\n#model the data using different models and compare\n\n#model the data using logistic regression\nlog_reg=LogisticRegression(max_iter=200)\nlog_reg.fit(X_train,y_train)\n\nprint('score of logistic regression model:',log_reg.score(X_test,y_test))\ny_pred_log_reg=log_reg.predict(X_test)\nconfusion_matrix(y_test,y_pred_log_reg,normalize='true')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the bet parameters to use for a classification tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\ndt=DecisionTreeClassifier()\nparam_dt={'criterion':['gini','entropy'],'max_depth':[2,3,4,5,6,7,8,9,10],'random_state':[21,42]}\ngrid_dt=GridSearchCV(estimator=dt,param_grid=param_dt)\nprint('best paramaeters to use for decition tree are:',grid_dt.fit(X_train,y_train).best_params_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build a classification tree model\ndt1=DecisionTreeClassifier(criterion='entropy',max_depth=3,random_state=21)\ndt1.fit(X_train,y_train)\ny_pred_dt=dt1.predict(X_test)\nprint('score of decision tree model:',dt1.score(X_train,y_train))\nprint('confusion matrix of decision tree model:',confusion_matrix(y_test,y_pred_dt,normalize='true'))\n\n#decision tree has higher accuracy in predicting heart attacks than logistic regression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the best parameters for random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier()\nparam_rf={'n_estimators':[100,200,300,400,500],'criterion':['gini', 'entropy'],'max_depth':[2,3,4,5,6,7,8,9,10],'max_features':['auto', 'sqrt', 'log2']}\ngrid_rf=GridSearchCV(estimator=rf,param_grid=param_rf)\nprint(grid_rf.fit(X_train,y_train).best_params_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create random forest model with best parameters\nrf1=RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=2,max_features='auto')\nrf1.fit(X_train,y_train)\ny_pred_rf=rf1.predict(X_test)\nprint('score of random forset model:',rf1.score(X_train,y_train))\nprint('confusion matrix of random forest model:',confusion_matrix(y_test,y_pred_rf,normalize='true'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest is the best model, with accuracy 86.8% and best performance according to confusion matrix","metadata":{},"execution_count":null,"outputs":[]}]}