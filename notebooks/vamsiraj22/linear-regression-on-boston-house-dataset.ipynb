{"cells":[{"metadata":{},"cell_type":"markdown","source":"    Feature description - \n    - CRIM     per capita crime rate by town\n    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n    - INDUS    proportion of non-retail business acres per town\n    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    - NOX      nitric oxides concentration (parts per 10 million)\n    - RM       average number of rooms per dwelling\n    - AGE      proportion of owner-occupied units built prior to 1940\n    - DIS      weighted distances to five Boston employment centres\n    - RAD      index of accessibility to radial highways\n    - TAX      full-value property-tax rate per '$10,000'\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    '% lower status of the population'\n    - MEDV     Median value of owner-occupied homes in $1000's"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cols=['CRIM',\n'ZN',\n'INDUS',\n'CHAS',\n'NOX',\n'RM',\n'AGE',\n'DIS',\n'RAD',\n'TAX',\n'PTRATIO',\n'B',\n'LSTAT',\n'MEDV']\ndf=pd.read_csv(r'/kaggle/input/boston-house-prices/housing.csv',header=None,delim_whitespace=True)\ndf.columns=cols\nprice=df['MEDV']\n#df.drop('MEDV', axis=1, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To observe correlation, I just gave a look at the pairplot for the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"pplot=sns.pairplot(df[:-1])\npplot.fig.set_size_inches(15,15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that there is some correlation between House Price(MEDV) and features like LSTAT,RM,CRIM. "},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\ndf_stand = scaler.fit_transform(df)\ndf_stand=pd.DataFrame(df_stand,columns=cols)\ndf=df_stand","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(df.corr(), center=0, cmap='BrBG',annot=True)\nax.set_title('Multi-Collinearity of Features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the heat map we could see that there is some collinearity between some features NOX, DIS, RAD, TAX. but for better interpretation i prefer implenting VIF.  "},{"metadata":{},"cell_type":"markdown","source":"Inorder to check for outliers. We are plotting BOX plot for visualization. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\nprint(df.columns)\nplt.figure(num=None, figsize=(15,30), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)):\n    plt.subplot(4,4,index+1,xlabel=feature)\n    sns.boxplot(y=df[feature])\n    plt.grid()\n    #plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n    plt.subplots_adjust(top=0.92, bottom=0, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in few features and we remove them below."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\nmedv_correlation={}\nfor i in df.columns:\n    if i!='MEDV':\n        corr_value=df['MEDV'].corr(df[i]) \n        medv_correlation[i]=corr_value\n        print(\"Correlation value between MEDV and {} is {}\".format(i,corr_value))\nprint(\"MAX correlated features are:\", max(medv_correlation, key=medv_correlation.get),min(medv_correlation, key=medv_correlation.get))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have checked the correlation of the features with the dependent variable(MEDV) and found RM and LSTAT are two important features. So, I chose to remove outliers from RM feature."},{"metadata":{},"cell_type":"markdown","source":"Removing Outliers from the Data for the column RM which is highly correlated with MEDV(price)"},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage=0\nfor k, v in df.items():\n    if k=='RM':\n        Q1 = np.array(v.quantile(0.25))\n        Q3 = np.array(v.quantile(0.75))\n        IQR = Q3 - Q1\n        v_col = v[(v <= Q1 - 1.5 * IQR) | (v >= Q3 + 1.5 * IQR)]\n        percentage = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, percentage))\nprint(\"Total number of outliers for feature RM is %d  which is %.2f%% of total data points\"%(v_col.count(),percentage)) \n\ndf=df[~(df['RM'] >= Q3 + 1.5 * IQR)|(df['RM'] <=Q1 - 1.5 * IQR)]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardizing the data inorder to compute correlation using Variance Inflation Factor method."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nscaler = preprocessing.StandardScaler()\ndf_stand = scaler.fit_transform(df)\ndf_stand=pd.DataFrame(df_stand,columns=cols)\ndf_stand","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n# Calculating VIF\ns=df._get_numeric_data()\nvif = pd.DataFrame()\nvif[\"variables\"] = s.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(np.array(s.values,dtype=float), i) for i in range(s.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually, It is considered that if the VIF score is <5 they are less collinearity and near to or above 10 they have high collinearity. Best score is VIF = 1 meaning they have 0 collinearity.\n\nHere since there are majority below 5, I have chosen only the ones that are less than the mean of all VIF's and termed them as Non Collinear and rest as collinear."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"vif['Result'] = vif['VIF'].apply(lambda x: 'Non Collinear' if x <= 3.7 else 'Collinear')\nvif","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"features=list(vif[vif['Result']=='Non Collinear']['variables'])\n\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the collinear features and for further analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    if i!='MEDV':\n        if i not in features:\n            df.drop(i,axis=1,inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing the features relationship with dependent variable MEDV."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)):\n    colors = (0,0,0)\n    area = np.pi*3\n    plt.subplot(4,4,index+1,xlabel=feature,ylabel='MEDV')\n    sns.regplot(y='MEDV',x=feature,data=df,marker=\"+\",ci=80)\n    plt.grid()\n    plt.subplots_adjust(top=0.92, bottom=0, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly we understand that our price(MEDV) is getting lower as LSTAT and CRIM is increasing. And we can say that the price(MEDV) is increasing as the no. of rooms(RM) is increasing."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_plot=pd.read_csv(r'/kaggle/input/boston-house-prices/housing.csv',header=None,delim_whitespace=True)\ndf_plot.columns=cols\n\n\nfor i in df.columns:\n    if i!='MEDV':\n        if i not in features:\n            df_plot.drop(i,axis=1,inplace=True)\n\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)[:-1]):\n    counts,bin_edges= np.histogram(df_plot[feature],bins=10,density=True)\n    pdf=counts/sum(counts)\n    #a=440+var\n    cdf=np.cumsum(pdf)\n    plt.subplot(4,4,index+1,xlabel=feature)\n    plt.plot(bin_edges[1:],pdf)\n    plt.plot(bin_edges[1:],cdf)\n    plt.grid()\n    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see the PDF and CDF of the features. Almost 70-80% of our dataset has CRIM less than 20 ."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\ny=df_stand['MEDV'] \ndf_stand.drop('MEDV',axis=1,inplace=True)\nX=df_stand\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"regressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Error for our prediction is 0.55 . The Plot for our predicted and test data is below."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test,y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}