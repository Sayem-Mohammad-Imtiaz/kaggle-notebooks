{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# !pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n# !pip uninstall -y enum34\n# !pip install scikit-learn-extra\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.cluster import Birch, AgglomerativeClustering, KMeans, MiniBatchKMeans, DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n# from sklearn_extra.cluster import KMedoids\n\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Read data and clean"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/fifa-20-complete-player-dataset/players_20.csv')\ncol_list = np.r_[4:5, 6:8, 10:14, 16:19, 22:23, 31:43, 45:78]\ndf_numeric = df.iloc[:, col_list]\ndf_numeric = df_numeric.fillna(0)\ndf = df_numeric\n# df.drop(['Unnamed: 0'], inplace=True, axis=1)\ndf.drop(['goalkeeping_diving'], inplace=True, axis=1)\ndf.drop(['goalkeeping_handling'], inplace=True, axis=1)\ndf.drop(['goalkeeping_kicking'], inplace=True, axis=1)\ndf.drop(['goalkeeping_positioning'], inplace=True, axis=1)\ndf.drop(['goalkeeping_reflexes'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n# Standardize the data to have a mean of ~0 and a variance of 1\nX_std = StandardScaler().fit_transform(df)\n# Create a PCA instance: pca\npca = PCA(n_components=0.95)\nprincipalComponents = pca.fit_transform(X_std)\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_ratio_, color='black')\nplt.xlabel('PCA features')\nplt.ylabel('variance %')\nplt.xticks(features)\n# Save components to a DataFrame\nPCA_components = pd.DataFrame(principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtaf = pd.read_csv('/kaggle/input/fifa-20-complete-player-dataset/players_20.csv')\n# dtaf = dtaf.select_dtypes([np.number])\ncol_list = np.r_[4:5, 6:8, 10:14, 16:19, 22:23, 31:43, 45:78]\ndtaf = dtaf.iloc[:, col_list]\ndtaf.isna().sum() \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/newdataset/players_20_numeric_normalized-2.csv')\ndf.dataframeName = 'players_20_numeric_normalized.csv'\n\n#clean\ndf.drop(['Unnamed: 0'], inplace=True, axis=1)\ndf.drop(['goalkeeping_diving'], inplace=True, axis=1)\ndf.drop(['goalkeeping_handling'], inplace=True, axis=1)\ndf.drop(['goalkeeping_kicking'], inplace=True, axis=1)\ndf.drop(['goalkeeping_positioning'], inplace=True, axis=1)\ndf.drop(['goalkeeping_reflexes'], inplace=True, axis=1)\nrowNum, colNum = df.shape\nprint(f\"There are {rowNum} rows and {colNum} cols\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom collections import Counter\ndf_o = df[df['gk_speed']==0]\n# print(df_gk)\n\ndf_gk = df[df['gk_speed']!=0]\nNUM_GOALKEEPER = df_gk.shape[0]\n# print(df_o)\nprint(NUM_GOALKEEPER)\n\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# # calculate the distance from each point to its closest neighbor\n# nn = NearestNeighbors(n_neighbors = 50)\n\n# # fit the nearest neighbor\n# nbrs = nn.fit(df_o)\n\n# # returns two arrays - distance to the closest n_neighbors points and index for each point\n# distances, indices = nbrs.kneighbors(df_o)\n\n# # sort the distance and plot it\n# distances = np.sort(distances, axis=0)\n# distances = distances[:,1]\n# plt.plot(distances)\ndb = DBSCAN(eps=1, min_samples=4).fit(StandardScaler().fit_transform(PCA_components.iloc[:,:3]))\n\n# db = DBSCAN(eps=3, min_samples=4).fit(StandardScaler().fit_transform(df_o))\n\n# df_concat = pd.concat([df_gk, df_o]);\n# gk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=3, dtype=np.int)\n# labels_concat = np.concatenate((gk_labels, db.labels_))\n\n# print(db.labels_.value_counts())\nprint(Counter(db.labels_))\n# print(len(set(db.labels_)) - (1 if -1 in db.labels_ else 0))\n# print(metrics.calinski_harabasz_score(df_o,db.labels_))\n# print(metrics.calinski_harabasz_score(df_concat, labels_concat))\n# print(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.calinski_harabasz_score(df, db.labels_))\nprint(metrics.davies_bouldin_score(df, db.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Exploration:"},{"metadata":{},"cell_type":"markdown","source":"# correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\nplotCorrelationMatrix(df, 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Clustering"},{"metadata":{},"cell_type":"markdown","source":"# splitting goalkeeper & non-goalkepper"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_gk = df[df['gk_speed']!=0]\nNUM_GOALKEEPER = df_gk.shape[0]\n# print(df_o)\nprint(NUM_GOALKEEPER)\n\ndf_o = df[df['gk_speed']==0]\n# print(df_gk)\nNUM_OTHERS = df_o.shape[0]\nprint(NUM_OTHERS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0. K-MEANS"},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = KMeans(n_clusters=4)\nclust_model.fit(df)\nlabels = clust_model.labels_\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in range(2, 10):\n    clust_model = KMeans(n_clusters=n)\n    clust_model.fit(df)\n    labels = clust_model.labels_\n    print(n)\n    print(metrics.calinski_harabasz_score(df, labels))\n    print(metrics.davies_bouldin_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import Birch, AgglomerativeClustering, KMeans, MiniBatchKMeans\nclust_model = MiniBatchKMeans(n_clusters=4)\nclust_model.fit(df)\nlabels = clust_model.labels_\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import kmeans_plusplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df => df_o, df_gk\nclust_model = KMeans(n_clusters=3)\nclust_model.fit(df_o)\nlabels = clust_model.labels_\n\ndf_concat = pd.concat([df_gk, df_o]);\ngk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=3, dtype=np.int)\nlabels_concat = np.concatenate((gk_labels, labels))\n# print(type(gk_labels))\n# print(gk_labels.shape)\n# print(type(labels))\n# print(labels.shape)\n\nprint(metrics.calinski_harabasz_score(df_o, labels))\nprint(metrics.davies_bouldin_score(df_o, labels))\nprint(metrics.calinski_harabasz_score(df_concat, labels_concat))\nprint(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. BIRCH"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendrogrm = sch.dendrogram(sch.linkage(df_o, method = 'ward', metric='euclidean'))\nplt.title('Dendrogram')\nplt.xlabel('Players')\nplt.ylabel('Euclidean distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best #cluster seems to be 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df => df_o, df_gk\nclust_model = Birch(n_clusters=2)\nclust_model.fit(df_o)\nlabels = clust_model.labels_\n\ndf_concat = pd.concat([df_gk, df_o]);\ngk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=3, dtype=np.int)\nlabels_concat = np.concatenate((gk_labels, labels))\n# print(type(gk_labels))\n# print(gk_labels.shape)\n# print(type(labels))\n# print(labels.shape)\n\nprint(metrics.calinski_harabasz_score(df_o, labels))\nprint(metrics.davies_bouldin_score(df_o, labels))\nprint(metrics.calinski_harabasz_score(df_concat, labels_concat))\nprint(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. AHC"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df => df_o, df_gk\nclust_model = AgglomerativeClustering(n_clusters=3)\nclust_model.fit(df_o)\nlabels = clust_model.labels_\n\ndf_concat = pd.concat([df_gk, df_o]);\ngk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=3, dtype=np.int)\nlabels_concat = np.concatenate((gk_labels, labels))\n# print(type(gk_labels))\n# print(gk_labels.shape)\n# print(type(labels))\n# print(labels.shape)\n\nprint(metrics.calinski_harabasz_score(df_o, labels))\nprint(metrics.davies_bouldin_score(df_o, labels))\nprint(metrics.calinski_harabasz_score(df_concat, labels_concat))\nprint(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"3. CLARA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClaraClustering(object):\n    \"\"\"The clara clustering algorithm.\n\n    Basically an iterative guessing version of k-mediods that makes things a lot faster\n    for bigger data sets.\n    \"\"\"\n\n    def __init__(self, max_iter=100000):\n        \"\"\"Class initialization.\n\n        :param max_iter: The default number of max iterations\n        \"\"\"\n        self.max_iter = max_iter\n        self.dist_cache = dict()\n\n    def clara(self, _df, _k, _fn):\n        \"\"\"The main clara clustering iterative algorithm.\n\n        :param _df: Input data frame.\n        :param _k: Number of medoids.\n        :param _fn: The distance function to use.\n        :return: The minimized cost, the best medoid choices and the final configuration.\n        \"\"\"\n        size = len(_df)\n        if size > 100000:\n            niter = 1000\n            runs = 1\n        else:\n            niter = self.max_iter\n            runs = 5\n\n        min_avg_cost = np.inf\n        best_choices = []\n        best_results = {}\n\n        for j in range(runs):\n            sampling_idx = random.sample([i for i in range(size)], (40+_k*2))\n            sampling_data = []\n            for idx in sampling_idx:\n                sampling_data.append(_df.iloc[idx])\n\n            sampled_df = pd.DataFrame(sampling_data)\n            pre_cost, pre_choice, pre_medoids = self.k_medoids(sampled_df, _k, _fn, niter)\n            tmp_avg_cost, tmp_medoids = self.average_cost(_df, _fn, pre_choice)\n            if tmp_avg_cost <= min_avg_cost:\n                min_avg_cost = tmp_avg_cost\n                best_choices = list(pre_choice)\n                best_results = dict(tmp_medoids)\n\n        return min_avg_cost, best_choices, best_results\n\n    def k_medoids(self, _df, _k, _fn, _niter):\n        \"\"\"The original k-mediods algorithm.\n\n        :param _df: Input data frame.\n        :param _k: Number of medoids.\n        :param _fn: The distance function to use.\n        :param _niter: The number of iterations.\n        :return: Cluster label.\n\n        Pseudo-code for the k-mediods algorithm.\n        1. Sample k of the n data points as the medoids.\n        2. Associate each data point to the closest medoid.\n        3. While the cost of the data point space configuration is decreasing.\n            1. For each medoid m and each non-medoid point o:\n                1. Swap m and o, recompute cost.\n                2. If global cost increased, swap back.\n        \"\"\"\n        print('K-medoids starting')\n        # Do some smarter setting of initial cost configuration\n        pc1, medoids_sample = self.cheat_at_sampling(_df, _k, _fn, 17)\n        prior_cost, medoids = self.compute_cost(_df, _fn, medoids_sample)\n        current_cost = prior_cost\n        iter_count = 0\n        best_choices = []\n        best_results = {}\n\n        print('Running with {m} iterations'.format(m=_niter))\n        while iter_count < _niter:\n            for m in medoids:\n                clust_iter = 0\n                for item in medoids[m]:\n                    if item != m:\n                        idx = list(medoids_sample).index(m)\n                        swap_temp = medoids_sample[idx]\n                        medoids_sample[idx] = item\n                        tmp_cost, tmp_medoids = self.compute_cost(_df, _fn, medoids_sample, True)\n                        if (tmp_cost < current_cost) & (clust_iter < 1):\n                            best_choices = list(medoids_sample)\n                            best_results = dict(tmp_medoids)\n                            current_cost = tmp_cost\n                            clust_iter += 1\n                        else:\n                            best_choices = best_choices\n                            best_results = best_results\n                            current_cost = current_cost\n                        medoids_sample[idx] = swap_temp\n\n            iter_count += 1\n            if best_choices == medoids_sample:\n                print('Best configuration found!')\n                break\n\n            if current_cost <= prior_cost:\n                prior_cost = current_cost\n                medoids = best_results\n                medoids_sample = best_choices\n\n        return current_cost, best_choices, best_results\n\n    def compute_cost(self, _df, _fn, _cur_choice, cache_on=True):\n        \"\"\"A function to compute the configuration cost.\n\n        :param _df: The input data frame.\n        :param _fn: The distance function.\n        :param _cur_choice: The current set of medoid choices.\n        :param cache_on: Binary flag to turn caching.\n        :return: The total configuration cost, the mediods.\n        \"\"\"\n        size = len(_df)\n        total_cost = 0.0\n        medoids = {}\n        for idx in _cur_choice:\n            medoids[idx] = []\n\n        for i in range(size):\n            choice = -1\n            min_cost = np.inf\n            for m in medoids:\n                if cache_on:\n                    tmp = self.dist_cache.get((m, i), None)\n\n                if not cache_on or tmp is None:\n                    if _fn == 'manhattan':\n                        tmp = self.manhattan_distance(_df.iloc[m], _df.iloc[i])\n                    elif _fn == 'cosine':\n                        tmp = self.cosine_distance(_df.iloc[m], _df.iloc[i])\n                    elif _fn == 'euclidean':\n                        tmp = self.euclidean_distance(_df.iloc[m], _df.iloc[i])\n                    elif _fn == 'fast_euclidean':\n                        tmp = self.fast_euclidean(_df.iloc[m], _df.iloc[i])\n                    else:\n                        print('You need to input a distance function.')\n\n                if cache_on:\n                    self.dist_cache[(m, i)] = tmp\n\n                if tmp < min_cost:\n                    choice = m\n                    min_cost = tmp\n\n            medoids[choice].append(i)\n            total_cost += min_cost\n\n        return total_cost, medoids\n\n    def average_cost(self, _df, _fn, _cur_choice):\n        \"\"\"A function to compute the average cost.\n\n        :param _df: The input data frame.\n        :param _fn: The distance function.\n        :param _cur_choice: The current medoid candidates.\n        :return: The average cost, the new medoids.\n        \"\"\"\n        _tc, _m = self.compute_cost(_df, _fn, _cur_choice)\n        avg_cost = _tc / len(_m)\n        return avg_cost, _m\n\n    def cheat_at_sampling(self, _df, _k, _fn, _nsamp):\n        \"\"\"A function to cheat at sampling for speed ups.\n\n        :param _df: The input data frame.\n        :param _k: The number of mediods.\n        :param _fn: The distance function.\n        :param _nsamp: The number of samples.\n        :return: The best score, the medoids.\n        \"\"\"\n        size = len(_df)\n        score_holder = []\n        medoid_holder = []\n        for _ in range(_nsamp):\n            medoids_sample = random.sample([i for i in range(size)], _k)\n            prior_cost, medoids = self.compute_cost(_df, _fn, medoids_sample, True)\n            score_holder.append(prior_cost)\n            medoid_holder.append(medoids)\n\n        idx = score_holder.index(min(score_holder))\n        ms = medoid_holder[idx].keys()\n        return score_holder[idx], ms\n\n    def euclidean_distance(self, v1, v2):\n        \"\"\"Slow function for euclidean distance.\n\n        :param v1: The first vector.\n        :param v2: The second vector.\n        :return: The euclidean distance between v1 and v2.\n        \"\"\"\n        dist = 0\n        for a1, a2 in zip(v1, v2):\n            dist += abs(a1 - a2)**2\n        return dist\n\n    def fast_euclidean(self, v1, v2):\n        \"\"\"Faster function for euclidean distance.\n\n        :param v1: The first vector.\n        :param v2: The second vector.\n        :return: The euclidean distance between v1 and v2.\n        \"\"\"\n        return np.linalg.norm(v1 - v2)\n\n    def manhattan_distance(self, v1, v2):\n        \"\"\"Function for manhattan distance.\n\n        :param v1: The first vector.\n        :param v2: The second vector.\n        :return: The manhattan distance between v1 and v2.\n        \"\"\"\n        dist = 0\n        for a1, a2 in zip(v1, v2):\n            dist += abs(a1 - a2)\n        return dist\n\n    def cosine_distance(self, v1, v2):\n        \"\"\"Function for cosine distance.\n\n        :param v1: The first vector.\n        :param v2: The second vector.\n        :return: The cosine distance between v1 and v2.\n        \"\"\"\n        xx, yy, xy = 0, 0, 0\n        for a1, a2 in zip(v1, v2):\n            xx += a1*a1\n            yy += a2*a2\n            xy += a1*a2\n        return float(xy) / np.sqrt(xx*yy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clara_new(_df,_k):\n    \"\"\"The main clara clustering iterative algorithm.\n\n        :param _df: Input data frame.\n        :param _k: Number of medoids.\n        :return: The minimized cost, the best medoid choices and the final configuration.\n        \"\"\"\n    size = len(_df)\n    if size > 100000:\n        niter = 1000\n        runs = 1\n    else:\n        niter = 100000\n        runs = 5\n        # runs = 2\n\n    min_avg_cost = np.inf\n    best_choices = []\n    best_results = {}\n    \n    for j in range(runs):\n        sampling_idx = random.sample([i for i in range(size)], (40+_k*2))\n        sampling_data = []\n        for idx in sampling_idx:\n            sampling_data.append(_df.iloc[idx])\n        \n        sampled_df = pd.DataFrame(sampling_data)\n        cobj = KMedoids(n_clusters=_k,random_state=0,max_iter=niter).fit(sampled_df)\n        pre_choice = cobj.medoid_indices_\n        print(pre_choice);\n        tmp_avg_cost, tmp_medoids = Clara.average_cost(_df, 'euclidean', pre_choice)\n#         print(\"debug 1: \"+str(tmp_avg_cost));\n        # print(\"debug 2:\" + str(tmp_medoids))\n        result = dict(tmp_medoids)\n#         print(type(result[pre_choice[0]]))\n#         print(len(result[pre_choice[0]]))\n        if tmp_avg_cost <= min_avg_cost:\n            min_avg_cost = tmp_avg_cost\n            best_choices = list(pre_choice)\n            best_results = dict(tmp_medoids)\n    return min_avg_cost, best_choices, best_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cluster = 4\nresult = clara_new(df_o,num_cluster)\n\nbest_choices = np.full(shape=NUM_OTHERS, fill_value = 0,dtype=np.int)\nfor cluster_index in range(num_cluster):\n    for index in result[2][result[1][cluster_index]]:\n        best_choices[index] = cluster_index\n\nprint(sk.metrics.calinski_harabasz_score(df,best_choices))\nprint(sk.metrics.davies_bouldin_score(df,best_choices))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(df.isna().sum())\nprint(df.columns)\ndf.head(5)\ndf['goalkeeping_handling'].value_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n#     df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n#     print(nCol)\n#     print(nGraphShown)\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\nplotPerColumnDistribution(df, 57, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\nplotScatterMatrix(df, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dendogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendrogrm = sch.dendrogram(sch.linkage(df, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Players')\nplt.ylabel('Euclidean distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering"},{"metadata":{},"cell_type":"markdown","source":"# Birch"},{"metadata":{"trusted":true},"cell_type":"code","source":"Birch-clustering full dataset 2 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = Birch(n_clusters=2)\nclust_model.fit(df)\nlabels = clust_model.labels_\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))\nprint(metrics.silhouette_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Birch-clustering full dataset 4 cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = Birch(n_clusters=4)\nclust_model.fit(df)\nlabels = clust_model.labels_\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"10211.76140489249\n1.6618000246511475","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels)\n#print(labels.value_counts)\nfrom collections import Counter\n\nCounter(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Birch-clustering separate goal-keeper 3 clusters"},{"metadata":{},"cell_type":"markdown","source":"Birch-clustering separate goal-keeper 5 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df => df_o, df_gk\nclust_model = Birch(n_clusters=5)\nclust_model.fit(df_o)\nlabels = clust_model.labels_\n\ndf_concat = pd.concat([df_gk, df_o]);\ngk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=5, dtype=np.int)\nlabels_concat = np.concatenate((gk_labels, labels))\n\nprint(metrics.calinski_harabasz_score(df_concat, labels_concat))\nprint(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AHC"},{"metadata":{"trusted":true},"cell_type":"code","source":"AHC fulldataset 2 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = AgglomerativeClustering(n_clusters=2)\nclust_model.fit(df)\nlabels = clust_model.labels_\n\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))\nprint(metrics.silhouette_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AHC full dataset 4 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = AgglomerativeClustering(n_clusters=4)\nclust_model.fit(df)\nlabels = clust_model.labels_\n\nprint(metrics.calinski_harabasz_score(df, labels))\nprint(metrics.davies_bouldin_score(df, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"13756.801318502405\n1.3900641688585644","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AHC separate goal-keeper 3 clusters"},{"metadata":{},"cell_type":"markdown","source":"AHC separate goal-keeper 5 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df => df_o, df_gk\nclust_model = AgglomerativeClustering(n_clusters=5)\nclust_model.fit(df_o)\nlabels = clust_model.labels_\n\ndf_concat = pd.concat([df_gk, df_o]);\ngk_labels = np.full(shape = NUM_GOALKEEPER, fill_value=5, dtype=np.int)\nlabels_concat = np.concatenate((gk_labels, labels))\n# print(type(gk_labels))\n# print(gk_labels.shape)\n# print(type(labels))\n# print(labels.shape)\n\nprint(metrics.calinski_harabasz_score(df_concat, labels_concat))\nprint(metrics.davies_bouldin_score(df_concat, labels_concat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize=(16, 9))\nelbow_ch = figure.add_subplot(2,2,1) #elbow charts\nelbow_db = figure.add_subplot(2,2,2)\nelbow_sil = figure.add_subplot(2,2,3)\ncost_ch=[]\ncost_db=[]\ncost_sil=[]\n\nelbow_ch.set_ylabel('Calinski_Harabasz Score', fontsize = 15)\nelbow_ch.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_ch.set_title('BIRCH Clustering: Elbow Chart', fontsize = 15)\n    \nelbow_db.set_ylabel('Davies-Bouldin Score', fontsize = 15)\nelbow_db.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_db.set_title('BIRCH Clustering: Elbow Chart', fontsize = 15)\n\nelbow_sil.set_ylabel('Silhouette Score', fontsize = 15)\nelbow_sil.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_sil.set_title('BIRCH Clustering: Elbow Chart', fontsize = 15)\nfor num in range(2, 25):\n    clust_model = Birch(n_clusters=num)\n    clust_model.fit(df)\n    labels = clust_model.labels_\n    cost_ch.append(metrics.calinski_harabasz_score(df, labels))\n    cost_db.append(metrics.davies_bouldin_score(df, labels))\n    cost_sil.append(metrics.silhouette_score(df, labels, metric='euclidean'))\n    elbow_ch.plot(cost_ch, 'bx-')\n    elbow_db.plot(cost_db, 'bx-')\n    elbow_sil.plot(cost_sil, 'bx-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize=(16, 9))\nelbow_ch = figure.add_subplot(2,2,1) #elbow charts\nelbow_db = figure.add_subplot(2,2,2)\nelbow_sil = figure.add_subplot(2,2,3)\ncost_ch=[]\ncost_db=[]\ncost_sil=[]\n\nelbow_ch.set_ylabel('Calinski_Harabasz Score', fontsize = 15)\nelbow_ch.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_ch.set_title('Agglomerative Clustering: Elbow Chart', fontsize = 15)\n    \nelbow_db.set_ylabel('Davies-Bouldin Score', fontsize = 15)\nelbow_db.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_db.set_title('Agglomerative Clustering: Elbow Chart', fontsize = 15)\n\nelbow_sil.set_ylabel('Silhouette Score', fontsize = 15)\nelbow_sil.set_xlabel('Number of Clusters', fontsize = 15)\nelbow_sil.set_title('Agglomerative Clustering: Elbow Chart', fontsize = 15)\n# clustering = AgglomerativeClustering().fit(X)\n# for num in range(2, 25):\nfor num in range (2, 10):\n    clust_model = AgglomerativeClustering(n_clusters=num)\n    clust_model.fit(df)\n    labels = clust_model.labels_\n    cost_ch.append(metrics.calinski_harabasz_score(df, labels))\n    cost_db.append(metrics.davies_bouldin_score(df, labels))\n    cost_sil.append(metrics.silhouette_score(df, labels, metric='euclidean'))\n    elbow_ch.plot(cost_ch, 'bx-')\n    elbow_db.plot(cost_db, 'bx-')\n    elbow_sil.plot(cost_sil, 'bx-')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}