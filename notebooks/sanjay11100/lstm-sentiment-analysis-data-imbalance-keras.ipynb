{"cells":[{"metadata":{"_uuid":"50bbbd47a98fd909d5838623ad52ef7cf28702a3"},"cell_type":"markdown","source":"The code in this post can be found at my [Github](https://github.com/sanjay-raghu) repository. If you are also interested in trying out the code\nI have also written a blog on my [website](https://sanjay-raghu.github.io/Sentiment-Analysis-Using-LSTM/) you can visit there.   \n\n\n\n**Sentiment Analysis:**<br> \nThe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral. In common ML words its just a classification problem. \n\n**What is class imbalance:**<br>\nIt is the problem in machine learning where the total number of a class of data (positive) is far less than the total number of another class of data (negative). This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.\n\nIf you want to know more about class imbalance problem, [here](http://www.chioka.in/class-imbalance-problem/) is a link of a great blog post\n\n**Solving class imbalanced data:**<br>\nI am using the two most effective ways to mitigate this:<br>\n- Up sampling \n- Using class weighted loss function\n\n**Dataset**<br>\nFirst GOP Debate Twitter Sentiment\nAbout this Dataset\nThis data originally came from [Crowdflower's Data for Everyone library ](http://www.crowdflower.com/data-for-everyone).\n\n> As the original source says,\n> We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both\n> sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned,\n> what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from\n> the uploaded dataset.\n\n**Details about model**<br>\n - model contains 3 layers (Embedding, LSTM, Dense with softmax).\n - Up-sampling is used to balance the data of minority class.\n - Loss function with different class weight in keras to further reduce class imbalance.\n"},{"metadata":{"_uuid":"24f5d09448a55ff218617b74ce635b85ca7af80b"},"cell_type":"markdown","source":"## Lets start coding\n### Importing useful packages\nLets first import all libraries. Please make sure that you have these libraries installed.   \n"},{"metadata":{"_cell_guid":"6c53202d-5c34-4859-e7e9-8ef5c7068287","_uuid":"717bb968c36b9325c7d4cae5724a3672e49ff243","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2bc2702e-d6f4-df5f-b80e-50ab23a6d29e","_uuid":"9b520acffb5cd85d0e1ada968ad0f12cee33a4b5"},"cell_type":"markdown","source":"### Data Preprocessing\n- reading the data\n- kepping only neccessary columns\n- droping \"Neutral\" sentiment data\n"},{"metadata":{"_cell_guid":"89c8c923-c0bf-7b35-9ab8-e63f00b74e5a","_uuid":"d2bc3bbd2ea3961c49e6673145a0a7226c160e58","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Sentiment.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndata = data[data.sentiment != \"Neutral\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68989f3bc936825f4425d2d08467ce17c4a2f092"},"cell_type":"markdown","source":"Let See the few lines of the data"},{"metadata":{"trusted":true,"_uuid":"f6a102c71c8e281450f7e73a5678cc9d0bb99e99"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4c0ec63b-cdf8-8e29-812b-0fbbfcea2929","_uuid":"ff12d183224670f9c4c96fd24581b9924d4dff20"},"cell_type":"markdown","source":"> A few things to notice here\n- \"RT @...\" in start of every tweet\n- a lot of special characters <br>\n> We have to remove all this noise also lets convert text into lower case.\n"},{"metadata":{"_cell_guid":"43632d2d-6160-12ce-48b0-e5eb1c207076","_uuid":"d0f8b4542106a279f7398db7285ae5e370b2e813","trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: x.lower())\n# removing special chars\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata['text'] = data['text'].str.replace('rt','')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43632d2d-6160-12ce-48b0-e5eb1c207076","_uuid":"d0f8b4542106a279f7398db7285ae5e370b2e813","trusted":true},"cell_type":"markdown","source":"This looks better.<br>\nLets pre-process the data so that we can use it to train the model\n- Tokenize\n- Padding (to make all sequence of same lengths)\n- Converting sentiments into numerical data(One-hot form)\n- train test split\n"},{"metadata":{"_cell_guid":"43632d2d-6160-12ce-48b0-e5eb1c207076","_uuid":"d0f8b4542106a279f7398db7285ae5e370b2e813","trusted":true},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)\n\nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9753421e-1303-77d5-b17f-5f25fa08c452","_uuid":"aa7d103e946e631133d86ef3adc73e1a8b1a1e89"},"cell_type":"markdown","source":"### Defining model\nNext, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyper parameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."},{"metadata":{"_cell_guid":"1ba3cf60-a83c-9c21-05e0-b14303027e93","_uuid":"05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda","trusted":true,"scrolled":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15f4ee61-47e4-88c4-4b81-98a85237333f","_uuid":"2dae0f3b95a4ba533453c512e573560a8358e162"},"cell_type":"markdown","source":"### Let's train the model\nHere we train the Network. We should run much more than 15 epoch, but I would have to wait forever (run more epochs later), so it is 15 for now. you will see progress bar (if you want to shut it up use verbose = 0)\n\n"},{"metadata":{"_cell_guid":"d5e499ac-2eba-6ff7-8d9a-ff65eb04099b","_uuid":"d0b239912cf67294a9f5af6883bb159c44318fc7","trusted":true},"cell_type":"code","source":"batch_size = 128\nmodel.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434","_uuid":"47e99d7ed1f27a85eb01dbafc71b66b329fb1d12"},"cell_type":"markdown","source":"### Let evaluate the model\n"},{"metadata":{"trusted":true,"_uuid":"4701961e44bd243e505fc2c1b53b323311ad2b80"},"cell_type":"code","source":"Y_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\nprint(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"018ebf39-9414-27d0-232c-a34de051feaf","_uuid":"4b54f18bbf22a953c60f271c318cb076e684df9c"},"cell_type":"markdown","source":"> It is clear that finding negative tweets (**class 0**) goes very well (**recall 0.92**) for the Network but deciding whether is positive (**class 1**) is not really (**recall 0.52**). My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets.\n## Solving data imbalance problem\n\n**Up-sample Minority Class**\n\nUp-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal. There are several heuristics for doing so, but the most common way is to simply re-sample with replacement.\n\nIt's important that we separate test set before up-sampling because after up-sampling there will be multiple copies of same data point and if we do train test split after up-sampling the test set will not be completely unseen.\n"},{"metadata":{"trusted":true,"_uuid":"384ed509b57715fbc7cce5ad37802a8785603b52"},"cell_type":"code","source":"# Separate majority and minority classes\ndata_majority = data[data['sentiment'] == 'Negative']\ndata_minority = data[data['sentiment'] == 'Positive']\n\nbias = data_minority.shape[0]/data_majority.shape[0]\n# lets split train/test data first then \ntrain = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n         data_minority.sample(frac=0.8,random_state=200)])\ntest = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n\ntrain = shuffle(train)\ntest = shuffle(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ce10e582bd894ec271f2587ceb618a9cf8ab03c5"},"cell_type":"code","source":"print('positive data in training:',(train.sentiment == 'Positive').sum())\nprint('negative data in training:',(train.sentiment == 'Negative').sum())\nprint('positive data in test:',(test.sentiment == 'Positive').sum())\nprint('negative data in test:',(test.sentiment == 'Negative').sum())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1006f3ca7009e0838d74a8fd5a3fee80375bbe0"},"cell_type":"markdown","source":"Now Lets do up-sampling"},{"metadata":{"trusted":true,"_uuid":"384ed509b57715fbc7cce5ad37802a8785603b52"},"cell_type":"code","source":"# Separate majority and minority classes in training data for upsampling \ndata_majority = train[train['sentiment'] == 'Negative']\ndata_minority = train[train['sentiment'] == 'Positive']\n\nprint(\"majority class before upsample:\",data_majority.shape)\nprint(\"minority class before upsample:\",data_minority.shape)\n\n# Upsample minority class\ndata_minority_upsampled = resample(data_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples= data_majority.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled])\n \n# Display new class counts\nprint(\"After upsampling\\n\",data_upsampled.sentiment.value_counts(),sep = \"\")\n\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values) # training with whole data\n\nX_train = tokenizer.texts_to_sequences(data_upsampled['text'].values)\nX_train = pad_sequences(X_train,maxlen=29)\nY_train = pd.get_dummies(data_upsampled['sentiment']).values\nprint('x_train shape:',X_train.shape)\n\nX_test = tokenizer.texts_to_sequences(test['text'].values)\nX_test = pad_sequences(X_test,maxlen=29)\nY_test = pd.get_dummies(test['sentiment']).values\nprint(\"x_test shape\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ba3cf60-a83c-9c21-05e0-b14303027e93","_uuid":"05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda","trusted":true,"scrolled":true},"cell_type":"code","source":"# model\nembed_dim = 128\nlstm_out = 192\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a775979-a930-e627-2963-18557d7bf6e6","_uuid":"8799a667a2c0254cb367c193d86e07ee36d91dd7"},"cell_type":"markdown","source":"Lets define class weights as a dictionary, I have defined weight of majority class to be 1 and of minority class to be a multiple of 1/bias\n"},{"metadata":{"_cell_guid":"d5e499ac-2eba-6ff7-8d9a-ff65eb04099b","_uuid":"d0b239912cf67294a9f5af6883bb159c44318fc7","trusted":true,"scrolled":false},"cell_type":"code","source":"batch_size = 128\n# also adding weights\nclass_weights = {0: 1 ,\n                1: 1.6/bias }\nmodel.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n          class_weight=class_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2803c5699e0aec22463aadccd1255e63155c1b09"},"cell_type":"code","source":"Y_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\nprint(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c23e4d6b4cab5b46a58b4c2962abd7a0d29df42"},"cell_type":"markdown","source":"So the class imbalance is reduced significantly recall value for positive tweets (Class 1) improved from 0.54 to 0.74. It is always not possible to reduce it completely. \nYou may also noticed that the recall value for Negative tweets also decreased from 0.90 to 0.79  but this can be improved using training model to more epochs and tuning the hyper-parameters.\n"},{"metadata":{"trusted":true,"_uuid":"085294a5409b58c2188019177041ceb1756f1826"},"cell_type":"code","source":"# running model to few more epochs\nmodel.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n          class_weight=class_weights)\nY_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\nprint(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd","_uuid":"d9aac68e2013b3beffb6a764cc5b85be83073e66","trusted":true},"cell_type":"code","source":"twt = ['keep up the good work']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\nprint(twt)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53b88b4e6b56cf0e00084e3a41506d9abb5127e0"},"cell_type":"markdown","source":"1. ## Tuning the hyper-parameters using gridsearch"},{"metadata":{"trusted":true,"_uuid":"ee81d1776849d65f0f4f472be81b53a497e77795"},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from keras.models import Sequential\n# from keras.layers import Dense\n# from keras.wrappers.scikit_learn import KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8258932d80c8cd64308c919b5830bdb71a2fe2a"},"cell_type":"code","source":"# # Function to create model, required for KerasClassifier\n# def create_model(dropout_rate = 0.0):\n#     # create model\n#     embed_dim = 128\n#     lstm_out = 192\n#     model = Sequential()\n#     model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n#     model.add(SpatialDropout1D(dropout_rate))\n#     model.add(LSTM(lstm_out, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n#     model.add(Dense(2,activation='softmax'))\n#     model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n# #     print(model.summary())\n#     return model\n# # fix random seed for reproducibility\n# seed = 7\n# np.random.seed(seed)\n\n# model = KerasClassifier(build_fn=create_model, verbose=2,epochs=30, batch_size=128)\n# # define the grid search parameters\n# # batch_size = [128]\n# # epochs = [5]\n# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n# # class_weight = [{0: 1, 1: 1/bias},{0: 1, 1: 1.2/bias},{0: 1, 1: 1.5/bias},{0: 1, 1: 1.8/bias}]\n# param_grid = dict(dropout_rate = dropout_rate)\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# grid_result = grid.fit(X_train, Y_train)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}