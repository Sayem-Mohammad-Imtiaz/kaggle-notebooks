{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam Detection "},{"metadata":{},"cell_type":"markdown","source":"This is a simple solution of this problem for begginers to introduce data science principles for a easy classification problem. In this `Kernel` we use `Pandas Dataframe` as Data structure and `sklearn` for machine learning stuffs."},{"metadata":{},"cell_type":"markdown","source":"### A Data Science Framework\n\n\n\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n\n\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our model overfit, generalize, or underfit our dataset.\n\n\n7. **Optimize and Strategize:** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate.\n\n*Source : [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)*"},{"metadata":{},"cell_type":"markdown","source":"## Step1: Define the Problem\nIn this problem we want to find all spams using classification problems. There is some significant words in spams which can be used to find spams. "},{"metadata":{},"cell_type":"markdown","source":"## Step2: Gather the Data\nWe are using [this](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset from kaggle website. This is a collection of SMS messages tagged if they are `spam` or `ham`."},{"metadata":{},"cell_type":"markdown","source":"## Step3: Prepare the Data \nI think this step is the most step of these kind of problems. "},{"metadata":{},"cell_type":"markdown","source":"### Meet and Greet Data"},{"metadata":{},"cell_type":"markdown","source":"First we create a repository for our inputs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\")) # For kaggle kernel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As i said we use `Pandas Dataframe` as our data structure. so we import it and load `spam.csv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nDATASET_DIR = '../input'  # For kaggle kernel\n# DATASET_DIR = './datasets/spam'\nDATASET_NAME = 'spam.csv'\ndataset_path = os.path.join(DATASET_DIR, DATASET_NAME)\ndataset = pd.read_csv(dataset_path, encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split TrainSet and TestSet \nIt is important to split these two. You always should split test set at the beggining of your solution. At the end of solution you return to test set again and use it just for test!\n"},{"metadata":{},"cell_type":"markdown","source":"#### How to split these two ?\nWell there is some ways to do that. For example you can select a fraction of your dataset by index (aka. 0 to 1000 indices). Another way which used is random selection. But what if we want to run our thest again?? Well we can select a const random seed. I use `sklearn` for this which used random strategy. "},{"metadata":{},"cell_type":"markdown","source":"First we split labels from other datas"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.drop('v1', axis=1)\ny = dataset['v1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then Split them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method also shuffle datas. So you don't need to shuffle them. If you selected train and test sets manually you should shuffle them. Becasue it could be dependancy between datas in a sequence and it impacts on your classification."},{"metadata":{},"cell_type":"markdown","source":"### The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting"},{"metadata":{},"cell_type":"markdown","source":"#### Correcting and Completing\nWe should first review our data and see if there is not to be unaccaptable features and datas. Also there is null values in our data some algorithms can handle null values but it's better to handle these null values.\n\nFor this first see our data informations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is a lot's of null values in 3 last columns. We can recognize these columns are not usefull and we can drop them.**"},{"metadata":{},"cell_type":"markdown","source":"For this goal we write a sklearn transformer to use it with our pipelines in future."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameSelection(BaseEstimator, TransformerMixin):\n    def __init__(self, attrs):\n        self.attrs = attrs\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X[attrs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attrs = ['v2']\nselector = DataFrameSelection(attrs)\nX = selector.transform(X_train)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should also change labels to a numeric code. We use sklearn LabelEncoder for these purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.fit_transform(y_test)\ny_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating\nTo create new features that can help us to solve this problem we should first analys our data to find out important words. then we can use them to create a `sparse matrix` of existence of these words in message."},{"metadata":{},"cell_type":"markdown","source":"First let join X and y to can analys data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = X\nXy['status'] = y_train\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First See how much of them are spams\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.bar(('ham', 'spam'), (len(Xy[Xy['status'] == 0]),len(Xy[Xy['status'] == 1])), color='r')\nprob = len(Xy[Xy['status'] == 1])/len(Xy)\nprint(f\"prob of spam is {prob}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well there is a lots of ham data in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy.groupby('status').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We add length to find out if it is important or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LengthAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2'):\n        self.msg_attr = msg_attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        dataset['len'] = X[self.msg_attr].apply(len)\n        return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = LengthAdder().transform(Xy)\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy.hist(column='len', by='status', bins=25, figsize=(15, 7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we can recognize that length of msg can be usefull to find out spam messages."},{"metadata":{},"cell_type":"markdown","source":"##### Creating Sparse Matrix\nto find out spam messages there is a way to get all possible words in messages and create a sparse matrix of existence of word in message or not. Then train classifier on this sparse matrix.\n\nFor that we write a transformer which convert our messages to corresponding sparse matrix."},{"metadata":{},"cell_type":"markdown","source":"But before that we should add some hyperparamters. Like converting messages to lowercase, remove punctuation, replace all URLs with `URL`, replace all numbers with `NUMBER`, or even perform stemming."},{"metadata":{},"cell_type":"markdown","source":"#### To Lower Case \nif we want to compare words to each other they should all be in lower case. So lets transform it."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToLowerCase(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2'):\n        self.msg_attr = msg_attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        dataset[self.msg_attr] = dataset[self.msg_attr].str.lower()\n        return dataset\nXy = ToLowerCase().transform(Xy)\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Delete Stop Words and Punctuations\n**stop word** is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import WhitespaceTokenizer\nimport nltk \nclass DeleteStopWordsAndPunc(BaseEstimator, TransformerMixin):\n    def __init__(self, lang='english', msg_attr='v2'):\n        self.lang = lang\n        self.msg_attr = msg_attr\n        nltk.download('stopwords')\n        self.stop_words = set(stopwords.words('english')) \n        self.punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~+~'''\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        dataset[self.msg_attr] = dataset[self.msg_attr].apply(self.delete_stop_words)\n        dataset[self.msg_attr] = dataset[self.msg_attr].apply(self.delete_punc)\n        return dataset\n    def delete_stop_words(self, msg):\n        tokens = WhitespaceTokenizer().tokenize(msg)\n        return ' '.join([w for w in tokens if w not in self.stop_words])\n    def delete_punc(self, msg):\n        no_punct = \"\"\n        for char in msg:\n            if char not in self.punctuations:\n                no_punct = no_punct + char\n        return no_punct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok. Now let see dataframe **before** this transform :"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And **after** :"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = DeleteStopWordsAndPunc().transform(Xy)\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stemming Words\nStemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.\n\nLast part of cleaning messages are stemming them. with stemming them we improve chance of finding apearance of a word in messages. To stemming words we can use `nltk` library. It provide us usefull tools to stemming words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nclass Stemmer(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2'):\n        self.msg_attr = msg_attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        dataset[self.msg_attr] = dataset[self.msg_attr].apply(self.stemming)\n        return dataset\n    def stemming(self, msg):\n        return ' '.join([PorterStemmer().stem(w) for w in WhitespaceTokenizer().tokenize(msg)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = Stemmer().transform(Xy)\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change Numbers and Urls\nIn this section we change numbers to string `NUMBER` and change urls to `URL`. Because existence of url or number is important for us. Not that number or url itself.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re #regex\nclass ChangeNumbersAndUrls(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2'):\n        self.msg_attr = msg_attr\n        self.url_regex = re.compile(\n                    r'^(?:http|ftp)s?://' # http:// or https://\n                    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n                    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n                    r'(?::\\d+)?' # optional port\n                    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        dataset[self.msg_attr] = dataset[self.msg_attr].apply(self.change_urls_numbers)\n        return dataset\n    def change_urls_numbers(self, msg):\n        tokens = WhitespaceTokenizer().tokenize(msg)\n        new_msg = []\n        for w in tokens:\n            if w.isnumeric() :\n                new_msg.append('NUMBER')\n            elif re.match(self.url_regex, w):\n                new_msg.append('URL')\n            else : new_msg.append(w)\n        return ' '.join(new_msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = ChangeNumbersAndUrls().transform(Xy)\nXy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting \nLast, but certainly not least, we'll deal with formatting. For this problem we said that we want to make a matrix of existence of every word in message. \n\n* First we should find all seperate words\n* Then fill all words existence in matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CreateExistenceMatrix(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2', len_index=2):\n        self.word_set = set({})\n        self.msg_attr = msg_attr\n        self.len_index = len_index\n    def fit(self, X, y=None):\n        for m in X[self.msg_attr]: \n            for w in WhitespaceTokenizer().tokenize(m):\n                self.word_set.add(w)\n        return self\n    def transform(self, X, y=None):\n        dataset = X.copy()\n        for word in self.word_set:\n            dataset[word] = 0\n        indices = dataset.columns[self.len_index:]\n        for index, row in dataset.iterrows():\n            tokens = WhitespaceTokenizer().tokenize(row[self.msg_attr])\n            for word in tokens:\n                if word in dataset.columns:\n                    dataset.at[index, word] = dataset.at[index, word] + 1\n        return dataset\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xy = CreateExistenceMatrix(len_index=3).fit_transform(Xy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conver to Sparse Matrix \nSparse matrix just contains vlues which is not zero. And there is lots of algorithms which support sparse matrix. So we can have a very faster way to classify our big matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\nclass ToSparseMatrix(BaseEstimator, TransformerMixin):\n    def __init__(self, msg_attr='v2'):\n        self.msg_attr = msg_attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return csr_matrix(X.drop(self.msg_attr, axis=1).values)\nXy = ToSparseMatrix().transform(Xy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Pipeline\nsklearn pipelines help us to set a pipeline for all of our process. Here we make a pipeline to process all we did to here in a single pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nattrs = ['v2']\nfull_pipeline = Pipeline([\n    ('selector', DataFrameSelection(attrs)),\n    ('length_adder', LengthAdder()),\n    ('to_lower', ToLowerCase()),\n    ('delete_stop_punc', DeleteStopWordsAndPunc()),\n    ('stemming', Stemmer()),\n    ('change_number_url', ChangeNumbersAndUrls()),\n    ('create_existance', CreateExistenceMatrix()),\n    ('to_sparse', ToSparseMatrix()),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sparse = full_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\nNow lets do the magic. Here we select some classificaiton models. For this i use RandomForest model from tree models, SGD model from linear models and some Navie Bayes modles. We examine all models and select some of them to optimise and make them better. \n"},{"metadata":{},"cell_type":"markdown","source":"Our error scoring use precision, recall and f1_score. If you don't know what they are you can find usefull informations [here](https://en.wikipedia.org/wiki/Precision_and_recall). \n\nFirst we define all of our models in an array. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nmodels = [\n    ('svc', SVC(kernel='rbf')),\n    ('neighbors', KNeighborsClassifier(3)),\n    ('random_forest', RandomForestClassifier()),\n    ('sgd', SGDClassifier()), \n    ('mutlinomial_nb', MultinomialNB()),\n    ('complement_nb', ComplementNB()),\n    ('bernoli_nb', BernoulliNB()),\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then compute every score for all of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score, recall_score, precision_score, precision_recall_curve, roc_curve\nscores = pd.DataFrame([], columns=['model_name', 'f1', 'precision', 'recall'])\nfor model in models:\n    predictions = cross_val_predict(model[1], X_train_sparse, y_train, cv=3, n_jobs=-1)\n    scores = scores.append({\n        'model_name':model[0],\n        'f1': f1_score(y_train, predictions),\n        'precision': precision_score(y_train, predictions),\n        'recall': recall_score(y_train, predictions),\n    }, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimise and Strategize \nNow we should see what is important for us. Well in spam problem it is important to block all spams and recognize not spam messages. But we can say it is more important to block all spams. So we can say `precision` and `recall` are both important. but `recall` is more important than `precision`.\n\nSo if model have a low f1 it is not good for us. it also should have a recall more than 90. \n\nOk. We select `multinomial navie bayes`. Also we can select `random forest` because it has a high precision and we can sacrifise it to get more recall. "},{"metadata":{},"cell_type":"markdown","source":"### Try to Select Best Model\nWe know we can change precision to have better recall. For that we see all possible probablity thresholds for every model to select best threshold. \n\nSo we plot recall precisions with various thresholds."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_plot_precision_recall(precision, recall, threshold, model_name, ax):\n    ax.plot(threshold, precision[:-1], \"b--\", label=\"Precision\")\n    ax.plot(threshold, recall[:-1], \"r-\", label=\"Recall\")\n    ax.set_xlabel(\"Threshold\")\n    ax.set_ylabel(model_name)\n    ax.legend(loc=\"lower right\")\n    ax.set_ylim([0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_models_score_plot(models):\n    i = 1\n    fig = plt.figure(figsize=(20, 15))\n    fig.subplots_adjust(hspace=0.4)\n    for model in models:\n        ax = fig.add_subplot(3, 3, i)\n        i+= 1\n        y_scores = []\n        if hasattr(model[1], \"decision_function\"):\n            y_scores = cross_val_predict(model[1], X_train_sparse, y_train, cv=3, method=\"decision_function\", n_jobs=-1)\n        else :\n            y_probs = cross_val_predict(model[1], X_train_sparse, y_train, cv=3, method=\"predict_proba\", n_jobs=-1)\n            y_scores = y_probs[:, -1]\n        precision, recall, thresholds = precision_recall_curve(y_train, y_scores)\n        show_plot_precision_recall(precision, recall, thresholds, model[0], ax)\ndraw_models_score_plot(models) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFrom these plots we recognize we should select one of navie bayes.\n\nThese kind of models have an aplha parameters which is very important. So let select three of them."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"models = [\n    ('mutlinomial_nb_0.2', MultinomialNB(alpha=0.2)),\n    ('mutlinomial_nb_0.5', MultinomialNB(alpha=0.5)),\n    ('mutlinomial_nb_1', MultinomialNB(alpha=1.0)),\n    ('complement_nb_0.2', ComplementNB(alpha=0.2)),\n    ('complement_nb_0.5', ComplementNB(alpha=0.5)),\n    ('complement_nb_1', ComplementNB(alpha=1)),\n    ('bernoli_nb_0.2', BernoulliNB(alpha=0.2)),\n    ('bernoli_nb_0.5', BernoulliNB(alpha=0.5)),\n    ('bernoli_nb_1', BernoulliNB(alpha=1)),\n]\ndraw_models_score_plot(models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see we can select a bernoli navie bayes with low alpha to have great score in both recall and precision. "},{"metadata":{},"cell_type":"markdown","source":"### Try to Select Best HyperParameters \nFor this goal we use grid search. It means try some possible hyperparameters of model but using sklearn tools. \n\nWe use recall scoring. because recall is more important for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nestimator = BernoulliNB()\ngrid_params = {\n    'alpha': [0, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2],\n    'fit_prior': [True, False],\n}\ngrid_search = GridSearchCV(estimator, grid_params, scoring='recall')\ngrid_search.fit(X_train_sparse, y_train)\ngrid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = grid_search.best_estimator_\nbest_estimator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets select a good threshold to have both good recall and precision. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_probs = cross_val_predict(best_estimator, X_train_sparse, y_train, cv=3, method=\"predict_proba\", n_jobs=-1)\ny_scores = y_probs[:, -1]\nprecision, recall, thresholds = precision_recall_curve(y_train, y_scores)\nax = plt.axes()\nshow_plot_precision_recall(precision, recall, thresholds, \"Bernoli NB\", ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well we can select threshold now. I select 0.4 threshold. Because it have a better recall and good precision. We should see what we need. Here recall is more important for me."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(estimator, X, threshold=0.5):\n    y_probs = estimator.predict_proba(X)[:,1]\n    return (y_probs >= threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_transformed = full_pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = predict(best_estimator, X_test_transformed, 0.4)\nrecall = recall_score(y_test, test_predictions)\nprecision = precision_score(y_test, test_predictions)\nf1 = f1_score(y_test, test_predictions)\nprint(f'Precision is {precision}\\nRecall is {recall}\\nF1 Score is {f1}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}