{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n\n## FE & EDA with 3D and abnormals filtering of Used Cars Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Technology for feature importance building: xgb, lgbm, linreg","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1 Import libararies","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\n\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\n\npd.set_option('max_columns',100)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Download Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')\ncar.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Preparing for analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(car)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Remove unnecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_columns = ['url', 'region', 'region_url', 'title_status', 'vin', 'size', 'image_url','description', 'lat','long', 'paint_color']\n\ncar = car.drop(columns = drop_columns,inplace=True)\ncar = car.dropna()\ncar.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = car.columns.values.tolist()\nfor col in features:\n    if car[col].dtype in numerics: continue\n    categorical_columns.append(col)\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in car.columns:\n        le = LabelEncoder()\n        le.fit(list(car[col].astype(str).values))\n        car[col] = le.transform(list(car[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car = reduce_mem_usage(car)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    \n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplotting_3_chart(car, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection=\"3d\")\n\nz_points = car['price']\nx_points = car['odometer']\ny_points = car['year']\nax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');\n\nax.set_xlabel('odometer')\nax.set_ylabel('year')\nax.set_zlabel('price')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\ny = np.array(car.price)\nplt.subplot(131)\nplt.plot(range(len(y)),y,'.');plt.ylabel('price');plt.xlabel('index');\nplt.subplot(132)\nsns.boxplot(y=car.price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check percentiles 5%, 10% and 90%, 95%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car_stat = car.describe(percentiles = [.05,.1, .9,.95])\ncar_stat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting the conditions for effective filter of abnormal values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car_stat.loc['max',:]-car_stat.loc['95%',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_stat.loc['95%',:]-car_stat.loc['90%',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(car_stat.loc['max',:]-car_stat.loc['95%',:])/(car_stat.loc['95%',:]-car_stat.loc['90%',:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_stat.loc['10%',:]-car_stat.loc['5%',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.loc['5%',:]-df_stat.loc['min',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(car_stat.loc['5%',:]-car_stat.loc['min',:])/(car_stat.loc['10%',:]-car_stat.loc['5%',:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I suggest to make filters only for features that were numeric in the basic dateset:\nprice, year, odometer\n\nThere are problems:\n\n**- smallest values: year**\n\n**- largest values: price, odometer**\n\n\nI suggest adding a **filter by low values of price** because free cars and cars which cost as scrap require other models\n\nSet the filter by the quantile values of 10% and 95% for abnormal smallest and largest values, respectively","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"car_stat.loc[['10%','90%','95%'],:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vbmokin\ndef abnormal_filter(df, threshold_first, threshold_second):\n    # Abnormal values filter for DataFrame df:\n    # threshold_first (5%-min or max-95%)\n    # threshold_second (second diff., times)\n    df_describe = df.describe([.05, .1, .9, .95])\n    cols = df_describe.columns.tolist()\n    i = 0\n    abnorm = 0\n    for col in cols:\n        i += 1\n        # abnormal smallest\n        P10_5 = df_describe.loc['10%',col]-df_describe.loc['5%',col]\n        P_max_min = df_describe.loc['max',col]-df_describe.loc['min',col]\n        if P10_5 != 0:\n            if (df_describe.loc['5%',col]-df_describe.loc['min',col])/P10_5 > threshold_second:\n                #abnormal smallest filter\n                df = df[(df[col] >= df_describe.loc['5%',col])]\n                print('1: ', i, col, df_describe.loc['min',col],df_describe.loc['5%',col],df_describe.loc['10%',col])\n                abnorm += 1\n        else:\n            if P_max_min > 0:\n                if (df_describe.loc['5%',col]-df_describe.loc['min',col])/P_max_min > threshold_first:\n                    # abnormal smallest filter\n                    df = df[(df[col] >= df_describe.loc['5%',col])]\n                    print('2: ', i, col, df_describe.loc['min',col],df_describe.loc['5%',col],df_describe.loc['max',col])\n                    abnorm += 1\n\n        \n        # abnormal biggest\n        P95_90 = df_describe.loc['95%',col]-df_describe.loc['90%',col]\n        if P95_90 != 0:\n            if (df_describe.loc['max',col]-df_describe.loc['95%',col])/P95_90 > threshold_second:\n                #abnormal biggest filter\n                df = df[(df[col] <= df_describe.loc['95%',col])]\n                print('3: ', i, col, df_describe.loc['90%',col],df_describe.loc['95%',col],df_describe.loc['max',col])\n                abnorm += 1\n        else:\n            if P_max_min > 0:\n                if ((df_describe.loc['max',col]-df_describe.loc['95%',col])/P_max_min > threshold_first) & (df_describe.loc['95%',col] > 0):\n                    # abnormal biggest filter\n                    df = df[(df[col] <= df_describe.loc['95%',col])]\n                    print('4: ', i, col, df_describe.loc['min',col],df_describe.loc['95%',col],df_describe.loc['max',col])\n                    abnorm += 1\n    print('Number of abnormal values =', abnorm)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car = abnormal_filter(car, 0.5, 3)\ncar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add filter: train['price'] >= 1700\ncar = car[car['price'] >= 1700]\ncar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_3_chart(car, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nax = plt.axes(projection=\"3d\")\n\nz_points = car['price']\nx_points = car['odometer']\ny_points = car['year']\nax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');\n\nax.set_xlabel('odometer')\nax.set_ylabel('year')\nax.set_zlabel('price')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = car['price']\ndel car['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5 Feature Engineering : tuning models and building the feature importance diagrams","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 LGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = car\nz = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(Zval, modelL.predict(Xval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 XGB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(Zval, modelx.predict(data_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization for regression model\ntrain = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns,\n    index=train.index\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train, target)\ncoeff_linreg = pd.DataFrame(train.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6 Comparison of the all feature importance diagrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values('mean', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.35*feature_score['score_xgb'] + 0.15*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values('total', ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}