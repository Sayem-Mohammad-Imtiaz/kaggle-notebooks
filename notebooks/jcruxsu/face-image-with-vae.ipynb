{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as tt\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba'\nbatch_size=16\nIMAGES_COUNT = 20000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nc = 3\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_file_list = os.listdir(train_path)\nimg_file_file = [os.path.join(train_path,one) for one in img_file_list]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.zeros((IMAGES_COUNT,3,64,64,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfms = tt.Compose([tt.RandomHorizontalFlip(), \n                          tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                          tt.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,pic_file  in tqdm(enumerate(img_file_file[:IMAGES_COUNT])):\n    train_x[i,:,:,:] = train_tfms(Image.open(pic_file).resize((64, 64)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DataLoader(train_x,batch_size, shuffle=True,num_workers=3, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_batch(dl):\n    for images in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:64], nrow=4).permute(1, 2, 0).clamp(0,1))\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(train_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_size = 4\nstride = 1\npadding = 0\ninit_kernel = 16\n\nclass VAE(nn.Module):\n    def __init__(self,device):\n        super(VAE, self).__init__()\n \n        # encoder\n        self.enc1 = nn.Conv2d(\n            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.enc2 = nn.Conv2d(\n            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.enc3 = nn.Conv2d(\n            in_channels=init_kernel*2, out_channels=init_kernel*4, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.enc4 = nn.Conv2d(\n            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.enc5 = nn.Conv2d(\n            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n\n        # decoder \n        self.dec1 = nn.ConvTranspose2d(\n            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.dec2 = nn.ConvTranspose2d(\n            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.dec3 = nn.ConvTranspose2d(\n            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.dec4 = nn.ConvTranspose2d(\n            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n        self.dec5 = nn.ConvTranspose2d(\n            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n            stride=stride, padding=padding\n        )\n\n    def reparameterize(self, mu, log_var):\n        \"\"\"\n        :param mu: mean from the encoder's latent space\n        :param log_var: log variance from the encoder's latent space\n        \"\"\"\n        std = torch.exp(0.5*log_var) # standard deviation\n        eps = torch.randn_like(std) # `randn_like` as we need the same size\n        sample = mu + (eps * std) # sampling\n        return sample\n \n    def forward(self, x):\n        # encoding\n        x = F.relu(self.enc1(x))\n        x = F.relu(self.enc2(x))\n        x = F.relu(self.enc3(x))\n        x = F.relu(self.enc4(x))\n        x = self.enc5(x)\n\n        # get `mu` and `log_var`\n        mu = x\n        log_var = x\n\n        # get the latent vector through reparameterization\n        z = self.reparameterize(mu, log_var)\n \n        # decoding\n        x = F.relu(self.dec1(z))\n        x = F.relu(self.dec2(x))\n        x = F.relu(self.dec3(x))\n        x = F.relu(self.dec4(x))\n        reconstruction = torch.sigmoid(self.dec5(x))\n        return reconstruction, mu, log_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae = VAE(device).double().to(device)\noptimizer = torch.optim.Adam(params=vae.parameters(), lr=0.0001, weight_decay=1e-5)\ndef vae_loss(recon_x,x,mu,log_var):\n    recon_loss = F.binary_cross_entropy(recon_x.view(-1,3,64,64).to(device), x.view(-1, 3,64,64).to(device), reduction='sum')\n    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return recon_loss + kl_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.utils import save_image\n\ndef save_fake_images(sample_vectors,model,name):\n    fake_images = model(sample_vectors)[0]\n    fake_images = fake_images.reshape(fake_images.size(0), 3, 64, 64).to(device)\n    fake_fname = name+'fake_images.png'\n    print('Saving', fake_fname)\n    save_image(fake_images, os.path.join('./', fake_fname), nrow=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(model, dataloader,epochs):\n    model.train()\n    running_loss = 0.0\n    for epoch in range(0,epochs): \n        print(f\"Epoch {epoch+1}\")\n        for i,data in enumerate(dataloader): \n            optimizer.zero_grad()\n            reconstruction, mu, logvar = model(data.double().to(device))\n            loss = vae_loss(reconstruction,data, mu, logvar)\n            loss.backward()\n            running_loss += loss.item()\n            optimizer.step()\n        if epoch%100==0:\n            save_fake_images(reconstruction,vae,str(epoch))\n        train_loss = running_loss/len(dataloader.dataset)\n        print(f\"Train Loss: {train_loss:.4f}\")\n        \n            \n    return train_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(vae.to(device),train_dl,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_res_batch(images):\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images[:16], nrow=4).permute(1, 2, 0).clamp(0,1).detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_input = torch.randn([16,3,64,64]).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_noise = vae.forward(r_input.double())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show_res_batch(res_noise[0])\nsave_fake_images(res_noise[0],vae,'fake')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}