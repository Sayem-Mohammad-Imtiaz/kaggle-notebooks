{"cells":[{"metadata":{"id":"m1TO5VPgqgT0"},"cell_type":"markdown","source":"Hi Everone,\n\nThis is my first notebook on Kaggle. Please feel free to suggest me or correct me if any changes needs to be done to improve myself.\n\n** Please upvote if you like the notebook **\n\n\n\n\n# Linear Regression Intuition\nBefore we dive into the actual technique of Linear Regression, lets look at some intuition of it.\n\nLet’s say, I give you the following puzzle:\n\nGiven the following values of X and Y, what is the value of Y when X = 5.\n\n(1,1), (2,2), (4,4), (100,100), (20, 20)\n\nThe answer is : 5. Not very difficult, right?\n\nNow, let’s take a look at different example. Say you have the following pairs of X and Y. Can you calculate the value of Y, when X = 5?\n\n(1,1), (2,4), (4,16), (100,10000), (20, 400)\n\nThe answer is : 25. Was it difficult?\n\nLet’s understand a bit as to what happened in the above examples.\n\nWhen we look at the first example, after looking at the given pairs, one can establish that the relationship between X and Y is Y = X.\n\nSimilarly, in the second example, the relationship is Y = X*X.\n\nIn these two examples, we can determine the relationship between two given variables (X and Y) because we could easily identify the relationship between them. Overall, machine learning works in the same way.\n\nYour computer looks at some examples and then tries to identify “the most suitable” relationship between the sets X and Y. Using this identified relationship, it will try to predict (or more) for new examples for which you don’t know Y.\n\nKeeping the above idea in mind, I will try to explain what is linear regression.","execution_count":null},{"metadata":{"id":"KEGqRRKyq3Ze"},"cell_type":"markdown","source":"# Regression\nRegression is usually termed as determining relationship(s) between two or more variables.\n\nFor example, in the above two examples, X and Y are the variables. X is termed as the independent variable and Y is termed as the dependent variable (because its value is calculated using X). Also, Y has a continous range (unlike classification where Y is discrete).","execution_count":null},{"metadata":{"id":"ztZSk3m_q54I"},"cell_type":"markdown","source":"# Linear Regression/ Simple Linear Regression\nSimple Linear Regression (SLR) is termed as simple because there is only independent variable.\n\nSuppose we have a dataset which contains information about relationship between 'Years of experience' and 'Salary' in a particular work field.\n\nThe dependent variable could represent salary. You could assume that level of experience will impact salary. So, you would label the independent variable as experience.\n\nRepresenting the experience with variable x and salary with y, we can say that y ∝ x. ( y is proportional to x).\n\nHowever, a change in x does not usually mean an equal change in y.\n\nThe coefficient can be thought of as a multiplier that connects the independent and dependent variables. It translates how much y will be affected by a unit change in x.\n\nLet's add a coefficient b1 to our example.\n\nWe thus get y = b1*x\n\nNow the salaries in a particular job always start with the base amount or the lowest possible salary. We thus need to take that constant in consideration.\n\nThe constant b0 would be the starting salary for someone with a zero level of experience. Assuming every fresher in the company gets 30K as starting salary we can set it as the base price.\n\nFinally, we have:\n\nSalary = b0 + b1* Experience\n\nor\n\ny = b0 + b1*x\n\nThis is similar to the equation of a straight line y= m*x +c\n\nEquation\n\nLet's bring our theory into practice now!\n\n# Objective\nThe objective is to use linear regression to understand how years of experience impact Salary.","execution_count":null},{"metadata":{"id":"oMNLfHGztBfY","trusted":true},"cell_type":"code","source":"# importing packages\nimport numpy as np # to perform calculations \nimport pandas as pd # to read data\nimport matplotlib.pyplot as plt # to visualise","execution_count":null,"outputs":[]},{"metadata":{"id":"hFP-urpMnz4q","trusted":true},"cell_type":"code","source":"#Loading the salary dataset\ndata = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Linear_Regression_Introduction/master/Salary_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"zc04FRP-n5mS","outputId":"6b41bee6-eee2-4f1c-989d-e82dd52b8949","trusted":true},"cell_type":"code","source":"#Let's have a look at what our data is like\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"P0F2ae4xoFkL"},"cell_type":"markdown","source":"# Plotting the Data\nLet’s plot our data points on a 2-D graph to eyeball our dataset(get a rough overview) and see if we can manually find any relationship between the data.","execution_count":null},{"metadata":{"id":"Wx8dAJ4Vn_hS","outputId":"9a845c93-d4f9-429e-e81c-e51ffefde74b","trusted":true},"cell_type":"code","source":"# Scatter plot helps in visualising the data distribution\nplt.plot(data.YearsExperience, data.Salary,'rx')","execution_count":null,"outputs":[]},{"metadata":{"id":"kbG1f6XBoYdO"},"cell_type":"markdown","source":"As you can see, there is a clear relationship between the years of experience and salary.\n\n# Setting variables\nOur next step is to divide the data into “attributes” and “labels” or as you've already known as input and target variables.\n\nIn our dataset, we only have two columns. We want to predict the Salary depending upon the Years of Experience recorded. Therefore our attribute set will consist of the “YearsExperience” column which is stored in the X variable, and the label will be the “Salary” column which is stored in y variable.","execution_count":null},{"metadata":{"id":"phpkltbyoSB1","trusted":true},"cell_type":"code","source":"X = data[['YearsExperience']]\ny = data['Salary']","execution_count":null,"outputs":[]},{"metadata":{"id":"azBYqxsnrnBU"},"cell_type":"markdown","source":"If you are wondering why a capital X is used for features, and lowercase y for labels, it is mainly due to convention.\n\n# Splitting the data\nNext, we split 80% of the data to the training set while 20% of the data to test set using below code. The test_size variable is where we actually specify the proportion of the test set.\n\nBy passing our X and y variables into the train_test_split method, we are able to capture the splits in data by assigning 4 variables to the result.","execution_count":null},{"metadata":{"id":"TqhI6Fy4pUWo","trusted":true},"cell_type":"code","source":"# import SK Learn train test split\nfrom sklearn.model_selection import train_test_split \n\n# Assign variables to capture train test split output\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=47)","execution_count":null,"outputs":[]},{"metadata":{"id":"8VaOpoeOr-Jt"},"cell_type":"markdown","source":"# Understanding the working of Linear Regression\nThe term “linearity” in algebra refers to a linear relationship between two or more variables. If we draw this relationship in a two-dimensional space (between two variables), we get a straight line.\n\nIf we plot the independent variable (x) on the x-axis and dependent variable (y) on the y-axis, linear regression gives us a straight line that \"best fits\" the data points.It’s impossible to connect all the marks with a straight line, so you use a best fitting line.\n\nThe equation for this line would be the result of your simple linear regression(Remember the equation y= b0 + b1*x that we just derived?). The regression finds the best fitting line.\n\nNow, how do you find the best fitting line? Since our data points(values of x and y) will remain constant for a particular dataset, we can only alter b0 and b1.\n\nOur objective is to find the values of b0 and b1 that will best fit this data.\n\nThese 2 variables/coefficients are actually called hyperparameters. In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. And we must always try to find some optimal parameters while building a machine learning model.\n\nThis line is your regression model.\n\nTo perform Linear Regression quickly, we will be using the library scikit-learn. If you don’t have it already you can install it using pip:\n\npip install scikit-learn","execution_count":null},{"metadata":{"id":"5cUcaGaVshBX"},"cell_type":"markdown","source":"# Training our model\nAfter splitting the data into training and testing sets, finally, the time is to train our algorithm. Firstly, importing of sklearn.linear_model is required for us to access LinearRegression. It then needs to be instantiated and model fit to our training data. This is seen below.","execution_count":null},{"metadata":{"id":"P6Q0Am6LsnAx","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"id":"e9u4iL5Ksr7b","outputId":"2ecb1989-f080-46cc-d718-65343fa2b9cc","trusted":true},"cell_type":"code","source":"linear_regressor = LinearRegression()  # create object for the class\n\n#fit model to our training data i.e learn coefficients\nlinear_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"wXjYDeAGs08A"},"cell_type":"markdown","source":"# Interpreting Coefficients\nThe coefficients(b0 and b1) will allow us to model our equation with values and find the best fit line. The linear_regressor variable (assigned to a LinearRegression object), is able to have the intercept and coefficients extracted, using the code below.","execution_count":null},{"metadata":{"id":"PHQIX8jbss4C","outputId":"c4e09a27-07b6-47e2-8a1a-c39de4f1f656","trusted":true},"cell_type":"code","source":"# prints y-intercept\nprint(linear_regressor.intercept_)\n\n# prints the coefficient\nprint(linear_regressor.coef_)","execution_count":null,"outputs":[]},{"metadata":{"id":"AbtMCQv0tDQL"},"cell_type":"markdown","source":"The intercept will be your b0 value; and coefficient will be b1.\n\n# Making predictions based on your model\nNow that we have trained our algorithm, it’s time to make some predictions. To do so, we will use our test data and see how accurately our algorithm predicts the salaries.\n\nMaking predictions based on your model is as simple as using the code below: passing the predict method your test data. This will return predicted values of y given the new test X data.","execution_count":null},{"metadata":{"id":"AMXH5cPUs-FL","trusted":true},"cell_type":"code","source":"y_pred = linear_regressor.predict(X_test)  # make predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"kfKRCJXvtWeb"},"cell_type":"markdown","source":"We have our predictions in y_pred. Now lets visualize the data set and the regression line with the test data:","execution_count":null},{"metadata":{"id":"FC65ZhC6tQFf","outputId":"a72082ac-4f6a-400a-bb95-7ede1c968403","trusted":true},"cell_type":"code","source":"plt.plot(X_test, y_test,'rx')\nplt.plot(X_test, y_pred, color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1Osl6CYGtkh3"},"cell_type":"markdown","source":"As you can see, the algorithm has drawn a line that passes through the maximum test data points and has the minimum distance from the others. This line is known as the \"best-fit\" or the regression line.\n\nSince this line has a positive slope, we can say that the salary increases as no. of years of experience increase.\n\nUsing this line, you can even compute the salaries for the years of experience not present in the dataset by finding the corresponding value of y on the line","execution_count":null},{"metadata":{"id":"TQnuH1gYtxxM"},"cell_type":"markdown","source":"# Model Evaluation\nThere are three primary metrics used to evaluate linear models. These are: Mean absolute error (MAE), Mean squared error (MSE), or Root mean squared error (RMSE).","execution_count":null},{"metadata":{"id":"uybnQFu4t0Uo","outputId":"fc854cdb-855a-4316-9c2e-94eff79750bc","trusted":true},"cell_type":"code","source":"# import metrics library\nfrom sklearn import metrics\n\n# print result of MAE\nprint(metrics.mean_absolute_error(y_test, y_pred))\n\n#print result of MSE\nprint(metrics.mean_squared_error(y_test, y_pred))\n\n#print result of RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference - DPhi Bootcamp","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}