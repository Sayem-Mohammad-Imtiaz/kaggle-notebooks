{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport warnings\nimport re\nimport string\nwarnings.filterwarnings(\"ignore\")\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop= stopwords.words('english')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemm= WordNetLemmatizer()\nfrom textblob import TextBlob\nimport bs4\nimport requests\nfrom spacy import displacy\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#using pandas to read the train and test file \ntrain= pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv', encoding=\"ISO-8859-1\")\ntest=pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv', encoding=\"ISO-8859-1\")\ndf= pd.concat([train,test])\nprint(\"Train data frame shape:\",train.shape)\nprint(\"Test data frame shape:\",test.shape)\nprint(\"Complete data frame shape:\",df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"label\"].value_counts() #we have an imbalance class problem ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning"},{"metadata":{},"cell_type":"markdown","source":"I am going cover text cleaning teachniques one by one in detail"},{"metadata":{},"cell_type":"markdown","source":"The first step of text analytics is to clean text from noises. You may be thinking what is noise in text. A simple definition of noise is anything that will be not useful in our analysis is noise. If I try to classify noise then it can be of 4 types:\n1. Common Entities- Things like stopwords(is, the etc.), URLs, Hashtags, Punctuations, Numbers etc.\n2. Slangs- Commonly used words that are not part of dictionaries\n3. Grammatical and spelling errors\n4. Keyword Variations\n\nText cleaning not only helps us get rid of noise or repetitive information but also reduces dimension of data and makes machine learning model simpler.\nWe will now remove noise one by one. I will show you how you can clean text at once by creating user defined functions."},{"metadata":{},"cell_type":"markdown","source":"**Maintain uniformity**\n1. *Fixing encoding*- If you would have noticed we had already imported and converted data in 'ISO-8859-1' because we are dealing with tweets here. Each language has its own encoding such as 'ASCII' for English, 'BIG5' for Chinese and 'LATIN' for West Europe. It is always a good practice to convert them into a standard and unique format. One of the commonly used format is 'UTF8'.\n2. *Change casing*- To maintain uniformity, it always advisable to convert the text into lower case. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet']= df[\"tweet\"].str.lower()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Removal of HTML Noise/characters- \n\nIf you are extracting from web, it might contain some HTML noise or tags such <,> etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet']=df['tweet'].str.replace(\"<[^<]+?>\",\"\",regex=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Contractions\n\nBecause of character limit in Twitter, people often use contracted form of word to fit more characters. I have managed to get a dictionary of such word. Let's look up and normalize"},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"i'd\": \"i had / i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i shall / I will\",\n\"i'll've\": \"i shall have / i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",  \n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncont_re = re.compile('(%s)' % '|'.join(contractions.keys()))\ndef expand(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return cont_re.sub(replace, s)\ndf['tweet'] =df['tweet'].apply(expand)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Removal of URLs, Hashtag, Mentions-\n\nURLs, punctuations, new line characters etc. are very common in text data. If we doing any analysis, it is rare to have imporatnce of these characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove http and url\ndf['tweet']=df['tweet'].str.replace('https?://\\S+|www\\.\\S+', '',regex=True)\n#Remove punctuations and \ndf['tweet']=df['tweet'].str.replace('[%s]' % re.escape(string.punctuation), '', regex=True)\n#lets remove new line characters if any\ndf['tweet']=df['tweet'].str.replace('\\n', '', regex=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Remove Emojis\n\nEmojis are common in tweets or any social media data. We can remove them or analyze separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"#for this analysis, I am removing the emojis\ndef emojis(text):\n    emoji = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji.sub(r'', text)\n\ndf['tweet']=df['tweet'].apply(lambda x: emojis(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Stopwords Removal\n\nStopwords are common and frequently used words in sentence. They add very little value to the analysis and increase dimensionality of the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet'].apply(lambda x: [item for item in x if item not in stop])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. Normalization (Lemmatization)\n\nInflectional form of word such better, best for good are often found in text data. They increases the dimensionality of data and little value "},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(df):\n    df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9. Additional Repetitive word\n\nWe know from the problem statement that names have been replaced by word \"user\". Therefore it will right exclude them as well "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet']=df['tweet'].str.replace('user', '',regex=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Restricting graph to 5000 tweets only\ntweets=df['tweet'][:5000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the term of semantic web, a sentence consist of subject, predicate and object. It is also known as triples. With below code, we going to ectract subject and object from sentences and form a knowledge graph. If you want to read more about the below codes \nplease click attached [link](https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/)- An article by Prateek Joshi"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n\n\n  return [ent1.strip(), ent2.strip()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entity_pairs = []\n\nfor i in tqdm(tweets):\n  entity_pairs.append(get_entities(i))\n\nentity_pairs[10:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to subject and object, let's extract predicates as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predicates(sent):\n    doc = nlp(sent)\n    # Matcher class object \n    matcher = Matcher(nlp.vocab)\n\n    #define the pattern \n    pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n    matcher.add(\"matching_1\", None, pattern) \n\n    matches = matcher(doc)\n    k = len(matches) - 1\n\n    span = doc[matches[k][1]:matches[k][2]] \n\n    return(span.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicates = [predicates(i) for i in tqdm(tweets)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(predicates).value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now create a data frame with subject predicate and object"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract subject\nsubject = [i[0] for i in entity_pairs]\n\n# extract object\nobject1 = [i[1] for i in entity_pairs]\n\ngraph_df = pd.DataFrame({'subject':subject,  'predicate':predicates, 'object':object1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now create a multi directed knowledge graph using networkx library. If you want to read more about the library, please click the below link\n\nhttps://networkx.github.io/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a directed-graph from a dataframe\ngraph=nx.from_pandas_edgelist(graph_df, \"subject\", \"object\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets plot it using plotly\nplt.figure(figsize=(12,12))\n\npos = nx.spring_layout(graph)\nnx.draw(graph, with_labels=True, node_color='Cyan', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's perform some graph analytics and what people are reffering to as \"love\" in the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"graph=nx.from_pandas_edgelist(graph_df[graph_df['predicate']==\"love\"], \"subject\", \"object\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(graph, k = 0.5) # k regulates the distance between nodes\nnx.draw(graph, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope it was helpful. Please upvote for support.\nAlso, refer to my other notebooks if you are looking for word embedding techniques Word2Vec, GloVe etc. and text classification."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}