{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Collection**"},{"metadata":{},"cell_type":"markdown","source":"Let's start by reading in the diabetes.csv file into a pandas dataframe."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explorer Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columns*rows\ndf.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How many NA elements in every column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For getting some information about the dataset you can use info() command**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To check the first 5 rows of the data set, we can use head(5).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To check out last 5 row of the data set, we use tail() function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To pop up 5 random rows from the data set, we can use sample(5) function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To give a statistical summary about the dataset, we can use describe()****"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To check out how many null info are on the dataset, we can use isnull().sum().**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To print dataset columns, we can use columns atribute**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualization**"},{"metadata":{},"cell_type":"markdown","source":"**Histogram**"},{"metadata":{},"cell_type":"markdown","source":"**We can also create a histogram of each input variable to get an idea of the distribution.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms\ndf.hist(figsize=(16,48))\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(8,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pairplot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Heatmap**"},{"metadata":{},"cell_type":"markdown","source":"**Plot rectangular data as a color-encoded matrix.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing Data**"},{"metadata":{},"cell_type":"markdown","source":"We can use seaborn to create a simple heatmap to see where we are missing data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distplot**"},{"metadata":{},"cell_type":"markdown","source":"Flexibly plot a univariate distribution of observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Pregnancies'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Countplot**"},{"metadata":{},"cell_type":"markdown","source":"Show the counts of observations in each categorical bin using bars."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x='Outcome',data=df)\nplt.title('Positive Outcome to Diabetes in Dataset')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(data=df,x='Outcome',y='Pregnancies')\nplt.title('Pregnancies Among Diabetes Outcomes.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x='Pregnancies',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to Pregnancies')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.countplot(x='Age',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.countplot(x='SkinThickness',data=df,hue='Outcome')\nplt.title('Diabetes Outcome to SkinThickness')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']]\ny=df['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Outcome',axis=1), \n                                                    df['Outcome'], test_size=0.2, \n                                                    random_state=201)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Logistic Regression**"},{"metadata":{},"cell_type":"markdown","source":"Now its time to train our model on our training data!\n\n--Import LinearRegression from sklearn.linear_model--"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create an instance of a LogisticRegression() model named lm.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel = LogisticRegression()\n#** Train/fit lm on the training data.**\nlogmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predicting Test Data**"},{"metadata":{},"cell_type":"markdown","source":"Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n\n** Use lm.predict() to predict off the X_test set of the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = logmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardize the Variables**"},{"metadata":{},"cell_type":"markdown","source":"Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardize the Variables\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(df.drop('Outcome',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features = scaler.transform(df.drop('Outcome',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now above data are Standardize**"},{"metadata":{},"cell_type":"markdown","source":"**Using KNN**\n\nRemember that we are trying to come up with a model to predict whether someone will TARGET CLASS or not. We'll start with k=1."},{"metadata":{},"cell_type":"markdown","source":"**Train Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['Outcome'],\n                                                    test_size=0.10,random_state=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using KNN\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictions and Evaluations**\n\n\nLet's evaluate our KNN model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predictions and Evaluations\nfrom sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Choosing a K Value**"},{"metadata":{},"cell_type":"markdown","source":"Let's go ahead and use the elbow method to pick a good K Value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing a K Value\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that that after arouns K>16 the error rate just tends to hover around 0.0-0.16 Let's retrain the model with that and check the classification report!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOW WITH K=19\nknn = KNeighborsClassifier(n_neighbors=16)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\n#print('WITH K=19')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**"},{"metadata":{},"cell_type":"markdown","source":"**In this kernel, I have tried to cover all the parts related to the process of Machine Learning algorithm logistic regression and Support vector machine with a variety of Python packages . I hope to get your feedback to improve it.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}