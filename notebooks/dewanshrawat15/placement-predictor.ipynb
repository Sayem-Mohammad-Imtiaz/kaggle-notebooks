{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We load our dataset into a Pandas DataFrame object.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describing our dataset\nWe display the first 10 entries of the DataFrame object of the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting null values in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the salary column as it has a lot of null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['salary'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label Encoding various columns in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf['ssc_b'] = le.fit_transform(df['ssc_b'])\ndf['hsc_b'] = le.fit_transform(df['hsc_b'])\ndf['hsc_s'] = le.fit_transform(df['hsc_s'])\ndf['degree_t'] = le.fit_transform(df['degree_t'])\ndf['workex'] = le.fit_transform(df['workex'])\ndf['specialisation'] = le.fit_transform(df['specialisation'])\ndf['status'] = le.fit_transform(df['status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summarising our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using heatmaps\nGraphs can give a pretty fair picture about the relationship between the targetted data and the feature. But using a heatmap shows a more accurate picture about the correlation between different features and the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\ncorr_mat = df.corr().round(2)\nsns.heatmap(data=corr_mat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the correlation factor we choose a few features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['ssc_p', 'hsc_p', 'degree_p', 'workex', 'specialisation']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding relation between the target and features\nWe plot a graph to see how the target feature vary with different features we selected above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['status']\nfor i in features:\n  x = df[i]\n  plt.xlabel(i)\n  plt.ylabel(\"Placed or not\")\n  plt.scatter(x, y)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shape our dataset into X and Y variables according to features selected from the heatmap and graphs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[features]\nY = df['status']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the dataset\nWe use train_test_split to test our dataset into training and testing variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, x_test, Y_train, y_test = train_test_split(X, Y, random_state=4, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating TF, TN, FP, FN\nWriting a function to calculate True Positives, False Positives, True Negatives and False Negatives.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a SVM Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svmModel = SVC()\nsvmModel.fit(X_train, Y_train)\nsvmModel.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions using the given x test values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_svm_hat = svmModel.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a ROC Curve for the same SVM Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(svmModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a confusion matrix to see how the SVM Model fares.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(svmModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating specificity, recall, precision and accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_svm_hat))\nprint(\"Precision is\", (truePositive / (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive / (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative / (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) / (truePositive + falsePositive + falseNegative + trueNegative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lrModel = LogisticRegression()\nlrModel.fit(X_train, Y_train)\nlrModel.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions using the given x test values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr_hat = lrModel.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a ROC curve for our logistic regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(lrModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a confusion matrix for our Logistic Regression Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(lrModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating specificity, recall, precision and accuracy for our Logistic Regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_lr_hat))\nprint(\"Precision is\", (truePositive / (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive / (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative / (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) / (truePositive + falsePositive + falseNegative + trueNegative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor i in range(1, 21):\n  knnModel = KNeighborsClassifier(n_neighbors=i)\n  knnModel.fit(X_train, Y_train)\n  score = knnModel.score(x_test, y_test)\n  scores.append(score)\n\nmax(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errors = [(1 - x) for x in scores]\n\nplt.figure(figsize=(12, 12))\nplt.plot(range(1, 21), errors, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value') \nplt.xlabel('K') \nplt.ylabel('Error Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_n = scores.index(max(scores)) + 1\nknnModel = KNeighborsClassifier(n_neighbors=best_n)\nknnModel.fit(X_train, Y_train)\ny_knn_hat = knnModel.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a ROC Curve for our KNN model and seeing how it fares.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(knnModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a confusion matrix for our KNN Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(knnModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating specificity, recall, precision and accuracy for the KNN Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_knn_hat))\nprint(\"Precision is\", (truePositive / (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive / (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative / (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) / (truePositive + falsePositive + falseNegative + trueNegative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThis data set consists of Placement data of students in Jain University Bangalore campus. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and Work experience and salary offers to the placed students.\n\nDisplayed the correlation between different features in the dataset using heatmaps and graphs. Also calculated the accuracy, specificity indicating the accuracy of our model. Also, visualised our predictions in the form of a confusion matrix.\n\nThe KNN model makes pretty good predictions and has a good accuracy rate of about 89%.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}