{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix, jaccard_similarity_score\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/married-at-first-sight/mafs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Name', 'Occupation'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Location'] = le.fit_transform(df['Location'])\ndf['Gender'] = le.fit_transform(df['Gender'])\ndf['Decision'] = le.fit_transform(df['Decision'])\n\ndf['Status'] = le.fit_transform(df['Status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,14))\ncorr_matrix = df.corr().round(2)\nsns.heatmap(data=corr_matrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['Decision', 'Season', 'Age', 'DrLoganLevkoff', 'DrJosephCilona', 'ChaplainGregEpstein', 'PastorCalvinRoberson', 'DrJessicaGriffin']]\nY = df[['Status']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Decision', 'Season', 'Age', 'DrLoganLevkoff', 'DrJosephCilona', 'ChaplainGregEpstein', 'PastorCalvinRoberson', 'DrJessicaGriffin']:\n  x = df[i]\n  y = df['Status']\n  plt.xlabel(i)\n  plt.ylabel('Status')\n  plt.scatter(x, y)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, x_test, Y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Logistic Regression\nWe create a logistic regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating TF, TN, FP, FN\nWriting a function to manually calculate the True Positives, False Positives, True Negatives and False Negatives.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_hat))\nprint(\"Precision is\", (truePositive / (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive / (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative / (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) / (truePositive + falsePositive + falseNegative + trueNegative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising the model's performance\nWe plot the actual data and predicted data for different features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Decision', 'Season', 'Age', 'DrLoganLevkoff', 'DrJosephCilona', 'ChaplainGregEpstein', 'PastorCalvinRoberson', 'DrJessicaGriffin']:\n  x_temp = x_test[i]\n  plt.scatter(x_temp, y_test, color='grey')\n  plt.scatter(x_temp, y_hat, color='red')\n  plt.xlabel(i)\n  plt.ylabel(\"Status\")\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using KNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Finding the value of K\nTo find the value of k that would give the most suitable result, we use a hit and trial approach. We plot a graph of errors vs k values and pick the value of K where the error becomes minimum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nn = []\nfor i in range(1, 24):\n  knnModel = KNeighborsClassifier(n_neighbors=i)\n  knnModel.fit(X_train, Y_train)\n  score = knnModel.score(x_test, y_test)\n  scores.append(score)\n  n.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score_index = scores.index(max(scores))\nbest_n = n[(best_score_index)]\nbest_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = [1 - x for x in scores]\nplt.figure(figsize =(10, 6))\nplt.plot(range(1, 24), error_rate, color ='blue', linestyle ='dashed', marker ='o', markerfacecolor ='red', markersize = 10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the errors decrease sharply for k = 10. We create a K Nearest Neighbors model, set n_neighbors to 10 and train the KNN model using the fit method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knnModel = KNeighborsClassifier(n_neighbors=10)\nknnModel.fit(X_train, Y_train)\nknnModel.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We make predictions for the X values in the testing dataset and store it in the y_knn_hat variable. Now we can compare the y_knn_hat variable with the y_test variable to compare the accuracy of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_knn_hat = knnModel.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting a confusion matrix\nVisualising the predictions of our model in the form of a confusion matrix to see how well our model performs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(knnModel, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Caculating True Positive, False Positive, True Negative, False Negatives. Also, we calculate the precision, recall, specificity and accuracy of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"truePositive, falsePositive, trueNegative, falseNegative = perf_measure(np.asarray(y_test), np.asarray(y_knn_hat))\nprint(\"Precision is\", (truePositive / (truePositive + falsePositive)))\nprint(\"Recall is\", (truePositive / (truePositive + falseNegative)))\nprint(\"Specificity is\", (trueNegative / (trueNegative + falsePositive)))\nprint(\"Accuracy is\", ((truePositive + trueNegative) / (truePositive + falsePositive + falseNegative + trueNegative)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the actual data and predicted data for different features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Decision', 'Season', 'Age', 'DrLoganLevkoff', 'DrJosephCilona', 'ChaplainGregEpstein', 'PastorCalvinRoberson', 'DrJessicaGriffin']:\n  x_temp = x_test[i]\n  plt.scatter(x_temp, y_test, color='grey')\n  plt.scatter(x_temp, y_knn_hat, color='red')\n  plt.xlabel(i)\n  plt.ylabel(\"Status\")\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"j_score = jaccard_similarity_score(y_test, y_knn_hat)\nprint(\"Jaccard similarity score\", j_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nTrained two different models, one using K Nearest Neighbors and Logistic Regression. Displayed the correlation between different features in the dataset using heatmaps and graphs. Also calculated the accuracy, specificity indicating the accuracy for both models. Also, visualised our predictions in the form of a confusion matrix.\n\nWe were able to train a model with about 85% accuracy for our given dataset, thus giving a fair insight about the thought \"What makes couples stay together?\" using features provided from the \"Married at first sight\" dataset.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}