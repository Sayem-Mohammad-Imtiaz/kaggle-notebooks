{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectKBest,chi2,f_regression\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/insurance/insurance.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving forward we've to do some cleaning with the data. Cleaning is the most important part of data analysis as the accuracy of results depends a lot upon the the quality of data. Without quality data predicting accurate results is not possible.\n\nWe'll here check for basic missing values and try to impute them with the help of various data cleaning techniques which will involve use of Imputer or deleting unnecessary data. \n\nRemoving of data from available information depends on how much data is missing for a feature becuase if we'll move substantial amount of data then we'll not be left with much to work upon. It'll will result in a less accurate model."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From Basic Info of dataset, we're able to see that the we're having 2 features of float, 2 features of integer and 3 of Object Data Type. Since the statistical models are available for numeric data so the features which are of object data type needs to be changed to ordinal values during later stages.\n\nCharges is our target variable so we shouldn't be worried much about it.\n\nThe information is showing us that it's not having null values so it means that we can proceed towards further stages of data processing without worrying about null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(dataset.drop(['charges'],axis=1),dataset['charges'],test_size=0.25,random_state=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all the categorical features will be changes to numeric. This step is required since most of stastical models\nare not able to process categorical data. Here we'll try with most basic Label Encoder and One Hot Encoding to make sure that categorical features are changed to ordinal values."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\n\nX_train['sex'] = encoder.fit_transform(X_train['sex'])\nX_test['sex'] = encoder.fit_transform(X_test['sex'])\nX_train['smoker'] = encoder.fit_transform(X_train['smoker'])\nX_test['smoker'] = encoder.fit_transform(X_test['smoker'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.region.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnew_cols_train = pd.get_dummies(X_train['region'])\nnew_cols_test = pd.get_dummies(X_test['region'])\nX_train.drop(columns=['region'],axis=1,inplace=True)\nX_test.drop(columns=['region'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train,new_cols_train],axis=1)\nX_test = pd.concat([X_test,new_cols_test],axis=1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we're having dataset in it's most basic form and is having all the possible features. Now i'll create a predictive model using Linear Regression without any changes to dataset. After this we'll move to feature selection stage to improve the features and move towards an enhanced and more focused dataset. This will show us how much we can improve if we enhance the given data.\n\nYou'll be clearly able to observe the change in results after the enhancements. So let's move towards basic Linear Regression model creation."},{"metadata":{"trusted":true},"cell_type":"code","source":"l_reg = LinearRegression()\nl_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_reg.predict(X_test).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score here tells us that our model is able to predict 79% accurate results which is quite good for a model with raw data. Now time is to see if we can improve the accuracy of this by applying feature selection and feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.concat([X_train,y_train],axis=1).corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the last 4 columns are showing quite high negative corelation. It was supposed to be like that only because these 4 columns were derived from only one feature so it has to behave this way.\n\nApart from them the corelation value for smoker and charges is quite high. Insurance Premiums are high for people who are smokers since they've more chances of dying early which will result in more chances of insurance getting claimed by the family of insured. This means that insurance charges are very strongly dependent on a person being smoker.\n\nNot only this but age is also having high corelation with charges. It is because the chances of a person dying in an early age is quite less than the person dying at later age."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_best = SelectKBest(f_regression,k=5)\nk_best_transformed = k_best.fit_transform(X_train,y_train)\nk_best.scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scores we can see that the We're having maximum score for 1st and 5th features which means that they will influence the target variable more than the others. We've seen the simillar behavior for these features using corelation heatmap also. \n\nBased on the scores we're going to consider only the first 5 column and the southeast column to be used for prediction purpose.\nI'll train the model again for these columns and see how much differenc we're having based on these enhancements."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(X_train,hue='sex',vars=['age','bmi','children'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['northeast','northwest','southwest']\nX_train.drop(columns=cols_to_drop,axis=1,inplace=True)\nX_test.drop(cols_to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_reg.fit(X_train,y_train)\nl_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we've seen that since the results have improved but not significantly so we can try some feature engineering and try to introduce some features which will help us get the best of available data.\n\nFeature Engineering is creation of new features based on existing so that we can get some more information of the data which will help us predict the data.This process involves more business understanding. The better we know about the business more we'll be able to create features which will help us. We've to try with multiple feature combinations and see how better each combination works out.\n\nAs a result of feature combinations and my understanding of insurance domain, i've able to produce following few features which has helped the result getting better."},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_transform(ages):\n    transformed_list = []\n    #Here 1 means 'Young', 2 means 'Middle Aged' and 3 means 'Old Age'\n    for age in ages:\n        if age <= 30:\n            transformed_list.append(1)\n        elif age < 60:\n            transformed_list.append(2)\n        else:\n            transformed_list.append(3)\n    \n    return transformed_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding a new feature 'life_stage' based on persons age\nX_train['life_stage'] = age_transform(X_train.age.values)\nX_test['life_stage'] = age_transform(X_test.age.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bmi_category(bmi):\n    transformed_list = []\n    #Here 1 means 'Under weight', 2 means 'Normal' , 3 means 'Over Weight' and 4 means 'Obese'\n    for index in bmi:\n        if index < 18.5:\n            transformed_list.append(1)\n        elif index >= 18.5 and index <= 24.9:\n            transformed_list.append(2)\n        elif index >= 25 and index <= 29.9:\n            transformed_list.append(3)\n        else:\n            transformed_list.append(4)\n    \n    return transformed_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We'll shift the bmi values to it's corresponding category\nX_train['bmi'] = bmi_category(X_train.bmi.values)\nX_test['bmi'] = bmi_category(X_test.bmi.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_risk(life_stage,smoker,bmi):\n    transformed_list = []\n    #Here from 1 till 6 we've increasing risk based on life stage, smoker and bmi\n    counter = 0\n    if len(life_stage) == len(smoker):\n        for stage,smoke in zip(life_stage,smoker):\n            if (stage == 1) and (smoke == 1) and (bmi[counter] == 2):\n                transformed_list.append(1)\n            elif (stage == 1) and (smoke == 1) and (bmi[counter] == 3):\n                transformed_list.append(2)\n            elif (stage == 2) and (smoke == 1) and (bmi[counter] == 2):\n                transformed_list.append(3)\n            elif (stage == 2) and (smoke == 1) and (bmi[counter] == 3):\n                transformed_list.append(4)\n            elif (stage == 3) and (smoke == 1) and (bmi[counter] == 2):\n                transformed_list.append(5)\n            elif (stage == 3) and (smoke == 1) and (bmi[counter] == 3):\n                transformed_list.append(6)\n            else:\n                transformed_list.append(0)\n            counter=counter+1\n    \n    return transformed_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['life_risk'] = calculate_risk(X_train.life_stage.values,X_train.smoker.values,X_train.bmi.values)\nX_test['life_risk'] = calculate_risk(X_test.life_stage.values,X_test.smoker.values,X_test.bmi.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_reg.fit(X_train,y_train)\nl_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"As here you'll be able to see that after doing Feature Engineering the score of our existing model has improved by 8%. Earlier it was able to predict 79.36% but now we're getting accuracy of 86.75%. This is a huge difference. As you've seen that with basic Linear Regression we can improve the results drastically if we analyse the data properly. I haven't tried any other model on this dataset. You can try and see how other models help getting results on this dataset.\n\nHope you've liked it. Thankyou!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}