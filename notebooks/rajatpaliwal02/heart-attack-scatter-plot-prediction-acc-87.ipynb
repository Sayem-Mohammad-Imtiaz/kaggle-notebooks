{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd \"/kaggle\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the dataset\nfile_path = \"input/heart-attack-analysis-prediction-dataset/heart.csv\"\nheart_data = pd.read_csv(file_path)\nheart_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Set the width and height of the figure\nplt.figure(figsize=(16,6))\n\nsns.lineplot(data=heart_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This line plot of the all the parameters does not provide a good insight. Lets take a look at the heat map below for a better insight into the data.","metadata":{}},{"cell_type":"code","source":"#creating a heatmap of cholestrol \nplt.figure(figsize=(30,20))\nplt.title(\"Heart attack analysis for number of patients\")\nsns.heatmap(data=heart_data[:50], annot=True)\n\nplt.xlabel(\"Various factors\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatmap above is of first 50 rows , but it does tells us a story. It seems there is a correlation between age , cholestrol , resting blood pressure and highest blood pressure. We will strengthen this point using a correlation plot.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf= pd.DataFrame(heart_data,columns=[\"age\",\"sex\",\"cp\",\"trtbps\",\"chol\",\"fbs\",\"restceg\",\"thalachh\",\"exng\",\"oldpeak\",\"slp\",\"caa\",\"thall\",\"output\"])\ncorrMatrix= df.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corrMatrix, annot= True,fmt='.1g')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This correlation heatmap definitely provides, very good insight into the data.Cholestrol, Resting blood pressure and oldpeak have positive correlation with age.\n* There are no two columns whose values are highly correlated , so we have to keep all the columns as they are contributing to the final output independently.\n* The heart rate output is closely correlated with cp, thalachh and slp. We will be plotting scatter plots to dive deep into this insight.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=heart_data['cp'], y=heart_data['thalachh'], hue=heart_data['output'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference drawn from above scatter plot:\n*  If the number of major vessels are less the chances of heart attack is less. It increases a bit with the number of major vessels.\n*  As the value of maximum blood pressure is increased the heart attacks increases. basically if it is above 180 mm Hg.\n","metadata":{}},{"cell_type":"code","source":"# lets do a swarm plot of age and cholestrol against the heart rate output.\nsns.swarmplot(x=heart_data['output'],\n              y=heart_data['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems chances of heart rate are more in between the age of 40-55. It decreases above the age of 55.","metadata":{}},{"cell_type":"code","source":"sns.swarmplot(x=heart_data['output'],\n              y=heart_data['chol'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=heart_data['age'], y=heart_data['chol'], hue=heart_data['output'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above scatter plot it is seems cholestrol in between 300- 200 mg/dl for the age of 40-55 is the perfect recipe for heart attack.","metadata":{}},{"cell_type":"markdown","source":"**Now we will do some preprocessing on the dataset**.","metadata":{}},{"cell_type":"code","source":"#Doing test train split\nfrom sklearn.model_selection import train_test_split\n\n#Selecting target\ny=heart_data.output\n\n#using only numerical predictors\nheart_predictors= heart_data.drop(['output'],axis=1)\nX = heart_predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We make a score dataset function to check the correctness of different approach\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\nprint(cols_with_missing)\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\nprint(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in this dataset. So handling of missing values is not required.","metadata":{}},{"cell_type":"code","source":"#Performing prediction using XGBoost\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\n#Initialize the classifier\nclf= XGBClassifier()\nclf.fit(reduced_X_train,y_train)\npredictions =clf.predict(reduced_X_valid)\nmetrics.accuracy_score(y_valid,predictions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy achieved is 78.68% . Its a decent accuracy but it definitely can be improved.Lets try GridSearchCV to imporve the accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom xgboost import XGBClassifier\n\n#Initialize the classifier\nclf= XGBClassifier()\n\n#Parameters\nparameters =  {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n               'seed': [1337]}\n\n\n\nscorer = make_scorer(fbeta_score,beta=0.5)\n\ngrid_obj = GridSearchCV(clf, parameters,scorer)\n\ngrid_fit = grid_obj.fit(reduced_X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using unoptimized and optimized model\npredictions = (clf.fit(reduced_X_train, y_train)).predict(X_valid)\nbest_predictions = best_clf.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on validation data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\nprint(\"F-score on validation data: {:.4f}\".format(fbeta_score(y_valid, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the validation data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\nprint(\"Final F-score on the validation data: {:.4f}\".format(fbeta_score(y_valid, best_predictions, beta = 0.5)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, using GridSearCV definitely increases the accuracy of XGboost classifier from 0.7869 to 0.8689","metadata":{}},{"cell_type":"code","source":"#implementing random forset classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nmodel_rf= RandomForestClassifier()\nmodel_rf.fit(reduced_X_train,y_train)\n# Make predictions for the test set\ny_pred_rf = model_rf.predict(reduced_X_valid)\n# View accuracy score\nmetrics.accuracy_score(y_valid, y_pred_rf)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the accuracy using logistic regression\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=50).fit(reduced_X_train, y_train)\ny_pred_lr=clf.predict(reduced_X_valid)\nmetrics.accuracy_score(y_valid, y_pred_rf)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}