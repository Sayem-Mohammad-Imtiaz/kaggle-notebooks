{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport os\nimport re\nimport nltk.corpus\nfrom nltk import word_tokenize\n# Importing Lemmatizer library from nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n#importing chunk library from nltk\n#from nltk import ne_chunk\nfrom string import punctuation\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news['label'] = 'TRUE'\nfake_news['label'] = 'FAKE'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news.info()\nfake_news.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([true_news,fake_news])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to remove tags\ndef remove_tags(string):\n    result = re.sub('<.*?>','',string)\n    return result\n#remove tags\ndf['title'] = df['title'].apply(lambda x : remove_tags(x))\ndf['text'] = df['text'].apply(lambda x : remove_tags(x))\ndf['subject'] = df['subject'].apply(lambda x : remove_tags(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing number"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove number\ndef remove_number(data):\n    output = ''.join(c for c in data if not c.isnumeric())\n    return output\n\ndf['title']=df['title'].apply(lambda x : remove_number(x))\ndf['text']=df['text'].apply(lambda x : remove_number(x))\ndf['subject']=df['subject'].apply(lambda x : remove_number(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\ndf['title'] = df['title'].apply(lambda x:expand_contractions(x))\ndf['text'] = df['text'].apply(lambda x:expand_contractions(x))\ndf['subject'] = df['subject'].apply(lambda x:expand_contractions(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing white spaces"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove white spaces\ndf['title'] = df['title'].apply(lambda x: re.sub(' +',' ',x))\ndf['text'] = df['text'].apply(lambda x: re.sub(' +',' ',x))\ndf['subject'] = df['subject'].apply(lambda x: re.sub(' +',' ',x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization with stopwords removal and tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = stopwords.words('english')\ndef lemmatize_text(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w) for w in word_tokenize(text) if w not in stopwords]\n\ndf['title'] = df['title'].apply(lemmatize_text)\ndf['text'] = df['text'].apply(lemmatize_text)\ndf['subject'] = df['subject'].apply(lemmatize_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting to lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing to lowercase\ndf['title'] = df['title'].astype(str).str.lower()\ndf['text'] = df['text'].astype(str).str.lower()\ndf['subject'] = df['subject'].astype(str).str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rrmoving punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove punctuation\ndef strip_punctuation(s):\n    return ''.join(c for c in s if c not in punctuation)\ndf['title'] = df['title'].apply(lambda x: strip_punctuation(x))\ndf['text'] = df['text'].apply(lambda x: strip_punctuation(x))\ndf['subject'] = df['subject'].apply(lambda x: strip_punctuation(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('cleaned_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv('cleaned_data.csv')\nd.info()\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize  \nimport nltk.corpus\nimport seaborn as sns\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer, TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the cleaned Data<a id='3.4_Load the Data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/output/kaggle/working/cleaned_data.csv',index_col=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary Statistics<a id='3.4.1_Summary Statistics'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove NaN values<a id='3.4.2_Remove_NaN_Values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map categorical features to numeric values<a id='3.5.1_Map_categorical_features_to_numeric_values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'] = df['label'].map({'FAKE':0, 'TRUE':1, np.nan:2} )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore the data<a id='3.5_Explore_the_data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 1000,\n        height = 500,\n        background_color = 'black').generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud by True Label<a id='3.5.1_WordCloud_by_True_Label'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true = df[df['label'] == 1 ]\n\ngenerate_word_cloud(df_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud by Fake Label<a id='3.5.2_WordCloud_by_Fake Label'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake = df[df['label'] == 0 ]\n\ngenerate_word_cloud(df_fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Histograms of True and Fake labels in the data<a id='3.5.3_Histograms_of_True_and_Fake_labels_in_the_data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nsns.set(style=\"darkgrid\")\n\ncolor = sns.color_palette(\"Set2\")\nax = sns.countplot(x=\"label\", data=df, palette=color)\n\nax.set(xticklabels=df.label.unique())\n\nplt.title(\"Data distribution of fake and true news\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pie-chart showing various data sources<a id='3.5.4_Pie-chart_showing_various_data_sources'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"slices = df.subject.value_counts().to_list()\nlabel = df.subject.unique()\nexplode = (0.1, 0.1, 0.1, 0,0,0,0,0) \nplt.pie(slices, labels = label, startangle = 30, shadow = True, explode=explode,autopct='%1.1f%%')\nplt.title('Pie Chart of News Subject')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in each sentence<a id='3.5.5_Number_of_characters_in_each_sentence'></a>"},{"metadata":{},"cell_type":"markdown","source":"Here we will explore the number of characters in each sentence and analyze if there is a difference in the character count for Fake and True news. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true['title'].str.len().hist(bins=10,range=[20,200])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['title'].str.len().hist(bins=10,range=[20,200])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true['text'].str.len().hist(bins=10,range=[0,30000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['text'].str.len().hist(bins=10,range=[0,30000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average length of word in each sentence<a id='3.5.6_Average_length_of_word_in_each_sentence'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true['title'].str.split().\\\n   apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist(bins=10, range=[3.0, 10])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###try binning it not based on max min and make the plots similar for comparison\ndf_fake['title'].str.split().\\\n   apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist(bins=10, range=[3.0, 10])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true['text'].str.split().\\\n   apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist(bins=10, range=[3.5, 6.5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['text'].str.split().\\\n   apply(lambda x : [len(i) for i in x]). \\\n   map(lambda x: np.mean(x)).hist(bins=10, range=[3.5, 6.5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing high dimensional data<a id='3.6_Visualizing_high_dimensional_data'></a>"},{"metadata":{},"cell_type":"markdown","source":"### Creating unigrams from vectorized data<a id='3.6.1_Creating _unigrams_from_vectorized_data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 20 unigrams\nword_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_true['text'])\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Blues_d\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 20 unigrams\nword_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_fake['text'])\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Reds_r\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating bigrams from vectorized data<a id='3.6.2_Creating _bigrams_from_vectorized_data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"##top 20 bigrams\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_true.text)\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Blues_d\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##top 20 bigrams\nword_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_fake.text)\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Reds_d\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating trigrams from vectorized data<a id='3.6.3_Creating _trigrams_from_vectorized_data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"##top 20 trigrams\nword_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_true.text)\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Blues_d\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##top 20 trigrams\nword_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\nsparse_matrix = word_vectorizer.fit_transform(df_fake.text)\nfrequencies = sum(sparse_matrix).toarray()[0]\nword = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\nword = word.nlargest(columns = \"frequency\", n = 20)\nplt.figure(figsize=(12,8))\nax = sns.barplot(data = word, y = word.index, x = \"frequency\", palette=(\"Reds_d\"))\nsns.set_context(\"poster\")\nax.set(ylabel = \"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Summary<a id='3.7_Summary'></a>"},{"metadata":{},"cell_type":"markdown","source":"The dataset has a balanced number of both TRUE and FAKE classes.  Few things can be observed in the text that demarcates the tow labels:\n * Number of characters in 'title' for True labels are mostly between 50 to 100. For Fake labels this has a wide range of variations suggesting that data are collected from different sources. \n * Average length of sentences are very similar for both classes.\n \nLooking at the n-grams and the word cloud it is evident that the news mostly contains political content and president Trump is most common across both labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"},{"metadata":{},"cell_type":"markdown","source":"### Imports<a id='4.3_Imports'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.util import ngrams\nimport nltk.corpus\nfrom stop_words import get_stop_words\n\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, precision_recall_curve\nimport time\nfrom sklearn import __version__ as sklearn_version\nfrom sklearn.pipeline import make_pipeline\nimport os, time\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the cleaned Data<a id='4.4_Load the Data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/output/kaggle/working/cleaned_data.csv',index_col=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map categorical features to numeric values<a id='4.4.1_Map_categorical_features_to_numeric_values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'] = df['label'].map({'FAKE':0, 'TRUE':1, np.nan: 2} )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove Missing Values<a id='4.4.1_Remove_missing_values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering with Count Vectorizer and Tfidf Vectorizer<a id='4.5_Feature_Engineering_with_Count_Vectorizer_and_Tfidf_Vectorizer'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['allwords'] = df['title']+\" \"+df['text']\ndf_all = df[['allwords','label']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df_all.drop_duplicates(subset=['allwords', 'label'], keep=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = df_all[df_all['label']==1].sample(n=12530, random_state=42)\nfake = df_all[df_all['label']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([true,fake],ignore_index=True)\ndf_all = df_all.sample(frac=1).reset_index(drop=True)\ndf_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split<a id='4.6_Train/Test_Split'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_all['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_words = ['reuter','reuters','reutersus','image','via']\nf = lambda x: ' '.join([item for item in x.split() if item not in selected_words])\nX = df_all[\"allwords\"].apply(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stopwords removal<a id='4.6_Stopwords_removal'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [word for word in X if word not in get_stop_words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train,y_test= train_test_split(X, y, test_size=0.30,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Model Name', 'accuracy','precision','recall','ROC AUC score','run time']\nresults = pd.DataFrame(columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models and Metrics<a id='4.7_Models_and_Metrics'></a>"},{"metadata":{},"cell_type":"markdown","source":"### Metrics<a id='4.7.1_Metrics'></a>"},{"metadata":{},"cell_type":"markdown","source":"In classification task there are a number of things that we can look to understand how good a model is performing. Precision, recall, f1 score, and ROC-AUC score are typically used metrics for classification problems. `sklearn.metrics` provides many commonly used metrics, included the ones mentioned here."},{"metadata":{"trusted":true},"cell_type":"code","source":" def metrics(model_name,y_test,y_pred):\n    accuracy = accuracy_score(y_test,y_pred)\n    roc_auc =roc_auc_score(y_test, y_pred)\n    precision = precision_score(y_pred=y_pred, y_true=y_test,zero_division=1)\n    recall = recall_score(y_pred=y_pred, y_true=y_test,zero_division=1)\n    \n    print(classification_report(y_test, y_pred,zero_division=1))\n\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cf_matrix, annot=True,fmt='3', cmap='Blues')\n    plt.xlabel('Predicted user status',fontsize=12)\n    plt.ylabel('True user status',fontsize=12)\n    plt.title('%s Confusion Matrix' % model_name,fontsize=20)\n    plt.show()\n\n    fpr, tpr, threshold = roc_curve(y_test, y_pred)\n    plt.plot([0,1], [0,1], 'k--')\n    plt.plot(fpr, tpr, label=model_name)\n    plt.xlabel('False Positive Rate',fontsize=12)\n    plt.ylabel('True Positive Rate',fontsize=12)\n    plt.title('%s ROC Curve'% model_name,fontsize=20)\n    plt.legend(fontsize=12)\n    plt.show()\n    #pipes = pipes.append(pipe)\n    #return pd.DataFrame([[model_name,accuracy, precision, recall,roc_auc,t1]],columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Model<a id='4.7.2_Dummy_Model'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Dummy Model'\n       \npipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',lowercase=False,ngram_range=(2,2))),\n    ('tfidf', TfidfTransformer()),\n    ('clf', DummyClassifier()),\n])\n\nparameters = {\n    'vect__max_df': (0.25,0.5, 0.75),\n    # 'vect__max_features': (None, 5000, 10000, 50000),\n}\n\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    print(parameters)\n    t0 = time.time()\n    grid_search.fit(X_train, y_train)\n    print(\"done in %0.3fs\" % (time.time() - t0))\n    print()\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n\ny_pred = grid_search.predict(X_test)\nresults = results.append(metrics(model_name,y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MultiNomial Naive Bayes<a id='4.7.3_MultiNomial_Naive_Bayes'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Naive Bayes'\n       \npipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',lowercase=False,ngram_range=(2,2))),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])\n\nparameters = {\n    'vect__max_df': (0.25,0.5, 0.75),\n    # 'vect__max_features': (None, 5000, 10000, 50000),\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    print(parameters)\n    t0 = time.time()\n    grid_search.fit(X_train, y_train)\n    print(\"done in %0.3fs\" % (time.time() - t0))\n    print()\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n\ny_pred = grid_search.predict(X_test)\nresults = results.append(metrics(model_name,y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression<a id='4.7.4_Logistic_Regression'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Logistic Regression'\n       \npipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',lowercase=False,ngram_range=(2,2))),\n    ('tfidf', TfidfTransformer()),\n    ('clf', LogisticRegression()),\n])\n\nparameters = {\n    'vect__max_df': (0.25, 0.5, 0.75,),\n    'clf__penalty': ('l1','l2'),\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    print(parameters)\n    t0 = time.time()\n    grid_search.fit(X_train, y_train)\n    print(\"done in %0.3fs\" % (time.time() - t0))\n    print()\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n\ny_pred = grid_search.predict(X_test)\nresults = results.append(metrics(model_name,y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent<a id='4.7.5_Stochastic_Gradient_Descent'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Stochastic Gradient Descent'\n       \npipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',lowercase=False,ngram_range=(2,2))),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\n\nparameters = {\n    'vect__max_df': (0.25,0.5, 0.75),\n    'clf__penalty': ('elasticnet',),\n    'clf__loss': ('log',)\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    print(parameters)\n    t0 = time.time()\n    grid_search.fit(X_train, y_train)\n    print(\"done in %0.3fs\" % (time.time() - t0))\n    print()\n\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n\ny_pred = grid_search.predict(X_test)\nresults = results.append(metrics(model_name,y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier<a id='4.7.6_Random_Forest_Classifier'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'Random Forest'\n       \npipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='word',lowercase=False, ngram_range=(2,2))),\n    ('tfidf', TfidfTransformer()),\n    ('clf', RandomForestClassifier()),\n])\n\nparameters = {\n    'vect__max_df': (0.25,0.5, 0.75),\n    'clf__criterion': ('gini','entropy'),\n    'clf__max_features': ('auto', 'sqrt'),\n}\n\nif __name__ == \"__main__\":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(\"Performing grid search...\")\n    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n    print(\"parameters:\")\n    print(parameters)\n    t0 = time.time()\n    grid_search.fit(X_train, y_train)\n    print(\"done in %0.3fs\" % (time.time() - t0))\n    print()\n\nprint(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n\ny_pred = grid_search.predict(X_test)\nresults = results.append(metrics(model_name,y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data quantity assessment<a id='4.9_Data_quantity_assessment'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fractions = [0.25, 0.35, 0.5, 0.75, 1.0]\ntrain_size, train_scores, test_scores = learning_curve(grid_search.best_estimator_, X_train, y_train, train_sizes=fractions)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10, 5))\nplt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)\nplt.xlabel('Training set size')\nplt.ylabel('CV scores')\nplt.title('Cross-validation score as training set size increases');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save best model object from pipeline<a id='4.10_Save_best_model_object_from_pipeline'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import sklearn.externals.joblib as extjoblib\nimport joblib\nimport pickle\njoblib.dump(grid_search.best_estimator_, 'best_model_version1.2.pkl', compress = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = joblib.load('best_model_version1.2.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imps = pipeline.named_steps['clf'].feature_importances_\nfi = {'Importance':imps}\nImportance = pd.DataFrame(fi,index=None).sort_values('Importance',ascending=False).head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pipeline.named_steps['vect'].get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = []\nfor i in Importance.index:\n    term = a[i]\n    index.append(term)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"terms = pd.DataFrame({'Term': index, 'Position': Importance.index,'Importance': Importance.Importance})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\n_ = sns.barplot(y = 'Term', x='Importance',data=terms)\nplt.xlabel('features')\nplt.ylabel('importance')\nplt.title('Best random forest regressor feature importances');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\n\n### Model Summary<a id='4.11_Summary'></a>"},{"metadata":{},"cell_type":"markdown","source":"From the 4 models it is observed that the exception of Naive Bayes all of them have very few mis-classification. The ROC_AUC score is 0.964 which is exceptionally well for text classification.  \n    \n   * The top three models are Logistic Regression, Stochastic Gradient Descent and Random Forest\n\n   * Random Forest, SGD and Logistic Regression has comparable ROC-AUC score. Although Logistic Regression has the highest ROC-AUC score it trained only a max_df = 0.25. This means the modeled ignored terms that have a document frequency strictly lower than the given threshold. However, Random Forest is more scalable, and interpretable and also performs better with noisy data. Considering these, we chose Random Forest as the best model. \n\n   * Feature Importance of the model shows that 'president Donald Trump', Washington' 'President Obama' were given the highest importance. This is in alignment with the fact that this dataset is indeed a repersentation of the news during the 2016 US Presidential Election.\n\nAlthough the model performance is very good, looking at both the feature importance of the model the n-gram analysis from EDA  it can be inferred that the data is biased towards US Presidential Election and any news outside this scope might be difficult for the model to predict. We need a bigger dataset covering a wide range of news for both TRUE and FAKE labels to make a more generalized model. "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}