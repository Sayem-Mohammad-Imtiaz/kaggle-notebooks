{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the insurance data from kaggle and see if we can \n1. Find any useful information through Exploratory Data Analysis.\n2. Use any machine learning algorithms to predict any useful parameters."},{"metadata":{},"cell_type":"markdown","source":"First, we load the data into a dataframe and look at the sample/head of the data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Looks pretty straight-forward. Let's check for other stats info and if the data has any nulls."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Seems like we are going to miss out on all the fun of cleaning the dirty data today. May be wait for the next project. \n>However, we can replace \"yes\" or \"no\" to binary (1s and 0s) to make it easier for the machine to understand."},{"metadata":{"trusted":true,"_uuid":"c964fea5e69000e5c08ebec39c78bc985fbb22c1"},"cell_type":"code","source":"df_clean = df.replace(to_replace={'yes':1, 'no':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Converted \"smoker\" column from \"yes\" or \"no\" to binary (1s, 0s)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.sex.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Hmm. No other genders included. Not sure why. TODO: Research about how healthcare categorizes LGBTQ insured."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.region.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can convert these regions into numerical variables later on if needed. Keeping it as strings for easy interpretability."},{"metadata":{"trusted":true,"_uuid":"8befe63e4ed9700fb1c415b545e6e0bf3815c4e3"},"cell_type":"code","source":"#defining my own palette for smokers and non-smokers to appear as red and green respectively.\npal = ['#FF0000', #Red\n       '#006400', #Green\n      ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"851ef53074f2491b2c7c975b210860e61bdd5c22"},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(df_clean.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Charges are heavily correlated to being a smoker, followed by age, bmi, and children in that order."},{"metadata":{"trusted":true,"_uuid":"46a4f23e347dc66b71a5ce673eea31cfa12aacbe"},"cell_type":"code","source":"sns.pairplot(df_clean, hue='smoker', palette=pal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Visualizing the data definitely shows us some unique differences between the smokers and non-smokers. Let's dive a layer deeper."},{"metadata":{"trusted":true,"_uuid":"438fddc94c7c56c43db3b57451b174b79466d805"},"cell_type":"code","source":"sns.scatterplot(x='charges', y='age', data=df_clean, hue='smoker', palette=pal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1cec41b1a9a56eb1e09d26fcdf68dbeb52ea19d"},"cell_type":"code","source":"sns.boxplot(x='smoker', y='charges', data=df_clean, palette=pal, order=[1, 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So, smokers are charged significantly higher on average than non-smokers. Let's dive into what separates the two streams of smokers (one with higher charges and one with lesser).\nSince, BMI was another correlated factor to the charges let's use that as a hue to see the trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='charges', y='age', data=df_clean[(df_clean['smoker']==1)], color=\"Red\", hue='bmi', palette='Blues')\nplt.title('Smoker\\'s \"Age vs Charges\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Seems like BMI is the factor that splits the two categories. "},{"metadata":{"trusted":true,"_uuid":"23177250c11e11c9131266584390e593a157f82d"},"cell_type":"code","source":"df_clean['BMI below limit'] = df_clean['bmi'].apply(lambda x: 1 if x<=30 else 0)\nsns.scatterplot(x='charges', y='age', data=df_clean[(df_clean['smoker']==1)], color=\"Red\", hue='BMI below limit', palette='Blues')\nplt.title('Smoker\\'s \"Age vs Charges\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> After experimentation, I found BMI of 30 to be the \"sweet-spot\" that divides these streams.\nSo, people who are smokers and have BMI over 30 pay significantly higher rates than other smokers."},{"metadata":{},"cell_type":"markdown","source":"## Time for some machine learning. Let's start with Linear Regression and see how it does"},{"metadata":{"trusted":true,"_uuid":"5c73a1f60b26390aef87fa042dbbfadbcccfa193"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb09dc05be8176a3e898e3a8087c28f57d62632"},"cell_type":"code","source":"y= df_clean[df_clean['smoker']==1]['charges']\nX= df_clean[df_clean['smoker']==1][['age', 'bmi', 'children']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed8bbb43c31b4dfaa74c5bcf1811bab7ef8f757"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af02eedc135fc01a6b460831587358e9af9e5560"},"cell_type":"code","source":"lm.fit(X_train, y_train)\npredictions = lm.predict(X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346743be399dd001c1d76d926033ec8598552914"},"cell_type":"code","source":"coefficients = pd.DataFrame(lm.coef_,X.columns)\ncoefficients.columns = ['Coefficient']\ncoefficients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> These coeff how much a unit change in the age, bmi, and having children will affect the Charges. e.g. A Unit increase in \"age\" will increase \"charges\" by ~246."},{"metadata":{"trusted":true,"_uuid":"427509e945d1181e728b22b1cf55978e0b8853c6"},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, classification_report\n\nprint('MAE:', mean_absolute_error(y_test, predictions))\nprint('MSE:', mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))\nprint('R2 test_data', r2_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Lazy implementation gave around 69% accuracy which is not that great."},{"metadata":{"trusted":true,"_uuid":"27ad48d686f546f7ec56b7a253fa03ad76ee2405"},"cell_type":"code","source":"sns.distplot((y_test-predictions),bins=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can normalize the data and recalculate the predictions but I think RandomForestRegressor should do better in this scenario as it uses bagging and the summation of results from different forests."},{"metadata":{"trusted":true,"_uuid":"404357d526d615613f155c6a91a81fe1b27ffc8a"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators =100, criterion = 'mse',random_state = 42,n_jobs = -1)\nrfr.fit(X_train,y_train)\nrfr_pred_train = rfr.predict(X_train)\nrfr_pred_test = rfr.predict(X_test)\n\n\nprint('MSE train_data: ', round((mean_squared_error(y_train,rfr_pred_train)), 1))\nprint('MSE test_data: ', round(mean_squared_error(y_test,rfr_pred_test), 1))\nprint('R2 train_data: ',round(r2_score(y_train,rfr_pred_train), 2))\nprint('R2 test_data: ', round(r2_score(y_test,rfr_pred_test), 2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 87% accuracy is pretty good given that we haven't normalized the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot((y_test - rfr_pred_test),bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}