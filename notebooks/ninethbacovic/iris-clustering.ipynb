{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The source of this type of study is from: https://medium.com/pizzadedados/kmeans-e-metodo-do-cotovelo-94ded9fdf3a9"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\niris = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the classic iris dataset with 4 features that can help to classify the specie of flower. I am going to take out the last column of species and check how many clusters (species) I could find with the K-means model. This is a study notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"iris['species'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df = iris.drop(columns = 'species')\niris_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix\niris_df.corr().style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap(\"coolwarm\"), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest correlation is between the petal_length and the petal_width, the second is the petal_length and the sepal_length"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(iris_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These charts confirm the high correlation between the petal_width and the petal_length"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the model and use the default number of clusters\nkmeans = KMeans(random_state = 42)\nkmeans.fit(iris_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the centroids value\ncentroids = kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make the prediction\ny_kmeans = kmeans.predict(iris_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Elbow Method\nlist_clusters = list(range(1,9)) #until the kmeans default clusters number\ninertia = []\nfor number_clusters in list_clusters:\n    kmeans = KMeans(n_clusters = number_clusters, random_state = 42)\n    kmeans.fit(iris_df)\n    inertia.append(kmeans.inertia_)\n\n#Plot the elbow chart\nplt.plot(list_clusters, inertia, marker = \"+\", linestyle = 'solid', mec = 'red' )\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this chart we can notice that if we choose one cluster (it doesn't make sense, but just for the analysis) the inercia value is  disparate and higher than 2 clusters set, it means that the data is sparse. The goal of clustering is to have the lower inertia for having more compact clusters.\nJust to remember --> Inertia: it is the within-clusters sum-of-squares distance."},{"metadata":{},"cell_type":"markdown","source":"Which number of clusters is the best for our data? Just looking at the elbow method, It seems that 2 or 3 clusters are good, but it is better to make another aproach to choose this number\nLet's try to find the optimal number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Elbow Method STARTING WITH 2 CLUSTERS\nlist_clusters = list(range(2,9)) #until the kmeans default clusters number\ninertia = []\nfor number_clusters in list_clusters:\n    kmeans = KMeans(n_clusters = number_clusters, random_state = 42)\n    kmeans.fit(iris_df)\n    inertia.append(kmeans.inertia_)\n\n#Plot the elbow chart\nplt.plot(list_clusters, inertia, marker = \"+\", linestyle = 'solid', mec = 'red' )\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it seems that 3 or 4 clusters are the ideal number."},{"metadata":{},"cell_type":"markdown","source":"The ideal point where we will find the unity within clusters and the biggest difference between them it is the elbow point further from the straight line created by the points with the minimum and maximum number of clusters that we choose before (2 and 8). The formula of the distance between a line and a point is: https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Straight line between 2 clusters and 8 clusters\nx0, y0 = 2, inertia[0]\nx1, y1 = 8, inertia[-1]\nX = [x0, x1]\nY = [y0, y1]\n#Plot the elbow chart\nplt.plot(list_clusters, inertia, marker = \"+\", linestyle = 'solid', mec = 'red' )\nplt.plot(X, Y)\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.legend(['Elbow method', 'Straight line between clusters'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make the distance function and decide the best number of clusters\nfrom math import sqrt\ndef number_of_clusters(list_clusters, inertia):\n    distances = []\n    dictionary = {}\n    x0, y0 = list_clusters[0], inertia[0]\n    x1, y1 = list_clusters[-1], inertia[-1]\n    i = 0\n    for i in range(len(list_clusters)):\n        x = list_clusters[i]\n        y = inertia[i]\n        a = abs((y1-y0)*x - (x1 - x0)*y + (x1*y0) - (y1*x0))\n        b = sqrt((y1-y0)**2 + (x1-x0)**2)\n        distances.append(a/b)\n        dictionary = dict(zip(distances, list_clusters))\n        i +=1     \n    return print('Optimal number of clusters is:', dictionary.get(max(dictionary.keys())), \"\\nDistances between the point and the straight line: \", distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_clusters(list_clusters, inertia)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the model again but with n_clusters = 4 and adding a new column to the DataFrame\nkmeans = KMeans(n_clusters = 4, random_state = 42)\nkmeans.fit(iris_df)\niris_df['clusters'] = kmeans.predict(iris_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df['clusters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(iris_df, hue = 'clusters')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 0 --> It is really a separated group with the lower values of petal_width, petal_length.\nCluster 1, 2, 3 --> They are mixed and in scale, the values of cluster 1 are lower for petal_width and petal_length than the others and so on.\nLet's compare these result with the real classification of the real iris dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cluster chart of the  petal_length and the petal_width\nplt.subplots(figsize = (12,7))\nplt.subplot(1,2,1)\nplt.title('K-means result')\nplt.scatter(iris_df['petal_length'], iris_df['petal_width'], c = iris_df['clusters'], cmap = 'winter')\nplt.xlabel('petal_length')\nplt.ylabel('petal_width')\n\nplt.subplot(1,2,2)\nplt.title('Real classification')\nplt.scatter(iris['petal_length'], iris['petal_width'], c = iris['species'].replace(iris['species'].value_counts().index, [0,1,2]), cmap = 'winter')\nplt.xlabel('petal_length')\nplt.ylabel('petal_width')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, our model found more clusters than the real dataset. It happened because the point distances of 3 clusters and 4 clusters from the straight line were very similar (2.599709771898713, 2.66059664173226), and the inertia value of 4 clusters is less than 3 clusters.\nIf we pay attention to the number of samples inside each cluster, the cluster 0 has 50, the cluster 1 has 40, the cluster 2 has 32, the cluster 3 has 28 - from that info when we divide the dataset into a cluster we expect to have similar numbers of samples, and the clusters 2 and 3 have less samples than the others. \nMaybe, if we choose more clusters to do the elbow method, the straight line and the point position could change and the result clould be closer to the real. But the majority of the unsupervised models will not have the \"answer\" as we have here, and the next part of the method could be study more about these clusters 2 and 3 and see if they really make sense to belong to theses clusters or its better to join them somehow."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}