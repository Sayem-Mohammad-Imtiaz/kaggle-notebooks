{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Objective: Examine why its customers have left in the past and which features are more important to determine who will churn in the future.","metadata":{}},{"cell_type":"markdown","source":"### 1. Set up environment and import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport sklearn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Exploratory Data Analysis\nIn this section, the followings will be done:\n- Looking for missing values\n- See if there is any outliers\n- Distribution of Churning Rate due to different features\n- Remove unused columns","metadata":{}},{"cell_type":"markdown","source":"#### Looking for missing values","metadata":{}},{"cell_type":"code","source":"# Look for missing values\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total Charges is default in object type, so we need to change it back to float type.","metadata":{}},{"cell_type":"code","source":"df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove all 11 rows with missing values in TotalCharges","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True,drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look for Outliers\ndf.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tenure']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = ['tenure','MonthlyCharges','TotalCharges']\nfig, axes = plt.subplots(1,3,figsize=(10,5))\nfor i in num_features:\n    sns.boxplot(ax=axes[num_features.index(i)],data=df[i],palette='Set2').set_title(i)\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Look for Outliers","metadata":{}},{"cell_type":"markdown","source":"To check for outliers, we define outliers as beyond the range of +/-1.5 IQR and see if there is any datas lying beyond the range","metadata":{}},{"cell_type":"code","source":"# Create a function to find outlier\ndef iqr_outliers(num_features):\n    outlier_position=[]\n    for i in num_features:\n        q1 = df[num_features].quantile(0.25)[i]\n        q3 = df[num_features].quantile(0.75)[i]\n        iqr = q3-q1\n        Lower_tail = q1 - 1.5 * iqr\n        Upper_tail = q3 + 1.5 * iqr\n        for j in df[i]:\n            if j > Upper_tail or j < Lower_tail:\n                outlier_position.append(i)\n    print(\"Outliers:\",outlier_position)\niqr_outliers(num_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It shows that no outliers in our dataset.","metadata":{}},{"cell_type":"markdown","source":"#### Remove unused column","metadata":{}},{"cell_type":"code","source":"# Remove unused column\ndf.drop(columns='customerID',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distribution of Churning Rate","metadata":{}},{"cell_type":"code","source":"plt.pie(df['Churn'].value_counts(),labels=df['Churn'].unique(),autopct='%1.1f%%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the pie chart above, the ratio of churning rate (yes/no) is nearly 3:1, if we use the data for training directly, our model will predict customer will not churn due to the dominating data of not churning. To deal with such imbalance data set, data augmentation skills has to be used. I will demonstrate undersampling in the data preprocessing part of this project.","metadata":{}},{"cell_type":"code","source":"category_features = df.loc[:, ~df.columns.isin(num_features)]\ncategory_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_categoricals(columns, title):\n    fig, axs = plt.subplots(ncols=2, nrows=int(len(columns) / 2) + len(columns) % 2)\n    fig.set_size_inches(15, 45)\n\n    row = col = 0\n    for column in columns:\n        plot_title = '{}: {}'.format(title, column)\n        sns.countplot(x=column, hue=\"Churn\", data=category_features, ax=axs[row][col]).set_title(plot_title)\n\n        if col == 1:\n          col = 0\n          row += 1\n        else:\n          col += 1\n\n    # this prevents plots from overlapping\n    plt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_categoricals(category_features.columns.to_list(), 'Demographic Variables')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, we can have some insights:\n- Overall, it is an highly imbalanced data\n- The ratio of churning rate is more or less the same on each gender, so gender is not considered to be an important factor\n- Internet Service: there are relatively more people having fiber optic internet service. At the same time, the ratio of churning rate is much higher in this category. \n- Onliine Security: there are relatively more people not acquiring online security service. At the same time, the ratio of churning rate is much higher in this category. \n- Online Backup: there are relatively more people not having online backup serviece. Again, the ratio of churning rate is much higher in this category.\n- Tech Support: there are relatively more people not having tech support serviece. Again, the ratio of churning rate is much higher in this category.\n- Contract: most of people using telco service are based on month-to-month contract. The ratio of churning rate is much higher in this category.\n- Payment Method: most of people using telco service are using electronic check to pay. The ratio of churning rate is much higher in this category.\n- Among the observations in differnent factors, we can conclude there may be some reasons relating to the follow-up services such as the poor service of fibre optic etc pushing people to churn. We may investigate more to see if there are some groups of people having similar pattern and give advice to specific cluster for the sake of pulling them back.","metadata":{}},{"cell_type":"markdown","source":"### 3. Data Preprocessing\nIn this section, the followings will be done:\n- Feature Scaling\n- Encoding\n- Dealing with imbalanced dataset","metadata":{}},{"cell_type":"markdown","source":"#### Feature Scaling\n\nAmong all features, we have three numerical features: tenure, MonthlyCharges, TotalCharges. From the boxplots shown above, as they have vast difference in range of values, we will adopt min-max scaler to limit their range from 0 to 1","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\ndf[num_features] = scaler.fit_transform(df[num_features])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Churn'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding\n\nIn this case, we will use one-hot encoding for those categorical features with multiple labels","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['gender'] = df['gender'].replace({'Male':1,'Female':0})\ndf['Partner'] = df['Partner'].replace({'Yes':1,'No':0})\ndf['Dependents'] = df['Dependents'].replace({'Yes':1,'No':0})\ndf['PhoneService'] = df['PhoneService'].replace({'Yes':1,'No':0})\ndf['PaperlessBilling'] = df['PaperlessBilling'].replace({'Yes':1,'No':0})\ndf['Churn'] = df['Churn'].replace({'Yes':1,'No':0})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = [\n    'MultipleLines', \n    'InternetService', \n    'OnlineSecurity',\n    'OnlineBackup', \n    'DeviceProtection', \n    'TechSupport', \n    'StreamingTV',\n    'StreamingMovies', \n    'Contract',\n    'PaymentMethod'\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder(drop='first')\nencoded_df =onehotencoder.fit_transform(df[categorical_features]).toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df = pd.DataFrame(encoded_df, columns=onehotencoder.get_feature_names(categorical_features))\nencoded_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df,encoded_df],axis=1)\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df.drop(categorical_features,axis=1)\ndf2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['Churn'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dealing with imblanced dataset\n\nIn this case, we will use undersampling to reduce the data as long as the churing ratio of yes to no is 50:50","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df2['Churn'] # target\nX = df2.drop(columns='Churn') # all features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_resampled.shape)\nprint(y_resampled.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To confirm our dataset is balance, we plot a pie chart for illustration","metadata":{}},{"cell_type":"code","source":"plt.pie(y_resampled.value_counts(),labels=y_resampled.value_counts().index,autopct='%1.1f%%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Model Training\n\nIn this section, the followings will be done:\n- Train-test Split\n\nFor the classification model, we will use machine learning algorithms listed as follows:\n- Logistic Regression\n- Random Forest\n- KNN\n- XGBoost\n\nTo evaluate their performance, we will focus on the recall score and try to minimum the false negative (we predict customers not going to churn but they actually churn)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train-test split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=41)\nprint('Train data stroke count')\ndisplay(y_train.value_counts())\nprint('Test data stroke count')\ndisplay(y_test.value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# GridSearchCV\nlog_clf = Pipeline([\n#    ( 'column-onehot', col_trans ),\n    ( 'classifier', LogisticRegression() )\n])\nhyperparams = { \n    'classifier__C': np.linspace(0.0001, 0.01, 50),\n    'classifier__max_iter': range(80, 111)\n}\n\nlog_search = GridSearchCV(log_clf,  hyperparams, n_jobs = -1,cv=5, verbose=1)\nlog_search.fit(X_train, y_train)\ny_pred = log_search.predict(X_test)\nprint(\"Best params\", log_search.best_params_)\nprint(\"Best score\", log_search.best_score_)\nlog_C = log_search.best_params_['classifier__C']\nlog_max_iter = log_search.best_params_['classifier__max_iter']\n# print(classification_report(y_test, y_pred))\n# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n# print([tp,fp])\n# print([fn,tn])\n# log_score = recall_score(y_test,y_pred)\n# print('recall score: {}'.format(log_score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest","metadata":{}},{"cell_type":"code","source":"forest_clf = Pipeline([\n    ( 'classifier', RandomForestClassifier() )\n])\nhyperparams = { \n    'classifier__n_estimators': [100,200],\n    'classifier__max_depth': [2,6,8,10],\n    'classifier__max_leaf_nodes': [10,20,30,40,50,60,70,80,90100]\n\n}\n\nforest_search = GridSearchCV(forest_clf,  hyperparams, n_jobs = -1,cv=5,verbose=1)\nforest_search.fit(X_train, y_train)\ny_pred = forest_search.predict(X_test)\nprint(\"Best params\", forest_search.best_params_)\nprint(\"Best score\", forest_search.best_score_)\nforest_n_estimators = forest_search.best_params_['classifier__n_estimators']\nforest_max_depth = forest_search.best_params_['classifier__max_depth']\nforest_max_leaf_nodes = forest_search.best_params_['classifier__max_leaf_nodes']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KNN","metadata":{}},{"cell_type":"code","source":"knn_clf = Pipeline([\n    ( 'classifier', KNeighborsClassifier() )\n])\nhyperparams = { \n    'classifier__n_neighbors': np.arange(1,100,2),\n    'classifier__weights': ['distance']\n}\n\nknn_search = GridSearchCV(knn_clf,  hyperparams, n_jobs = -1)\nknn_search.fit(X_train, y_train)\ny_pred = knn_search.predict(X_test)\nprint(\"Best params\", knn_search.best_params_)\nprint(\"Best score\", knn_search.best_score_)\nn_neighbors = knn_search.best_params_['classifier__n_neighbors']\nknn_weights = knn_search.best_params_['classifier__weights']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_clf = Pipeline([\n    ( 'classifier', xgb.XGBClassifier(\n        booster='gbtree',\n        learning_rate=0.3,\n        base_score=0.5,\n        colsample_bylevel=1, \n        colsample_bytree=1, \n        gamma=0,\n        reg_alpha=0,\n        random_state=40\n        \n    ) \n    )\n])\nhyperparams = { \n    'classifier__n_estimators': np.arange(500,800,50),\n    'classifier__max_depth':[2,6,8,10],\n    \n}\n\nxgb_search = GridSearchCV(xgb_clf,  hyperparams, n_jobs = -1,cv=5,verbose=2)\nxgb_search.fit(X_train, y_train)\ny_pred = xgb_search.predict(X_test)\nprint(\"Best params\", xgb_search.best_params_)\nprint(\"Best score\", xgb_search.best_score_)\nxgb_n_estimators = xgb_search.best_params_['classifier__n_estimators']\nxgb_max_depth = xgb_search.best_params_['classifier__max_depth']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"algorithm = ['LogisticRegression','KNeighborsClassifier','RandomForestClassifier','XGBClassifier']\nhyperparameters = [\n    LogisticRegression(\n        C = log_C, \n        max_iter=log_max_iter\n    ), \n    KNeighborsClassifier(\n        n_neighbors = n_neighbors, \n        weights = knn_weights\n    ),\n    RandomForestClassifier(\n        n_estimators = forest_n_estimators,\n        max_depth = forest_max_depth,\n        max_leaf_nodes = forest_max_leaf_nodes\n    ),\n    xgb.XGBRFClassifier(\n        n_estimators = xgb_n_estimators,\n        max_depth = xgb_max_depth\n    )\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=dict(zip(algorithm,hyperparameters))\nprint(models)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_score\nacc_score_list =[]\nrecall_score_list=[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    acc_score_list.append(acc)\n    recall_score_list.append(recall)\n    print(name,acc)\n    print(classification_report(y_test, y_pred))\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    print([tp,fp])\n    print([fn,tn])\n    ax = plt.axes()\n    ax.set_title(name)\n    plt.figure(figsize=(10,5))\n    cm_data = [tp, fp], [fn, tn]\n    conf_matrix = pd.DataFrame(data=cm_data,columns=['Predicted: No Churn','Predicted: Churn'],index=['No Churn','Churn'])\n    sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"Blues\",ax=ax)\n    plt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(acc_score_list)\nplt.figure(figsize = (10,5))\nsns.barplot(x = acc_score_list, y = algorithm , palette='pastel')\nplt.title(\"Model Accuracies Score\", fontsize=16, fontweight=\"bold\")\n\nprint(recall_score_list)\nplt.figure(figsize = (10,5))\nsns.barplot(x = recall_score_list, y = algorithm , palette='pastel')\nplt.title(\"Model Recall Score\", fontsize=16, fontweight=\"bold\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, KNN did the best job among four ML models.","metadata":{}},{"cell_type":"markdown","source":"### 5. Feature Importance evaluation","metadata":{}},{"cell_type":"markdown","source":"To go further, we are going to investigate on each feature importance to see which features have great influence on the prediction and remove the rest of them","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(\n        n_neighbors = 65, \n        weights = 'distance',\n    )\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint([tp,fp])\nprint([fn,tn])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nimportance = permutation_importance(model, X_train, y_train, scoring='recall')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_score = importance['importances_mean'].tolist()\nfeature = X_train.columns.values.tolist()\nfeature_importance = {'Features':feature,'Score':importance_score}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df = pd.DataFrame(feature_importance)\nfeature_importance_df.sort_values('Score',ascending=False,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['Features'].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The top 10 important features affecting the recall score are:\n1. Monthly Charges\n2. Tenure\n3. Total Charges\n4. InternetService_Fiber optic\n5. StreamingMovies_Yes\n6. MultipleLines_Yes\n7. Partner\n8. StreamingTV_Yes\n9. gender\n10. OnlineBackup_Yes","metadata":{}},{"cell_type":"markdown","source":"### 6. Recommendation\n\n#### In view of the top 10 features, Telco Company could review their service in three aspects:\n\n#### 1. Service Charges\nWhether they fine tune the price or offer discounts/bundle_price in a long term contract\n\n#### 2. Connection Stability of Service\nThey should review the connection stability of internet service espically for users connecting via optic fibre.\n\n#### 3. Choice of Streaming Channel\nReviewing the selection/choices of both Streaming TV and Movies seems to be one of the directions to think of. They may also give special offers to those in pairs having lower price to watch Streaming TV/Movies.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}