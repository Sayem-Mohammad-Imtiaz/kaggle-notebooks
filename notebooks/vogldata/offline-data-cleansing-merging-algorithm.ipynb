{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n\"\"\" This contribution is submitted by Markus Vogl of Markus Vogl {Business & Data Science}\nas part of the lecture \"Data Science for algorithmic financial markets & time-series analysis\" \nat the University of Applied Sciences Aschaffenburg Germany\n(Technische Hochschule Aschaffenburg-Deutschland). \nMarkus Vogl is part of the Behavioral Finance & Accounting Lab and a PhD candidate\nin financial & risk modelling and Chaos Theory.\nPlease note that the contribution to kaggle will not be all encompassing and at a lower level\nsince the major aim is to focus on basic applications of ML & Data Science for the lecture. \nNevertheless, we try to contribute a respective level of data analysis.\nNote, that the rights of the course and the code belongs to Markus Vogl. \n\nYou can visit the Lab at: \nhttps://www.th-ab.de/ueber-uns/organisation/labor/behavioral-accounting-and-finance-lab/\n\nOur Data Science operation at: \nhttps://vogl-datascience.de/\n\nOur full lecture on YouTube at:\nhttps://www.youtube.com/playlist?list=PLFXw4NpfUWMi4enJ2_jtKjSwHPhnNbd2G\n\nFurther, the lecture and code fall under an CC-BY-NC-SA 3.0 DE license.\n\nIf you are interested in our research, please contact us anytime. \n\nThis is a Python 3 environment.\"\"\"\n\n#Import relevant packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom tabulate import tabulate as tabu \nfrom functools import reduce\n\n#set path: Please note that we will not loop through the files to save lines of code, \n#due to the capability of our students to replicate the results.\nt_path = \"../input/fatal-police-shootings-in-the-us/\"\n\n#Import relevant data sets: Please note, that we will use the raw data sets as provided \n#via the kaggle download: https://www.kaggle.com/kwullum/fatal-police-shootings-in-the-us\n\nt_1 = pd.read_csv(t_path + \"MedianHouseholdIncome2015.csv\", \n                  sep=\",\", encoding=\"unicode_escape\")\n\nt_2 = pd.read_csv(t_path + \"PercentagePeopleBelowPovertyLevel.csv\", \n                  sep=\",\", encoding=\"unicode_escape\")\n\nt_3 = pd.read_csv(t_path + \"PercentOver25CompletedHighSchool.csv\", \n                  sep=\",\", encoding=\"unicode_escape\")\n\nt_4 = pd.read_csv(t_path + \"ShareRaceByCity.csv\", \n                  sep=\",\", encoding=\"unicode_escape\")\n\n\"\"\" Merge t_1 to t_4 into one df:\n(1) Define a list of all dfs to be merged\n(2) use reduce function and lambda statement to conduct inner join based on City column\n(3) drop redundant columns \n(4) rename some columns for consistency\"\"\"\n\nt_data_frames = [t_1, t_2, t_3, t_4]\n\ndf_living_information_data = reduce(lambda left,right: pd.merge(left,right,\n                                    on=['City'],how='inner'),\n                                    t_data_frames)\n\ndf_living_information_data.drop(\n                                [\"Geographic Area_y\",\"Geographic Area\", \n                                 \"Geographic area\"], \n                                axis=1, inplace=True)\n\ndf_living_information_data.rename(columns={ #beginning of dict\n                                            \"Geographic Area_x\":\"geographic_area\",\n                                            \"Median Income\":\"median_income\", \n                                            \"City\":\"city\"\n                                            }, #end of dict\n                                            inplace=True)    \n\n# print the heads of the two dfs: We see the raw data is not consistent, since \n# several cities display different other parameter realisations. \n#print(tabu(df_living_information_data.head(), missingval=\"?\", tablefmt=\"simple\",\n#          headers=df_living_information_data.columns), \"\\n\", \"\\n\")\n#print(tabu(df_police_killings_data.head(), missingval=\"?\", tablefmt=\"simple\", \n#          headers=df_police_killings_data.columns), \"\\n\", \"\\n\")\n\n\"\"\" We will now count cities and will replace the duplicates via the mean values \nof all delivered values. Since we do not have an extraction date,we cannot \ndetermine the latest entry dates.\nWe do it as follows:\n(1) Create combined string of geographic_area and city to ensure doubles are \nreally doubles and not same named cities in different areas\n(2) check for missing values\n(3) create unique city_area combinations in new df\n(4) split the combination again\n(5) fill in the mean values of the doubles\"\"\"\n\n#create (1)\ndf_living_information_data[\"cat_city\"] = df_living_information_data[\"geographic_area\"] + df_living_information_data[\"city\"]\n\n#display boolean matrix, where Nulls are True == 1 and values are False == 0: We only have one missing household income;\n#therefore we drop the row accordingly.\n#sns.heatmap(df_living_information_data.isnull(), cmap=\"viridis\")\n#plt.show()\n\n#drop NaNs as in (2)\ndf_living_information_data.dropna(axis=0,inplace=True)\n\n# Since no row is complete (e.g. (X) blanks)!!! We cannot solve the issue with this approach:\n#df_living_information_data = df_living_information_data[df_living_information_data.applymap(np.isreal).all(1)]\n#print(df_living_information_data.head(25), \"\\n\", len(df_living_information_data))\n\n# check max Length of geographic_area IDs and create (3)\n#print(max(df_living_information_data[\"geographic_area\"].str.len()))\nt_cat_city = list(dict.fromkeys(df_living_information_data[\"cat_city\"].to_list()))\ndf_cleansed_living_information_data = pd.DataFrame(columns=df_living_information_data.columns, dtype=float)\n\n# fill new df with unique values before calculation of means and set cat_city2 as new index of df\ndf_cleansed_living_information_data[\"cat_city\"] = t_cat_city\ndf_cleansed_living_information_data[\"cat_city2\"] = t_cat_city\ndf_cleansed_living_information_data[\"geographic_area\"] = df_cleansed_living_information_data[\"cat_city\"].str[:2]\ndf_cleansed_living_information_data[\"city\"] = df_cleansed_living_information_data[\"cat_city\"].str[2:]\ndf_cleansed_living_information_data= df_cleansed_living_information_data.set_index([\"cat_city2\"])\n\n#since the dataset is inconsistent, redundant and has wrong values (\"shame to the US departements...poor job!\")\n# we clean the data on city-level! meaning, we take every cat_city and erase the errors and calculate the mean\n# of every column as part of feature engineering.\n\nt_cols = df_living_information_data.columns\n#print(t_cols)\nprint(\"Loop started.\")\n\n# each loop picks a column and calculates the means and appends it to the cleaned df\nfor cols in range(2,len(t_cols)-1):\n\n    t_dict = {}\n  \n    t_df = pd.DataFrame(columns=[\"cat_city\",str(t_cols[cols])], dtype=float)\n    t_df[\"cat_city\"] = df_living_information_data[\"cat_city\"]\n    t_df[str(t_cols[cols])] = df_living_information_data[str(t_cols[cols])]\n\n    for cat_city in df_cleansed_living_information_data[\"cat_city\"]:\n\n        if len(t_df[\"cat_city\"]) > 1:\n\n            tt_df = t_df[t_df[\"cat_city\"]==cat_city]\n            t_errors = (tt_df == \"(X)\") | (tt_df == \"-\") | (tt_df == \"2,500-\") | (tt_df == \"250,000+\")\n            tt_df = tt_df[t_errors==False]\n            \n            tt_df[str(t_cols[cols])] = pd.to_numeric(tt_df[str(t_cols[cols])]).mean()\n            t_dict[str(cat_city)] = tt_df[str(t_cols[cols])].unique()\n\n    t_dict_series = pd.DataFrame.from_dict(t_dict, orient=\"index\")\n    df_cleansed_living_information_data[str(t_cols[cols])]= t_dict_series[0]\n\n# Now we save the cleansed data for further processing in an excel notebook.\n# Code will continue in script file 2. \ndf_cleansed_living_information_data.to_excel(\"../input/output/df_cleansed.xlsx\", \n                                            sheet_name='US_Shootings', index = True)\n\nprint(\"Completed!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}