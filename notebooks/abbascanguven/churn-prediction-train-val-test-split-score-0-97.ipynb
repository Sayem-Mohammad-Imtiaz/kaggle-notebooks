{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Ä°mport sacred libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro\nfrom sklearn import preprocessing\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CLIENTNUM** = Client number. Unique identifier for the customer holding the account\n\n**Attrition_Flag** = Internal event (customer activity) variable - if the account is closed then 1 else 0\n\n**Customer_Age** = Demographic variable - Customer's Age in Years\n\n**Gender** = Demographic variable - M=Male, F=Female\n\n**Dependent_count** = Demographic variable - Number of dependents\n\n**Education_Level** = Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n\n**Marital_Status** = Demographic variable - Married, Single, Divorced, Unknown\n\n**Income_Category** = Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, $120K >)\n\n**Card_Category** = Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n\n**Months_on_book** = Period of relationship with bank\n\n**Total_Relationship_Count** = Total no. of products held by the customer\n\n**Months_Inactive_12_mon** = No. of months inactive in the last 12 months\n\n**Contacts_Count_12_mon** = No. of Contacts in the last 12 months\n\n**Credit_Limit** = Credit Limit on the Credit Card\n\n**Total_Revolving_Bal** = Total Revolving Balance on the Credit Card\n\n**Avg_Open_To_Buy** = Open to Buy Credit Line (Average of last 12 months)\n\n**Total_Amt_Chng_Q4_Q1** = Change in Transaction Amount (Q4 over Q1)\n\n**Total_Trans_Amt** = Total Transaction Amount (Last 12 months)\n\n**Total_Trans_Ct** = Total Transaction Count (Last 12 months)\n\n**Total_Ct_Chng_Q4_Q1** = Change in Transaction Count (Q4 over Q1)\n\n**Avg_Utilization_Ratio** = Average Card Utilization Ratio"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\ndf = data[data.columns[:-2]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Let's first look at the data*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing_Values(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Income_Category\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Income_Category'].replace({'$60K - $80K': \"60k_80k\", 'Less than $40K': \"0_40k\", '$80K - $120K': \"80k_120k\",\n                                        '$40K - $60K': \"40k_60k\", \"$120K +\": \"120k_plus\"}, inplace = True)\ndf.drop(\"CLIENTNUM\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Income_Category\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_function(val):\n    return f'{val / 100 * len(df):.0f}\\n{val:.0f}%'\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 10))\ndf.groupby(\"Attrition_Flag\").size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 20},\n                                  colors=['tomato', 'gold', 'skyblue'],ax=ax1)\n\ndf.groupby(\"Gender\").size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 20},\n                                  colors=['violet', 'lime'],ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(15, 15), sharey=True)\nfig.suptitle('Group by Attrition Flag')\nsns.countplot(x=\"Gender\", hue = \"Attrition_Flag\",  data=df, ax=axes[0,0], palette=\"Set2\")\naxes[0,0].set_title(\"GENDER & ATTRITION FLAG\")\n\nsns.countplot(x=\"Income_Category\", hue = \"Attrition_Flag\",  data=df, ax=axes[0,1], palette=\"Set2\")\naxes[0,1].set_title(\"INCOME CATEGORY & ATTRITION FLAG\")\n\nsns.countplot(x=\"Education_Level\", hue = \"Attrition_Flag\",  data=df, ax=axes[1,0], palette=\"Set2\")\naxes[1,0].set_title(\"EDUCATION LEVEL & ATTRITION FLAG\")\n\nsns.countplot(x=\"Card_Category\", hue = \"Attrition_Flag\",  data=df, ax=axes[1,1], palette=\"Set2\")\naxes[1,1].set_title(\"CARD CATEGORY & ATTRITION FLAG\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing_Values(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize = (5,8))\nsns.boxplot(y=df[\"Total_Ct_Chng_Q4_Q1\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize = (5,8))\nsns.boxplot(y=df[\"Total_Trans_Ct\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(df['Customer_Age'], shade=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"Customer_Age\", 'Dependent_count', 'Months_on_book',\n       'Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\nfor column_name in columns:\n    Q1 = df[column_name].quantile(0.25)\n    Q3 = df[column_name].quantile(0.75)\n    IQR = Q3 - Q1\n    df = df[~((df[column_name] < (Q1 - 3 * IQR)) |(df[column_name] > (Q3 + 3 * IQR)))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were 10127 row observations before outlier cleanup. We currently have 9240 number of row"},{"metadata":{},"cell_type":"markdown","source":"### Normality test "},{"metadata":{"trusted":true},"cell_type":"code","source":"def shapiro_wilk(data):\n    columns = df.columns\n    int_columns = []\n    normality_column_name = []\n    normality_statistic = []\n    normality_result = []\n    for column_type in columns:\n        if df[column_type].dtypes == \"int64\" or df[column_type].dtypes == \"float64\":\n            int_columns.append(column_type)\n    for column in int_columns:\n        stat, p = shapiro(data[column])\n        statistic = ('Statistics=%.3f, p=%.3f' % (stat, p))\n        #print('Statistics=%.3f, p=%.3f' % (stat, p))\n        # interpret\n        alpha = 0.05\n        if p > alpha:\n            normality_column_name.append(column)\n            normality_statistic.append(statistic)\n            normality.append(\"Sample looks Gaussian\")\n                    \n        else:\n            normality_column_name.append(column)\n            normality_statistic.append(statistic)\n            normality_result.append(\"Sample does not look Gaussian\")\n        \n    normality_column_name = pd.DataFrame(normality_column_name, columns =['Column Name'])\n    normality_statistic = pd.DataFrame(normality_statistic, columns =['Statistic']) \n    normality_result = pd.DataFrame(normality_result, columns =['Result']) \n\n\n    result = pd.concat([normality_column_name, normality_statistic, normality_result], axis=1, sort=False)\n    \n    return result ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro_wilk(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df.corr()\nfig, ax = plt.subplots(figsize=(10, 8))\nmask = np.triu(np.ones_like(df_corr, dtype=np.bool))\n# mask = mask[1:, :-1]\n# corr = df_corr.iloc[1:,:-1].copy()\nsns.heatmap(df_corr, mask = mask, annot = True, fmt = \".2f\", cmap = sns.diverging_palette(240, 10, n=19),\n           vmin = -1, vmax = 1, cbar_kws = {\"shrink\": .8})\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThere is high correlation between Avg Open To buy and Credit Limit\n\nCredit_Limit = Credit Limit on the Credit Card\n\nAvg_Open_To_Buy = Open to Buy Credit Line (Average of last 12 months)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"Credit_Limit\", \"Avg_Open_To_Buy\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"Credit_Limit\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAIN - VALIDATION - TEST Split"},{"metadata":{},"cell_type":"markdown","source":"prepare the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition_Flag'] = df['Attrition_Flag'].map({'Attrited Customer':1, 'Existing Customer':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_dummies.drop(\"Attrition_Flag\", axis = 1)\ny = df_dummies[\"Attrition_Flag\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train - Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Train - Validation Split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)\n\n# Test = 0.20\n# Validation = 0.20\n# Train = 0.60\n\nx_train_shape = X_train.shape\nx_val_shape = X_val.shape\nx_test_shape = X_test.shape\nprint(\"X_train shape = {}\\nX_val shape = {}\\nX_test shape = {}\".format(x_train_shape, x_val_shape, x_test_shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeller = []\nmodelName = []\n\n        \ndef dictforloop(index,accuracy,recall,f1):\n    dictForPlot = []\n    dictForList =[]\n    \n    dictForPlot.append({\"Model_Name\":index,\"ScoreType\":\"accuracy\",\"Score\":accuracy})\n    dictForPlot.append({\"Model_Name\":index,\"ScoreType\":\"Recall_Score_0\",\"Score\":recall[0]}) \n    dictForPlot.append({\"Model_Name\":index,\"ScoreType\":\"Recall_Score_1\",\"Score\":recall[1]})\n    dictForPlot.append({\"Model_Name\":index,\"ScoreType\":\"F1_Score_0\",\"Score\":f1[0]}) \n    dictForPlot.append({\"Model_Name\":index,\"ScoreType\":\"F1_Score_1\",\"Score\":f1[1]})\n    \n    dictForList = {\"Model_Name\":index,\"Accuracy\":accuracy,\"Recall_Score_0\":recall[0],\n              \"Recall_Score_1\":recall[1],\"F1_Score_0\":f1[0],\"F1_Score_1\":f1[1]}\n    \n    return dictForPlot,dictForList\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)    \ndef importance_plot(model_name, thresh_hold):\n    Importance=pd.DataFrame({\"Importance\":model_name.feature_importances_*100},\n                       index=X_train.columns)\n    importance = Importance.sort_values(by=\"Importance\",\n                      axis=0,ascending=False).iloc[0:10,:]\n    importance.sort_values(by=\"Importance\",\n                      axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\n    plt.xlabel(\"DeÄiÅken Ãnem DÃ¼zeyleri\")\n\n\n    a = Importance.sort_values(\"Importance\", ascending = False)\n    display_all(a[a['Importance']>thresh_hold])\n\ndef conf_mtrx(y_test, y_pred, model): \n\n    cm = confusion_matrix(y_test,y_pred)    \n    f, ax = plt.subplots(figsize =(5,5))\n    cm = confusion_matrix(y_test,y_pred)\n    sns.heatmap(cm,annot = True, linewidths=0.5, linecolor=\"red\",fmt = \".0f\",ax=ax)#,cmap=plt.cm.RdPu\n    plt.xlabel(\"predicted y values\")\n    plt.ylabel(\"real y values\")\n    plt.title(\"\\nConfusion Matrix\")\n    plt.show()\n    \ndef rc_recis_scres(y_test, y_pred, algorithm_name):\n    from sklearn.metrics import recall_score,precision_score,accuracy_score, f1_score\n\n    rs=recall_score(y_test, y_pred) \n    ps=precision_score(y_test, y_pred)\n    f1=f1_score(y_test, y_pred)\n\n    print()    \n\ndef roc_auc_curve_plot(model_name, X_testt, y_testt):\n    ns_probs = [0 for _ in range(len(y_testt))]\n    # fit a model\n    # predict probabilities\n    model_probs = model_name.predict_proba(X_testt)\n    # keep probabilities for the positive outcome only\n    model_probs = model_probs[:, 1]\n    # calculate scores\n    ns_auc = roc_auc_score(y_testt, ns_probs)\n    lr_auc = roc_auc_score(y_testt, model_probs)\n    # summarize scores\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    print( ': ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y_testt, ns_probs)\n    model_fpr, model_tpr, _ = roc_curve(y_testt, model_probs)\n    # plot the roc curve for the model\n    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    pyplot.plot(model_fpr, model_tpr, marker='.', label='Logistic')\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show() \n    \ndef plot_roc_curve(fpr, tpr):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr,tpr, label = roc_auc_score) \n    plt.plot([0,1],ls='--')\n    plt.plot([0,0],[1,0],c='.5')\n    plt.plot([1,1],c='.5')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()   \n    \ndef ML_Algorithms(model,alg_name, x_trainn,x_testt, y_trainn, y_testt):\n    from sklearn.metrics import roc_auc_score,roc_curve\n    \n    modell = model()\n    result_model = modell.fit(x_trainn, y_trainn)\n    y_pred = result_model.predict(x_testt)    \n    conf_mtrx(y_testt, y_pred, model)\n    print(\"*****\",alg_name,\" ALGORITHM:\") \n    rc_recis_scres(y_testt, y_pred, alg_name)\n    \n    \n    \n    modeller.append(result_model)\n    modelName.append(alg_name)\n    \n    print(\"classification_report\\n\",classification_report(y_testt, y_pred))\n    print(\"Accuracy Score for \", alg_name ,accuracy_score(y_testt, y_pred))\n    \n    tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n    y_probs = modell.predict_proba(x_testt)[:,1] # This will give you positive class prediction probabilities  \n    y_pred = np.where(y_probs > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n\n    probs = modell.predict_proba(X_test)\n    probs = probs[:, 1]\n    \n    print()\n    auc = roc_auc_score(y_test, probs)\n    print('AUC: %.2f' % auc)\n    #plot_roc_curve(fpr, tpr)\n    return model  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(RandomForestClassifier, \"Random Forest Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(DecisionTreeClassifier, \"Decision Tree Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(GradientBoostingClassifier, \"Gradient Boosting Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(XGBClassifier, \"XGB Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(LGBMClassifier, \"LGBM Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_Algorithms(CatBoostClassifier, \"CatBoost Classifier\", X_train, X_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictForPlot=[]\ndictForDf = []\nfor i in range(len(modeller)):\n    isim = modelName[i]\n    y_pred = modeller[i].predict(X_val)\n    \n    ac = accuracy_score(y_val, y_pred)\n    recallscore = recall_score(y_val, y_pred,average=None)\n    f1score = f1_score(y_val, y_pred,average=None)\n    \n    szFp,szFl=dictforloop(isim,ac,recallscore,f1score)\n    dictForPlot=dictForPlot + szFp\n    dictForDf.append(szFl)\n    \ndictPlotDf = pd.DataFrame(dictForPlot)\ndictListDf = pd.DataFrame(dictForDf)\n\nfigg=plt.figure(figsize=(20,7))\nplt.title('Accuracy Rates Of Models');    \n\nsns.barplot(x=\"ScoreType\",y = \"Score\",hue =\"Model_Name\" , data = dictPlotDf);\ndictListDf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### I will focus on 2 models for  tuned phase and these are Random Forest and LGBM"},{"metadata":{},"cell_type":"markdown","source":"# Model Tuning "},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf_model = rf.fit(X_train, y_train)\ny_pred = rf_model.predict(X_val)\n\n\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(class_weight =\"balanced\")\nrf_model = rf.fit(X_train, y_train)\ny_pred = rf_model.predict(X_val)\n\n\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_plot(rf_model, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [100, 300, 500, 800, 1200, 2000]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10, 15] \n\n\n\n\ngrid_param = dict(n_estimators = n_estimators, max_depth = max_depth, \n                  min_samples_split = min_samples_split, \n                  min_samples_leaf = min_samples_leaf,\n                  )\n\nrf_tuned = GridSearchCV(rf, grid_param, cv = 5, verbose = 2, \n                      n_jobs = -1)\nrf_tuned = rf_tuned.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"rf_tuned.best_params_ = (max_depth= 15, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 800)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(max_depth= 15, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 800)\nrf_model = rf.fit(X_train, y_train)\ny_pred = rf_model.predict(X_val)\n\n\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ny_probs = rf_model.predict_proba(X_val)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.35, 1, 0) # This will threshold the probabilities to give class predictions.\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to get test with test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_name = []\nfinal_model_ac = []\nfinal_model_recall = []\nfinal_model_f1 = []\n\nac = accuracy_score(y_test, y_pred)\nrecallscore = recall_score(y_test, y_pred,average=None)\nf1score = f1_score(y_test, y_pred,average=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(max_depth= 15, min_samples_leaf= 1, min_samples_split= 10, n_estimators= 800)\nrf_model = rf.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\ntn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ny_probs = rf_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.35, 1, 0) # This will threshold the probabilities to give class predictions.\n\n\nac_rf = accuracy_score(y_test, y_pred)\nrecallscore_rf = recall_score(y_test, y_pred,average=None)\nf1score_rf = f1_score(y_test, y_pred,average=None)\n\nfinal_model_name.append(\"Random Forest\")\nfinal_model_ac.append(ac_rf)\nfinal_model_recall.append(recallscore_rf)\nfinal_model_f1.append(f1score_rf)\n\n\nprint(\"classification_report\\n\",classification_report(y_test, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_test, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Random Forest\nAccuracy Score for 0.9502164502164502,\n\nRecall Score is 0.89 for 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_curve_plot(rf_model,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Ä°mportance"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_plot(rf_model, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier()\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_val)\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred)),","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_plot(lgbm_model, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {'learning_rate': np.arange(0.0, 1.0, 0.1),\n                 'max_depth': [5,10,15,25,30,40,50,70,100],\n                 'num_leaves': [10,20,30,40,50,60,70,90,100,120],\n                 'feature_fraction': np.arange(0.0, 1.0, 0.1),\n                 'subsample': np.arange(0.2, 1.0, 0.1)}\n\n\n\nlgbm_tuned = GridSearchCV(lgbm, lgbm_params, cv = 5, verbose = 2, \n                      n_jobs = -1)\nlgbm_tuned = lgbm_tuned.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_tuned.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" lgbm_tuned.best_params_ = {'feature_fraction': 0.7,\n 'learning_rate': 0.5,\n 'max_depth': 15,\n 'num_leaves': 20,\n 'subsample': 0.2}"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(feature_fraction= 0.7, learning_rate= 0.5, max_depth= 15, num_leaves= 20, subsample= 0.2)\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_val)\n\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ny_probs = lgbm_model.predict_proba(X_val)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(feature_fraction= 0.7, learning_rate= 0.5, max_depth= 15, num_leaves= 20, subsample= 0.2)\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_val)\n\ntn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ny_probs = lgbm_model.predict_proba(X_val)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.40, 1, 0) # This will threshold the probabilities to give class predictions.\n\n\nprint(\"classification_report\\n\",classification_report(y_val, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time to get test with test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(feature_fraction= 0.7, learning_rate= 0.5, max_depth= 15, num_leaves= 20, subsample= 0.2)\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\n\ntn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ny_probs = lgbm_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.40, 1, 0) # This will threshold the probabilities to give class predictions.\n\n\nac_lgbm = accuracy_score(y_test, y_pred)\nrecallscore_lgbm = recall_score(y_test, y_pred,average=None)\nf1score_lgbm = f1_score(y_test, y_pred,average=None)\n\nfinal_model_name.append(\"LGBM\")\nfinal_model_ac.append(ac_lgbm)\nfinal_model_recall.append(recallscore_lgbm)\nfinal_model_f1.append(f1score_lgbm)\n\n\nprint(\"classification_report\\n\",classification_report(y_test, y_pred))\nprint(\"Accuracy Score for \",accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### LGBM\nAccuracy Score for 0.9702380952380952,\n\nRecall Score is 0.90 for 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_curve_plot(lgbm_model,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_name_df = pd.DataFrame(final_model_name, columns=['Model Name'])\nfinal_model_ac_df = pd.DataFrame(final_model_ac, columns=['Model Accuracy Score'])\nfinal_model_recall_df = pd.DataFrame(final_model_recall, columns=['Recall  0', \"Recall 1\"])\nfinal_model_f1_df = pd.DataFrame(final_model_f1, columns=['F1  0', \"F1 1\"])\nfinal_model_result = pd.concat([final_model_name_df, final_model_ac_df, final_model_recall_df, final_model_f1_df], axis = 1)\nfinal_model_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a result our best model is LGBM and Model Accuracy Score 0.97023 ~ **97**, For 1's Recall is 0.897516 ~ **90**."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}