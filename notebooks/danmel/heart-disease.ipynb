{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preamble\nThis is an ongoing notebook that I'm using to teach myself machine learning on the side. Most explanations are not meant to teach others but to teach myself. Things are incomplete and will be updated when I get the time. I welcome comments, suggestions, pointers, what-have-you.","metadata":{}},{"cell_type":"markdown","source":"# Introduction & Exploratory Analysis\n### About this dataset\n\nA detailed description of the data is given [here](https://www.kaggle.com/carlosdg/a-detail-description-of-the-heart-disease-dataset/), and I encourage anyone to check out that post as it's going to have more details than I give below.\n\nFeatures:\n\n*     Age : Age of the patient\n*     Sex : Sex of the patient\n*     cp : Chest Pain type (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic)\n*     trtbps : resting blood pressure (in mm Hg). Judging by the numbers on the histogram below, it's systolic blood pressure, for which the [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/high-blood-pressure/in-depth/blood-pressure/art-20050982) indicates anything higher than 130 is considered \"high blood pressure\".\n*     chol : cholestoral in mg/dl fetched via BMI sensorm. [Mayo Clinic ](https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/diagnosis-treatment/drc-20350806) gives below 200 mg/dL as \"desirable.\n*     fbs : Whether or not the patient's fasting blood sugar is above 120 mg/dl, 1 = true, 0 = false. Once again from the [Mayo Clinic:](https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451) 100 to 125 is considered prediabetic. \n\nThe rest of the variables are from a stress test to test patient blood flow.\n\n*     rest_ecg : resting electrocardiographic results.\n             -0: showing probable or definite left ventricular hypertrophy by Estes' criteria\n             -1: normal\n             -2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n*     exang: Whether or not there was exercise-induced angina, which is chest pain caused by reduced blood flow to the chest. 1 = yes, 0 = no             \n*     thalach : maximum heart rate achieved during the test.\n*     oldpeak: decrease of the ST segment during exercise, compared to the same segment at rest.\n*     slope: slope of the ST segment during the most demanding part of the exercise; 0 = descending, 1 = flat, 2 = ascending\n*     thal: results of the blood flow observed in the test. 0 = Null, removed from the data (see Data problems below), 1 = no blood flow in some parts of the heart, 2 = normal blood flow, 3 = abnormal blood flow\n*     ca: number of major vessels that were examined in the test. (0-3)\n*     target : 0= less chance of heart attack 1= more chance of heart attack\n\n\n### Other Information\n\n\n### Data problems\nWe're going to drop rows that contain incorect information (from above source):\n>    A few more things to consider:\n>    data #93, 159, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed)\n>    data #49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.\n\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop([49,93,159,164,165,252])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histograms for each category:","metadata":{}},{"cell_type":"code","source":"def make_some_hists(df):\n    targ = df['target']\n    df = df.drop('target', axis=1).copy()\n    \n    for col in df.columns:    \n\n        plt.figure(figsize=(7,4))\n        sns.histplot(df, x=col, hue=targ,multiple=\"stack\")\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_some_hists(data)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# want to one-hot encode the chest pain type and electrocardiogram results\n# replace number values with the types\ndata['cp'] = data['cp'].replace(0, 'asymptomatic')\ndata['cp'] = data['cp'].replace(1, 'atypical angina')\ndata['cp'] = data['cp'].replace(2, 'non-anginal pain')\ndata['cp'] = data['cp'].replace(3, 'typical angina')\n\ndata['restecg'] = data['restecg'].replace(0, 'left ventricular hypertrophy')\ndata['restecg'] = data['restecg'].replace(1, 'normal')\ndata['restecg'] = data['restecg'].replace(2, 'T/ST abnormalities')\n\ndata\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    \n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df,dummies],axis=1)\n    df = df.drop(column, axis=1)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = onehot_encode(data, column='cp',prefix='cp')\nX2 = onehot_encode(X1, column='restecg', prefix='restecg')\nX2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Everything else, we're going to treat as a linear variable. It's possible that chest pains may have some order of severity, in which case we could leave them as is/rearrange them to some appropriate ranking. It is also possible that the slope and thal (slope of the ST segment and blood flow having a fixed defect, being normal, or reversible defect, respectively) could be treated as categorical - I'm not a doctor! For the purposes of this exercise, I'm going to leave them as they are.\n\nTaking a quick look at the target:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,9))\nsns.histplot(X2,x=\"target\")\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n","metadata":{}},{"cell_type":"code","source":"# data processing imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    \n    # split into target vector and feature matrix\n    y = df['target']\n    X = df.drop('target', axis=1)\n    \n    # split into training and testing\n    # small-ish dataset so we'll go with an 80% split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=13)\n    \n    # scale the feature matrix with a standard scaler\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n    \n    return X_train, X_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocess(X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Modelling\nGoing to try a few classification models: K-Nearest Neighbours, an SGD classifier, SVC.","metadata":{}},{"cell_type":"code","source":"# import models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, Ridge, SGDRegressor\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a dictionary of the models to iterate through to train and test each one \nmodels = {\n    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Logistic Regression\": LogisticRegression(),\n    \"Ridge\": Ridge(),\n    \"Stochastic Gradient Descent Regressor\": SGDRegressor(),\n    \"Support Vector Classifier\": SVC(),\n    \"Linear Support Vector Classifier\": LinearSVC(),\n    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifer\": RandomForestClassifier()         \n         }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name + \" trained.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\nWe'll get results from a loop and then dig into a couple of the better models.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, model in models.items():\n    print(name + \" R^2 Score: {:.5f}\".format(model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression\nLogistic Regressions tend to work better with highly uncorrelated data, and judging by the heatmap below, it does seem that the features are very independant. ","metadata":{}},{"cell_type":"code","source":"sns.heatmap(X_train.corr(), center =0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Logistic Regression R^2 Score: {:.5f}\".format(logmodel.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = logmodel.predict(X_test)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes on what the classification report is presenting:\n* *Precision* measures the accuracy of the positive predictions, defined as the ratio between true positive predictions and all positive predictions - in simpler terms, what fraction of the patients for which the presence of heart disease was detected by the model actually had heart disease?\n\n* *Recall* measures the amount of true positives that were actually detected, defined as the ratio between true positive predictions and the amount of positives present in the data. What fraction of the patients who actually have heart disease were detected by the model?\n\n* *f1-score* is a combination of these. It presents a single metric for model evaluation by taking the harmonic mean of the two measures: $ f_{1} =  \\frac{2}{ \\frac{1}{p} + \\frac{1}{r} } $, where *p* is the precision and *r* is the recall. The Harmonic mean is frequently used for averaging rates, since it overweights lower values, and hence will only be high when all the values are high.\n\n* *Support* is simply the frequency of each class in the data. \n\nIn the first row, where the logistic model is predicting the not-heart disease group, the precision is 0.88 and recall is 0.69. \n* 88% of the people that the model predicted *did not have* heart disease, actually did not have heart disease; recall 0 means no heart disease.\n\n* 69% of the cases that actually *did not have* heart disease were correctly detected by the model.\n\nThe second row is the same for the positive class, or those who did have heart disease.\n\n","metadata":{}},{"cell_type":"code","source":"# plot a ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    \nplot_roc(fpr, tpr)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}