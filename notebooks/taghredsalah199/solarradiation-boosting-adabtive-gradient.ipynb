{"cells":[{"metadata":{},"cell_type":"markdown","source":"## When nothing works, Boosting does. Nowadays many people use either XGBoost or LightGBM or CatBoost to win competitions at Kaggle or Hackathons. AdaBoost is the first stepping stone in the world of Boosting.\n## So here in this notebook I will take about Adaboost and Gradient Boost.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1- Importing data and libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_solar= pd.read_csv('../input/SolarEnergy/SolarPrediction.csv')\ndf_solar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick visualization:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_solar['UNIXTime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_solar=df_solar.drop(['Data','Time','TimeSunRise','TimeSunSet'],axis=1)\ndf_solar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure= plt.figure(figsize=(10,10))\nsns.heatmap(df_solar.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure= plt.figure(figsize=(20,10))\nsns.pairplot(df_solar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Frame to cotain regression Evaluation:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X= df_solar.drop('Radiation',axis=1)\ny=df_solar['Radiation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nresults_df = pd.DataFrame()\ncolumns = [\"Model\", \"Cross Val Score\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n\ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square\n\ndef append_results(model_name, model, results_df, y_test, pred):\n    results_append_df = pd.DataFrame(data=[[model_name, *evaluate(y_test, pred) , cross_val_score(model, X, y, cv=10).mean()]], columns=columns)\n    results_df = results_df.append(results_append_df, ignore_index = True)\n    return results_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1- Regression AdaBoost :\n#### AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple “weak classifiers” into a single “strong classifier”\n####  → The weak learners in AdaBoost are decision trees with a single split, called decision stumps.\n#### → AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.\n#### → AdaBoost algorithms can be used for both classification and regression problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nada_reg= AdaBoostRegressor(RandomForestRegressor())\nada_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = ada_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df= append_results(\"AdaBoost Regression\",AdaBoostRegressor(RandomForestRegressor()),results_df,y_test,pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To see the normal distribution between y_test and prediction\n#### If it a correct model it will normally distributed with y_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot((y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2- Gradient Boosting Regression\n#### Gradient Boosting trains many models in a gradual, additive and sequential manner. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees).\n#### While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function (y=ax+b+e , e needs a special mention as it is the error term). The loss function is a measure indicating how good are model’s coefficients are at fitting the underlying data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngr_reg= GradientBoostingRegressor()\ngr_reg.fit(X_train,y_train)\npred = gr_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To see the normal distribution between y_test and prediction\n#### If it a correct model it will normally distributed with y_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot((y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df= append_results(\"Gradient Regression\",GradientBoostingRegressor(),results_df,y_test,pred)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So that in our case Gradiet Boosting is better than AdaBoost!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"source1 : https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab\n\nsource2: https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}