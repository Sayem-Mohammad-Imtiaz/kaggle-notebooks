{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/health-care-data-set-on-heart-attack-possibility/heart.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['target'], label='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalizedData = scaler.fit_transform(df)\nX = normalizedData[:,0:13]\nY = normalizedData[:,13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing Cross-Validation and Ensemble Learning using AdaBoostClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=10, random_state=7)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Ensemble with Cross-Val scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nseed = 10\nnum_trees = 60\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1/3, random_state = 21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def models(x_train, y_train):\n    \n    #Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state = 0)\n    log.fit(x_train, y_train)\n    \n    #Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion='entropy', random_state = 0)\n    tree.fit(x_train, y_train)\n    \n    #Support Vector Machines\n    from sklearn.svm import SVC\n    svm = SVC(kernel='linear')\n    svm.fit(x_train,y_train)\n    \n    #Naive Bayes\n    from sklearn.naive_bayes import GaussianNB \n    gnb = GaussianNB() \n    gnb.fit(x_train, y_train) \n    \n    #Bagging Classifier\n    from sklearn.neighbors import KNeighborsClassifier  \n    knn = KNeighborsClassifier(n_neighbors=10, algorithm='kd_tree', metric='minkowski', p=5)  \n    knn.fit(x_train, y_train)  \n    from sklearn.ensemble import BaggingClassifier\n    bag = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n    bag.fit(x_train, y_train)\n    \n    print(\"Logistic Regression Training Accuracy:\", log.score(x_train, y_train))\n    print(\"Decision Tree Classifier Training Accuracy:\", tree.score(x_train, y_train))\n    print(\"SVM Training Accuracy:\", svm.score(x_train, y_train))\n    print(\"Naive Bayes Training Accuracy:\", gnb.score(x_train, y_train))\n    print(\"Bagging Classifier Training Accuracy:\", knn.score(x_train, y_train))\n    \n    return log, tree, svm, gnb, bag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression: \")\nprint()\nprint(classification_report(y_test, model[0].predict(x_test)))\nprint(accuracy_score(y_test, model[0].predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Classifier: \")\nprint()\nprint(classification_report(y_test, model[1].predict(x_test)))\nprint(accuracy_score(y_test, model[1].predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Support Vector Machines Classifier: \")\nprint()\nprint(classification_report(y_test, model[2].predict(x_test)))\nprint(accuracy_score(y_test, model[2].predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Naive Bayes Classifier: \")\nprint()\nprint(classification_report(y_test, model[3].predict(x_test)))\nprint(accuracy_score(y_test, model[3].predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Bagging Classifier: \")\nprint()\nprint(classification_report(y_test, model[4].predict(x_test)))\nprint(accuracy_score(y_test, model[4].predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy: \")\nprint(\"Logistic Regression:\", accuracy_score(y_test, model[0].predict(x_test))*100)\nprint(\"Decision Tree Classifier:\", accuracy_score(y_test, model[1].predict(x_test))*100)\nprint(\"Support Vector Machines (SVM):\", accuracy_score(y_test, model[2].predict(x_test))*100)\nprint(\"Naive Bayes:\", accuracy_score(y_test, model[3].predict(x_test))*100)\nprint(\"Bagging Classifier: \", accuracy_score(y_test, model[4].predict(x_test))*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second Ensemble","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('decision', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\nmodel4 = GaussianNB()\nestimators.append(('naive', model4))\nmodel5 = BaggingClassifier()\nestimators.append(('bag', model5))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, x_test, y_test)\nprint(results.mean()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = accuracy_score(y_test, model[0].predict(x_test))*100\ndecision = accuracy_score(y_test, model[1].predict(x_test))*100\nsvm = accuracy_score(y_test, model[2].predict(x_test))*100\nnaive = accuracy_score(y_test, model[3].predict(x_test))*100\nbag = accuracy_score(y_test, model[4].predict(x_test))*100\nvoting = results.mean()*100\nli = [logistic, decision, svm, naive, bag, voting]\n\nplt.bar(['Logistic', 'Decision', 'SVM', 'Naive', 'Bag', 'Voting'], li)\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('Final Accuracy of the individual models')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, an ANN (Artificial Neural Network) to compare all of them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Dropout\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmodel = Sequential()\n\nmodel.add(Dense(8))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n\nprint('<< Compiling Model >>')\n\nhistory_1 = model.fit(x_train,y_train ,batch_size = 32 ,epochs = 300)\ny_pred_1 = model.predict(x_test)\ny_pred_1 = (y_pred_1 > 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Accuracy of ANN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_1.history['accuracy'], color='red')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred_1)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred_1))\nann = 100*accuracy_score(y_test,y_pred_1)\nprint('percentage Accuracy : ',ann)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More updates soon.\n\nPlease leave an Upvote.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}