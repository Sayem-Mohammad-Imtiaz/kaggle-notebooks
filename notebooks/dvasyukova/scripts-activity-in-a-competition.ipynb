{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm"},{"cell_type":"markdown","metadata":{},"source":"# Scripts activity in competitions\n\nThe things I want to look at:\n\n- How do public and private scores of script submissions evolve?\n- How many new versions appear daily, how many contain changes?\n- How many submissions are made from the scripts, how many use the script version currently leading on LB?\n\n## Load the data"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Competitions - use only those that award points\ncompetitions = (pd.read_csv('../input/Competitions.csv')\n                .rename(columns={'Id':'CompetitionId'}))\ncompetitions = competitions[(competitions.UserRankMultiplier > 0)]\n# Scriptprojects to link scripts to competitions\nscriptprojects = (pd.read_csv('../input/ScriptProjects.csv')\n                    .rename(columns={'Id':'ScriptProjectId'}))\n# Evaluation algorithms\nevaluationalgorithms = (pd.read_csv('../input/EvaluationAlgorithms.csv')\n                          .rename(columns={'Id':'EvaluationAlgorithmId'}))\ncompetitions = (competitions.merge(scriptprojects[['ScriptProjectId','CompetitionId']],\n                                   on='CompetitionId',how='left')\n                            .merge(evaluationalgorithms[['IsMax','EvaluationAlgorithmId']],\n                                   on='EvaluationAlgorithmId',how='left')\n                            .dropna(subset = ['ScriptProjectId'])\n                            .set_index('CompetitionId'))\ncompetitions['ScriptProjectId'] = competitions['ScriptProjectId'].astype(int)\n# Fill missing values for two competitions\ncompetitions.loc[4488,'IsMax'] = True # Flavours of physics\ncompetitions.loc[4704,'IsMax'] = False # Santa's Stolen Sleigh\n# Scripts\nscripts = pd.read_csv('../input/Scripts.csv')\n# Script versions\n# List necessary columns to avoid reading script versions content\nsvcols = ['Id','Title','DateCreated','ScriptId',\n          'LinesInsertedFromPrevious','LinesDeletedFromPrevious', \n          'LinesChangedFromPrevious','LinesInsertedFromFork', \n          'LinesDeletedFromFork', 'LinesChangedFromFork']\nscriptversions = pd.read_csv('../input/ScriptVersions.csv', \n                             usecols=svcols)\nscriptversions['DateCreated'] = pd.to_datetime(scriptversions['DateCreated'])\n# Determine if a script version contains changes \n#(either from fork parent or from previous version)\nisfirst = scriptversions.Id.isin(scripts.FirstScriptVersionId)\nscriptversions.loc[isfirst, 'IsChanged'] = scriptversions.loc[isfirst, \n            ['LinesInsertedFromFork', \n             'LinesDeletedFromFork', \n             'LinesChangedFromFork']].any(axis=1)\nscriptversions.loc[~(isfirst), 'IsChanged'] = scriptversions.loc[~(isfirst), \n            ['LinesInsertedFromPrevious', \n             'LinesDeletedFromPrevious', \n             'LinesChangedFromPrevious']].any(axis=1)\n# Submissions\nsubmissions = pd.read_csv('../input/Submissions.csv')\nsubmissions = submissions.dropna(subset=['Id','DateSubmitted','PublicScore'])\nsubmissions.DateSubmitted = pd.to_datetime(submissions.DateSubmitted)"},{"cell_type":"markdown","metadata":{},"source":"Some functions for analyzing scripts activity."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def report_script_activity(scriptversions, submissions, ismax):\n    scores = pd.DataFrame()\n    scores['BestPublic'] = submissions.PublicScore.cummax() if ismax else submissions.PublicScore.cummin()\n    scores.loc[scores.BestPublic == submissions.PublicScore, 'BestPrivate'] = submissions.PrivateScore\n    scores.BestPrivate = scores.BestPrivate.fillna(method='ffill')\n    scores['DateSubmitted'] = submissions['DateSubmitted']\n    activity = pd.DataFrame()\n    activity['Submissions'] = submissions.groupby(submissions.DateSubmitted.dt.date)['Id'].size()\n    activity['SubmissionsBest'] = ((submissions['PublicScore']==scores['BestPublic'])\n                                   .groupby(submissions.DateSubmitted.dt.date).sum())\n    activity['Versions'] = scriptversions.groupby(scriptversions.DateCreated.dt.date)['Id'].size()\n    activity['VersionsChanged'] = scriptversions.groupby(scriptversions.DateCreated.dt.date)['IsChanged'].sum()\n    return scores, activity\n\ndef plot_script_activity(scores, activity):\n    fig, ax = plt.subplots(3,1, figsize=(10,8), sharex=True, gridspec_kw = {'height_ratios':[1,3,1]})\n    colors = cm.Blues(np.linspace(0.5, 0.8, 2))\n    ax[0].bar(activity.index, activity.Versions,color=colors[0])\n    ax[0].bar(activity.index, activity.VersionsChanged,color=colors[1])\n    ax[0].set_title('Daily new versions')\n    ax[0].legend(['all','with changes'])\n    ax[1].plot(scores.DateSubmitted, scores.BestPublic, '-', \n               scores.DateSubmitted, scores.BestPrivate, '-')\n    ax[1].set_title('Best public submission scores')\n    ax[1].legend(['Public','Private'],loc=4)\n    ax[2].bar(activity.index, activity.Submissions,color=colors[0]);\n    ax[2].bar(activity.index, activity.SubmissionsBest,color=colors[1]);\n    ax[2].set_title('Daily submissions');\n    ax[2].legend(['all','best public']);\n    return fig, ax"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def report_competition(competitionId):\n    ismax = competitions.loc[competitionId,'IsMax']\n    scriptprojectid = competitions.loc[competitionId, 'ScriptProjectId']\n    s = scripts.loc[scripts.ScriptProjectId==scriptprojectid,'Id'].values\n    v = scriptversions.loc[scriptversions.ScriptId.isin(s)]\n    sub = (submissions.loc[submissions.SourceScriptVersionId.isin(v.Id)]\n                      .sort_values(by='DateSubmitted'))\n    scores, activity = report_script_activity(v,sub,ismax)\n    fig, ax = plot_script_activity(scores, activity)\n    plt.suptitle(competitions.loc[competitionId,'Title'],fontsize='large')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Recent competitions\ncompetitions.sort_values(by='Deadline',ascending=False)[['Title']].head()"},{"cell_type":"markdown","metadata":{},"source":"## Expedia Hotel Recommendations\n\nLet's look at scripts activity in the Expedia Hotel Recommendations competition."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"report_competition(5056)"},{"cell_type":"markdown","metadata":{},"source":"Things to note:\n\n- Public and private LB scores follow each other closely.\n- There are 3 levels of submission scores\n  1. the first is the \"most popular local hotel\" approach, \n  1. the second improves on that result by partially using the data leak\n  1. the third and highest is from the \"Leakage solution\" script by ZFTurbo and its many forks.\n- The number of script submissions increased drastically after a high-scoring script appeared.\n- The number of script submissions is low in the last week of the competition. Are script submissions mostly made by new entrants? Or do people start being more careful with the limited number of submissions left?\n\nLet's take a closer look at the \"Leakage solution\" script and its forks."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# find scriptIds of scripts forked from ancestors, and of their forks, and of their forks...\ndef find_descendants(ancestors, scripts, scriptversions):\n    if len(ancestors) == 0:\n        return np.array([],dtype=int)\n    ancestors_versions = scriptversions.loc[scriptversions.ScriptId.isin(ancestors),'Id']\n    children = scripts.loc[scripts.ForkParentScriptVersionId.isin(ancestors_versions.values),'Id'].values\n    return np.concatenate((children, find_descendants(children, scripts, scriptversions)))\n# find scripts with most descendants in a competition\ndef find_most_forked_scripts(competitionId, n = 5):\n    print('Most forked scripts in {}'.format(competitions.loc[competitionId,'Title']))\n    # Find scripts project id\n    projectId = competitions.loc[competitionId,'ScriptProjectId']\n    # Read in scripts and scriptversions data\n    s = scripts.loc[(scripts.ScriptProjectId==projectId)].copy()\n    v = scriptversions.loc[scriptversions.ScriptId.isin(s.Id)]\n    origmask = s.ForkParentScriptVersionId.isnull()\n    s.loc[origmask,'Nforks'] = s.loc[origmask,'Id'].apply(lambda x,s,v: find_descendants([x],s,v).shape[0],args=(s,v))\n    return s[['Id','UrlSlug','Nforks']].sort_values(by='Nforks',ascending=False).head(n)"},{"cell_type":"markdown","metadata":{},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"find_most_forked_scripts(5056)"},{"cell_type":"markdown","metadata":{},"source":"### The Leakage solution\n\nThe 'Leakage solution' script by @ZFTurbo combined the most popular local hotel approach with exploiting the data leak. There are 371 scripts descended from it. Variations they made included:\n\n- tuning the weights of booking and click events\n- tuning the weights of old versus new events\n- including other grouping strategies and weighting their contributions\n- probably something else as well (drop a comment to add something)\n\nHow did the scores evolve?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def report_competition_script(competitionId, scriptId):\n    ismax = competitions.loc[competitionId,'IsMax']\n    children = find_descendants([scriptId],scripts,scriptversions)\n    family = np.append(children,[scriptId])\n    v = scriptversions.loc[scriptversions.ScriptId.isin(family)]\n    sub = (submissions.loc[submissions.SourceScriptVersionId.isin(v.Id)]\n                      .sort_values(by='DateSubmitted'))\n    scores, activity = report_script_activity(v,sub,ismax)\n    fig, ax = plot_script_activity(scores, activity)\n    scriptname = scripts.loc[scripts.Id==scriptId,'UrlSlug'].values[0]\n    competitionname = competitions.loc[competitionId,'Title']\n    title = '{} script and all its forks\\n{}'.format(scriptname, competitionname)\n    plt.suptitle(title,fontsize='large')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"report_competition_script(5056, 60666)"},{"cell_type":"markdown","metadata":{},"source":"A zoom in to this subset of scripts reveals that\n\n- 20-40 variations of the model appeared every day, trying new ideas and parameters.\n- Both private and public scores of best public submission improved as a result of this model tuning. \n- The gap between public and private scores was widening over time which may indicate some public LB overfitting.\n- Submissions made from scripts followed the leaderboard closely, half or more of script submissions came from the current best-scoring script.\n\n### The history of forks\n\nWhat if we look at the history of forks of this script? I want to visualize how new versions appear, see their scores and number of submissions from each one."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def plot_script(scriptId, ax, x=0, vmin=0.49, vmax = 0.502):\n    ax.set_title('The history of forks of {}'.format(s.loc[scripts.Id==scriptId,'UrlSlug'].values[0]),\n                 fontsize='x-large')\n    versions = v.loc[v.ScriptId==scriptId].sort_values(by='DateCreated', ascending=False)\n    ax.plot(versions.DateCreated.values,\n            np.ones(versions.shape[0])*x, \n            'k-',zorder=1, linewidth=0.5)\n    ax.scatter(versions.DateCreated.values, \n               np.ones(versions.shape[0])*x, \n               s = 2*versions.Nsubmissions.values,\n               c = versions.PublicScore.values,\n               cmap = cm.rainbow,marker='o',alpha=0.9,\n               vmin = vmin, zorder=2,vmax=vmax)\n    n = 1\n    for versionId in versions.index:\n        versionDate = versions.loc[versionId,'DateCreated']\n        desc = s.loc[s.ForkParentScriptVersionId==versionId]\n        if desc.shape[0] == 0:\n            continue\n        desc = desc.sort_values(by='Id',ascending=False)\n        for script in desc.Id.values:\n            forkversion = desc.loc[desc.Id==script,'FirstScriptVersionId'].values[0]\n            forkversionDate = v.loc[forkversion,'DateCreated']\n            ax.plot([versionDate, forkversionDate],\n                    [x,x+n],\n                    'k-',zorder=1, linewidth=0.5,alpha = 0.5)\n            nd = plot_script(script, ax, x=x+n)\n            n += nd\n    return n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"scriptId=60666\nchildren = find_descendants([scriptId],scripts,scriptversions)\nfamily = np.append(children,[scriptId])\ns = scripts.loc[scripts.Id.isin(family)]\nv = scriptversions.loc[scriptversions.ScriptId.isin(family)].set_index('Id')\nsub = (submissions.loc[submissions.SourceScriptVersionId.isin(v.index)]\n                  .sort_values(by='DateSubmitted'))\nv['Nsubmissions'] = sub.groupby('SourceScriptVersionId').size()\nv['PublicScore'] = sub.groupby('SourceScriptVersionId')['PublicScore'].agg('first')"},{"cell_type":"markdown","metadata":{},"source":"There's probably some library for this but I have created a function to plot the tree of forks using matplotlib. \n\n- Horizontal levels are scripts\n- Markers denote script versions, date created is their x coordinate\n- Color is public LB score (redder is better)\n- Size is the number of submissions from this version"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,12))\nn = plot_script(60666, ax)\noneday = pd.to_timedelta(1, unit='day')\nax.set_xlim(v.DateCreated.min()-oneday, \n            competitions.loc[5056,'Deadline']);\nax.set_ylim(-10, n+10);"},{"cell_type":"markdown","metadata":{},"source":"The original script is the lowermost line. Its last version appeared on May the 10th. The later model developments took place in forks. It seems like the best scoring fork or version gains initiative and receives a lot of forks in turn leading to a gradual improvement of scores. There is also probably some exchange of ideas between simultaneously developing scripts (like those long lines at y=55 and y=125).\n\nWhat other interesting things might we see in this structure? Leave a comment if you have an idea =).\n\n\n## Santander Customer Satisfaction\n\nSantander Customer Satisfaction was a large competition that suffered a really big leaderboard shakeup. What can scripts activity tell us about it?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"competitionId = 4986\nreport_competition(competitionId)"},{"cell_type":"markdown","metadata":{},"source":"- After an initial jump there is very little improvement in scores throughout the competition.\n- The improvement in script scores on March 23rd leads to an increase in the number of script submissions.\n- After that there is a long period of stagnation where despite 100-200 new versions of scripts appearing every day the scores stay largely the same.\n- In the last eight days of the competition there is a lot of new script versions appearing. Kagglers desperate for some improvement started postprocessing their models' predictions with handcrafted rules. As a result the public LB scores are improving and the private scores do the opposite - a clear case of public LB overfitting.\n- There is a spike in script submissions 8 days before the competition deadline. Here we probably see script submissions playing the role of sample submission, a means for people who haven't yet started working on a competition to make a claim before the first submission deadline."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"find_most_forked_scripts(competitionId)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"projectId = competitions.loc[competitionId,'ScriptProjectId']\ns = scripts.loc[(scripts.ScriptProjectId==projectId)]\nv = scriptversions.loc[scriptversions.ScriptId.isin(s.Id)].set_index('Id')\nsub = (submissions.loc[submissions.SourceScriptVersionId.isin(v.index)]\n                  .sort_values(by='DateSubmitted'))\nv['Nsubmissions'] = sub.groupby('SourceScriptVersionId').size()\nv['PublicScore'] = sub.groupby('SourceScriptVersionId')['PublicScore'].agg('first')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(12,12))\nn = 0\nfor script in [49934, 43840]:\n    n += plot_script(script, ax,x=n,vmin=0.841, vmax=v.PublicScore.max())\nax.set_xlim(competitions.loc[competitionId,'DateEnabled'], \n            competitions.loc[competitionId,'Deadline']);\nax.set_ylim(-10,n+10);\nax.set_title('The history of forks of popular Santander scripts',fontsize='large');"},{"cell_type":"markdown","metadata":{},"source":"## Some conclusions\n\n- Posting of a well-performing script in a competition leads to a multitude of forks and new versions appearing. Variations on the script's model start playing leapfrog game on the Leaderboard.\n- The better scoring forked scripts or versions gain initiative and receive a lot of forks in turn.\n- Appearing of a high scoring script results in an increase in script submissions number, [McScriptface](https://www.kaggle.com/dvasyukova/d/kaggle/meta-kaggle/scripty-mcscriptface-the-lazy-kaggler) seizing his chance of a good submission.\n- This type of crowdsourced model tuning runs a risk of overfitting to the public LB (Santander shakeup).\n- Script submissions seem to be used by people as sample submission - a way to show that they are going to participate in a competition (less submissions in the last week, a spike the day before first entry deadline).\n\nToo bad we won't see the Mad Scripts Battles of Facebook checkins predictions here =).\n\nIn case of any questions or suggestions regarding this analysis, please leave a comment. If you want to look at other competitions in a similar way - feel free to fork this notebook."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}