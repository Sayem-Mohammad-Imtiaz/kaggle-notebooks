{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Price Prediction::"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement::"},{"metadata":{},"cell_type":"markdown","source":"A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\n>**Which variables are significant in predicting the price of a car**\n\n>**How well those variables describe the price of a car**\n\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n\n# task::\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market."},{"metadata":{},"cell_type":"markdown","source":"# WORKFLOW ::"},{"metadata":{},"cell_type":"markdown","source":"1.Load Data\n\n2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n\n3.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n\n4.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n\n5.Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)\n\n6.Train the Model with Epochs (100) and validate it\n\n7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .\n\n8.Evaluation Step\n\n9.Prediction"},{"metadata":{},"cell_type":"markdown","source":"## 1.Load Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"#importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars = pd.read_csv('/kaggle/input/car-price-prediction/CarPrice_Assignment.csv')\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Check Missing Values"},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Splitting company name from CarName column\nCompanyName = cars['CarName'].apply(lambda x : x.split(' ')[0])\ncars.insert(3,\"CompanyName\",CompanyName)\ncars.drop(['CarName'],axis=1,inplace=True)\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.CompanyName.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*Fixing invalid values*\n\nThere seems to be some spelling error in the CompanyName column.\n\n`maxda` = `mazda`\n\n`Nissan` = `nissan`\n\n`porsche` = `porcshce`\n\n`toyota` = `toyouta`\n\n`vokswagen` = `volkswagen` = `vw`"},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.CompanyName = cars.CompanyName.str.lower()\n\ndef replace_name(a,b):\n    cars.CompanyName.replace(a,b,inplace=True)\n\nreplace_name('maxda','mazda')\nreplace_name('porcshce','porsche')\nreplace_name('toyouta','toyota')\nreplace_name('vokswagen','volkswagen')\nreplace_name('vw','volkswagen')\n\ncars.CompanyName.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">**Deriving some new features**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Fuel economy\ncars['fueleconomy'] = (0.55 * cars['citympg']) + (0.45 * cars['highwaympg'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Binning the Car Companies based on avg prices of each Company.\ncars['price'] = cars['price'].astype('int')\ntemp = cars.copy()\ntable = temp.groupby(['CompanyName'])['price'].mean()\ntemp = temp.merge(table.reset_index(), how='left',on='CompanyName')\nbins = [0,10000,20000,40000]\ncars_bin=['Budget','Medium','Highend']\ncars['carsrange'] = pd.cut(temp['price_y'],bins,right=False,labels=cars_bin)\ncars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars_lr = cars[['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase',\n                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower', \n                    'fueleconomy', 'carlength','carwidth', 'carsrange']]\ncars_lr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(cars_lr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">**Dummy Variables**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Defining the map function\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], drop_first = True)\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n# Applying the function to the cars_lr\n\ncars_lr = dummies('fueltype',cars_lr)\ncars_lr = dummies('aspiration',cars_lr)\ncars_lr = dummies('carbody',cars_lr)\ncars_lr = dummies('drivewheel',cars_lr)\ncars_lr = dummies('enginetype',cars_lr)\ncars_lr = dummies('cylindernumber',cars_lr)\ncars_lr = dummies('carsrange',cars_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cars_lr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Train-Test Split and feature scaling"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(cars_lr, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnum_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Correlation using heatmap\nplt.figure(figsize = (30, 25))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Highly correlated variables to price are - `curbweight`, `enginesize`, `horsepower`,`carwidth` and `highend`."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Dividing data into X and y variables\ny_train = df_train.pop('price')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model Building"},{"metadata":{"trusted":false},"cell_type":"code","source":"#RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm, 10)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_model(X,y):\n    X = sm.add_constant(X) #Adding the constant\n    lm = sm.OLS(y,X).fit() # fitting the model\n    print(lm.summary()) # model summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_new = build_model(X_train_rfe,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Analysis of Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_new).fit()\ny_train_price = lm.predict(X_train_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Error terms seem to be approximately normally distributed, so the assumption on the linear modeling seems to be fulfilled."},{"metadata":{},"cell_type":"markdown","source":"## 8,9. Prediction and Evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Scaling the test set\nnum_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']\ndf_test[num_vars] = scaler.fit_transform(df_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Dividing into X and y\ny_test = df_test.pop('price')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now let's use our model to make predictions.\nX_train_new = X_train_new.drop('const',axis=1)\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Making predictions\ny_pred = lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluation of test via comparison of y_pred and y_test"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#EVALUATION OF THE MODEL\n# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of the model using Statistics"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*Inference :*\n\n* R-sqaured and Adjusted R-squared (extent of fit) - 0.929 and 0.923 - 95% variance explained.\n* F-stats and Prob(F-stats) (overall model fit) - 172.0 and 1.29e-70(approx. 0.0) - Model fir is significant and explained 95% variance is just not by chance.\n* p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the predictors are statistically significant."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}