{"cells":[{"metadata":{},"cell_type":"markdown","source":"Please don't forget to upvote this notebook. I am novice and it is need for me.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LSTM tutorial with Bitcoin price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## RNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Basic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feed-forward neural network has some disadvantages in compare with recurent neural network:\n\n* Cannot handle sequential data;\n\n* Considers only the current input;\n\n* Cannot memorize previous inputs. [1]\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see how RNN is work. [1]\n\n<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif\">\n\nHere:\n* \"x\" is the input layer;\n* \"h\" is the hidden layer;\n* \"y\" is the output layer;\n* A (V), B (U), C (W) are the network parametres.\n\nAt any given time t, the current input is a combination of input at x(t) and h(t-1).\n\n<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Long_Short_Term_Memory_Networks.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Backpropagation Through Time","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see how Backpropagation Through Time [2]\n\n<img src=\"http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png\">\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\ns_t = tanh(Ux_t+Ws_{t-1}) \\\\\n\\end{align}\n\\begin{align}\n\\hat{y_t} = softmax(Vs_t) \\\\\n\\end{align}\n\\begin{align}\nE = \\sum^{}_{t} {E_t}\n\\end{align}\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"E - loss function, cross entropy loss, for example.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n\\frac{\\partial E}{\\partial W} =\n\\sum^{}_{t} {\\frac{\\partial E_t}{\\partial W}}\n\\end{align}","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n\\frac{\\partial E_3}{\\partial V} =\n\\frac{\\partial E_3}{\\partial \\hat{y_3}}\n\\frac{\\partial \\hat{y_3}}{\\partial z_3}\n\\frac{ \\partial z_3}{\\partial V}\n\\end{align}","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"$ \\frac{\\partial E_3}{\\partial V} $ depends only on the values at the current time step. But the story is diffetent for $ \\frac{\\partial E_3}{\\partial W} $ (and for U)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n\\frac{\\partial E_3}{\\partial W} =\n\\frac{\\partial E_3}{\\partial \\hat{y_3}}\n\\frac{\\partial \\hat{y_3}}{\\partial s_3}\n\\frac{ \\partial s_3}{\\partial W}\n\\end{align}\n\n\\begin{align}\ns_3 = tanh(Ux_t+Ws_2)\n\\end{align}","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note that $ s_3 = \\tanh(Ux_t + Ws_2) $ depends on $s_2$, which depends on $W$ and $s_1$, and so on.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Vanishing and exploding Gradient Problem\n\nRNNs have the problem of vanishing gradient. When the gradient becomes too small, the parameter updates become insignificant. [1]\n\nAlso there is the problem of exploding gradient. This problem arises when large error gradients accumulate, resulting in very large updates to the neural network model weights during the training process. [1]\n\ntanh (or sigmoid) activation function maps all values into a range between -1 and 1, and the derivative is bounded by 1 (1/4 in the case of sigmoid) and tanh functions have derivatives of 0 at both ends. [2]\n\nThe gradient values are shrinking exponentially fast, eventually vanishing completely after a few time steps. Gradient contributions from “far away” steps become zero, and the state at those steps doesn’t contribute to what you are learning: You end up not learning long-range dependencies. Vanishing gradients aren’t exclusive to RNNs. They also happen in deep Feedforward Neural Networks. It’s just that RNNs tend to be very deep (as deep as the sentence length in our case), which makes the problem a lot more common. [2]\n\nFortunately, there are a few ways to combat the vanishing gradient problem. Proper initialization of the W matrix can reduce the effect of vanishing gradients. So can regularization. A more preferred solution is to use ReLU instead of tanh or sigmoid activation functions. The ReLU derivative is a constant of either 0 or 1, so it isn’t as likely to suffer from vanishing gradients. [2]\n\nAnd even more popular solution is to use Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architectures. Both of these RNN architectures were explicitly designed to deal with vanishing gradients and efficiently learn long-range dependencies. [2]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## LSTM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Structure of LSTM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Instead of having a single neural network layer, LSTM has four, interacting in a very special way. [3]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step-by-Step LSTM Walk Through\n\n#### Forget gate layer\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Input gate layer\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Update the old cell state\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Output\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Bitcoin Time Series Prediction with LSTM","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# First step, import libraries.\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the dataset and encode the date\ndf = pd.read_csv('/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-04-22.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group = df.groupby('date')\nReal_Price = group['Weighted_Price'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data\nprediction_days = 30\ndf_train= Real_Price[:len(Real_Price)-prediction_days]\ndf_test= Real_Price[len(Real_Price)-prediction_days:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preprocess\ntraining_set = df_train.values\ntraining_set = np.reshape(training_set, (len(training_set), 1))\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntraining_set = sc.fit_transform(training_set)\nX_train = training_set[0:len(training_set)-1]\ny_train = training_set[1:len(training_set)]\nX_train = np.reshape(X_train, (len(X_train), 1, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n\n* pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n\n* pass instead a batch_input_shape argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).\n\nmodel = Sequential()\nmodel.add(Dense(32, batch_input_shape=(None, 784)))\n#note that batch dimension is \"None\" here,\n#so the model will be able to process batches of any size. [4]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the input layer and the LSTM layer\nregressor.add(LSTM(units = 4, activation = 'sigmoid', input_shape = (None, 1)))\n\n# units: Positive integer, dimensionality of the output space.\n# activation: Activation function to use.\n# Default: hyperbolic tangent (tanh).\n# If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, batch_size = 5, epochs = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(regressor, to_file='model_plot.png', show_shapes=True, show_layer_names=True, expand_nested=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the predictions\ntest_set = df_test.values\ninputs = np.reshape(test_set, (len(test_set), 1))\ninputs = sc.transform(inputs)\ninputs = np.reshape(inputs, (len(inputs), 1, 1))\npredicted_BTC_price = regressor.predict(inputs)\npredicted_BTC_price = sc.inverse_transform(predicted_BTC_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the results\nplt.figure(figsize=(15,5), dpi=80, facecolor='w', edgecolor='k')\nax = plt.gca()  \nplt.plot(test_set, color = 'red', label = 'Real BTC Price')\nplt.plot(predicted_BTC_price, color = 'blue', label = 'Predicted BTC Price')\nplt.title('BTC Price Prediction', fontsize=14)\ndf_test = df_test.reset_index()\nx=df_test.index\nlabels = df_test['date']\nplt.xticks(x, labels, rotation = 'vertical')\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label1.set_fontsize(14)\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label1.set_fontsize(14)\nplt.xlabel('Time', fontsize=14)\nplt.ylabel('BTC Price(USD)', fontsize=14)\nplt.legend(loc=2, prop={'size': 14})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References:\n\n1. https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn?source=sl_frs_nav_playlist_video_clicked\n\n2. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\n\n3. https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n4. https://faroit.com/keras-docs/1.1.0/getting-started/sequential-model-guide/","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}