{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nimport math\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss\nimport cufflinks as cf\nfrom collections import Counter\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D, Bidirectional\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import roc_auc_score\nfrom keras.preprocessing.text import Tokenizer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Tweets.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Data\n"},{"metadata":{},"cell_type":"markdown","source":"Pandas head() method is used to return top 100 rows of a data frame or series."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looping on the data columns to get the columns names and its count"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = []\ncolumn_counts = []\nfor column in data:\n    column_names.append(column)\n    column_counts.append(data[column].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualize the data columns names and counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize columns count\nplt.subplots(figsize=(25,20))\nsns.barplot(x=column_names, y=column_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"found out that airline sentiment gold and negative reason gold and tweet coord have too many null data to be important"},{"metadata":{},"cell_type":"markdown","source":"## data correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize columns names\nlist(data.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## airline_sentiment distribution"},{"metadata":{},"cell_type":"markdown","source":"creating probability distributions for some columns of my data frame by distplot from seaborn library sns.distplot()."},{"metadata":{"trusted":true},"cell_type":"code","source":"cf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\ndata['airline_sentiment'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='airline_sentiment Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='text Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove duplicated rows to clean the data"},{"metadata":{},"cell_type":"markdown","source":"## apply histogram to see the distribution of data"},{"metadata":{},"cell_type":"markdown","source":"A histogram is used to summarize discrete or continuous data. In other words, it provides a visual interpretation of numerical data by showing the number of data points that fall within a specified range of values (called “bins”). It is similar to a vertical bar graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text_length'] = data['text'].apply(len)\nhist = data.hist(bins=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## calc mean of retweet count for each category to decide if this feature is important"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"dp=data[ data['airline_sentiment'] == 'positive']\ndg=data[ data['airline_sentiment'] == 'negative']\ndn=data[ data['airline_sentiment'] == 'neutral']\npositive_retweet_mean=dp['retweet_count'].mean()\nnegative_retweet_mean=dg['retweet_count'].mean()\nneutral_retweet_mean=dn['retweet_count'].mean()\nprint(\"positive_retweet_mean\", positive_retweet_mean)\nprint(\"negative_retweet_mean\", negative_retweet_mean)\nprint(\"neutral_retweet_mean\",neutral_retweet_mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute pairwise correlation of columns, excluding NA/null values.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_ratio(categories, measurements):\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator/denominator)\n    return eta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"select rows in my data frame contains negative the choose the rows from the selected columns in the second line meets up to the selected rows, group it bu airline and sorted by negative reasons"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rank for the bad airline\nnegative_tweets = data[data['airline_sentiment'].str.contains(\"negative\")]\nbad_airline = negative_tweets[['airline','airline_sentiment_confidence','negativereason']]\nbad_airline_count = bad_airline.groupby('airline', as_index=False).count()\nbad_airline_count.sort_values('negativereason', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"select rows in my data frame contains positive the choose the rows from the selected columns in the second line meets up to the selected rows, group it bu airline and sorted by negative reasons"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rank for the good airline\npositive_tweets = data[data['airline_sentiment'].str.contains(\"positive\")]\ngood_airline = positive_tweets[['airline','airline_sentiment_confidence']]\ngood_airline_count = good_airline.groupby('airline', as_index=False).count()\ngood_airline_count.sort_values('airline_sentiment_confidence', ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualize reasons based on the negative tweets of the selected columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"reason = negative_tweets[['airline','negativereason']]\nbad_flight_reason_count = reason.groupby('negativereason', as_index=False).count()\nbad_flight_reason_count.sort_values('negativereason', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create df with the important columns"},{"metadata":{},"cell_type":"markdown","source":"after visualizations and analysis, i selected the columns that seemed to be most important to create new data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = pd.read_csv('../input/Tweets.csv')\ndf = pd.DataFrame([])\ndf['airline_sentiment'] = data_new['airline_sentiment']\ndf['text'] = data_new['text']\ndf['negativereason'] = data_new['negativereason']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"based on correlation: airline_sentiment,text and negativereason correlates more with the target"},{"metadata":{},"cell_type":"markdown","source":"Pandas head() method is used to return top 5 -by default- rows of a data frame or series."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Text Data"},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{},"cell_type":"markdown","source":"We create a normalizer class that is responsible for doing preprocessing and cleaning functions like removing punctuations, mentions, urls, emails, stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Normalizer:\n    def __init__(self):\n        self.stop_words = stopwords.words('english')\n    \n    def lower(self, text):\n        return text.lower()\n    \n    def remove_punctuations(self, text):\n        return re.sub('[^\\w\\s]','', text)\n    \n    def remove_mentions(self, text):\n        return re.sub('@[a-zA-Z0-9-._]+', '', text)\n    \n    def remove_url(self, text):\n        url_ptrn = r'''(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|\n            www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\n            \\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|\n            (\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»\"\"'']))'''\n        return re.sub(url_ptrn, '', text)\n    \n    def remove_email(self, text):\n        return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',\n                             '', text)\n    def remove_stop_words(self, text):\n        return \" \".join(x for x in text.split() if x not in self.stop_words)\n    \n    def correct_spelling(self, text):\n        return str(TextBlob(text).correct())\n    \n    def tokenize(self, text):\n        return word_tokenize(text)\n    \n    def normalize(self, text):\n        text = self.lower(text)\n        text = self.remove_url(text)\n        text = self.remove_email(text)\n        text = self.remove_mentions(text)\n# #         text = self.correct_spelling(text)\n#         text = self.remove_stop_words(text)\n        text = self.remove_punctuations(text)\n        text = text.strip()\n        return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We initialize the Normalizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"normalizer = Normalizer()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply all the selected normalizer operations on the text column in our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(normalizer.normalize)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word cloud"},{"metadata":{},"cell_type":"markdown","source":"we create a Word Cloud to visualize the most occuring words in our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_cloud(text, Stop_words=None,path=None,height=3000, width=3000):\n    wc = WordCloud(background_color=\"white\", max_words=2000,\n                contour_width=3, contour_color='steelblue', stopwords=Stop_words,height=3000,width=3000).generate(text)\n    wc.to_file(path)\n    plt.imshow(wc, interpolation='bilinear', shape=(width, height ))\n    plt.axis(\"off\")\n    plt.figure()\n\nall_text = ''\nfor row in df.text:\n    all_text += ' ' + str(row)\nword_cloud(all_text, path='Cloud.png')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Model"},{"metadata":{},"cell_type":"markdown","source":"## preprocessing for ML model"},{"metadata":{},"cell_type":"markdown","source":"We create a tf-idf vectorizer to represent our data in format that would be acceptable for the ML models."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1,1))\nX = vectorizer.fit_transform(df.text)\ny = df.airline_sentiment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### split the data into train and test"},{"metadata":{},"cell_type":"markdown","source":"we use stratify to equally split the three classes into train and test groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create ml model"},{"metadata":{},"cell_type":"markdown","source":"### MNB Classifier"},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf works well."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_mnb = MultinomialNB().fit(X_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM Classifier"},{"metadata":{},"cell_type":"markdown","source":"Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\nLinearSVC is capable of performing multi-class classification on a dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svm = svm.LinearSVC(penalty='l2',C=1).fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evluate Ml model"},{"metadata":{},"cell_type":"markdown","source":"score() : In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"MNB score for train\",clf_mnb.score(X_train, y_train))\nprint (\"MNB score for test\",clf_mnb.score(X_test, y_test))\nprint (\"SVM score for train\",clf_svm.score(X_train, y_train))\nprint (\"SVM score for test\",clf_svm.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"tried naive bayes, random forest and svm and select the two highest accuracy"},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Model"},{"metadata":{},"cell_type":"markdown","source":"## preprocessing for DL"},{"metadata":{},"cell_type":"markdown","source":"### tokenizer"},{"metadata":{},"cell_type":"markdown","source":"converts Python text strings to streams of token objects, where each token object is a separate word, punctuation sign, number/amount, date, e-mail, URL/URI, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df.text.values)\nX = tokenizer.texts_to_sequences(df.text.values)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### padding"},{"metadata":{},"cell_type":"markdown","source":"padding each vector to fit the dl model required format"},{"metadata":{},"cell_type":"markdown","source":"takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### split data into train and test"},{"metadata":{},"cell_type":"markdown","source":"encoded the labels of the three classes to fit the model required format"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(df['airline_sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42, stratify=Y)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### split train data to train and validation set"},{"metadata":{},"cell_type":"markdown","source":"The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vald = X_train[:500]\nY_vald = Y_train[:500]\nx_train = X_train[500:]\ny_train = Y_train[500:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create DL Model"},{"metadata":{},"cell_type":"markdown","source":" I used: Tensorflow and Keras to define the neural network\n \n Embedding: will learn an embedding for all of the words in the training datase\n \n Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n \n \n LSTM :  allows you to specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer\n \n \n Dense: recieves input from all the neurons in the previous layer, thus densely connected\n \n \n categorical_crossentropy: as our target to classify three labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train the model"},{"metadata":{},"cell_type":"markdown","source":"The batch size defines the number of samples that will be propagated through the network."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\nhistory = model.fit(x_train, \n                    y_train, \n                    epochs = 10, \n                    batch_size=batch_size, \n                    validation_data=(X_vald, Y_vald))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualize the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model"},{"metadata":{},"cell_type":"markdown","source":"Returns the loss value & metrics values for the model in test mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test, Y_test)\nscores","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}