{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Context\nThis is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)\n\n# Content\nAttribute Information:\n\nId number: 1 to 214 (removed from CSV file)\nRI: refractive index\nNa: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\nMg: Magnesium\nAl: Aluminum\nSi: Silicon\nK: Potassium\nCa: Calcium\nBa: Barium\nFe: Iron\n\nType of glass: (class attribute) \n* -- 1 buildingwindowsfloatprocessed -- 2 buildingwindowsnonfloatprocessed -- 3 vehiclewindowsfloatprocessed \n* -- 4 vehiclewindowsnonfloatprocessed (none in this database) \n* -- 5 containers \n* -- 6 tableware \n* -- 7 headlamps"},{"metadata":{},"cell_type":"markdown","source":"# **Principal Component Analysis (PCA)**\nPCA is a dimensionality-reduction technique that is often used to transform a high-dimensional dataset into a smaller-dimensional subspace prior to running a machine learning algorithm on the data\n## So how can this algorithm help us? What are the uses of this algorithm?\n* Identifies the most relevant directions of variance in the data.\n* Helps capture the most “important” features.\n* Easier to make computations on the dataset after the dimension reductions since we have fewer data to deal with.\n* Visualization of the data.\n\n## So what are the steps to make PCA work? How do we apply the magic?\n1. Take the dataset you want to apply the algorithm on.\n2. Calculate the covariance matrix.\n3. Calculate the eigenvectors and their eigenvalues.\n4. Sort the eigenvectors according to their eigenvalues in descending order.\n5. Choose the first K eigenvectors (where k is the dimension we’d like to end up with).\n6. Build new reduced dataset."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries¶\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Dataset¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/glass/glass.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a look at what our dataset looks like:¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\nThe first preprocessing step is to divide the dataset into a feature set and corresponding labels. The following script performs this task: The script above stores the feature sets into the X variable and the series of corresponding labels in to the y variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:,0:9]\ny = data.iloc[:,9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization of the data¶\nIf you’re familiar with data analysis and processing, you know that missing out on standardization will probably result in a biased outcome. Standardization is all about scaling your data in such a way that all the variables and their values lie within a similar range.We will perform standard scalar normalization to normalize our feature set. To do this, execute the following code"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data = StandardScaler().fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA\nIt is only a matter of three lines of code to perform PCA using Python's Scikit-Learn library. The PCA class is used for this purpose. PCA depends only upon the feature set and not the label data. Therefore, PCA can be considered as an unsupervised machine learning technique. Performing PCA using Scikit-Learn is a two-step process:\n\n1. Initialize the PCA class by passing the number of components to the constructor.\n2. Call the fit and then transform methods by passing the feature set to these methods. The transform method returns the specified number of principal components."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\npct = pca.fit_transform(x)\n\nprincipal_df = pd.DataFrame(pct,columns=['pc1','pc2'])\n\nfinaldf= pd.concat([principal_df,data[['Type']]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nsn.FacetGrid(finaldf, hue=\"Type\", size=6).map(plt.scatter, 'pc1', 'pc2').add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA for dimensionality redcution (not for visualization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npca.n_components = 9\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (t-SNE) t-Distributed Stochastic Neighbor Embedding \n* t-SNE is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n* t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing. To keep things simple, here’s a brief overview of working of t-SNE\n                                                                                                \n                                                                                                "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = sample_data[0:214,:]\nlabels_1000 = y[0:214]\n\nmodel = TSNE(n_components=2, random_state=0,perplexity=30,n_iter=5000)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"Type\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"Type\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}