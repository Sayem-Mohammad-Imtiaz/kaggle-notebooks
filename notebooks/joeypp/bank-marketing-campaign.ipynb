{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Bank Marketing Campaign Analysis**\n\n**Introduction**\nThis dataset describes Portugal Bank marketing campaign results. The campaigns were conducted mostly on direct phone calls to offer clients a term deposit in the bank. If clients agreed, the result is marked as 'yes' or else 'no'.\nClient specific information is gathered like job, age, education, marital status, if there was a previous effort etc.\n\nTask: Predict if a customer will be willing to open a term deposit given certain information about the client. This way, we can target certain people who can be potential customers.\n\nApproach:\n1. Initially, load the dataset and do some EDA\n2. Perform encoding on categorical data\n3. Fit a basic logistic regression model\n4. Perform Grid Search CV and check if there is any score improvement.\n5. Handle imbalanced classes by oversampling using SMOTE\n6. Check if normalization or scaling is required\n7. Plot AUC curve and check confusion matrix to look at TP and FP\n\nNow, to improve score, lets try feature engineering, ensemble techniques. As the classes are imbalanced (we have more 'no' values then 'yes'), lets consider oversampling for 'yes' class and see if score improved.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n!python3 -m pip install -U scikit-learn\n!pip install imblearn\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        inp= os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.\n\ninp_file = pd.read_csv(inp, sep=';')\n\nfor i in ['job', 'marital', 'education', 'contact']:\n    plt.figure(figsize=(10,4))\n    sns.countplot(x=i,hue='y', data=inp_file)\n    \ncorr = inp_file.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True)\n\ninp_file = pd.get_dummies(inp_file, columns=['job', 'marital', 'education', 'default', 'housing', 'loan',\n       'contact', 'month', 'day_of_week', 'poutcome'], drop_first=True)\nlabels = inp_file['y'].unique().tolist()\nmapping = dict( zip(labels,range(len(labels))) )\ninp_file.replace({'y': mapping},inplace=True)\n\ninp_file=inp_file.drop(columns={'job_unknown', 'marital_unknown', 'education_unknown', 'default_unknown', 'housing_unknown', 'loan_unknown'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the dataset into train and test and use stratified split by 'y' variable as classes are imbalanced.\n\ntrain, test = train_test_split(inp_file, test_size=0.2, random_state=0, stratify=inp_file['y'])\ntrain_x=train.drop(columns={'y'})\ntrain_y=train['y']\ntest_x=test.drop(columns={'y'})\ntest_y=test['y']\n\n#####Base model with all features################\nbasemodel = LogisticRegression(solver='lbfgs',max_iter=10000)\nbasemodel.fit(train_x, train_y)\npredictions_bm=basemodel.predict(test_x)\nscore_bm = basemodel.score(test_x, test['y'])\n\nprint(\"Base training model accuracy score: \"+str(basemodel.score(train_x, train['y'])))\nprint(\"Score of base model on test data is:\"+str(score_bm))\ny_probas = basemodel.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\ncm_bm = metrics.confusion_matrix(test_y, predictions_bm, [0,1])\nprint(\"Confusion Matrix of base model:\")\nprint(cm_bm)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy for base model for training data is 91% and for test data is almost 91%. Looks like data is generalizing well on test data.\nBut, we have many false positives and false negatives. So lets try checking if there is any multicollinearity."},{"metadata":{"trusted":true},"cell_type":"code","source":"#####Base model after dropping highly correlated features########\n##Calculating VIF####\n\ncc = np.corrcoef(train_x, rowvar=False)\nVIF = np.linalg.inv(cc)\na=list(VIF.diagonal())\n\nprint(\"IVF values:\")\nfor i in a:\n    if i>=5:\n        print(train_x.columns.values[a.index(i)]+':'+str(i))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the features which have high VIF\n\ntrain_x=train_x.drop(columns=[ 'emp.var.rate','cons.price.idx', 'poutcome_success', 'euribor3m', 'nr.employed'])\ntest_x=test_x.drop(columns=[  'emp.var.rate','cons.price.idx', 'poutcome_success', 'euribor3m', 'nr.employed'])\n\nbasemodel = LogisticRegression(solver='lbfgs',max_iter=10000)\nbasemodel.fit(train_x, train_y)\npredictions_bm=basemodel.predict(test_x)\nscore_bm = basemodel.score(test_x, test['y'])\n\nprint(\"Base training model accuracy score: \"+str(basemodel.score(train_x, train['y'])))\nprint(\"Score of base model is:\"+str(score_bm))\ny_probas = basemodel.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\ncm_bm = metrics.confusion_matrix(test_y, predictions_bm, [0,1])\nprint(\"Confusion Matrix of base model:\")\nprint(cm_bm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see there is a tiny drop in the score after dropping highly correlated features. It looks like our model was overfitting earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#####Base model after dropping highly correlated features########\n##Calculating VIF####\n\ncc = np.corrcoef(train_x, rowvar=False)\nVIF = np.linalg.inv(cc)\na=list(VIF.diagonal())\n\nprint(\"IVF after dropping some columns:\")\nfor i in a:\n    if i>=5:\n        print(train_x.columns.values[a.index(i)]+':'+str(i))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply grid search cv and run logistic reg\n#Dropped correlated features.\n\ngridmodel = LogisticRegression(solver='lbfgs')\npenalty = ['l2']\nmax_iter=[10000]    \n# Create regularization hyperparameter space\nC = [1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5]\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty, max_iter=max_iter)\nmodel_gs = GridSearchCV(gridmodel, hyperparameters, cv=5, verbose=0)\nbest_model = model_gs.fit(train_x,train_y)\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\nprint(best_model.score(train_x, train_y))\nscore_best = best_model.score(test_x, test_y)\nprint(\"Best model score:\"+str(score_best))\npredictions_best=best_model.predict(test_x)\ny_probas = best_model.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\ncm_best = metrics.confusion_matrix(test_y, predictions_best, [0,1])\nprint(\"Confusion matrix of best model:\")\nprint(cm_best)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above step, we tried cross validation using grid search and picked the best parameters for the model. The accuracy score for training data is 90.7% and for test data is 90.5%. They are pretty close."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Perform oversampling as true positve rate is low for 'yes' and then fit basic log reg\n#Dropped correlated features.\n\nX_resampled, y_resampled = SMOTE().fit_resample(train_x, train_y)\nbasemodel_resampled = LogisticRegression(solver='lbfgs',max_iter=10000)\nbasemodel_resampled.fit(X_resampled, y_resampled)\npredictions_bm=basemodel_resampled.predict(test_x)\nscore_bm = basemodel_resampled.score(test_x, test['y'])\n\nprint(basemodel_resampled.score(X_resampled, y_resampled))\nprint(\"Score of base model after oversampling is:\"+str(score_bm))\ny_probas = basemodel_resampled.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\ncm_bm = metrics.confusion_matrix(test_y, predictions_bm, [0,1])\nprint(\"Confusion Matrix of base model after oversampling:\")\nprint(cm_bm)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy for test rate dropped, but improved for training set after oversampling. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resampling, hyperparameter tuning with grid search cv to pick best model.\n\ngrid_resampled = LogisticRegression(solver='lbfgs')\npenalty = ['l2']\nmax_iter=[10000]    \n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty, max_iter=max_iter)\nmodel_gs = GridSearchCV(grid_resampled, hyperparameters, cv=5, verbose=0)\nbest_model = model_gs.fit(X_resampled,y_resampled)\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\nscore_best = best_model.score(test_x, test_y)\n\nprint(best_model.score(X_resampled, y_resampled))\nprint(\"Best model score:\"+str(score_best))\npredictions_best=best_model.predict(test_x)\ny_probas = best_model.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\ncm_best = metrics.confusion_matrix(test_y, predictions_best, [0,1])\nprint(\"Confusion matrix of best model:\")\nprint(cm_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}