{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Requirements\n1. Load the data and check its correctness\n2. Explore the basic parameters: how many data points do we have? What are the targets and what is their distribution? Any kind of exploratory data analysis is welcome\n3. Identify the problem: is it regression? classification?\n4. Identify metric you're going to use\n5. Design and run the experiment: train and validate your model\n6. Compare your results with some kind of baseline (simplest possible solution to the problem)\n7. (Optional) estimate feature importances and select the most important features\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Task 1 âœ…\nLoad the data and check its correctness\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data into df\ndf = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check content\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> All features are categorical. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Task 2 âœ…\nExplore the basic parameters: how many data points do we have? What are the targets and what is their distribution? Any kind of exploratory data analysis is welcome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> There are 8124 datapoints.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe data: simple stats\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data types\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distributions of features and target (categorical)\ncategorical_features = df.columns # all columns are categorical\nfig, ax = plt.subplots(len(categorical_features), 1, figsize=(6,len(categorical_features)*5))\nfor i, categorical_feature in enumerate(df[categorical_features]):\n    df[categorical_feature].value_counts().plot(kind=\"bar\", ax=ax[i]).set_title(categorical_feature)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> The target class can be \"e\" or \"p\". Both \"e\" and \"p\" seem to be about equally likely.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\ndf.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 3 âœ…\nIdentify the problem: is it regression? classification?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"class\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> The target is class. Class is categorical, therefore the task is a classification.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Task 4 âœ…\nIdentify metric you're going to use","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use the F1 score, since it considers both precision and recall.\n\nComparison of classification metrics:\nhttps://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n\nWe can alter the Î² parameter to value precision over recall, because wrongly predicting a poisonous (\"p\") mushroom as edible (\"e\") is worse than predicting a edible (\"e\") mushroom as poisonous (\"p\").\n\n\"The beta parameter determines the weight of recall in the combined score. beta < 1 lends more weight to precision, while beta > 1 favors recall (beta -> 0 considers only precision, beta -> +inf only recall).\" - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"beta = 0.5 # we give recall half the importance of precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\n# fbeta_score example\n\ny_true = [0, 1, 1, 0, 1, 1]\ny_pred = [0, 0, 1, 0, 0, 1]\n\nfbeta_score(y_true, y_pred, beta=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 5 âœ…\nDesign and run the experiment: train and validate your model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode target: we aim to predict poisonous mushrooms => we need high precision\ny = df[\"class\"].map({'p':1, 'e':0})\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode features\nX = df.drop(columns=[\"class\"]).apply(LabelEncoder().fit_transform)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Train/Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y,\n                                                   test_size=0.2,\n                                                   random_state=0)\n\nprint(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"Y_train: \", Y_train.shape)\nprint(\"Y_test: \", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Train Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We choose XGBoost, because this is a classification with only categorical features and XGBoost has proven to give a good idea of what is possible on this type of dataset especially with classification tasks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_train, Y_train, eval_metric=\"auc\", eval_set=[(X_test, Y_test)], verbose=False) # TODO use fbeta_score for eval_metric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Evaluate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X)\nfbeta_score(y, pred, beta=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> We are able to achieve perfect classification ðŸŽ‰.\nEven when training with auc instead of fbeta_score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Task 6 âœ…\nCompare your results with some kind of baseline (simplest possible solution to the problem)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We choose the baseline of predicting all mushrooms as poisonous (class=1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_baseline = np.ones(y.shape) # class=1\nfbeta_score(y, pred_baseline, beta=beta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The baseline of predicting all mushrooms as edible (class=0) leeds to the worst score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_baseline = np.zeros(y.shape) # class=0\nfbeta_score(y, pred_baseline, beta=beta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 7 âœ…\n(Optional) estimate feature importances and select the most important features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBoost allows us to display feature importance for the trained model. More details: https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importance(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> The three most important features according to the trained XGBoost model are:\n1. spore-print-color\n2. odor\n3. gill-size","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}