{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet50 import ResNet50,preprocess_input\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Vocab dictionary and caption preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/flickr-image-dataset/flickr30k_images/results.csv',delimiter='|')\n\ndf.head()\n\ndf.iloc[0]\n\nid=df['image_name'].values\n\nid.shape\n\ncomment=df[' comment'].values\n\ncomment.shape\n\ncomment[5]\n\ndef sentence_cleaning(sentence):\n    try:\n        sentence=sentence.lower()\n        sentence=re.sub('[^a-z]+',' ',sentence)\n        sentence=sentence.split()\n        sentence=[s for s in sentence if len(s)>1]\n        sentence=' '.join(sentence)\n        return(sentence)\n    except:\n        return(sentence_cleaning('A dog runs across the grass .'))\n\nvocab_dic={}\nfor i in range(comment.shape[0]):\n    if id[i] not in vocab_dic:\n        vocab_dic[id[i]]=[]\n    sen=sentence_cleaning(comment[i])\n    vocab_dic[id[i]].append(sen)\n\nlen(vocab_dic)\n\nprint(vocab_dic['1000092795.jpg'])\n\nword_dic={}\n\nfor i in vocab_dic:\n    for j in vocab_dic[i]:\n        l=j.split()\n        for k in l:\n            if k not in word_dic:\n                word_dic[k]=1\n            else:\n                word_dic[k]+=1\n\nprint(len(word_dic))\n\nfinal_words=[x for x in word_dic if word_dic[x]>10]\n\nprint(len(final_words))\n\nfor i in vocab_dic:\n    for j in range(len(vocab_dic[i])):\n        vocab_dic[i][j]='startseq '+vocab_dic[i][j]+' endseq'\n\ns=1\nword_to_idx={}\nidx_to_word={}\nfor i in final_words:\n    word_to_idx[i]=s\n    idx_to_word[s]=i\n    s+=1\n\nprint(len(word_to_idx))\n\n### Two Special words\nword_to_idx['startseq']=5119\nword_to_idx['endseq']=5120\nidx_to_word[5119]='startseq'\nidx_to_word[5120]='endseq'\n\nlen(word_to_idx)\n\nvocab_size=len(word_to_idx)+1 # adding one for 0 because that will also in our vector\n\nmax_len=20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=ResNet50(weights='imagenet',input_shape=(224,224,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model=Model(model.input,model.layers[-2].output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_img(path):\n    img=image.load_img(path,target_size=(224,224,3))\n    img=image.img_to_array(img)\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)#mormalizing the img\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img=preprocess_img('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/10002456.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_img(path):\n    img=preprocess_img(path)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((-1,))\n    return feature_vector\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_description['1244140539_da4804d828']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_img_dic={}\ns=0\nfor i in vocab_dic:\n    path='../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+i\n    encoded_img_dic[i]=encode_img(path)\n    s+=1\n    if s%100==0:\n        print(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('encoded_img_dic.npy',encoded_img_dic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l --b=M  ./encoded_img_dic.npy | cut -d \" \" -f5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Custom Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(train_description,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size):\n    X1,X2,y=[],[],[]\n    n=0\n    while True:\n        for key,desc_list in train_description.items():\n            n+=1\n            encoding_of_photo=encoded_img_dic[key]\n            for desc in desc_list:\n                seq=[word_to_idx[i] for i in desc.split() if i in word_to_idx]\n                for i in range(1,len(seq)):\n                    xi=seq[0:i]\n                    yi=seq[i]\n                    \n                    xi=pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n                    yi=to_categorical([yi],num_classes=vocab_size)[0]\n                    \n                    \n                    X1.append(encoding_of_photo)\n                    X2.append(xi)\n                    y.append(yi)\n                if n==batch_size:\n                    yield [np.array(X1),np.array(X2)],np.array(y)\n                    X1,X2,y=[],[],[]\n                    n=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('glove.6B.50d.txt',encoding='utf8') as f:\n#     glove_data=f.read()\n\n# len(glove_data)\n\n# type(glove_data)\n\n# glove_data=glove_data.split('\\n')\n\n# type(glove_data)\n\n# len(glove_data)\n\n# glove_data=glove_data[:-1]\n\n# len(glove_data)\n\n# glove_data[0].split()[0]\n\n# embedding_index={}\n# for line in glove_data:\n#     line=line.split()\n#     word=line[0]\n#     embeding=np.array(line[1:],dtype='float')\n#     embedding_index[word]=embeding\n    \n\n# len(embedding_index)\n\n# embedding_index['the'].shape\n\n# def get_embedding_matrix():\n#     dim=50\n#     matrix=np.zeros((vocab_size,dim))\n#     for word,number in word_to_idx.items():\n#         embedding_vector=embedding_index.get(word)\n#         if embedding_vector is not None:\n#             matrix[word_to_idx[word]]=embedding_vector\n#     return matrix\n    \n\n# embedding_matrix=get_embedding_matrix()\n\n# len(embedding_matrix)\n\n# embedding_matrix[word_to_idx['the']]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## loading embedding matrix using numpy"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix=np.load('../input/embedding-matrix/embedding_matrix.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_img_dic['10002456.jpg'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For images\ninput_img_features=Input(shape=(2048,))\ninp_img1=Dropout(0.3)(input_img_features)\ninp_img2=Dense(256,activation='relu')(inp_img1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_captions=Input(shape=(max_len,))\ninp_cap1=Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\ninp_cap2=Dropout(0.3)(inp_cap1)\ninp_cap3=LSTM(256)(inp_cap2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder1=add([inp_img2,inp_cap3])\ndecoder2=Dense(256,activation='relu')(decoder1)\noutputs=Dense(vocab_size,activation='softmax')(decoder2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_model=Model(inputs=[input_img_features,input_captions],outputs=outputs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_model.layers[2].output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_model.layers[2].set_weights([embedding_matrix])\nactual_model.layers[2].trainable=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_model.compile(loss='categorical_crossentropy',optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=10\nbatch_size=3\nsteps=len(vocab_dic)//batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    for i in range(epochs):\n        generator=data_generator(vocab_dic,vocab_size,word_to_idx,encoded_img_dic,max_len,batch_size)\n        actual_model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n        actual_model.save('./Models/model'+str(i)+'.h5')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.mkdir('Models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting the caption using the trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_caption(img):\n    img=img.reshape(1,224,224,3)\n    img=preprocess_input(img)\n    feature_vector=new_model.predict(img)\n    feature_vector=feature_vector.reshape((1,2048,1))\n    in_text='startseq'\n    for i in range(max_len):\n        seq=[word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n        seq=pad_sequences([seq],maxlen=max_len,padding='post')\n        y_pred=actual_model.predict([feature_vector,seq])\n        y_pred=y_pred.argmax()\n        word=idx_to_word[y_pred]\n        in_text+=' '+word\n        \n        if word=='endseq':\n            break\n    final_caption=in_text.split()[1:-1]\n    final_caption=' '.join(final_caption)\n    return final_caption","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img=image.load_img('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/10010052.jpg',target_size=(224,224,3))\nimg=image.img_to_array(img)\nplt.imshow(img/255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_caption(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -l --b=M  ./Models/model5.h5 | cut -d \" \" -f5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}