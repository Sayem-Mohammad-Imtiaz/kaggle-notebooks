{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\nToday we are going to try to classify a movie genre using it's plot"},{"metadata":{},"cell_type":"markdown","source":"### Main imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### data loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"wiki_movie_pure_data = pd.read_csv('/kaggle/input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_movie_pure_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration [Part 1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(wiki_movie_pure_data['Release Year'], color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y=\"Origin/Ethnicity\", data=wiki_movie_pure_data, color=\"pink\", order = wiki_movie_pure_data[\"Origin/Ethnicity\"].value_counts().index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Remove all unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = wiki_movie_pure_data[['Plot', 'Genre']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove all plots with unknown genre"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = df_data[df_data['Genre'] != 'unknown']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the chosen data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count of rows in the dataframe: ', len(df_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Delete puctuation from plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ntranslator = str.maketrans('','',string.punctuation)\ndf_data['Plot'] = df_data.apply(lambda row : row['Plot'].translate(translator), axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encode genre"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n  \ndf_data['Genre']= label_encoder.fit_transform(df_data['Genre']) \n  \ndf_data['Genre'].unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at length of plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean plot length: ', df_data['Plot'].apply(lambda x: len(x.split())).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save current splitted data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_validation, df_test = np.split(df_data.sample(frac=1), [int(.7*len(df_data)), int(.8*len(df_data))])\n\ndf_train.to_csv('data_train.csv', index=False)\ndf_validation.to_csv('data_validation.csv', index=False)\ndf_test.to_csv('data_test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create DataText model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import Field\ntokenize = lambda x: x.split()\nSENTENCE_LEN = 400\n\nTEXT = Field(sequential=True, tokenize=tokenize, lower=True, init_token='<START>', eos_token='<END>', fix_length=SENTENCE_LEN)\nLABEL = Field(sequential=False, use_vocab=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import TabularDataset\n\nwiki_movie_datafields = [(\"Plot\", TEXT), (\"Genre\", LABEL)]\ntrain_td, vad_td = TabularDataset.splits(\n               path=\"/kaggle/working\",\n               train='data_train.csv', validation=\"data_validation.csv\",\n               format='csv',\n               skip_header=True,\n               fields=wiki_movie_datafields)\n\ntest_datafields = [(\"Plot\", TEXT), (\"Genre\", None)]\ntest_td = TabularDataset(\n           path=\"/kaggle/working/data_test.csv\",\n           format='csv',\n           skip_header=True,\n           fields=test_datafields)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT.build_vocab(train_td)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build iterators"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import Iterator, BucketIterator\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nBATH_SIZE = 64\n\ntrain_iter, val_iter = BucketIterator.splits(\n (train_td, vad_td),\n batch_sizes=(BATH_SIZE, BATH_SIZE),\n device=-1, # if you want to use the GPU, specify the GPU number here\n sort_key=lambda x: len(x.Plot),\n sort_within_batch=False,\n repeat=False \n)\n\ntest_iter = Iterator(test_td, batch_size=BATH_SIZE, device=-1, sort=False, sort_within_batch=False, repeat=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wrap batch iterators"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BatchWrapper:\n      def __init__(self, dl, x_var, y_vars):\n            self.dl, self.x_var, self.y_vars = dl, x_var, y_vars \n\n      def __iter__(self):\n            for batch in self.dl:\n                  x = getattr(batch, self.x_var)\n                  if self.y_vars is not None:\n                      y = getattr(batch, self.y_vars)\n                      yield (x, y)\n                  else: \n                      yield (x, -1)\n\n      def __len__(self):\n            return len(self.dl)\n\ntrain_dl = BatchWrapper(train_iter, \"Plot\", \"Genre\")\nvalid_dl = BatchWrapper(val_iter, \"Plot\", \"Genre\")\ntest_dl = BatchWrapper(test_iter, \"Plot\", None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nclass SimpleLSTMBaseline(nn.Module):\n    def __init__(self, hidden_dim, emb_dim=300, num_linear=1):\n        super().__init__() # don't forget to call this!\n        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1)\n        self.linear_layers = []\n        for _ in range(num_linear - 1):\n            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n            self.linear_layers = nn.ModuleList(self.linear_layers)\n        self.predictor = nn.Linear(hidden_dim, 1)\n\n    def forward(self, seq):\n        hdn, _ = self.encoder(self.embedding(seq))\n        feature = hdn[-1, :, :]\n        for layer in self.linear_layers:\n          feature = layer(feature)\n        preds = self.predictor(feature)\n        return preds\n\nem_sz = 100\nnh = 500\nnl = 10\nmodel = SimpleLSTMBaseline(nh, emb_dim=em_sz, num_linear=nl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nimport torch\n\nopt = optim.Adam(model.parameters(), lr=1e-3)\nloss_func = nn.MSELoss()\n\nepochs = 2\n\nfor epoch in range(1, epochs + 1):\n    running_loss = 0.0\n    running_corrects = 0\n    model.train()\n    for x, y in tqdm.tqdm(train_dl, position=0, leave=True):\n        opt.zero_grad()\n\n        preds = model(x)\n        loss = loss_func(y, preds.squeeze())\n        loss.backward()\n        opt.step()\n\n        running_loss += loss.data * x.size(0)\n\n    epoch_loss = running_loss / len(train_td)\n\n    val_loss = 0.0\n    model.eval()\n    for x, y in tqdm.tqdm(valid_dl, position=0, leave=True):\n        preds = model(x)\n        loss = loss_func(y, preds)\n        val_loss += loss.data * x.size(0)\n\n    val_loss /= len(vad_td)\n    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}