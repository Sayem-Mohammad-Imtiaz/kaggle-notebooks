{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This dataset gives a number of variables along with a target condition of having or not having heart disease. It contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In addition, we will analyze for this dataset. We will use a wide range of tools for this part. If there's value in there, we'il do it there. Finally, machine learning algorithms are estimated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading appropriate libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.svm import SVC\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cause of Heart Disease","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1.  Excess weight, especially around the stomach area, increases a woman's risk of developing cardiovascular disease and lack of physical activity makes it worse.\n2. Diabetes causes damage to blood vessels so diabetes is a major factor in developing cardiovascular disease.\n3. Unhealthy foods, lack of exercise, lead to heart disease. So can high blood pressure, infections, and birth defects.\n4. Smoking is one of the biggest causes of cardiovascular disease.\n5. Just a few cigarettes a day can damage the blood vessels and reduce the amount of oxygen available in our blood.\n*  But other things might surprise you.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Loading the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n1.age: The person's age in years\n\n2.sex: The person's sex (1 = male, 0 = female)\n\n3.cp: The chest pain experienced (Value 0: typical angina, Value 1: atypical angina, Value 2: non-anginal pain, Value 3: asymptomatic)\n\n4.trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\n5.chol: The person's cholesterol measurement in mg/dl\n\n6.fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n\n7.restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n8.thalach: The person's maximum heart rate achieved\n\n9.exang: Exercise induced angina (1 = yes; 0 = no)\n\n10.oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\n11.slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\n12.ca: The number of major vessels (0-3)\n\n13.thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n14.target: Heart disease (0 = no, 1 = yes)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe function is a function that allows analysis between the numerical values contained in the data set. Using this function count, mean, std, min, max, 25%, 50%, 75%.\nAs seen in this section, most values are generally categorized. This means that we need to integrate other values into this situation. These; age, trestbps, chol, thalach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet/nutrition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there are no missing data in this dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(10,8), dpi= 80)\nsns.heatmap(df.corr(), cmap='RdYlGn', center=0)\n\n# Decorations\nplt.title('Correlogram of mtcars', fontsize=22)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from theabove corelation plot we see that cp(chest pain),thalch and slope are highly corelated with the target.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visulization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### TARGET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['target'],rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"target\"\ncolors = ['gold', 'blue']\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\n#trace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0],\n               marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nlayout = {'title': 'Target(0 = No, 1 = Yes)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the above plot we see that 45.5% people does not have disease and 54.5% have heart disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### SEX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['sex'],rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"sex\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Male(1), Female(0)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the heart disease UCI dataset only 31.7% are female and rest are male. we have find that either male or female which have likely to heart disease.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,5))\nsns.countplot(x=df.target,hue=df.sex)\nplt.legend(labels=['Female', 'Male'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### from the above plot we see that the rate of heart disease in females have more in comprission of male. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Women are 4 times more likely to die from heart disease than breast cancer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Cardiovascular disease is the leading cause of death in women in Australia with 90% of women having one risk factor. \n* The causes including high blood pressure, high cholesterol, smoking, diabetes, weight and family history are discussed.\n* A woman's risk also goes up if she's had a miscarriage or had her ovaries or uterus removed.\n*  Women's hearts are affected by stress and depression more than men's. Depression makes it difficult to maintain a healthy lifestyle.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### CHAIST PAIN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['cp'],rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,5))\nsns.countplot(x=df.target,hue=df.cp)\nplt.legend(labels=['0: typical angina', '1: atypical angina','2: non-anginal pain','3: asymptomatic'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above plot we see that 27.2% persons having chaist pain type 0, 82% having chaist pain type 1, 79.3% having chaist pain type 2 and 69.5% having chaist pain type 3. These person have heart disease, from this data we observe that those who have chaist pain type 1 and chaist pain type 2 is more likely to affected by heart disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### THALACH","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['thalach'],rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The person having the heart rate over 140 is more likely to have heart disease, therefor we conclude that we have to check our heart rate monthly if its over the thalach 140 then we have to concult the doctor and much concious to the health.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### FASTING BLOOD SUGAR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,5))\nsns.countplot(x=df.target,hue=df.fbs)\n#fbs(> 120 mg/dl, 1 = true; 0 = false\nplt.title('fbs(> 120 mg/dl)')\nplt.legend(labels=['0: False', '1: True'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 51.2% person having the fasting blood sugar rate over 120 mg/dl and 55% person having the fasting blood sugar rate below 120 mg/dl is affected by heart disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### AGE","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"col='age'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"blue\"),mode='lines+markers')\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"red\"),mode='lines+markers')\ndata = [trace1, trace2]\nlayout={'title':\"Target With Respect to age\",'xaxis':{'title':\"Age\"}}\nfig = go.Figure(data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot we identify that person having age between 40 to 65 year is more likely to affected by heart disease, therefor person having the age in this range have to more concisous about there health. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Symptoms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Chest pain, chest tightness, chest pressure and chest discomfort (angina)\n* Shortness of breath\n* Pain, numbness, weakness or coldness in your legs or arms if the blood vessels in those parts of your body are narrowed\n* Pain in the neck, jaw, throat, upper abdomen or back.\n* Heart failure is also an outcome of heart disease, and breathlessness can occur when the heart becomes too weak to circulate blood.\n* Some heart conditions occur with no symptoms at all, especially in older adults and individuals with diabetes.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"chest_pain=pd.get_dummies(df['cp'],prefix='cp',drop_first=True)\ndf=pd.concat([df,chest_pain],axis=1)\ndf.drop(['cp'],axis=1,inplace=True)\nsp=pd.get_dummies(df['slope'],prefix='slope')\nth=pd.get_dummies(df['thal'],prefix='thal')\nframes=[df,sp,th]\ndf=pd.concat(frames,axis=1)\ndf.drop(['slope','thal'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['target'], axis = 1)\ny = df.target.values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\nfrom sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"StandardScaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Training & Testing of Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.XGBoost\n\n> XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\n> > > XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) ... A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nalg = XGBClassifier(learning_rate=0.01, n_estimators=2000, max_depth=8,\n                        min_child_weight=0, gamma=0, subsample=0.52, colsample_bytree=0.6,\n                        objective='binary:logistic', nthread=4, scale_pos_weight=1, \n                    seed=27, reg_alpha=5, reg_lambda=2, booster='gbtree',\n            n_jobs=-1, max_delta_step=0, colsample_bylevel=0.6, colsample_bynode=0.6)\nalg.fit(X_train, y_train)\nprint('train accuracy',alg.score(X_train, y_train))\nprint('test accuracy',alg.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nxgb_prob = alg.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(y_test, xgb_prob)\nskplt.metrics.plot_ks_statistic(y_test, xgb_prob)\nskplt.metrics.plot_cumulative_gain(y_test, xgb_prob)\nskplt.metrics.plot_lift_curve(y_test, xgb_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas_list1 = [alg.predict_proba(X_test)]\nxy=['xgb']\nskplt.metrics.plot_calibration_curve(y_test,\n                                      probas_list1,\n                                    xy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport,ConfusionMatrix\nclasses=[0,1]\nvisualizer = ClassificationReport(alg, classes=classes)\n#visualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Random Forest\n \n> Random forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.\n> > > The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=600,random_state=0, n_jobs= -1)\nrf = rf.fit(X_train, y_train)\nprint('train accuracy',rf.score(X_train, y_train))\nprint('test accuracy',rf.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nrdf_prob = rf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(y_test, rdf_prob)\nskplt.metrics.plot_ks_statistic(y_test, rdf_prob)\nskplt.metrics.plot_cumulative_gain(y_test, rdf_prob)\nskplt.metrics.plot_lift_curve(y_test, rdf_prob)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas_list1 = [rf.predict_proba(X_test)]\nxy=['rdf']\nskplt.metrics.plot_calibration_curve(y_test,\n                                      probas_list1,\n                                    xy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport,ConfusionMatrix\nclasses=[0,1]\nvisualizer = ClassificationReport(rf, classes=classes)\n#visualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plots Used for showing the Classification Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ROC curve\n* A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n* A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve. A no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases\n\n#### Lift curve\n> **A lift curve shows the ratio of a model to a random guess ('model cumulative sum' / 'random guess' from above). Cumulative gains charts are a bit mor. A lift curve is a way of visualizing the performance of a classification model. Lift curves are closely related to, and frequently confused with, cumulative gains charts**\n\n#### Calibration curve\n> **In analytical chemistry, a calibration curve, also known as a standard curve, is a general method for determining the concentration of a substance in an unknown sample by comparing the unknown to a set of standard samples of known**\n\n#### KS plot\n> **Kolmogorov-Smirnov chart measures performance of classification models. K-S is a measure of the degree of separation between the positive and negative distributions.**\n\n#### Cumulative gain plot\n> **The cumulative gains chart shows the percentage of the overall number of cases in a given category \"gained\" by targeting a percentage of the total number of cases.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preventions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Quit smoking.\n* Control other health conditions, such as high blood pressure, high cholesterol and diabetes.\n* Exercise at least 30 minutes a day on most days of the week.\n* Eat a diet that's low in salt and saturated fat.\n* Maintain a healthy weight.\n* Reduce and manage stress.\n* Practice good hygiene.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Summary\nWe started with the data exploration where we got a feeling for the dataset, checked about missing data and learned which features are important. During this process we used Plotly, seaborn and matplotlib to do the visualizations. During the data preprocessing part, we converted features into numeric ones, grouped values into categories and created a few new features. Afterwards we started training machine learning models, and applied cross validation on it. Of course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. You could also do some ensemble learning.Lastly, we looked at it’s confusion matrix and computed the models precision.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## I hope this kernel is helpfull for you","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}