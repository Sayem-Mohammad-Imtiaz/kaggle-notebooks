{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Detecting Myocardial Infarction"},{"metadata":{},"cell_type":"markdown","source":"The method and model are based on this article : https://arxiv.org/abs/1805.00794"},{"metadata":{},"cell_type":"markdown","source":"### Let's start with some imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config IPCompleter.greedy=True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fetching data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mitbih_train = pd.read_csv('../input/heartbeat/mitbih_train.csv', header=None)\ndf_mitbih_test = pd.read_csv('../input/heartbeat/mitbih_test.csv', header=None)\ndf_mitbih = pd.concat([df_mitbih_train, df_mitbih_test], axis=0)\n\ndf_ptbdb_normal = pd.read_csv('../input/heartbeat/ptbdb_normal.csv', header=None)\ndf_ptbdb_abnormal = pd.read_csv('../input/heartbeat/ptbdb_abnormal.csv', header=None)\ndf_ptbdb = pd.concat([df_ptbdb_normal, df_ptbdb_abnormal], axis=0)\n\nprint(df_mitbih.info())\nprint(df_ptbdb.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MITBIH dataset is constituted of 109446 beats, labeled with 5 different classes : \n\n'N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4\n\n    N : Non-ecotic beats (normal beat) \n\n    S : Supraventricular ectopic beats \n\n    V : Ventricular ectopic beats\n\n    F : Fusion Beats \n\n    Q : Unknown Beats\n\nThe PTBHB dataset is constituted of 14552 beats, labeled with two different classes : \n\n    '0' for normal beat\n    '1' for abnormal beat (Myocardial infarction)\n\nAll the beats are recorded with 187 points. The shorter beats are padded with zeros to reach 187."},{"metadata":{},"cell_type":"markdown","source":"## Visualization of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ptbdb\nM_ptbdb = df_ptbdb.values\nX_ptbdb = M_ptbdb[:,:-1]\ny_ptbdb = M_ptbdb[:,-1]\n\n# mitbih\nM_mitbih = df_mitbih.values\nX_mitbih = M_mitbih[:,:-1]\ny_mitbih = M_mitbih[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes={0:\"Normal\",\n         1:\"Artial Premature\",\n         2:\"Premature ventricular contraction\",\n         3:\"Fusion of ventricular and normal\",\n         4:\"Fusion of paced and normal\"}\nplt.figure(figsize=(15,4))\nfor i in range(0,5):\n    plt.subplot(2,3,i + 1)\n    all_samples_indexes = np.where(y_mitbih == i)[0]\n    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n    rand_samples = X_mitbih[rand_samples_indexes]\n    plt.plot(rand_samples.transpose())\n    plt.title(\"Samples of class \" + classes[i], loc='left', fontdict={'fontsize':8}, x=0.01, y=0.85)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes={0:\"Normal\", 1:\"Abnormal (MI)\"}\nplt.figure(figsize=(10,2))\nfor i in range(0,2):\n    plt.subplot(1,2,i + 1)\n    all_samples_indexes = np.where(y_ptbdb == i)[0]\n    rand_samples_indexes = np.random.randint(0, len(all_samples_indexes), 3)\n    rand_samples = X_ptbdb[rand_samples_indexes]\n    plt.plot(rand_samples.transpose())\n    plt.title(\"Samples of class \" + classes[i], loc=\"left\", fontdict={'fontsize':8})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repartition = df_mitbih[187].astype(int).value_counts()\nprint(repartition)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\ncircle=plt.Circle( (0,0), 0.8, color='white')\nplt.pie(repartition, labels=['n','q','v','s','f'], colors=['red','green','blue','skyblue','orange'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(circle)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data repartition is very imbalanced : 83% of the data are normal beats."},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv1D, MaxPool1D, Activation, Add, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (187, 1)\n\ndef make_model(final_layer_size=5):\n    I = Input(input_shape)\n    C = Conv1D(filters=32, kernel_size=5)(I)\n\n    C11 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(C)\n    C12 = Conv1D(filters=32, kernel_size=5, padding='same')(C11)\n    A11 = Add()([C, C12])\n    R11 = Activation(activation='relu')(A11)\n    M11 = MaxPool1D(pool_size=5, strides=2)(R11)\n\n    C21 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M11)\n    C22 = Conv1D(filters=32, kernel_size=5, padding='same')(C21)\n    A21 = Add()([M11, C22])\n    R21 = Activation(activation='relu')(A21)\n    M21 = MaxPool1D(pool_size=5, strides=2)(R21)\n\n    C31 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M21)\n    C32 = Conv1D(filters=32, kernel_size=5, padding='same')(C31)\n    A31 = Add()([M21, C32])\n    R31 = Activation(activation='relu')(A31)\n    M31 = MaxPool1D(pool_size=5, strides=2)(R31)\n\n    C41 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M31)\n    C42 = Conv1D(filters=32, kernel_size=5, padding='same')(C41)\n    A41 = Add()([M31, C42])\n    R41 = Activation(activation='relu')(A41)\n    M41 = MaxPool1D(pool_size=5, strides=2)(R41)\n\n    C51 = Conv1D(filters=32, kernel_size=5, activation='relu', padding='same')(M41)\n    C52 = Conv1D(filters=32, kernel_size=5, padding='same')(C51)\n    A51 = Add()([M41, C52])\n    R51 = Activation(activation='relu')(A51)\n    M51 = MaxPool1D(pool_size=5, strides=2)(R51)\n\n    F1 = Flatten()(M51)\n    D1 = Dense(32)(F1)\n    R1 = Activation(activation='relu')(D1)\n    D2 = Dense(32)(R1)\n    D3 = Dense(final_layer_size)(D2)\n\n    O = Activation(activation='softmax')(D3)\n\n    return Model(inputs=I, outputs=O)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"For training the networks, we used Adam optimization method with the learning rate, beta-1, and beta-2 of 0.001, 0.9, and 0.999, respectively. Learning rate is decayed exponentially with the decay factor of 0.75 every 10000 iterations.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=10000, decay_rate=0.75)\nadam = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, amsgrad=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detecting MI with ptbdb dataset only"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = len(np.unique(y_ptbdb))\nmodel_ptbdb = make_model(n_classes)\nmodel_ptbdb.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ptbdb, X_test_ptbdb, y_train_ptbdb, y_test_ptbdb = train_test_split(X_ptbdb, y_ptbdb, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ptbdb.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_ptbdb.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                          y_train_ptbdb, \n                          validation_split=0.15,\n                          epochs=30,\n                          batch_size=256,\n                          verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning(history):\n    plt.subplot(211)\n    plt.plot(history.history['accuracy'])\n    plt.legend([\"accuracy\"])\n    plt.subplot(212)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'], label = \"val_loss\")\n    plt.legend([\"loss\", \"val_loss\"])\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The loss converges at around 15 epochs."},{"metadata":{},"cell_type":"markdown","source":"Let's try on the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique, counts = np.unique(y_test_ptbdb, return_counts=True)\nprint(f\"The testing set contains {counts[0]} normal recordings and {counts[1]} with myocardial infarction.\\nLet's compute the confusion matrix.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model_ptbdb.evaluate(np.expand_dims(X_test_ptbdb, axis=2), y_test_ptbdb, batch_size=128)\nprint(f\"The accuracy on the testing set is {np.round(results[1]*100,1)}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_ptbdb = model_ptbdb.predict(np.expand_dims(X_test_ptbdb, axis=2))\ny_pred_ptbdb_bool = np.argmax(y_pred_ptbdb, axis=1)\nprint(classification_report(y_test_ptbdb, y_pred_ptbdb_bool))\n\nconfusion_matrix = tf.math.confusion_matrix(y_test_ptbdb, y_pred_ptbdb_bool)\nprint(f\"Confusion matrix :\\n {confusion_matrix}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"{confusion_matrix[0][0]}/{counts[0]} MI were correctly classified\")\n\nprint(f\"{confusion_matrix[1][1]}/{counts[1]} normal beats were correctly classified\")\n\nprint(f\"{confusion_matrix[1][0]} beats were classified as MI\")\n\nprint(f\"{confusion_matrix[0][1]} MI were classified as normal\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results for detecting MI by training directly with the ptbdb dataset are excellent : precision, recall, f1 and accuracy are all at around 0,99 on the testing set."},{"metadata":{},"cell_type":"markdown","source":"## Detecting MI with the mitbih dataset and transfert learning"},{"metadata":{},"cell_type":"markdown","source":"The research article suggests a training on the mitbih dataset with 5 output classes to detect high level features.\n\nThen, this model is frozen and followed by two dense layers of size 32 trained on the ptbdb dataset. Let's try to implement this."},{"metadata":{},"cell_type":"markdown","source":"### Training on mitbih"},{"metadata":{},"cell_type":"markdown","source":"This dataset has 5 classes as outputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mitbih, X_test_mitbih, y_train_mitbih, y_test_mitbih = train_test_split(X_mitbih, y_mitbih, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes_mitbih = len(np.unique(y_mitbih))\nmodel_mitbih = make_model(n_classes_mitbih)\nmodel_mitbih.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_mitbih.fit(np.expand_dims(X_train_mitbih, axis=2), \n                           y_train_mitbih, \n                           validation_split=0.15,\n                           epochs=30,\n                           batch_size=256,\n                           verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model converges at around 7-10 epochs on the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model_mitbih.evaluate(np.expand_dims(X_test_mitbih, axis=2), y_test_mitbih, batch_size=128)\nprint(f\"The accuracy on the testing set is {np.round(results[1]*100,1)}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_mitbih.predict(np.expand_dims(X_test_mitbih, axis=2))\nconfusion_matrix = tf.math.confusion_matrix(y_test_mitbih, np.argmax(predictions[:], axis=1))\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_mitbih = model_mitbih.predict(np.expand_dims(X_test_mitbih, axis=2))\ny_pred_mitbih_bool = np.argmax(y_pred_mitbih, axis=1)\nprint(classification_report(y_test_mitbih, y_pred_mitbih_bool))\n\nconfusion_matrix = tf.math.confusion_matrix(y_test_mitbih, y_pred_mitbih_bool)\nprint(f\"Confusion matrix :\\n {confusion_matrix}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results for the classification with 5 classes are quite good although it struggles a bit for the classes 1 and 3 with a recall of 0.74-0.78 and an f1-score of 0.80-0.84."},{"metadata":{},"cell_type":"markdown","source":"### Transfer learning"},{"metadata":{},"cell_type":"markdown","source":"Now, let's add the two dense layers with transfer learning.\nThis implies a bottleneck since the last layers will follow these sizes :\n\n[...] --> 32 --> **5** --> 32 --> 32 --> 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"D1 = Dense(32)(model_mitbih.output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\n\nfor layer in model.layers[:-3]:\n    layer.trainable = False\n\nfor layer in model.layers[-3:]:\n    layer.trainable = True\n    \nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=5,\n                    batch_size=128,\n                    verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results seem to be drastically worse than without transfer learning. The validate accuracy is constant after one epoch only. I'm not sure how they reached 95,9% accuracy on the article. The model designed above seems correct, with only the last layers set as trainable, as we can see below."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Trainability of the layers \\n\")\nfor layer in model.layers:\n    config = layer.get_config()\n    print(f\"{config['name']} : {config.get('trainable')}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try again by removing the bottleneck."},{"metadata":{"trusted":true},"cell_type":"code","source":"D1 = Dense(32)(model_mitbih.layers[-3].output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers[:-3]:\n    layer.trainable = False\n\nfor layer in model.layers[-3:]:\n    layer.trainable = True\n    \nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=100,\n                    batch_size=128,\n                    verbose=0)\nplot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even without the bottleneck, the results are pretty bad. Let's try one last time without bottleneck and with the whole model trainable."},{"metadata":{"trusted":true},"cell_type":"code","source":"D1 = Dense(32)(model_mitbih.layers[-3].output)\nD2 = Dense(32)(D1)\nO = Dense(2, activation='softmax')(D2)\nmodel = Model(inputs=model_mitbih.input, outputs=O)\nmodel.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(np.expand_dims(X_train_ptbdb, axis=2), \n                    y_train_ptbdb, \n                    validation_split=0.15,\n                    epochs=100,\n                    batch_size=128,\n                    verbose=0)\nplot_learning(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is converging at 76% accuracy, this is way below the 99% reached without transfer learning. I wonder how they managed to reach 95.6% accuracy with transfer learning in the article, and even though they did it would still be below the score reached without TL."}],"metadata":{"kernelspec":{"display_name":"Python 3.6.3 64-bit ('base': conda)","language":"python","name":"python36364bitbaseconda4f4bf55239c34c51aa8d6cc193367c64"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10-final"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}