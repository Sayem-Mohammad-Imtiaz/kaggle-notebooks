{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is result of my reading \n<b> <font color = 'Red'>Hands-on Machine Learning using Scikit-Learn, Tensorflow, Keras by Aurelien Geron</font></b>\n\nI utilized his way of approaching the problem and my way of exploring the options in the Scikit-Learn."},{"metadata":{},"cell_type":"markdown","source":"## 1. Objective\nPredict each districts median price of the house in California State. Target variable name : `median_house_value`"},{"metadata":{},"cell_type":"markdown","source":"**Performance Measure**\nRMSE : Root Mean Squared Error"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=  pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Null check</font>: Except Total Bedrooms column all the columns doesnt have null"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Housing Median Age\nfig = px.histogram(data, x=\"housing_median_age\", nbins = 30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>House Median Age:</font> Almost a good number of houses having the age of above 15 and below 40. Also a good number of houses are having the age of 50+"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Rooms\nfig = px.histogram(data, x=\"total_rooms\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'>Total Rooms:</font> Since this data is collected in 1990, the total number of rooms in the district is by average 1800 and max 5000 rooms. very few districts hase more than 10000 rooms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Bed Rooms\nfig = px.histogram(data, x=\"total_bedrooms\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Total Rooms: </font> Similar to Total number of Rooms, Total number of Bedrooms also less than 1500 for most of the districts. very few districts having more than 3000. It may be developed district."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Population\nfig = px.histogram(data, x=\"population\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Households\nfig = px.histogram(data, x=\"households\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Population and Households</font> Similar to Total Rooms and Total Bedrooms, Population and Households are having the similar range of values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# median_income\nfig = px.histogram(data, x=\"median_income\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Median Income:</font> Most district people are earning more then 2K and upt0 7K. very few districts has more than 10K income. But also based on the textbook, its capped with some range of values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# median_house_value\nfig = px.histogram(data, x=\"median_house_value\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Median House Value:</font> The Median house value is start from 50K to 300K. very few house values are in 500K. It may be near to bay."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ocean Proximity\ndf = data.groupby('ocean_proximity')['ocean_proximity'].count().reset_index(name = 'count')\nfig = px.bar(df, x='ocean_proximity', y='count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Ocean Proximity: </font> High number of houses are in 1 hour distance from Ocean, followed by Inland. these areas might be little cheaper. Near Bay and Ocean is good number of Houses."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data, x=\"median_house_value\", y=\"ocean_proximity\", color = 'housing_median_age')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data, x=\"median_house_value\", y=\"housing_median_age\", color = 'ocean_proximity')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='Blue'>Age vs House Value vs Ocean Proximity:</font> \n* Island is having very few houses which is also more than 50+ age and price is also little high.\n* Inland house values are very low and its spread in all the ages.\n* Near Ocean and 1H Ocean is having spread over all the range either Price or Age.\n* Near Bay is costly and its spread over all the age range."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data, x=\"longitude\", y=\"latitude\", color = 'ocean_proximity')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Ocean Proximity Map View:</font>\nYou can see the Ocean Proximity value in a clear way in the chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data, x=\"longitude\", y=\"latitude\", color = 'median_house_value')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>House Value:</font>\nThe price of the houses in the bay area is too high compare to all other places."},{"metadata":{},"cell_type":"markdown","source":"## Correlation charts"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = data.corr()\nfig = px.imshow(df_corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'Blue'>Features Correlation:</font>\nAlmost most of the features are having negative correlation with Median house value and few features are having positive correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nattr = ['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(data[attr], figsize = (15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous correlation heatmap and the above scatter chart clearly mentions there is a high correlation between median house value and median income in the district. which means who are all getting good income, they are buying a costly house or because of them the real estate values are going high. Since Real estate is one of the investment option :)"},{"metadata":{},"cell_type":"markdown","source":"**Data Set Split** \n\nHere we are spliting the dataset into two parts. 75% would be Train and 25% would be Test dataset. also to get the right combination of data in each features values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data,data['income_cat']):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]\nprint(strat_train_set.shape)\nprint(strat_test_set.shape)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"It plays a very important role in Machine Learning Model Building. Data is in raw format. even though we had completed the Analysis to understand the data and business we cant say this data is good to feed into a ML Model.\n\nFeature Engineering would convert the raw data into Machine consumable data such as proper matrix after Scaling different range of values into a fixed range of value, filling Null values, Encoding Text values into binary values."},{"metadata":{},"cell_type":"markdown","source":"### Creating New Features\n\n<b> How creation of new features would help model?</b>\n\nModel requires a lot of data or features to learn a pattern. \nIf current number of features is not enough, we can(if possible) create some more features from existing features. \nalso creating new feature would help to increase the accuracy of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new features from existing\ndata['rooms_per_household'] = data['total_rooms'] / data['households']\ndata['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\ndata['population_per_household'] = data['population'] / data['households']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr['median_house_value'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Bedrooms_per_room is having negative correlation\n* rooms_per_household is having positive correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.imshow(corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Data with Null values or different ranges or missed values would not acceptable by the model. we need to provide quality data to get a better model with high accuracy. There are multiple options to do the data cleaning. lets explore one by one."},{"metadata":{},"cell_type":"markdown","source":"Null Values:\nRemoving null values from the dataset causes data shortage. so better way is filling the values, if its numerical we can give a try with median or mean value, if its categorical mode value.\n\nInstead of traditional approach like fillna, we are going to try Sklearn's Imputer classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer  = SimpleImputer(strategy = 'median')\nhousing_num = data.drop(\"ocean_proximity\", axis = 1)\nmedian = imputer.fit(housing_num)\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Features**\n\nGenerally ML algorithms accepts the numerical values to train or predict. there are multiple ways to convert the categorical values into numerical values. pandas provides pandas.dummies() to convert the categorical values to numerical values by creating new columns and placing yes means 1 and no means 0, male means 1 and female means 0 and so more.\n\nSklearn provides an easy way to handle this. OnehotEncoder. once its fits with data it remembers the respected values. so it would help to fill the data which is used to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nhousing_cat = data[['ocean_proximity']]\ncat_encoder = OneHotEncoder()\nhousing_onehot = cat_encoder.fit_transform(housing_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pipelines**\n\nA sequence of actions like imputation, encoding the features, normalizing the values would be arranged and kept it as pipeline before the data feed into the ML algorithms.\n\nGenerally this pipelines helps to achieve the data preprocessing activities in a proper flow and it can be useable any time."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = strat_train_set.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom Transformers to create some extra features.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the Pipeline, the abouve code, Imput the missing values especially in number of bedrooms, create 3 features(bedrooms_per_room, rooms_per_household, population_per_household) then normalized the values in different ranges."},{"metadata":{"trusted":true},"cell_type":"code","source":"# add the categorical columns with transformer\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training"},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validate Model Performance**\n\nTo know the performance of the trained model, how good its generalize the new dataset or unknown dataset, there are two methods Root Mean Squred Error and Mean Absolute Error.\n\nIn this problem we are using Root Mean Squared Error.\n\nTo learn about RMSE: https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(lin_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor().fit(housing_prepared, housing_labels)\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(tree_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation\n**Scikit learn's K-Fold Cross validation**\n\nEvery time this K-fold cross validation would pick a split of sample test data for example 10%, the remaining 90% would be Train data. Data present in the Train data would not be there in Test data. K- number of times it should be folded. 10 means, 10 times with different trained and tested."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\ntree_rmse_scores = np.sqrt(-scores)\nprint(\"Scores : \",str(tree_rmse_scores))\nprint(\"Mean : \",str(tree_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(tree_rmse_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\nprint(\"Scores : \",str(lin_rmse_scores))\nprint(\"Mean : \",str(lin_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(lin_rmse_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared the Linear Regression and Decision Tree Models. Linear Regression is performing better as Decision Tree overfits the training data. So will explore Ensemble model- Random Forest.\n\n## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor()\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest model's result is acceptable"},{"metadata":{},"cell_type":"markdown","source":"## GridSearchCV\n\nThis is one of the option to get the optimal values for Hyperparameters to train the RandomForest Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators':[10,30, 50, 70, 100],'max_features':[6,8,10,15]},\n    {'bootstrap':[False], 'n_estimators':[3,10],'max_features':[2, 3, 4]}]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, \n                           param_grid, \n                           cv = 5, \n                           scoring = 'neg_mean_squared_error', \n                           return_train_score = True)\ngrid_search.fit(housing_prepared, housing_labels)\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply the GridSearch Result best parameters\n\nJust I have used the exact result of grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"st_time = time.time()\nforest = RandomForestRegressor(max_features = 8)\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))\nend_time = time.time()\nprint(\"Total Time : \",str(round(end_time - st_time,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let me change the max features to 10 and see the result"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nst_time = time.time()\nforest = RandomForestRegressor(max_features = 15)\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))\nend_time = time.time()\nprint(\"Total Time : \",str(round(end_time - st_time,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see the difference of timing with 8 and 15 features. so we are using the gridsearch option to get the optimal hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"If you <font color = 'orange'>like</font> this kernel and want to <font color = 'orange'>fork</font> plz <font color = 'red'>UPVOTE.</font>\n\nIf you have suggestions to improve this kernel plz <font color = 'red'>COMMENT.</font>\n\n<font color = 'orange'>************************ Notebook is under construction ************************</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}