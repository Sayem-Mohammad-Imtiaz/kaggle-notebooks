{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let me clone my custom ml-framework"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Import Sklearn libraries\nfrom sklearn import preprocessing, model_selection, feature_selection\n\n# to make this notebook's output stable across runs\nnp.random.seed(2210)\n\n# Set some options\npd.set_option(\"display.max_colwidth\", 100)\nsns.set_theme(style=\"whitegrid\", palette=\"Set3\")\n\n# Set Matplotlib defaults\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and explore the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_file = \"/kaggle/input/breast-cancer-wisconsin-data/data.csv\"\ndf = pd.read_csv(input_file)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_min_max = pd.concat(\n    [df.describe().loc[\"min\"], df.describe().loc[\"mean\"], df.describe().loc[\"max\"]],\n    axis=1,\n)\n\ndf_min_max.drop(df.columns[-1], inplace=True)\ncolor = \"#b753e6\"\nsns.reset_defaults()\nsns.reset_orig()\nsns.set_context(\n    rc={\n        \"lines.linewidth\": 5,\n        \"axes.labelsize\": 15,\n    }\n)\n\nfig, axs = plt.subplots(ncols=3, figsize=(15, 5))\nfig.suptitle(\"Min-Max distribution\")\nfor i, col in enumerate(df_min_max.columns.to_list()):\n    sns.histplot(df_min_max[col], bins=5, ax=axs[i], color=\"#b753e6\", alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = df.hist(figsize=(20, 15), color=\"#b753e6\", alpha=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n1. All columns have value preset and of type Float except Unnamed:32\n2. diaognisis class is the one we have to predict\n3. All numerical values are positive, we can apply Box-Cox transformer to unskew the data\n4. Unnamed:32 column is not required and can be dropped\n5. Id column can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -rf ml_framework\n! git clone https://github.com/maindolaamit/ml_framework.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nlib_dir = os.path.join('/kaggle/working', 'ml_framework')\n\nsys.path.append(lib_dir)\n\nfrom mllib import helper, charts, ml, metrics, features\n# from ml_framework import mllib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data():\n    df = pd.read_csv(input_file)\n    df.drop([\"id\", \"Unnamed: 32\"], axis=1, inplace=True)\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n    # Return the dataframe\n    le.fit(df[\"diagnosis\"])\n    df[\"diagnosis\"] = le.transform(df[\"diagnosis\"])\n\n    # return dataframe\n    return df, le\n\ndf, le = get_data()\nX, y = df.drop(\"diagnosis\", axis=1), df[\"diagnosis\"].values\ncolumns = X.columns.to_list()\n\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try various transformations and view the performance of classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_list = features.NumericalFeatures.get_encoders_list()\ncv = model_selection.StratifiedKFold(n_splits=5, shuffle=False)\n# Split the data based on Stratified\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, stratify=y)\n\ndef eval_clf_with_transformations(X, y, columns, encoding_list):\n    clf_val_df = pd.DataFrame()\n    clf_test_df = pd.DataFrame()\n\n    # Loop for each encoding and save the results\n    for encoding in encoding_list:\n        encodings_features = {encoding: (columns, \"mode\")}\n        num = features.NumericalFeatures(X, encodings_features)\n        try:\n            df_encoded = num.fit_transform()\n            df_val_score, df_test_score, search_grids = ml.evaluate_classifiers(\n                X_train, y_train, X_test, y_test, is_binary=True, cv=cv, sort_by=\"f1-score\"\n            )\n            if clf_val_df is None:\n                df_val_score[\"encoder\"] = encoding\n                df_test_score[\"encoder\"] = encoding\n                clf_val_df = df_val_score\n                clf_test_df = df_test_score\n            else:\n                df_val_score[\"encoder\"] = encoding\n                df_test_score[\"encoder\"] = encoding\n                clf_val_df = pd.concat([clf_val_df, df_val_score])\n                clf_test_df = pd.concat([clf_test_df, df_test_score])\n        except ValueError as ve:\n            # Skip box-cox as data may not be positive in some cases\n            if encoding == 'box-cox':\n                continue\n    return clf_val_df, clf_test_df\n\n\nclf_val_df, clf_test_df = eval_clf_with_transformations(X, y, X.columns.to_list(), encoding_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets view the Validation and test performance with each transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_val_df.sort_values(by='mean_val_score', ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_test_df.sort_values(by='f1-score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost, LightGBM gave the best performances on the Test results\n<li> We will use mix-max scalar as transformation\n<li> Let's select only the limited features from the data and check on them"},{"metadata":{},"cell_type":"markdown","source":"#### View the information gain of each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\ninfo_gain = pd.Series(mutual_info_classif(X, y), index=X.columns)\ninfo_gain.sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Select best 20 columns with Min-Max Scalar"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, mutual_info_classif\n\nselect_best = SelectKBest(mutual_info_classif, 20)\nselect_best.fit(X, y)\nbest_columns = X.columns[select_best.get_support()].to_list()\nX_best = X[best_columns]\n\n# Split the data based on Stratified\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X_best, y, test_size=0.2, stratify=y)\nclf_val_df, clf_test_df = eval_clf_with_transformations(X, y, best_columns, ['min-max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score_df = pd.Series(select_best.scores_, index=X.columns.to_list())\nbest_score_df.sort_values(ascending=False, inplace=True)\n# Plot for feature importance\nplt.figure(figsize=(20, 8))\nplt.style.use('fivethirtyeight')\nsns.set_style(\"white\")\nsns.barplot(x=best_score_df.index[:10], y=best_score_df[0:10], palette='muted')\nplt.title(f'Importance for the Top 10 Features (Gini criterion) ',\n          fontweight='bold')\nplt.grid(True, alpha=0.1, color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Use PCA"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=15)\n\nX_pca = pca.fit_transform(X_best)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X_pca, y, test_size=0.2, stratify=y)\ndf_val_score, df_test_score, search_grids = ml.evaluate_classifiers(\n    X_train, y_train, X_test, y_test, is_binary=True, cv=cv, sort_by=\"f1-score\"\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine Tuning \nLet's use best K-best columns RandomForest, Adaboost and XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X_best, y, test_size=0.2, stratify=y)\nxgb_clf = ml.fine_tune_classifier('xgb', X_train, y_train, cv=cv, randomized=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_clf = ml.fine_tune_classifier('ada', X_train, y_train, cv=cv, randomized=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = ml.fine_tune_classifier('rf', X_train, y_train, cv=cv, randomized=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Prediction\nLets view the confusion matrix and classification score with models trained"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nfor model in [rf_clf, ada_clf, xgb_clf]:\n    y_pred = model.predict(X_test)\n    cnf = confusion_matrix(y_test, y_pred)\n    report = classification_report(y_test, y_pred, target_names=le.classes_)\n    \n    charts.plot_confusion_matrix(cnf, le.classes_, f'Confusion Matrix - {type(model).__name__}')\n    print(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost and RandomForest gives 98% accuracy\nLet's take AdaBoost as final classifier for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = ada_clf\ny_pred = final_model.predict(X_test)\ncnf = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred, target_names=le.classes_)\n    \ncharts.plot_confusion_matrix(cnf, le.classes_, f'Confusion Matrix - {type(final_model).__name__}')\nprint(report)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}