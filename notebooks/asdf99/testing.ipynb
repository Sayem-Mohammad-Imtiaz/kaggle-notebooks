{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"outputs":[],"metadata":{"scrolled":true,"_kg_hide-input":false,"_uuid":"6b715f9db34a9c54099779bda34e435520dc3c42","_cell_guid":"ecdc660b-50b5-4a75-953b-b98eeeab4ceb"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib\nimport matplotlib.pyplot as plt\ntest = pd.read_csv(\"../input/test.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")\n\nclass data:\n    '''A class to handle input data.\n    \n    Syntax:\n        Returns tuple (X, y, subject).\n        X, y, subject = dt1.s // Training set, small non-changing (no shuffle)\n        X, y, subject = dt1.f // Full training set \n        X, y, subject = dt2.s // Test set, small non-changing (no shuffle)\n        X, y, subject = dt2.f // Full test set\n        \n    '''\n    s: (None, None, None) # Small data set\n    f: (None, None, None) # Full data set\n    fi_test: (None, None, None) # Trimmmed data set with 30 most important features\n    fi_train: (None, None, None) # Trimmmed data set with 30 most important features\n    #all_but_one_subject: [] # Array of data frames.\n    def __init__(self, test_or_train=\"Train\", subject=False):\n        '''Initialize data class.\n        \n        Initializes test or train and whether to include subject column.\n        '''\n        global test, train\n        if test_or_train == \"Train\":\n            ds = train\n        else:\n            ds = test\n        if subject:\n            num_cols = 562\n        else:\n            num_cols = 561\n        # Small non-changing data set (no shuffle)\n        # 20 rows.\n        self.s = (ds.iloc[0:100,0:num_cols],\n                  ds['Activity'].iloc[0:100],\n                  ds['subject'].iloc[0:100])\n        # Full data\n        self.f = (ds.iloc[:,0:num_cols], ds['Activity'], ds['subject'])\n        # If train, then populate feat_importance\n        if test_or_train == 'Train':\n            self.set_fi()\n        \n    def set_fi(self):\n        '''Creates full training set with only the most important features included.\n        \n        Todo: Graph feature importance to see how each feature ranks high to low.\n        '''\n        from sklearn.ensemble import ExtraTreesClassifier\n        from sklearn.feature_selection import SelectFromModel\n        from sklearn.utils import shuffle\n\n        X, y, subject = self.f\n        print('Initial data:', X.shape)\n        # Outputs: (7352, 561)\n        \n        # Perform feature trim actions on test set.\n        # Does not include subject column.\n        X_test, y_test = (test.iloc[:,0:561], test['Activity'])\n        \n        clf = ExtraTreesClassifier()\n        clf = clf.fit(X, y)\n        # Outputs: array([ 0.04...,  0.05...,  0.4...,  0.4...])\n        model = SelectFromModel(clf, prefit=True)\n        X_new = model.transform(X)\n        X_test = model.transform(X_test)\n        print('New data:', X_new.shape)\n        # Outputs: (7352, 101)\n        s1 = model.get_support() # shows kept and removed indexes.\n        \n        clf = ExtraTreesClassifier()\n        clf = clf.fit(X_new, y)\n        model = SelectFromModel(clf, prefit=True)\n        X_new = model.transform(X_new)\n        X_test = model.transform(X_test)\n        print('New data x new:', X_new.shape)\n        print('New data x test:', X_test.shape)\n        # Outputs: (7352, 35)\n        s2 = model.get_support()\n        \n        self.fi_test = (X_test, y, subject)\n        self.fi_train = (X_new, y, subject)\n        \n        # Match up the support columns\n        # (Not using.)\n        #counts1 = 0\n        #counts2 = 0\n        #for i in s1:\n        #    if i == True: # All trues are in s2.\n        #        if s2[counts2] == False:\n        #            s1[counts1] = False\n        #        counts2+=1\n        #    counts1+=1\n        \n\n\ndt1 = data(\"Train\", False)\ndt2 = data(\"Test\", False)\n","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"scrolled":false,"_uuid":"43f69d98bd87cfab077bea0ecd72e53645aa030c","_cell_guid":"f820e7e8-dd4a-4ced-9a7f-b7025be8b374"},"source":"# Loop through and get 29/30 subjects, removing one per loop.\n# df[df['A'].isin([3, 6])]\n# df1.loc[:, df1.loc['a'] > 0]\n#x = train.loc[:, train['subject'] != 1]\nx = train.iloc[[1, 3, 5], :]\nprint(x.shape)\nx = train[train['subject'].isin([1])]\nprint(x.shape)\nx = train[~train['subject'].isin([1])]\nprint(x.shape)\nprint(train.shape)\n#n = pd.DataFrame(train.values - x.values, columns=train.columns)\n#print(n.shape)\n#x = train.iloc[train.Subject.isin([30, 29]), 0]\n#print(x.shape)\n#x = train.iloc[train.subject.isin([30]), :]\n#print(x.shape)\n#print('--')\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\n#X, y, _ = dt1.f\ntarget_names = ['STANDING', 'SITTING', 'LAYING', 'WALKING_UPSTAIRS', 'WALKING', 'WALKING_DOWNSTAIRS']\nfor i in range(1,30):\n    print('Removing subject:',i)\n    x = train[~train['subject'].isin([i])]\n    y = x['Activity']\n    x = x.iloc[:,0:561] # Remove subject column (col 562).\n    # Default layers: 5, 2\n    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                         hidden_layer_sizes=(50, 20), random_state=1)\n\n    clf.fit(x, y)\n    \n    X2, y2, _ = dt2.f\n    z = clf.predict(X2)\n    print(classification_report(y, z, target_names=target_names))\n    \n    #x = train.subject.isin([i])\n    #print('Rows for subject ',i,'=',x.shape)\n    #data = train[~train.subject.isin([i])]\n    #print(data.shape)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"scrolled":false,"_uuid":"f9854d99c331869491107e827294b0a8e81ec1a2","_cell_guid":"b78cd508-dace-4171-a755-a17960a936c8"},"source":"dt1.f[0].head(3)\n# Note included 'subject' column.","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"aef41cd416b933a02e23da91c8f7d75361e391e2","_cell_guid":"afaecf1d-abbc-40b8-8a69-b2e92e424648"},"source":"dt1.f[1].head(3)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"a758125ddbd444899d9dce97a743da92b048d00c","_cell_guid":"f3dc2400-313a-44cd-b514-21d1b2981ebd"},"source":"np.unique(dt1.f[1])","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"cd842af3911ebc7e5387103621d944d263068a30","_cell_guid":"05a691d3-d3b8-4097-a3a3-a56819b81607"},"source":"dt1.s[0].head(3)","execution_count":null,"cell_type":"code"},{"source":"# From http://scikit-learn.org/stable/modules/feature_selection.html","metadata":{"_uuid":"f5667ccc155a454e2d0450f276ca491d76855e55","_cell_guid":"b0dbb1fd-6af0-47af-9a78-dd6c64aa0bce"},"cell_type":"markdown"},{"source":"# 1.13.1\nVarianceThreshold: Removes all features whose variance doesnâ€™t meet some threshold.\n\nThere are 562 features in total.\n\nWith 0.8 variance, VarianceThreshold removes the first array items in this example:\n\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n\nAs the variance is five 0's and one 1, or 5/6, or 80%.\n\nNote: Variance[X]=p(1-p).\n\nAccording to this formula, variance tops out at 0.25 (when p=0.5).\n\nNote: With V[X]= ... all but one feature is removed.","metadata":{"_uuid":"e4beae3c92309bb1c51172c6687774ab45875518","_cell_guid":"1cb8effc-4881-4606-bd08-df5a79c3945e"},"cell_type":"markdown"},{"outputs":[],"metadata":{"scrolled":true,"_kg_hide-input":false,"_uuid":"fdf6fc428e1726c54498d5d235d38f8132ad31c4","_kg_hide-output":false,"_cell_guid":"484f56ad-0058-4dca-9d37-2de8f833c454"},"source":"# Training\nfrom sklearn.feature_selection import VarianceThreshold\nX, y, _ = dt1.f\np = 0\noutput = [[],[]]\nwhile p <= 0.52:\n    sel = VarianceThreshold(threshold=(p*(1-p)))\n    v = sel.fit_transform(X)\n    output[0].append('{0:.2f}'.format(p*(1-p)))\n    output[1].append(len(v[0]))\n    p = p+0.01\nx_points0 = output[0]\ny_points0 = output[1]\n\n# Testing\nX, y, _ = dt2.f\np = 0\noutput = [[],[]]\nwhile p <= 0.52:\n    sel = VarianceThreshold(threshold=(p*(1-p)))\n    v = sel.fit_transform(X)\n    output[0].append('{0:.2f}'.format(p*(1-p)))\n    output[1].append(len(v[0]))\n    p = p+0.01\nfig = plt.figure()\nax = fig.add_subplot(111)\nx_points1 = output[0]\ny_points1 = output[1]\np = ax.plot(x_points0, y_points0, 'b')\np = ax.plot(x_points1, y_points1, 'r')\nax.set_xlabel('Variance')\nax.set_ylabel('Number of features')\nax.set_title('Variance Testing Set')\nfig.show()","execution_count":null,"cell_type":"code"},{"source":"The number of features ranges from 35 (with variance=0.25) to 562 (with variance=0).","metadata":{"_uuid":"5f9599894b0afb98f7b024512ab012676c195f79","_cell_guid":"1c2205c3-2b88-490d-bbcb-ec2ddd31b659"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"8de3e9da7090242224d34d9455c7f83e2ba8138f","_cell_guid":"59fc3718-82ac-4548-b9b8-ab0ba697bd5e"},"source":"# Training\nfig = plt.figure()\nax = fig.add_subplot(111)\n#print (dt1.s[0].items())\n\nax.plot(dt1.s[0], 'b')\n\n#for row in dt1.s[0].iterrows(): #s[0]=features,s[1]=labels\n        #ax.plot(c, row, 'b')\n        #ax.plot(x_points1, y_points1, 'r')\n        #c+=1\n#ax.set_xlabel('features')\n#ax.set_ylabel('Number of features')\n#ax.set_title('Variance Testing Set')\nfig.show()","execution_count":null,"cell_type":"code"},{"source":"# Plotting Learning Curves\nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html","metadata":{"_uuid":"827aae5e161d3fe42e8daafef806123b1dd7bdaf","_cell_guid":"8866a295-20d1-4de5-88e7-bf95517c23e0"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"1dae7d532dd68d6faa9b3cbac979c4f488fc8b96","collapsed":true,"_cell_guid":"77bee90d-bf3f-402d-ad5a-d75ffd81a7f8"},"source":"# Define: plot_learning_curve()\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"scrolled":true,"_kg_hide-input":true,"_uuid":"8c49675454c92cc0390b054405c872eecbe35d85","_cell_guid":"72c670cc-eee2-4fe7-9a2d-987ae2fa3de9"},"source":"# Small data set\n# Variance threshold (p=0.2).\nX, y, subject = dt1.s\n#y = subject\n#X = [subject,subject]\n#print(X)\n#print(y)\n# Reshape your data either using array.reshape(-1, 1)\n# if your data has a single feature or array.reshape(1, -1)\n# if it contains a single sample.\n#X.reshape(-1, 1)\n#p = 0.2\n#sel = VarianceThreshold(threshold=(p*(1-p)))\n#X = sel.fit_transform(subject)\n#print('V[X] =', '{0:.2f}'.format(p*(1-p)),\n    #': num features =',len(X[0]))\n\ntitle = \"Training: Learning Curves (Naive Bayes) + VarianceThreshold\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.2, 1.01), cv=cv, n_jobs=40)\n\ntitle = \"Training: Learning Curves (SVM, RBF kernel, $\\gamma=0.001$) + VarianceThreshold\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=1, test_size=0.02, random_state=0)\nprint('cv',cv)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.2, 1.01), cv=cv, n_jobs=40)\n\nplt.show()","execution_count":null,"cell_type":"code"},{"source":"# How well does the subject (1-30) predict the activity?","metadata":{"_uuid":"6616ae54eda7b0b26b4c44dabe14cc188ba7bf06","_cell_guid":"7f3747e7-0432-44aa-b116-e855dff05dfe"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"a20a5f38388e226932286e9c445d312cdb63cfda","collapsed":true,"_cell_guid":"4ea034e1-f669-480b-84da-75ed89d11d66"},"source":"\n#..","execution_count":null,"cell_type":"code"},{"source":"# Neural network\n# http://scikit-learn.org/stable/modules/neural_networks_supervised.html","metadata":{"_uuid":"f7a6bf37af1f42e8210899d5e2bebe61e75e1cae","_cell_guid":"2bb388a3-fa0c-4962-bcb5-d1c5ba4591a3"},"cell_type":"markdown"},{"outputs":[],"metadata":{"scrolled":false,"_kg_hide-input":false,"_uuid":"3d6ff2c7dca91ae14f5a39e760b0ed4b0db0b0d2","_cell_guid":"c6ba4da7-51bc-4707-8383-7250954f0392"},"source":"from sklearn.neural_network import MLPClassifier\nX, y, _ = dt1.f\n#X = [[0., 0.], [1., 1.]]\n#y = [0, 1]\n# Default layers: 5, 2\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(5, 2), random_state=1)\n\nclf.fit(X, y)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"75975e38c7adf25f3c377b49aad83ecfbbffe336","_cell_guid":"a5b26dab-30fa-4836-87c5-a6aa69061148"},"source":"X, y, _ = dt2.f\nz = clf.predict(X)\nfrom sklearn.metrics import classification_report\ntarget_names = ['STANDING', 'SITTING', 'LAYING', 'WALKING_UPSTAIRS', 'WALKING', 'WALKING_DOWNSTAIRS']\nprint(classification_report(y, z, target_names=target_names))","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"6435440cc450b34773fc9680a8d8fb003e47f754","_cell_guid":"763fe4b5-f42e-49d4-a9a9-000ca65e755b"},"source":"from sklearn.neural_network import MLPClassifier\nimport time\nX, y, _ = dt1.f\n#X = [[0., 0.], [1., 1.]]\n#y = [0, 1]\n# Default hidden layers: 5, 2\n# 500, 200 takes longer but performs slightly worse than 50, 20.\nt = time.time()\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(50, 20), random_state=1)\n\nclf.fit(X, y)\nprint('Elapsed time: {}'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-t))))","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"scrolled":false,"_uuid":"447190c0dfca8ecea300a9c8db06b97849013676","_cell_guid":"7745676e-9048-433a-bba4-57105ba61118"},"source":"X, y, _ = dt2.f\nz = clf.predict(X)\nfrom sklearn.metrics import classification_report\ntarget_names = ['STANDING', 'SITTING', 'LAYING', 'WALKING_UPSTAIRS', 'WALKING', 'WALKING_DOWNSTAIRS']\nprint(classification_report(y, z, target_names=target_names))","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"ef819412b11a8126b50aa43547c4266cd32e798d","_cell_guid":"11ea5419-9265-49e9-a044-66c48b7590cb"},"source":"# Loop through most important features and test removing one.\nX, y, _ = dt1.fi_train\nprint(X.shape)\nprint(y.shape)\n\nfrom sklearn.neural_network import MLPClassifier\nimport time\n# Default hidden layers: 5, 2\n# 500, 200 takes longer but performs slightly worse than 50, 20.\nt = time.time()\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(50, 20), random_state=1)\n\nclf.fit(X, y)\nprint('Elapsed time: {}'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-t))))\n\nX, y, _ = dt1.fi_test\nprint(X.shape)\nprint(y.shape)\n#X, y, _ = dt2.s\nz = clf.predict(X)\nfrom sklearn.metrics import classification_report\ntarget_names = ['STANDING', 'SITTING', 'LAYING', 'WALKING_UPSTAIRS', 'WALKING', 'WALKING_DOWNSTAIRS']\nprint(classification_report(y, z, target_names=target_names))","execution_count":null,"cell_type":"code"},{"source":"# K Nearest Neighbor","metadata":{"_uuid":"b0d67a6a45a57ea20a8b7deed5b31f4e4a60ae7d","_cell_guid":"5e1b94cf-ba3a-4ee0-891b-d1eabadcaf63"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"55fe9334e02f36cededb6c978db611eb6900e166","collapsed":true,"_cell_guid":"d5122f2b-eec1-4587-a6b2-ab883abddd74"},"source":"# knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n#np.random.seed(0)\nX, y, _ = dt1.f\n#indices = np.random.permutation(len(X))\n#iris_X_train = iris_X[indices[:-10]]\n#iris_y_train = iris_y[indices[:-10]]\n#iris_X_test  = iris_X[indices[-10:]]\n#iris_y_test  = iris_y[indices[-10:]]\n\n# Create and fit a nearest-neighbor classifier\nknn = KNeighborsClassifier()\nknn.fit(X, y) \nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\nz = knn.predict(dt2.f[0])\ntarget_names = ['STANDING', 'SITTING', 'LAYING', 'WALKING_UPSTAIRS', 'WALKING', 'WALKING_DOWNSTAIRS']\nprint(classification_report(dt2.f[1], z, target_names=target_names))","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"d88fbb50f98b86337ec84755f19139fe4bd5a963","collapsed":true,"_cell_guid":"820b3c80-20e5-4742-ae78-c4b761b84396"},"source":"from sklearn import datasets\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\n# Full\nlr = linear_model.LinearRegression()\nX,y,_ = dt1.f\n# get_dummies\n# one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\ny = pd.get_dummies(y)\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validation:\npredicted = cross_val_predict(lr, X, y, cv=10)\n# Plot\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n\n# Small\nlr = linear_model.LinearRegression()\nX,y,_ = dt1.s\n# get_dummies\n# one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\ny = pd.get_dummies(y)\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validation:\npredicted = cross_val_predict(lr, X, y, cv=10)\n# Plot\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n\n# Full, Subject only\nlr = linear_model.LinearRegression()\nX,y,subject = dt1.f\nX = subject.reshape(-1, 1)\n# get_dummies\n# one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\ny = pd.get_dummies(y)\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validation:\npredicted = cross_val_predict(lr, X, y, cv=10)\n# Plot\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"ecc9533aeda7933630c21e275a8ebca1a5c46422","collapsed":true,"_cell_guid":"cb6d94e3-4144-4fb4-8903-447098d8bdf2"},"source":"# Concatenating multiple feature extraction methods\n# http://scikit-learn.org/stable/auto_examples/plot_feature_stacker.html#sphx-glr-auto-examples-plot-feature-stacker-py\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\nX, y, _ = dt1.s\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\n\nsvm = SVC(kernel=\"linear\")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"8816c1bf263c6098991c8f870de235eb0beb3c12","collapsed":true,"_cell_guid":"33f77649-f8db-41a4-bf2b-744fde0898df"},"source":"# Pipelining: chaining a PCA and a logistic regression\n# http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py\n# Code source: GaÃ«l Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nlogistic = linear_model.LogisticRegression()\n\npca = decomposition.PCA()\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\ndigits = datasets.load_digits()\nX,y,_ = dt1.f\nX_digits = X\ny_digits = y\n\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nplt.figure(1, figsize=(12, 9))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_')\n\n# Prediction\nn_components = [20, 40, 64]\nCs = np.logspace(-4, 4, 3)\n\n# Parameters of pipelines can be set using â€˜__â€™ separated parameter names:\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_components,\n                              logistic__C=Cs))\nestimator.fit(X_digits, y_digits)\n\nplt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n            linestyle=':', label='n_components chosen')\nplt.legend(prop=dict(size=12))\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":true,"_uuid":"1e4cc4ad9a492397ef65235e036db2d9b7e030b2","collapsed":true,"_cell_guid":"4a060b9f-3630-40df-bc25-6a163cb01ba6"},"source":"# The Johnson-Lindenstrauss bound for embedding with random projections\n# http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py\nimport sys\nfrom time import time\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# Part 1: plot the theoretical dependency between n_components_min and\n# n_samples\n\n# range of admissible distortions\neps_range = np.linspace(0.1, 0.99, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(1, 9, 9)\n\nplt.figure()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n    plt.loglog(n_samples_range, min_n_components, color=color)\n\nplt.legend([\"eps = %0.1f\" % eps for eps in eps_range], loc=\"lower right\")\nplt.xlabel(\"Number of observations to eps-embed\")\nplt.ylabel(\"Minimum number of dimensions\")\nplt.title(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n\n# range of admissible distortions\neps_range = np.linspace(0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(2, 6, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\n\nplt.figure()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n    plt.semilogy(eps_range, min_n_components, color=color)\n\nplt.legend([\"n_samples = %d\" % n for n in n_samples_range], loc=\"upper right\")\nplt.xlabel(\"Distortion eps\")\nplt.ylabel(\"Minimum number of dimensions\")\nplt.title(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n\n# Part 2: perform sparse random projection of some digits images which are\n# quite low dimensional and dense or documents of the 20 newsgroups dataset\n# which is both high dimensional and sparse\n\n#if '--twenty-newsgroups' in sys.argv:\n#    # Need an internet connection hence not enabled by default\n#    data = fetch_20newsgroups_vectorized().data[:500]\n#else:\n#    data = load_digits().data[:500]\ndata,y,_ = dt1.f\n\nn_samples, n_features = data.shape\nprint(\"Embedding %d samples with dim %d using various random projections\"\n      % (n_samples, n_features))\n\nn_components_range = np.array([300, 1000, 10000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = time()\n    rp = SparseRandomProjection(n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print(\"Projected %d samples from %d to %d in %0.3fs\"\n          % (n_samples, n_features, n_components, time() - t0))\n    if hasattr(rp, 'components_'):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print(\"Random matrix with size: %0.3fMB\" % (n_bytes / 1e6))\n\n    projected_dists = euclidean_distances(\n        projected_data, squared=True).ravel()[nonzero]\n\n    plt.figure()\n    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu)\n    plt.xlabel(\"Pairwise squared distances in original space\")\n    plt.ylabel(\"Pairwise squared distances in projected space\")\n    plt.title(\"Pairwise distances distribution for n_components=%d\" %\n              n_components)\n    cb = plt.colorbar()\n    cb.set_label('Sample pairs counts')\n\n    rates = projected_dists / dists\n    print(\"Mean distances rate: %0.2f (%0.2f)\"\n          % (np.mean(rates), np.std(rates)))\n\n    plt.figure()\n    plt.hist(rates, bins=50, normed=True, range=(0., 2.), edgecolor='k')\n    plt.xlabel(\"Squared distances rate: projected / original\")\n    plt.ylabel(\"Distribution of samples pairs\")\n    plt.title(\"Histogram of pairwise distance rates for n_components=%d\" %\n              n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"scrolled":false,"_uuid":"7b2ff398ec7a15844c5cb7195c9729b60a5cdc82","collapsed":true,"_cell_guid":"67970aed-4c6d-4534-a37d-1f25e450d838"},"source":"# http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html#sphx-glr-auto-examples-plot-kernel-ridge-regression-py\n# ValueError: bad input shape (80, 6)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"c0233abc0283913c350fb6b729553958daa933d3","collapsed":true,"_cell_guid":"2842a006-ed35-4fd4-b338-69db08673bf1"},"source":"# http://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html#sphx-glr-auto-examples-applications-plot-model-complexity-influence-py\n# Author: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nimport time\nfrom mpl_toolkits.axes_grid1.parasite_axes import host_subplot\nfrom mpl_toolkits.axisartist.axislines import Axes\nfrom scipy.sparse.csr import csr_matrix\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm.classes import NuSVR\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.metrics import hamming_loss\n\n# #############################################################################\n# Routines\n\n\n# Initialize random generator\nnp.random.seed(0)\n\n\ndef generate_data(case, sparse=False):\n    \"\"\"Generate regression/classification data.\"\"\"\n    bunch = None\n    X, y, _ = dt1.f\n    #y = pd.get_dummies(y)\n    #if case == 'regression':\n    #    pass\n    #elif case == 'classification':\n    #    y = pd.get_dummies(y)\n    #    pass\n    \n    \n    #X, y = shuffle(bunch.data, bunch.target)\n    offset = int(X.shape[0] * 0.8)\n    X_train, y_train = X[:offset], y[:offset]\n    X_test, y_test = X[offset:], y[offset:]\n    if sparse:\n        X_train = csr_matrix(X_train)\n        X_test = csr_matrix(X_test)\n    else:\n        X_train = np.array(X_train)\n        X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    y_train = np.array(y_train)\n    data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,\n            'y_test': y_test}\n    return data\n\n\ndef benchmark_influence(conf):\n    \"\"\"\n    Benchmark influence of :changing_param: on both MSE and latency.\n    \"\"\"\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf['changing_param_values']:\n        conf['tuned_params'][conf['changing_param']] = param_value\n        estimator = conf['estimator'](**conf['tuned_params'])\n        print(\"Benchmarking %s\" % estimator)\n        estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n        conf['postfit_hook'](estimator)\n        complexity = conf['complexity_computer'](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf['n_samples']):\n            y_pred = estimator.predict(conf['data']['X_test'])\n        elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n        prediction_times.append(elapsed_time)\n        pred_score = conf['prediction_performance_computer'](\n            conf['data']['y_test'], y_pred)\n        prediction_powers.append(pred_score)\n        print(\"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\" % (\n            complexity, conf['prediction_performance_label'], pred_score,\n            elapsed_time))\n    return prediction_powers, prediction_times, complexities\n\n\ndef plot_influence(conf, mse_values, prediction_times, complexities):\n    \"\"\"\n    Plot influence of model complexity on both accuracy and latency.\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    host = host_subplot(111, axes_class=Axes)\n    plt.subplots_adjust(right=0.75)\n    par1 = host.twinx()\n    host.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])\n    y1_label = conf['prediction_performance_label']\n    y2_label = \"Time (s)\"\n    host.set_ylabel(y1_label)\n    par1.set_ylabel(y2_label)\n    p1, = host.plot(complexities, mse_values, 'b-', label=\"prediction error\")\n    p2, = par1.plot(complexities, prediction_times, 'r-',\n                    label=\"latency\")\n    host.legend(loc='upper right')\n    host.axis[\"left\"].label.set_color(p1.get_color())\n    par1.axis[\"right\"].label.set_color(p2.get_color())\n    plt.title('Influence of Model Complexity - %s' % conf['estimator'].__name__)\n    plt.show()\n\n\ndef _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return np.count_nonzero(a)\n\n# #############################################################################\n# Main code\n#regression_data = generate_data('regression')\nclassification_data = generate_data('classification', sparse=True)\nconfigurations = [\n    {'estimator': SGDClassifier,\n     'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':\n                      'modified_huber', 'fit_intercept': True},\n     'changing_param': 'l1_ratio',\n     'changing_param_values': [0.25, 0.5, 0.75, 0.9],\n     'complexity_label': 'non_zero coefficients',\n     'complexity_computer': _count_nonzero_coefficients,\n     'prediction_performance_computer': hamming_loss,\n     'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',\n     'postfit_hook': lambda x: x.sparsify(),\n     'data': classification_data,\n     'n_samples': 30},\n   \n]\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = \\\n        benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times,\n                   complexities)","execution_count":null,"cell_type":"code"},{"source":"# feature importance","metadata":{"_uuid":"4c147ac9f207339629760b465055232139f48055","_cell_guid":"9545a73d-f9b3-4ec3-8620-10a6c994cc2a"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"c3ed3cf1e572750acaf408d0c3dbb91f0aefc766","collapsed":true,"_cell_guid":"440739c4-72f4-4fc5-9266-88fa8025cbfb"},"source":"#\n\n","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":false,"_uuid":"aa4f2db89e8726885be3ceee191f45045c527bcd","collapsed":true,"_kg_hide-output":false,"_cell_guid":"ac3771ec-9a63-430a-a6a4-3381ff31fbf0"},"source":"# Plot training data.\n\n# Load data\nX, y = dt1.f\n\ntitle = \"Training: Learning Curves (Naive Bayes)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=40)\n\ntitle = \"Training: Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=2, test_size=0.02, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=40)\n\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_kg_hide-input":false,"_uuid":"12eb363db46ac96c448188b059f6cbb3ab3fdf02","collapsed":true,"_cell_guid":"e8218210-9016-4e86-bff8-1f84c3633aab"},"source":"# Small data set\n# Variance threshold (p=0.2).\nX, y, subject = dt1.s\np = 0.2\nsel = VarianceThreshold(threshold=(p*(1-p)))\nX = sel.fit_transform(X)\nprint('V[X] =', '{0:.2f}'.format(p*(1-p)),\n    ': num features =',len(X[0]))\n\ntitle = \"Training: Learning Curves (Naive Bayes) + VarianceThreshold\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.2, 1.01), cv=cv, n_jobs=40)\n\ntitle = \"Training: Learning Curves (SVM, RBF kernel, $\\gamma=0.001$) + VarianceThreshold\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=1, test_size=0.02, random_state=0)\nprint('cv',cv)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.2, 1.01), cv=cv, n_jobs=40)\n\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"d3af9afc09d1b4e8d532e5527c4bc1b556344ecd","collapsed":true,"_cell_guid":"076428f6-3983-4f66-8371-d2213d507e30"},"source":"# Small training set.\n# Variance threshold (p=0.5).\nX, y = dt1.s\np = 0.5\nsel = VarianceThreshold(threshold=(p*(1-p)))\nX = sel.fit_transform(X)\nprint('V[X] =', '{0:.2f}'.format(p*(1-p)),\n    ': num features =',len(X[0]))\n\ntitle = \"Training: Learning Curves (Naive Bayes) + VarianceThreshold\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=8)\n\ntitle = \"Training: Learning Curves (SVM, RBF kernel, $\\gamma=0.001$) + VarianceThreshold\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=8)\n\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"d192dc4a609c0325840b0cea45dfbefa04c61630","collapsed":true,"_cell_guid":"fe7fdf30-e078-430b-963b-06509d5e8d18"},"source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\n\nX, y = dt1.f\n\nprint('Initial data:',X.shape)\n#(150, 4)\nclf = ExtraTreesClassifier()\nclf = clf.fit(X, y)\nprint('clf:',clf)\n\nprint('Feature importances:',clf.feature_importances_)\n#array([ 0.04...,  0.05...,  0.4...,  0.4...])\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nprint('New data:',X_new.shape)\n#(150, 2)\n\nclf = ExtraTreesClassifier()\nclf = clf.fit(X_new, y)\nprint('New data feature importances:',clf.feature_importances_)\n\n\n","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"a88bb0cc83aab656034787c81185f88e518fce3b","collapsed":true,"_cell_guid":"8ad1d58d-0ba8-4e18-b98d-0ef0f4772c45"},"source":"\n# 1.13.5\nfrom sklearn.svm import LinearSVC\nimport sklearn.pipeline\nfrom sklearn.feature_selection import SelectFromModel\nclf = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n  ('classification', RandomForestClassifier())\n])\nclf.fit(X, y)\n","execution_count":null,"cell_type":"code"}]}