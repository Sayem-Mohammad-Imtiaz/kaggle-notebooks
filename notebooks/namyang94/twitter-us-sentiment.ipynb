{"cells":[{"metadata":{"_uuid":"6f25b3a14be0fd4df36a372903d6cad70607be76"},"cell_type":"markdown","source":"As I am new to NLP, this will be a simple classification using only the 'text' and 'airline_sentiment ' columns to get my feet wet =)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"tweets = pd.read_csv(\"../input/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a552a1d6f5abc646e3a8da0b188fe9ff8f1c24","scrolled":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4bfdf2e2b9c2b3134c5b258af5b01c970f324d0"},"cell_type":"code","source":"# extract only the text and airline_sentiment columns\ndf = tweets[['text','airline_sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8afc40f8d7bbf0a60615d9fcd7bf467da29448c"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5198e6874e162c87cf5433515bb2d6b18b8bd1f2"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"660520d7a135a84aff2dd1ba4266fede6d7764d4"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ca53a10a39a461275a74688a67ea219f33d694b"},"cell_type":"code","source":"X = df['text']\ny = df['airline_sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbd388acba3d8876bcb9628a514375e9ea80a511"},"cell_type":"code","source":"#encode sentiment categories\nle = LabelEncoder()\nle.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1242cadf3958392b3b226cf06c8bd0fdc7bb8899"},"cell_type":"code","source":"df['airline_sentiment'].head()\n# encoding: neutral = 1, positive = 2, negative = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9e839c75827d5e2000a13bc74d4b58b521cdaad"},"cell_type":"markdown","source":"## Naive attempt"},{"metadata":{"_uuid":"cee9b4e63ca20a39b01bd635c726e85b390d6ee1"},"cell_type":"markdown","source":"Attempt classification without cleaning the X data"},{"metadata":{"trusted":true,"_uuid":"c5572952fa600d8d7a877135a3209cc2d3ba5b6a"},"cell_type":"code","source":"naive_pipe = Pipeline([\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('lm', LogisticRegression())\n    \n])\n\nscores = cross_val_score(naive_pipe, X,y, cv = 5)\nprint('Mean score: ',scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc0af327abbd367b8b25a9e98cc0ad58efb5224b"},"cell_type":"markdown","source":"As we can see, the accuracy's pretty decent even without cleaning the X data."},{"metadata":{"trusted":true,"_uuid":"1b7f340b425d9224eab602b05306048cf688794b"},"cell_type":"markdown","source":"## 2nd attempt with data cleaning"},{"metadata":{"_uuid":"750ec036cda7bbe7696400cd74a9d785f5297bcc"},"cell_type":"markdown","source":"We're going to remove stopwords and any unnecessary punctuation. Hopefully this will produce better results!"},{"metadata":{"trusted":true,"_uuid":"41b7a84d10733dd41db9df8ae33d7dbe16c18cb0"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn.base import BaseEstimator,TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5e656ea62a122521e72be025266ab606aa586ba"},"cell_type":"code","source":"class PunctuationRemover(BaseEstimator,TransformerMixin):\n    def fit(self, column, y = None):\n        return self\n    \n    def removePunctuation(self,text,punctuation, y = None):\n        clean_words = []\n        \n        for element in word_tokenize(text):\n            if element not in punctuation:\n                clean_words.append(element)\n\n        clean_text = ' '.join(clean_words)\n        return clean_text\n    \n    def transform(self, column, y = None):\n        punctuation = set(string.punctuation)\n        return column.apply(lambda x: self.removePunctuation(x,punctuation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a8605ec09d63d04094e51573b0e4884f576d7f8"},"cell_type":"code","source":"class StopwordRemover(BaseEstimator,TransformerMixin):\n    def fit(self, column, y = None):\n        return self\n    \n    def removeStopwords(self,text,stop_words, y = None):\n        clean_words = []\n        \n        for element in text.lower().split():\n            if element not in stop_words:\n                clean_words.append(element)\n\n        clean_text = ' '.join(clean_words)\n        return clean_text\n    \n    def transform(self, column, y = None):\n        stop_words = set(stopwords.words('english'))\n        return column.apply(lambda x: self.removeStopwords(x,stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbdcc25dcc001388d3a42a007eab48f13aba5c6b"},"cell_type":"code","source":"# visualize the newly-transformed text\nvisualization_pipe = Pipeline([\n    ('sw', StopwordRemover()),\n    ('punc', PunctuationRemover()),\n])\n\npd.DataFrame(visualization_pipe.fit_transform(X)).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e967b0a43d5000fc67482d984b0090bd0dec302c"},"cell_type":"code","source":"pipe2 = Pipeline([\n    ('sw', StopwordRemover()),\n    ('punc', PunctuationRemover()),\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('lm', LogisticRegression())\n    \n])\n\nscores = cross_val_score(pipe2, X,y, cv = 5)\nprint('Mean score: ',scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca3ef2e2b238ff165aa4b42f71c941097e3a5b7"},"cell_type":"markdown","source":"Interesting! Our accuracy dropped after cleaning the data. Upon further examination below, it seems like clearing out the punctuation could have removed some important features of the text such as smileys and exclamation marks. These could have been helpful in determining the sentiment of the airlines.  As seen below, smileys appear to be a common way to express approval (or disapproval)."},{"metadata":{"trusted":true,"_uuid":"87ef7b5e9d6337ec1e8f127625bd4214dd9cb7d8"},"cell_type":"code","source":"smileys = [r'=(',r'=)',r':)',r':(']\n\nfor text in df['text']:\n    for smiley in smileys:\n        if smiley in text:\n            print (text)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03340066bdb273a25969756845421d13e8877e8c"},"cell_type":"markdown","source":"Further examination can be done on the data set for sure (like above), but I'll stop here for now. Hope you guys enjoyed reading through my first time with NLP!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}