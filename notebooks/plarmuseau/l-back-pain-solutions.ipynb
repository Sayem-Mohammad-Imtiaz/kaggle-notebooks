{"metadata":{"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"_change_revision":0,"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{},"cell_type":"markdown","source":"We are searching a solution\n----\ndata"},{"metadata":{"_uuid":"c9216d6c22612eca89a726cc57a7863c58f37198","_cell_guid":"404d4f99-505b-c6ba-0700-d42db0d9ef36","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read data into dataset variable\ntrain = pd.read_csv(\"../input/Dataset_spine.csv\")\n\n\n# Drop the unnamed column in place (not a copy of the original)#\ntrain.drop('Unnamed: 13', axis=1, inplace=True)\ntrain.columns = ['Pelvic Incidence','Pelvic Tilt','Lumbar Lordosis Angle','Sacral Slope','Pelvic Radius', 'Spondylolisthesis Degree', 'Pelvic Slope', 'Direct Tilt', 'Thoracic Slope', 'Cervical Tilt','Sacrum Angle', 'Scoliosis Slope','Outcome']\n# Concatenate the original df with the dummy variables\n#data = pd.concat([data, pd.get_dummies(data['Class_att'])], axis=1)\n\n# Drop unnecessary label column in place. \n#data.drop(['Class_att','Normal'], axis=1, inplace=True)\nnew_col=train.describe().T\nnew_col['eff']=new_col['std']/new_col['mean']\nnew_col['eff2']=new_col['eff']*new_col['std']\nprint(new_col)","outputs":[]},{"metadata":{"_uuid":"b80ae877f82a9e578ac4852a1674a858d5c2edf4","_cell_guid":"779a9779-dd74-18ae-2ca8-e1d135bcfda8"},"cell_type":"markdown","source":"<h1>Exploratory Data Analysis </h1>\nduplicates ? category columns ?"},{"metadata":{"_uuid":"e83a43c5b863e463c4ed1aa6f01714d246a64694","_cell_guid":"5169942c-245a-4bfb-b458-736b2ed4cf6e","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"# Categorical features\ncat_cols = []\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        cat_cols.append(c)\nprint('Categorical columns:', cat_cols)\n\n# Dublicate features\nd = {}; done = []\ncols = train.columns.values\nfor c in cols: d[c]=[]\nfor i in range(len(cols)):\n    if i not in done:\n        for j in range(i+1, len(cols)):\n            if all(train[cols[i]] == train[cols[j]]):\n                done.append(j)\n                d[cols[i]].append(cols[j])\ndub_cols = []\nfor k in d.keys():\n    if len(d[k]) > 0: \n        # print k, d[k]\n        dub_cols += d[k]        \nprint('Dublicates:', dub_cols)\n\n# Constant columns\nconst_cols = []\nfor c in cols:\n    if len(train[c].unique()) == 1:\n        const_cols.append(c)\nprint('Constant cols:', const_cols)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some description stats \n---\ncomparing the normal/abnormal"},{"metadata":{"_uuid":"e20d210fb9e5ad288348662fa397268110c97b13","_cell_guid":"97a35da9-f138-467f-b849-9607b78af2ce","_execution_state":"idle","collapsed":true},"execution_count":null,"cell_type":"code","source":"def add_new_col(x):\n    if x not in new_col.keys(): \n        # set n/2 x if is contained in test, but not in train \n        # (n is the number of unique labels in train)\n        # or an alternative could be -100 (something out of range [0; n-1]\n        return int(len(new_col.keys())/2)\n    return new_col[x] # rank of the label\n\ndef clust(x):\n    kl=0\n    if x<0.75:\n        kl=1\n    if x>0.75 and x<4:\n        kl=2\n    if x>4:\n        kl=4\n    return kl\n\nnew_col= train[['Lumbar Lordosis Angle','Outcome']].groupby('Outcome').describe().fillna(method='bfill')\nnew_col.columns=['count','mean','std','min','p25','p50','p75','max']\nnew_col['eff']=new_col['std']/new_col['mean']\nnew_col['eff2']=new_col['eff']*new_col['std']\nnew_col['clust']=new_col['eff2'].map(clust)","outputs":[]},{"metadata":{"_uuid":"9cb35ef7bb976fcf97b163d9bc892ab8bb332358","_cell_guid":"029f3ec9-7e6b-468b-aa72-4070a6c7efc3","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"print(new_col)\n","outputs":[]},{"metadata":{"_uuid":"410cfa0f1523f7179f5e261a3ade3d110d12ffc0","_cell_guid":"52c00933-943d-44eb-b439-0cd60bdadbb2","_execution_state":"idle","collapsed":true},"execution_count":null,"cell_type":"code","source":"#train=pd.merge(train,new_col, how='inner', left_on='Outcome', right_index=True)\n#sns.pairplot(train[['Pelvic Tilt','std','eff2','Outcome']],hue='Outcome')\n#plt.show()\n\ndef dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()\n","outputs":[]},{"metadata":{"_uuid":"fc9dbf4967d44fccef6c0c016b14da6e7b5a2927","_cell_guid":"433af771-abd4-4ae4-a66f-f5730467f2b1","_execution_state":"idle"},"cell_type":"markdown","source":"What cluster is separating the data ?\n-----\nonly Birch has some potential"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nimport matplotlib.pyplot as plt\n\nn_col=12\nX = train.drop('Outcome',axis=1) # we only take the first two features.\nle = preprocessing.LabelEncoder()\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nle.fit(train['Outcome'])\nprint(list(le.classes_))\nY=le.transform(train['Outcome'])\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         'SparsePCA',\n         'SparseRP',\n         'Birch',\n         'NMF',    \n         'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=24),\n    SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=12, threshold=0.5),\n    NMF(n_components=n_col),    \n    LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n\n    #print('Ypredict *log_sec',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Three linear methods are solving the problem\n---\n\nKNN, Decisiontree, Randomforrest"},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import some data to play with\n       # those ? converted to NAN are bothering me abit...        \n\nfrom sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nX = train.drop('Outcome',axis=1) # we only take the first two features.\nle = preprocessing.LabelEncoder()\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nle.fit(train['Outcome'])\nprint(list(le.classes_))\nY=le.transform(train['Outcome'])\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         'ElasticNet',\n         'SVC',\n         'kSVC',\n         'KNN',\n         'DecisionTree',\n         'RandomForestClassifier',\n         'GridSearchCV',\n         'HuberRegressor',\n         'Ridge',\n         'Lasso',\n         'LassoCV',\n         'Lars',\n         'BayesianRidge',\n         'SGDClassifier',\n         'RidgeClassifier',\n         'LogisticRegression',\n         'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    ElasticNetCV(cv=10, random_state=0),\n    SVC(),\n    SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')","outputs":[]},{"metadata":{"_uuid":"87ba381ee8984f4d2178afb141bdf1d3e513899b","_cell_guid":"5cad00ad-5d11-4645-aa47-bcb64cfc556c","_execution_state":"idle"},"cell_type":"markdown","source":"The best methods to predict are\n---\n* LogisticRegressionCV % errors 14.5161290323\n* Birch % errors 15.4838709677\n* SparseRP % errors 16.4516129032\n* NMF % errors 16.4516129032\n* BayesianRidge % errors 16.7741935484\n* Ridge % errors 17.4193548387\n* HuberRegressor % errors 17.4193548387\n\n"},{"metadata":{"_uuid":"08cb8c1ef6aa108e11104dff3d8a3e47d0783c02","_cell_guid":"3a71e73f-d464-404e-ba19-2284a595850f","_execution_state":"idle"},"cell_type":"markdown","source":"Using TPOT\n----\nlets use a Tpot , but just for fun of using it\ndoesn't beat Birch or has to train longer i suppose"},{"metadata":{"_uuid":"5d70e00facc82bc5fdcb31043b8c5d0a303058f6","_cell_guid":"0df701fe-bd8e-4336-a91f-24d23dfa8429","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"from tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X.astype(np.float64),\n    Y.astype(np.float64), train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')","outputs":[]},{"metadata":{"_uuid":"f78b70c578cad65a4b58d1f49b8d01ac2c4f7f90","_cell_guid":"95cac1f9-fcf2-4457-b57c-96fb57c7664f"},"cell_type":"markdown","source":"XGBoost\n----\nthe Kaggle classic competition winner\nas traditional solves the problem"},{"metadata":{"_uuid":"e91816a2a4c54c13f9b081ffaa47deb83ee65c1c","_cell_guid":"9c07b6ad-40d2-0f84-1f1e-89dac46a7cea","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nimport xgboost as xgb\ndtrain = xgb.DMatrix(X, label=Y)\nparam = {\n    'max_depth': 5,  # the maximum depth of each tree\n    'eta': 0.1,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 2}  # the number of classes that exist in this datset\nnum_round = 700  # the number of training iterations\n\nbst = xgb.train(param, dtrain, num_round)\nbst.dump_model('dumptree.raw.txt')\npreds = bst.predict(dtrain)\n\nprint('% error',sum(  pd.DataFrame(preds.round()*[0,1]).sum(axis=1)\n - Y  ) ) \nprint(pd.DataFrame(preds.round()*[0,1]))","outputs":[]}],"nbformat_minor":1}