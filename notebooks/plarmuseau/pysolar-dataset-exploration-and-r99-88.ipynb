{"metadata":{"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"_change_revision":0,"language_info":{"file_extension":".py","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"a10fd9c376a1fb3677d65a8d399fa7fa6104d8b9","_cell_guid":"e55a9118-a1ce-ed6d-e0f3-47da3ba0b5e0"},"cell_type":"markdown","source":"## Solar Radiation Prediction\n\n> meteorological data from the HI-SEAS weather station from four months (September through December 2016) between Mission IV and Mission V.\n\nUnits:\n\n* Solar radiation: watts per meter^2\n* Temperature: degrees Fahrenheit\n* Humidity: percent\n* Barometric pressure: Hg\n* Wind direction: degrees\n* Wind speed: miles per hour\n* Sunrise/sunset: Hawaii time"},{"metadata":{"_uuid":"b4e6925c9bc54eb9806bcc575adddc90b9dbdf17","_cell_guid":"abb9b3b5-eea1-43ec-028c-908ca3af4d82"},"cell_type":"markdown","source":"### Useful imports and read the data"},{"metadata":{"_uuid":"cd917375ebbd06cceb21f032b15d342ba3f80276","_cell_guid":"bfdcdbd0-0b78-8b9a-a4d6-f049de2ab914","collapsed":true},"execution_count":null,"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","outputs":[]},{"metadata":{"_uuid":"b2a92fc109b001e5db4cafcd550b2768916f6335","_cell_guid":"8e21ab90-4a53-0e37-f755-a464e0ee9aaf","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Read the data\ndf = pd.read_csv('../input/SolarPrediction.csv', parse_dates=['Data'])\ndf.head()","outputs":[]},{"metadata":{"_uuid":"099a4648324b495b5f637c9650b3870aec5eb6ec","_cell_guid":"05a7c863-76d2-c4b0-18a2-ae4a39af1eeb","collapsed":true},"execution_count":null,"cell_type":"code","source":"df.describe()","outputs":[]},{"metadata":{"_uuid":"4e529937bf355e1acfc9164ff6f2c39f8532d735","_cell_guid":"b508aa93-a434-a608-2290-d82ede18f7ad"},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"_uuid":"134f3c990ed0fd1eb205d5e6257e952126222a39","_cell_guid":"af81e7ad-b7f3-ffd9-9c7e-f974a6d1e899","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Convert all dates and times to unix timestamp (timezone doesn't matter now)\ndf['Data'] = df['Data'].dt.date.astype(str)\ndf['TimeSunRise'] = df['Data'] + ' ' + df['TimeSunRise']\ndf['TimeSunSet'] = df['Data'] + ' ' + df['TimeSunSet']\ndf['Data'] = df['Data'] + ' ' + df['Time']\n\n# Convert to Unix timestamp\nfields = ['Data', 'TimeSunRise', 'TimeSunSet']\nfor x in fields:\n    df[x + '_UnixTimeStamp'] = df[x].apply(\n        lambda k: int(datetime.strptime(k, \"%Y-%m-%d %H:%M:%S\").timestamp())\n    )\n\n# New sun time field\ndf['SunTime'] = df['TimeSunSet_UnixTimeStamp'] - df['TimeSunRise_UnixTimeStamp']\n\n# Drop old columns\ndf.drop('UNIXTime', axis=1, inplace=True)\ndf.drop('Data', axis=1, inplace=True)\ndf.drop('Time', axis=1, inplace=True)\ndf.drop('TimeSunRise', axis=1, inplace=True)\ndf.drop('TimeSunSet', axis=1, inplace=True)\n\n","outputs":[]},{"metadata":{"_uuid":"ade7cd2f03f75edbc509c4cff0ce679174a51125","_cell_guid":"76a81d47-5422-4704-4cbf-f16c63eb1f4a"},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"_uuid":"e8c9c25e72bcf751ff1cec21ffa1ae255384e9fa","_cell_guid":"8b6ca0d8-d914-4fd1-a4b8-c81e54c1e91c","collapsed":true},"execution_count":null,"cell_type":"code","source":"def dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()","outputs":[]},{"metadata":{"_uuid":"6e956bbda2f15ea8c57430938fcce9bd35fc9953","_cell_guid":"5f898ba6-f6d5-72f2-d055-431ccfb5183c","collapsed":true},"execution_count":null,"cell_type":"code","source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nimport matplotlib.pyplot as plt\n\nn_col=12\nX = df.drop('Radiation',axis=1) # we only take the first two features.\n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nY=df['Radiation']\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         #'SparsePCA',\n         #'SparseRP',\n         #'Birch',\n         'NMF',    \n         #'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=24),\n    #SparsePCA(n_components=n_col),\n    #SparseRandomProjection(n_components=n_col, dense_output=True),\n    #Birch(branching_factor=10, n_clusters=12, threshold=0.5),\n    NMF(n_components=n_col),    \n    #LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n\n    #print('Ypredict *log_sec',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y))","outputs":[]},{"metadata":{"_uuid":"9ab038e75bf4771e176a3233d6796072f8526e8d","_cell_guid":"dd67a400-5352-1ff9-41f7-7f4dab3b4b73","collapsed":true},"execution_count":null,"cell_type":"code","source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import some data to play with\n       # those ? converted to NAN are bothering me abit...        \n\nfrom sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nX = df.drop('Radiation',axis=1) # we only take the first two features.\nle = preprocessing.LabelEncoder()\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nY=np.round(np.log(df['Radiation'])*10)\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         #'ElasticNet',\n         #'SVC',\n         #'kSVC',\n         'KNN',\n         'DecisionTree',\n         'RandomForestClassifier',\n         #'GridSearchCV',\n         #'HuberRegressor',\n         #'Ridge',\n         #'Lasso',\n         #'LassoCV',\n         #'Lars',\n         #'BayesianRidge',\n         #'SGDClassifier',\n         #'RidgeClassifier',\n         #'LogisticRegression',\n         #'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    #ElasticNetCV(cv=10, random_state=0),\n    #SVC(),\n    #SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')\n    df[name]=regr.predict(X)","outputs":[]},{"metadata":{"_uuid":"87bbb19e9f4288ff5ab98087d1e46e7d8b04c5ed","_cell_guid":"3cb86e07-72e4-4e1b-a286-e79c36877e64"},"cell_type":"markdown","source":"### Model train"},{"metadata":{"_uuid":"3ecc43ab4dde15c10686581a33c5f5d5d195b24a","_cell_guid":"f51b17e9-e358-b3f1-93a7-b94ff9aa6a0a","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Create the K-folds\nk_folds = 5\nkf = KFold(n_splits=k_folds, shuffle = True)\n\n# Prepare dataset\nX = df.drop(['Radiation','Data_UnixTimeStamp','TimeSunRise_UnixTimeStamp','TimeSunSet_UnixTimeStamp'] , axis=1).as_matrix()\ny = df['Radiation'].as_matrix()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","outputs":[]},{"metadata":{"_uuid":"f08c49710fecba31d350283cf7e437c29a19fc43","_cell_guid":"b84869b0-3fb2-0a72-e0c4-b0a9ea2b16fe"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"_uuid":"9988375915dac48d71aa2ae2af1af56669b0775a","_cell_guid":"fa6bb9d0-88e3-9e12-8106-77ee219f273e","collapsed":true},"execution_count":null,"cell_type":"code","source":"xgb_params = {\n    'n_trees': 50, \n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test)","outputs":[]},{"metadata":{"_uuid":"9da563cf2a303b92ef308db7701f06693499089d","_cell_guid":"4885b2cd-c22e-eb9d-1a18-36c181a31747","collapsed":true},"execution_count":null,"cell_type":"code","source":"cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=50,\n    verbose_eval=200, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\nplt.show()","outputs":[]},{"metadata":{"_uuid":"0213b136af533a4519ae316fba412c659a3524a2","_cell_guid":"36b9b584-1f60-c6da-5b56-ee94158d1f5a","collapsed":true},"execution_count":null,"cell_type":"code","source":"num_boost_rounds = len(cv_output)\nprint(num_boost_rounds)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)","outputs":[]},{"metadata":{"_uuid":"e284fd835135c78378be17ac3c3ceac99ebb0480","_cell_guid":"bed7cc8d-3623-6fcb-bf8d-4b8c65874d4f","collapsed":true},"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"R^2 in training: %s\"  % r2_score(dtrain.get_label(), model.predict(dtrain)))\nprint(\"R^2 in testing: %s\"  % r2_score(y_test, model.predict(dtest)))","outputs":[]}],"nbformat_minor":1}