{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/prodata2/xxx.csv', usecols=['type', 'text'])\n# name: full viet nam co dau\n# address: mix vietnam - usa \n# web: random website\n# can them dataset nhieu hon","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Created by Peter Nagy | 2018 May**<br/>\n [Github](https://github.com/nagypeterjob)  <br/>\n [Linkedin](https://www.linkedin.com/in/peternagyjob/)<br/>\nIn this kernel I do perform a multi-class classification with LSTM (Keras).","metadata":{"_uuid":"5a876d13cae7ddc8b480f7eb4d4af4c2b1217f72"}},{"cell_type":"code","source":"#M class has way less data than the orthers, thus the classes are unbalanced.\ndata.type.value_counts()","metadata":{"_cell_guid":"ea29d595-26b0-4d83-a005-853fa59f4506","_uuid":"acf2450933eb3586930df738829abd2e11646e14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I do aspire here to have balanced classes\nnum_of_categories = 45000\nshuffled = data.reindex(np.random.permutation(data.index))\nname = shuffled[shuffled['type'] == 'name'][:num_of_categories]\naddress = shuffled[shuffled['type'] == 'address'][:num_of_categories]\nwebsite = shuffled[shuffled['type'] == 'website'][:num_of_categories]\ncompany = shuffled[shuffled['type'] == 'company'][:num_of_categories]\nengname = shuffled[shuffled['type'] == 'engname'][:num_of_categories]\nconcated = pd.concat([name,address,website,company, engname], ignore_index=True)\n#Shuffle the dataset\nconcated = concated.reindex(np.random.permutation(concated.index))\nconcated['LABEL'] = 0","metadata":{"_cell_guid":"44de46e2-acce-470d-9c46-624cb0dd15b9","_uuid":"eb9f4766b60f5a23901a8bde1d901ced6c7a3b3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One-hot encode the lab\nconcated.loc[concated['type'] == 'name', 'LABEL'] = 0\nconcated.loc[concated['type'] == 'address', 'LABEL'] = 1\nconcated.loc[concated['type'] == 'website', 'LABEL'] = 2\nconcated.loc[concated['type'] == 'company', 'LABEL'] = 3\nconcated.loc[concated['type'] == 'engname', 'LABEL'] = 4\nprint(concated['LABEL'][:10])\nlabels = to_categorical(concated['LABEL'], num_classes=5)\nprint(labels[:10], \"...\")\nif 'type' in concated.keys():\n    concated.drop(['type'], axis=1)\n'''\n [1. 0. 0. 0.] e\n [0. 1. 0. 0.] b\n [0. 0. 1. 0.] t\n\n'''","metadata":{"_cell_guid":"2d3da0fd-6d73-4f3b-b06b-d2bba34bbd4a","_uuid":"60febe37826f220106adf69a51dad124cfae45cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"#","metadata":{}},{"cell_type":"code","source":"n_most_common_words = 20000\nmax_len = 130\n\ntokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-;<=>?@[\\]^_`{|}~', lower=True) # ./-:\n#print(concated['text'].values\ntexts =concated['text'].values.astype(\"str\")\nfor i in range (0,len(texts)):\n    texts[i]=texts[i].astype(\"str\").replace(\".\",\" \").replace(\"/\",\" \").replace(\"-\",\" \").replace(\":\",\" \")\n\nprint(texts[:10])\ntokenizer.fit_on_texts(texts)\n#tokenizer.fit_on_texts(concated['text'].values)\nsequences = tokenizer.texts_to_sequences(concated['text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = pad_sequences(sequences, maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)","metadata":{"_cell_guid":"2ca496ca-4bb7-40de-bf69-d86b521af51f","_uuid":"97226bf26ef141c228a1123e125ef7966612db47","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nemb_dim = 128\nbatch_size = 1024\nprint(labels[24])\nprint(X[24])\n#print(tokenizer.word_index)\n\n\n#name,address,website,company, engname","metadata":{"_cell_guid":"4e5bcf7a-4c6b-44fc-963d-415b9338abe4","_uuid":"28940e621602cfd9645a88dd43427b2431c75b5b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n\nmodel = Sequential()\nmodel.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.7))\nmodel.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())\n#history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])","metadata":{"_cell_guid":"79a42a6f-01f4-4e74-b645-321f2a0a6e39","_uuid":"f50c8494777ca5141da8c23bb932e531a82b89d5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = model.evaluate(X_test,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"_cell_guid":"fa53cfb9-75f7-47ee-b53d-b7f241ee082a","_uuid":"a16c336b7eae3d72c7c92cf799702eacf70677c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"_cell_guid":"1130440c-dd13-4f36-9657-01b13f322efb","_uuid":"f8400fe47eebbb7e8456d6f3617c6bcd7eccecf6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save(\"yy.h5\")\nimport keras\nmodel = keras.models.load_model(\"../input/ewwwww/eww.h5\")\ntxt = [\"john mcdonnel\"]\nprint(txt)\nseq = tokenizer.texts_to_sequences(txt)\n\n\nprint(seq)\npadded = pad_sequences(seq, maxlen=max_len)\npred = model.predict(padded)\nlabels = ['name', 'address', 'web', 'company', 'eng-name']\n\nnp_array = np.array( pred)\nnp_round_to_tenths = np.around(np_array, 2)\nround_to_tenths = list(np_round_to_tenths)\n\nprint(np.argsort(np.max(np_array, axis=0))[-2])\nprint(np.argmax(pred))\n\nprint(round_to_tenths, labels[np.argmax(pred)])\n\n\n\n\n\n\n\n\n\n\n","metadata":{"_cell_guid":"9af38e60-0811-485d-8da3-32a744b53365","_uuid":"25ba8e0b354451cc482a93324e42781fde6d0826","trusted":true},"execution_count":null,"outputs":[]}]}