{"metadata":{"_is_fork":false,"_change_revision":0,"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"source":"# Building a Chatbot\n\nIn this project, we will build a chatbot using scripts from the television shows South Park and The Simpsons. The main features of our model are LSTM cells, a bidirectional dynamic RNN, and an attention cell wrapper.\n\nThe data for our chatbot are from datasets on Kaggle: [South Park](https://www.kaggle.com/tovarischsukhov/southparklines), [The Simpsons](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data). Much more data could be added to this model, if you wish, but I thought it would be interesting to use similar types of data with the hope that a *personality* could be created. In this case, some sort of cartoon personality.","metadata":{"_uuid":"dd3c355ae460fb7bf7436fff580a655527e86438","_cell_guid":"767fe171-f706-a810-0fcd-406851e97cb2"},"cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport re\ntf.__version__\n","outputs":[],"metadata":{"_uuid":"672bb6b6ea3cb4f7968717ef07a415a400193702","_cell_guid":"efdb2fb0-f2d7-31e3-1ca9-e5ae2c885ecb"},"execution_count":null,"cell_type":"code"},{"source":"# Load the data\n# I'm only going to use a small portion of the data so that it load quickly on Kaggle.\nsouthpark = pd.read_csv(\"../input/All-seasons.csv\")[:100] \n#simpsons = pd.read_csv(\"simpsons.csv\") # can't load on Kaggle, but you can do it on your own pc.","outputs":[],"metadata":{"collapsed":true,"_uuid":"df05bd8cf3c9821cc65cf13efda32616a6cb5fba","_cell_guid":"e788d924-d0be-afe1-8932-a2e28f3bfa96"},"execution_count":null,"cell_type":"code"},{"source":"southpark.head()","outputs":[],"metadata":{"_uuid":"9c1645169c4300f92a6028ed19e2a60ee720d5c1","_cell_guid":"24a1f166-69e3-e9e1-2a14-35fd3931298a"},"execution_count":null,"cell_type":"code"},{"source":"print(\"***South Park lines:\")\nfor i in range(0,5):\n    print(\"AAA Line #\",i+1)\n    print(southpark.Line[i])","outputs":[],"metadata":{"_uuid":"72682e4aa6cbf990582e3a9dd0ffb8666f918083","_cell_guid":"b792f914-3015-5dea-d78e-bd06d6e62b12"},"execution_count":null,"cell_type":"code"},{"source":"def clean_text(text):\n    '''Clean text by removing unnecessary characters and altering the format of words.'''\n\n    text = text.lower()\n    \n    text = re.sub(r\"\\n\", \"\",  text)\n    text = re.sub(r\"[-()]\", \"\", text)\n    text = re.sub(r\"\\.\", \" .\", text)\n    text = re.sub(r\"\\!\", \" !\", text)\n    text = re.sub(r\"\\?\", \" ?\", text)\n    text = re.sub(r\"\\,\", \" ,\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"ohh\", \"oh\", text)\n    text = re.sub(r\"ohhh\", \"oh\", text)\n    text = re.sub(r\"ohhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhhh\", \"oh\", text)\n    text = re.sub(r\"ahh\", \"ah\", text)\n    \n    return text","outputs":[],"metadata":{"collapsed":true,"_uuid":"619502344c8bc90916352d651f67606837a6946b","_cell_guid":"1c320ecc-2384-6e5b-ad35-cf613a0a31cb"},"execution_count":null,"cell_type":"code"},{"source":"# Clean the scripts and add them to the same list.\ntext = []\n\nfor line in southpark.Line:\n    text.append(clean_text(line))","outputs":[],"metadata":{"collapsed":true,"_uuid":"0d7badb7beb88358ee005f045ff228719dc3ea61","_cell_guid":"6ee49fef-0271-d9df-509b-e5adb8d782af"},"execution_count":null,"cell_type":"code"},{"source":"# Take a look at some of the text to ensure that it has been cleaned well.\nlimit = 0\nfor i in range(limit,limit+20):\n    print(text[i])","outputs":[],"metadata":{"_uuid":"464581294e48bd41cfac70850c162b4a5c1d3abc","_cell_guid":"744f1cf1-e2aa-34cf-9200-0d013db08f95"},"execution_count":null,"cell_type":"code"},{"source":"# Find the length of lines\nlengths = []\nfor line in text:\n    lengths.append(len(line.split()))\n\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])","outputs":[],"metadata":{"collapsed":true,"_uuid":"1012c06f2726ecf52109698916ca9cead797cb2b","_cell_guid":"8d009c25-609a-bf13-6944-1926b80eaa03"},"execution_count":null,"cell_type":"code"},{"source":"lengths.describe()","outputs":[],"metadata":{"_uuid":"c02eb6e3d0a797907588e9a590e62aa581c252a3","_cell_guid":"c31dc57c-f4d7-536b-a7cd-44fcafbd0d31"},"execution_count":null,"cell_type":"code"},{"source":"print(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))\nprint(np.percentile(lengths, 99))","outputs":[],"metadata":{"_uuid":"958d5a77c4fa7fe48dc181fde7dc228a3710d625","_cell_guid":"c0ff8d02-4150-a598-4fbd-d3929d5f39a5"},"execution_count":null,"cell_type":"code"},{"source":"# Limit the text we will use to the shorter 95%.\nmax_line_length = 30\n\nshort_text = []\nfor line in text:\n    if len(line.split()) <= max_line_length:\n        short_text.append(line)","outputs":[],"metadata":{"collapsed":true,"_uuid":"2e04541962690e7011e6923137a8e02eb08ff3d2","_cell_guid":"d17503fb-e676-e8e0-eb76-9a3f9627a228"},"execution_count":null,"cell_type":"code"},{"source":"# Create a dictionary for the frequency of the vocabulary\nvocab = {}\nfor line in short_text:\n    for word in line.split():\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1","outputs":[],"metadata":{"collapsed":true,"_uuid":"aae6aef4f54ea93e9c65637b290cf3230eea2eab","_cell_guid":"7503d305-a360-19dd-04a9-8b68283a6dd4"},"execution_count":null,"cell_type":"code"},{"source":"# Limit the vocabulary to words used more than 3 times.\nthreshold = 3\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1","outputs":[],"metadata":{"collapsed":true,"_uuid":"ebea0842c5ab94b48b7de81080ceff8385b50025","_cell_guid":"044a7c8c-a71b-ab01-a2f2-c724fa7bc17f"},"execution_count":null,"cell_type":"code"},{"source":"print(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","outputs":[],"metadata":{"_uuid":"b554dc7d62b1fbedfa0424ef6f35ed86e3885286","_cell_guid":"8d47471f-0a77-3417-e073-108b92473468"},"execution_count":null,"cell_type":"code"},{"source":"# In case we want to use a different vocabulary sizes for the source and target text, \n# we can set different threshold values.\n# Nonetheless, we will create dictionaries to provide a unique integer for each word.\nsource_vocab_to_int = {}\n\nword_num = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        source_vocab_to_int[k] = word_num\n        word_num += 1\n        \ntarget_vocab_to_int = {}\n\nword_num = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        target_vocab_to_int[k] = word_num\n        word_num += 1","outputs":[],"metadata":{"collapsed":true,"_uuid":"ee0c2a752339da7128377d57250a4083ec6831af","_cell_guid":"314ed1d6-43c1-7e51-f838-c0b38d801c6e"},"execution_count":null,"cell_type":"code"},{"source":"# Add the unique tokens to the vocabulary dictionaries.\ncodes = ['<PAD>','<EOS>','<UNK>','<GO>']\n\nfor code in codes:\n    source_vocab_to_int[code] = len(source_vocab_to_int)+1\n    \nfor code in codes:\n    target_vocab_to_int[code] = len(target_vocab_to_int)+1","outputs":[],"metadata":{"collapsed":true,"_uuid":"b2bf1be2ac7cb41e3ee9083b399141e5b941e9b9","_cell_guid":"9627a0d8-405b-f3ec-0a54-2a56b2328bd8"},"execution_count":null,"cell_type":"code"},{"source":"# Create dictionaries to map the unique integers to their respective words.\n# i.e. an inverse dictionary for vocab_to_int.\nsource_int_to_vocab = {v_i: v for v, v_i in source_vocab_to_int.items()}\ntarget_int_to_vocab = {v_i: v for v, v_i in target_vocab_to_int.items()}","outputs":[],"metadata":{"collapsed":true,"_uuid":"8f35c94a8f7f61573610ad5c8c2d7c43479aa425","_cell_guid":"4d81c92a-f76e-ec54-801c-9cd3191576aa"},"execution_count":null,"cell_type":"code"},{"source":"# Check the length of the dictionaries.\nprint(len(source_vocab_to_int))\nprint(len(source_int_to_vocab))\nprint(len(target_vocab_to_int))\nprint(len(target_int_to_vocab))","outputs":[],"metadata":{"_uuid":"fcd5331a897b1dc7346ecaee7542824acd3da41e","_cell_guid":"73fbb82b-8425-9835-3c33-827976826606"},"execution_count":null,"cell_type":"code"},{"source":"# Create the source and target texts.\n# The target text is the line following the source text.\nsource_text = short_text[:-1]\ntarget_text = short_text[1:]\n\nfor i in range(len(target_text)):\n    target_text[i] += ' <EOS>'","outputs":[],"metadata":{"collapsed":true,"_uuid":"8e7ec3e1defa3a68cda6a925b03739652aec9b94","_cell_guid":"c69df23e-ed79-43e1-f8dd-8c95c23233eb"},"execution_count":null,"cell_type":"code"},{"source":"# Check if the source and target text lengths match.\nprint(len(source_text))\nprint(len(target_text))","outputs":[],"metadata":{"_uuid":"192a6b77f2bf7657a6c3fa1033974efc270eda3f","_cell_guid":"647f8137-c08d-8f59-b4fe-975e156fe4b1"},"execution_count":null,"cell_type":"code"},{"source":"# Convert the text to integers. \n# Replace any words that are not in the respective vocabulary with <UNK> (unknown)\nsource_int = []\nfor line in source_text:\n    sentence = []\n    for word in line.split():\n        if word not in source_vocab_to_int:\n            sentence.append(source_vocab_to_int['<UNK>'])\n        else:\n            sentence.append(source_vocab_to_int[word])\n    source_int.append(sentence)\n    \ntarget_int = []\nfor line in target_text:\n    sentence = []\n    for word in line.split():\n        if word not in target_vocab_to_int:\n            sentence.append(target_vocab_to_int['<UNK>'])\n        else:\n            sentence.append(target_vocab_to_int[word])\n    target_int.append(sentence)","outputs":[],"metadata":{"collapsed":true,"_uuid":"22634f9dbcf741ab491101e06bab947f11ef2ab2","_cell_guid":"1a061040-6dea-7033-51fe-53550f0ecb92"},"execution_count":null,"cell_type":"code"},{"source":"# Check the lengths\nprint(len(source_int))\nprint(len(target_int))","outputs":[],"metadata":{"_uuid":"d06f00ca4b5b1099d739aa10d5d7a81d64fed23e","_cell_guid":"c6dbe731-d743-ae21-6815-833f76db5596"},"execution_count":null,"cell_type":"code"},{"source":"def model_inputs():\n    '''Create palceholders for inputs to the model'''\n    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n    lr = tf.placeholder(tf.float32, name='learning_rate')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\n    return input_data, targets, lr, keep_prob","outputs":[],"metadata":{"collapsed":true,"_uuid":"8f41a4833686427d998bc6666cd57ae497cfc05e","_cell_guid":"d87bdca8-69c0-0829-957d-70b9117379b0"},"execution_count":null,"cell_type":"code"},{"source":"def process_encoding_input(target_data, vocab_to_int, batch_size):\n    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n\n    return dec_input","outputs":[],"metadata":{"collapsed":true,"_uuid":"b3534d8c57aa4c0f8ed7a4e657f0dd7e885d1bf9","_cell_guid":"055e1f24-fe03-9d41-6a92-1258b9a425ea"},"execution_count":null,"cell_type":"code"},{"source":"def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length, attn_length):\n    '''Create the encoding layer'''\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n    cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length, state_is_tuple = True)\n    enc_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n                                                   cell_bw = enc_cell,\n                                                   sequence_length = sequence_length,\n                                                   inputs = rnn_inputs, \n                                                   dtype=tf.float32)\n\n    return enc_state","outputs":[],"metadata":{"collapsed":true,"_uuid":"ac30437edd2e1c31fb1903bea8335dc077ccbc5f","_cell_guid":"f06afb01-75dc-afbe-08e5-d2312c70d4f2"},"execution_count":null,"cell_type":"code"},{"source":"def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n                         output_fn, keep_prob):\n    '''Decode the training data'''\n    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n        dec_cell, train_decoder_fn, dec_embed_input, sequence_length, scope=decoding_scope)\n    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n    return output_fn(train_pred_drop)","outputs":[],"metadata":{"collapsed":true,"_uuid":"70d4563b677221c933f3ee9fdade0b8bf268c0dc","_cell_guid":"0a0c42c6-4bcb-e347-f2e4-2eb774911592"},"execution_count":null,"cell_type":"code"},{"source":"def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n    '''Decode the prediction data'''\n    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(\n        output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size)\n    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n    return infer_logits","outputs":[],"metadata":{"collapsed":true,"_uuid":"37975c1e8893403a14bdd4ad6d7a4cf9b54dd5a3","_cell_guid":"cd4fcaca-5383-6f17-6b86-cbd144a90a40"},"execution_count":null,"cell_type":"code"},{"source":"def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n                   num_layers, vocab_to_int, keep_prob, attn_length):\n    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n    \n    with tf.variable_scope(\"decoding\") as decoding_scope:\n        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n        cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length, state_is_tuple = True)\n        dec_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n        \n        weights = tf.truncated_normal_initializer(stddev = 0.1)\n        biases = tf.zeros_initializer()\n        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n                                                                vocab_size, \n                                                                None, \n                                                                scope=decoding_scope,\n                                                                weights_initializer = weights,\n                                                                biases_initializer = biases)\n\n        train_logits = decoding_layer_train(\n            encoder_state[0], dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)\n        decoding_scope.reuse_variables()\n        infer_logits = decoding_layer_infer(encoder_state[0], dec_cell, dec_embeddings, vocab_to_int['<GO>'],\n                                            vocab_to_int['<EOS>'], sequence_length, vocab_size,\n                                            decoding_scope, output_fn, keep_prob)\n\n    return train_logits, infer_logits","outputs":[],"metadata":{"collapsed":true,"_uuid":"9540c68b7152e63efdd300d8ce8f6ce4df2d342d","_cell_guid":"1a011d47-bf8a-1270-fe60-b5de34e803f6"},"execution_count":null,"cell_type":"code"},{"source":"def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, vocab_to_int, attn_length):\n    \n    '''Use the previous functions to create the training and inference logits'''\n    \n    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size+1, enc_embedding_size)\n    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length, attn_length)\n\n    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size+1, dec_embedding_size], -1.0, 1.0))\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, target_vocab_size+1, \n                                                sequence_length, rnn_size, num_layers, vocab_to_int, keep_prob, \n                                                attn_length)\n    \n    return train_logits, infer_logits","outputs":[],"metadata":{"collapsed":true,"_uuid":"4af2cc6c0420dc2326feec8046270184ddc10227","_cell_guid":"68148843-ab59-f13e-f23b-b8dde1939b52"},"execution_count":null,"cell_type":"code"},{"source":"# Set the parameters\nepochs = 100\nbatch_size = 128\nrnn_size = 512\nnum_layers = 2\nencoding_embedding_size = 512\ndecoding_embedding_size = 512\nattn_length = 10\nlearning_rate = 0.0005\nkeep_probability = 0.8","outputs":[],"metadata":{"collapsed":true,"_uuid":"20bc55147f46e3fa1476677682457318424609d6","_cell_guid":"9489a254-0d1a-d636-54d5-3dd8ed521081"},"execution_count":null,"cell_type":"code"},{"source":"train_graph = tf.Graph()\nwith train_graph.as_default():\n    \n    # Load the model inputs\n    input_data, targets, lr, keep_prob = model_inputs()\n    # Sequence length will be the max line length for each batch\n    sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n    input_shape = tf.shape(input_data)\n    \n    # Create the logits from the model\n    train_logits, inference_logits = seq2seq_model(\n        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), \n        len(target_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n        target_vocab_to_int, attn_length)\n    \n    # Create a tensor to be used for making predictions.\n    tf.identity(inference_logits, 'logits')\n    with tf.name_scope(\"optimization\"):\n        # Loss function\n        cost = tf.contrib.seq2seq.sequence_loss(\n            train_logits,\n            targets,\n            tf.ones([input_shape[0], sequence_length]))\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)","outputs":[],"metadata":{"_uuid":"4c00c4fd1e4e08209bb9be365a912c61980a2467","_cell_guid":"5aa9f21c-ac73-913b-35b6-59c67ab352f1"},"execution_count":null,"cell_type":"code"},{"source":"def pad_sentence_batch(sentence_batch, vocab_to_int):\n    \"\"\"Pad lines with <PAD> so each line of a batch has the same length\"\"\"\n    max_sentence = max([len(sentence) for sentence in sentence_batch])\n    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]","outputs":[],"metadata":{"collapsed":true,"_uuid":"9409f3a2c34e258f13a2d1683b3aa4f6f17dea6b","_cell_guid":"ab4ff29b-e288-3c85-3222-386d06d6075e"},"execution_count":null,"cell_type":"code"},{"source":"def batch_data(source, target, batch_size):\n    \"\"\"Batch source and target together\"\"\"\n    for batch_i in range(0, len(source)//batch_size):\n        start_i = batch_i * batch_size\n        source_batch = source[start_i:start_i + batch_size]\n        target_batch = target[start_i:start_i + batch_size]\n        yield (np.array(pad_sentence_batch(source_batch, source_vocab_to_int)), \n               np.array(pad_sentence_batch(target_batch, target_vocab_to_int)))","outputs":[],"metadata":{"collapsed":true,"_uuid":"94d5d6924015187ce3a4d6b5b0110c25ae489f6d","_cell_guid":"e00733d7-7416-61b7-7df6-f89b210fec1d"},"execution_count":null,"cell_type":"code"},{"source":"train_valid_split = int(len(source_int)*0.1)\n\ntrain_source = source_int[train_valid_split:]\ntrain_target = target_int[train_valid_split:]\n\nvalid_source = source_int[:train_valid_split]\nvalid_target = target_int[:train_valid_split]\n\nprint(len(train_source))\nprint(len(valid_source))","outputs":[],"metadata":{"collapsed":true,"_uuid":"89b0fe066216c2f593e941e86434ec1bfb312b53","_cell_guid":"8b8ae633-16d9-a82b-8c01-018e2d771496"},"execution_count":null,"cell_type":"code"},{"source":"import time\n\nlearning_rate_decay = 0.95\ndisplay_step = 50\nstop_early = 0\nstop = 3\ntotal_train_loss = 0\nsummary_valid_loss = []\n\n\ncheckpoint = \"best_model.ckpt\" \n\nwith tf.Session(graph=train_graph) as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for epoch_i in range(1, epochs+1):\n        for batch_i, (source_batch, target_batch) in enumerate(\n                batch_data(train_source, train_target, batch_size)):\n            start_time = time.time()\n            _, loss = sess.run(\n                [train_op, cost],\n                {input_data: source_batch,\n                 targets: target_batch,\n                 lr: learning_rate,\n                 sequence_length: target_batch.shape[1],\n                 keep_prob: keep_probability})\n\n            total_train_loss += loss\n            end_time = time.time()\n            batch_time = end_time - start_time\n            \n            if batch_i % display_step == 0:\n                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n                      .format(epoch_i,\n                              epochs, \n                              batch_i, \n                              len(train_source) // batch_size, \n                              total_train_loss / display_step, \n                              batch_time*display_step))\n                total_train_loss = 0\n\n            if batch_i % 235 == 0 and batch_i > 0:\n                total_valid_loss = 0\n                start_time = time.time()\n                for batch_ii, (source_batch, target_batch) in \\\n                        enumerate(batch_data(valid_source, valid_target, batch_size)):\n                    valid_loss = sess.run(\n                    cost, {input_data: source_batch,\n                           targets: target_batch,\n                           lr: learning_rate,\n                           sequence_length: target_batch.shape[1],\n                           keep_prob: 1})\n                    total_valid_loss += valid_loss\n                end_time = time.time()\n                batch_time = end_time - start_time\n                avg_valid_loss = total_valid_loss / (len(valid_source) / batch_size)\n                print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n                \n                learning_rate *= learning_rate_decay\n                \n                summary_valid_loss.append(avg_valid_loss)\n                if avg_valid_loss <= min(summary_valid_loss):\n                    print('New Record!') \n                    stop_early = 0\n                    saver = tf.train.Saver() \n                    saver.save(sess, checkpoint)\n                \n                else:\n                    print(\"No Improvement.\")\n                    stop_early += 1\n                    if stop_early == stop:\n                        break\n        if stop_early == stop:\n            print(\"Stopping Training.\")\n            break","outputs":[],"metadata":{"collapsed":true,"_uuid":"7cc284abd68193ec55c0c8f18404f8115f189178","_cell_guid":"c58b89eb-3f19-7fc5-e502-9ba690499467"},"execution_count":null,"cell_type":"code"},{"source":"def sentence_to_seq(sentence, vocab_to_int):\n    '''Prepare the predicted sentence for the model'''\n    \n    sentence = clean_text(sentence)\n    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in sentence.split()]","outputs":[],"metadata":{"collapsed":true,"_uuid":"b04a6958c82a16e0b0cddf76d009ec591ac19def","_cell_guid":"a4f2dc58-a95b-e127-fa58-46d005c2e719"},"execution_count":null,"cell_type":"code"},{"source":"# This part of the project won't work on Kaggle since it requires loading checkpoints of the model\n\n# To create your own input sentence\n#input_sentence = 'Oh my God they killed Kenny!'\n\n# To use an sentence from the data\n#random = np.random.choice(len(short_text))\n#input_sentence = short_text[random]\n\n# Clean the input sentence before it is used in the model\n#input_sentence = sentence_to_seq(input_sentence, source_vocab_to_int)\n\n#checkpoint = \"./\" + checkpoint \n\n#loaded_graph = tf.Graph()\n#with tf.Session(graph=loaded_graph) as sess:\n#    # Load the saved model\n#    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n#    loader.restore(sess, checkpoint)\n    \n    # Load the tensors to be used as inputs\n#    input_data = loaded_graph.get_tensor_by_name('input:0')\n#    logits = loaded_graph.get_tensor_by_name('logits:0')\n#    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n    \n#    response_logits = sess.run(logits, {input_data: [input_sentence],keep_prob: 1.0})[0]\n\n#print('Input')\n#print('  Word Ids:      {}'.format([i for i in input_sentence]))\n#print('  Input Words: {}'.format([source_int_to_vocab[i] for i in input_sentence]))\n\n#print('\\nResponse')\n#print('  Word Ids:      {}'.format([i for i in np.argmax(response_logits, 1)]))\n#print('  Response Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(response_logits, 1)]))","outputs":[],"metadata":{"collapsed":true,"_uuid":"b6fd6ea349950a41e59a41abffdb0c4dde24901f","_cell_guid":"5b38e8c5-99c3-5268-d8f7-ac9b65273330"},"execution_count":null,"cell_type":"code"},{"source":"# Summary\n\nI hope that you have found this project to be rather interesting and informative. There are many ways to improve and alter this model, which could make a fun project for yourself. I encourage you to find some other neat projects out there and see if you can combine some methods from all of us to make an even better project! This might be able to give you some ideas: http://web.stanford.edu/class/cs20si/assignments/a3.pdf\n\nOne thing that I really like about this model is that it can be applied to many other types of projects, such as language translation or text simplification. seq2seq models are pretty flexible, so it's cool to see and build a wide variety of projects.\n\nThanks for reading and best of luck building your models!","metadata":{"_uuid":"d87380894160464c573e594b4cba6a4a5ec051c4","_cell_guid":"98a22c61-45a7-3062-2e7a-4beea91466a8"},"cell_type":"markdown"},{"source":"","outputs":[],"metadata":{"collapsed":true,"_uuid":"55e0cf414da2bb87b1e09d8798ddf0e88263cfd4","_cell_guid":"83c455bf-b9f3-0093-d232-48649d2b5ce4"},"execution_count":null,"cell_type":"code"}],"nbformat":4,"nbformat_minor":1}