{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Mining Term Project\n\n## Student: Haojin Liao\n\n## ID: 1001778275\n\n## Demo Link: [http://liaohaojin.pythonanywhere.com/](http://liaohaojin.pythonanywhere.com/)\n\n## Github Repo Link [https://github.com/YapheeetS/dm_term_project](https://github.com/YapheeetS/dm_term_project)\n\n## Video Link [https://youtu.be/aBTan4dVh04](https://youtu.be/aBTan4dVh04)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Environment\n\n<font size=4>Python==3.6</font>\n\n<br/>\n\n<font size=4>tensorflow-gpu==2.0.0</font>\n\n<br/>\n\n<font size=4>scikit-learn==0.22.1</font>\n\n<br/>\n\n<font size=4>nltk==3.4.5</font>\n\n<br/>\n\n<font size=4>matplotlib==3.1.3</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n<font size=4>The task of the project is to predict the rating given a review. The datasets is the game reviews users posted on boardgamegeek, the world's largest board game site. This is actually a natural laguage process task and it contains the flowing steps: data collection, data preprocessing, training an algorithm, evaluate the algorithm, use the model to deploy a demo.</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data collection\n\n<font size=4>The datasets from './boardgamegeek-reviews/bgg-13m-reviews.csv' contains six columns(index, user, rating, comment, ID, name). But only two columns will be used for this task which is rating and comment. After this two columns are chosen, there are still some NaNs in the datasets.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\n\ndata_frame = pd.read_csv('./boardgamegeek-reviews/bgg-13m-reviews.csv', index_col=0)\ndata_frame.drop(data_frame.columns[4], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[3], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[0], axis=1, inplace=True)\ndata_frame.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4>The rows which contains NaN should be removed from the datasets</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data_frame = data_frame[~data_frame.comment.str.contains(\"NaN\",na=True)]\nprint(data_frame.head())\nprint('data shape: ', data_frame.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis\n\n<font size=4>The task is considered to be a classification task, so the rating is rounded and there are only 11 numbers of the rating which is from zero to ten. Most of the ratings concentrate from 6 to 8 and there are only 11 zero ratings.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data_frame[\"rating\"] = data_frame[\"rating\"].round(0).astype(int)\ndata_frame.groupby([\"rating\"]).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(data_frame.rating, 50)\nplt.xlabel('Ratings')\nplt.ylabel('Count')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocess\n\n### Raw data\n<font size=4>There are 2637755 rows with 2 columns(comments and rating) in the datasets which is divided into 11 classes from 0 to 10. Each class contains diffrent numbers of datasets</font>\n\n<br/>\n\n### Normalization\n<font size=4>The first step is to uniform text format to lower-case. For example, From the perspective of machine learning algorithms, it is hard to distinguish between 'Car' / 'car' / 'CAR', but they all mean the same thing. Therefore, I generally convert all letters in the text to lowercase, and each word is represented by a unique word.</font>\n\n<br/>\n\n### Tokenization\n<font size=4>Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.Word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class.</font>\n\n<br/>\n\n### Stop Word\n<font size=4>Stop Words means that in information searching, useless words are automatically filtered out before processing of natural language data(or text) in order to save storage space and improve search efficiency.These words are called Stop Words. These stop words are manually input, non-automatically generated. The nltk is used to help to find the stop words. </font>\n\n<br/>\n\n### Remove punctuation\n<font size=4>The punctuation should be removed because they usually do not have meaning.</font>\n\n<br/>\n\n### Stemming\n<font size=4>This is a word stemmer based on the Porter stemming algorithm which can extract the stem of word. Remove useless information from words. Reduce the size of the corpus. For example, 'branching' / 'branched' / 'branches' can be restored to 'branch'. They all express the meaning of dividing into multiple routes or branches. This helps reduce complexity while retaining the basic meaning of the words. Stemming is done with very simple search and replace style rules.Stemming sometimes generates stems that are not complete English words. But as long as all forms of the word are reduced to the same stem. So they all have a common underlying meaning.</font>\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nimport string\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nnltk.download('punkt')\nnltk.download('stopwords')\n\nx = np.array(data_frame.comment)\ny = np.array(data_frame.rating)\ny = np.round(y)\n\nall_texts = []\nfor index, text in tqdm(enumerate(x)):\n    # lower case\n    text = text.lower()\n    # tokenize\n    words = word_tokenize(text)\n    # topwords\n    words = [w for w in words if w not in stopwords.words('english')]\n    # remove punctuation\n    words = [w for w in words if w not in string.punctuation]\n    # Stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    all_texts.append(words)\n\nx = np.array(all_texts)\nindex = [i for i in range(len(x))]\nrandom.shuffle(index)\nx = x[index]\ny = y[index]\n\nnp.save('x.npy', x)\nnp.save('y.npy', y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training an algorithm\n\n<font size=4>Before starting, The dataset is first devided into train data and test data. Then TF-IDF is used to assign a weight to each word. To assess the importance of a word to a document set or a document in a corpus. The importance of a word increases in proportion to the number of times it appears in the document.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nimport sklearn.metrics as metrics\nimport numpy as np\nfrom tqdm import tqdm\n\nprint('load data')\n\nx = np.load('x.npy', allow_pickle=True)\ny = np.load('y.npy', allow_pickle=True)\n\nfor i, d in tqdm(enumerate(x)):\n    sentence = ' '.join(x[i])\n    x[i] = sentence\n    \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n\n\ncount_vect = CountVectorizer(min_df=0.001, max_df=0.5, max_features=1000)\nX_train_counts = count_vect.fit_transform(x_train)\nX_test_counts = count_vect.fit_transform(x_test)\n\ncount_vect = TfidfTransformer()\ntf_transformer = TfidfTransformer().fit(X_train_counts)\nx_train = tf_transformer.transform(X_train_counts)\nx_train = x_train.toarray()\nprint(x_train.shape)\n\ntf_transformer = TfidfTransformer().fit(X_test_counts)\nx_test = tf_transformer.transform(X_test_counts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree\n\n<font size=4>The decision tree function is loaded from sklearn. The parameters used here means: </font>\n\n<br/>\n\n<font size=4>criterion: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.</font>\n\n<br/>\n\n<font size=4>max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</font>\n\n<br/>\n\n<font size=4>min_samples_split: The minimum number of samples required to split an internal node</font>\n\n<br/>\n\n<font size=4>min_samples_leafint: he minimum number of samples required to be at a leaf node. </font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def decision_tree(train_x, train_y, test_x, test_y):\n    print('...Decision_tree...')\n    clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, min_samples_split=20, min_samples_leaf=5).fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('Decision tree accuracy: ', metrics.accuracy_score(test_y, predict_y))\n\n\ndecision_tree(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost(Decision tree)\n\n<font size=4>The AdaBoostClassifier function is loaded from sklearn. The parameters used here means:</font>\n\n<br/>\n\n<font size=4>algorithm: The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</font>\n\n<br/>\n\n<font size=4>n_estimators: The maximum number of estimators at which boosting is terminated.</font>\n\n<br/>\n\n<font size=4>learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\ndef adaboost(train_x, train_y, test_x, test_y):\n    print('...adaboost...')\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n                         algorithm=\"SAMME.R\",n_estimators=50, learning_rate=0.8).fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('Adaboost accuracy: ', metrics.accuracy_score(test_y, predict_y))\n    \nadaboost(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayesian\n\n<font size=4>The MultinomialNB function is loaded from sklearn. The parameters used here means:</font>\n\n<br/>\n\n<font size=4>alpha: Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def bayes_fit_and_predicted(train_x, train_y, test_x, test_y):\n    print('...Bayes...')\n    clf = MultinomialNB().fit(train_x, train_y, alpha=1.0)\n    predict_y = clf.predict(test_x)\n    print('Bayes accuracy: ', metrics.accuracy_score(test_y, predict_y))\n    \nbayes_fit_and_predicted(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM\n\n<font size=4>The LinearSVC function is loaded from sklearn. The parameters used here means:</font>\n\n<br/>\n\n<font size=4>penalty: Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.</font>\n\n<br/>\n\n<font size=4>C: Regularization parameter. The strength of the regularization is inversely proportional to C.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def svm_fit_and_predicted(train_x, train_y, test_x, test_y, C=1.0):\n    print('...SVM...')\n    clf = LinearSVC(C=1.0, penalty='l2').fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('SVM accuracy: ', metrics.accuracy_score(test_y, predict_y))\n\nsvm_fit_and_predicted(x_train, y_train, x_test, y_test)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks\n\n<font size=4>Deep learning models have achieved great achievement in text classification. The recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence.The input of the RNN model not only has x, but also the value generated by itself will be used as the next input. RNN can be seen as multiple copies of a neural network, each neural network The generated information will be passed to the next one, so the loop of the RNN can be expanded to make it easier for us to understand its chain-like characteristics.</font>\n\n<br/>\n\n<font size=4>The model used in the flowing code is call GRU. GRU(Gate Recurrent Unit) is a type of Recurrent Neural Network (RNN). Like LSTM (Long-Short Term Memory), it is also proposed to solve the problems of long-term memory and gradient in back propagation.</font>\n\n<br/>\n\n<font size=4>The code is running on a GPU(1080Ti) server and the tensorflow-gpu is used to accelerate the speed of training</font>\n\n<br/>\n\n<font size=4>The accuracy is 0.345 which is better than the traditional machine learning model</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os\nimport sklearn.metrics as metrics\nimport datetime\nfrom sklearn.model_selection import train_test_split\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 1000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\nprint('load data')\n\nx = np.load('x.npy', allow_pickle=True)\ny = np.load('y.npy', allow_pickle=True)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)\nx_train = tokenizer.texts_to_sequences(x_train)\n\n# {word1: index1, word2: index2}\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n\n\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM,\n                                    input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.GRU(256, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.GRU(128))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(11, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()\n\n# model.fit(x_train, y_train, batch_size=128, epochs=10,validation_split=VALIDATION_SPLIT)\nmodel.fit(x_train, y_train, batch_size=128, epochs=5)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n# model save\nmodel.save('gru_model.h5')\n\nresults = model.evaluate(x_test, y_test)\nprint('Test loss:', results[0])\nprint('Test accuracy:', results[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2vec + GRU\n\n<font size=4>Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus.</font>\n\n<br/>\n\n<font size=4>The difference normal GRU model and Word2vec + GRU model is that the first embedding layer is different. The normal GRU model will train the embedding when training the GRU model, but the Word2vec + GRU model would pre-train the embedding layer and the pre-trained vectors is used as initial embeddings layer to be put into the GRU model</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport gensim\nimport logging\nfrom gensim.models import Word2Vec\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import multi_gpu_model\nimport pickle\n\nlogging.basicConfig(\n    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 1000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\n\nprint('load data')\n\nx = np.load('x.npy', allow_pickle=True)\ny = np.load('y.npy', allow_pickle=True)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n\n# model = Word2Vec(x_train, size=100, min_count=1, window=5)\n# model.save('Word2Vec2.dict')\nword2vec_model = Word2Vec.load('Word2Vec2.dict')\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)\nx_train = tokenizer.texts_to_sequences(x_train)\n\n# {word1: index1, word2: index2}\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n\n\nprint('Preparing embedding matrix.')\n# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NUM_WORDS:\n        continue\n    embedding_vector = word2vec_model[word]\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM,\n                                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                                    mask_zero=True,\n                                    input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.GRU(256, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.GRU(128))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(11, activation='softmax'))\n\n# print(model.layers[0])\n# model.layers[0].trainable = False\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()\n# model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2)\nmodel.fit(x_train, y_train, batch_size=256, epochs=3)\n\n# model save\nmodel.save('gru_word2vec.h5')\n\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n\nresults = model.evaluate(x_test, y_test)\nprint('Test loss:', results[0])\nprint('Test accuracy:', results[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using the whole datasets to train the GRU model again and save the model for deployment","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os\nimport sklearn.metrics as metrics\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 1000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\nprint('load data')\n\nx = np.load('x.npy', allow_pickle=True)\ny = np.load('y.npy', allow_pickle=True)\n\n\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\nx_train, y_train = x, y\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)\n\nfn = 'tokenizer.pkl'\nwith open(fn, 'wb') as f:\n    picklestring = pickle.dump(tokenizer, f)\n\n\nx_train = tokenizer.texts_to_sequences(x_train)\n\n# {word1: index1, word2: index2}\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n\n\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM,\n                                    input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.GRU(256, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.GRU(128))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(11, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()\n\n# model.fit(x_train, y_train, batch_size=128, epochs=10,validation_split=VALIDATION_SPLIT)\nmodel.fit(x_train, y_train, batch_size=128, epochs=5)\n# model save\nmodel.save('gru_model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balanced dataset\n\n<font size=4>After deploying the model and testing some reviews, the model often returns values from 6 to 8. There is few values such as 1,2,3. That is probably because of the unbalanced datasets</font>\n\n<br/>\n\n<font size=4>The following code is to sample 20000 datasets from each class except 0. Because class 0 has only 11 datasets.</font>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import random\nimport numpy as np\nfrom tqdm import tqdm\nimport string\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\n\ndata_frame = pd.read_csv('./boardgamegeek-reviews/bgg-13m-reviews.csv', index_col=0)\ndata_frame.drop(data_frame.columns[4], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[3], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[0], axis=1, inplace=True)\n\n\ndata_frame = data_frame[~data_frame.comment.str.contains(\"NaN\", na=True)]\nprint(data_frame.head())\nprint('data shape: ', data_frame.shape)\n\ndata_frame[\"rating\"] = data_frame[\"rating\"].round(0).astype(int)\n# print(data_frame.groupby([\"rating\"]).count())\n\nrating_subset = data_frame[data_frame['rating'] == 1]\nbalance_df = rating_subset.sample(20000)\n\nfor i in range(9):\n    rating_subset = data_frame[data_frame['rating'] == (i+2)]\n    r = rating_subset.sample(20000)\n    balance_df = balance_df.append(r)\n\nprint(balance_df.groupby([\"rating\"]).count())\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nx = np.array(balance_df.comment)\ny = np.array(balance_df.rating)\n\nall_texts = []\nfor index, text in tqdm(enumerate(x)):\n    # lower case\n    text = text.lower()\n    # tokenize\n    words = word_tokenize(text)\n    # topwords\n    words = [w for w in words if w not in stopwords.words('english')]\n    # remove punctuation\n    words = [w for w in words if w not in string.punctuation]\n    # Stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    all_texts.append(words)\n\nx = np.array(all_texts)\nindex = [i for i in range(len(x))]\nrandom.shuffle(index)\nx = x[index]\ny = y[index]\n\nnp.save('balance_x.npy', x)\nnp.save('balance_y.npy', y)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using the balanced datasets on GRU model\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os\nimport sklearn.metrics as metrics\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 1000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\nprint('load data')\n\nx = np.load('balance_x.npy', allow_pickle=True)\ny = np.load('balance_y.npy', allow_pickle=True)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n# x_train, y_train = x, y\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)\n\nfn = 'balance_tokenizer.pkl'\nwith open(fn, 'wb') as f:\n    picklestring = pickle.dump(tokenizer, f)\n\n\nx_train = tokenizer.texts_to_sequences(x_train)\n\n# {word1: index1, word2: index2}\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM,\n                                    input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.GRU(256, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.GRU(128))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(11, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=5,validation_split=VALIDATION_SPLIT)\n# model.fit(x_train, y_train, batch_size=128, epochs=5)\n# model save\n# model.save('balance_gru_model.h5')\n\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\nresults = model.evaluate(x_test, y_test)\nprint('Test loss:', results[0])\nprint('Test accuracy:', results[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result & Conclusion\n\n<font size=4>The accuracy of GRU on unbalanced datasets is 0.345</font>\n\n<br/>\n\n<font size=4>The accuracy of GRU on balanced datasets is 0.262</font>\n\n<br/>\n\n<font size=4>The accuracy of GRU on unbalanced datasets is higher than the balanced datasets. The reasons are:</font>\n\n<br/>\n\n<font size=4>1. The unbalanced datasets has more datasets than balanced datasets</font>\n\n<br/>\n\n<font size=4>2. GRU on unbalanced datasets probably learned that giving more weight to the rating from 6-8 will get a higher score</font>\n\n<br/>\n\n<font size=4>Both GRU on unbalanced datasets and balanced datasets have been deployed to the demo. Welcome to test!</font>","execution_count":null}],"metadata":{"kernelspec":{"display_name":"tf2.0","language":"python","name":"tf2.0"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"nbformat":4,"nbformat_minor":1}