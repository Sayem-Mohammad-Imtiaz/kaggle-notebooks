{"cells":[{"metadata":{"id":"cdMQx1c5DNdh","colab_type":"text","_uuid":"af69c3468ebbd7f5cdfd676829d075b65abacd9b"},"cell_type":"markdown","source":"\n# How to prove the pretraining work in MNIST dataset?\n\n> In this tutorial, I tried to check the improvement on model performance by pretrained autoencoder when there are lots of unlabeled data but limited labeled data.\n\n\nMethod:\n*   baseline model: \nI used 1000 labeled mnist data to train plain multilayer neural network, and check the validation accuracy by model prediction on the rest 69000 data.\n\n*   autoencoder-pretrained model:\nIn pretraining step, I used 69000 unlabeled mnist data to train an autoencoder in unsupervised way. When the training of autoencoder is completed, encoder part of autoencoder is extracted to compress the input signal. In subsequent, supervised classification step,  1000 labeled data are compressed by encoder, and then fed into plain multineural network as baseline model.\n\nConclusion:\nValidation accuracy of baseline model saturated at around 0.83 after 1000 epoches, while that of autoencoder-pretrained model can reach 0.93. Pretraining works!!\n\n\n\n![](https://drive.google.com/uc?export=view&id=1c_Ut7jJYbYNmko9z8HpEIUgrypKR5HHq)\n\nFuture works:\n\n\n*   Check the effect of encoding dimension(encoding_dim)\n*   Try different pretraining architectures: ex: restricted boltzmann machine(RBM)\n*   Will pretraining work for time series??\n*   Try different image dataset: FashionMNIST and NOTMNIST\n\n\n\nReference:\n\n\n*   This post tell how to classify mnist w/ merely 1000 labels\nhttps://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95\n*  This post tells how to compress signal w/ autoencoder.\nhttps://towardsdatascience.com/unsupervised-learning-of-gaussian-mixture-models-on-a-selu-auto-encoder-not-another-mnist-11fceccc227e\n\n*    This lecture tells why and when pretraining will work.\nHinton coursera Lecture 14 on pretraining\nhttps://d3c33hcgiwev3.cloudfront.net/_4bd9216688e0605b8e05f5533577b3b8_lec14.pdf?Expires=1540425600&Signature=QYnddOB54RDuCJ8ETkAq7xc3E05nUMeFGWtbUsvArIHkE2SVWLfvMe~Qz6ph~LB~HaDfnQz8eaITic-8qqk3CJwXkOyoFKlXdLo8bCddK8C1sr-ASE2WS6kmuyl-oPwuz1oaKuKQkazeUuapPLdV7RpG2X35jVHSt0yRef6JjqM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5\n\n\n\n\n\n"},{"metadata":{"id":"R9gf9ivbqXC6","colab_type":"text","_uuid":"fb1757686e3aee5885fe95eb0fb869ec63874ed7"},"cell_type":"markdown","source":"Load the mnist"},{"metadata":{"id":"4lIH89kiFwy8","colab_type":"code","colab":{},"trusted":true,"_uuid":"fb3c38b7183407e46c6516af1b46c769606b0b53"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom keras.datasets import mnist\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import TensorBoard\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.utils import to_categorical\nimport keras\n\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JONou3G7rKzi","colab_type":"code","colab":{},"trusted":true,"_uuid":"76c8843ea8dd2ca13b2c1583e3335e50e179e0e9"},"cell_type":"code","source":"#### Load the data\nfile = open(\"../input/mnist_train.csv\")\ndata_train = pd.read_csv(file)\n\ny_train = np.array(data_train.iloc[:, 0])\nx_train = np.array(data_train.iloc[:, 1:])\n\nfile = open(\"../input/mnist_test.csv\")\ndata_test = pd.read_csv(file)\ny_test = np.array(data_test.iloc[:, 0])\nx_test = np.array(data_test.iloc[:, 1:])\n\nx_train = x_train.astype('float32')/ 255.\nx_test = x_test.astype('float32')/ 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\nx_all = np.concatenate((x_train,x_test))\ny_all = to_categorical(np.concatenate((y_train,y_test)))\n\nprint('Shape of x_train:',x_train.shape)\nprint('Shape of x_test:', x_test.shape)\nprint('Shape of y_train:', y_train.shape)\nprint('Shape of y_test:',y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3571c519600e9e73c1f8dd89ff4f165dae8e3388"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"g5kOsSTgrsG8","colab_type":"code","colab":{},"trusted":true,"_uuid":"2fd414615bf21164e857735ee78988cb0801932a"},"cell_type":"code","source":"#### Split into train and test set\nn_labeled = 1000\nx_train = x_all[:n_labeled,:]\nx_test = x_all[n_labeled:,:]\ny_train = y_all[:n_labeled,:]\ny_test = y_all[n_labeled:,:]\n\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"ILbVWJiesCxP","colab_type":"code","colab":{},"trusted":true,"_uuid":"f5a3b6ae350b57e0c695fd7d9736baaf0d21601f"},"cell_type":"code","source":"#### Construct neural Architeture for baseline model\ninput_img = Input(shape=(784,))\nd = Dense(20, activation='relu')(input_img)\nd = Dense(10, activation='relu')(d)\noutput = Dense(10, activation='softmax', kernel_regularizer=l2(0.01))(d)\nbaseline = Model(input_img,output)\nbaseline.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"D8h6q0bbskqx","colab_type":"code","colab":{},"trusted":true,"_uuid":"50fc55b3b6dcaa19df8c17d71c714d0d03f9fd68"},"cell_type":"code","source":"#### Train the model\nhistory_baseline = baseline.fit(x_train, y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=2,\n                validation_data=[x_test,y_test])\n\n#### Check the model performance\nscore = baseline.evaluate(x_test, y_test)\nprint ('keras test accuracy score:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2v-tpp00yye7","colab_type":"text","_uuid":"9de499a2b3b2d14b5b1de1c0115cddb23350f767"},"cell_type":"markdown","source":"After 1000 epoches, training accuracy is 100% but test accuracy remains at ~86%. The overfitting w/ too limited data is quite obvious."},{"metadata":{"id":"Uc_mmPR-tIFB","colab_type":"code","colab":{},"trusted":true,"_uuid":"4858df872b4dc17cd3f0accc99e8340a3d4d02ab"},"cell_type":"code","source":"#### Check the model performance\nscore = baseline.evaluate(x_test, y_test)\nprint ('keras test accuracy score:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1SF_9rYYxjUA","colab_type":"text","_uuid":"a8b7b3be52496e3778689672d9cbbac099616019"},"cell_type":"markdown","source":"**Autoencoder-pretrained model:**"},{"metadata":{"id":"QSuZMpph-DaN","colab_type":"code","colab":{},"trusted":true,"_uuid":"68b5b8502a150ed41aa4fd742e0761b45bdfeb14"},"cell_type":"code","source":"#### Construct neural architecture of autoencoder\n'''ref: https://towardsdatascience.com/unsupervised-learning-of-gaussian-\\\nmixture-models-on-a-selu-auto-encoder-not-another-mnist-11fceccc227e'''\n# this is the size of our encoded representations\nencoding_dim = 6  \n# Specify the layer of autoencoder\ninput_img = Input(shape=(784,))\nd = Dense(256, activation='selu')(input_img)\nd = Dense(128, activation='selu')(d)\nencoded = Dense(encoding_dim, activation='selu', kernel_regularizer=l2(0.01))(d)\nd = Dense(128, activation='selu')(encoded)\nd = Dense(256, activation='selu')(d)\ndecoded = Dense(784, activation='sigmoid')(d)\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n# create a placeholder for an encoded (6-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndeco = autoencoder.layers[-3](encoded_input)\ndeco = autoencoder.layers[-2](deco)\ndeco = autoencoder.layers[-1](deco)\n# create the decoder model\ndecoder = Model(encoded_input, deco)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"id":"PNNQvsQm_RAi","colab_type":"code","colab":{},"trusted":true,"_uuid":"3fa52e24fe43e20d64c6117970a7d96bad114570"},"cell_type":"code","source":"#### Train the autoencoder. Note we use test data to train.\nautoencoder.fit(x_test, x_test,\n                epochs=500,\n                batch_size=2000,\n                shuffle=True,\n                verbose=2,\n                validation_data=(x_test, x_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"3Buex0toJAq4","colab_type":"code","colab":{},"trusted":true,"_uuid":"42f5b0daf9c14721a96767df33c6ba276045ed02"},"cell_type":"code","source":"#### Use encoder part of autoencoder to compress signal for supervised training\nx_train_en = encoder.predict(x_train)\nx_test_en = encoder.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"2Gs0s-axJViY","colab_type":"code","colab":{},"trusted":true,"_uuid":"ce555b36aa0a5dadf312a2fd71608adacf8f0f58"},"cell_type":"code","source":"#### Construct neural architecture for autoencoder-pretrained model, same as baseline model.\ninput_encoded_img = Input(shape=(6,))\nd = Dense(20, activation='relu')(input_encoded_img)\nd = Dense(10, activation='relu')(d)\noutput = Dense(10, activation='softmax', kernel_regularizer=l2(0.01))(d)\n\nclassifier = Model(input_encoded_img,output)\nclassifier.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"UwA8bLrSODVO","colab_type":"code","colab":{},"trusted":true,"_uuid":"b344975e694bdac18ef9636e03e0c364f8f5e56a"},"cell_type":"code","source":"#### Train the model\nhistory_pretrained = classifier.fit(x_train_en, y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=2,\n                validation_data=[x_test_en,y_test])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Vw3NdIKzOwVh","colab_type":"code","colab":{},"trusted":true,"_uuid":"e56a5669a0d0a97b5dc854736494598969aa7d17"},"cell_type":"code","source":"#### Check the score \nscore = classifier.evaluate(x_test_en, y_test)\nprint ('keras test accuracy score:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"id":"njAq8zOmTWm-","colab_type":"code","colab":{},"trusted":true,"_uuid":"46ce0a593feff24b2dfc2d03cd7be2b25ea580db"},"cell_type":"code","source":"#### Visualize train history\nplt.plot(history_baseline.history['val_categorical_accuracy'])  \nplt.plot(history_pretrained.history['val_categorical_accuracy'])  \nplt.title('Train History')  \n#plt.ylabel('')  \nplt.xlabel('Epoch')  \nplt.legend(['baseline:val_categorical_accuracy', \n            'pretrained:val_categorical_accuracy'], loc='lower right')  \nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Tutorial: How to prove pretraining works in mnist dataset? .ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}