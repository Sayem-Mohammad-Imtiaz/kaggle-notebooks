{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction of avocado price using neural network\n\n\n## Introduction\nThere is definitely an avocado trend these days! I mean why not, it is healthy. Because of that, it is really interesting to get an idea of the market of avocados.   \nOur goal will be to get a good picture of that market by trying to predict the average price.\n\n## Our method\nUsually, the way I work on this kind of data is pretty iterative, the goal here is to produce a first 'silly' model and then try to improve it, so it can generalize the problem more efficiently.  \nHere we will be using neural network."},{"metadata":{},"cell_type":"markdown","source":"## The Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# seed\nnp.random.seed(1337)\n\n# load avocado file\navocadoCSV = pd.read_csv('/kaggle/input/avocado-prices/avocado.csv', parse_dates=['Date'], dtype={\"region\": \"category\",\"type\": \"category\",\"year\": \"category\"})  \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A bit of data visualization\n\nLet's check the distribution of avocado prices"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-darkgrid')\nfig1 = plt.figure(1, figsize=(14,7))\n\nsns.distplot(avocadoCSV['AveragePrice'],color='b',  axlabel='Average Price')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from there we can see that most of the avocado have prices between 0.7 and 1.7, but we can interpretate the two peaks as the preferred prices, the fact that the prices range in a small interval can also cause some issues for our model. "},{"metadata":{},"cell_type":"markdown","source":"In order to have a more specific idea of the data, we want to have a look at the repartition by type (organic, conventional)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig2 = plt.figure(2, figsize=(5,5))\nsns.boxplot(x=\"type\", y=\"AveragePrice\", data=avocadoCSV, palette=\"Set1\")\n\nfig3 = plt.figure(3, figsize=(8,7))\nsns.boxplot(x=\"type\", y=\"AveragePrice\", hue='year', data=avocadoCSV, palette=\"Set1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Big news! Organic product are generally more expensive :)  \nAlso it seems that the year 2017 is the year where the product became the most expensive, the interesting thing to notice is that the boxplots for conventional avocado prices show less variance than the ones for organic. The way i interpretate that is the 'quality' of an organic product is subject to a lot of natural causes (bad season, different expiration dates by species, etc.) which can strongly affect the price of the avocado.  \n\nAnother important thing is that because we don't have a lot of years in the dataset, it is difficult to extract some trends."},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing  \n\nFirst we load the sample, the goal here is to use the dataset for supervised training in order to predict the average price"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"avocadoCSV = avocadoCSV.sample(frac=1) # randomize sample\navocadoCSV.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"dataset shape = \",avocadoCSV.shape)\navocadoCSV.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we might have some seasonality effect on the average price, it is important to add a new column for the months.  \n  \n  \nAlso you can see that we remove the Total bags column because this information is redundant because of the columns: 'Small Bags', 'Large Bags', 'XLarge Bags'"},{"metadata":{"trusted":true},"cell_type":"code","source":"avocadoCSV['month'] = avocadoCSV['Date'].map(lambda x: x.month)\navocadoCSV['month'] = avocadoCSV['month'].astype('category')\n\n#avocado=avocadoCSV[[ 'month' ,'year', 'region', 'Small Bags', \n#'Large Bags','XLarge Bags', 'type','AveragePrice']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need our model to predict the average price depending on all those features selected, however we need first to convert some those columns to categories, and therefore affect weights to each codes of those categories.  \n\nFor instance we have 2 types of avocados: organic,and conventional; we want instead of one column for the two possible values, two columns, each one of them representing a value.\ntype -> (type1, type2)\n\nWe do the same for month, year, region"},{"metadata":{"trusted":true},"cell_type":"code","source":"def makeAvocadoWithCategory(data, categoryColumns, fieldsToKeep):\n\n\tallFields = categoryColumns + fieldsToKeep\n\tdf = data[allFields]\n\n\tdfCategories = [ pd.get_dummies(df[column], prefix=column) for column in categoryColumns ]\n\tdf = pd.concat([df] + dfCategories, axis=1)\n\tdf = df.drop(columns=categoryColumns)\n\n\treturn df\n\navocado = makeAvocadoWithCategory(\n\tavocadoCSV,\n\t['month' ,'year', 'region','type'],\n\t['Small Bags','Large Bags','XLarge Bags','AveragePrice']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avocado.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avocado.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Neural Network\n\nNow we can make the train and the test sets."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def makeTrainAndTestSet(data):\n\tdataAveragePrice = data['AveragePrice']\n\tdataNoAveragePrice = data.drop(columns=['AveragePrice'])\n\n\tdataTrain = dataNoAveragePrice[:15000]\n\tdataYTrain = dataAveragePrice[:15000]\n\n\n\tdataTest = dataNoAveragePrice[15001:]\n\tdataYTest = dataAveragePrice[15001:]\n\treturn dataTrain, dataYTrain, dataTest,dataYTest\n\navocadoTrain, avocadoYTrain, avocadoTest, avocadoYTest = makeTrainAndTestSet(avocado)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avocadoTrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\n\nmodel = Sequential()\nmodel.add(Dense(4, activation='relu', input_dim=75))\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile('adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model.fit(avocadoTrain, avocadoYTrain, epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def makePredictSummary(modelTo, XtestData, YtestData):\n\tpred = modelTo.predict(XtestData).T[0]\n\treal = YtestData.values\n\t# compute relative error\n\terr = np.abs((real - pred) / real)\n\tpredictionSummary = pd.DataFrame({'real': real, 'pred': pred, 'err(%)': err})\n\n\treturn predictionSummary\n\nsummary = makePredictSummary(model, avocadoTest, avocadoYTest)\nsummary[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning the model\n### First remarks\n\n\n1. The model can return negative values\n2. Because the values for the prices are so close, the model have difficulties to differentiate avocados sometimes\n3. If u rerun the model u'll see that the result are not stable\n * The model is very sensitive to the number of epochs\n * With different epoch u can see that at some point the model will just return constant values\n4. Some features need to be adjusted\n * this is the case for the region features, we need a more abstract representation of that otherwise the model will overfit\n * This one is a bit tricky, but the year is not something to change to be a categorical variable, what we want instead is to get an idea of the trend of the current year (see [ARIMA](http://https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model) or any temporal serie analysis method) - honestly, i don't know if it's possible for that one :)\n 5. the data doesn't have a lot of rows\n \n### Change the model\n\nFirst we're going to improve the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization\n\nmodel2 = Sequential()\nmodel2.add(Dense(6, activation='relu', input_dim=75))\nmodel2.add(Dense(6, activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dense(10, activation='relu'))\nmodel2.add(Dropout(0.25))\nmodel2.add(Dense(16, activation='elu'))\nmodel2.add(Dense(10, activation='elu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(6, activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dense(4, activation='relu'))\nmodel2.add(Dense(4, activation='relu'))\nmodel2.add(Dense(1))\n\nmodel2.compile(\"adam\", loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(avocadoTrain, avocadoYTrain, epochs=20, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so my idea there is to add more layers, with more neurons. Because i often noticed that my gradient flow get killed, i had to:\n1. Change the relu in the deep layers to be elu\n * This allow me to have an extended non saturation regime compared to relu\n * I also have the intuition that in the deep layers you want the gradient to be *passed* whereas in the first or the last layers you want to *summarize* the information, that's why u need activation functions like relu, that will *cut* the space, and cut the gradient flow with their saturation regime\n2. I added Dropouts to prevent from overfitting, and batch normalization to stabilize the model  \n\n\nI changed the batch size to get a more accurate batch stochastic gradient, at each step, which is important given the size of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary2 = makePredictSummary(model2, avocadoTest, avocadoYTest)\nsummary2[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because the average prices are really close, I suggest to predict \n$$expAveragePrice = 3^{AveragePrice}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"avocadoYTrainExp = np.power(3, avocadoYTrain)\navocadoYTestExp = np.power(3, avocadoYTest)\nhistoryExp2 = model2.fit(avocadoTrain, avocadoYTrainExp, epochs=20, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summaryExp = makePredictSummary(model2, avocadoTest, avocadoYTestExp)\nsummaryExp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n### Learning rate\n\nThen we are going to fix the learning rate.  \nIn the case of the first model you can see the overfitting\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"plt.figure(4, figsize = (7,4))\n\nplt.plot(history1.history['loss'], '-p', markersize=6, linewidth=2)\nplt.title('First Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['learning rate 0.001'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the loss function is dropping too fast! \nLet's see the evolution with our second model and different learning rates"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras import optimizers\n\n\nadam2 = optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=adam2, loss='mean_squared_error')\nhistoryExp2_2 = model.fit(avocadoTrain,  avocadoYTrainExp, epochs=20, batch_size=64, verbose=0)\n\nadam3 = optimizers.Adam(learning_rate=0.0005)\nmodel.compile(optimizer=adam3, loss='mean_squared_error')\nhistoryExp2_3 = model.fit(avocadoTrain,  avocadoYTrainExp, epochs=20, batch_size=64, verbose=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(5, figsize = (9,4))\nplt.plot(historyExp2_2.history['loss'], '-p', markersize=6, linewidth=2)\nplt.plot(historyExp2_3.history['loss'], '-p', markersize=6, linewidth=2)\nplt.title('Second Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['0.0001', '0.0005'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the two curve we can see that a good learning rate would be between 0.0001 and 0.0005, let's choose 0.0003 with  20 epochs.  \n\n## Remove the year column (TBC)\n\nAs I said earlier, putting year as a category is not the best thing to do, instead we want to incorporate the trend in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the year column as a category\n\navocadoNew = makeAvocadoWithCategory(\n\tavocadoCSV,\n\t['month' ,'region','type'],\n\t['Small Bags','Large Bags','XLarge Bags','AveragePrice', 'year']\n)\n\navocadoNewTrain, avocadoNewYTrain, avocadoNewTest, avocadoNewYTest = makeTrainAndTestSet(avocado)\n\navocadoNewYTrain = np.power(3, avocadoNewYTrain)\navocadoNewYTest = np.power(3, avocadoNewYTest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our new model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(Dense(6, activation='relu', input_dim=75))\nmodel3.add(Dense(6, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dense(10, activation='relu'))\nmodel3.add(Dropout(0.25))\nmodel3.add(Dense(16, activation='elu'))\nmodel3.add(Dense(10, activation='elu'))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(6, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dense(4, activation='relu'))\nmodel3.add(Dense(4, activation='relu'))\nmodel3.add(Dense(1))\n\nadam = optimizers.Adam(learning_rate=0.0003) #new learning rate\nmodel3.compile(adam, loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Need to be improved\n\nAdd new features (regional features)  \nIncoroporate a trend year feature  \nCheck for R-NNN to get an ARIMA-like behavior in the model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}