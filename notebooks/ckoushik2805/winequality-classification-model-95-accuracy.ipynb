{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### I will be using Random Forest Classifier to predict the quality of wine, which is the target variable","metadata":{}},{"cell_type":"markdown","source":"### Acknowledgement","metadata":{}},{"cell_type":"markdown","source":"Before I begin, my work here is an improvement to an existing approach. So, credits to https://www.kaggle.com/taha07/wine-quality-prediction-data-analysis for his accuracy of 93%","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now I will be taking you through the steps","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/wine-quality/winequalityN.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning and Preprocessing\n\nBefore performing any analysis on data, it's important to deal with null values as they are prone to major errors and inconsistencies","metadata":{}},{"cell_type":"markdown","source":"Now we check for total no. of null values in each column","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are null values present in the following columns:\n'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'pH', 'sulphates'","metadata":{}},{"cell_type":"markdown","source":"For each column, instead of dropping rows with null values, I will instead be replacing them with either median or mean. Simply dropping the rows will considerably reduce the size of the dataset and hence might degrade performance of the models","metadata":{}},{"cell_type":"code","source":"#Replacing null values in fixed acidity with median\ndf['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf['fixed acidity'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in volatile acidity with mean\ndf['volatile acidity'].fillna(df['volatile acidity'].mean(), inplace=True)\ndf['volatile acidity'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in citric acid with mean\ndf['citric acid'].fillna(df['citric acid'].mean(), inplace=True)\ndf['citric acid'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in residual sugar with mean\ndf['residual sugar'].fillna(df['residual sugar'].mean(), inplace=True)\ndf['residual sugar'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in chlorides with median\ndf['chlorides'].fillna(df['chlorides'].median(), inplace=True)\ndf['chlorides'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in pH with mean\ndf['pH'].fillna(df['pH'].mean(), inplace=True)\ndf['pH'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing null values in sulphates with median\ndf['sulphates'].fillna(df['sulphates'].median(), inplace=True)\ndf['sulphates'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No more null values.","metadata":{}},{"cell_type":"markdown","source":"Now, since we're predicting the target variable quality, we'll have to categorize the numbers into low, medium and high and then encode it to 0,1 and 2 for classification","metadata":{}},{"cell_type":"code","source":"df['quality'].min()\ndf['quality'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mapping values of target variable quality to 'low', 'medium' and 'high' categories for classification\ndf['quality']=df['quality'].map({3:'low', 4:'low', 5:'medium', 6:'medium', 7:'medium', 8:'high', 9:'high'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['quality']=df['quality'].map({'low':0,'medium':1,'high':2})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removal of Outliers","metadata":{}},{"cell_type":"markdown","source":"Outliers are extreme cases of data that may severely affect the prediction capailities of the machine learning models. Therefore, its critical that we remove them.","metadata":{}},{"cell_type":"markdown","source":"I will now be plotting a boxplot to view the general distribution of data across all features to check for outliers.","metadata":{}},{"cell_type":"code","source":"sn.set()\nplt.figure(figsize=(30,15))\nsn.boxplot(data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,3)\nplt.subplots_adjust(right=2.5, top=1.5)\nsn.boxplot(df['residual sugar'], df['type'], ax=ax[0])\nsn.boxplot(df['free sulfur dioxide'], df['type'], ax=ax[1])\nsn.boxplot(df['total sulfur dioxide'], df['type'], ax=ax[2])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In these three columns we can notice significant outliers. Therefore, they must be removed from the respective columns.","metadata":{}},{"cell_type":"code","source":"#Removing outliers in residual sugar\nlower = df['residual sugar'].mean()-3*df['residual sugar'].std()\nupper = df['residual sugar'].mean()+3*df['residual sugar'].std()\ndf = df[(df['residual sugar']>lower) & (df['residual sugar']<upper)]\n\n#Removing outliers in free sulfur dioxide\nlower = df['free sulfur dioxide'].mean()-3*df['free sulfur dioxide'].std()\nupper = df['free sulfur dioxide'].mean()+3*df['free sulfur dioxide'].std()\ndf = df[(df['free sulfur dioxide']>lower) & (df['free sulfur dioxide']<upper)]\n\n#Removing outliers in total sulfur dioxide\nlower = df['total sulfur dioxide'].mean()-3*df['total sulfur dioxide'].std()\nupper = df['total sulfur dioxide'].mean()+3*df['total sulfur dioxide'].std()\ndf = df[(df['total sulfur dioxide']>lower) & (df['total sulfur dioxide']<upper)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-Hot encoding","metadata":{}},{"cell_type":"markdown","source":"The 'type' column must be 1-hot encoded for classification. 1-hot encoding creates a binary column for each category. Here we use pd.get_dummies() to remove the first category and essentially bring it to one column of 1's and 0's where 1 denotes white wine and 0 denotes not white (red wine).","metadata":{}},{"cell_type":"code","source":"dummies = pd.get_dummies(df['type'], drop_first=True)\ndf = pd.concat([df, dummies], axis=1)\ndf.drop('type', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation between features","metadata":{}},{"cell_type":"code","source":"#Checking relationship between features\ncor=df.corr()\nplt.figure(figsize=(20,10))\nsn.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True)\ncor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test split\n\nI will be splitting the dataset into training and testing sets in the ratio of 0.80:0.20","metadata":{}},{"cell_type":"code","source":"X = df.loc[:,df.columns!='quality']\ny = df['quality']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Fitting","metadata":{}},{"cell_type":"markdown","source":"I will be fitting the sklearn's <b>RandomForestClassifier</b> model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nprint(rfc.get_params())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the default parameters used by the classifier","metadata":{}},{"cell_type":"code","source":"# Fit the model\nrfc.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=rfc.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have achieved an accuracy score of 0.946 (94.6%). That's great! But the performance can further be enhanced by tuning the parameters.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"I will now use RandomizedSearchCV for searching over and performing 3-fold cross validation on the grid of parameters that can be used for the Random Forest model for this dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=90, stop=200, num=12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start=10, stop=110, num=11)]\nmax_depth.append(None)\nmin_samples_split=[2, 5, 10]\nmin_samples_leaf=[1, 2, 4]\nbootstrap=[True, False]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search_grid = {'n_estimators': n_estimators,\n                      'max_features': max_features,\n                      'max_depth': max_depth,\n                      'min_samples_split': min_samples_split,\n                      'min_samples_leaf': min_samples_leaf,\n                      'bootstrap': bootstrap}\nprint(random_search_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc=RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator=rfc, param_distributions = random_search_grid, n_iter=100, \n                          cv=3, verbose=2, random_state=0, n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now get the best set of parameters from the function the evaluated grid.","metadata":{}},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's fit the model once again with the updated parameters.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=90, min_samples_split=2, min_samples_leaf=1, \n                             max_features='auto', max_depth=50, bootstrap=True,\n                             random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=rfc.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 94.71%","metadata":{}},{"cell_type":"markdown","source":"<b>There seems to be a slight improvement. Nevertheless, we get around 95% accuracy, which is awesome!</b>\n\nNow I will be showing the confusion matrix and the classification report.","metadata":{}},{"cell_type":"code","source":"print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint('\\nClassification Report:\\n', classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I hope you found this useful :)","metadata":{}}]}