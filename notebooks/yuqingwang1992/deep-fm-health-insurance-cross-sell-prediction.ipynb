{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# kgg problem: https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Embedding, Dense, Activation, Layer, Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# config\nFEATURES = {\n    \"Gender\": 0,\n    \"Age\": 1,\n    \"Driving_License\": 2,\n    \"Region_Code\": 3,\n    \"Previously_Insured\": 4,\n    \"Vehicle_Age\": 5,\n    \"Vehicle_Damage\": 6,\n    \"Annual_Premium\": 7,\n    \"Policy_Sales_Channel\": 8,\n    \"Vintage\": 9\n}\nDIM = 128\nFEATURE_NAMES = list(FEATURES.keys())\nFEATURE_SIZES = [] # need compute after features encoding\nDEEP_DENSE_OUT_DIM = [128, 64, 32, 1]\nBATCH_SIZE = 32\n\nLR = 0.001\noptimizer = tf.keras.optimizers.Adam(LR)\nmetric = tf.keras.metrics.Accuracy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DeepFM model\nclass SingleBiasLayer(Layer):\n    def __init__(self, *args, **kwargs):\n        super(SingleBiasLayer, self).__init__(*args, **kwargs)\n\n    def build(self, units):\n        # one single bias value add to batch so shape is (1, )\n        self.bias = self.add_weight('bias',\n                                    shape=(1, ),\n                                    initializer='random_normal',\n                                    trainable=True)\n                                \n    def call(self, X):\n        \"\"\"\n        Add the same single bias value to each x\n            @X: (batch_size, )\n        \"\"\"\n        return X + self.bias\n\n\nclass DeepFM(keras.Model):\n    def __init__(self, dim, feature_names, feature_sizes, batch_size, deep_dense_out_dim):\n        super(DeepFM, self).__init__()\n        self.dim = dim\n        self.feature_names = feature_names\n        self.feature_sizes = feature_sizes\n        self.batch_size = batch_size\n        self.init_fm_part()\n        self.deep_dense_out_dim = deep_dense_out_dim\n        self.init_deep_part()\n\n    def init_features_embeds(self):\n        self.features_embeds = []\n        for feature_name, feature_size in zip(self.feature_names, self.feature_sizes):\n            # feature_size+1 for randomly weight representing unseen feature in train\n            # unseen value for one certrain feature in test are all encoded as `feature_size`\n            embed_lookup = Embedding(feature_size+1, self.dim, name=\"embed\"+feature_name)\n            self.features_embeds.append(embed_lookup)\n    \n    def init_fm_dense(self):\n        \"\"\"\n            Using 1-dim value embed lookup to represent one-hot vector dot weight vector\n        \"\"\"\n        self.fm_dense = []\n        for feature_name, feature_size in zip(self.feature_names, self.feature_sizes):\n            # feature_size+1 for randomly embedding representing unseen feature in train\n            # unseen value for one certrain feature in test are all encoded as number: `feature_size`\n            dense_weight_lookup = Embedding(feature_size+1, 1, name=\"dense\"+feature_name)\n            self.fm_dense.append(dense_weight_lookup)\n        self.fm_bias = SingleBiasLayer()\n\n    def init_fm_part(self):\n        self.init_features_embeds()\n        self.init_fm_dense()\n\n    def init_deep_part(self):\n        self.flatten = Flatten()\n        self.deep_dense_layers = []\n        # first layer's input size is dim*n_fea, last layer's output size is 1\n        for i, output_dim in enumerate(self.deep_dense_out_dim):\n            layer = Dense(units=output_dim,\n                          activation=\"relu\",\n                          use_bias=True,\n                          name=\"deep_dense_\"+str(i))\n            self.deep_dense_layers.append(layer)\n\n    def fm_part(self, X):\n        \"\"\"\n            @X: (batch_size, n_fea)\n        \"\"\"\n        # 1-order part (dense and bias):\n        one_order = []\n        for i in range(X.shape[1]):\n            feature_w_lookup = self.fm_dense[i]\n            feature_batch_w = feature_w_lookup(X[:, i]) # (batch_size, 1)\n            one_order.append(feature_batch_w)\n        # (n_fea, batch_size, 1) --stack--> (batch_size, n_fea, 1) --sum&squeeze--> (batch_size, )\n        one_order = tf.squeeze(tf.reduce_sum(tf.stack(one_order, axis=1), axis=1))\n\n        # 2-order part:\n        embeds = []\n        for i in range(len(self.feature_names)):\n            feature_embed = self.features_embeds[i]\n            batch_embeds = feature_embed(X[:, i]) # (batch_size, dim)\n            embeds.append(batch_embeds)\n        # (n_fea, batch_size, dim) -> (batch_size, n_fea, dim)\n        embeds = tf.stack(embeds, axis=1)\n        # feature embeddings crossing dot product\n        #two_order = tf.zeros(self.batch_size) # don't use this, for reminder may not be of shape (self.batch_size, )\n        two_order = tf.zeros(X.shape[0])\n        for i in range(len(self.feature_names)):\n            for j in range(i+1, len(self.feature_names)):\n                # (batch_size, dim)\n                V_i, V_j = embeds[:, i, :], embeds[:, j, :]\n                # perform batch dot operation between V_i and V_j:\n                # (batch_size, 1, dim).dot((batch_size, dim, 1)) --squeeze--> (batch_size, )\n                batch_dot = tf.squeeze(tf.matmul(tf.expand_dims(V_i, 1), \n                                                 tf.expand_dims(V_j, -1)))\n                two_order += batch_dot\n\n        return self.fm_bias(one_order + two_order) # (batch_size, )\n\n    def deep_part(self, X):\n        \"\"\"\n            @X: (batch_size, n_fea)\n        \"\"\"\n        embeds = []\n        for i in range(X.shape[1]):\n            feature_embed = self.features_embeds[i]\n            batch_embeds = feature_embed(X[:, i])\n            embeds.append(batch_embeds)\n        # stack to (n_fea, batch_size, dim) then concat(flat) to (batch_size, n_fea*dim)\n        layer_out = self.flatten(tf.stack(embeds, axis=1))\n        # feed into dense layers\n        for i, layer in enumerate(self.deep_dense_layers):\n            layer_out = layer(layer_out)\n\n        return tf.squeeze(layer_out) # (batch_size, )\n\n    def call(self, X):\n        \"\"\"\n        Forward function for training\n            X: (batch_size, n_fea)\n        \"\"\"\n        # (batch_size, )\n        return tf.sigmoid(self.fm_part(X) + self.deep_part(X))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training functions\n@tf.function\ndef train_step(X, Y, model):\n    with tf.GradientTape() as tape:\n        batch_pred = model(X)\n        loss = tf.losses.binary_crossentropy(Y, batch_pred)\n\n    batch_loss = (loss / X.shape[0])\n\n    variables = model.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss\n\n\ndef train_fm_model(model, dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        total_loss, total_acc, steps = 0, 0, 0\n\n        for (batch_index, (X, Y)) in enumerate(dataset):\n            batch_loss = train_step(X, Y, model)\n            total_loss += batch_loss\n            steps += 1\n\n            if steps % 1000 == 0:\n                print('Epoch {} Batch {} Loss {:.4f}'.\\\n                    format(epoch + 1,\n                           batch_index,\n                           batch_loss))\n\n        print('====== Epoch {} Loss {:.4f} ======'.format(epoch + 1, total_loss / steps))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bucketize Annual_Premium\ntrain_samples = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/train.csv\")\nannual_premium = train_samples[\"Annual_Premium\"].values\npercentiles = []\nfor p in np.arange(0, 100, 0.25):\n    percentiles.append(np.percentile(annual_premium, p))\n\ndef bucketize(x, percentiles):\n    return np.searchsorted(percentiles, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"raw Annual_Premium\")\nprint(len(pd.unique(train_samples[\"Annual_Premium\"])))\nhist(train_samples[\"Annual_Premium\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_samples[\"Annual_Premium\"] = train_samples[\"Annual_Premium\"].apply(bucketize, args=(percentiles, ))\nprint(\"after bucketize\")\nprint(len(pd.unique(train_samples[\"Annual_Premium\"])))\nhist(train_samples[\"Annual_Premium\"])\n\ntest_samples = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/test.csv\")\ntest_samples[\"Annual_Premium\"] = test_samples[\"Annual_Premium\"].apply(bucketize, args=(percentiles, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature encoding functions\ndef prepare_encoder(train_samples):\n    features_value_encoder = [{} for _ in FEATURES.keys()]\n\n    for i in range(len(features_value_encoder)):\n        encode_value = 0\n        _field_value_encoder = features_value_encoder[i]\n\n        for sample in train_samples.itertuples(index=False):\n            field_value = sample[i]\n            if field_value in _field_value_encoder:\n                continue\n            else:\n                _field_value_encoder[field_value] = encode_value\n                encode_value += 1\n\n    # set unseen feature value's encode value\n    for feature_value_encoder in features_value_encoder:\n        unseen_value = max(feature_value_encoder.values()) + 1\n        feature_value_encoder[\"unseen\"] = unseen_value\n\n    return features_value_encoder\n\ndef encode_train_samples(train_samples, encoder):\n    encoded_train_samples = []\n    for sample in train_samples.itertuples(index=False):\n        _sample = []\n        for i in range(len(sample)-1):\n            encoded_value = encoder[i][sample[i]]\n            _sample.append(encoded_value)\n        encoded_train_samples.append(_sample)\n\n    return encoded_train_samples\n\ndef encode_test_samples(test_samples, encoder):\n    encoded_test_samples = []\n    for sample in test_samples.itertuples(index=False):\n        _sample = []\n        for i in range(len(sample)):\n            feature_encoder = encoder[i]\n            try:\n                encoded_value = feature_encoder[sample[i]]\n            except KeyError:\n                # encode feature value not seen in train set as max feature encode value + 1\n                print(\"feature {} value {} not seen in training set\".format(i, sample[i]))\n                encoded_value = feature_encoder[\"unseen\"]\n            _sample.append(encoded_value)\n        encoded_test_samples.append(_sample)\n\n    return encoded_test_samples\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode train samples\ndel train_samples[\"id\"]\nencoder = prepare_encoder(train_samples)\nX_encoded = encode_train_samples(train_samples, encoder)\nY = train_samples[\"Response\"].tolist()\n# compute feature_sizes\nfor feature_encoder in encoder:\n    FEATURE_SIZES.append(len(feature_encoder))\n# prepare train dataset\ndataset = tf.data.Dataset.from_tensor_slices((X_encoded, Y))\ndataset = dataset.batch(BATCH_SIZE).shuffle(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode test samples\ndel test_samples[\"id\"]\nX_encoded_test = encode_test_samples(test_samples, encoder)\n# prepare test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(X_encoded_test)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nmodel = DeepFM(DIM, FEATURE_NAMES, FEATURE_SIZES, BATCH_SIZE, DEEP_DENSE_OUT_DIM)\nmodel = train_fm_model(model, dataset, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor X in test_dataset:\n    batch_pred = model(X)\n    for pred in batch_pred:\n        preds.append(pred)\npreds = [x.numpy() for x in preds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write submission file\nIDs = []\nwith open(\"../input/health-insurance-cross-sell-prediction/test.csv\", \"r\") as f:\n    first = True\n    for line in f.readlines():\n        if first:\n            first = False\n            continue\n        IDs.append(line.split(\",\")[0])\n\nlines = [\"id,Response\\n\"]\nassert len(IDs) == len(preds)\nfor ID, pred in zip(IDs, preds):\n    lines.append(ID + \",\" + str(pred) + \"\\n\")\nwith open(\"submission.csv\", \"w\") as f:\n    f.writelines(lines)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}