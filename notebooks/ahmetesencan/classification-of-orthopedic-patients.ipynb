{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2>Necessary libraries<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.transforms as transforms\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Data Analysis and Visualizations*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Let's check if there are null values or not. <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>There is no any null value.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Now, let's get some visualizations about target column which is class. <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df[\"class\"].unique().tolist()\nsizes = df[\"class\"].value_counts().tolist()\ncolors = [\"#C95555\", \"#D8AFAF\"]\nexplode = (0, 0)\nfig, ax = plt.subplots(1,2, figsize=(14,6))\nsns.countplot(df[\"class\"], palette=\"Oranges\", ax=ax[0])\nax[0].set_title(\"Distribution of Patients\", size=28, fontweight=\"bold\")\nax[0].set_xlabel(\"Class\", size=18, fontweight=\"bold\")\nax[0].set_ylabel(\"Count\", size=18, fontweight=\"bold\")\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, textprops={'fontsize': 14, \"fontweight\" : \"bold\"}, colors=colors)\nplt.title(\"Distribution of Patients\", size=28, fontweight=\"bold\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>It's clearly seen that we have 67.7% abnormal patients and 32.3% normal Patients.<h2>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Let's try to understand the relations between feature columns by visualizing a correlation matrix.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\ndf_corr = df.corr()\nsns.heatmap(df_corr, annot=True, cmap=\"YlGn\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> From the matrix, we can see that there are highly positive correlations between Pelvic Incidence and Sacral Slope,  as well as, between Pelvic Incidence and Lumbar Lordosis Angle.<h2>"},{"metadata":{},"cell_type":"markdown","source":"<h2> Let's visualize some of those correlations! <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.scatterplot(df[\"pelvic_incidence\"], df[\"sacral_slope\"], hue=df[\"class\"], palette=\"deep\", s=100)\nplt.ylabel(\"Sacral Slope\", fontsize=15, fontweight=\"bold\")\nplt.xlabel(\"Pelvic Incidence\", fontsize=15, fontweight=\"bold\")\nplt.title(\"Distribution of Patients with Respect to Pelvic Incidence \\nAnd Sacral Slope\", fontsize=22, fontweight=\"bold\")\nplt.legend(prop={\"size\":15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.scatterplot(df[\"pelvic_radius\"], df[\"sacral_slope\"], hue=df[\"class\"], palette=\"cubehelix\", s=100)\nplt.ylabel(\"Sacral Slope\", fontsize=15, fontweight=\"bold\")\nplt.xlabel(\"Pelvic Radius\", fontsize=15, fontweight=\"bold\")\nplt.title(\"Distribution of Patients with Respect to Pelvic Radius \\nAnd Sacral Slope\", fontsize=22, fontweight=\"bold\")\nplt.legend(prop={\"size\":15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue='class', height=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> To understand more about the data and compare the patients with respect to their orthopedic features, let's get the average values for both abnormal patients and normal patients. <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grouped = df.groupby(\"class\").agg(\"mean\")\ndf_grouped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grouped.plot.bar(rot=0, figsize=(16,8))\nplt.title(\"Comparison of Abnormal and Normal Patients \\n(Average Values)\", fontsize=22, fontweight=\"bold\")\nplt.xlabel(\"Patient\", fontsize=15, fontweight=\"bold\")\nplt.grid()\nplt.legend(prop={\"size\":12})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>We can see that the average degree spondylolisthesis value in abnormal patients is pretty higher than average the degree spondylolisthesis value in normal patients.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grouped.plot.bar(rot=0, figsize=(16,8), subplots=True, layout=(3,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Let's take a glance at the distribution of each feature for each class.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_normal = df[df[\"class\"] == \"Normal\"]\ndf_abnormal = df[df[\"class\"] == \"Abnormal\"]\nnormal_df = df_normal.drop(\"class\", axis=1)\nabnormal_df = df_abnormal.drop(\"class\", axis=1)\nnormal_columns = normal_df.columns.tolist()\nabnormal_columns = abnormal_df.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3,2, figsize=(20,12))\nfig.suptitle('Distribution Plots of Biomechanical Attributes for Normal Patients', \n             fontsize=25, fontweight=\"bold\")\nax_iter = iter(axs.flat)\nfor columns in normal_columns:\n    ax = next(ax_iter)\n    sns.distplot(df_normal[columns], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3,2, figsize=(20,12))\nfig.suptitle('Distribution Plots of Biomechanical Attributes for Abnormal Patients', \n             fontsize=25, fontweight=\"bold\")\nax_iter = iter(axs.flat)\nfor columns in abnormal_columns:\n    ax = next(ax_iter)\n    sns.distplot(df_abnormal[columns], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Machine Learning Algorithms***"},{"metadata":{},"cell_type":"markdown","source":"<h2> Classification <h2>"},{"metadata":{},"cell_type":"markdown","source":"<h2> In this section, I am going to create some machine learning models by using KNN (K-Nearest Neighbors), Logistic Regression, Decision Tree Classifier, Random Forest Classifier and SVM (Support Vector Machines) algorithms. <h2>  "},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict = {}\ndf[\"class\"].replace({\"Abnormal\": 1, \"Normal\": 0}, inplace=True)\nx = df.drop(\"class\", axis=1)\ny = df[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *KNN (K-Nearest Neighbors)*"},{"metadata":{},"cell_type":"markdown","source":"<h2>Firstly, let's create a loop and try to find the best k value to reach the highest accuracy score.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbors_list = list(range(3,20,2))\nknn_score_list = []\n\nfor number in neighbors_list:\n    knn = KNeighborsClassifier(n_neighbors=number)\n    knn.fit(X_train, y_train)\n    y_predict_knn = knn.predict(X_test)\n    knn_score_list.append(accuracy_score(y_test, y_predict_knn))\n    \nfig, ax = plt.subplots(1,1, figsize=(10,6))\nplt.plot(neighbors_list, knn_score_list, marker=\"o\", markerfacecolor=\"red\", markersize=8)\nplt.xticks(np.arange(3, 20, 2))\nplt.xlabel(\"k value\", size=12)\nplt.ylabel(\"Accuracy Score\", size=12)\nax.axhline(y = max(knn_score_list) , linewidth = 1.5, color = \"red\", linestyle=\"dashed\")\ntrans = transforms.blended_transform_factory(\n    ax.get_yticklabels()[0].get_transform(), ax.transData)\nax.text(0, max(knn_score_list), \"{:.4f}\".format(max(knn_score_list)), color=\"red\", transform=trans, \n        ha=\"right\", va=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>It looks like we get the highest accuracy score for k value = 5 which is 90.29%.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict[\"KNN\"] = knn.score(X_test, y_test)\ny_predict_knn = knn.predict(X_test)\nknn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Now, let's take a glance at the confusion matrix. <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_knn = confusion_matrix(y_test, y_predict_knn)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_knn, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Let's check the cross validation score. <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=knn, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>It's 80.62 %. Not bad. But, is it possible to increase the cross validation score?<h2>"},{"metadata":{},"cell_type":"markdown","source":"<h2>We can use GridSearchCV method to tune hyperparameters for the best CV score. In the end, we can select the best parameters from the listed hyperparameters. For this model, my hyperparameter for tuning is n_neighbors number.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors': np.arange(1,20)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameter: {}\".format(knn_gscv.best_params_)) \nprint(\"Best score: {}\".format(knn_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>As it's seen, n_neighbors = 15 gives the best CV score which is 83.05%.<h2>"},{"metadata":{},"cell_type":"markdown","source":"# *Logistic Regression*"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C = 0.1)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict[\"Logistic Regression\"] = lr.score(X_test, y_test)\ny_predict_lr = lr.predict(X_test)\nlr.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_lr = confusion_matrix(y_test, y_predict_lr)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_lr, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=lr, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nlr_gscv = GridSearchCV(lr, param_grid, cv=5)\nlr_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameters {}\".format(lr_gscv.best_params_)) \nprint(\"Best score: {}\".format(lr_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Decision Tree Classifier*"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, min_samples_split = 50, random_state=10)\ndtc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict[\"Decision Tree Classifier\"] = dtc.score(X_test, y_test)\ny_predict_dtc = dtc.predict(X_test)\ndtc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_dtc = confusion_matrix(y_test, y_predict_dtc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_dtc, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_dtc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=dtc, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"criterion\" : ['gini', 'entropy'], \"max_depth\" : np.arange(2,21,2), \n              'min_samples_split' : np.arange(10,200,10)}\ndtc_gscv = GridSearchCV(dtc, param_grid, cv=5)\ndtc_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameters {}\".format(dtc_gscv.best_params_)) \nprint(\"Best score: {}\".format(dtc_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Random Forest Classifier*"},{"metadata":{},"cell_type":"markdown","source":"<h2>Let's create a random forest model with all randomly selected hyperparameters.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=10)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=rfc, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': np.arange(100,500,50), 'max_depth' : [4,5,6,7,8], \n              'criterion' :['gini', 'entropy']}\nrfc_gscv = GridSearchCV(rfc, param_grid, cv=5)\nrfc_gscv.fit(X_train,y_train)\nprint(\"Tuned hyperparameters {}\".format(rfc_gscv.best_params_)) \nprint(\"Best score: {}\".format(rfc_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Now, we can create a new model with tuned hyperparameters.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(criterion = 'gini', max_depth = 4, n_estimators = 200, random_state=10)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict[\"Random Forest Classifier\"] = rfc.score(X_test, y_test)\ny_predict_rfc = rfc.predict(X_test)\nrfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=rfc, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Cross validation score has been increased from 80.61% to 83.04%. That's good! <h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_rfc = confusion_matrix(y_test, y_predict_rfc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_rfc, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *SVM*"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(kernel=\"linear\", probability=True)\nsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict[\"SVM\"] = svm.score(X_test, y_test)\ny_predict_svm = svm.predict(X_test)\nsvm.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(estimator=svm, X = X_train, y = y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_svm = confusion_matrix(y_test, y_predict_svm)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_svm, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_svm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Comparison*"},{"metadata":{},"cell_type":"markdown","source":"<h2>Now, let's compare these models.<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dict","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"models = score_dict.keys()\nscores = score_dict.values()\n\nplt.figure(figsize=(16,6))\nplt.bar(models, scores, color=\"#A67EB0\")\nplt.yticks(np.arange(0, 1.05, 0.05))\nplt.xlabel(\"Model\", fontsize=15)\nplt.ylabel(\"Accuracy Score\", fontsize=15)\nfor i, v in enumerate(score_dict.values()):\n    plt.text(i-0.1, v+0.03, \"{:.4f}\".format(v), color='black', va='center', fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Checking the ROC curves and AUCs for each model. Higher the AUC, better the model is at predicting normals as normals and abnormals as abnormals.<h2>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"prob_knn = knn.predict_proba(X_test)[:,1]\nprob_lr = lr.predict_proba(X_test)[:,1]\nprob_dtc = dtc.predict_proba(X_test)[:,1]\nprob_rfc = rfc.predict_proba(X_test)[:,1]\nprob_svm = svm.predict_proba(X_test)[:,1]\n\nprob_dict = {\"ROC KNN\": prob_knn, \"ROC LR\": prob_lr, \"ROC DTC\": prob_dtc, \n             \"ROC RFC\": prob_rfc, \"ROC SVM\": prob_svm}\n\nfor model, prob in prob_dict.items():\n    fpr, tpr, threshold = metrics.roc_curve(y_test, prob)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, color = \"b\", label = \"AUC = %0.2f\" %roc_auc)\n    plt.legend(loc=\"lower right\", prop={\"size\":15})\n    #plt.xlim([-0.005,1])\n    #plt.ylim([0,1.015])\n    plt.xlabel(\"False Positive Rate\", size=12)\n    plt.ylabel(\"True Positive Rate\", size=12)\n    plt.plot([0,1], [0,1], \"r--\")\n    plt.title(str(model), size=20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}