{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n* This is the demo notebook on how to use the [Cassava Trained Models](http://www.kaggle.com/dataset/80c06af6e1386ee747dc16103dd54a802680014cb20502f94263e673aeeb911d)\n\n* Upvote the [Dataset](http://www.kaggle.com/dataset/80c06af6e1386ee747dc16103dd54a802680014cb20502f94263e673aeeb911d) and to go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!\n\n* Before running the kernel turn on the **GPU(cuda)**."},{"metadata":{},"cell_type":"markdown","source":"# Import Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/vision-transformer-pytorch/VisionTransformer-Pytorch')\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nsys.path.append('../input/efficientnet-pytorch')\n\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom vision_transformer_pytorch import VisionTransformer\nfrom efficientnet_pytorch import EfficientNet\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Directory Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config settings\n\nclass CFG:\n    img_size = 512\n    num_classes = 5\n    num_workers = 4\n    batch_size = 64\n    epochs = 1\n\n\n# Directory settings\n\nOUTPUT_DIR = './'\n\nROOT_DIR = '../input/cassava-leaf-disease-classification/'\n\nTRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\n\nTEST_PATH = '../input/cassava-leaf-disease-classification/test_images'\n\nMODEL_DIR = '../input/cassava-models'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Dataset\n\n* In this cell, there are two different augmentations are defined. One for direct inference and other is for **TTA(Test Time Augmentation)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_augmentation(data):\n\n    if data=='test':\n        return A.Compose([\n            A.Resize(CFG.img_size, CFG.img_size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                max_pixel_value=255.0,\n                p=1.0),\n            ToTensorV2()\n        ])\n\n\n# augmentations taken from: https://www.kaggle.com/khyeh0719/pytorch-efficientnet-baseline-inference-tta\ntest_aug = A.Compose([\n    A.Resize(CFG.img_size, CFG.img_size),\n    A.Transpose(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.HueSaturationValue(\n        hue_shift_limit=0.2, \n        sat_shift_limit=0.2,\n        val_shift_limit=0.2, \n        p=0.5\n    ),\n    A.RandomBrightnessContrast(\n        brightness_limit=(-0.1,0.1), \n        contrast_limit=(-0.1, 0.1), \n        p=0.5\n    ),\n    A.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225], \n        max_pixel_value=255.0, \n        p=1.0\n    ),\n    ToTensorV2()\n], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['image_id'].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{TEST_PATH}/{file_name}'\n        image = Image.open(file_path).convert('RGB')\n        image = np.array(image)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n\n* In this section, different model calss are defined which can be replace or add with other model class.\n* After that, some functions are created to load and initialize the models.\n* And at the end of this section, **Ensemble class** is created to ensemble the models and perform mean prediction(make prediction for every model and took mean of the predictions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- CustomResNext ----- #\n\nclass CustomResNext(nn.Module):\n\n    def __init__(self, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model('resnext50_32x4d', pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, CFG.num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n# ----- Efficientnet-b4 with_torch_CrossEntorpy ----- #\n\nclass LeafModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.effnet = EfficientNet.from_name(\"efficientnet-b4\")\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(1792, 5)\n\n    def forward(self, image):\n        batch_size,_,_,_ = image.shape\n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        x = self.out(self.dropout(x))\n        return x\n\n\nclass Efficient_b4(nn.Module):\n\n    def __init__(self, PATH, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model('tf_efficientnet_b4_ns', pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, CFG.num_classes)\n        checkpoint = torch.load(PATH)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nclass Efficient_b7(nn.Module):\n\n    def __init__(self, PATH, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model('tf_efficientnet_b7_ns', pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, CFG.num_classes)\n        checkpoint = torch.load(PATH)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n# ----- Vision Transformer ----- #\n\nclass VitModel(nn.Module):\n\n    def __init__(self,PATH):\n        super().__init__()\n        self.model = VisionTransformer.from_name('ViT-B_16', num_classes=5)\n        self.model.load_state_dict(torch.load(PATH))\n\n    def forward(self, image):\n        batch_size,_,_,_ = image.shape\n        image = F.interpolate(image,(384,384))\n        x = self.model(image)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Efficientnet-b4 with_torch_CrossEntorpy\ndef get_leaf_model(PATH):\n    model = LeafModel()\n    checkpoint = torch.load(PATH)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    return model.to(device)\n\n\ndef get_resnext_model(PATH):\n    model = CustomResNext()\n    model.load_state_dict(torch.load(PATH))\n    model.eval()\n    return model.to(device)\n\n\ndef get_efficient_b4_model(PATH):\n    model = Efficient_b4(PATH)\n    model.eval()\n    return model.to(device)\n\n\ndef get_efficient_b7_model(PATH):\n    model = Efficient_b7(PATH)\n    model.eval()\n    return model.to(device)\n\ndef get_vit_model(PATH):\n    model = VitModel(PATH)\n    model.eval()\n    return model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EnsembledModel function\n\nclass EnsembledModel():\n\n    def __init__(self, model_paths):\n        super().__init__()\n        self.num_models = len(model_paths)\n\n        self.leafmodel1 = get_leaf_model(model_paths[0])\n        self.leafmodel2 = get_leaf_model(model_paths[1])\n\n        self.effb4_model1 = get_efficient_b4_model(model_paths[2])\n        self.effb4_model2 = get_efficient_b4_model(model_paths[3])\n        self.effb4_model3 = get_efficient_b4_model(model_paths[4])\n        self.effb4_model4 = get_efficient_b4_model(model_paths[5])\n        self.effb4_model5 = get_efficient_b4_model(model_paths[6])\n        \n        self.effb7_model1 = get_efficient_b7_model(model_paths[7])\n        self.effb7_model2 = get_efficient_b7_model(model_paths[8])\n        self.effb7_model3 = get_efficient_b7_model(model_paths[9])\n\n        self.res50_model1 = get_resnext_model(model_paths[10])\n        self.res50_model2 = get_resnext_model(model_paths[11])\n        self.res50_model3 = get_resnext_model(model_paths[12])\n        self.res50_model4 = get_resnext_model(model_paths[13])\n        self.res50_model5 = get_resnext_model(model_paths[14])\n\n        self.res50_lr_model1 = get_resnext_model(model_paths[15])\n        self.res50_lr_model2 = get_resnext_model(model_paths[16])\n        self.res50_lr_model3 = get_resnext_model(model_paths[17])\n\n\n    def predict(self, x):\n        with torch.no_grad():\n            l1 = self.leafmodel1(x)\n            l2 = self.leafmodel2(x)\n\n            b4_e1 = self.effb4_model1(x)\n            b4_e2 = self.effb4_model2(x)\n            b4_e3 = self.effb4_model3(x)\n            b4_e4 = self.effb4_model4(x)\n            b4_e5 = self.effb4_model5(x)\n\n            r1 = self.res50_model1(x)\n            r2 = self.res50_model2(x)\n            r3 = self.res50_model3(x)\n            r4 = self.res50_model4(x)\n            r5 = self.res50_model5(x)\n\n            b7_e1 = self.effb7_model1(x)\n            b7_e2 = self.effb7_model2(x)\n            b7_e3 = self.effb7_model3(x)\n\n            lr_r1 = self.res50_lr_model1(x)\n            lr_r2 = self.res50_lr_model2(x)\n            lr_r3 = self.res50_lr_model3(x)\n\n            pred = (l1+l2 + b4_e1+b4_e2+b4_e3+b4_e4+b4_e5 + b7_e1+b7_e2+b7_e3 + r1+r2+r3+r4+r5 + lr_r1+lr_r2+lr_r3) / (self.num_models)\n\n            return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_paths = [\n    '../input/cassava-trained-models/with_torch_crossentropy_LeafDiseasesModel Eff-4_fold-1.pt',\n    '../input/cassava-trained-models/with_torch_crossentropy_LeafDiseasesModel Eff-4_fold-5.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-4_fold-1.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-4_fold-2.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-4_fold-3.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-4_fold-4.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-4_fold-5.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-7_fold-1.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-7_fold-2.pt',\n    '../input/cassava-trained-models/LeafDiseasesModel Eff-7_fold-4.pt',\n    '../input/cassava-trained-models/Resnext50_fold-1.pth',\n    '../input/cassava-trained-models/Resnext50_fold-2.pth',\n    '../input/cassava-trained-models/Resnext50_fold-3.pth',\n    '../input/cassava-trained-models/Resnext50_fold-4.pth',\n    '../input/cassava-trained-models/Resnext50_fold-5.pth',\n    '../input/cassava-trained-models/Resnext50_with_lr_scheduler_fold-1.pth',\n    '../input/cassava-trained-models/Resnext50_with_lr_scheduler_fold-2.pth',\n    '../input/cassava-trained-models/Resnext50_with_lr_scheduler_fold-3.pth'\n]\n\nmodel = EnsembledModel(model_paths)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(model, data_loader):\n\n    epoch_preds = 0\n    for epoch in range(CFG.epochs):\n\n        preds = []\n        for images in tqdm(data_loader):\n            images = images.to(device)\n            logits = model.predict(images)\n            preds += [logits.softmax(1).detach().cpu().numpy()]\n        all_img_preds = np.concatenate(preds, axis=0)\n        epoch_preds += all_img_preds\n\n    epoch_preds = epoch_preds / CFG.epochs\n    return epoch_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n\n#test_data = TestDataset(test, transform=test_aug)       # Use for TTA\ntest_data = TestDataset(test, transform=get_augmentation(data='test'))\n\ntest_loader = DataLoader(test_data, batch_size=CFG.batch_size, num_workers=CFG.num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = inference(model, test_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['label'] = predictions.argmax(1)\ntest[['image_id', 'label']].to_csv(OUTPUT_DIR + 'submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n### Upvote the notebook if you like it.\n\n### To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}