{"cells":[{"metadata":{},"cell_type":"markdown","source":"Problem Statement\n\nA large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n\n\nThe former employeesâ€™ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n\nA sizeable department has to be maintained, for the purposes of recruiting new talent\n\nMore often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n\nHence, the management has contracted an HR analytics firm to <b>understand what factors they should focus on, in order to curb attrition.</b> In other words, they want to know <b>what changes they should make to their workplace, in order to get most of their employees to stay.</b> Also, they want to know which of these variables is most important and needs to be addressed right away.\n\nSince you are one of the star analysts at the firm, this project has been given to you.\n\nGoal of the case study\nYou are required to model the probability of attrition using a logistic regression. The results thus obtained will be used by the management to understand what changes they should make to their workplace, in order to get most of their employees to stay."},{"metadata":{},"cell_type":"markdown","source":"1) Understand the factors they should focus on to curb attrition\n\n2) What changes to make to get most of employees to stay\n\n3) Which variables are most important (rank them) and needs to be addressed straight away\n\n4) Use logistic regression to model probability of attrition for management to understand"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict = \"/kaggle/input/hr-analytics-case-study/data_dictionary.xlsx\"\ndata_dict= pd.read_excel(data_dict)\nemployee_survey = \"/kaggle/input/hr-analytics-case-study/employee_survey_data.csv\"\n\nsatis_survey= pd.read_csv(employee_survey)\nsatis_survey.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time data that employees come in every morning"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_in = \"/kaggle/input/hr-analytics-case-study/in_time.csv\"\n\ntime_in = pd.read_csv(time_in)\ntime_in.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manager's rating of employee in job involvement and performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"manager_rating = \"/kaggle/input/hr-analytics-case-study/manager_survey_data.csv\"\nmanager_rating= pd.read_csv(manager_rating)\nmanager_rating.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time out of employee"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_out = \"/kaggle/input/hr-analytics-case-study/out_time.csv\"\n\ntime_out= pd.read_csv(time_out)\ntime_out.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\n1) Combine employee, manager survey data and some oridinal data (Education, 'JobLevel','TrainingTimesLastYear')\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"general = \"/kaggle/input/hr-analytics-case-study/general_data.csv\"\n\ngeneral = pd.read_csv(general)\ngeneral.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"general.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal = general[[\"Education\", 'JobLevel','TrainingTimesLastYear']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survey = satis_survey.merge(manager_rating, on = \"EmployeeID\")\nsurvey = survey.astype(\"float64\")\nsubset = survey.drop([\"EmployeeID\"],axis =1)\nsurvey_results= pd.concat([subset,ordinal], axis =1)\nsurvey_results\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\n\nb= imputer.fit_transform(survey_results)\ncolumn_names= list(survey_results.columns)\nfinal_ordinal= pd.DataFrame(b, columns = column_names)\n\nfinal_ordinal_df = pd.concat([final_ordinal, general[\"Attrition\"]],axis = 1)\nfinal_ordinal_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#general.dtypes\nquali = [i for i in general.columns if general[i].dtypes == \"object\"]\nquanti = [i for i in general.columns if i not in quali]\nquanti_final = list(set(quanti).difference({'StockOptionLevel','Education','JobLevel','TrainingTimesLastYear',\"EmployeeID\",\"EmployeeCount\",\"StandardHours\"}))\nprint(\"Number of quali features :\", len(quali))\nprint(\"=\"* 50)\nprint(quali)\nprint(\"=\"* 50)\nprint(\"Number of quanti features :\", len(quanti_final))\nprint(\"=\"* 50)\nprint(quanti_final)\nprint(\"=\"* 50)\nprint(\"Number of ordinal features: \",len(list(final_ordinal.columns)))\nprint(\"=\"* 50)\nprint(list(final_ordinal.columns))\n\n#general.dtypes.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final features subset\n1) Removed StockOptionLevel, Education, Joblevel, trainingtimeslastyear from quantitative variables \n\n2) Removed EmployeeID (index), EmployeeCOunt and StandardHours(no variance)\n\n2) Added StockOptionLevel into qualitative features. The data is not considered ordinal\n\n3) Added  Education, Joblevel, trainingtimeslastyear into ordinal data together with EvironmentSatisfaction, JobSatisfaction, WorklifeBalance, JobINvolvment, PerformanceRating as final_ordinal_df"},{"metadata":{},"cell_type":"markdown","source":"## Filling the nan rows in with their most frequent occurence"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_quanti= general[quanti_final]\ndf_quanti[\"TotalWorkingYears\"].value_counts(dropna = False)\ndf_quanti[\"TotalWorkingYears\"] = df_quanti[\"TotalWorkingYears\"].fillna(df_quanti[\"TotalWorkingYears\"].mode()[0])\ndf_quanti[\"NumCompaniesWorked\"] = df_quanti[\"NumCompaniesWorked\"].fillna(1.0)\ndf_quanti[\"NumCompaniesWorked\"].value_counts(dropna = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_quanti.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_quanti_total = pd.concat([df_quanti,general[[\"Attrition\"]]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_quanti_total[\"YearsSinceLastPromotion\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing- Quanti\n## Use scatterplot to check the distribution of data between these features. \n- If they are linear, we can use Pearson's corr to get their correlation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(9,9, figsize = (30,30))\nfig.tight_layout(pad = 4.0)\naxes = ax.flatten()\ncol_list = df_quanti.columns.to_list()\ni = 0\nfor xfeatures in col_list:\n    for yfeatures in col_list:\n        sns.scatterplot(data = df_quanti_total,x = xfeatures, y = yfeatures, ax = axes[i])\n        i +=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let us just use spearman's corr"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_quanti.corr(\"spearman\")\nfig,ax = plt.subplots(figsize = (12,6))\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ncmap = sns.diverging_palette(220,10,as_cmap = True)\nsns.heatmap(corr, mask = mask, cmap = cmap, annot = True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing - Ordinal \n- Use spearman's corr to get their corr and check for collinearity "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = final_ordinal.corr(\"spearman\")\nfig,ax = plt.subplots(figsize = (12,6))\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ncmap = sns.diverging_palette(220,10,as_cmap = True)\nsns.heatmap(corr, mask = mask, cmap = cmap, annot = True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Since there aren't variables that are highly correlated with each other, let us proceed to try fitting a logistic regression first and go from there"},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_continuous = pd.concat([df_quanti, final_ordinal_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = merged_continuous.drop([\"Attrition\"], axis =1 )\ny = merged_continuous[\"Attrition\"]\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As the dataset is imbalanced, let us use SMOTE to resample our minority group"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE \noversample = SMOTE()\nX_over,y_over= oversample.fit_resample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\npipe = Pipeline(steps = [(\"scaler\",MinMaxScaler()),(\"logerg\", LogisticRegression(penalty = \"l1\",\n                                                                                 C = 50,\n                                                                                max_iter = 250,\n                                                                                solver =\"saga\",\n                                                                                class_weight = \"balanced\"))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.fit(X_over,y_over)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report \ny_over_pred = pipe.predict(X_over)\nprint(classification_report(y_over, y_over_pred))\ny_train_pred = pipe.predict(X_train)\nprint(classification_report(y_train, y_train_pred))\ny_test_pred = pipe.predict(X_test)\nprint(classification_report(y_test, y_test_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let us try using other classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\npipe_3 = Pipeline(steps = [(\"scaler\",MinMaxScaler()),(\"gbc\", GradientBoostingClassifier(n_estimators = 100,\n                                                                                learning_rate = 1.0,\n                                                                                        min_samples_split = 30,\n                                                                                        max_features = \"sqrt\",\n                                                                                           random_state = 42))])\npipe_3.fit(X_over,y_over)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report \ny_over_pred = pipe_3.predict(X_over)\nprint(classification_report(y_over, y_over_pred))\ny_train_pred = pipe_3.predict(X_train)\nprint(classification_report(y_train, y_train_pred))\ny_test_pred = pipe_3.predict(X_test)\nprint(classification_report(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance of the Gradient Boosted classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_without_attrition= merged_continuous.drop([\"Attrition\"],axis =1)\na = pd.DataFrame(dict(zip(df_without_attrition.columns,pipe_3[\"gbc\"].feature_importances_)),index = [0])\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\ndescending_list = a.iloc[0].sort_values(ascending = False).index.tolist()\na_ordered_df = a[descending_list]\nsns.barplot(y = a_ordered_df.columns, \n            x = a_ordered_df.iloc[0],\n           orient = 'h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using gradientboosting, we are able to predict just using quantiative variables. \n- The top 8 factors are \n- Environmentsatisfaction, YearsAtCompany, Job satisfaction,Age,Years with current manager, monthly income,Total working years, WOrklifebalance ,Jobinvolvement ,Joblevel. \n\nManagement can work towards improving jobsatisfaction and environment satisfaction, Environmentstatisfaction,jobinvolvement and worklifebalance for their employees. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#important_features = merged_continuous[[\"Attrition\",\"YearsWithCurrManager\",\"JobSatisfaction\",\"MonthlyIncome\",\"EnvironmentSatisfaction\",\"Age\",\"JobInvolvement\",\"WorkLifeBalance\",\"DistanceFromHome\"]].groupby(\"Attrition\").mean()\n#features_df= important_features.reset_index()\n#features_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig,ax = plt.subplots(3,3, figsize = (12,10))\n#fig.tight_layout(pad = 2.0)\n#axes = ax.flatten()\n#without_attrition= features_df.drop([\"Attrition\"],axis = 1)\n#col_list = without_attrition.columns.to_list()\n#i = 0\n#\n#for features in col_list:\n#    sns.barplot(x = \"Attrition\", y = without_attrition[features] ,data = features_df,ax = axes[i])\n#    i +=1\n#\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}