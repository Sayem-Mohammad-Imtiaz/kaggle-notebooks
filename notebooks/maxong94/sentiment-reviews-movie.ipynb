{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"imdb = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\nimdb.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import re \n#import nltk\n#import string\n#from nltk.corpus import stopwords\n#from nltk.tokenize import word_tokenize\n#cleaned_reviews = []\n#docu = document\n#docu = docu.lower()\n#docu = re.sub(r\"<br />\",\" \",docu)\n#pattern = re.compile(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\")\n## substitute the characters above with \n#docu = re.sub(pattern,\"\", docu)\n## tokenize them so we can remove the stopwords from english language\n#tokens = word_tokenize(docu)\n#stop_words = set(stopwords.words(\"english\"))\n#tokens = [w for w in tokens if w not in stop_words]\n#table = str.maketrans(\"\",\"\",string.punctuation)\n#stripped = [w.translate(table) for w in tokens]\n#PS = PorterStemmer()\n#cleaned_tokens = [PS.stem(w) for w in stripped if w.isalpha()]\n#joined_tokens = \" \".join(cleaned_tokens)\n#cleaned_reviews.append(joined_tokens)\n#cleaned_reviews\n\n#Bag of words model\n#This will be our tutorial to show how the various metheods of producing encoding through the bag of words model will work in terms of the performance of our sentiment reviews movie model. \n#\n#We will use the Tokenizer in Keras API to score words. \n#\n#1) binary \n#\n#2) Count \n#\n#3) tfidf \n#\n#4) frequency \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the reviews from df to a list of reviews \n\nFor each review in the list: \n\n1) Convert to lower case \n\n2) Use regex to remove special characters \n\n- pattern = re.compile(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\")\n\n3) Tokenize the words through whitespace and punctuations \n\n4) Remove punctuations again just in case \n\n5) Return the tokens only if they are alphabetical \n\n6) Get the list of stop words from english language \n\n7) only include words not found in stop words \n\n8) Join all the tokens back. Since each tokens are from each review, we get back a list which each element is a review. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import re \nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import Counter\n\nstopwords_list = stopwords.words('english')\ndef clean_text(df):\n    cleaned_reviews= []\n    reviews = df['review'].tolist()\n    vocab = Counter()\n    # change each element within the list to lower punctuation\n    \"\"\"Docu will be a each row in the dataframe\"\"\"\n    for sentence in reviews:\n        sentence = re.sub(r\"<br />\",\" \",sentence)\n        pattern = re.compile(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\")\n        # substitute the characters above with \n        sentence = re.sub(pattern,\" \", sentence)\n        # tokenize them so we can remove the stopwords from english language\n        token_list = word_tokenize(sentence)\n        token_list = [tokens.lower() for tokens in token_list]\n        # remove stopwords from english \n        token_list = [tokens for tokens in token_list if tokens not in stopwords_list]\n        # remove words if they are less than 2\n        token_list = [tokens for tokens in token_list if len(tokens) >2 ]\n        cleaned_tokens = [tokens for tokens in token_list if tokens.isalpha()]\n        vocab.update(cleaned_tokens)\n        joined_tokens = \" \".join(cleaned_tokens)\n        cleaned_reviews.append(joined_tokens)\n    return vocab, cleaned_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_copy= imdb.copy()\nimdb_copy['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_positive_reviews = imdb_copy[imdb_copy['sentiment'] == 'positive']\ndf_negative_reviews = imdb_copy[imdb_copy['sentiment'] == 'negative']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can we see any difference in the type of words used across positive vs negative words? "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\ndef plot_word_count(df):\n    # call the function to get our vocab count and cleaned_reviews \n    vocab,cleaned_reviews = clean_text(df)\n    # change counter to dictionary object\n    dict_vocab = dict(vocab)\n    # sort dictionary base on values \n    sorted_vocab_dict = sorted(dict_vocab.items(),key = lambda x: x[1],reverse = True)\n    # create x and y list to append to for plotting \n    y = []\n    x = []\n    for i in sorted_vocab_dict[:50]:\n        y.append(i[1])\n        x.append(i[0])\n    \n    fig,ax = plt.subplots(figsize = (20,8))\n    plot = ax.bar(x,y)\n    plt.xticks(rotation = 50)\n    plt.title(\"{}\".format(df[\"sentiment\"].to_numpy()[0]))\n    return plot \n\nplot_word_count(df_positive_reviews)\nplot_word_count(df_negative_reviews)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outcome of cleaning the corpus should be a list of documents for us, so it can be inputted correctly into the texts_to_sequence function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer \n\nmax_nb_words = 50000\n\nmax_sequence_length = 250\n\nembedding_dim = 100\n\ntokenizer = Tokenizer(num_words = max_nb_words,\n             lower = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab,cleaned_reviews = clean_text(imdb_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_reviews[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to fit the tokenzier\ntokenizer.fit_on_texts(cleaned_reviews)\nword_index = tokenizer.word_index\nprint(\"Unique tokens:\", len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nX = tokenizer.texts_to_sequences(cleaned_reviews)\n# pre padding\nX = pad_sequences(X,maxlen = max_sequence_length)\nprint(\"shape of data tensor:\", X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = imdb_copy[\"sentiment\"].replace({\"positive\":1,\"negative\":0}).to_numpy()\nlen(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After cleaning, let us use the TF-IDF vectorizer to help us transform our document and vocab as inputs for our model. \n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.feature_selection import SelectKBest \n# from sklearn.feature_selection import f_classif\n# # we will use unigrams and bigrams in our vocab list \n# \n# k_value = 20000\n# def tfid_vectorize(texts, labels):\n#     tfidf_vectorizer = TfidfVectorizer(# both unigrams and bigrams\n#                         ngram_range = (1,2),\n#                         # whether the feature will be made of word or character n-gram\n#                         analyzer = \"word\",\n#                         # cut of minimum number of times vocab word to appear in document. Minimum 2 to be accepted as vocab \n#                         min_df = 2)\n#     text_transformed = tfidf_vectorizer.fit_transform(texts)\n#     \n#     # select best k features, with feature importance measured by f_classif\n#     # set k = 20000\n#     # initialize\n#     selector = SelectKBest(score_func = f_classif, k = min(k_value,text_transformed.shape[1]))\n#     selector.fit(text_transformed,labels)\n#     transformed_texts = selector.transform(text_transformed)\n#     return transformed_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorized_data = tfid_vectorize(X_cleaned,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Shape of transformed text matrix\",vectorized_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                 y,\n                                                 test_size = 0.2,\n                                                 random_state = 42)\nprint(\"Shape of training data\",X_train.shape,\" Shape of y_train\",y_train.shape)\nprint(\"Shape of test data\",X_test.shape, \" Shape of y_test\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_words = X_train.shape[1]\nn_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\n#from tensorflow.keras.layers import Dropout\ndef define_model(n_words):\n    model = Sequential()\n    model.add(Embedding(max_nb_words,embedding_dim,input_length = n_words))\n    # similar to dropout, but drops the whole slice along 0 axis. \n    #model.add(SpatialDropout1D(0.9))\n    model.add(Dropout(rate = 0.9, noise_shape = (1,embedding_dim)))\n    model.add(LSTM(5,dropout = 0.8, recurrent_dropout = 0.3)) # this dropout drops the inputs and outputs, not the hidden states of our LSTM\n    # sigmoid because only 2 classes, if there are more than 2 classes, use softmax\n    model.add(Dense(1,activation = \"sigmoid\"))\n    # compile network\n    model.compile(loss = \"binary_crossentropy\",optimizer = \"adam\",metrics = [\"accuracy\"])\n    model.summary()\n    return model\nmodel = define_model(n_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After some experimenting, using SpatialDropout1D helps to reduce overfitting significantly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 40\nBATCH_SIZE = 64\nfile_path = 'model.h5'\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease on two consecutive tries, stop training\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = file_path, save_freq = 'epoch')\n# put multiple call backs?\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and validate model\n# To start training, call the model.fit method—the model is \"fit\" to the training data.\n# Note that fit() will return a History object which we can use to plot training vs. validation accuracy and loss.\n\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=BATCH_SIZE, callbacks=[early_stopping,model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Let's plot training and validation accuracy as well as loss.\ndef plot_history(history):\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(1,len(accuracy) + 1)\n    \n    # Plot accuracy  \n    plt.figure(1)\n    plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n    plt.plot(epochs, val_accuracy, 'g', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.figure(2)\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,y_test)\nprint(\"Test set \\n Loss: {} \\n Accuracy: {}\".format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}