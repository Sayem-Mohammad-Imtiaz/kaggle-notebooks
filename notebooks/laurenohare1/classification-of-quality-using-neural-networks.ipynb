{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using Neural Networks to classify the Quality of Red Wine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* In this notebook I will use the dataset \"Red Wine Quality\". \n* The aim of this notebook is to use a basic fully connected neural network to classify the quality of the red wine.\n* This deep learning task will be completed with the python package \"Pytorch\". ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.autograd import Variable\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder, scale\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reproducibility","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(1)\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\nnp.random.seed(1)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inital Exploratory Data Analysis of Quality\n\nExplore the \"quality\" variable to :\n* Check how many unique classes we have.\n* Check if the classes are balanced. \n    - Data that is not balanced can be challenging for a classification problem!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Load the data to a pandas dataframe","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# get the directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Check the number of unique output classes in \"quality\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"quality\"].unique() #printed in the order they appear in the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This tells us that there are **six** unique output classes that the wine can belong to. The quality classes are ranked as 3 being the lowest quality in this data and 8 being the highest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = \"quality\", data = data)\nplt.title(\"Number of observations per category of wine\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart above shows that the most common categories are \"5\" and \"6\". Since certain categories have more observations this plot displays the imbalanced nature of the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing the data for deep learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns an array of the text files\n# Skipping the first row which is the column names\n\nwine = np.loadtxt(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\", delimiter= \",\", skiprows= 1)\n\n# These options determine the way floating point numbers, arrays and other NumPy objects are displayed.\n\n# Number of digits of precision for floating point output\nnp.set_printoptions(precision = 2)\n\n# If True, always print floating point numbers using fixed point notation, \n# in which case numbers equal to zero in the current precision will print as zero.\nnp.set_printoptions(suppress = True)\n\nprint(\"# of instances:\", len(wine))\n\n# print the first five rows\nprint(wine[:5])\n\n# print the last five rows\nprint(wine[-5:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 1,599 observations in the data.\n* There are 11 input features.\n* There is one target class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the test percentage = 20%\ntest_per = 0.2\n\n# number of input features\nn_features = 11\n\n# X is all the rows in the dataset and the first 11 columns, ie the 11 input features\nX = wine[:,:n_features]\n\n# Standardize the input features\nX = scale(X)\n\n# Now we have X - a scaled dataset with all the features\n\n# label is all the rows but only from the 11th column. \n# Ie this is just the target values\nlabel = wine[:, n_features:]\n\n# Encode categorical features as a one-hot numeric array.\noneHot = OneHotEncoder()\n\n# Fit OneHotEncoder to X, then transform X. Then to an array\nlabel = oneHot.fit_transform(label).toarray()\n\n# Split the data for testing and training\n# Using X which are the features\n# Using label which is the oneHot encoded targets\nX_train , X_test , Y_train , Y_test = train_test_split(X , label, test_size = test_per)\n\n# check the lengths of the data \nprint(len(X_train))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this analysis there will be 1279 observations in the training data and 320 observations in the testing data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Define the Fully Connected Neural Network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A fully connected neural network consists of linear layers where the connections between nodes are controlled by weights.\nThese weights are updated and trained using **backpropagation** to aid in optimisation of the neural network.\nIf you would like to read more about fully connected neural networks and backpropagation check out this [artical](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528)!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before looking at how the neural network is coded in Pytorch, lets visualise it! The image below displays the architecture of my neural network. \nThe input layer has 11 nodes which is **fully connected** to 11 nodes (this is the hidden layer), which is **fully connected** to 6 nodes (the output classes).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/fcnimage/FCN model.JPG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FCN(nn.Module):\n    def __init__(self):\n        super(FCN , self).__init__()\n        \n        # Input to 11 features (n_features defined as 11 above) to 11 hidden nodes. \n        self.layer1 = nn.Linear(n_features , n_features)\n        \n        # 11 hidden nodes to 6 output classes.\n        self.layer2 = nn.Linear(n_features , 6)  # 6 output classes\n        \n        # Forward pass\n    def forward(self , data):\n        \n        # apply layer one to input \n        activation1 = self.layer1(data)\n        \n        # sigmoid activation on the first layer\n        activation1 = torch.sigmoid(activation1)\n        \n        # layer two \n        activation2 = self.layer2(activation1)\n        \n        # return the output activation on the sigmoid\n        return torch.sigmoid(activation2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the FCN!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the model instance\nmodel = FCN()\n\n# define the criterion \n# Creates a criterion that measures the Binary Cross Entropy between the target and the output\ncriterion = nn.BCELoss()\n\n# gradient decent\noptimizer = optim.SGD(model.parameters() , lr = 0.2)\n\n# convert the numpy to tensor\n# variable wraps a tensor\nX = Variable(torch.from_numpy(X_train).float())\nY = Variable(torch.from_numpy(Y_train).float())\n\n# set n to 0\n# create an empty array to hold the losses, actual class, predicted class and training accuracy.\nn = 0\nlosses = []\nact_class = []\npred_class = []\ntrain_acc = []\n\n# iterate the data 10,000 times\nfor epoch in range(10000):\n    \n    # forward pass\n    out = model(X)\n    loss = criterion(out , Y)\n    losses.append(loss.data)\n    \n    optimizer.zero_grad()\n    \n    # back propagation\n    loss.backward()\n    optimizer.step()\n    \n    # save the training accuracy to the train_acc array\n    train_acc.append(accuracy_score(oneHot.inverse_transform(Y), oneHot.inverse_transform(model(X).data.numpy())))\n    \n    if epoch % 500 == 0:\n        print(epoch, loss.data)\n        \n\n# Training Over.\n\n# add the losses to the losses array\nlosses = np.array(losses , dtype = np.float)\n\n# Convert the data back to the original representation.\ntrain_out = oneHot.inverse_transform(model(X).data.numpy())\n\nprint('Training accuracy', accuracy_score(oneHot.inverse_transform(Y), train_out))\n\ntest_out=oneHot.inverse_transform(model(torch.from_numpy(X_test).float()).data.numpy())\n\nprint('prediction accuracy', accuracy_score(oneHot.inverse_transform(Y_test), test_out))\n\n# gather class results\nact_class.append(oneHot.inverse_transform(Y_test))\npred_class.append(test_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.plot(losses)\nplt.title(\"Loss during training\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_acc)\nplt.title(\"Training Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(oneHot.inverse_transform(Y_test), test_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This model has approximately 60% accuracy and some correct classifications have been made.\n* Wine that belongs to quality class \"5\" has 107/170 correct classifications.\n* Wine that belongs to quality class \"6\" has 80/138 correct classifications.\n* Wine that belongs to quality class \"7\" has 5/12 correct classifications.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook and analysis was just for a bit of fun and to show how a neural network can be used for classification! Any feedback is really appreciated!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}