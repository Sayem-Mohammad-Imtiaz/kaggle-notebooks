{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nfrom scipy.sparse import hstack\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom prettytable import PrettyTable \n\n\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport pickle\nfrom tqdm import tqdm\nimport os\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:32:14.862277Z","iopub.execute_input":"2021-07-04T21:32:14.862646Z","iopub.status.idle":"2021-07-04T21:32:16.41792Z","shell.execute_reply.started":"2021-07-04T21:32:14.862563Z","shell.execute_reply":"2021-07-04T21:32:16.417193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ndf.iloc[50:55]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:36:21.970898Z","iopub.execute_input":"2021-07-04T21:36:21.97129Z","iopub.status.idle":"2021-07-04T21:36:22.275084Z","shell.execute_reply.started":"2021-07-04T21:36:21.971258Z","shell.execute_reply":"2021-07-04T21:36:22.274297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_loc = df['Location'].value_counts()\nlen(top_loc)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:36:43.244065Z","iopub.execute_input":"2021-07-04T21:36:43.244532Z","iopub.status.idle":"2021-07-04T21:36:43.265421Z","shell.execute_reply.started":"2021-07-04T21:36:43.244504Z","shell.execute_reply":"2021-07-04T21:36:43.264261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clas = df['Sentiment'].value_counts()\nclas","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:37:01.075493Z","iopub.execute_input":"2021-07-04T21:37:01.075802Z","iopub.status.idle":"2021-07-04T21:37:01.090489Z","shell.execute_reply.started":"2021-07-04T21:37:01.075776Z","shell.execute_reply":"2021-07-04T21:37:01.089426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/datatattle/covid-19-tweets-eda-viz?scriptVersionId=43030869&cellId=57\n\n#Remove Urls and HTML links\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:37:10.364179Z","iopub.execute_input":"2021-07-04T21:37:10.364491Z","iopub.status.idle":"2021-07-04T21:37:10.67819Z","shell.execute_reply.started":"2021-07-04T21:37:10.364455Z","shell.execute_reply":"2021-07-04T21:37:10.677301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:lower(x))\n\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_num(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:37:26.517056Z","iopub.execute_input":"2021-07-04T21:37:26.517387Z","iopub.status.idle":"2021-07-04T21:37:26.87327Z","shell.execute_reply.started":"2021-07-04T21:37:26.517361Z","shell.execute_reply":"2021-07-04T21:37:26.87258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove stopwords & Punctuations\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:punct_remove(x))\n\n\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_stopwords(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:37:36.772176Z","iopub.execute_input":"2021-07-04T21:37:36.772688Z","iopub.status.idle":"2021-07-04T21:37:37.423218Z","shell.execute_reply.started":"2021-07-04T21:37:36.772658Z","shell.execute_reply":"2021-07-04T21:37:37.42234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove mentions and hashtags\ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_mention(x))\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_hash(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:37:58.883773Z","iopub.execute_input":"2021-07-04T21:37:58.884082Z","iopub.status.idle":"2021-07-04T21:37:59.020005Z","shell.execute_reply.started":"2021-07-04T21:37:58.884053Z","shell.execute_reply":"2021-07-04T21:37:59.019185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_space(x))\n\ndf = df.drop(columns=['preprocessed_tweets'])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:38:08.023156Z","iopub.execute_input":"2021-07-04T21:38:08.023528Z","iopub.status.idle":"2021-07-04T21:38:08.484362Z","shell.execute_reply.started":"2021-07-04T21:38:08.023485Z","shell.execute_reply":"2021-07-04T21:38:08.483584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:38:16.581469Z","iopub.execute_input":"2021-07-04T21:38:16.581919Z","iopub.status.idle":"2021-07-04T21:38:16.591525Z","shell.execute_reply.started":"2021-07-04T21:38:16.581891Z","shell.execute_reply":"2021-07-04T21:38:16.590947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['Location'].isnull().values.any())\nprint(\"number of nan values\",df['Location'].isnull().values.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:38:38.453086Z","iopub.execute_input":"2021-07-04T21:38:38.453516Z","iopub.status.idle":"2021-07-04T21:38:38.46495Z","shell.execute_reply.started":"2021-07-04T21:38:38.453486Z","shell.execute_reply":"2021-07-04T21:38:38.464076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/questions/60701329/different-ways-to-pre-process-date-in-machine-learning-using-python\nimport datetime\npre_tweetat = []\norigin = datetime.datetime(2020,3,16)\nfor i in tqdm(df[\"TweetAt\"]):\n    days = (datetime.datetime.strptime(i, '%d-%m-%Y') - origin).days\n    pre_tweetat.append(days)\ndf[\"pre_tweetat\"] = pre_tweetat\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:38:46.915246Z","iopub.execute_input":"2021-07-04T21:38:46.915597Z","iopub.status.idle":"2021-07-04T21:38:47.43438Z","shell.execute_reply.started":"2021-07-04T21:38:46.915567Z","shell.execute_reply":"2021-07-04T21:38:47.433581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:38:59.951022Z","iopub.execute_input":"2021-07-04T21:38:59.951369Z","iopub.status.idle":"2021-07-04T21:38:59.96424Z","shell.execute_reply.started":"2021-07-04T21:38:59.951337Z","shell.execute_reply":"2021-07-04T21:38:59.963077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_pre = []\nfor i in tqdm(df[\"Sentiment\"]):\n    if i == 'Extremely Positive':\n        sentiment_pre.append(1)\n    elif i == 'Positive':\n        sentiment_pre.append(1)\n    elif i == 'Neutral':\n        sentiment_pre.append('N')\n    elif i == 'Negative':\n        sentiment_pre.append(0)\n    elif i == 'Extremely Negative':\n        sentiment_pre.append(0)    \ndf[\"sentiment_pre\"] = sentiment_pre\n\ndf = df[df.sentiment_pre != 'N']\ndf.sentiment_pre.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:39:35.930662Z","iopub.execute_input":"2021-07-04T21:39:35.931017Z","iopub.status.idle":"2021-07-04T21:39:36.012249Z","shell.execute_reply.started":"2021-07-04T21:39:35.930986Z","shell.execute_reply":"2021-07-04T21:39:36.011268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop([\"ScreenName\"],axis=1)\ndf = df.drop([\"UserName\"],axis=1)\ndf = df.drop([\"TweetAt\"],axis=1)\ndf = df.drop([\"Sentiment\"],axis=1)\ndf = df.drop([\"Location\"],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:39:58.809578Z","iopub.execute_input":"2021-07-04T21:39:58.809942Z","iopub.status.idle":"2021-07-04T21:39:58.82744Z","shell.execute_reply.started":"2021-07-04T21:39:58.809913Z","shell.execute_reply":"2021-07-04T21:39:58.82656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:46:11.105409Z","iopub.execute_input":"2021-07-04T21:46:11.105908Z","iopub.status.idle":"2021-07-04T21:46:11.119398Z","shell.execute_reply.started":"2021-07-04T21:46:11.105878Z","shell.execute_reply":"2021-07-04T21:46:11.118554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into **X** and **Y**","metadata":{}},{"cell_type":"code","source":"Y=df['sentiment_pre'].values\nY=Y.astype('int')                 #https://stackoverflow.com/a/45347800\nX=df.drop([\"sentiment_pre\"],axis=1)\nlen(Y)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:40:24.439991Z","iopub.execute_input":"2021-07-04T21:40:24.440595Z","iopub.status.idle":"2021-07-04T21:40:24.449891Z","shell.execute_reply.started":"2021-07-04T21:40:24.44056Z","shell.execute_reply":"2021-07-04T21:40:24.448997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train Test Split","metadata":{}},{"cell_type":"code","source":"#splitting the data and in this I will use RandomSearchCV therefore will only split into xtrain and xtest\n# train test split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,stratify=Y)\n#X_train, X_cv, Y_train, y_cv = train_test_split(X_train, Y_train, test_size=0.33, stratify=Y_train)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n#print(X_cv.shape,y_cv.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:43:51.209646Z","iopub.execute_input":"2021-07-04T21:43:51.210154Z","iopub.status.idle":"2021-07-04T21:43:51.269525Z","shell.execute_reply.started":"2021-07-04T21:43:51.210125Z","shell.execute_reply":"2021-07-04T21:43:51.268456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding the Columns**","metadata":{}},{"cell_type":"code","source":"#Encoding the preprocessed_tweets column\nmodel_tweets = CountVectorizer(max_features=41157,ngram_range=(1,2),min_df=10)\nmodel_tweets.fit(X_train[\"OriginalTweet\"].values)\n\nX_train_tweet_bow = model_tweets.transform(X_train[\"OriginalTweet\"].values)\nX_test_tweet_bow = model_tweets.transform(X_test[\"OriginalTweet\"].values)\n\nprint(X_train_tweet_bow.shape, Y_train.shape)\nprint(X_test_tweet_bow.shape, Y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:43:53.844124Z","iopub.execute_input":"2021-07-04T21:43:53.84448Z","iopub.status.idle":"2021-07-04T21:43:57.485626Z","shell.execute_reply.started":"2021-07-04T21:43:53.844454Z","shell.execute_reply":"2021-07-04T21:43:57.48502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding Numerical features : pre_tweetat\nmodel_time = Normalizer()\nmodel_time.fit(X_train[\"pre_tweetat\"].values.reshape(1,-1))\nmodel_time2 = Normalizer()\nmodel_time2.fit(X_test[\"pre_tweetat\"].values.reshape(1,-1))\n\nX_train_time = model_time.transform(X_train[\"pre_tweetat\"].values.reshape(1,-1)).T\nX_test_time = model_time2.transform(X_test[\"pre_tweetat\"].values.reshape(1,-1)).T\n\nprint(X_train_time.shape, Y_train.shape)\nprint(X_test_time.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:49:01.51319Z","iopub.execute_input":"2021-07-04T21:49:01.513566Z","iopub.status.idle":"2021-07-04T21:49:01.52065Z","shell.execute_reply.started":"2021-07-04T21:49:01.513537Z","shell.execute_reply":"2021-07-04T21:49:01.519974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Concatinating all the features\n\nX_tr = hstack((X_train_tweet_bow,X_train_time)).tocsr()\nX_te = hstack((X_test_tweet_bow,X_test_time)).tocsr()\n\nprint(X_tr.shape, Y_train.shape)\n\nprint(X_te.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:49:20.793884Z","iopub.execute_input":"2021-07-04T21:49:20.794395Z","iopub.status.idle":"2021-07-04T21:49:20.823713Z","shell.execute_reply.started":"2021-07-04T21:49:20.794363Z","shell.execute_reply":"2021-07-04T21:49:20.822956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Appling Multinomial NB: BOW featurization\n\ndef batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:49:52.478416Z","iopub.execute_input":"2021-07-04T21:49:52.478938Z","iopub.status.idle":"2021-07-04T21:49:52.483756Z","shell.execute_reply.started":"2021-07-04T21:49:52.478908Z","shell.execute_reply":"2021-07-04T21:49:52.483195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n\nnbayes = MultinomialNB(class_prior = [0.5,0.5],fit_prior=False)\nparameters = {'alpha':[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\nclf = GridSearchCV(nbayes,parameters,cv=10, scoring='roc_auc',return_train_score=True,n_jobs=-1)\nclf.fit(X_tr,Y_train)\n\nresults = pd.DataFrame.from_dict(clf.cv_results_)\nresults = results.sort_values([\"param_alpha\"])\n\ntrain_auc = results[\"mean_train_score\"]\ncv_auc = results[\"mean_test_score\"]\n\nalph = results[\"param_alpha\"]             #Reference from https://www.youtube.com/watch?v=RMqT5kDtJhs&t=807s @12:48mins\nlog_alp=[]\nfor i in range(0,len(alph),1):\n    l=math.log10(alph[i])\n    log_alp.append(l)\nlog_alp.sort()\n\nplt.figure(figsize=(20,9))\n\nplt.plot(log_alp,train_auc,label='Train AUC')\nplt.plot(log_alp,cv_auc,label='CV AUC')\nplt.scatter(log_alp,train_auc,label='Train AUC Points')\nplt.scatter(log_alp,cv_auc,label='CV AUC Points')\nplt.legend()\nplt.xlabel(\"Log-Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Hyper parameter Vs AUC plot\")\n\nplt.grid()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:50:01.233746Z","iopub.execute_input":"2021-07-04T21:50:01.234264Z","iopub.status.idle":"2021-07-04T21:50:04.382674Z","shell.execute_reply.started":"2021-07-04T21:50:01.234233Z","shell.execute_reply":"2021-07-04T21:50:04.38187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the results for better observation\nresults[[\"param_alpha\",\"mean_test_score\",\"mean_train_score\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:50:20.094984Z","iopub.execute_input":"2021-07-04T21:50:20.095411Z","iopub.status.idle":"2021-07-04T21:50:20.10906Z","shell.execute_reply.started":"2021-07-04T21:50:20.095371Z","shell.execute_reply":"2021-07-04T21:50:20.107886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_alpha = 0.5\n# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n\nmulti_nb = MultinomialNB(alpha=best_alpha,class_prior=[0.5,0.5])\nmulti_nb.fit(X_tr,Y_train)\n\ny_train_pred = batch_predict(multi_nb,X_tr)\ny_test_pred = batch_predict(multi_nb,X_te)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(Y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n\n\nplt.figure(figsize=(10,10))\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:50:44.873747Z","iopub.execute_input":"2021-07-04T21:50:44.874059Z","iopub.status.idle":"2021-07-04T21:50:45.092909Z","shell.execute_reply.started":"2021-07-04T21:50:44.874034Z","shell.execute_reply":"2021-07-04T21:50:45.092094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:51:04.64271Z","iopub.execute_input":"2021-07-04T21:51:04.643067Z","iopub.status.idle":"2021-07-04T21:51:04.648875Z","shell.execute_reply.started":"2021-07-04T21:51:04.643042Z","shell.execute_reply":"2021-07-04T21:51:04.647974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\nprint('='*50)\nprint(\"Train confusion matrix\")\nprint('='*50)\ntrain_cm = confusion_matrix(Y_train, predict_with_best_t(y_train_pred, best_t))\nsns.heatmap(train_cm, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:51:16.512673Z","iopub.execute_input":"2021-07-04T21:51:16.513014Z","iopub.status.idle":"2021-07-04T21:51:16.775419Z","shell.execute_reply.started":"2021-07-04T21:51:16.512986Z","shell.execute_reply":"2021-07-04T21:51:16.77481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('='*50)\nprint(\"Test confusion matrix\")\nprint('='*50)\ntest_cm = confusion_matrix(Y_test, predict_with_best_t(y_test_pred, best_t))\nsns.heatmap(test_cm, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:51:30.523912Z","iopub.execute_input":"2021-07-04T21:51:30.524575Z","iopub.status.idle":"2021-07-04T21:51:30.900012Z","shell.execute_reply.started":"2021-07-04T21:51:30.524529Z","shell.execute_reply":"2021-07-04T21:51:30.899206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting True Positives for WordClouds\n\npred1 = predict_with_best_t(y_test_pred, best_t)\nfal_pos = []\nfor i in range(len(Y_test)):\n    if (Y_test[i]==1) and (pred1[i]==1):\n        fal_pos.append(i)\n\n\n#extracting the Tweets from X_test\npos_tweets = []\nfor i in tqdm(fal_pos):\n    pos_tweets.append(X_test['OriginalTweet'].values[i])\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:51:41.25491Z","iopub.execute_input":"2021-07-04T21:51:41.255282Z","iopub.status.idle":"2021-07-04T21:51:41.300613Z","shell.execute_reply.started":"2021-07-04T21:51:41.255248Z","shell.execute_reply":"2021-07-04T21:51:41.299877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.geeksforgeeks.org/generating-word-cloud-python/\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n    \ncomment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in pos_tweets:\n       \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='White',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:52:06.273009Z","iopub.execute_input":"2021-07-04T21:52:06.273527Z","iopub.status.idle":"2021-07-04T21:52:08.458112Z","shell.execute_reply.started":"2021-07-04T21:52:06.273498Z","shell.execute_reply":"2021-07-04T21:52:08.4573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting True Negatives for WordClouds\n\npred0 = predict_with_best_t(y_test_pred, best_t)\ntru_neg = []\nfor i in range(len(Y_test)):\n    if (Y_test[i]==0) and (pred0[i]==0):\n        tru_neg.append(i)\n\n\n#extracting the Tweets from X_test\nneg_tweets = []\nfor i in tqdm(tru_neg):\n    neg_tweets.append(X_test['OriginalTweet'].values[i])\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:52:20.572623Z","iopub.execute_input":"2021-07-04T21:52:20.573086Z","iopub.status.idle":"2021-07-04T21:52:20.622114Z","shell.execute_reply.started":"2021-07-04T21:52:20.573042Z","shell.execute_reply":"2021-07-04T21:52:20.619627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.geeksforgeeks.org/generating-word-cloud-python/\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n    \ncomment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in neg_tweets:\n       \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='White',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:52:39.0425Z","iopub.execute_input":"2021-07-04T21:52:39.04282Z","iopub.status.idle":"2021-07-04T21:52:41.162507Z","shell.execute_reply.started":"2021-07-04T21:52:39.042793Z","shell.execute_reply":"2021-07-04T21:52:41.161488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting the top 20 Features of Postive and the Negative Class\n\n#https://imgur.com/mWvE7gj\ntop_feat = multi_nb.feature_log_prob_\nsort_tf=np.argsort(top_feat)\n\n\nneg_feat = sort_tf[0]\nneg_feat = neg_feat[::-1][:20]         #top 20 negative features\npos_feat = sort_tf[1]\npos_feat = pos_feat[::-1][:20]         #top 20 positive features\n\n\nl = []\nl.extend(model_tweets.get_feature_names())\nl.append('pre_tweetat')\n\n\nprint(\"-\"*100)\nprint(\"Top 20 Positive features\")\nprint(\"-\"*100)\nfor i in pos_feat:\n    print(l[i])\n    \nprint(\"-\"*100)\nprint(\"Top 20 Negative features\")\nprint(\"-\"*100)\nfor j in neg_feat:\n    print(l[j])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T21:52:55.814912Z","iopub.execute_input":"2021-07-04T21:52:55.815218Z","iopub.status.idle":"2021-07-04T21:52:55.835353Z","shell.execute_reply.started":"2021-07-04T21:52:55.815192Z","shell.execute_reply":"2021-07-04T21:52:55.834551Z"},"trusted":true},"execution_count":null,"outputs":[]}]}