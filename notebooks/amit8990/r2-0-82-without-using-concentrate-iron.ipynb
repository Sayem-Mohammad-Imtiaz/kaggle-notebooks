{"cells":[{"metadata":{},"cell_type":"markdown","source":"Objective: We need to predict the silica in the product based on the feed charcteristics and the different process parameters. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Approach: We shall start with understanding the data and do some EDA. After that we shall use the simple linear regression model and move ahead with different models with an objective of acheiving higher r2 value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/quality-prediction-in-a-mining-process/MiningProcess_Flotation_Plant_Database.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment: We have 24 columns out of which 2 columsn % iron conc and % silica conc give the product quality and rest are feed characteristics and process parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the column for the dataset\ncolname=df.columns\ncolname\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correcting the integer formats\nfor col in colname:\n    df[col]=df[col].str.replace(',','.')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colname1=colname[1::]\ncolname1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting string columns to numeric depending on the column types based on the technical details\nfor col in colname1:\n    df[col]=df[col].apply(pd.to_numeric) \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outliers are checked for few numeric columns through Boxplots\n\nplt.figure(figsize=(3600,2600),dpi=300)\n\ndf.plot(kind='box')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a scatter plot to observe the distribution of silica % with time \nplt.figure(figsize=(20,5),dpi=100)\nsns.scatterplot(x=df['date'],y=df['% Silica Concentrate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop('date', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating function for outlier removal.\ndef remove_outlier(df_in, col_name): \n    Q1 = df_in[col_name].quantile(0.01) \n    Q3 = df_in[col_name].quantile(0.99) \n    IQR=Q3-Q1               # Interquantile range\n    df_out = df_in.loc[(df_in[col_name] >= (Q1 - 1.5 * IQR)) & (df_in[col_name] <= (Q3 + 1.5 * IQR))]\n    return(df_out)\n\n# Removing outliers in numeric columns\nfor i in colname1:\n    df= remove_outlier(df,i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colname2=colname1[:-1]\ncolname2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the correlation between dependent and independent variables\nfor col in colname2:\n    sns.pairplot(data=df,x_vars=col,y_vars=['% Silica Concentrate'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment: Apart from the % iron concentrate, we dont see much correlation with any other parameter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nsns.heatmap(df[colname1].corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing into X and Y sets for model building\n# Putting target variable to y\ny = df['% Silica Concentrate']\n\n# Putting feature variables to X\nX = df.drop(['% Silica Concentrate','% Iron Concentrate'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating an Standard Scaler object\nscaler = StandardScaler()\n\n# Scaling the numeric variables of train dataset\nX_train[X_train.columns]= scaler.fit_transform(X_train[X_train.columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the numeric variables of test dataset\nX_test[X_test.columns]= scaler.transform(X_test[X_test.columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n# Running RFE with the output no variable of 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 10)             # running RFE\nrfe = rfe.fit(X_train, y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)\nlm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking VIF for the above model:\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)\n\n# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_rfe.drop([\"Flotation Column 03 Air Flow\"], axis = 1)\nX_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\n\nlm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking VIF for the above model:\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Residual Analysis of the train data\ny_train_pred= lm.predict(X_train_lm)\nres=y_train - y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment: We observe that the error terms are normally distributed and mean is zero.  So we can conclude that fundamental assumtions of linear regression is complied here. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting for test values\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_test_pred = lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_pred_m', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_true=y_test,y_pred=y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Principal Component Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating PCA\npca = PCA(svd_solver='randomized', random_state=42)\n\n# Performing PCA\npca.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the principal components\npca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the variance explained by principal components\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scree plot for the explained variance\nvar_cumu = np.cumsum(pca.explained_variance_ratio_)\nfig = plt.figure(figsize=[7,3])\nplt.plot(range(1,len(var_cumu)+1), var_cumu)\nplt.vlines(x=10, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.90, xmax=140, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.ylabel(\"Cumulative variance explained\")\nplt.xlabel(\"Number of components\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Applying the PCA on the train set\n\n# Performing PCA with 28 components\npca_final = IncrementalPCA(n_components=10)\nX_train_pca = pca_final.fit_transform(X_train)\nX_train_pca.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the PCA transformation on the test set\nX_test_pca = pca_final.transform(X_test)\nX_test_pca.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## linear regression Model on Principal components data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_pca= sm.add_constant(X_train_pca)\nlm = sm.OLS(y_train,X_train_pca).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Comment: We dont see an increase in r2 value by applying PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Applying Random forest on the train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor\n\n\ncv_num =   5  #--> list of values\n\nparam={'max_depth': range(17,18,1)}\n\n\nrf= RandomForestRegressor(warm_start=True)\n\n\nmodel2 = GridSearchCV(estimator = rf, \n                        param_grid = param, \n                        scoring= 'r2', \n                        cv = cv_num, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel2.fit(X_train, y_train) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results2 = pd.DataFrame(model2.cv_results_)\ncv_results2 = cv_results2[cv_results2['param_max_depth']<=20]\ncv_results2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results2['param_max_depth'] = cv_results2['param_max_depth'].astype('int32')\n\n# plotting\nplt.plot(cv_results2['param_max_depth'], cv_results2['mean_train_score'])\nplt.plot(cv_results2['param_max_depth'], cv_results2['mean_test_score'])\nplt.xlabel('max_depth')\nplt.ylabel('Score')\nplt.title(\"Score and param_max_depth\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\nprint('We can get auc of',model2.best_score_,'using',model2.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment: The r2 value is 0.82 for a max depth of 17. The r2 value is reasable. We can explore more depth however, due to run time of the model, have restircted the run param of max depth of 17.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing XGboost classifier\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n\n# num_C = ______  #--> list of values\ncv_num =   5 #--> list of values\n\nparam={'learning_rate': [0.01, 0.1, 0.3, 0.5], \n             'subsample': [0.3, 0.6, 0.9]}\n\n\n# specify model\nxgb1= xgb.XGBRegressor(max_depth=2, n_estimators=200)\n\n\nmodel3 = GridSearchCV(estimator = xgb1, \n                        param_grid = param, \n                        scoring= 'r2', \n                        cv = cv_num, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel3.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results3 = pd.DataFrame(model3.cv_results_)\n\ncv_results3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # plotting\nplt.figure(figsize=(16,6))\n\nparam_grid = {'learning_rate': [0.01, 0.1, 0.3, 0.5], \n             'subsample': [0.3, 0.6, 0.9]} \n\nfor n, subsample in enumerate(param['subsample']):\n    \n    # subplot 1/n\n    plt.subplot(1,len(param['subsample']), n+1)\n    df = cv_results3[cv_results3['param_subsample']==subsample]\n\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n    plt.xlabel('learning_rate')\n    plt.ylabel('r2')\n    plt.title(\"subsample={0}\".format(subsample))\n    plt.ylim([0.4, 1])\n    plt.legend(['test score', 'train score'], loc='upper left')\n    plt.xscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\n\nprint('We can get r2 of',model3.best_score_,'using',model3.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment: We find that with randomforest we get the best r2 value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Test data on final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model=RandomForestRegressor(max_depth=17,warm_start=True)\nfinal_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the test set and the r2 value\ny_test_pred = final_model.predict(X_test)\nfrom sklearn.metrics import r2_score\nr2_score(y_true=y_test,y_pred=y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_test_pred', fontsize=20)         # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}