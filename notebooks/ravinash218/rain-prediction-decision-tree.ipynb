{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n%matplotlib inline\nimport os\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pyarrow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Dataset","metadata":{}},{"cell_type":"code","source":"raw_df = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\nraw_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove rows for which target column is empty","metadata":{}},{"cell_type":"code","source":"raw_df.dropna(subset=['RainTomorrow'], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train, Validation, Test Split","metadata":{}},{"cell_type":"code","source":"plt.title('No. of Rows Per Year');\nsns.countplot(x=pd.to_datetime(raw_df.Date).dt.year);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While working with chronological data, it's often a good idea to separate the training, validation and test sets with time, so that the model is trained on data from the past and evaluated on data from the future.\n\nWe'll use the data till 2014 for the training set, data from 2015 for the validation set, and the data from 2016 & 2017 for the test set.  ","metadata":{}},{"cell_type":"code","source":"year = pd.to_datetime(raw_df.Date).dt.year\n\ntrain_df = raw_df[year < 2015]\nval_df = raw_df[year == 2015]\ntest_df = raw_df[year > 2015]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify Inputs & Targets Columns","metadata":{}},{"cell_type":"code","source":"input_cols = list(train_df.columns[1:-1])\ntarget_cols = train_df.columns[-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_cols,target_cols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify Xs & Ys","metadata":{}},{"cell_type":"code","source":"X_train = train_df[input_cols].copy()\nY_train = train_df[target_cols].copy()\nX_val = val_df[input_cols].copy()\nY_val = val_df[target_cols].copy()\nX_test = test_df[input_cols].copy()\nY_test = test_df[target_cols].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identify Numerical & Categorical Columns","metadata":{}},{"cell_type":"code","source":"numeric_cols = list(X_train.select_dtypes(include=np.number).columns)\ncategorical_cols = list(X_train.select_dtypes(include='object').columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_cols, categorical_cols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Impute Missing Values","metadata":{}},{"cell_type":"code","source":"X_train[numeric_cols].isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='mean')\nimputer.fit(raw_df[numeric_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[numeric_cols] = imputer.transform(X_train[numeric_cols])\nX_val[numeric_cols] = imputer.transform(X_val[numeric_cols])\nX_test[numeric_cols] = imputer.transform(X_test[numeric_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[numeric_cols].isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling Numeric Features","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(raw_df[numeric_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[numeric_cols] = scaler.transform(X_train[numeric_cols])\nX_val[numeric_cols] = scaler.transform(X_val[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical Columns","metadata":{}},{"cell_type":"code","source":"categorical_cols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Note :***<br>\nFill Nans with 'Unknown' value in categorical columns","metadata":{}},{"cell_type":"code","source":"X_train[categorical_cols].isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[categorical_cols] = X_train[categorical_cols].fillna('Unknown')\nX_val[categorical_cols] = X_val[categorical_cols].fillna('Unknown')\nX_test[categorical_cols] = X_val[categorical_cols].fillna('Unknown')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[categorical_cols].isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nencoder.fit(X_train[categorical_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_cols = list(encoder.get_feature_names(categorical_cols))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_cols","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[encoded_cols] = encoder.transform(X_train[categorical_cols])\nX_val[encoded_cols] = encoder.transform(X_val[categorical_cols])\nX_test[encoded_cols] = encoder.transform(X_test[categorical_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[numeric_cols + encoded_cols]\nX_val = X_val[numeric_cols + encoded_cols]\nX_test = X_test[numeric_cols + encoded_cols]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training & Visualizing Decision Trees\nA decision tree in general parlance represents a hierarchical series of binary decisions:\n\nA decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually.","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier(random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"X_train_pred = model.predict(X_train)\nX_train_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.value_counts(X_train_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems prediction has more Nos.<br>\nThis is because the training set is also skewed","metadata":{}},{"cell_type":"code","source":"train_probs = model.predict_proba(X_train)\ntrain_probs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Accuracy :',accuracy_score(X_train_pred,Y_train)*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too. \n\nWe can make predictions and compute accuracy in one step using `model.score`","metadata":{}},{"cell_type":"code","source":"print('Validation Acuracy :',model.score(X_val,Y_val)*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_val.value_counts() / len(Y_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the training accuracy is 100%, the accuracy on the validation set is just about 79%, which is only marginally better then always predicting \"No\", i.e,predicting always 'No' also gives around 78.8 % accuracy.<br>\n\nThis is because of overfitting.<br>\n<b>Note :</b><br>\nDecision Trees tends to overfit.","metadata":{}},{"cell_type":"markdown","source":"## Visualization\nWe'll visualize the decision tree learned from training data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(80,50))\nplot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.tree_.max_depth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns))\nprint(tree_text)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\nDecision Trees can find iportance of features by itself.<br>\nBelow are thew importances of 119 features(total number of features in the training dataset)","metadata":{}},{"cell_type":"code","source":"model.feature_importances_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df = pd.DataFrame({\n    'Feature' : X_train.columns,\n    'Importance' : model.feature_importances_\n}).sort_values(by='Importance', ascending=False)\nfeature_importance_df","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Feature Importance')\nsns.barplot(data = feature_importance_df.head(20), x='Importance', y='Feature');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning To Reduce Overfitting\n\nThe `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting.<br>\n\n- `max_depth`\n- `max_leaf_nodes`","metadata":{}},{"cell_type":"markdown","source":"## max_depth\nBy reducing the tree maximum depth can reduce overfitting.<br>\nMaximum depth (default) is 48 which is reduced to 3 to reduce overfittting as below.","metadata":{}},{"cell_type":"code","source":"model.tree_.max_depth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(random_state=42, max_depth=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy in Training Dataset :',model.score(X_train, Y_train)*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy in Validation Dataset :',model.score(X_val, Y_val)*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualisation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(80,50))\nplot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns))\nprint(tree_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### max_depth Tuning","metadata":{}},{"cell_type":"markdown","source":"Since the max_depth value without manual constraint for which our model overfitted is 48.<br>\nAnd the max_depth value obviously can't be 0 (or lesser).<br>\nSo let's find what the best value of max_depth would be by trial and error method and use the max_depth for<br>\nwhich the errors of train and validation dataset is optimal.","metadata":{}},{"cell_type":"code","source":"def max_depth_accuracy1(max_depth_val):\n    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth_val)\n    model.fit(X_train, Y_train)\n    train_accuracy = model.score(X_train, Y_train)*100\n    val_accuracy = model.score(X_val, Y_val)*100\n    return {'Max_Depth' : max_depth_val, 'Training_Accuracy' : train_accuracy, 'Validation_Accuracy' : val_accuracy}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\naccuracies_df1 = pd.DataFrame([max_depth_accuracy1(i) for i in range(1,48)])\naccuracies_df1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save accuracies_df1 dataframe","metadata":{}},{"cell_type":"code","source":"accuracies_df1.to_parquet('Accuracies_max_depth_tuning1.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load saved accuracies_df1","metadata":{}},{"cell_type":"code","source":"accuracies_df1 = pd.read_parquet('Accuracies_max_depth_tuning1.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies_df1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the dataframe, it can be seen that the training accuracy increases with increase in max_depth.<br>\nIt is also to be noted that validation accuracy first increases and then decreases.<br>","metadata":{}},{"cell_type":"markdown","source":"##### Plotting Tuning Graph\nLet'us visualise the training accuracy and validation accuracy with different max_depths.<br>","metadata":{}},{"cell_type":"code","source":"plt.title('Training Accuracy Vs Validation Accuracy');\nplt.plot(accuracies_df1['Max_Depth'], accuracies_df1['Training_Accuracy']);\nplt.plot(accuracies_df1['Max_Depth'], accuracies_df1['Validation_Accuracy']);\nplt.legend(['Training Accuracy', 'Validation Accuracy']);\nplt.xticks(range(0,48, 2))\nplt.xlabel('Max Depth');\nplt.ylabel('Errors');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph it can also be seen that training accuracy increases with increase in max_depth<br>\nwhile validation accuracy first increases (till max_depth = 7) and then decreases.<br>\nTherefore, optimal max_depth is 7.","metadata":{}},{"cell_type":"markdown","source":"#### Buiild Decision Tree with max_depth = 7","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier(random_state=42, max_depth=7)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## max_leaf_nodes\nAnother way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. ","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.tree_.max_depth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the accuracies when max_depth was set to 12 while tuning max_depth parameter.<br>\nThey are not same because number of nodes in that case might be different.","metadata":{}},{"cell_type":"code","source":"accuracies_df1.loc[accuracies_df1['Max_Depth'] == model.tree_.max_depth]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_text = export_text(model, feature_names=list(X_train.columns))\nprint(model_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42, max_depth=6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_depth_accuracy2(max_depth_val):\n    model = DecisionTreeClassifier(random_state=42, max_depth=max_depth_val, max_leaf_nodes=128)\n    model.fit(X_train, Y_train)\n    train_accuracy = model.score(X_train, Y_train)*100\n    val_accuracy = model.score(X_val, Y_val)*100\n    return {'Max_Depth' : max_depth_val, 'Training_Accuracy' : train_accuracy, 'Validation_Accuracy' : val_accuracy}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\naccuracies_df2 = pd.DataFrame([max_depth_accuracy2(i) for i in range(1,14)])\naccuracies_df2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save accuracies_df2 dataframe","metadata":{}},{"cell_type":"code","source":"accuracies_df2.to_parquet('Accuracies_max_depth_tuning2.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load saved accuracies_df2","metadata":{}},{"cell_type":"code","source":"accuracies_df2 = pd.read_parquet('Accuracies_max_depth_tuning2.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Plotting Tuning Graph\nLet'us visualise the training accuracy and validation accuracy with different max_depths and max_leaf_nodes = 128.<br>","metadata":{}},{"cell_type":"code","source":"plt.title('Training Accuracy Vs Validation Accuracy');\nplt.plot(accuracies_df2['Max_Depth'], accuracies_df2['Training_Accuracy']);\nplt.plot(accuracies_df2['Max_Depth'], accuracies_df2['Validation_Accuracy']);\nplt.legend(['Training Accuracy', 'Validation Accuracy']);\nplt.xticks(range(0,16, 2))\nplt.xlabel('Max Depth');\nplt.ylabel('Errors');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems max_depth = 9 and max_leaf_nodes = 128 is the optimal hyperparameters","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth=9, max_leaf_nodes=128, random_state=42)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :', model.score(X_train,Y_train)*100)\nprint('Validation Accuracy :', model.score(X_val, Y_val)*100)","metadata":{},"execution_count":null,"outputs":[]}]}