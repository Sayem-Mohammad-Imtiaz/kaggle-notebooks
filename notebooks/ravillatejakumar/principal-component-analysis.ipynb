{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Principal Component  Analysis :"},{"metadata":{},"cell_type":"markdown","source":"As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. For example, let’s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? Yes, it’s approximately the line that matches the purple marks because it goes through the origin and it’s the line in which the projection of the points (red dots) is the most spread out. Or mathematically speaking, it’s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin)."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://builtin.com/sites/default/files/inline-images/Principal%20Component%20Analysis%20second%20principal.gif\">"},{"metadata":{},"cell_type":"markdown","source":"# Steps Involved in the PCA : \n\nStep 1: Standardize the dataset. \n    \nStep 2: Calculate the covariance matrix for the features in the dataset. \n    \nStep 3: Calculate the eigenvalues and eigenvectors for the covariance matrix. \n    \nStep 4: Sort eigenvalues and their corresponding eigenvectors. \n    \nStep 5: Pick k eigenvalues and form a matrix of eigenvectors. \n    \nStep 6: Transform the original matrix.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom numpy.linalg import eig \nfrom sklearn.decomposition import PCA \nimport plotly.express as px\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/class12/Class2.csv\") \ndf1 = pd.read_csv(\"../input/class12/Class2.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"id\",\"gender\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Standardize the Dataset :\nAssume we have the below dataset which has 4 features and a total of 100 training examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we need to standardize the dataset and for that, we need to calculate the mean and standard deviation for each feature."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/333/1*X4YeGxtzOhnnOWBfoBBJfA.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_std = df-df.mean()/df.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Create the covariance matrix :\n\nNext, we’ll create the covariance matrix for this dataset using the numpy function cov(), specifying that bias = True so that we are able to calculate the population covariance matrix. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cov()#covariance matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.var()#variance of matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Calculate eigen values and eigen vectors :  \n\nAn eigenvector is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled. \n       \nLet A be a square matrix (in our case the covariance matrix), ν a vector and λ a scalar that satisfies Aν = λν, then λ is called eigenvalue associated with eigenvector ν of A. \n    \nRearranging the above equation, \n    \n                         Aν-λν =0 ; (A-λI)ν = 0\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array(df).reshape(200,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values,vectors = eig(df.cov())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Sort eigenvalues and their corresponding eigenvectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"eigen_vectors = pd.DataFrame(vectors,columns=['e1','e2','e3','e4']) \neigen_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eigen_values = pd.DataFrame(values.reshape(1,4),columns=['test1','test2','test3','test4'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eigen_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Pick k eigenvalues and form a matrix of eigenvectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors = eigen_vectors[['e1','e2']].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Transform the original matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_components = pd.DataFrame(np.array(df)@np.array(vectors),columns=['pc1','pc2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = pca.fit_transform(df) \ncomponents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels =  pca.explained_variance_ratio_*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n} \nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(components[0:,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"component = pd.DataFrame(components,columns=['A','B','C','D']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_matrix(components,labels = labels,dimensions=range(4),color=df1['gender']) \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis : \n\nLinear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs. \n    \nBoth Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores” class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png\"></img>"},{"metadata":{},"cell_type":"markdown","source":"# Steps involved in the LDA approach :"},{"metadata":{},"cell_type":"markdown","source":"Listed below are the 5 general steps for performing a linear discriminant analysis; we will explore them in more detail in the following sections.\n\n\n1.Compute the d-dimensional mean vectors for the different classes from the dataset.\n    \n2.Compute the scatter matrices (in-between-class and within-class scatter matrix).\n    \n3.Compute the eigenvectors (ee1,ee2,...,eed) and corresponding eigenvalues (λλ1,λλ2,...,λλd) for the scatter matrices.\n    \n4.Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix W (where every column represents an eigenvector).\n    \n5.Use this d×k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: YY=XX×WW (where XX is a n×d-dimensional matrix representing the n samples, and yy are the transformed n×k-dimensional samples in the new subspace)."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://lh4.googleusercontent.com/6NfVmmG39n41HvHiER7x-mBs8sjIDtAZnzZdt4cBUVU2Jw4chLOVEgYs28eqFq2w6P3Ow2sSDpFFkJ3VwCfqcEEqs_lbkEhjPZ36hOu-gAh6adJ5kgSnVgCA0LzDrCP4WeIhXAM\">"},{"metadata":{},"cell_type":"markdown","source":"# Assumptions of LDA :   \nLDA assumes:\n\n1.Each feature (variable or dimension or attribute) in the dataset is a gaussian distribution. In other words, each feature in the dataset is shaped like a bell-shaped curve. \n\n2. Each feature has the same variance, the value of each feature varies around the mean with the same amount on average. \n\n\n3. Each feature is assumed to be randomly sampled.\n\n4. Lack of multicollinearity in independent features. Increase in correlations between independent features and the power of prediction decreases."},{"metadata":{},"cell_type":"markdown","source":"# **LDA TOPIC WILL BE UPDATED SOON...***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}