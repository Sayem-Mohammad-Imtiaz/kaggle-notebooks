{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:50.817243Z","iopub.execute_input":"2021-08-04T13:37:50.817685Z","iopub.status.idle":"2021-08-04T13:37:50.823451Z","shell.execute_reply.started":"2021-08-04T13:37:50.817588Z","shell.execute_reply":"2021-08-04T13:37:50.822362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\nprint(heart)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:50.829193Z","iopub.execute_input":"2021-08-04T13:37:50.829781Z","iopub.status.idle":"2021-08-04T13:37:50.882628Z","shell.execute_reply.started":"2021-08-04T13:37:50.829729Z","shell.execute_reply":"2021-08-04T13:37:50.881362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the description of the dataset\n\n\n1. age - age in years\n\n2. sex - sex (1 = male; 0 = female)\n\n3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\n\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\n5. chol - serum cholestoral in mg/dl\n\n6. fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n\n7. restecg - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)\n\n8. thalach - maximum heart rate achieved\n\n9. exang - exercise induced angina (1 = yes; 0 = no)\n\n10. oldpeak - ST depression induced by exercise relative to rest\n\n11. slope - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n\n12. ca - number of major vessels (0-3) colored by flourosopy\n\n13. thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n14. num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)","metadata":{}},{"cell_type":"markdown","source":"We want to predict who is at a greater chance of a heart disease. My hunch is that we will apply logistic regression since it is ideal for predicting binary outcomes. \n\nBefore, we do that however. Let's look at the relationship between our target/output variable that indicates chance of a heart disease and some of our continuous mesures first.","metadata":{}},{"cell_type":"code","source":"#We have four continuous predictors: age, resting blood pressure, cholestrol, and mximum heart rate. Let's look at how those are distributed across our outcome variable\n\n#Sort data into numpy\nage_data = [heart.age[heart['output']==x] for x in heart.output.unique()]\nbp_data = [heart.trtbps[heart['output']==x] for x in heart.output.unique()]\nchol_data = [heart.chol[heart['output']==x] for x in heart.output.unique()]\nhr_data = [heart.thalachh[heart['output']==x] for x in heart.output.unique()]\noldpeak_data = [heart.oldpeak[heart['output']==x] for x in heart.output.unique()]\n\n#Plot data\nfig1,ax1 = plt.subplots(2,3,figsize=(10, 6))\nax1[0,0].violinplot(age_data,heart.output.unique())\nax1[0,0].set_ylabel('Age')\nax1[0,1].violinplot(bp_data,heart.output.unique())\nax1[0,1].set_ylabel('Blood Pressure (mm Hg)')\nax1[0,2].violinplot(chol_data,heart.output.unique())\nax1[0,2].set_ylabel('Cholestrol (mg/dl)')\nax1[1,0].violinplot(hr_data,heart.output.unique())\nax1[1,0].set_ylabel('Heart Rate (beats/sec)')\nax1[1,1].violinplot(oldpeak_data,heart.output.unique())\nax1[1,1].set_ylabel('Old Peak')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:50.8864Z","iopub.execute_input":"2021-08-04T13:37:50.886997Z","iopub.status.idle":"2021-08-04T13:37:51.701341Z","shell.execute_reply.started":"2021-08-04T13:37:50.886915Z","shell.execute_reply":"2021-08-04T13:37:51.699846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above figures suggest that most of these measures do not substantially differ across our output variable. Interestingly it looks like heart rate differs the most. \n\nBefore we begin working with the data more directly, lets look at grouping these data using some of the built in functions of pandas, violin plots, and the seaborn package. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfig2,ax2 = plt.subplots(2,3,figsize=(10, 6))\nsns.violinplot(ax=ax2[0,0],x=\"output\", y=\"age\", hue=\"sex\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax2[0,1],x=\"output\", y=\"trtbps\", hue=\"sex\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax2[0,2],x=\"output\", y=\"chol\", hue=\"sex\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax2[1,0],x=\"output\", y=\"thalachh\", hue=\"sex\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax2[1,1],x=\"output\", y=\"oldpeak\", hue=\"sex\", data=heart, palette=\"Pastel1\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:51.70332Z","iopub.execute_input":"2021-08-04T13:37:51.703674Z","iopub.status.idle":"2021-08-04T13:37:53.843264Z","shell.execute_reply.started":"2021-08-04T13:37:51.70364Z","shell.execute_reply":"2021-08-04T13:37:53.841699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspection of these figures suggest that our distributions are fairly similar across sexes, which is surprising because we might think that men are at a greater risk for heart disease","metadata":{}},{"cell_type":"markdown","source":"There are five more predictors, all of which are either counts or labels coded in an ordinal fashion. A full exploration of the full factorial combination of these predictors with our continuous predictors would be exhausting. Instead, let's focus on heart rate since it appears elevated for individuals who are at a higher risk of a heat disease and see if it varies with any other predictors of interest. Specifically, I am thinking exercise, chest pain, and ecocardiogram. ","metadata":{}},{"cell_type":"code","source":"fig3,ax3 = plt.subplots(1,3,figsize=(20, 6))\nsns.violinplot(ax=ax3[0],x=\"output\", y=\"thalachh\", hue=\"exng\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax3[1],x=\"output\", y=\"thalachh\", hue=\"cp\", data=heart, palette=\"Pastel1\")\nsns.violinplot(ax=ax3[2],x=\"output\", y=\"thalachh\", hue=\"restecg\", data=heart, palette=\"Pastel1\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:53.845294Z","iopub.execute_input":"2021-08-04T13:37:53.845654Z","iopub.status.idle":"2021-08-04T13:37:54.650162Z","shell.execute_reply.started":"2021-08-04T13:37:53.84562Z","shell.execute_reply":"2021-08-04T13:37:54.648797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These graphs suggest, much like the previous ones, not a lot of variation across our categorical variables. Nonetheless, what's worth noting is that maximum heart rate is elevated across the board when someone is at risk of a heart disease. It may be particularly higher in those with abnormal ecocardiograms and chest pain but not exercise. ","metadata":{}},{"cell_type":"markdown","source":"Wtih these data visualized, I still think a logistic regression is the best approach.\n\nFirst, we need to set some of our data aside on which to test our model. ","metadata":{}},{"cell_type":"code","source":"#select training and test data sets\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(heart, test_size=0.3,random_state=42)\nprint(train_set.restecg.unique(), test_set.restecg.unique())","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:54.653211Z","iopub.execute_input":"2021-08-04T13:37:54.653534Z","iopub.status.idle":"2021-08-04T13:37:55.073117Z","shell.execute_reply.started":"2021-08-04T13:37:54.653503Z","shell.execute_reply":"2021-08-04T13:37:55.071993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start by z-scoring our continuous predictors","metadata":{}},{"cell_type":"code","source":"\ndef custom_zscore(data):\n    reshaped_data = data.to_numpy().reshape(-1,1)\n    zscore = (reshaped_data - np.mean(reshaped_data)) / np.std(reshaped_data)\n    return zscore\n\ntrain_set['zAge'] = custom_zscore(train_set.age)\ntrain_set['zTrtbps'] = custom_zscore(train_set.trtbps)\ntrain_set['zChol'] = custom_zscore(train_set.chol)\ntrain_set['zThalachh'] = custom_zscore(train_set.thalachh)\ntrain_set['zoldpeak'] = custom_zscore(train_set.oldpeak)\nprint(train_set)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:55.075325Z","iopub.execute_input":"2021-08-04T13:37:55.075838Z","iopub.status.idle":"2021-08-04T13:37:55.111471Z","shell.execute_reply.started":"2021-08-04T13:37:55.075799Z","shell.execute_reply":"2021-08-04T13:37:55.110284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets confirm that these four continuous variables have been z-scored","metadata":{}},{"cell_type":"code","source":"fig4,ax4 = plt.subplots(2,3,figsize=(10, 6))\nsns.violinplot(ax=ax4[0,0],x=\"output\", y=\"zAge\", data=train_set, palette=\"Pastel1\")\nsns.violinplot(ax=ax4[0,1],x=\"output\", y=\"zTrtbps\", data=train_set, palette=\"Pastel1\")\nsns.violinplot(ax=ax4[0,2],x=\"output\", y=\"zChol\", data=train_set, palette=\"Pastel1\")\nsns.violinplot(ax=ax4[1,0],x=\"output\", y=\"zThalachh\", data=train_set, palette=\"Pastel1\")\nsns.violinplot(ax=ax4[1,1],x=\"output\", y=\"zoldpeak\", data=train_set, palette=\"Pastel1\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:55.113078Z","iopub.execute_input":"2021-08-04T13:37:55.113446Z","iopub.status.idle":"2021-08-04T13:37:55.904221Z","shell.execute_reply.started":"2021-08-04T13:37:55.113411Z","shell.execute_reply":"2021-08-04T13:37:55.903028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that this figure looks much like the first one we produced, except now all of these data have the same scale.","metadata":{}},{"cell_type":"markdown","source":"Next, we will insert labels for our categorical predictors and then use sklearns dummy code labeler to create our dummy codes","metadata":{}},{"cell_type":"code","source":"#Let's use one-hot encoding to encode our categories with labels (cp, rest_ecg)\nfrom sklearn.preprocessing import OneHotEncoder\ncpcat_encoder = OneHotEncoder()\nresetecgcat_encoder = OneHotEncoder()\n\ntrain_set.loc[train_set['cp'] == 0,'cp'] = 'Typical'\ntrain_set.loc[train_set['cp'] == 1,'cp'] = 'Atypical'\ntrain_set.loc[train_set['cp'] == 2,'cp'] = 'Non-anginal'\ntrain_set.loc[train_set['cp'] == 3,'cp'] = 'Asymptomatic'\n\ntrain_set.loc[train_set['restecg'] == 0,'restecg'] = 'Normal'\ntrain_set.loc[train_set['restecg'] == 1,'restecg'] = 'ST-T'\ntrain_set.loc[train_set['restecg'] == 2,'restecg'] = 'LeftVent'\n\ntrain_set.loc[train_set['slp'] == 0,'slp'] = 'UpSlope'\ntrain_set.loc[train_set['slp'] == 1,'slp'] = 'FlatSlope'\ntrain_set.loc[train_set['slp'] == 2,'slp'] = 'DownSlope'\n\ntrain_set.loc[train_set['thall'] == 1,'thall'] = 'normal'\ntrain_set.loc[train_set['thall'] == 2,'thall'] = 'fixed'\ntrain_set.loc[train_set['thall'] == 3,'thall'] = 'defect'\n\ncp_onehot = cpcat_encoder.fit_transform(train_set.cp.values.reshape(-1,1))\nrestecg_onehot = resetecgcat_encoder.fit_transform(train_set.restecg.values.reshape(-1,1))\nslp_onehot = resetecgcat_encoder.fit_transform(train_set.restecg.values.reshape(-1,1))\nthall_onehot = resetecgcat_encoder.fit_transform(train_set.restecg.values.reshape(-1,1))\n\ncp_dummy = cp_onehot.toarray()\nrestecg_dummy = restecg_onehot.toarray()\nslp_dummy = slp_onehot.toarray()\nthall_dummy = thall_onehot.toarray()\n\n\n#Pass data back into dataframe\ntrain_set.insert(1,'cp0',cp_dummy[:,0])\ntrain_set.insert(1,'cp1',cp_dummy[:,1])\ntrain_set.insert(1,'cp2',cp_dummy[:,2])\ntrain_set.insert(1,'cp3',cp_dummy[:,3])\n\ntrain_set.insert(1,'re0',restecg_dummy[:,0])\ntrain_set.insert(1,'re1',restecg_dummy[:,1])\ntrain_set.insert(1,'re2',restecg_dummy[:,2])\n\ntrain_set.insert(1,'slp0',slp_dummy[:,0])\ntrain_set.insert(1,'slp1',slp_dummy[:,1])\ntrain_set.insert(1,'slp2',slp_dummy[:,2])\n\ntrain_set.insert(1,'thall0',thall_dummy[:,0])\ntrain_set.insert(1,'thall1',thall_dummy[:,1])\ntrain_set.insert(1,'thall2',thall_dummy[:,2])\nprint(train_set)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:55.905675Z","iopub.execute_input":"2021-08-04T13:37:55.905993Z","iopub.status.idle":"2021-08-04T13:37:55.975794Z","shell.execute_reply.started":"2021-08-04T13:37:55.905963Z","shell.execute_reply":"2021-08-04T13:37:55.974139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will drop all the original columns for which we now have transformed data, followed by fitting a CrossValidated Logistic Regression model. \n\nTo measure its performance I computed the probability of a hit, correct rejection, miss and false alarm. We want to see that the hit and correct rejection rate are both substantially greater than the miss and false alarm rate. ","metadata":{}},{"cell_type":"code","source":"#  drop non-transformed vars\ncleaned_train_set = train_set.drop(['age', 'cp', 'trtbps', 'restecg', 'chol', 'thalachh', 'oldpeak', 'slp', 'thall'],axis=1)\nprint(cleaned_train_set)\n\nfrom sklearn.linear_model import LogisticRegressionCV\n\n\nX = cleaned_train_set.drop(['output'],axis=1)\ny = cleaned_train_set.output\nclf = LogisticRegressionCV(cv=5,random_state=42).fit(X,y)\nprint(clf.score(X,y))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:55.97771Z","iopub.execute_input":"2021-08-04T13:37:55.978204Z","iopub.status.idle":"2021-08-04T13:37:56.447991Z","shell.execute_reply.started":"2021-08-04T13:37:55.978157Z","shell.execute_reply":"2021-08-04T13:37:56.446686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model seems to have done a good job. Our hit and correct rejection rates are around 80 % and our miss and false alarm rates are less than 20%\n\nNow let's do the same transform on our test data and see how the model does. ","metadata":{}},{"cell_type":"code","source":"#transform and clean test set\n\n#z-score\ntest_set['zAge'] = custom_zscore(test_set.age)\ntest_set['zTrtbps'] = custom_zscore(test_set.trtbps)\ntest_set['zChol'] = custom_zscore(test_set.chol)\ntest_set['zThalachh'] = custom_zscore(test_set.thalachh)\ntest_set['zoldpeak'] = custom_zscore(test_set.oldpeak)\n\n#one hot encoding\n\ntest_set.loc[test_set['cp'] == 0,'cp'] = 'Typical'\ntest_set.loc[test_set['cp'] == 1,'cp'] = 'Atypical'\ntest_set.loc[test_set['cp'] == 2,'cp'] = 'Non-anginal'\ntest_set.loc[test_set['cp'] == 3,'cp'] = 'Asymptomatic'\n\ntest_set.loc[test_set['restecg'] == 0,'restecg'] = 'Normal'\ntest_set.loc[test_set['restecg'] == 1,'restecg'] = 'ST-T'\ntest_set.loc[test_set['restecg'] == 2,'restecg'] = 'LeftVent'\n\ntest_set.loc[test_set['slp'] == 0,'slp'] = 'UpSlope'\ntest_set.loc[test_set['slp'] == 1,'slp'] = 'FlatSlope'\ntest_set.loc[test_set['slp'] == 2,'slp'] = 'DownSlope'\n\ntest_set.loc[test_set['thall'] == 1,'thall'] = 'normal'\ntest_set.loc[test_set['thall'] == 2,'thall'] = 'fixed'\ntest_set.loc[test_set['thall'] == 3,'thall'] = 'defect'\n\ncp_onehot = cpcat_encoder.fit_transform(test_set.cp.values.reshape(-1,1))\nrestecg_onehot = resetecgcat_encoder.fit_transform(test_set.restecg.values.reshape(-1,1))\nslp_onehot = resetecgcat_encoder.fit_transform(test_set.restecg.values.reshape(-1,1))\nthall_onehot = resetecgcat_encoder.fit_transform(test_set.restecg.values.reshape(-1,1))\n\ncp_dummy = cp_onehot.toarray()\nrestecg_dummy = restecg_onehot.toarray()\nslp_dummy = slp_onehot.toarray()\nthall_dummy = thall_onehot.toarray()\n\n\n#Pass data back into dataframe\ntest_set.insert(1,'cp0',cp_dummy[:,0])\ntest_set.insert(1,'cp1',cp_dummy[:,1])\ntest_set.insert(1,'cp2',cp_dummy[:,2])\ntest_set.insert(1,'cp3',cp_dummy[:,3])\n\ntest_set.insert(1,'re0',restecg_dummy[:,0])\ntest_set.insert(1,'re1',restecg_dummy[:,1])\ntest_set.insert(1,'re2',restecg_dummy[:,2])\n\ntest_set.insert(1,'slp0',slp_dummy[:,0])\ntest_set.insert(1,'slp1',slp_dummy[:,1])\ntest_set.insert(1,'slp2',slp_dummy[:,2])\n\ntest_set.insert(1,'thall0',thall_dummy[:,0])\ntest_set.insert(1,'thall1',thall_dummy[:,1])\ntest_set.insert(1,'thall2',thall_dummy[:,2])\n\n#drop what's no longer needed\ncleaned_test_set = test_set.drop(['age', 'cp', 'trtbps', 'restecg', 'chol', 'thalachh', 'oldpeak', 'slp', 'thall'],axis=1)\nprint(cleaned_test_set)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:56.450496Z","iopub.execute_input":"2021-08-04T13:37:56.450978Z","iopub.status.idle":"2021-08-04T13:37:56.524908Z","shell.execute_reply.started":"2021-08-04T13:37:56.450928Z","shell.execute_reply":"2021-08-04T13:37:56.523197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test model\nX_test = cleaned_test_set.drop(['output'],axis=1)\ny_test = cleaned_test_set.output\n\nprint(clf.score(X_test,y_test))\n\n#\nfrom sklearn.metrics import f1_score\ny_pred = clf.predict(X_test)\nprint(f1_score(y_test,y_pred))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:56.5266Z","iopub.execute_input":"2021-08-04T13:37:56.526932Z","iopub.status.idle":"2021-08-04T13:37:56.543328Z","shell.execute_reply.started":"2021-08-04T13:37:56.5269Z","shell.execute_reply":"2021-08-04T13:37:56.541955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:37:56.54471Z","iopub.execute_input":"2021-08-04T13:37:56.545015Z","iopub.status.idle":"2021-08-04T13:37:56.558678Z","shell.execute_reply.started":"2021-08-04T13:37:56.544985Z","shell.execute_reply":"2021-08-04T13:37:56.557352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall the model's performance generalizes fairly well to the test set, with little indication of a systematic error in our modeling exercise. \n\nWe also see that the number of true positives and true negatives is substantially higher than the false negatives and false positives. ","metadata":{}}]}