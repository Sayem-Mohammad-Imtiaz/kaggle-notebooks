{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Attack Analysis & Prediction Dataset\n## A dataset for heart attack classification","metadata":{}},{"cell_type":"markdown","source":"### About this dataset\n* Age : Age of the patient\n* Sex : Sex of the patient\n* exang: exercise induced angina (1 = yes; 0 = no)\n* ca: number of major vessels (0-3)\n* cp : Chest Pain type chest pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* rest_ecg : resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n* thalach : maximum heart rate achieved\n* output : 0= less chance of heart attack 1= more chance of heart attack","metadata":{}},{"cell_type":"code","source":"# Import libraries \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evalution\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\n\n# Deep Copy\ndf = data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.output.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heart Disease frequency according to Sex","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df.sex, df.output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df.sex, df.output).plot(kind='bar', figsize=(10, 6), color=['lightblue', 'salmon'])\n\nplt.title('Heart Disease frequency for Sex')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Female', 'Male'])\nplt.xlabel('0 = Not Disease, 1 = Disease')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age vs Max Heart Rate for Heart Disease","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.scatterplot(x=df.age, y=df.thalachh, hue=df.output, alpha=0.5)\nplt.title('Heart Disease Age vs Max Heart Rate')\nplt.ylabel('Max Heart Rate')\nplt.xlabel('Age')\nplt.legend(['Not disease', 'Disease'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heart Desease Frequency per Chest Pain Type\n\n* cp : Chest Pain type chest pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df.cp, df.output).plot(kind='bar', figsize=(10,8), color=['lightblue', 'salmon'])\nplt.ylabel('Frequency')\nplt.xlabel('Chest Pain Type')\nplt.title('Heart Disease Chest Pain Frequency')\nplt.legend(['Note Disease', 'Disease'])\nplt.xticks(rotation=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.heatmap(data=df.corr(), annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"# Split Data\nx = df.drop(columns=['output'])\n\ny = df['output']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model selection","metadata":{}},{"cell_type":"code","source":"model_selection = {'Logistic Regression': LogisticRegression(),\n                   'K Neighbors Classifier': KNeighborsClassifier(),\n                   'Random Forest Classifier': RandomForestClassifier()}\n\ndef fit_and_score(x_train, x_test, y_train, y_test):\n    # Make empty Score dic\n    score = {}\n    # Model fit\n    for name, model in model_selection.items():\n        # Fit the data in to model\n        model.fit(x_train, y_train)\n        # save the score \n        score[name] = model.score(x_test, y_test)\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_scores = fit_and_score(x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n\nmodel_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n\nmodel_compare.T.plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For improve accuracy following steps :\n* Hypyterparameter tuning\n* feature importance\n* Confusion Matrix\n* Cross Validation\n* Precision\n* Recall\n* F1 score\n* Classification Report\n* ROC Curve\n* Area under the curve (AUC)\n\n### HyperParameters Tuning (by hand)","metadata":{}},{"cell_type":"code","source":"# KNN tune\ntrain_score = []\ntest_score = []\n\n# Create a list of different values for K-Neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instant\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(x_train, y_train)\n    \n    # update training score list\n    train_score.append(knn.score(x_train, y_train))\n    \n    # update test score list\n    test_score.append(knn.score(x_test, y_test))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nsns.lineplot(x=neighbors, y=train_score)\nsns.lineplot(x=neighbors, y=test_score)\nplt.xticks(np.arange(1,21))\nplt.xlabel('Different values of Neighbors')\nplt.ylabel('Model Score')\nplt.legend(['train_score', 'test_score'])\n\nprint(f'Maximum KNN Score on the test data:{max(test_score)*100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tunning with RandomizedSearchCV\n\nNow tunning following:\n* Logistic Regression()\n* Random Forest Classifier()\n\n... using RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"# Create a hyperparameter grid for Logistic Regression\nlog_reg_grid = {'C': np.logspace(-4, 4, 20),\n                'solver': ['liblinear']}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {'n_estimators': np.arange(10, 1000, 50),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split': np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now hyperparameter grids setup for each models ansd tune them using RandomizedSearchCV...","metadata":{}},{"cell_type":"code","source":"# setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True, \n                                random_state=42)\n\n# fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the best hyperparameters\nrs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the randomized search randomforestClassifier model\nrs_log_reg.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now tuned LogisticRegression(), Let's do the same for RandomForestClassifer()...","metadata":{}},{"cell_type":"code","source":"# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True,\n                           random_state=42)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the best hyperparameters\nrs_rf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the randomized search randomforestClassifier model\nrs_rf.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning with GridSearchCV\n\nLogisticRegression model provides the best scores so far,  therefore improve again using GridSearchCV...","metadata":{}},{"cell_type":"code","source":"# Diffrent hyperparameters for our LogisticRegression model\nlog_reg_grid = {'C': np.logspace(-4,4,30),\n                'solver': ['liblinear']}\n\n# setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid= log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the best hyperparameters\ngs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluting our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* classification report\n* Precision\n* Recall\n* F1-Score\n\n....and it would be great it cross-validaton was used where possible\n\nto make comaparistions and evaluate our trained model, first we need to make predictions.","metadata":{}},{"cell_type":"code","source":"# make predictions with tuned model\ny_preds = gs_log_reg.predict(x_test)\n\ny_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(gs_log_reg, x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix\nconfusion_matrix(y_test, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cbar=False)\nplt.xlabel('True Value')\nplt.ylabel('Predicted Value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate evaluation metrics using cross-validation\n\ncalculate accuracy, precision, recall and f1-score model using cross-validation and `cross_val_score()`.","metadata":{}},{"cell_type":"code","source":"# Check best hyperparameters \ngs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n\ncv_acc = np.mean(cv_acc)\ncv_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated precision\ncv_precision = cross_val_score(clf, x, y, cv=5, scoring='precision')\n\ncv_precision = np.mean(cv_precision)\ncv_precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated recall\ncv_recall = cross_val_score(clf, x, y, cv=5, scoring='recall')\n\ncv_recall = np.mean(cv_recall)\ncv_recall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated f1\ncv_f1 = cross_val_score(clf, x, y, cv=5, scoring='f1')\n\ncv_f1 = np.mean(cv_f1)\ncv_f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({'Accuracy': cv_acc,\n                           'Precision': cv_precision,\n                           'Recall': cv_recall,\n                           'F1': cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title='Cross-validated classification metrics', legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear')\n\nclf.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check coef_\nclf.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title='Feature Importance', legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"x_raw = df.drop(columns=['output'])\ny_raw = df['output']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Data Empty Dataframe\ninput_ = {'age':0, 'sex':0, 'cp':0, 'trtbps':0, 'chol':0, 'fbs':0, 'restecg':0, 'thalachh':0, 'exng':0, 'oldpeak':0, \n     'slp':0, 'caa':0, 'thall':0}\noutput = {'Logistic Regression': 0, 'K Neighbors Classifier': 0, 'Random Forest Classifier': 0}\n\n# Create Data Empty Dataframe\nfinal = {'age':0, 'sex':0, 'cp':0, 'trtbps':0, 'chol':0, 'fbs':0, 'restecg':0, 'thalachh':0, 'exng':0, 'oldpeak':0, \n     'slp':0, 'caa':0, 'thall':0, 'Logistic Regression': 0, 'K Neighbors Classifier': 0, 'Random Forest Classifier': 0}\ndata_final = pd.DataFrame(columns=['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', \n                                   'slp', 'caa', 'thall', 'Logistic Regression', 'K Neighbors Classifier', \n                                   'Random Forest Classifier'])\n\n\ndef input_data():\n    # Make temporary dataframe\n    temp = pd.DataFrame(columns=['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', \n                                   'oldpeak', 'slp', 'caa', 'thall'])\n    \n    # Make some intruction\n    intruction = {\"age\": \"Age: age in years\",\n                  \"sex\": \"Sex: sex (1 = male; 0 = female)\",\n                  \"cp\": \"\"\"Chest Pain: chest pain type\n-- Value 0: typical angina\n-- Value 1: atypical angina\n-- Value 2: non-anginal pain\n-- Value 3: asymptomatic\"\"\",\n                  \"trtbps\": \"Trest_bps: resting blood pressure (in mm Hg on admission to the hospital)\",\n                  \"chol\": \"Cholestoral: serum cholestoral in mg/dl\",\n                  \"fbs\": \"Fasting Blood Sugar: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\",\n                  \"restecg\": '''Resting Electrocardiographic Results:\n-- Value 0: normal\n-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria''',\n                  \"thalachh\": \"Thalach: maximum heart rate achieved\",\n                  \"exng\": \"Ex_Ang: exercise induced angina (1 = yes; 0 = no)\",\n                  \"oldpeak\": \"Old_Peak: ST depression induced by exercise relative to rest\",\n                  \"slp\": \"Slope: the slope of the peak exercise ST segment\",\n                  \"caa\": \"CA: number of major vessels (0-3) colored by flourosopy\",\n                  \"thall\": \"Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\",\n                  \"output\": \"\"\"Output: diagnosis of heart disease (angiographic disease status)\n-- Value 0: < 50% diameter narrowing\n-- Value 1: > 50% diameter narrowing\"\"\"}\n    \n    # Input your data\n    for name, val in input_.items():\n        print(intruction[name])\n        input_[name] = input(f'{name} : ')\n    \n    # Input all data into dataframe\n    temp = temp.append(input_, ignore_index=True)\n    \n    # Conver all value into float\n    temp = temp.astype(np.float64)\n    \n    # Make Model \n    models = {'Logistic Regression': LogisticRegression(),\n              'K Neighbors Classifier': KNeighborsClassifier(),\n              'Random Forest Classifier': RandomForestClassifier()}\n    \n    # Set random seed\n    np.random.seed(42)\n    # Model fit and pridict optput\n    for name, model in models.items():\n        model.fit(x_raw, y_raw)  # fit the model\n        \n        # predict value\n        y_preds = model.predict(temp)\n#         if name == 'K Neighbors Classifier':\n#             y_preds = model.predict_proba(temp)\n#         else:\n#             y_preds = model.predict_log_proba(temp)\n            \n        output[name] = y_preds\n    \n    # save data in final\n    for name, val in input_.items():\n        final[name] = input_[name]\n    for name, val in output.items():\n        final[name] = output[name]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_final = data_final.append(final, ignore_index=True)\n\n# data_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}