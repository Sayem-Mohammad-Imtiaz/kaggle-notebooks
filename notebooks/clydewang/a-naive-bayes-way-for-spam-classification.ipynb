{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"71937b91-6de7-bfcf-0865-0f0157d3bce6"},"source":"**A Naive Bayes Way for Spam Classification using Tabu List**\n=============================================================\nImplemented by Clyde Wang, Feb, 17, 2017\n\nThis spam classifier is implemented by Naive Bayes Model, a simple but very efficient solution in spam classification problem. In brief, Naive Bayes treats every features independent from each other, making inference very efficient. You can refer to [Naive Bayes][1] for more details. This article briefly introduces the process of the selection of tabu list and building the learning algorithm. I hope you will enjoy it.\n\nThis code runs quite well with the accuracy of 98.31% on training sample and 97.81% on test sample.\nI hope someone could improve it and enhance its performance. And Here we go!\n\n  [1]: https://en.wikipedia.org/wiki/Naive_Bayes_classifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"f0e046f7-a766-6dfa-f05c-3d8ba49f1e2a"},"source":"## 1. Environment Setting ##\nBefore we start we need set up the environment, please make sure those packages are installed in your computer when you copy this code to the local file."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"100e1654-dc9c-3ff7-51b9-4ca782dfbbfc"},"outputs":[],"source":"#coding:utf-8\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.naive_bayes import BernoulliNB"},{"cell_type":"markdown","metadata":{"_cell_guid":"b98cf320-7a0c-a003-dce5-b2f794244212"},"source":"2. Read Data File\n-----------------\nThe first task is to read data from .csv file, and here we could take advantage of the `pandas.DataFrame` to store and process the raw data. (see [here][1]) Notice that we have to split our raw data into two parts in order to test the generalisation ability of our model. Here is the code:\n\n\n  [1]: http://pandas.pydata.org/pandas-docs/stable/10min.html"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b46f245-3cb2-7cea-6d32-132cf6b7108f"},"outputs":[],"source":"def readData():\n\tSMS_df = pd.read_csv('../input/spam.csv',usecols=[0,1],encoding='latin-1')\n\tSMS_df.columns=['label','content']\n\tn = int(SMS_df.shape[0])\n    # split into training data and test data\n\treturn SMS_df.iloc[:int(n/2)], SMS_df.iloc[int(n/2):]"},{"cell_type":"markdown","metadata":{"_cell_guid":"b25b31ab-b108-f232-5fc8-47d2449c6b55"},"source":"3. Generate a Tabu List\n--------------------\nA **tabu list** is a list of those significant indicators of spam SMS. Here we select TF-IDF as the principle of list generation.\n\nTerm Frequency(TF) is the frequency of a word in a certain kind of document. If there is a article of 50 words with 2 'data' in it, then the TF of the 'data' is given by `2/50=0.01`.\n\nHowever, there are some words of high frequency in English, like 'a', 'is', 'are', etc. We have to remove those words from our list. And here comes the IDF.\n\nInverse Document Frequency(IDF) is the indicator to reflect how important a word is related to some certain topic. It is given by `log(#total articles/#articles containing w)`, for example, if we have 5 articles, only one has word 'gene' of term frequency of 0.002, but all the five articles contains the word 'technology' of term frequency of 0.5, the IDF of 'gene' is `log(5/1)>0` but  the IDF of 'technology' is `log(5/5)=0`.\n\nTF-IDF is the product of TF and IDF: in the example above, `TFIDF('gene')=0.002*log(5/1)>0` while `TFIDF('technology')=0.5*log(5/5)=0`, so in this case, 'gene' is a better indicator than 'technology'.\n\nIn my code, I compute the TF-IDF of each word for 'spam' and 'ham', and compute the difference between them, thus I can select the words which are the most representative for the 'spam' class.\n\nHere is the code:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7a4eada-c2cc-e24f-4c82-39f4ba8739e4"},"outputs":[],"source":"def generate_tabu_list(path, tabu_size=200,ignore=3):\n\ttrain_df,_ = readData()\n\tspam_TF_dict = dict()\n\tham_TF_dict = dict()\n\tIDF_dict = dict()\n\n\t# ignore all other than letters.\n\tfor i in range(train_df.shape[0]):\n\t\tfinds = re.findall('[A-Za-z]+', train_df.iloc[i].content)\n\t\tif train_df.iloc[i].label == 'spam':\n\t\t\tfor find in finds:\n\t\t\t\tif len(find)<ignore: continue\n\t\t\t\tfind = find.lower()\n\t\t\t\ttry:\n\t\t\t\t\tspam_TF_dict[find] = spam_TF_dict[find] + 1\n\t\t\t\texcept:\t\n\t\t\t\t\tspam_TF_dict[find] = spam_TF_dict.get(find,1)\n\t\t\t\t\tham_TF_dict[find] = ham_TF_dict.get(find,0)\n\t\telse:\n\t\t\tfor find in finds:\n\t\t\t\tif len(find)<ignore: continue\n\t\t\t\tfind = find.lower()\n\t\t\t\ttry:\n\t\t\t\t\tham_TF_dict[find] = ham_TF_dict[find] + 1\n\t\t\t\texcept:\t\n\t\t\t\t\tspam_TF_dict[find] = spam_TF_dict.get(find,0)\n\t\t\t\t\tham_TF_dict[find] = ham_TF_dict.get(find,1)\n\t\t\n\t\tword_set = set()\n\t\tfor find in finds:\n\t\t\tif len(find)<ignore: continue\n\t\t\tfind = find.lower()\n\t\t\tif not(find in word_set):\n\t\t\t\ttry:\n\t\t\t\t\tIDF_dict[find] = IDF_dict[find] + 1\n\t\t\t\texcept:\t\n\t\t\t\t\tIDF_dict[find] = IDF_dict.get(find,1)\n\t\t\tword_set.add(find)\n\n\tword_df = pd.DataFrame(list(zip(ham_TF_dict.keys(),ham_TF_dict.values(),spam_TF_dict.values(),IDF_dict.values())))\n\tword_df.columns = ['keyword','ham_TF','spam_TF','IDF']\n\tword_df['ham_TF'] = word_df['ham_TF'].astype('float')/train_df[train_df['label']=='ham'].shape[0]\n\tword_df['spam_TF'] = word_df['spam_TF'].astype('float')/train_df[train_df['label']=='spam'].shape[0]\n\tword_df['IDF'] = np.log10(train_df.shape[0]/word_df['IDF'].astype('float'))\n\tword_df['ham_TFIDF'] = word_df['ham_TF']*word_df['IDF']\n\tword_df['spam_TFIDF'] = word_df['spam_TF']*word_df['IDF']\n\tword_df['diff']=word_df['spam_TFIDF']-word_df['ham_TFIDF']\n\n\tselected_spam_key = word_df.sort_values('diff',ascending=False)\n\n\tprint('>>>Generating Tabu List...\\n  Tabu List Size: {}\\n  File Name: {}\\n  The words shorter than {} are ignored by model\\n'.format(tabu_size, path, ignore))\n\tfile = open(path,'w')\n\tfor word in selected_spam_key.head(tabu_size).keyword:\n\t\tfile.write(word+'\\n')\n\tfile.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"32ea7bd3-f5c1-c7f5-19b8-677ec5dd6209"},"source":"4. Read Tabu List and Convert SMS\n-----------------------------------\nSince the message is of variant length, it is not easy for the implementation of learning algorithm. So we define a Function above generating tabu list and storing them in the local file. And we can use this file to convert a SMS expressed in string to a vector of fixed length expressed in binary value.\n\nThe idea is given like this: If we have a tabu list then we could find those word in the list and represent them by a index. Thus a string can be converted to an array of int. Further, we could define an array filled with zeros with the same length of tabu list. if this str contains the word in the tabu list, we could assign 1 to the corresponding element of the array representing 'message contains word w'. (tips: the query of `python.dict` is of constant time, much faster than `python.list`)\n\nBy taking this step, we could convert our raw data of variant length into the numeric data of fixed length.\n\nThese two function is given below:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46534282-54e9-4786-aabb-45df63148b66"},"outputs":[],"source":"def read_tabu_list():\n\tfile = open('tabu.txt','r')\n\tkeyword_dict = dict()\n\ti = 0\n\tfor line in file:\n\t\tkeyword_dict.update({line.strip():i})\n\t\ti+=1\n\treturn keyword_dict\n\ndef convert_Content(content, tabu):\n\tm = len(tabu)\n\tres = np.int_(np.zeros(m))\n\tfinds = re.findall('[A-Za-z]+', content)\n\tfor find in finds:\n\t\tfind=find.lower()\n\t\ttry:\n\t\t\ti = tabu[find]\n\t\t\tres[i]=1\n\t\texcept:\n\t\t\tcontinue\n\treturn res"},{"cell_type":"markdown","metadata":{"_cell_guid":"3a6fb48a-b327-cc67-bfe7-ee8dded0202b"},"source":"5. Learning, Testing and Predicting\n-----------------------------------\nAfter we generate our tabu list and those supporting functions, we are now well prepared for the learning part in this problem. And here we could use the library `from sklearn.naive_bayes import BernoulliNB`. It will help us train this model.\n\nBefore this part, let review our data: our feature input X is a n*m matrix, where X[i,j] = 1 means the sample #i contains the word j in the tabu list, and supervised label Y is a n*1 vector where Y[i] = 1 representing for a spam and 0 for a ham.\n\nLet prepare the materials for the learning algorithm."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35051cb6-93f5-cc38-bab3-c94b1fb5ed91"},"outputs":[],"source":"def learn():\n\tglobal tabu, m\n\ttrain,_ = readData()\n\tn = train.shape[0]\n\tX = np.zeros((n,m)); Y=np.int_(train.label=='spam')\n\tfor i in range(n):\n\t\tX[i,:] = convert_Content(train.iloc[i].content, tabu)\n\n\tNaiveBayes = BernoulliNB()\n\tNaiveBayes.fit(X, Y)\n\n\tY_hat = NaiveBayes.predict(X)\n\tprint('>>>Learning...\\n  Learning Sample Size: {}\\n  Accuarcy (Training sample): {:.2f}％\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n\treturn NaiveBayes"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f8040c8-7cc3-4db9-000e-58ae145ce898"},"source":"The Function above returns a well trained Naive Bayes Model object, and we could use it to make prediction."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c98127ed-e089-9edd-eae0-97828efaad31"},"outputs":[],"source":"def test(NaiveBayes):\n\tglobal tabu, m\n\t_,test = readData()\n\tn = test.shape[0]\n\tX = np.zeros((n,m)); Y=np.int_(test.label=='spam')\n\tfor i in range(n):\n\t\tX[i,:] = convert_Content(test.iloc[i].content, tabu)\n\tY_hat = NaiveBayes.predict(X)\n\tprint ('>>>Cross Validation...\\n  Testing Sample Size: {}\\n  Accuarcy (Testing sample): {:.2f}％\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n\treturn\n\ndef predictSMS(SMS):\n\tglobal NaiveBayes, tabu, m\n\tX = convert_Content(SMS, tabu)\n\tY_hat = NaiveBayes.predict(X.reshape(1,-1))\n\tif int(Y_hat) == 1:\n\t\tprint ('SPAM: {}'.format(SMS))\n\telse:\n\t\tprint ('HAM: {}'.format(SMS))"},{"cell_type":"markdown","metadata":{"_cell_guid":"52bc3c19-af79-ece6-0379-a74c4b064e41"},"source":"6. Overall Assembly\n-------------------\nAfter we define the every modules we need in this problem, we could integrate them into a whole part."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4b028ff-859f-0977-2bc0-804ee5364504"},"outputs":[],"source":"print('UCI SMS SPAM CLASSIFICATION PROBLEM SET\\n  -- implemented by Bernoulli Naive Bayes Model\\n')\ntabu_file = 'tabu.txt'          # user defined tabu file\ntabu_size = 300                 # how many features are used to classify spam\nword_len_ignored = 3            # ignore those words shorter than this variable\n# build a tabu list based on the training data\ngenerate_tabu_list(tabu_file,tabu_size, word_len_ignored)\n\ntabu = read_tabu_list()\nm = len(tabu)\n# train the Naive Bayes Model using training data\nNaiveBayes=learn()\n# Test Model using testing data\ntest(NaiveBayes)\nprint('>>>Testing')\n# I select two messages from the test data here.\npredictSMS('Ya very nice. . .be ready on thursday')\npredictSMS('Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd0f767c-6ad4-f9bb-42f3-8a06f14f1f70"},"source":"Okay, it works! A accuracy of 98.28% in training set and 97.77% is acceptable for me.\n\nPleas feel free to ask me, if you have any question. And if you like this guide, please upvote, Thanks a lot.\n\n--Clyde Wang"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}