{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:28:18.446308Z","iopub.execute_input":"2021-08-14T11:28:18.446852Z","iopub.status.idle":"2021-08-14T11:28:19.626619Z","shell.execute_reply.started":"2021-08-14T11:28:18.44674Z","shell.execute_reply":"2021-08-14T11:28:19.625494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Case Study\n\nIt is no secret students drink alcohol before reaching legal age, this dataset contains alcohol consumption data for two secondary schools students. It also contains data about their grades, families and how the students spend their free time. I hope that by analyzing such an intresting dataset I'll be able to find interesting patterns and correlations between alcohol consumption and academic perfromance.","metadata":{}},{"cell_type":"code","source":"MATH = '../input/student-alcohol-consumption/student-mat.csv'\n\ndataset = pd.read_csv(MATH)\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:11.744914Z","iopub.execute_input":"2021-08-14T11:30:11.745653Z","iopub.status.idle":"2021-08-14T11:30:11.818071Z","shell.execute_reply.started":"2021-08-14T11:30:11.7456Z","shell.execute_reply":"2021-08-14T11:30:11.816717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:13.220927Z","iopub.execute_input":"2021-08-14T11:30:13.221368Z","iopub.status.idle":"2021-08-14T11:30:13.234913Z","shell.execute_reply.started":"2021-08-14T11:30:13.221329Z","shell.execute_reply":"2021-08-14T11:30:13.233753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Alcohol consumption\n\nFirst let's check out the distributions in two most interesting columns - **daily alcohol consumption** and **weekly alcohol consumption.** ","metadata":{}},{"cell_type":"code","source":"def create_plot(n: int, m: int, size: tuple = (12, 5)): \n    fig, ax = plt.subplots(n, m, figsize=size)\n    return fig, ax","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:13.999235Z","iopub.execute_input":"2021-08-14T11:30:13.999673Z","iopub.status.idle":"2021-08-14T11:30:14.005423Z","shell.execute_reply.started":"2021-08-14T11:30:13.999634Z","shell.execute_reply":"2021-08-14T11:30:14.004057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = create_plot(1, 2)\n\nsns.histplot(dataset['Dalc'], ax=ax[0])\nsns.histplot(dataset['Walc'], ax=ax[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:14.308058Z","iopub.execute_input":"2021-08-14T11:30:14.308442Z","iopub.status.idle":"2021-08-14T11:30:14.789139Z","shell.execute_reply.started":"2021-08-14T11:30:14.308414Z","shell.execute_reply":"2021-08-14T11:30:14.787896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily = dataset[dataset['Dalc'] >= 3.0]\nweekly = dataset[dataset['Walc'] >= 3.0]\n\ndaily_count = len(daily)\nweekly_count = len(weekly)\n\np_daily = np.round((daily_count / len(dataset)) * 100)\np_weekly = np.round((weekly_count / len(dataset)) * 100)\n\nprint(f'{p_daily}%, {p_weekly}%')","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:14.790898Z","iopub.execute_input":"2021-08-14T11:30:14.791289Z","iopub.status.idle":"2021-08-14T11:30:14.806209Z","shell.execute_reply.started":"2021-08-14T11:30:14.791253Z","shell.execute_reply":"2021-08-14T11:30:14.804975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**\"Dalc**\" and **\"Walc\"** - Daily alcohol consumption and weekly alcohol consumption are discrete values ranging from 1.0 (very low) to 5.0 (very high). Even though distributions might look fairly normal, one curious observations that I've made is that **40% of students drink alcohol on weekly basis with \"Walc\" value of bigger or equal 3.0**, which can be considered quite high for secondary school students.","metadata":{}},{"cell_type":"markdown","source":"Even more disturbing is the fact that **11% of daily drinkers are students with \"Dalc\" value bigger or equal to 3.0.** Imo this is a very worring fact.","metadata":{}},{"cell_type":"markdown","source":"**Let's take a closer look at those students.**\n\n**It might be not that bad if they aren't too young.**","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(1, 2)\n\nsns.histplot(daily['age'], ax=ax[0], kde=True)\nsns.histplot(weekly['age'], ax=ax[1], kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:15.936684Z","iopub.execute_input":"2021-08-14T11:30:15.937133Z","iopub.status.idle":"2021-08-14T11:30:16.512822Z","shell.execute_reply.started":"2021-08-14T11:30:15.937095Z","shell.execute_reply":"2021-08-14T11:30:16.511623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_mean = np.round(daily['age'].mean())\nweekly_mean = np.round(weekly['age'].mean())\n\nprint(f\"Daily mean {daily_mean}, weekly mean {weekly_mean}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:16.514398Z","iopub.execute_input":"2021-08-14T11:30:16.514737Z","iopub.status.idle":"2021-08-14T11:30:16.524894Z","shell.execute_reply.started":"2021-08-14T11:30:16.514706Z","shell.execute_reply":"2021-08-14T11:30:16.523618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Atleast means aren't too low, however looking at both distributions it seems quite weird for me how there are more younger people who frequently consume alcohol than older people. This might be due to dataset imbalances, let's check it out!","metadata":{}},{"cell_type":"code","source":"sns.displot(dataset['age'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:16.748839Z","iopub.execute_input":"2021-08-14T11:30:16.749217Z","iopub.status.idle":"2021-08-14T11:30:17.07583Z","shell.execute_reply.started":"2021-08-14T11:30:16.749182Z","shell.execute_reply":"2021-08-14T11:30:17.07463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, clearly there are more younger students in this dataset \n\nKnowing that the data is biased, having ~90% students aged 15 - 18 compared to ~10% of ages 19 - 22, let's take a look at some **pivot tables.**","metadata":{}},{"cell_type":"code","source":"dataset['count'] = 1\n\npivot_1 = pd.pivot_table(dataset, \n                         values='count',\n                         index='age', \n                         columns='Dalc', \n                         aggfunc=np.sum, \n                         fill_value=0)\n\npivot_2 = pd.pivot_table(dataset, \n                         values='count', \n                         index='age', \n                         columns='Walc', \n                         aggfunc=np.sum, \n                         fill_value=0)\n\nfig, ax = create_plot(1, 2)\n\nsns.heatmap(pivot_1, ax=ax[0], annot=True)\nsns.heatmap(pivot_2, ax=ax[1], annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:17.330634Z","iopub.execute_input":"2021-08-14T11:30:17.331014Z","iopub.status.idle":"2021-08-14T11:30:18.465372Z","shell.execute_reply.started":"2021-08-14T11:30:17.330981Z","shell.execute_reply":"2021-08-14T11:30:18.464155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pivots are great and even though here they are quite irrepresentative we can still find some valuable insight looking at them. For example value **1 in both Walc and Dalc is by far the most common for 15 year olds**, yay underage students don't drink that much! **Values are more evenly distributed for ages 17 and 18** which is to be expected. **Anything past 19 is just not worth considerning since we lack much data in those age ranges.**","metadata":{}},{"cell_type":"markdown","source":"Now let's take a look at perhaps the most intresting topic: **how alcohol consumption affects academic performance.**","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(3, 2, (20,20))\n\nfor idx, j in enumerate(['Dalc', 'Walc']):\n    for i in range(3):\n        sns.boxplot(data=dataset, x=f'{j}', y=f'G{i+1}', ax=ax[i][idx])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:18.467284Z","iopub.execute_input":"2021-08-14T11:30:18.467627Z","iopub.status.idle":"2021-08-14T11:30:19.905628Z","shell.execute_reply.started":"2021-08-14T11:30:18.467594Z","shell.execute_reply":"2021-08-14T11:30:19.904271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite interestingly there dosn't seem to be a noticible trend in academic performance in regards to alcohol consumption. Most boxplots have a median of around 10, Q2 and Q3 are very simillar for all the cases, usually bigger values of alcohol consumption have slimer range for grades, however this is to be attributed to much fewer students who drink so much alcohol.","metadata":{}},{"cell_type":"markdown","source":"# Academic performance\n\nLet's forget about alcohol for a moment and take a closer look at **grades**. The dataset contains three grades:\n* G1 - First period grade\n* G2 - Second period grade\n* G3 - Final grade\n\nOne interesting thing that I always wanted to know is **how correlated really are grades and study time**. But first let's take a look at grades distribution.","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(1, 3, (15, 5))\n\nfor i in range(3):\n    sns.kdeplot(dataset[f'G{i+1}'], ax=ax[i], fill=True, linewidth=2, alpha=0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:19.907845Z","iopub.execute_input":"2021-08-14T11:30:19.90831Z","iopub.status.idle":"2021-08-14T11:30:20.362248Z","shell.execute_reply.started":"2021-08-14T11:30:19.908262Z","shell.execute_reply":"2021-08-14T11:30:20.36101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions are quite close to normal distributions.","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(3, 1, (5, 15))\n\nfor i in range(3):\n    sns.boxplot(data=dataset, x='studytime', y=f'G{i+1}', ax=ax[i])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:20.364292Z","iopub.execute_input":"2021-08-14T11:30:20.364662Z","iopub.status.idle":"2021-08-14T11:30:21.091084Z","shell.execute_reply.started":"2021-08-14T11:30:20.364626Z","shell.execute_reply":"2021-08-14T11:30:21.089942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maybe suprisingly, studytime is only slightly correlated with grades. **If not studytime then what is?**","metadata":{}},{"cell_type":"code","source":"corr = dataset.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nfig, ax = create_plot(1, 1, (12,12))\n\nax = sns.heatmap(corr, mask=mask, square=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:21.092917Z","iopub.execute_input":"2021-08-14T11:30:21.093297Z","iopub.status.idle":"2021-08-14T11:30:21.597311Z","shell.execute_reply.started":"2021-08-14T11:30:21.093264Z","shell.execute_reply":"2021-08-14T11:30:21.595934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like **Medu** and **Fedu** (respectively **Mother's education** and **Father's education**) have the biggest impact on chilldren's academic perfomrance. Let's take a look at those variables.","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(1, 2)\n\nsns.kdeplot(dataset['Medu'], ax=ax[0], fill=True, linewidth=2, alpha=0.1)\nsns.kdeplot(dataset['Fedu'], ax=ax[1], fill=True, linewidth=2, alpha=0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:21.599134Z","iopub.execute_input":"2021-08-14T11:30:21.599525Z","iopub.status.idle":"2021-08-14T11:30:21.940783Z","shell.execute_reply.started":"2021-08-14T11:30:21.599489Z","shell.execute_reply":"2021-08-14T11:30:21.93957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like Mothers are on average better educated that Fathers","metadata":{}},{"cell_type":"markdown","source":"# Modeling\n\n**Now it's time for sexiest element in data science - modeling data with machine learning**\n\n* Handeling cateogrical variables\n* Normalizing the data\n* Fitting the models and evaluation","metadata":{}},{"cell_type":"code","source":"# School is irrelevant for my purpose\ndataset = dataset.drop(['school'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:21.942895Z","iopub.execute_input":"2021-08-14T11:30:21.943353Z","iopub.status.idle":"2021-08-14T11:30:21.949044Z","shell.execute_reply.started":"2021-08-14T11:30:21.943304Z","shell.execute_reply":"2021-08-14T11:30:21.94796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"T = True\n\nif T:\n    dataset.drop(['G1', 'G2'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:22.240967Z","iopub.execute_input":"2021-08-14T11:30:22.241334Z","iopub.status.idle":"2021-08-14T11:30:22.248419Z","shell.execute_reply.started":"2021-08-14T11:30:22.241303Z","shell.execute_reply":"2021-08-14T11:30:22.246988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling categorical variables\n\nSome cateogircal variables have only two possible values, these ones I am simply going to map either to 1 or 0, others have many classes for such cases we can encode them in sparse representations.","metadata":{}},{"cell_type":"code","source":"# Selecting categorical variables\ncategorical = dataset.select_dtypes(exclude=['int64', 'float64'])\ncategorical.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:22.889151Z","iopub.execute_input":"2021-08-14T11:30:22.889626Z","iopub.status.idle":"2021-08-14T11:30:22.916336Z","shell.execute_reply.started":"2021-08-14T11:30:22.889586Z","shell.execute_reply":"2021-08-14T11:30:22.915043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary = ['sex', \n          'address', \n          'famsize', \n          'Pstatus', \n          'schoolsup', \n          'famsup', \n          'paid', \n          'activities', \n          'nursery', \n          'higher', \n          'internet', \n          'romantic'\n         ]\n\nfor col in categorical.columns:\n    if col in binary:\n        unique = categorical[col].unique()\n        categorical[col] = categorical[col].map({\n            f'{unique[0]}': 0, \n            f'{unique[1]}': 1\n        }) \n    else:\n        dummies = pd.get_dummies(categorical[col])\n        for dummy in dummies:\n            categorical[dummy] = dummies[dummy]\n        categorical.drop(col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:23.184921Z","iopub.execute_input":"2021-08-14T11:30:23.185377Z","iopub.status.idle":"2021-08-14T11:30:23.22607Z","shell.execute_reply.started":"2021-08-14T11:30:23.185341Z","shell.execute_reply":"2021-08-14T11:30:23.22481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:23.51862Z","iopub.execute_input":"2021-08-14T11:30:23.519141Z","iopub.status.idle":"2021-08-14T11:30:23.543355Z","shell.execute_reply.started":"2021-08-14T11:30:23.519094Z","shell.execute_reply":"2021-08-14T11:30:23.542578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now with clean categorical variables let's move onto numerical columns.","metadata":{}},{"cell_type":"code","source":"numerical = dataset.select_dtypes(include=['int64', 'float64'])\nnumerical.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:24.134508Z","iopub.execute_input":"2021-08-14T11:30:24.135151Z","iopub.status.idle":"2021-08-14T11:30:24.15486Z","shell.execute_reply.started":"2021-08-14T11:30:24.135113Z","shell.execute_reply":"2021-08-14T11:30:24.153906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_clean = pd.concat([numerical, categorical], axis=1)\ndataset_clean.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:24.429441Z","iopub.execute_input":"2021-08-14T11:30:24.430374Z","iopub.status.idle":"2021-08-14T11:30:24.459549Z","shell.execute_reply.started":"2021-08-14T11:30:24.430307Z","shell.execute_reply":"2021-08-14T11:30:24.458082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical data\n\nPreprocessing numerical variables usually means scaling and normalizing them. I am going to use StandardScaler with Pipeline from sklearn during model training, but for now I would like to know if the data forms some kind of pattern.\n\n# PCA\n\n**PCA** is an algorithm that is able to reduce the dimensionality (i.e reduce the number of columns from 39 to 2) of dataset and retain the most valuable information about underlaying patters in the data. It is often used to visualize the dataset and helps with model selection. I am also going to use sklearn's implementation of PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso, LinearRegression, SGDRegressor, ElasticNet\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error as mse","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:25.068625Z","iopub.execute_input":"2021-08-14T11:30:25.06907Z","iopub.status.idle":"2021-08-14T11:30:25.396631Z","shell.execute_reply.started":"2021-08-14T11:30:25.069031Z","shell.execute_reply":"2021-08-14T11:30:25.395559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset_clean.iloc[:, :-1]\ny = dataset_clean['G3']","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:25.554602Z","iopub.execute_input":"2021-08-14T11:30:25.55525Z","iopub.status.idle":"2021-08-14T11:30:25.561285Z","shell.execute_reply.started":"2021-08-14T11:30:25.555205Z","shell.execute_reply":"2021-08-14T11:30:25.560495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dimensionality after PCA\nn_components = 2\n\npca = PCA(n_components=n_components)\n\nprincipal_components = StandardScaler().fit_transform(pca.fit_transform(X))\n# Explained variance ration is the % of information that is retained after PCA\nprint(f\"Explained variance ration = {np.sum(pca.explained_variance_ratio_)}\")\n\npca_df = pd.DataFrame(principal_components)\npca_df = pd.concat([pca_df, y], axis=1)\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:26.196503Z","iopub.execute_input":"2021-08-14T11:30:26.197423Z","iopub.status.idle":"2021-08-14T11:30:26.307841Z","shell.execute_reply.started":"2021-08-14T11:30:26.197368Z","shell.execute_reply":"2021-08-14T11:30:26.306572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**0.84 explained variance ratio is quite good, now let's visualize our reduced dataset.**","metadata":{}},{"cell_type":"code","source":"fig, ax = create_plot(1, 2)\n\nfor i in range(n_components):\n    ax[i].scatter(x=pca_df[i], y=pca_df['G3']) \n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:27.518856Z","iopub.execute_input":"2021-08-14T11:30:27.51933Z","iopub.status.idle":"2021-08-14T11:30:27.856601Z","shell.execute_reply.started":"2021-08-14T11:30:27.519286Z","shell.execute_reply":"2021-08-14T11:30:27.855254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First component of pca_df dosn't tell us too much about the patterns in data, however in the second one there is a clearly visible linear trend, let's visualize the whole dataset with 3D plotly scatterplot.**","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter_3d(pca_df, x=0, y=1, z='G3',\n              color='G3')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:28.998682Z","iopub.execute_input":"2021-08-14T11:30:28.999162Z","iopub.status.idle":"2021-08-14T11:30:32.397533Z","shell.execute_reply.started":"2021-08-14T11:30:28.999121Z","shell.execute_reply":"2021-08-14T11:30:32.396094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This 3D scatterplot is really helpful for understandig the patterns and correlations in the dataset, besides that don't you think it looks super cool?** ","metadata":{}},{"cell_type":"markdown","source":"# Model selection\n\nThanks to PCA we already know that the dataset has a linear trend, so obvious model choice is beloved **least squares**, however just for fun I am going to fit few other models and see how they perform.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:42.564741Z","iopub.execute_input":"2021-08-14T11:30:42.56519Z","iopub.status.idle":"2021-08-14T11:30:42.574739Z","shell.execute_reply.started":"2021-08-14T11:30:42.565149Z","shell.execute_reply":"2021-08-14T11:30:42.573497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODELS = [Lasso, LinearRegression, SGDRegressor, ElasticNet, SVR, LinearSVR]\n\ndef train_model(model):\n    pipe = Pipeline([\n        ('scaler', StandardScaler()), \n        (f'{str(model.__name__).lower()}', model())\n    ])\n\n    pipe.fit(X_train, y_train)\n    preds = pipe.predict(X_test)\n    score = mse(y_test, preds)\n    print(str(model.__name__) + \" score: \" + str(score))\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:46.445174Z","iopub.execute_input":"2021-08-14T11:30:46.445701Z","iopub.status.idle":"2021-08-14T11:30:46.453621Z","shell.execute_reply.started":"2021-08-14T11:30:46.44565Z","shell.execute_reply":"2021-08-14T11:30:46.451981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = {}\n\nfor model in MODELS:\n    scores[str(model.__name__)] = train_model(model)\n\nprint()\nprint(min(scores, key=scores.get) + \" is the winner!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-14T11:30:47.58949Z","iopub.execute_input":"2021-08-14T11:30:47.590194Z","iopub.status.idle":"2021-08-14T11:30:47.693031Z","shell.execute_reply.started":"2021-08-14T11:30:47.590135Z","shell.execute_reply":"2021-08-14T11:30:47.691808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No big surprise here, Linear Regression won the competition","metadata":{}},{"cell_type":"markdown","source":"**To be continued...**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}