{"cells":[{"metadata":{"_uuid":"6f4d8377f4b31cc8cdf0206f088ec2356ee812ec"},"cell_type":"markdown","source":"# Table of Contents:"},{"metadata":{"_uuid":"e5fc5eb9b264df22afe26c29d0adc74eb6d0f457"},"cell_type":"markdown","source":"* **1. Data Exploration:**\n   * 1.1 Data Completeness.\n   * 1.2 Features Engineering.\n* **2. Data Visualization:**\n  * 2.1 Features Importance.<br>\n  * 2.2 Lat and long plotting against prices.<br>\n  * 2.3 The areas of living against prices.<br>\n  * 2.4 Grades against prices.<br>\n  * 2.5 Grades against other features.<br>\n* **3. Modeling:**\n  * 3.1 Regression using Multiple Linear Regression.<br>\n  * 3.2 Regression using Random Forest.<br>\n  * 3.3 Regression using XGBRegressor.<br>\n  * 3.4 Models accuracies."},{"metadata":{"_uuid":"3719f080616ba401c9441cdc616f316b79c9d198"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"bbd6556db0577ef19834374fe59d50b89b8ea6db"},"cell_type":"markdown","source":"## Importing Libraries:"},{"metadata":{"trusted":false,"_uuid":"7e102af33021da3a1d5ac652c56c9cd4cf9fa5bc"},"cell_type":"code","source":"# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb6747d160856bba61a5b437718d961e60445a3d"},"cell_type":"markdown","source":"### First, we upload the data and check if there are missing values:"},{"metadata":{"trusted":false,"_uuid":"e7345374251c8c3da57ea173823f7df234af3c08"},"cell_type":"code","source":"# Upload data\ndata = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a244873d55af6543edaa716c1189f5bdef648eb"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"a386ee7567c9314da7d145307a98b5e5ba42f3a3"},"cell_type":"markdown","source":"# 1. Data Exploration:"},{"metadata":{"_uuid":"f16c2d22886f579fb89f6a81a2acda1455fb4d19"},"cell_type":"markdown","source":"## 1.1 Data Completeness:"},{"metadata":{"trusted":false,"_uuid":"cdf23cfb1247b16845efaf5cc9938695a78b9402"},"cell_type":"code","source":"# Check if there are missing observations\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e1a602c95c42f25d9e137481f9e47be47f37e67c"},"cell_type":"code","source":"# Check some of the observations\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecadbaed76ac36acd9ef6551a3639ce0f5aa571d"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"6d1488021bea348ab0c129a4b60443049c50a48c"},"cell_type":"markdown","source":"## 1.2 Features Engineering:"},{"metadata":{"_uuid":"0a73b4fce3187219d4af4e0fde2160abd75845c2"},"cell_type":"markdown","source":"#### By having a look at the features, i think it would be better to:\n\n1- Break down (date) into 3 new columns (year, month & day) and then drop (date & id) columns.<br>\n2- Round the number of bathrooms and floors since representing their number as fraction wouldn't make sense.<br>\n3- Include all independent features in the analysis."},{"metadata":{"trusted":false,"_uuid":"98a93b3b48fe59ce252f9d9a9aec42ca442962e1"},"cell_type":"code","source":"# 1- Break down (date) into (year, month & day)\ndata['year'] = data['date'].apply(lambda x: x[:4]).astype(int)\ndata['month'] = data['date'].apply(lambda x: x[4:6]).astype(int)\ndata['day'] = data['date'].apply(lambda x: x[6:8]).astype(int)\n\n# Dropp (id and date)\ndata.drop(['id', 'date'], axis=1, inplace=True)\n\n\n# 2- Round the number of bathrooms\ndata['bathrooms'] = data['bathrooms'].apply(lambda x: round(x, 0))\ndata['bathrooms'] = data['bathrooms'].astype(int)\n\n# Round the number of floors\ndata['floors'] = data['floors'].apply(lambda x: round(x, 0))\ndata['floors'] = data['floors'].astype(int)\n\n\n# 3- Create X & y (independent and dependent variables) vectors to be used in charts and models\nX = data.drop(\"price\",axis=1).values\ny = data[\"price\"].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d663dd4a0d579ae620ba300766115302efa7d0"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"9140754dc7fcc0bff9769204a860fa9d4da3f5e4"},"cell_type":"markdown","source":"# 2. Data Visualization:"},{"metadata":{"_uuid":"304dc09152ea047483c2150acd310ad0bfc594fc"},"cell_type":"markdown","source":"## 2.1 Features Importance:"},{"metadata":{"_uuid":"01eeccd8c8970aa4f03ff1a49f5ec644c2d84601"},"cell_type":"markdown","source":"#### Now, it's time to check how important each feature in the prediction process of the prices using two different algorithms (Random Forest and XGB Regressor):"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"91f0196dc60b9a13218bb81e0c8978fde1d47557"},"cell_type":"code","source":"# Create a list with the algorithms that will be used to check features importance\ncombine = [RandomForestRegressor(random_state=5), XGBRegressor(random_state=5)]\n\n# Make a list of features' names to label the importance bars with it\ncolumns = data.drop(\"price\",axis=1).columns\n\n# Plot features importance charts when using Random Forest and XGBRegressor algorithms\nfor classifier in combine:\n    classifier.fit(X, y)\n    f, axes = plt.subplots(1, 1, figsize=(12, 4))\n    (pd.Series(classifier.feature_importances_, index=columns)\n       .nlargest(len(classifier.feature_importances_))\n       .plot(kind='barh'))\n    if classifier == combine[0]:\n        plt.title('Random Forest')\n    else:\n        plt.title('XGB Regressor')\n    plt.show()\n    accuracy = cross_val_score(estimator = classifier, X = X, y = y, cv = 10, n_jobs = -1)\n    print('Prices prediction accuracy using this model is: ', str(round((accuracy.mean() * 100), 2)))\n    print('The highest 5 features in terms of importance represent %.2f percent\\n\\n' % (pd.Series(classifier.feature_importances_, index=columns).sort_values(ascending=False)[0:5].sum() * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e397b2d4121401ac095c67436644f2819413df75"},"cell_type":"markdown","source":"#### If we check the above charts, we will see that Random Forest Regressor expects that the highest 5 features in term of importance will contribute with around 85% in predicting prices. While according to XGBRegressor, their contribution will be around 58%.\n\n#### Based on the above assumption, we will check the top 5 most important features (lat,  long, sqft_living, sqft_living15, grade) and see their relation with prices."},{"metadata":{"_uuid":"5127cbd24e42ccb5ca907a5ece4bbe39ad2059cc"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"6350a155951c9149241964dfac83031020e1324d"},"cell_type":"markdown","source":"## 2.2 Lat and long plotting against prices:"},{"metadata":{"_uuid":"ea11d5306fbf221c63896e185b8b4d06cbe3ed59"},"cell_type":"markdown","source":"#### Checking the relationship between geographical locations and prices: "},{"metadata":{"trusted":false,"_uuid":"22c0d826877181f3a6441838fd66920fe49f17aa"},"cell_type":"code","source":"# Set chart size\nplt.figure(figsize = (17,8))\n\n# Create scatter plot to check the relationship between (lat & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('lat', 'price', data=data, fit_reg=False, ax=ax1)\n\n# Create scatter plot to check the relationship between (long & price)\nax2 = plt.subplot(222)\nax2 = sns.regplot('long', 'price', data=data, fit_reg=False, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df7a2a5090a787e7b9a955f9c8e8e937feee61d6"},"cell_type":"markdown","source":"#### According to the above plots, we can notice that the most expensive houses are in the north west (lat: 47.63, long: -122.2). Prices start to decrease by heading south and east."},{"metadata":{"_uuid":"30dd582742246713be26fc05926efc654042e9b2"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"0161e815f53630a19e17b4e4c136a4754d1865ff"},"cell_type":"markdown","source":"## 2.3 The areas of living against prices:"},{"metadata":{"_uuid":"8782cb6d83491c93a3de2aa49d997f54244c299d"},"cell_type":"markdown","source":"#### Checking the relationship between living areas and prices: "},{"metadata":{"trusted":false,"_uuid":"9b2fe09db7ab61c37c412fe4423c75f846f51181"},"cell_type":"code","source":"# Set chart size\nplt.figure(figsize = (17,8))\n\n# Create scatter plot to check the relationship between (sqft_living & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('sqft_living', 'price', data=data, fit_reg=False, ax=ax1)\n\n# Create scatter plot to check the relationship between (sqft_living15 & price)\nax2 = plt.subplot(222)\nax2 = sns.regplot('sqft_living15', 'price', data=data, fit_reg=False, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ec10c3c0b02150aef8e2321813679f4f2933ac"},"cell_type":"markdown","source":"#### We also can see that living area matters in determining the price."},{"metadata":{"_uuid":"fe8a7bed8ec712dea29a20ff76174fc21bf3dd2b"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"9fad9aa7d11efe299f0900cd932298633cedf248"},"cell_type":"markdown","source":"## 2.4 Grades against prices:"},{"metadata":{"_uuid":"8ba592929409aaa9d674241531495b15a863e972"},"cell_type":"markdown","source":"#### Checking the relationship between grades and prices: "},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"aeb0addf23748eeb756fdf99dad2d3e6fe7625a3"},"cell_type":"code","source":"# Set chart size\nplt.figure(figsize = (25,8))\n\n# Create scatter plot to check the relationship between (grade & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('grade', 'price', data=data, fit_reg=False, ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bd46837abb9c546f8c6f3e8c6fa6bd9b025fea9"},"cell_type":"markdown","source":"#### A positive correlation is illustrated indicating that prices increase with higher grades. What about the relashipship between grades and other features? In other words, to what extent other features determine the grade?"},{"metadata":{"_uuid":"8f6b04bab122d21bc83312df73f3734f16983674"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"2d4a9dd0422bc5ee73dc76c9b04f6836c79ad518"},"cell_type":"markdown","source":"## 2.5 Grades against other features:"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"6e670cf84fb82c2c799551232da9946fc9c5ef16"},"cell_type":"code","source":"# Create a new dataframe\ncopy_data = data.copy()\n\n# Group 'yr_built' values\nfor number in range(1890, 2021, 10):\n    copy_data.loc[(copy_data['yr_built'] > number) & (copy_data['yr_built'] <= (number + 10)), 'yr_built'] = number + 10\n\n# Group 'yr_renovated' values\nfor number in range(1890, 2021, 10):\n    copy_data.loc[(copy_data['yr_renovated'] > number) & (copy_data['yr_renovated'] <= (number + 10)), 'yr_renovated'] = number + 10\n\n# Group 'sqft_lot' values\nfor number in range(0, 1700001, 100000):\n    copy_data.loc[(copy_data['sqft_lot'] > number) & (copy_data['sqft_lot'] <= (number + 100000)), 'sqft_lot'] = number + 100000\n    \n# Group 'sqft_basement' values\nfor number in range(0, 5001, 500):\n    copy_data.loc[(copy_data['sqft_basement'] > number) & (copy_data['sqft_basement'] <= (number + 500)), 'sqft_basement'] = number + 500\n    \n# Group 'sqft_above' values\nfor number in range(0, 10001, 1000):\n    copy_data.loc[(copy_data['sqft_above'] > number) & (copy_data['sqft_above'] <= (number + 1000)), 'sqft_above'] = number + 1000\n\n# Create a list of all features that we will checked against grade\nparameters = ['yr_built', 'yr_renovated', 'sqft_lot', 'sqft_basement', 'sqft_above', 'floors', 'month', 'bedrooms', \n            'condition', 'waterfront', 'view', 'grade']\ncm = sns.light_palette(\"green\", as_cmap=True)\n\n# Display a table for each feature against grade\nfor number in range(0, len(parameters) - 1):\n    display(pd.crosstab(copy_data[parameters[number]], copy_data[parameters[len(parameters) - 1]]).style.background_gradient(cmap = cm))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b8e75d5a53d6bbec46bbbc9ab3379a3dedf85ad"},"cell_type":"markdown","source":"#### From the above cross tables, we can see how each grouped values related to each feature can affect the determination of grades. Another way to see these correlations is to check the corresponding correlation coefficient below:"},{"metadata":{"trusted":false,"_uuid":"1ced947074d5f580ee15be0ee636beedca0e3443"},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features', fontsize=20)\nsns.heatmap(copy_data.corr().astype(float).corr(),vmax=1.0, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfa7acf6ca6bfa84b026069d5ce81f53c3078c92"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"2693ea3969a9f3901b7a9e0146c82341416940a9"},"cell_type":"markdown","source":"# 3. Modeling:"},{"metadata":{"_uuid":"d2caadf8028e981d3eb76c8e54516a90b7e0132b"},"cell_type":"markdown","source":"#### At this stage, it's time to see which algorithm will predict the prices with highest accuracy rate. Starting off by creating a function to check the best hyperparameters for each model:"},{"metadata":{"trusted":false,"_uuid":"368256de629dc6a55b65d71658c96903bcae950a"},"cell_type":"code","source":"# Create function to check the best hyperparameters for each model\ndef params_checker (algo, parameters, x, y):\n    grid_search = GridSearchCV(estimator = algo, param_grid = parameters, scoring = 'neg_mean_absolute_error', cv = 10, n_jobs = -1)\n    grid_search = grid_search.fit(x, y)\n    \n    # Print the mean absolute error for best parameters reached\n    print(\"- mean absolute error: %.2f\" % ((round(grid_search.best_score_, 2))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37aefe1aaed0d6514f0586d687c05742a89602d"},"cell_type":"markdown","source":"#### Create a dataframe that holds the predictions' accuracies obtained from each model:"},{"metadata":{"trusted":false,"_uuid":"db39aba61340f3229ff73da0c33969cbcf7d4aa1"},"cell_type":"code","source":"# Create a dataframe that will hold each model's prediction accuracy calculated using cross validation.\naccuracy_dataframe = pd.DataFrame(columns=['Model', 'CV_Score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7f0ff9ad1fe9d8d7a0b4607d4c23b90eca15875"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"ac2d3028e37e8756853748de81a7c43ab7d21008"},"cell_type":"markdown","source":"## 3.1 Multiple Linear Regression:"},{"metadata":{"_uuid":"f81833837ebb12e16d2310c358561f09a2513375"},"cell_type":"markdown","source":"#### Check the significance of each feature \"(P) value\":"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"55b7e5b84cd5d18cd22d6bf4cdc1424fc3010c14"},"cell_type":"code","source":"# Aggregate all independent features and add a column of ones at the beginning (since statsmodels 'sm' library doesn't take in consideration the constant coefficient (b0) at the multiple linear regression equation)\nX_multi = np.append(arr = np.ones((len(data), 1)).astype(int), values = X, axis = 1)\n\n# Create an object with the significant features (after performing backward elimination, i removed feature number 20 \"month\" since it had a (P) value above 5% \"its (P) value was 12.8%\")\nX_opt = X_multi [:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21]]\n\n# Fitting the data and checking the R-squared\nclassifier_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nclassifier_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c43b2dd67219658ad37a5ccbac9694764aa5d46"},"cell_type":"markdown","source":"#### Check the best hyperparameters and its prediction accuracy:"},{"metadata":{"trusted":false,"_uuid":"fee3a9e72c50d023e9592269ee71fd44776acc33"},"cell_type":"code","source":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(normalize = [False])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(LinearRegression(), params, X, y)\n\n# Create object without 'month' feature (since it's not a significant feature according to it's (P) value)\nX_opt = data.drop([data.columns[0], data.columns[len(data.columns)-2]], axis=1).values\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = LinearRegression(), X = X_opt, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Multiple Linear', (str(round((accuracies.mean()) * 100, 3)) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ea225ca5fa2209f48ef604c24862f55a39bc679"},"cell_type":"markdown","source":"#### We can see that the values of R-squared and accuracy obtained from performing cross validatoin are almost the same (70%) which indicates that, according to multiple linear regression, the independent features can explain almost 70% of the changes in prices."},{"metadata":{"_uuid":"8819ca1d5b715fb1cbc80ea4cefff8ba99a5c561"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"581c09f86d5da325bf86a7d9a6926ceb109c5d5e"},"cell_type":"markdown","source":"## 3.2 Random Forest:"},{"metadata":{"_uuid":"2e7cc132ac6db6f291614b1514649c16122dc5f8"},"cell_type":"markdown","source":"#### Check the best hyperparameters and its corresponding prediction accuracy:"},{"metadata":{"trusted":false,"_uuid":"f706d5fc3480045e133b2b22b592bbd04f4a67ba"},"cell_type":"code","source":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(n_estimators=[165], min_samples_split=[3], random_state=[0])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(RandomForestRegressor(), params, X, y)\n\n# Create object with the best parameters to use it in the cross validation\nalgo = RandomForestRegressor(n_estimators=165, min_samples_split=3, random_state=0)\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = algo, X = X, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Random Forest', (str(round((accuracies.mean()) * 100, 3)) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02f18104a5348c17f4d5a06809a53e0ee003f0e9"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"425801a17908a4016048d83340a6fb718ea225e6"},"cell_type":"markdown","source":"## 3.3 XGBRegressor:"},{"metadata":{"_uuid":"339b880552b35274431290a82cf2badf4ce07f5d"},"cell_type":"markdown","source":"#### Check the best hyperparameters and its corresponding prediction accuracy:"},{"metadata":{"trusted":false,"_uuid":"bbedb80e6341e74f3d545258ebff89c376122db2"},"cell_type":"code","source":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(max_depth=[7], learning_rate=[0.1], n_estimators=[350], gamma=[0.00001], min_child_weight=[3], colsample_bytree=[0.7])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(XGBRegressor(), params, X, y)\n\n# Create object with the best parameters to use it in the cross validation\nalgo = XGBRegressor(max_depth=7, learning_rate=0.1, n_estimators=350, gamma=0.00001, min_child_weight=3, colsample_bytree=0.7)\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = algo, X = X, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'XGBRegressor', (str(round((accuracies.mean()) * 100, 3)) + ' %')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65aed9fa5d5cd60f98443de0f1711de59eafc85"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"1321035c7450fed8eb65a5bb84c83ebf51f5afef"},"cell_type":"markdown","source":"## 3.4 Models accuracies"},{"metadata":{"_uuid":"db60fb44ffb808209c08977e835b1bc1a64995d2"},"cell_type":"markdown","source":"#### Now, let's have a look on the scores for each model ranked from highest to lowest:"},{"metadata":{"trusted":false,"_uuid":"d2deb44661f947bfddd6709e9461bd5ce081ed3f"},"cell_type":"code","source":"accuracy_dataframe = accuracy_dataframe.sort_values(['CV_Score'], ascending=False)\naccuracy_dataframe.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f50969ff88f1023bdf4ee157dca0f53a61254370"},"cell_type":"markdown","source":"#### According to the above results, XGBRegressor model has the best price prediction accuracy."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}