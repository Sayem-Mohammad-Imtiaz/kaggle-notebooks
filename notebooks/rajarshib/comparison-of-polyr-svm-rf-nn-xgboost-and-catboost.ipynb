{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we compare the performance of Polynomial Regression,Kernel-Svm,Random Forest,Neural Network,XGBoost and CatBoost\nHyper-parameter tuning using RandomSearchCV will be peformed on Random-Forest, NN and XGBoost. Catboost as their website suggests doesn't require any hyper-parameter tuning.\n\nThis is my first notebook submission, pointing out mistakes and suggestions for improvement will greatly help!","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import preprocessing as p\n\n\ndata=pd.read_csv('../input/insurance/insurance.csv')\ndata.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding correlations between variables(Only numerical ones)\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that age has the highest correlation followed by bmi. Its obvious that both are deciding factors of the medical costs as aged people and people with high bmi experience more health problems in general.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data.iloc[:,0:6].values\ny=data.iloc[:,-1].values\n\nprint(X[0:5,:])\nprint(\"\\n\")\nprint(y[0:5])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will split the dataset into a training set and a test set using train test split class of scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#splitting the dataset in Train:Test=75:25\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['sex'].unique())\nprint(data['smoker'].unique())\nprint(data['region'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen the 'sex' and 'smoker' column has only two categories so they need to be label encoded only, while the region column needs label encoding followed by one hot encoding as there are four categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Label encoding of 'sex' column\nlabel_en=p.LabelEncoder()\nX_train[:,1]=label_en.fit_transform(X_train[:,1]) \nX_test[:,1]=label_en.transform(X_test[:,1])\n\n\n#Label encoding 'smoker' Column\nX_train[:,4]=label_en.fit_transform(X_train[:,4]) \nX_test[:,4]=label_en.transform(X_test[:,4])\n\n#Label encoding of 'region' Column\nX_train[:,5]=label_en.fit_transform(X_train[:,5])\nX_test[:,5]=label_en.transform(X_test[:,5])\n\n\n\nprint(X_train[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encoding of 'region' column\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.compose import ColumnTransformer \n\n\ncolumnTransformer = ColumnTransformer([('encoder', \n                                        OneHotEncoder(), \n                                        [5])], \n                                        remainder='passthrough') \nX_train = np.array(columnTransformer.fit_transform(X_train), dtype = np.float)\nX_test = np.array(columnTransformer.transform(X_test), dtype = np.float)\n\nprint(X_train[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first four columns denote the the one hot encoded column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Scaling of the features are required to prevent one category from being meaninglessly more dominant(during traing process) based on the value assigned during label encoding.Example(southwest=3,northwest=1) doesn't mean southwest is more valuable than northwest while in terms of prediction.\n\nWe will use Standard Scaler here as we want the ouliers to have and effect on out model.\nsource : https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler_X=StandardScaler()\nX_train=scaler_X.fit_transform(X_train)\nX_test=scaler_X.transform(X_test)\n\nprint(X_train[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Polynomial regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X_train)\nPOLYreg = LinearRegression()\nPOLYreg.fit(X_poly, y_train)\n\nX_poly_test=poly_reg.transform(X_test)\nPOLY_cod=POLYreg.score(X_poly_test,y_test)\nprint('Coefficient of determination(R^2) = {}'.format(POLY_cod)) \nmse=(metrics.mean_squared_error(POLYreg.predict(X_poly_test),y_test))\nprint('Mean squared error = {}'.format(mse))\nPOLY_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(POLY_rmse))\n\ny_pred = POLYreg.predict(poly_reg.transform(X_test))\nplt.scatter(y_test,y_pred)\n\nplt.plot(y_test,y_test,color='red')\nplt.title('Polynomial Regression')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support vector regression\n\nScaling of dependent variable is solely for the puspose of Support Vector Regression Model. SVR without the dependent variable scaled performs pretty poorly. Other models don't have this issue.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_y=StandardScaler()\n\n\n#Reshaping to 2D array as per as StandardScaler function requirements\ny_train_svr=y_train.reshape(len(y_train),1)\ny_test_svr=y_test.reshape(len(y_test),1)\n\ny_train_svr=scaler_y.fit_transform(y_train_svr)\ny_test_svr=scaler_y.transform(y_test_svr)\n\ny_train_svr=y_train_svr.reshape(len(y_train_svr))\ny_test_svr=y_test_svr.reshape(len(y_test_svr))\n\nprint(y_train_svr[:5])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.svm import SVR\nSVRreg = SVR(kernel = 'rbf')\nSVRreg.fit(X_train, y_train_svr)\n\ny_pred_svr=(scaler_y.inverse_transform(SVRreg.predict(X_test))).flatten()\n\nSVR_cod=SVRreg.score(X_test,y_test_svr)\nprint('Coefficient of determination(R^2) = {}'.format(SVR_cod)) \nmse=metrics.mean_squared_error(y_pred_svr,y_test)\nprint('Mean squared error = {}'.format(mse))\nSVR_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(SVR_rmse))\n\nplt.scatter(y_test,y_pred_svr)\n\nplt.plot(y_test,y_test,color='red')\nplt.title('SVR')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CATBoost\n\nTrying out catboost model without any tuning of hyperparameters.","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nCATreg = CatBoostRegressor()\nCATreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAT_cod=CATreg.score(X_test,y_test)\nprint('Coefficient of determination(R^2) = {}'.format(CAT_cod)) \nmse=(metrics.mean_squared_error(CATreg.predict(X_test),y_test))\nprint('Mean squared error = {}'.format(mse))\nCAT_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(CAT_rmse))\n\nplt.scatter(y_test,CATreg.predict(X_test))\nplt.plot(y_test,y_test,color='red')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('CatBoost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi-layer perceptron\n\nFirst we need to find the best parameters for the neural net using GridSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nmlp = MLPRegressor(random_state=0)\n\nparameters_grid = {\n    'hidden_layer_sizes': [(3,3),(3,4,3),(3,5,3),(10,),(5,5),(5,4,5),(4,3,4)],\n    'activation': ['relu'],\n    'solver': ['lbfgs'],\n    'max_iter': [5000]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngcv=GridSearchCV(estimator=mlp,param_grid=parameters_grid,cv=10,scoring='neg_root_mean_squared_error',n_jobs=-1)\nsearch=gcv.fit(X_train,y_train)\nsearch.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nNNreg = MLPRegressor(random_state=0, max_iter=5000,hidden_layer_sizes=(3,5,3),solver='lbfgs').fit(X_train, y_train)\nNN_cod=NNreg.score(X_test,y_test)\nprint('Coefficient of determination(R^2) = {}'.format(NN_cod)) \nmse=(metrics.mean_squared_error(NNreg.predict(X_test),y_test))\nprint('Mean squared error = {}'.format(mse))\nNN_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(NN_rmse))\n\n\nplt.scatter(y_test,NNreg.predict(X_test))\nplt.plot(y_test,y_test,color='red')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb=XGBRegressor(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcv = GridSearchCV(\n    estimator=xgb,\n    param_grid=parameters,\n    scoring = 'neg_root_mean_squared_error',\n    n_jobs = -1,\n    cv = 10,\n    verbose=True\n)\n\nsearch=gcv.fit(X_train,y_train)\nsearch.best_params_\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nXGBreg = XGBRegressor(learning_rate=0.05,max_depth=3,n_estimators=100,random_state=0)\nXGBreg.fit(X_train,y_train)\nXGB_cod=XGBreg.score(X_test,y_test)\nprint('Coefficient of determination(R^2) = {}'.format(XGB_cod)) \nmse=(metrics.mean_squared_error(XGBreg.predict(X_test),y_test))\nprint('Mean squared error = {}'.format(mse))\nXGB_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(XGB_rmse))\n\n\nplt.scatter(y_test,XGBreg.predict(X_test))\nplt.plot(y_test,y_test,color='red')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('XGBoost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf=RandomForestRegressor(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparameters={'max_depth': [10, 20, 30, 40, 50, None],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [50,100,200]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcv = GridSearchCV(\n    estimator=rf,\n    param_grid=parameters,\n    scoring = 'neg_root_mean_squared_error',\n    n_jobs = -1,\n    cv = 10,\n    verbose=True\n)\n\n\nsearch=gcv.fit(X_train,y_train)\nsearch.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nRFreg = RandomForestRegressor(n_estimators=100,max_depth=10,min_samples_leaf=4,min_samples_split=10,random_state=0)\nRFreg.fit(X_train,y_train)\nRF_cod=RFreg.score(X_test,y_test)\nprint('Coefficient of determination(R^2) = {}'.format(RF_cod)) \nmse=(metrics.mean_squared_error(RFreg.predict(X_test),y_test))\nprint('Mean squared error = {}'.format(mse))\nRF_rmse=mse**0.5\nprint('Root Mean squared error = {}'.format(RF_rmse))\n\n\nplt.scatter(y_test,RFreg.predict(X_test))\nplt.plot(y_test,y_test,color='red')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = {'Model':  ['PolyR', 'SVM','RF','NN','XGBoost','CatBoost'],\n        'RMSE': [POLY_rmse, SVR_rmse,RF_rmse,NN_rmse,XGB_rmse,CAT_rmse],\n        'Coeff Of Det':[POLY_cod,SVR_cod,RF_cod,NN_cod,XGB_cod,CAT_cod]\n        }\n\ndf = pd.DataFrame (data, columns = ['Model','RMSE','Coeff Of Det'])\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So XGBoost comes out to be the best for this dataset!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As mentioned earlier suggestions for improvement are always welcome.\n\nThank You!\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}