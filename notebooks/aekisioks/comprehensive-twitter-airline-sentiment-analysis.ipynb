{"cells":[{"metadata":{"_uuid":"cd884b94-7add-49cf-856f-9dbe0acf81ed","_cell_guid":"6b8f7b01-019d-49d5-a778-c020331d9277","trusted":true},"cell_type":"markdown","source":"\n# #### ANY FEEDBACK IN THE COMMENTS WILL BE HIGHLY APPRECIATED."},{"metadata":{"_uuid":"200dec1d-2805-4216-bea5-97fa648c5d40","_cell_guid":"31733a7a-11f9-4961-b732-c4b1abab85bb","trusted":true},"cell_type":"markdown","source":"Breakdown of this notebook:\n# 1. Loading the dataset: Load the data and import the libraries.\n# 2. Data Preprocessing:\n#      - Analysing missing data \n#      - Removing redundant columns.\n# 3. Visualising and counting sentiments of tweets for each airline\n# 4. Wordcloud plots for positive and negative tweets to visualise most frequent words for each.\n# 5. Analysing the reasons for negative tweets for each airline.\n# 6. Visualising negative tweet-sentiment relationship with dates.\n# 7. Predicting the tweet sentiments with tweet text data with:\n#       - Decision Tree Classifier\n#       - Random Forest Classifier\n# 8. Calculating accuracies, plotting the confusion matrix and comparing the models."},{"metadata":{"_uuid":"abab8519-a33b-4a9b-8eae-dc8459848ae7","_cell_guid":"edb615cf-0530-4f43-8c55-5dd18c743535","trusted":true},"cell_type":"markdown","source":"### References:- \n# I learnt a lot from this blog which shows you how to handle nlp data and implement data preprocessing and explanatory visualization for better understanding.\n# \n# https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"},{"metadata":{"_uuid":"64682f6a-ecbe-4f1a-8ade-4554443b8ad6","_cell_guid":"c89486b3-0581-4365-b16e-cb49e75f98b8","trusted":true},"cell_type":"markdown","source":"### Importing the libraries and loading the data"},{"metadata":{"_uuid":"6d656669-6eb2-4fc7-be68-5a72ab51cbb6","_cell_guid":"fe126028-e078-4893-abea-c428b02843ef","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n# Any results you write to the current directory are saved as output.","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"9ca9b913-a5f4-49ec-bed6-c9f5cd23a967","_cell_guid":"84a6e4e4-cae4-4d51-942c-2acd01ee0a7b","trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/Tweets.csv\")\ndf.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"2ef6c322-222c-40b3-8772-8cab971f351c","_cell_guid":"0b4487e6-d5bb-44bc-bbce-8720f731fbb6","trusted":true},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"_uuid":"acbdd5ef-b41a-46b0-84c5-4be785e8455f","_cell_guid":"98ef4c67-0fa5-4b6d-ad9b-fed9cb9e0fea","trusted":true},"cell_type":"markdown","source":"The first step should be to check the shape of the dataframe and then check the number of null values in each column.\n# \n# In this way we can get an idea of the redundant columns in the data frame depending on which columns have the highest number of null values."},{"metadata":{"_uuid":"79cda7bf-c83f-4eaa-b453-e8d121b7a4e6","_cell_guid":"1f04e8c3-7205-4521-a15d-dea341e93d19","trusted":true},"cell_type":"code","source":"print(\"Shape of the dataframe is\",df.shape)\nprint(\"The number of nulls in each column are \\n\", df.isna().sum())","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"b78be6d1-53de-4b22-9499-5a5470a00834","_cell_guid":"516a3d41-ddec-4da0-98fd-b692574dccb9","trusted":true},"cell_type":"markdown","source":"To get a better idea, lets calculate the percentage of nulls or NA values in each column"},{"metadata":{"_uuid":"9205e89d-92f3-4b17-a8ee-ce155805d229","_cell_guid":"2538bfef-ea3a-4a56-acdc-56b981ad9601","trusted":true},"cell_type":"code","source":"print(\"Percentage null or na values in df\")\n((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"af41f7d7-c474-4d17-9134-f5da8c1d77eb","_cell_guid":"689ac230-8f8c-469a-b114-77dba8cc3cb6","trusted":true},"cell_type":"markdown","source":"\n# \n#"},{"metadata":{"_uuid":"318b8887-560f-4b50-9a02-97b40048313f","_cell_guid":"e9063cc5-a9e2-4449-a537-e8a04fa74ee7","trusted":true},"cell_type":"markdown","source":"**tweet_coord , airline_sentiment_gold, negativereason_gold**  have more than 90% missing data. It will be better to delete these columns as they will not provide any constructive information.\n# \n#"},{"metadata":{"_uuid":"23877d0b-1473-41f1-918b-d09d32388e51","_cell_guid":"1ecb5f8d-b3e0-47ac-af2b-6c3fae174497","trusted":true},"cell_type":"code","source":"del df['tweet_coord']\ndel df['airline_sentiment_gold']\ndel df['negativereason_gold']\ndf.head()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"735af73f-6cfd-44d0-943c-f8adb9f508e2","_cell_guid":"7171f652-842c-4714-b574-0c9209859fd3","trusted":true},"cell_type":"markdown","source":"\n# \n# \n#"},{"metadata":{"_uuid":"9f6d8fc8-63a9-4f17-841e-f9f30aa99d4b","_cell_guid":"7d8e3ee3-aa6c-4fda-8e93-f8141af21bcf","trusted":true},"cell_type":"markdown","source":"### Airline sentiments for each airline\n#"},{"metadata":{"_uuid":"d7263aa3-f28d-4d84-881b-cf981de458a8","_cell_guid":"0466c3c7-48a4-4657-92ff-c96eb866bc02","trusted":true},"cell_type":"markdown","source":"- Firstly lets calculate the total number of tweets for each airline\n# - Then, we are going to get the barplots for each airline with respect to sentiments of tweets (positive,negative or neutral).\n# - This will give us a clearer idea about the airline sentiments , airlines relationship."},{"metadata":{"_uuid":"e71f785b-2beb-4a45-ae80-a666317b183d","_cell_guid":"228a4d27-036e-44fd-9ec9-8f30180a181b","trusted":true},"cell_type":"markdown","source":"\n# \n#"},{"metadata":{"_uuid":"045bd40e-5208-4d9b-8d2a-6378263bdedc","_cell_guid":"4984043c-e2fa-4bb3-92c4-366b0eb030d3","trusted":true},"cell_type":"code","source":"print(\"Total number of tweets for each airline \\n \",df.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nplt.figure(1,figsize=(12, 12))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    new_df=df[df['airline']==i]\n    count=new_df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count, color=['red', 'green', 'blue'])\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Moods of '+i)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"12d6eaf4-be0e-4ae4-ba7e-23479a5beba5","_cell_guid":"8f14048e-b3d9-4387-bcee-28af5a854a9f","trusted":true},"cell_type":"markdown","source":" - United, US Airways, American substantially get negative reactions.\n#  - Tweets for Virgin America are the most balanced."},{"metadata":{"_uuid":"d2227364-fbd5-4671-9e85-9146fbacfdbd","_cell_guid":"25a26a21-9003-419a-8245-333ed57ecd9c","trusted":true},"cell_type":"markdown","source":"## Most used words in Positive and Negative tweets"},{"metadata":{"_uuid":"376b4a2f-9cc4-4da7-97ca-b7c9526854d8","_cell_guid":"cced66f1-035f-4f50-ba49-498371ef039c","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"28e02db7-c817-48ec-9faf-cc5116011532","_cell_guid":"671dd33a-c339-45ba-8181-7a31a579b4ab","trusted":true},"cell_type":"markdown","source":"- The goal is to firstly get an idea of the most frequent words in negative tweets.\n# - Get idea about most frequent words in positive tweets."},{"metadata":{"_uuid":"8f6b45bf-6f6e-4908-9116-e1a7319f6f30","_cell_guid":"dacf1166-ce4c-44be-ba72-aa37e1fd7a91","trusted":true},"cell_type":"markdown","source":"\n#"},{"metadata":{"_uuid":"0c773ac4-78a6-4725-916e-74c0518f3401","_cell_guid":"fcaa8fb6-e683-4402-9ad3-898db6266b21","trusted":true},"cell_type":"markdown","source":"### Wordcloud for Negative sentiments of tweets"},{"metadata":{"_uuid":"35719d40-7fc0-4a48-bce8-44f9be0393be","_cell_guid":"623bf69a-e8a8-4c95-a5a1-308879a7ea92","trusted":true},"cell_type":"markdown","source":"Wordcloud is a great tool for visualizing nlp data. The larger the words in the wordcloud image , the more is the frequency of that word in our text data."},{"metadata":{"_uuid":"054f1e96-b443-4aff-b396-6e18a0eed068","_cell_guid":"e3f708f4-c2a3-4002-ae3f-8196910ae221","trusted":true},"cell_type":"markdown","source":"\n#"},{"metadata":{"_uuid":"19a29964-d546-416e-b1fc-779790ac9665","_cell_guid":"65c3cc9a-b7f0-4d3e-a292-2df3c412c7c6","trusted":true},"cell_type":"code","source":"new_df=df[df['airline_sentiment']=='negative']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3f0a739f-ddc9-4715-babf-5a1ba16ed31e","_cell_guid":"5a36b7cb-c0c7-461f-abf0-ce2ba2137419","trusted":true},"cell_type":"markdown","source":"\n# \n#"},{"metadata":{"_uuid":"52345efa-1b72-4a77-b400-a2f86ba5583b","_cell_guid":"863bfd20-169a-491f-9a11-3e37c994871d","trusted":true},"cell_type":"markdown","source":"### Wordcloud for positive reasons"},{"metadata":{"_uuid":"819b8517-8274-49f4-9033-72deda737484","_cell_guid":"8d3410ea-a3c1-4589-8894-cedea650322c","trusted":true},"cell_type":"markdown","source":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line. Easy, right!"},{"metadata":{"_uuid":"cc2e22ad-e3f0-45a1-977e-ffac91bc9e0a","_cell_guid":"ee5f7fc9-42cf-4c31-a875-8933538b15c1","trusted":true},"cell_type":"markdown","source":"\n#"},{"metadata":{"_uuid":"a6ec6c3d-5b4b-48ec-ac23-0314195a219a","_cell_guid":"e18314a5-1f57-4ec6-abc0-66c21f86bc40","trusted":true},"cell_type":"code","source":"new_df=df[df['airline_sentiment']=='positive']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8260c343-447c-47cb-b6f1-3f6099aca4f5","_cell_guid":"5d3da13b-654b-4c79-a468-03a4cb841a8b","trusted":true},"cell_type":"markdown","source":"#### Lets try and calculate the highest frequency words in postive sentimental tweets"},{"metadata":{"_uuid":"99c464e5-9f15-453f-88f7-9b82d85de021","_cell_guid":"ba491f19-1ed2-485e-be99-3dc650218bee","trusted":true},"cell_type":"code","source":"\n\n# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0d3ca644-ccb4-4a78-b552-1db7f91e93a7","_cell_guid":"a0fbace7-09cc-4a52-98e4-14e1d11c40b4","trusted":true},"cell_type":"markdown","source":"* Words like **Thanks**, **best**, **customer** , **love**, **flying** , **good** are understandably present in the **most frequent** words of positive tweets. \n# * However, other than these, most of the words are stop words and need to be filtered. We will do so later.\n# * Lets try and visualize the reasons for negative tweets first !!"},{"metadata":{"_uuid":"27b3bb34-fe12-4a55-8bfc-4ea81a5491b2","_cell_guid":"9a8e8fcd-c09c-41a0-a9cd-08a46623c53e","trusted":true},"cell_type":"markdown","source":"\n# \n# \n# \n# \n#"},{"metadata":{"_uuid":"408cc979-d133-45ef-b975-90b5627a07d3","_cell_guid":"5dad45d0-4302-4d28-81dc-398d1390b0d7","trusted":true},"cell_type":"markdown","source":"### What are the reasons for negative sentimental tweets for each airline ?"},{"metadata":{"_uuid":"bdc54211-e373-4bef-8475-ae037693bf4d","_cell_guid":"638d2e22-c8c7-4e5a-8dbd-870862b46911","trusted":true},"cell_type":"markdown","source":"We will explore the **negative reason** column of our dataframe to extract conclusions about negative sentiments in the tweets by the customers"},{"metadata":{"_uuid":"71b10d12-0242-46b7-92ab-7e8041171aff","_cell_guid":"f22fcf17-b52a-4c9d-b3b7-e5062b2a4e76","trusted":true},"cell_type":"code","source":"#get the number of negative reasons\ndf['negativereason'].nunique()\n\nNR_Count=dict(df['negativereason'].value_counts(sort=False))\ndef NR_Count(Airline):\n    if Airline=='All':\n        a=df\n    else:\n        a=df[df['airline']==Airline]\n    count=dict(a['negativereason'].value_counts())\n    Unique_reason=list(df['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\ndef plot_reason(Airline):\n    \n    a=NR_Count(Airline)\n    count=a['count']\n    Index = range(1,(len(a)+1))\n    plt.bar(Index,count, color=['red','yellow','blue','green','black','brown','gray','cyan','purple','orange'])\n    plt.xticks(Index,a['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons for '+Airline)\n    \nplot_reason('All')\nplt.figure(2,figsize=(13, 13))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"49d665d3-451e-44d7-862f-07d5be25717b","_cell_guid":"6761de62-2a00-4233-a635-a707a2e8202e","trusted":true},"cell_type":"markdown","source":"- Customer Service Issue is the main neagtive reason for US Airways,United,American,Southwest,Virgin America\n# - Late Flight is the main negative reason for Delta  \n# - Interestingly, Virgin America has the least count of negative reasons (all less than 60)\n# - Contrastingly to Virgin America, airlines like US Airways,United,American have more than 500 negative reasons (Late flight, Customer Service Issue)"},{"metadata":{"_uuid":"aca60339-399f-44c9-a65c-34ea0b1b17b9","_cell_guid":"f8680f0e-9c76-48d5-aec2-7408ec764864","trusted":true},"cell_type":"markdown","source":"\n# \n# \n#"},{"metadata":{"_uuid":"526c0c61-4726-4c1d-a378-ee980ce28eb4","_cell_guid":"5f9145b1-3434-4eb4-bb3f-0ba22c09d71b","trusted":true},"cell_type":"markdown","source":"### Is there a relationship between negative sentiments and date ?"},{"metadata":{"_uuid":"f2106f0f-7c7e-48e0-a95a-41658de4b3ea","_cell_guid":"41d9d6e6-974b-46ed-bdff-c16c5829bbce","trusted":true},"cell_type":"markdown","source":"Our dataframe has data from **2015-02-17** to **2015-02-24**\n# \n# It will be interesting to see if the date has any effect on the sentiments of the tweets(*especially negative !*). We can draw various coclusions by visualizing this."},{"metadata":{"_uuid":"27854e42-fd41-4449-a02a-3f359be0f139","_cell_guid":"1f7aa9f0-e7ad-4e85-a847-6fd6b0c847bf","trusted":true},"cell_type":"code","source":"date = df.reset_index()\n#convert the Date column to pandas datetime\ndate.tweet_created = pd.to_datetime(date.tweet_created)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.tweet_created = date.tweet_created.dt.date\ndate.tweet_created.head()\ndf = date\nday_df = df.groupby(['tweet_created','airline','airline_sentiment']).size()\n# day_df = day_df.reset_index()\nday_df","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"6d05952d-23b4-4550-b8a2-7aa5f0464bfe","_cell_guid":"84dbb3ae-c7b5-43aa-801d-16f31d0d0684","trusted":true},"cell_type":"markdown","source":"This shows the sentiments of tweets for each date from **2015-02-17** to **2015-02-24** for every airline in our dataframe.\n# \n# Our next step will be to plot this and get better visualization for negative tweets."},{"metadata":{"_uuid":"a17f5d6d-84f1-4df4-9bc1-dde3ff2715d5","_cell_guid":"794e122d-b97e-4105-b19d-76a34daa3039","trusted":true},"cell_type":"markdown","source":"\n# \n#"},{"metadata":{"_uuid":"bc582611-a93b-4491-9cc8-703186d95a78","_cell_guid":"3669557e-26fe-4d67-9d24-8b28d2a983e4","trusted":true},"cell_type":"code","source":"day_df = day_df.loc(axis=0)[:,:,'negative']\n\n#groupby and plot data\nax2 = day_df.groupby(['tweet_created','airline']).sum().unstack().plot(kind = 'bar', color=['red', 'green', 'blue','yellow','purple','orange'], figsize = (15,6), rot = 70)\nlabels = ['American','Delta','Southwest','US Airways','United','Virgin America']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"b0fde168-375a-49b7-82f7-6ad6d9f24683","_cell_guid":"3ee5feaf-a069-4276-8cae-e9b3cea77017","trusted":true},"cell_type":"markdown","source":"- Interestingly, **American** has a sudden upsurge in negative sentimental tweets on **2015-02-23**, which reduced to half the very next day **2015-02-24**. (*I hope American is doing better these days and resolved their Customer Service Issue as we saw before*)\n# - **Virgin America** has the least number of negative tweets throughout the weekly data that we have. It should be noted that the total number of tweets for **Virgin America** was also significantly less as compared to the rest airlines, and hence the least negative tweets.\n# - The negative tweets for all the rest airlines is slightly skewed towards the end of the week !"},{"metadata":{"_uuid":"6d54398f-79c5-4eca-8619-8a2eef10becd","_cell_guid":"f62b1cde-c94e-48f0-9ec3-4da03e58f74b","trusted":true},"cell_type":"markdown","source":"### Preprocessing the tweet text data"},{"metadata":{"_uuid":"2b18fb78-52ad-4312-95a5-5987a3594042","_cell_guid":"38583e42-b8e4-4833-9826-93b501512ea1","trusted":true},"cell_type":"markdown","source":"Now, we will clean the tweet text data and apply classification algorithms on it"},{"metadata":{"_uuid":"0f8cfdb5-44f4-495b-9b05-10c969731e26","_cell_guid":"50940605-3fe2-4202-bcdc-cc487791fa02","trusted":true},"cell_type":"code","source":"def tweet_to_words(tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words ))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"6004513a-99bc-41f2-9c4d-0091d32cab3c","_cell_guid":"9c02ba76-cc02-4ac3-845e-0ba1f5336d07","trusted":true},"cell_type":"code","source":"df['clean_tweet']=df['text'].apply(lambda x: tweet_to_words(x))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f5a6a5b2-0f4e-409a-886e-ccd19cd28ddd","_cell_guid":"cd0d5789-02f4-4d55-a34f-83bf7ec3a5a5","trusted":true},"cell_type":"markdown","source":"\n# \n#"},{"metadata":{"_uuid":"731e2cd6-2a38-4c1a-83c4-342bd0f8a37f","_cell_guid":"05d01a0c-d193-4be6-9544-5668a617b64e","trusted":true},"cell_type":"markdown","source":"The data is split in the standard 80,20 ratio."},{"metadata":{"_uuid":"5b314938-ce87-4c45-b348-0d2a089a418a","_cell_guid":"d720d829-71df-43b1-9c04-93a678140359","trusted":true},"cell_type":"code","source":"train,test = train_test_split(df,test_size=0.2,random_state=42)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ece7082c-14ea-4021-a99c-f2f329ac2965","_cell_guid":"6bc7c73a-07da-49b7-8278-fb7a12299849","trusted":true},"cell_type":"code","source":"train_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3551fd71-1787-419c-9a3c-a8825a4d2331","_cell_guid":"0281ddce-c241-44a4-ae1a-050edf31ad45","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d61c0319-9015-494c-a14d-901295658be9","_cell_guid":"bc7a85c7-1b05-4403-bcdc-6e4faa09a73b","trusted":true},"cell_type":"markdown","source":"### Prediciting sentiments from tweet text data"},{"metadata":{"_uuid":"6690ece7-3781-4f28-991e-4d63ad33ceb7","_cell_guid":"7f0bf9ae-347e-4e01-afe8-977bd417ed47","trusted":true},"cell_type":"markdown","source":"- Decision Tree Classifier\n# - Random Forest Classifier"},{"metadata":{"_uuid":"9dafdaef-7ab4-43a7-b530-9dc89985fa3f","_cell_guid":"91ad12f9-360a-487a-a821-47570ba32146","trusted":true},"cell_type":"code","source":"Classifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200)]","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"3ca63505-d0a4-451a-bcd7-b85e334c8621","_cell_guid":"632310b7-da4d-47cf-ba1a-9cd33618944f","trusted":true},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['airline_sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['airline_sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['airline_sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print(classification_report(pred,test['airline_sentiment']))\n    cm=confusion_matrix(pred , test['airline_sentiment'])\n    plt.figure()\n    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\n    plt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\n    plt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\n    plt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0f8639b1-335a-46df-a82c-f5ab3f71c274","_cell_guid":"3e41551c-25f7-4bb6-afc0-9a9a297986c1","trusted":true},"cell_type":"markdown","source":"- As we you can see above we have plotted the **confusion matrix** for predicted sentiments and actual sentiments (negative,neutral and positive)\n# - **Random Forest Classifier** gives us the best accuracy score, precision scores according to the classification report.\n# - The confusion matrix shows the TP,TN,FP,FN for all the 3 sentiments(negative,neutral and positive),Here also **Random Forest Classifier** gives **better** results than the **Decision Tree Classifier**."},{"metadata":{"_uuid":"eedf14ac-7054-44a4-86c3-85f8deac29e9","_cell_guid":"df5d20ee-bad8-402e-8ca9-c543b064c05e","trusted":true},"cell_type":"markdown","source":"\n# \n# #### Please feel free to upvote and comment if you found this to be helpful !!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}