{"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"b9e26a2b-21c9-4664-8460-38f88c9b9a53","_uuid":"490e73aa4d371d1d7fc828ec9fba0f408f74548f"},"cell_type":"markdown","source":"## pandas, seaborn, and sklearn required"},{"metadata":{"_cell_guid":"fe76a2b3-886c-4231-b284-f6a7cc8d2e2f","_uuid":"968861e8ad1f0796e1a146a5211a353955addb65","scrolled":true},"cell_type":"code","outputs":[],"source":"import pandas as pd ##pandas dataframes are often used for statistical analysis,\nimport numpy as np ##calculate mean and standard deviation\nimport seaborn as sb ##includes convenient heatmaps and boxplots\nimport sklearn.linear_model as sklm ##Includes Logistic Regression, which will be tested for predictive capability\nimport sklearn.decomposition as skdc ##Includes Principal Component Analysis, a method of dimensionality reduction\nimport sklearn.pipeline as skpl ##Convenient module for calculating PCs and using them in logistic regression\n%matplotlib inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":1},{"metadata":{"_cell_guid":"5e29de4d-159a-4b1f-abb8-7cca299bf8c8","collapsed":true,"_uuid":"e7cb0f6a285197bfddf4aaa10ed36c285708c939"},"cell_type":"code","outputs":[],"source":"data = pd.read_csv('../input/data.csv') ##store csv in Pandas DataFrame","execution_count":2},{"metadata":{"_cell_guid":"8a27b46a-dbe5-40fa-9641-930760a117b1","_uuid":"54ac27ead4bdda92bba8e2c6281b39b4a61675b2"},"cell_type":"code","outputs":[],"source":"data.head() ##print first five rows of dataframe as a table","execution_count":3},{"metadata":{"_cell_guid":"71fc0337-ef95-40b5-a552-fa5b85580676","collapsed":true,"_uuid":"8a242198bd0eb5ae8436353e123b2303e9273f0c"},"cell_type":"code","outputs":[],"source":"data = data.drop('id', 1) ##id column provides no information on data\ndata = data.drop('Unnamed: 32', 1) ##column of missing values","execution_count":4},{"metadata":{"_cell_guid":"fdafd4a4-e0cb-41be-80f9-1a7c1f52dd52","_uuid":"b36db3537b7c7b18bda3ae73198ab14bfa531e47"},"cell_type":"code","outputs":[],"source":"print(pd.get_dummies(data['diagnosis']).head(1))#dummy variable to represent categorical data as numeric\ndata['diagnosis_dummies'] = pd.get_dummies(data['diagnosis']).iloc[:,1]#extract Malignant dummy category to dataframe\n###only inclulde one dummy category to avoid multicollinearity, either one could be chosen","execution_count":5},{"metadata":{"_cell_guid":"f3a62711-4889-42a5-9149-efb7d1e61c34","_uuid":"e2a460c90d26af7bd58598fe31ed378ed0091a19","scrolled":true},"cell_type":"code","outputs":[],"source":"datacorr = data.corr() #correlation matrix, showing correlation between each variable and all the others\ndata.corr().head()","execution_count":6},{"metadata":{"_cell_guid":"71e4d9c4-4664-4d4c-ba89-ecdde0fc7f74","_uuid":"041dfe1793901a6c8d26a806031f63316b8d8b42","scrolled":true},"cell_type":"code","outputs":[],"source":"sb.heatmap(datacorr, cmap = 'bwr') #heatmap of correlation matrix\n###darker colors represent higher correlation, several pairs of variables are highly correlated.","execution_count":7},{"metadata":{"_cell_guid":"d65f6891-c4d5-4c82-b2d1-544facb3ce6e","_uuid":"6f4023c3e9e42001902650af917b9277d9dec219"},"cell_type":"markdown","source":"### Two highly correlated variables should not be both used in model. PCA will later be performed to explain the same variance while avoiding multicollinearity"},{"metadata":{"_cell_guid":"4de8c29d-6428-4627-a0cf-510a41fa3bf3","_uuid":"ea82c17498a60d40cc0eb369da6ad8696d9f63ca"},"cell_type":"code","outputs":[],"source":"sb.boxplot(x=data['concave points_mean'], y=data['diagnosis'], data=data, linewidth=2.5) #boxplot\n###boxplot shows the distribution by classification of concave points_mean, chosen because \n###it had the highest correlation with the diagnosis dummy variable. There is a clear difference in distributions.","execution_count":8},{"metadata":{"_cell_guid":"59274281-9faa-4dfb-9662-ca2a31b20fef","_uuid":"d6d830a412b330d55bda55ce539bda135b35a347"},"cell_type":"code","outputs":[],"source":"sb.boxplot(x=data['texture_se'], y=data['diagnosis'], data=data, linewidth=2.5)\n###For comparison, this is the same boxplot construct with a variable that is minimally correlated with diagnosis","execution_count":9},{"metadata":{"_cell_guid":"34338d6f-a344-4fe1-aee1-8fc10e08d7c1","collapsed":true,"_uuid":"82886835f3750ecc8ec8dc456a06081bd327736c"},"cell_type":"code","outputs":[],"source":"def standardization(x): #Define function to standardize the data, since all variables are not in the same units\n    xmean = np.mean(x) ##calculate mean\n    sd = np.std(x) ##calculate standard deviation \n    x_z = (x - xmean) / sd ##calculate standardized value to return\n    return(x_z)","execution_count":10},{"metadata":{"_cell_guid":"1d445e81-b243-443f-ab84-9d20e1b76140","_uuid":"541f62239326c5d0e830e1322e87b971895cfcaa"},"cell_type":"code","outputs":[],"source":"data_stnd = data.drop(['diagnosis','diagnosis_dummies'], 1).apply(standardization,broadcast = True) \n##drop response variable and standardize predictor variables\ndata_stnd.head()","execution_count":11},{"metadata":{"_cell_guid":"5a21af2e-ae91-42c7-a23a-19898de0b8e2","_uuid":"641bb456d8409dff5b90637dd090fb61365e3667"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"718f756f-b4f4-4f5f-b0f3-9f97a195f463","collapsed":true,"_uuid":"42c84f97843328fc5e8443a4afff76afad067334"},"cell_type":"code","outputs":[],"source":"X = data_stnd #store predictor variables\ny = data['diagnosis_dummies'] #store response variable\npca = skdc.PCA() #empty model space","execution_count":12},{"metadata":{"_cell_guid":"01394b37-f78c-489d-91eb-b028741cc6de","collapsed":true,"_uuid":"ee1afd5bd3883db8ec98c300ea87a9f83d29dca4"},"cell_type":"code","outputs":[],"source":"pcafit = pca.fit_transform(X,y) ##apply dimensionality reduction to X","execution_count":14},{"metadata":{"_cell_guid":"9f275c2f-36db-49aa-829e-f7d8b9789eb9","_uuid":"c2ee4064bb4b2991c42ea95ae78e4a5fd92c0fc7","scrolled":false},"cell_type":"code","outputs":[],"source":"var_explained = pca.explained_variance_ratio_ #ratio of variance each PC explains\nprint(pd.Series(var_explained))\n###Since 29 components aren't necessary, the last 20 PCs will be disregarded \n###since they explain less than.01 of the variance\nprint(sum(var_explained[0:10]))\n##indeed,the first 10 PCs explain 95% of the variance","execution_count":15},{"metadata":{"_cell_guid":"2516c686-651c-4a25-a683-5a11c76d7dc9","collapsed":true,"_uuid":"45f100e6610bafda3723157506cb40dffbf030b0"},"cell_type":"code","outputs":[],"source":"pca = skdc.PCA(n_components = 10) #only include first 10 components\nlogreg = sklm.LogisticRegression()#empty model space\npipeline = skpl.Pipeline([('pca', pca), ('logistic', logreg)]) #create pipeline from pca to logregression space","execution_count":18},{"metadata":{"_cell_guid":"8293da2d-cac4-47b6-a634-6828769a9b26","_uuid":"178362abd659d25c877ea69ba60a30c774cf618b"},"cell_type":"markdown","source":"### Leave one out Cross Validation will be used to test if logistic regression using the first 10 PCs is a useful model"},{"metadata":{"_cell_guid":"4c1a86d3-9aa6-4b89-aa28-462a52c78893","collapsed":true,"_uuid":"d43ce02bc99090877d5de2e9bcf55fffcefa3ea6"},"cell_type":"code","outputs":[],"source":"predMalignantRight = 0 #create count variables\npredMalignantWrong = 0\npredBenignRight = 0\npredBenignWrong = 0","execution_count":19},{"metadata":{"_cell_guid":"5ea27747-9ddc-42ce-8952-5fe665d70648","_uuid":"771131f42a053f0bb19e48f606dc7f4aaafc56b8","scrolled":true},"cell_type":"code","outputs":[],"source":"for i in range(0,569): #run through each row in data set\n    trainX = X.drop(i, 0) #train model with predictor dataframe, remove single row\n    trainy = y.drop(i,0) #train model with response array, remove single row\n    testX = X.iloc[i,:].values.reshape(1,30) #Removed row will be test predictor (Got error message before using values.reshape)\n    testy = y[i] #Removed value will be test response\n    fit = pipeline.fit(trainX, trainy) #fit model\n    prediction = pipeline.predict(testX) #test model with left out value\n    if prediction == 1 and testy == 1:\n        predMalignantRight += 1\n    elif prediction == 1 and testy == 0:\n        predMalignantWrong += 1\n    elif prediction == 0 and testy == 1:\n        predBenignWrong += 1\n    else:\n        predBenignRight += 1","execution_count":20},{"metadata":{"_cell_guid":"e0eebabc-8a51-42ff-bc8a-b90f45eada97","_uuid":"f93b806e0870003d1bb7b521f96ad7fdb498de5a"},"cell_type":"code","outputs":[],"source":"print(predMalignantRight,predMalignantWrong,predBenignRight,predBenignWrong)","execution_count":24},{"metadata":{"_cell_guid":"4bc9a8ea-f763-4bd7-b1e8-f4144f08271d","_uuid":"660aab0aaa6baec2a36d9a8ad197e49723d7b19f"},"cell_type":"code","outputs":[],"source":"###Time to create a nice confusion matrix to visualize\nc = {'Predicted Benign' : pd.Series([predBenignRight, predBenignWrong],index=['Actual Benign', 'Actual Malignant']),\n    'Predicted Malignant': pd.Series([predMalignantWrong, predMalignantRight], index=['Actual Benign','Actual Malignant'])}\nconfusionmat = pd.DataFrame(c)\nconfusionmat\n###nearly 98% of the values lie on the correct diagonal","execution_count":25},{"metadata":{"_cell_guid":"46a0e96a-ebf3-43cf-a38e-ea53da230b44","collapsed":true,"_uuid":"505edf64cedbdf33c33034e76c3a348d7c9eb3ec"},"cell_type":"code","outputs":[],"source":"###Now sensitivity and specificity will be calculated\nmr,mw = float(predMalignantRight), float(predMalignantWrong)\nbw,br = float(predBenignWrong), float(predBenignRight)\nsens = mr/(mr+mw) #calculate sensitivity, or rate of correctly predicting disease\nspec = br/(br+bw) #calculate specificity, or rate of correctly predicting no disease\nacc = (sens + spec)/2 #calculate balanced accuracy, or average of sensitivty and specificity\nmis = (mw+bw)/(mw+bw+mr+br) #calculate misclassification rate","execution_count":22},{"metadata":{"_cell_guid":"e404acda-e42f-40f5-a2e8-0d2ba531c57f","_uuid":"1b1222e950c8b5e9a23970e0c292cea62860ad53"},"cell_type":"code","outputs":[],"source":"###create series of values, then convert to dataframe to print as table\noutputseries = pd.Series([sens,spec,acc,mis],index=['Sensitivity','Specificity','Balanced Accuracy','Misclassification rate'])\noutputdf = pd.DataFrame(outputseries)\noutputdf.columns = [''] #blank header name\noutputdf.head()","execution_count":23},{"metadata":{"_cell_guid":"ee009975-a659-4b82-8ae0-9412aa1dea12","_uuid":"dc75dec5cbf80d98cbbfc8861545e17fb5da7aba"},"cell_type":"markdown","source":""}],"nbformat_minor":1}