{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Movie Recommender System based on Wiki-plots using sentence embeddings**\n![recommended movie?](https://alvinalexander.com/sites/default/files/2017-09/netflix-christmas-movie-suggestions.jpg)*You may like these?*\n\n#### Dataset contains movie plots scraped from wikipedia from 1902-2017 from approximately 22 regions along with important metadata\n<!-- blank line -->\n----\n## Content (plot) based Recommender System\n### We embed all sentences within the plots using Google's [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175) and compare the input plot's associated embeddings using [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)"},{"metadata":{},"cell_type":"markdown","source":"### **Imports**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom nltk import sent_tokenize\nfrom tqdm import tqdm\nfrom scipy import spatial\nfrom operator import itemgetter\ntqdm.pandas()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotly imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"movie = pd.read_csv('../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\nfull_mov = pd.read_csv('../input/movie-database/full_mov_db.csv')\nfull_mov.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(movie)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping duplicate plots (multi lingual movie releases generally have wiki pages for more than one language versions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Drop duplicates\nmovie = movie.drop_duplicates(subset='Plot', keep=\"first\")\nlen(movie)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.reset_index(inplace=True)\nmovie.drop(columns=['index'],inplace=True)\nmovie.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** bare-bones EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['word count'] = movie['Plot'].apply(lambda x : len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['word count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='no of plots',\n    title='Plot Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['Origin/Ethnicity'].value_counts().iplot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['Release Year'].value_counts().iplot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Plot text preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_plot(text_list):\n    clean_list = []\n    for sent in text_list:\n        sent = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-.:;<=>?@[\\]^`{|}~\"\"\"), '',sent)\n        sent = sent.replace('[]','')\n        sent = re.sub('\\d+',' ',sent)\n        sent = sent.lower()\n        clean_list.append(sent)\n    return clean_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Embedding using USE** (for more info refer - [USE tutorial](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb))"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_emb_list = []\nwith tf.Graph().as_default():\n    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n    messages = tf.placeholder(dtype=tf.string, shape=[None])\n    output = embed(messages)\n    with tf.Session() as session:\n        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        for plot in tqdm_notebook(full_mov['Plot']):\n            sent_list = sent_tokenize(plot)\n            clean_sent_list = clean_plot(sent_list)\n            sent_embed = session.run(output, feed_dict={messages: clean_sent_list})\n            plot_emb_list.append(sent_embed.mean(axis=0).reshape(1,512))            \nfull_mov['embeddings'] = plot_emb_list\nfull_mov.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pickling the embeddings for future (re)use"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_mov.to_pickle('./updated_mov_df_use_2.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Similar Movie function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def similar_movie(movie_name,topn=5):\n    plot = full_mov.loc[movie_name,'Plot'][:1][0]\n    with tf.Graph().as_default():\n        embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n        messages = tf.placeholder(dtype=tf.string, shape=[None])\n        output = embed(messages)\n        with tf.Session() as session:\n            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n            sent_list = sent_tokenize(plot)\n            clean_sent_list = clean_plot(sent_list)\n            sent_embed2 = (session.run(output, feed_dict={messages: clean_sent_list})).mean(axis=0).reshape(1,512)\n            similarities = []\n            for tensor,title in zip(full_mov['embeddings'],full_mov.index):\n                cos_sim = 1 - spatial.distance.cosine(sent_embed2,tensor)\n                similarities.append((title,cos_sim))\n            return sorted(similarities,key=itemgetter(1),reverse=True)[1:topn+1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing our model - using Interstellar's plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_mov.set_index('Title', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar_movie('Interstellar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **The results seem to make sense**\n### You can use other embeddings like BERT/ELMo/Flair to get better/different results\n### An upvote will be appreciated :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}