{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nimport sklearn\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n%matplotlib inline\nfrom sklearn.datasets import *\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras import optimizers \n\n\n#필요한 라이브러리들을 import 하겠습니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading dataset\nwine = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the form of data\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information about the data columns\nwine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'fixed acidity', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'volatile acidity', data = wine)\n\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'citric acid', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'residual sugar', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'chlorides', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'free sulfur dioxide', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'total sulfur dioxide', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'sulphates', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'alcohol', data = wine)\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'pH', data = wine)\n\n\nfig = plt.figure(figsize = (15,9))\nsns.boxplot(x = 'quality', y = 'density', data = wine)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quality 와 요소별 그래프를 시각화함으로써, 어떤 요소가 상관관계가 있는지 파악해보았습니다.\n# 그 결과, 모든 요소가 quality에 영향을 주는 것은 아니라는 걸 확인할 수 있었습니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 - Bad / 2 - Average / 3 - Excellent 으로 등급을 셋으로 나누어 모델을 만들어보겠습니다.\n#quality = 2,3 --> Bad\n#quality = 4,5,6 --> Average\n#quality = 7,8 --> Excellent\n#Create an empty list called Reviews\n\nwine_reviews = []\nfor i in wine['quality']:\n    if i >= 2 and i <= 4:\n        wine_reviews.append('1')\n    elif i >= 5 and i <= 6:\n        wine_reviews.append('2')\n    elif i >= 7 and i <= 8:\n        wine_reviews.append('3')\nwine['Grade'] = wine_reviews\n\nwine['Grade'].value_counts()\n\nsns.countplot(wine['Grade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\nX = X.drop('Grade', axis=1)\nX = X.drop('fixed acidity', axis=1)\nX = X.drop('residual sugar', axis=1)\nX = X.drop('density', axis=1)\nX = X.drop('pH', axis=1)\nX = X.drop('alcohol', axis=1)\ny = wine['Grade']\n\nprint(X.head)\nwine=X\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 실패 기록 : 육안으로 보았을 때 영향을 줄 것 같은 요소들만 포함하는 것으로 data 를 조정했었는데, \n그걸 위와 같이 drop 으로 하는 게 아니라 PCA 를 이용하는 게 훨씬 더 깔끔하고 엄밀하다는 걸 알게 되어 코드를 수정하였습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nX = wine.drop('quality',axis = 1)\nX = X.drop('Grade',axis=1)\ny = wine['Grade']\nprint(X.head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view final data\nprint(X.columns)\nprint(X.head(10))\nprint(y.head(10))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now scale the data using StandardScalar for PCA**\nWhy? : 영향을 주는 주 요소의 갯수가 적다는 걸 파악했으므로, 좀 더 간단하게 하기 위하여!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view the scaled features\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_pca = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the graph to find the principal components\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 8개 정도의 원소들로 대부분 quality 가 설명 가능하다는 걸 그래프를 통해 확인하였으니, \nPCA 로 옮겨지는 dim(X) 를 8로 설정하겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"#AS per the graph, we can see that 8 principal components attribute for 90% of variation in the data. \n#we shall pick the first 8 components for our prediction.\npca_new = PCA(n_components=8)\nX_new = pca_new.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_new)\nX_new = pd.DataFrame(X_new)\nprint(type(X_new))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.2,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\nprint(type(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train)\nprint(y_train)\nprint(x_test)\nprint(y_test)\nprint(type(x_train))\nprint(type(y_train))\nprint(type(x_test))\nprint(type(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\n# concatenate our training data back together\nX = pd.concat([x_train, y_train], axis=1)\nprint(X.head(10))\nprint(X.Grade.head(10))\n\n#X_new = pd.DataFrame(X_new)\n#mask = df['A'] == 'foo'\n#df.query('A == \"foo\"')\n# separate minority and majority classes\nBad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n\nprint(Bad.head(20))\nprint(Average.head(20))\nprint(Excellent.head(20))\n# upsample minority\nBad_upsampled = resample(Bad,\n                          replace=True, # sample with replacement\n                          n_samples=int(len(Average)), # match number in majority class\n                          random_state=42) # reproducible results\n\nExcellent_upsampled = resample(Excellent,\n                          replace=True, # sample with replacement\n                          n_samples=int(len(Average)), # match number in majority class\n                          random_state=42) # reproducible results\n\n\n# combine majority and upsampled minority\nupsampled = pd.concat([Average, Bad_upsampled,Excellent_upsampled])\nupsampled = upsampled.sample(frac=1)\n# check new class counts\n\nprint(type(upsampled))\nupsampled.Grade.value_counts()\nprint(upsampled)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = upsampled.drop('Grade',axis=1)\ny_train = upsampled['Grade']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 문제점 : f1-score 를 통해서 oversampling하지 않은 Randomforest 모델과 비교하여 이 모델을 평가하였을 때, \nGrade 3 (Excellent)가 소폭 상승하고 , Grade 1(Bad)가 검출되었으며,  Grade 2(Average) 의 경우 거의 변화하지 않았음을 알 수 있었다.\n따라서 지금 우리는 두 가지의 개선할 부분을 생각해볼 수 있다 ::\n1)Oversampling 의 방법론이 잘못 되었다 ; oversampling 을 거쳤음에도 data imbalance를 해결하지 못하여 문제가 발생하였다. /\n2)RandomForest 모델이 적절하지 않은 모델이다 \n"},{"metadata":{},"cell_type":"markdown","source":"> 따라서, 생각해볼 수 있는 해결책은 1) 다른 방법의 oversampling 을 선택한다. 2) undersampling을 해본다 3) RandomForest 가 아닌 다른 모델을 적용해본다   ;; 정도로 생각한다.\n우선은, undersampling을 시도해보겠다."},{"metadata":{"trusted":true},"cell_type":"code","source":"Bad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downsample majority\nAverage_downsampled = resample(Average,\n                          replace=False, # sample with replacement\n                          n_samples=len(Bad), # match number in majority class\n                          random_state=42) # reproducible results\n\n\n# combine majority and upsampled minority\ndownsampled = pd.concat([Average_downsampled, Bad ,Excellent])\ndownsampled = downsampled.sample(frac=1)\n# check new class counts\n\nprint(type(downsampled))\ndownsampled.Grade.value_counts()\nprint(downsampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = downsampled.drop('Grade',axis=1)\ny_train = downsampled['Grade']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rfc = rfc.predict(x_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 결과값... 총체적 난국... 오히려 안하느니만 못해졌다.\n아마 훈련시킬 data set의 크기 자체가 매우 축소되어 이런 현상이 벌어진 듯 하다.\n적절하게 조율할 목적으로- Randomsampling 을 over와 under 를 섞어서 써보겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"Bad = X.query('Grade == \"1\"')\n\nAverage = X.query('Grade == \"2\"')\nExcellent = X.query('Grade == \"3\"')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# upsample minority\nBad_upsampled = resample(Bad,\n                          replace=True, # sample with replacement\n                          n_samples=(int(len(Average)*(1/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n\nExcellent_upsampled = resample(Excellent,\n                          replace=True, # sample with replacement\n                          n_samples=(int(len(Average)*(1/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downsample majority\nAverage_downsampled = resample(Average,\n                          replace=False, # sample with replacement\n                          n_samples=(int(len(Average)*(1/1.65))), # match number in majority class\n                          random_state=42) # reproducible results\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mixsampled = pd.concat([Bad_upsampled,Average_downsampled,Excellent_upsampled])\nmixsampled = mixsampled.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = mixsampled.drop('Grade',axis=1)\ny_train = mixsampled['Grade']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=220)\nrfc.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rfc = rfc.predict(x_test)\n\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\n\n#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> randomsampling 을 섞어서 써주니- 확실히 Grade 1와 Grade 3의 f1- score 가 상승함을 볼 수 있었다. \n> Grade 3는 매우 소폭 상승하였으며, Grade 1의 지표가 확실히 개선되는 걸 확인할 수 있었다. \n> 하지만, 아직 많이 부족하다. 어떻게 이 문제를 해결할 수 있을까?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}