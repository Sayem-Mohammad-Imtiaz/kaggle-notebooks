{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nimport statsmodels.api as sm\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport seaborn as sns\nsns.set_context(\"paper\", font_scale=1.3)\nsns.set_style('white')\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read csv file\ndf = pd.read_csv(r'/kaggle/input/brent-oil-prices/BrentOilPrices.csv',parse_dates=['Date'])\n#Sort dataset by column Date\ndf = df.sort_values('Date')\ndf = df.groupby('Date')['Price'].sum().reset_index()\ndf.set_index('Date', inplace=True)\ndf=df.loc[datetime.date(year=2000,month=1,day=1):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Print some data rows.\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read dataframe info\ndef DfInfo(df_initial):\n    # gives some infos on columns types and numer of null values\n    tab_info = pd.DataFrame(df_initial.dtypes).T.rename(index={0: 'column type'})\n    tab_info = tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0: 'null values (nb)'}))\n    tab_info = tab_info.append(pd.DataFrame(df_initial.isnull().sum() / df_initial.shape[0] * 100).T.\n                               rename(index={0: 'null values (%)'}))\n    return tab_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DfInfo(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Price'].resample('MS').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.plot(figsize=(15, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize the data_set \nsc = MinMaxScaler(feature_range = (0, 1))\ndf = sc.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\ntrain_size = int(len(df) * 0.70)\ntest_size = len(df) - train_size\ntrain, test = df[0:train_size, :], df[train_size:len(df), :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert an array of values into a data_set matrix def\ndef create_data_set(_data_set, _look_back=1):\n    data_x, data_y = [], []\n    for i in range(len(_data_set) - _look_back - 1):\n        a = _data_set[i:(i + _look_back), 0]\n        data_x.append(a)\n        data_y.append(_data_set[i + _look_back, 0])\n    return np.array(data_x), np.array(data_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back =90\nX_train,Y_train,X_test,Ytest = [],[],[],[]\nX_train,Y_train=create_data_set(train,look_back)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test,Y_test=create_data_set(test,look_back)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network regressor = Sequential() \nregressor = Sequential()\n\nregressor.add(LSTM(units = 60, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.1))\n\nregressor.add(LSTM(units = 60, return_sequences = True))\nregressor.add(Dropout(0.1))\n\nregressor.add(LSTM(units = 60))\nregressor.add(Dropout(0.1))\n\nregressor.add(Dense(units = 1))\n\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=5)\nhistory =regressor.fit(X_train, Y_train, epochs = 20, batch_size = 15,validation_data=(X_test, Y_test), callbacks=[reduce_lr],shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict = regressor.predict(X_train)\ntest_predict = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrain_predict = sc.inverse_transform(train_predict)\nY_train = sc.inverse_transform([Y_train])\ntest_predict = sc.inverse_transform(test_predict)\nY_test = sc.inverse_transform([Y_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))\nprint('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))\nprint('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\nprint('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))\nplt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare Actual vs. Prediction\naa=[x for x in range(180)]\nplt.figure(figsize=(8,4))\nplt.plot(aa, Y_test[0][:180], marker='.', label=\"actual\")\nplt.plot(aa, test_predict[:,0][:180], 'r', label=\"prediction\")\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Price', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}