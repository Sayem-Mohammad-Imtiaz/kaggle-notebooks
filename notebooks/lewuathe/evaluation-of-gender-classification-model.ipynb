{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"26daeb4f-6086-7c92-806b-56a5950c3564"},"source":"An introduction for using scikit learn pipeline and plot metrics for creating the best model which is tuned hyper parameters. \n\nTopic\n\n* Cross Validation\n* Learning Curve\n* Validation Curve\n* Grid Search"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de546b87-6689-0865-fd50-5363359e3fcf"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d979b18f-26bd-b1a0-7051-bdafb229c3cd"},"outputs":[],"source":"df = pd.read_csv('../input/voice.csv')\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d113c589-5873-8a44-735c-0e578b89b0c1"},"outputs":[],"source":"print(\"Total number of samples: {}\".format(df.shape[0]))\nprint(\"Number of male: {}\".format(df[df.label == 'male'].shape[0]))\nprint(\"Number of female: {}\".format(df[df.label == 'female'].shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0335123-5a34-95a7-30d2-1b8b3af1a488"},"source":"The target label of this classification is in \"label\" column and others are features.\nThese two categories (\"male\", \"female\") are just half and half. It seems no skew in this data set.\nFirst we confirm to there is any missing values in this data sets. This is a common case a dataset in real world has missing values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64c7507c-d332-71e8-9de7-be7d8dd72b35"},"outputs":[],"source":"df.isnull().sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f4f8d7d1-ca5b-f9c8-8b26-27e9ffed3ca9"},"source":"Okay we confirm there is not any missing values in any columns. Before creating learning pipeline it is not a bad idea to visualize overview and the relationship with each features. \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51f4685e-3c45-cc8a-1e5f-ce226331fda7"},"outputs":[],"source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn\ndf.head()\ndf.plot(kind='scatter', x='meanfreq', y='dfrange')\ndf.plot(kind='kde', y='meanfreq')\n#seaborn.pairplot(df['meanfreq', 'sd', 'skew'], hue='label', size=2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2185b150-a0db-5a42-a03d-f9a7651e957b"},"source":"You can also do this easily with seaborn for visializing multiple feature relations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ad204a3-cf33-a9e2-42dd-126797084970"},"outputs":[],"source":"seaborn.pairplot(df[['meanfreq', 'Q25', 'Q75', 'skew', 'centroid', 'label']], \n                 hue='label', size=2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"25e1e66d-c324-6e89-30a7-4c6a4b2952fe"},"source":"These information sometimes can be useful to select features to be used for training model.\nSo now is the time to create training logic. Data set first should be separated with training data and test data for evaluating trained model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"786b776d-6304-4cd9-ef92-c2ab5545c056"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n\n# Encode label category\n# male -> 1\n# female -> 0\n\ngender_encoder = LabelEncoder()\ny = gender_encoder.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ea3395f7-f86c-9a89-b411-d07fc7176c0b"},"source":"Since scikit-learn provide Pipeline API which enables us to create preprocessing and machine learning model at once. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba9394bf-8f8a-1bfe-e124-1b6a38caa821"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\npipe_svc = Pipeline([('std_scl', StandardScaler()), \n                    ('pca', PCA(n_components=10)),\n                    ('svc', SVC(random_state=1))])\n\npipe_svc.fit(X_train, y_train)\n\nprint('Test Accuracy: %.3f' % pipe_svc.score(X_test, y_test))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1aa9cd6-010d-8c95-ac6b-2f62b52160f6"},"source":"At first grance we already gained pretty good accuracy which is over 97%.\nBut we might have a room for improvements. Now we evaluate this model from various type of metrics.\n\n# Cross Validation\n\ncross validation can provide more general metric of this model and it can enables us to reduce variance of the model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c852856a-59e0-860b-b1eb-c37a5580ce83"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator=pipe_svc,\n                        X=X_train,\n                        y=y_train,\n                        cv=10,\n                        n_jobs=1)\n\nprint('Cross validation scores: %s' % scores)\n\nimport matplotlib.pyplot as plt\nplt.title('Cross validation scores')\nplt.scatter(np.arange(len(scores)), scores)\nplt.axhline(y=np.mean(scores), color='g') # Mean value of cross validation scores\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c73c30bb-848f-54ab-72fc-7bb8abd43ba7"},"source":"# Learning Curve\n\nLearning curve enables us decide a model is over fitting to given training data and training under appropriate bias and variance balance. Now we try to plot learning curve of this model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3889e856-0bf1-c821-e53c-7de60e3d6b55"},"outputs":[],"source":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=pipe_svc,\n                                                       X=X_train,\n                                                       y=y_train,\n                                                       train_sizes=np.linspace(0.1, 1.0, 10),\n                                                       cv=10)\n\n# Mean value of accuracy against training data\ntrain_mean = np.mean(train_scores, axis=1)\n\n# Standard deviation of training accuracy per number of training samples\ntrain_std = np.std(train_scores, axis=1)\n\n# Same as above for test data\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot training accuracies \nplt.plot(train_sizes, train_mean, color='red', marker='o', label='Training Accuracy')\n# Plot the variance of training accuracies\nplt.fill_between(train_sizes,\n                train_mean + train_std,\n                train_mean - train_std,\n                alpha=0.15, color='red')\n\n# Plot for test data as training data\nplt.plot(train_sizes, test_mean, color='blue', linestyle='--', marker='s', \n        label='Test Accuracy')\nplt.fill_between(train_sizes,\n                test_mean + test_std,\n                test_mean - test_std,\n                alpha=0.15, color='blue')\n\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e3b94f83-0f4c-782c-9dd3-a78f9f5b5eae"},"source":"Although the variance of score against test data is somewhat higher, we cannot see the trend of over fitting  notably because test scores also improved along with training samples.\n\nIn addition, we can validation score along with a parameter. So next we try to plot validation curve.\n\n# Validation Curve"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1b1780c-03fc-8fad-74bd-362f6269b0d5"},"outputs":[],"source":"from sklearn.model_selection import validation_curve\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(estimator=pipe_svc,\n                                             X=X_train,\n                                             y=y_train,\n                                             param_name='svc__C',\n                                             param_range=param_range,\n                                             cv=10)\n\n# Mean value of accuracy against training data\ntrain_mean = np.mean(train_scores, axis=1)\n\n# Standard deviation of training accuracy per number of training samples\ntrain_std = np.std(train_scores, axis=1)\n\n# Same as above for test data\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot training accuracies \nplt.plot(param_range, train_mean, color='red', marker='o', label='Training Accuracy')\n# Plot the variance of training accuracies\nplt.fill_between(param_range,\n                train_mean + train_std,\n                train_mean - train_std,\n                alpha=0.15, color='red')\n\n# Plot for test data as training data\nplt.plot(param_range, test_mean, color='blue', linestyle='--', marker='s', \n        label='Test Accuracy')\nplt.fill_between(param_range,\n                test_mean + test_std,\n                test_mean - test_std,\n                alpha=0.15, color='blue')\n\nplt.xscale('log')\nplt.xlabel('Regularization parameter C')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bb73feec-e602-5bd8-b5db-82bcb05b40ff"},"source":"So we can estimate the best value of C can be 1 if C is over 1 test accuracy is decreasing.\nIt can cause overfitting of this model."},{"cell_type":"markdown","metadata":{"_cell_guid":"c1de0ad4-e56f-8347-4c3a-60109c9b3327"},"source":"So how can we decide the best hyper parameter of this model without checking learning_curve and validation curve one by one? Grid search can be an option to do this.\n\n# Grid Search\n\nGrid search enables us to search all parameter combination of give hyper parameter space and evaluate models. After grid search you can obtain the best hyper parameter set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"928d2ad1-8a45-e68c-882b-287b131b1de1"},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\nparam_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']},\n              {'svc__C': param_range, 'svc__gamma': param_range, \n               'svc__kernel': ['rbf']}]\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=10)\n\n# Training and searching hyper parameter space and evaluating model\n# by using cross validation logic folded into 10\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ae22f8a-fc9a-3ee2-dc08-6a018ab97487"},"source":"Last but not least, you can check test accuracy with the best model again."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"723bf20b-dcfc-b79d-07e4-5658b4a0e939"},"outputs":[],"source":"best_model = gs.best_estimator_\nbest_model.fit(X_train, y_train)\nprint('Test Accuracy: %.3f' % best_model.score(X_test, y_test))"},{"cell_type":"markdown","metadata":{"_cell_guid":"862bdaac-deae-d9bd-3053-794f9990b9e2"},"source":"# Recap\n\nI introduced how to create hyper parameter tuned model with checking necessary metrics \nsuch as learning curve and validation curve.\n\nAlthough we cannot see any improvement by using grid search here \n(because the default parameter often provides good performance), I hope this is a good material how to use scikit learn pipeline and plot metrics. "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}