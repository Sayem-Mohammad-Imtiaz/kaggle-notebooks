{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  **Boston House Price Prediction by ( Machin Learning & Deep Learning ) Algorithms**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.unsplash.com/photo-1580587771525-78b9dba3b914?ixlib=rb-1.2.1&auto=format&fit=crop&w=600&q=60\" width=\"500\" height=\"300\" />","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Business Priorities**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Real estate economics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Real estate economics is the application of economic techniques to real estate markets. It tries to describe, explain, and predict patterns of prices, supply, and demand. The closely related field of housing economics is narrower in scope, concentrating on residential real estate markets, while the research on real estate trends focuses on the business and structural changes affecting the industry. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Use Case","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Housing Price Prediction Problem** \n\n\nThe dataset describes 13 numerical properties of houses in Boston.\nThose properties are concerned with modeling the price of houses in those suburbs in thousands of dollars. \nAs such, this is a regression predictive modeling problem. Input attributes details are defined below.\n\n**Regression Predictive Modeling Problem**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Data Analysis and Hypothesis Testing**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**DataSet Details :**\n\no\tcapitaCrimeRate - per capita crime rate by town\n\no\tresidenLandZone - proportion of residential land zoned for lots over 25,000 sq.ft.\n\no\tnonRetailBusinessAcres - proportion of non-retail business acres per town.\n\no\ttrackBoundRiver - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n\no\tnitricOxidesConcentra - nitric oxides concentration (parts per 10 million)\n\no\tAvgNumRoom - average number of rooms per dwelling\n\no\tbuildingAge - proportion of owner-occupied units built prior to 1940\n\no\tdistanceEmployCenter - weighted distances to five Boston employment centres\n\no\tradialHighWay - index of accessibility to radial highways\n\no\tpropertyTaxRate - full-value property-tax rate per 10,000\n\no\tteacherRatioTown - pupil-teacher ratio by town\n\no\tblackbyTown - 1000(Bk - 0.63) ^2 where Bk is the proportion of blacks by town\n\no\tworkingPoorNeighT - the percentage of homeowners in the neighborhood considered lower income group\n\no\tmedianValue - Median value of owner-occupied homes in 1000's\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<Img src=\"https://images.unsplash.com/photo-1568092775154-7fa176a29c0f?ixlib=rb-1.2.1&auto=format&fit=crop&w=600&q=60\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Data Set Details : Data Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import arange\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = \"House Price (Boston) Prediction Data\"\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = (\"../input/boston-house-prices/housing.csv\")\nnames = ['capitaCrimeRate', 'residenLandZone', 'nonRetailBusinessAcres', 'trackBoundRiver', 'nitricOxidesConcentra', 'AvgNumRoom', 'buildingAge', 'distanceEmployCenter', 'radialHighWay', 'propertyTaxRate', 'teacherRatioTown', 'blackbyTown', 'workingPoorNeigh', 'medianValue']\ndf = read_csv(filename, delim_whitespace=True, names=names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Quality Assessment","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Data quality assessment (DQA)** is the process of scientifically and statistically evaluating data in order to determine whether they meet the quality required for projects or business processes and are of the right type and quantity to be able to actually support their intended use.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scatter Plot Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A scatter plot matrix is table of scatter plots. Each plot is small so that many plots can be fit on a page. When you need to look at several plots, such as at the beginning of a multiple regression analysis, a scatter plot matrix is a very useful tool.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration (e.g. correlation between columns)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Correlation data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When two sets of data are strongly linked together we say they have a High Correlation. The word Correlation is made of Co- (meaning \"together\"), and Relation. **Correlation is Positive when the values increase together**, and. Correlation is Negative when one value decreases as the other increases.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df, 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Matrix', y=1.05, size=15)\n\ncorrelation_heatmap(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization (e.g. value distribution of columns)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_medianValue = df['medianValue']\ndf_main = df.drop(['capitaCrimeRate','residenLandZone','nonRetailBusinessAcres','nitricOxidesConcentra','distanceEmployCenter','radialHighWay','trackBoundRiver'], axis = 1)\ndf_others = df_main.drop('medianValue', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df_main, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_others.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['medianValue']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Bias Detection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering** is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself. A feature is an attribute or property shared by all of the independent units on which analysis or prediction is to be done. Any attribute could be a feature, as long as it is useful to the model. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Detection bias** refers to systematic differences between groups in how outcomes are determined. Blinding (or masking) of outcome assessors may reduce the risk that knowledge of which intervention was received, rather than the intervention itself, affects outcome measurement.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n# i: index\nfor i, col in enumerate(df_others.columns):\n    # 3 plots here hence 1, 3\n    plt.subplot(1, 13, i+1)\n    x = df[col]\n    y = df_medianValue\n    plt.plot(x, y, 'o')\n    # Create regression line\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('Median Value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"value increase if number of room increase. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots\ndf_main.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=df_main.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nsns.distplot(df_medianValue, hist=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal Probability Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nres = stats.probplot(df_medianValue, plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Type A : Machine Learning Algorithms**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Prepare Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"array = df_main.values\nX = array[:,0:5]\nY = array[:,5]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Root Mean Square Error Method","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 7\nRMS = 'neg_mean_squared_error'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KFold** will provide train/test indices to split data in train and test sets. It will split dataset into k consecutive folds (without shuffling by default). Each fold is then used a validation set once while the k - 1 remaining folds form the training set ****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**k-Fold Cross-Validation:** Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selection and justification of Model Performance Indicator (e.g. F1 score)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A good F1 score means that you have low false positives and low false negatives, so you're correctly identifying real threats and you are not disturbed by false alarms. An F1 score is considered perfect when it's 1 , while the model is a total failure when it's 0 .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n\nThe F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall). The F1 score is also known as the Sørensen–Dice coefficient or Dice similarity coefficient (DSC).\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Algorithm Comparison**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model performance between different feature engineerings and models compared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n\tkfold = KFold(n_splits=num_folds, random_state=seed)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scaled Algorithm Comparison**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **KNN Algorithm tuning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\"The k-nearest neighbors algorithm (KNN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression”-Wikipedia","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\ndf_others.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Optimization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss.Cross-validation is often used to estimate this generalization performance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Ensembles :** ensembles combine predictions from different models to generate a final prediction, and the more models we include the better it performs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembles = []\nensembles.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n\tkfold = KFold(n_splits=num_folds, random_state=seed)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=RMS)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\ndf_others.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pyplot.figure()\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**StandardScaler :** StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler makes the mean of the distribution 0.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It transforms the data in such a manner that it has mean as 0 and standard deviation as 1. In short, it standardizes the data. Standardization is useful for data which has negative values. It arranges the data in a standard normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient boosting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Gradient boosting** is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=400)\nmodel.fit(rescaledX, Y_train)\n\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Squared Error=1N∑i=1N(yi−yi^)2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Deep Learning Algorithms**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pip install scikit-learn=='.13'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neural network models** Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Sub Type I : Baseline Neural Network Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into input (X) and output (Y) variables\narray = df.values\nX = array[:,0:13]\nY = array[:,13]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KerasRegressor**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Keras deep learning library for modeling regression problems**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n\tmodel.add(Dense(1, kernel_initializer='normal'))\n\t# Compile model\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\treturn model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate model\nestimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sub Type II : Scikit-learn Pipeline Framework**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n\tmodel.add(Dense(1, kernel_initializer='normal'))\n\t# Compile model\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\treturn model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate model with standardized dataset\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sub Type III : Wider Network Topology**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def wider_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n\tmodel.add(Dense(1, kernel_initializer='normal'))\n\t# Compile model\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\treturn model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sub Type IV : Deeper Network Topology**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to improve classification performance without increasing network depth, a new Deep Topology Network (DTN) framework is proposed. The key idea of DTN is based on the iteration of multiple learning rate feedback. The framework consists of multiple sub-networks and each sub-network has its own learning rate. After the determined iteration period, these learning rates can be adjusted according to the feedback of training accuracy, in the feature learning process, the optimal learning rate is updated iteratively to optimize the loss function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def larger_model():\n\t# create model\n\tmodel = Sequential()\n\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n\tmodel.add(Dense(1, kernel_initializer='normal'))\n\t# Compile model\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\treturn model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}