{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport sys\nimport time\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\nimport textwrap\nimport re\nimport attr\nimport abc\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom IPython.display import HTML\nfrom os import listdir\nfrom os.path import isfile, join","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')\nMAX_ARTICLES = 1000\nbase_dir = '/kaggle/input'\ndata_path = base_dir + '/covid19csv/covid19.csv'\nmodel_path = base_dir + '/biobert-qa/biobert_squad2_cased'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Document Retrieval based on Query**","metadata":{}},{"cell_type":"code","source":"class Retrieval(abc.ABC):\n    \"\"\"Base class for retrieval methods.\"\"\"\n\n    def __init__(self, docs, keys=None):\n        self._docs = docs.copy()\n        if keys is not None:\n            self._docs.index = keys\n        self._model = None\n        self._doc_vecs = None\n\n    def _top_documents(self, q_vec, top_n=10):\n        similarity = cosine_similarity(self._doc_vecs, q_vec)\n        rankings = np.argsort(np.squeeze(similarity))[::-1]\n        ranked_indices = self._docs.index[rankings]\n        return self._docs[ranked_indices][:top_n]\n\n    @abc.abstractmethod\n    def retrieve(self, query, top_n=10):\n        pass\n    \nclass TFIDFRetrieval(Retrieval):\n    \"\"\"Retrieve documents based on cosine similarity of TF-IDF vectors with query.\"\"\"\n    def __init__(self, docs, keys=None):\n        \"\"\"\n        Args:\n          docs: a list or pd.Series of strings. The text to retrieve.\n          keys: a list or pd.Series. Keys (e.g. ID, title) associated with each document.\n        \"\"\"\n        super(TFIDFRetrieval, self).__init__(docs, keys)\n        self._model = TfidfVectorizer()\n        self._doc_vecs = self._model.fit_transform(docs)\n        \n    def retrieve(self, query, top_n=10):\n        q_vec = self._model.transform([query])\n        return self._top_documents(q_vec, top_n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ResearchQA(object):\n    def __init__(self, data_path, model_path):\n        print('Loading data from', data_path)\n        self.df = pd.read_csv(data_path)\n        print('Initializing model from', model_path)\n        self.model = TFAutoModelForQuestionAnswering.from_pretrained(model_path, from_pt=True)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.retrievers = {}\n        self.build_retrievers()\n        self.main_question_dict = dict()\n        \n    \n    def build_retrievers(self):\n        df = self.df\n        abstracts = df[df.abstract.notna()].abstract\n        self.retrievers['abstract'] = TFIDFRetrieval(abstracts)\n\n    def retrieve_candidates(self, section_path, question, top_n):\n        candidates = self.retrievers[section_path[0]].retrieve(question, top_n)\n        return self.df.loc[candidates.index]\n    \n        \n    def get_answers(self, question, section='abstract', keyword=None, max_articles=1000, batch_size=12):\n        df = self.df\n        answers = []\n        section_path = section.split('/')\n\n        if keyword:\n            candidates = df[df[section_path[0]].str.contains(keyword, na=False, case=False)]\n        else:\n            candidates = self.retrieve_candidates(section_path, question, top_n=max_articles)\n        if max_articles:\n            candidates = candidates.head(max_articles)\n\n        text_list = []\n        indices = []\n        for idx, row in candidates.iterrows():\n            text = row[section]\n            if text and isinstance(text, str):\n                text_list.append(text)\n                indices.append(idx)\n\n        num_batches = len(text_list) // batch_size\n        all_answers = []\n        for i in range(num_batches):\n            batch = text_list[i * batch_size:(i+1) * batch_size]\n            answers = self.get_answers_from_text_list(question, batch)\n            all_answers.extend(answers)\n\n        last_batch = text_list[batch_size * num_batches:]\n        if last_batch:\n            all_answers.extend(self.get_answers_from_text_list(question, last_batch))\n\n        columns = ['doi', 'authors', 'journal', 'publish_time', 'title']\n        processed_answers = []\n        for i, a in enumerate(all_answers):\n            if a:\n                row = candidates.reindex([indices[i]])\n#                 print(row)\n                new_row = [a.text, a.start_score, a.end_score, a.input_text]\n                new_row.extend(row[columns].values[0])\n#                 print(row[columns].values[0], len(new_row))\n                processed_answers.append(new_row)\n        answer_df = pd.DataFrame(processed_answers, columns=(['answer', 'start_score',\n                                                 'end_score', 'context'] + columns))\n        return answer_df.sort_values(['start_score', 'end_score'], ascending=False)\n\n    def get_answers_from_text_list(self, question, text_list, max_tokens=512):\n        tokenizer = self.tokenizer\n        model = self.model\n        inputs = tokenizer.batch_encode_plus(\n          [(question, text) for text in text_list], add_special_tokens=True, return_tensors='tf',\n          max_length=max_tokens, truncation_strategy='only_second', pad_to_max_length=True)\n        input_ids = inputs['input_ids'].numpy()\n#         answer_start_scores, answer_end_scores = model(inputs)\n        output = model(inputs)\n        answer_start = tf.argmax(\n          output.start_logits, axis=1\n      ).numpy()  # Get the most likely beginning of each answer with the argmax of the score\n        answer_end = (\n          tf.argmax(output.end_logits, axis=1) + 1\n      ).numpy()  # Get the most likely end of each answer with the argmax of the score\n\n        answers = []\n        for i, text in enumerate(text_list):\n            input_text = tokenizer.decode(input_ids[i, :], clean_up_tokenization_spaces=True)\n            input_text = input_text.split('[SEP] ', 2)[1]\n            answer = tokenizer.decode(\n                input_ids[i, answer_start[i]:answer_end[i]], clean_up_tokenization_spaces=True)\n            score_start = output.start_logits.numpy()[i][answer_start[i]]\n            score_end = output.end_logits.numpy()[i][answer_end[i]-1]\n            if answer and not '[CLS]' in answer:\n                answers.append(Answer(answer, score_start, score_end, input_text))\n            else:\n                answers.append(None)\n        return answers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@attr.s\nclass Answer(object):\n    text = attr.ib()\n    start_score = attr.ib()\n    end_score = attr.ib()\n    input_text = attr.ib()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa = ResearchQA(data_path, model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = input('Ask your question please!')\n# Ex: How effective are early treatments\nanswers = qa.get_answers(question , max_articles=10)\nprint(answers['answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers['answer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}