{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\nimport spacy\nimport gensim\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport nltk; nltk.download('stopwords')\nimport seaborn as sns\nimport operator\nimport pickle\nfrom nltk.corpus import stopwords\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocess Data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sys.path.insert(0, \"../\")\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\njson_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            doc_id = data['paper_id']\n            row['doc_id'] = doc_id\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [abst['text'] for abst in data['abstract']]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n    \ncorona_df = return_corona_df(json_filenames, corona_df, 'b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop words\nstop_words = stopwords.words(\"english\")\nstop_words.extend(['et', 'al'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentence):\n    \"\"\"\n    divides sentence into words and removes punctuations\n    \"\"\"\n    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n\ndef remove_stopwords(doc):\n    \"\"\"\n    removes stopwords\n    \"\"\"\n    return [word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words]\n\ndef get_bigrams(text, bigram_model):\n    \"\"\"\n    get bigrams\n    \"\"\"\n    return bigram_model[text]\n\ndef preprocess_text(text, bigram_model):\n    data_words = list(sent_to_words(text))\n    data_words = remove_stopwords(data_words)\n    data_words = get_bigrams(data_words, bigram_model)\n    return data_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model = gensim.models.phrases.Phrases(corona_df['text_body'].to_string(), min_count=1, threshold=2)\n# clean text\ncorona_df['preprocessed_text'] = corona_df['text_body'].apply(preprocess_text, args=(bigram_model,))\ncorona_df.to_csv('corona_preprocessed.csv', index = False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load preprocessed csv\n#corona_df = pd.read_csv('corona_preprocessed.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word2Vec on Corona Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train Word2Vec model\nmodel = gensim.models.Word2Vec(corona_df['preprocessed_text'], size=100, window=5, \n                 min_count=1, workers=4)\nmodel.save(\"corona_word2vec.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = gensim.models.Word2Vec.load(\"corona_word2vec.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visulaize Word Embeddings\nfrom sklearn.manifold import TSNE\n\nkeys = ['coronavirus', 'incubation', 'asymptomatic', 'transmission', 'materials',\n       'infection', 'diagnostics', 'model', 'morbidities']\n\nembedding_clusters = []\nword_clusters = []\nfor word in keys:\n    embeddings = []\n    words = []\n    for similar_word, _ in model.most_similar(word, topn=10):\n        words.append(similar_word)\n        embeddings.append(model[similar_word])\n    embedding_clusters.append(embeddings)\n    word_clusters.append(words)\n\n\n\nembedding_clusters = np.array(embedding_clusters)\nn, m, k = embedding_clusters.shape\ntsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\nembeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n\n\ndef tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n    plt.figure(figsize=(20, 20))\n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n        plt.scatter(x, y, c=color, alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n                         textcoords='offset points', ha='right', va='bottom', size=10)\n    plt.legend(loc=4)\n    plt.title(title)\n    plt.grid(True)\n    if filename:\n        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ntsne_plot_similar_words('Similar words', keys, embeddings_en_2d, word_clusters, 0.7,\n                        'similar_words.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LDA - Topic Modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create index to word dictionary\nid2word = gensim.corpora.Dictionary(corona_df['preprocessed_text'])\n\n# create indexed text\ncorpus = [id2word.doc2bow(text) for text in corona_df['preprocessed_text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_topics = 10\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=555)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df['lda_label'] =[max(lda_model[c], key=operator.itemgetter(1))[0] for c in corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(lda_model, open('lda_model.pk', 'wb'))\nlda_model = pickle.load(open('lda_model.pk', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_topics(model, topic_range, topn):\n    word_dict = {};\n    for i in topic_range:\n        words = model.show_topic(i, topn);\n        word_dict['Topic # ' + '{:02d}'.format(i)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict);\n\nn_words = 10\nget_topics(lda_model, range(0,n_topics), n_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# human topic annotation\nlda_dictionary = {0: 'experiments', 1: 'cells', 2: 'prep_latin', 3 : 'protein', 4: 'model', \n                  5: 'treatment', 6:'genetics', 7:'infection', 8:'risks', 9:'symptoms'}\n\ndef transform(num):\n    return lda_dictionary[num]\n\ncorona_df['lda_label_string'] = corona_df['lda_label'].apply(transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"lda_label_string\",data=corona_df)\nax.set_title('Number of text body per topic');\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}