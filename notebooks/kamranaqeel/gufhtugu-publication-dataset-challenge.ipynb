{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Challenges**\n1. Find top 10 books.\n2. Find co-relation between different columns.\n3. Probability of an order return based on given variables.\n4. Clean list of cities.\n5. Prediction on number of orders for given data/day for the year 2021\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing important Libraries. Numpy and pandas already imported in above code cell.\nimport matplotlib.pyplot as plt\nimport fuzzywuzzy\nfrom fuzzywuzzy import process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading dataset\ndf = pd.read_csv('/kaggle/input/gufhtugu-publications-dataset-challenge/GP Orders - 5.csv',parse_dates=['Order Date & Time'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#statistics about numerical columns of the dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find unique values in each column\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for null values in each column\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Book Name column contains only two null values so we replace it with the best selling book.\nSimilarly city has null values and a better pick will be to change it to the most occuring city. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#find best selling book and city.\nbest_seller = df['Book Name'].value_counts().nlargest(1,keep='all')\ntop_city = df['City'].value_counts().nlargest(1,keep='all')\nprint(\"Best selling book is \" + str(best_seller))\nprint(\"City with high number of purchases is \" + str(top_city))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill the null spaces with respective values\ndf['Book Name'] = df['Book Name'].fillna('انٹرنیٹ سے پیسہ کمائیں')\ndf['City'] = df['City'].fillna('karachi')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use most frequent value in Payment Method column to replace the null values\n#with that frequently occuring values.\ndf['Payment Method'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is spelling difference between \"Cash on Delivery\" and \"Cash on Delivert (COD)\" even though they are the same. To address this issue, we make both the spellings same."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make both spellings same.\ndf[\"Payment Method\"].replace({\"Cash on delivery\": \"Cash on delivery\", \"Cash on Delivery (COD)\": \"Cash on delivery\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check to see the changes.\ndf['Payment Method'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find frequently used Payment Method.\ndf['Payment Method'].value_counts().nlargest(1, keep='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace the null values in null values with \"Cash on Delivery\" as it is the most frequently occuring\n#value. \ndf['Payment Method'] = df['Payment Method'].isnull().fillna(\"Cash on delivery\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if any null value exists in the whole data.\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before doing any analysis over the data, it is wise to clean the data first. See, there are cities with names written in different formats for example Karachi, karachi, KARACHI, KCH, etc. Python is going to consider each one of these a different city name. So the first thing we should be doing is to clean the city column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some Column names are long. Replacing it with short ones.\ndf.rename(columns={\"Total weight (grams)\": \"Weight(grams)\" }, inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#City and Book Name columns are important columns so we do some cleaning on these columns.\ndf['City'] = df['City'].str.lower() #makes the letters lower case.\ndf['City'] = df['City'].str.replace('\\d+', '') #remove one or more digits.\ndf['City'] = df['City'].str.replace('pakistan', '') #remove the word pakistan from city names within a column\ndf['City'] = df['City'].str.replace('city', '') #remove the word city from city names\ndf['City'] = df['City'].str.replace('?', '') #remove ? sign from city \ndf['City'] = df['City'].str.strip() #remove extra spaces before and after the strings.\n#preprocess Book_Name\ndf['Book Name'] = df['Book Name'].str.replace(\"- مستحقین زکواة\", \"\")\ndf['Book Name'] = df['Book Name'].str.lower()\ndf['Book Name'] = df['Book Name'].str.replace(\"linux - an introduction  (release data - october 3, 2020)\", \"linux - an introduction\")\ndf['Book Name'] = df['Book Name'].str.replace(\"python programming- release date: august 14, 2020\", \"python programming\")\ndf['Book Name'] = df['Book Name'].str.replace(\"ڈیٹا سائنس ۔ ایک تعارف\", \"ڈیٹا سائنس\")\ndf['Book Name'] = df['Book Name'].str.replace(\"molo masali - مولو مصلی\", \"molo masali\")\ndf['Book Name'] = df['Book Name'].str.replace(\"مشین ل\", \"مشین لرننگ\")\ndf['Book Name'] = df['Book Name'].str.replace(\"مشین لرننگرننگ\", \"مشین لرننگ\")\ndf['Book Name'] = df['Book Name'].str.replace(\"r ka taaruf آر کا تعارف\", \"r ka taaruf\")\ndf['Book Name'] = df['Book Name'].str.strip()\ndf.sample(40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check how many city names are there.\ndf.City.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cities number seems too high. This is obviously due to the fact that some cities names spellings are different. For example the spellings for faisalabad are \"Faisalabad\", \"faisalabad\", \"FSD\", \"faisalabad city\" etc. Python treats them as different names that's why cities number is high."},{"metadata":{},"cell_type":"markdown","source":"We try to reduce the number of cities by choping cities names to single, original name."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import cities dataset\ndf_cities = pd.read_csv('../input/pakistan-cities-and-postal-codes/Pakistan Cities and Zip Codes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cities.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take only cities names column\ncities_list = df_cities['Area_Name'].str.lower().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cities_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is another cities list taken from github\npak_cities = ['islamabad', 'ahmed nager chatha', 'ahmadpur east', 'ali khan abad', 'alipur',\n              'arifwala', 'attock', 'bhera', 'bhalwal', 'bahawalnagar', 'bahawalpur', 'bhakkar',\n              'burewala', 'chillianwala', 'chakwal', 'chichawatni', 'chiniot', 'chishtian', \n              'daska', 'darya khan', 'dera ghazi khan', 'dhaular', 'dina', 'dinga', 'dipalpur',\n              'faisalabad', 'ferozewala', 'fateh jhang', 'ghakhar mandi', 'gojra', 'gujranwala',\n              'gujrat', 'gujar khan', 'hafizabad', 'haroonabad', 'hasilpur', 'haveli lakha',\n              'jatoi', 'jalalpur', 'jattan', 'jampur', 'jaranwala', 'jhang', 'jhelum', 'kalabagh',\n              'karor lal esan', 'kasur', 'kamalia', 'kamoke', 'khanewal', 'khanpur', 'kharian',\n              'khushab', 'kot addu', 'jauharabad', 'lahore', 'lalamusa', 'layyah', 'liaquat pur',\n              'lodhran', 'malakwal', 'mamoori', 'mailsi', 'mandi bahauddin', 'mian channu',\n              'mianwali', 'multan', 'murree', 'muridke', 'mianwali bangla', 'muzaffargarh',\n              'narowal', 'nankana sahib', 'okara', 'renala khurd', 'pakpattan', 'pattoki',\n              'pir mahal', 'qaimpur', 'qila didar singh', 'rabwah', 'raiwind', 'rajanpur', \n              'rahim yar khan', 'rawalpindi', 'sadiqabad', 'safdarabad', 'sahiwal', 'sangla hill',\n              'sarai alamgir', 'sargodha', 'shakargarh', 'sheikhupura', 'sialkot', 'sohawa',\n              'soianwala', 'siranwali', 'talagang', 'taxila', 'toba tek singh', 'vehari',\n              'wah cantonment', 'wazirabad', 'badin', 'bhirkan', 'rajo khanani', 'chak', 'dadu',\n              'digri', 'diplo', 'dokri', 'ghotki', 'haala', 'hyderabad', 'islamkot', 'jacobabad',\n              'jamshoro', 'jungshahi', 'kandhkot', 'kandiaro', 'karachi', 'kashmore', \n              'keti bandar', 'khairpur', 'kotri', 'larkana', 'matiari', 'mehar', 'mirpur khas',\n              'mithani', 'mithi', 'mehrabpur', 'moro', 'nagarparkar', 'naudero', 'naushahro feroze', 'naushara',\n              'nawabshah', 'nazimabad', 'qambar', 'qasimabad', 'ranipur', 'ratodero', 'rohri', 'sakrand',\n              'sanghar', 'shahbandar', 'shahdadkot', 'shahdadpur', 'shahpur chakar', 'shikarpaur', 'sukkur',\n              'tangwani', 'tando adam khan', 'tando allahyar', 'tando muhammad khan', 'thatta', 'umerkot',\n              'warah', 'abbottabad', 'adezai', 'alpuri', 'akora khattak', 'ayubia', 'banda daud shah', 'bannu', \n              'batkhela', 'battagram', 'birote', 'chakdara', 'charsadda', 'chitral', 'daggar', 'dargai',\n              'darya khan', 'dera ismail khan', 'doaba', 'dir', 'drosh', 'hangu',\n              'haripur', 'karak', 'kohat', 'kulachi', 'lakki marwat', 'latamber', 'madyan', 'mansehra', 'mardan',\n              'mastuj', 'mingora', 'nowshera', 'paharpur', 'pabbi', 'peshawar', 'saidu sharif',\n              'shorkot', 'shewa adda', 'swabi', 'swat', 'tangi', 'tank', 'thall', 'timergara', \n              'tordher', 'awaran', 'barkhan', 'chagai', 'dera bugti', 'gwadar', 'harnai', \n              'jafarabad', 'jhal magsi', 'kacchi', 'kalat', 'kech', 'kharan', 'khuzdar', \n              'killa abdullah', 'killa saifullah', 'kohlu', 'lasbela', 'lehri', 'loralai', 'mastung',\n              'musakhel', 'nasirabad', 'nushki', 'panjgur', 'pishin valley', 'quetta', 'sherani',\n              'sibi', 'sohbatpur', 'washuk', 'zhob', 'ziarat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to chop cities names\ndef get_nearest_city(city):\n  for check_city in cities_list:\n    if check_city in str(city):\n      return check_city\n  return city","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total unique cities in our dataset before normalization: {df.City.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['city'] = df['City'].apply(get_nearest_city)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total unique cities in our dataset after preprocessing: {df.city.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#writing the function again to check with another city list\ndef get_nearest_city(city):\n  for check_city in pak_cities:\n    if check_city in str(city):\n      return check_city\n  return city","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['city'] = df['City'].apply(get_nearest_city)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total unique cities in our dataset after preprocessing: {df.city.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List named pak_cities gets us 1854 unique cities which is better than its counterpart list named cities_list. So we keep pak_cites output."},{"metadata":{},"cell_type":"markdown","source":"Now lets use fuzzy wuzzy to replace incorrect spellings with correct ones. This will bar python from treating same cities as different due to spelling mistakes.\nFor Example Charsadda, chrsadda, charsada should be one name."},{"metadata":{},"cell_type":"markdown","source":"**Fuzzy matching:** The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time."},{"metadata":{},"cell_type":"markdown","source":"Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of districts that have the closest distance to \"charsadda\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"city_unique = df['city'].unique()\ncity_unique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matches = fuzzywuzzy.process.extract(\"charsadda\", city_unique, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n# take a look at them\nmatches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we replace the top 4 strings with charsadda to make them a single city name. Which actually is a single city name."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['city'].replace(['charssadda', 'charsadaa', 'chārsadda', 'charssadsa'] , 'charsadda')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doing this one-by-one is a tedius process. Lets automate it a bit to change a few more cities."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 81):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_matches_in_column(df=df, column='city', string_to_match=\"charsadda\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_matches_in_column(df=df, column='city', string_to_match=\"karachi\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"rawalpindi\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"peshawar\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"shikarpur\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"kohat\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"faisalabad\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"islamabad\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"sialkot\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"quetta\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"lahore\")\nreplace_matches_in_column(df=df, column='city', string_to_match=\"okara\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'total unique cities in our dataset after preprocessing: {df.city.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cities number reduced from 1854 to 1780"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.assign(Books_names_ordered=df['Book Name'].str.split(\"/\")).explode(\"Book Name\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top selling books\ntop_seller_books = df.Books_names_ordered.explode().value_counts().reset_index()\ntop_seller_books.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the top selling books\nfig = plt.figure(figsize=(12,6))\ntop_seller_books = df.Books_names_ordered.explode().value_counts()[:10].plot.barh(\n    color='green', rot=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cities with high number of purchased books\nfig = plt.figure(figsize=(12,6))\ntop_cities = df.city.value_counts()[:10].plot.barh(\n    color='red', rot=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Order Status']=df['Order Status'].astype('category').cat.codes\ndf['City']=df['City'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"City\"].corr(df['Order Status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Payment Method']=df['Payment Method'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Order Status'].corr(df['Payment Method'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"City\"].corr(df['Payment Method'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Association Rule Mining\n**In this section, we attempt to implement association rule mining by using Apriori Algorithm.\nThis algorithm tries to find association rules between different items which are also termed as market basket analysis. Simply put, this algorithm suggests if a certain customer buys a certain product, what next product he might potentially be buying.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install apyori #install apyori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from apyori import apriori #import apriori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Books name cell is filled with many books.Each book is seperated by \"/\" character\n#we split the book name by this character to and expand the cell via books.\ndf_books=df['Book Name'].str.split(\"/\", n = 300, expand = True) #we make expansion limited to 300 \n                                                                #to avoid burdon of plethora of rules mining\ndf_books","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_books =df_books.fillna(0)\ndf_books","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apriori algorithm take input as lists so we convert the dataframe to list.\n\nrecords = []      #lists within list\nfor i in range(0, 19239):\n    records.append([str(df_books.values[i,j]) for j in range(0, 194)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"association_rules =  apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\nassociation_results = list(association_rules)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total rules mined ' + str(len(association_results)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(association_results[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the apriori algortihm has returned the output in RelationRecord type. we convert it to list to print it.\nlist_rules = [list(association_results[i][0]) for i in range(0,len(association_results))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_rules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(association_results[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Three tasks done sor far:\n1. Top ten books found.\n2. City column cleaned. Althogh there still is space for improvement.\n3. Correlation has been found between different columns which might come handy when doing prediction."},{"metadata":{},"cell_type":"markdown","source":"Two tasks to go:\n1. Probability of an order return based on given variables.\n2. Prediction on number of books to be sold on a specific date."},{"metadata":{},"cell_type":"markdown","source":"**Feedback and correction is highly appreciated**\n\n**Upvote only if you have found it useful**  \n\n**Happy coding**"},{"metadata":{},"cell_type":"markdown","source":"# In progress"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}