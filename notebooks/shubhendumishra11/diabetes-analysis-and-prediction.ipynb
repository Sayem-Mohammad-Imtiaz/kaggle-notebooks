{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What we will see in this notebook:\n1. analysis on the dataset to understand each independent variable and its relationship with the target variable\n2. necessary preprocessing of data\n3. Application of  algorithms and select the best among them\n","metadata":{}},{"cell_type":"markdown","source":"# load libraries and read data","metadata":{}},{"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check for null values**","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Univariate Analysis of Data","metadata":{}},{"cell_type":"code","source":"sns.countplot(data['Pregnancies'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data['Glucose'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Most Observations in Glucose column lies within the limits of 100-150**","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['BloodPressure'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Systolic Blood Pressure is also Normal**","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['SkinThickness'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data['Insulin'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insulin column seems to be skewed. We need to check it further..**","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['BMI'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data['DiabetesPedigreeFunction'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This column also seems to be skewed**","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['Age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data['Outcome'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we see imbalance in the Target variable and we need to treat it before applying any algorithm**","metadata":{}},{"cell_type":"markdown","source":"# Relationship between variables","metadata":{}},{"cell_type":"code","source":"sns.swarmplot(x=\"Outcome\", y=\"Pregnancies\", data=data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.swarmplot(x=\"Outcome\", y=\"Age\", data=data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='Insulin',y='Glucose', hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If normal levels of Insulin are maintained in blood then glucose level is also low**","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='DiabetesPedigreeFunction',y='Glucose', hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Presence of Family history of diabetes can lead to rise inglucose levels but not necessarily. This can be seen in the above plot**","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='Pregnancies',y='Glucose', hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='BloodPressure',y='Glucose',hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Long term High systolic blood pressure can increase glucose level**","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='SkinThickness',y='BMI',hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='SkinThickness',y='Insulin',hue = 'Outcome',data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**skin thickness is more if insulin content is more**","metadata":{}},{"cell_type":"markdown","source":"# What does the relationships tell us?\n1. High levels of Insulin in blood post 2 hrs indiacte that glucose levels are high and can lead to diabetes\n2. Presence of family history does not necessarily lead to diabetes but person can be at risk\n3. High Systolic blood pressure in long term can also lead to increase in glucose level as we can observe it from graph\n4. Skin thickness can increase BMI and in turn becomes tolerant to insulin action due to which insulin effect on glucose level can decrease. This in long term can lead to diabetes and therefore it is said that BMI should be maintained.\n5. Insulin levels are increased in cases where skin thickness is more****","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(data.corr(method='pearson'), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building models","metadata":{}},{"cell_type":"markdown","source":"**Splitting into features and labels**","metadata":{}},{"cell_type":"code","source":"y=data['Outcome']\nX=data.drop(columns=['Outcome'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['Age', 'Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction']\nX=data[features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting of data into train and test**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression (Baseline Model)**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = log_reg.predict(X_test)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, y_pred)\nprint(classification_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**so we get an accuracy of around 76% with precision of positive case as 72%. let's see if we can improve it**","metadata":{}},{"cell_type":"markdown","source":"**Since target variable is imbalanced, Applying smote for its treatment**","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nx_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nx_test_smote, y_test_smote = smote.fit_resample(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = x_train_smote\nX_test = x_test_smote\ny_train = y_train_smote\ny_test = y_test_smote","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Again predicting results after treatment of imbalance**","metadata":{}},{"cell_type":"code","source":"pred1=log_reg.predict(X_test)\npred1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, pred1)\nprint(classification_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Though accuracy from the baseline model is little less but precision has improved very much. In this type of dataset where we need to be more precise about the early detection of positive case, accuracy becomes less important. Thus we need to understand what our model building purpose is.**","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, pred1)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn = cm[0,0]\nfp = cm[0,1]\ntp = cm[1,1]\nfn = cm[1,0]\naccuracy  = (tp + tn) / (tp + fp + tn + fn)\nprecision = tp / (tp + fp)\nrecall    = tp / (tp + fn)\nf1score  = 2 * precision * recall / (precision + recall)\nprint(f1score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC curve**","metadata":{}},{"cell_type":"code","source":"predicted_probab = log_reg.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Logistic Regression')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost classifier**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred1 = model.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, y_pred1)\nprint(classification_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_test, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This model is not performing well as compared to logistic regression**","metadata":{}},{"cell_type":"code","source":"predicted_probab = model.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='XGB')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0, criterion='entropy')\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, dt_pred)\nprint(classification_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(y_test, dt_pred)\nauc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This model also does not perform well as compared to logistic regression**","metadata":{}},{"cell_type":"code","source":"\npredicted_probab = dt_clf.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Decision Tree')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_predict = rfc.predict(X_test)\nroc_auc_score(y_test, rfc_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nclassification_report = classification_report(y_test, rfc_predict)\nprint(classification_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RF\npredicted_probab = rfc.predict_proba(X_test)\npredicted_probab = predicted_probab[:, 1]\nfpr, tpr, _ = roc_curve(y_test, predicted_probab)\nfrom matplotlib import pyplot\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\npyplot.plot(fpr, tpr, marker='.', color='red', label='Random Forest')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**From the above application of models we see that Random forest performs the best as its f1 score is the best among all the models.\n**For this type of dataset, we need our model to perform well to identify positive case and thus accuracy becomes secondary metric of secondary importance**","metadata":{}},{"cell_type":"markdown","source":"**The performance of models can further be improved by Hyperparameter Tuning and we might get other model which can perform better than Logistic Regression**\n\n**Work in progress....**","metadata":{}},{"cell_type":"markdown","source":"**If you find the notebook useful then provide feedback and do suggest for any sort of improvements which can be made. I am a beginner and would like to get any suggestions for further enhancement of my knowledge**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}