{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">  Bank Marketing Data </h1>\n\n#### [UCI: BANK MARKETING DATASET](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) \nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y)."},{"metadata":{},"cell_type":"markdown","source":"# Contents:\n1. [Attribute Information](#attribute)\n2. [Data Cleaning and Preprocessing](#cleaning)\n    -  [Standardization](#scaling)\n    -  [Train/Test Split](#split)\n3. [Approaches](#approach)\n    - [Approach 1: Baseline Model](#approach1)\n    - [Approach 2: Oversampling- RandomOverSampler](#approach2)\n    - [Approach 3: Undersampling - RandomUnderSampler](#approach3)\n    - [Approach 4: Oversampling- SMOTE](#approach4)\n4. [Feature Importance and Final Model Selection](#keyfeatures)\n    - [Feature Importance in RandomOverSampler](#keyfeatures1)\n    - [Feature Importance in RandomOverSampler](#keyfeatures2)"},{"metadata":{},"cell_type":"markdown","source":"#### Attribute Information:\n<a id=\"attribute\"></a>\n- Age\n- Job - type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n- marital - marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n- Education - Shows the level of education of each customer (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n- Default - Whether a customer has credit in default (categorical: 'no','yes','unknown')\n- Housing - Does the customer have a housing loan? (categorical: 'no','yes','unknown')\n- Loan - Does the customer have a personal loan? (categorical: 'no','yes','unknown')\n- Contact - The contact communication type (categorical: 'cellular','telephone')\n- Month - Last contact month of year\n- day_of_week - Last contact day of Week\n- Duration - Last contact duration in seconds. Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no').\n- Campaign - Number of contact performed for the client during the campaign\n- pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n- previous: number of contacts performed before this campaign and for this client\n- poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success') \n- emp.var.rate: employment variation rate - quarterly indicator\n- cons.price.idx: consumer price index - monthly indicator\n- cons.conf.idx: consumer confidence index - monthly indicator\n- euribor3m: euribor 3 month rate - daily indicator\n- nr.employed: number of employees - quarterly indicator\n- y - has the client subscribed a term deposit? (binary: 'yes','no')"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, recall_score\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV, cross_val_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC \nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom collections import Counter\nfrom pprint import pprint\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/bank-marketing-campaigns-dataset/bank-additional-full.csv\", sep=';')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning and Preprocessing\n<a id=\"cleaning\"></a>"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are 41188 observations with 21 features.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"# check for missing values in any column\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are no missing values in the dataset.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.hist(bins = 15, figsize = (10,10), xlabelsize = 0.1, ylabelsize = 0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.pdays.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Values of pdays column shows very little variation. Most of the values consist of 999 which means client was not previously contacted. It does not give us much information. Therefore, it is better to drop.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.catplot(x='default',hue='y',kind='count',data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(dataset['default'], dataset.y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Dropping default column is better because all values of default are no or unknown. It does not give much information.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('y', data=dataset, palette=colors)\nplt.title('Deposit Distributions \\n (0: No || 1: Yes)', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From the above distribution we can be sure that the data is imbalanced, as the number of \"no\"s are also 8 times the number of \"yes\".*"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(dataset.corr(),square=True,annot=True,cmap= 'twilight_shifted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*emp.var.rate, nr.employed and euribor3m are highly correlated. Since multicollinearity is not a problem for all algorithm, we decide to keep them.*"},{"metadata":{},"cell_type":"markdown","source":"## Standardization\n<a id=\"scaling\"></a>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# make a copy of dataset to scaling\nbank_scale=dataset.copy()\n\n# remove 'pdays' and 'default' columns\nbank_scale= bank_scale.drop(['pdays', 'default'], axis=1)\n\nbank_scale.y.replace(('yes', 'no'), (1, 0), inplace=True)\n\n# standardization for just numerical variables \ncategorical_cols= ['job','marital', 'education',  'housing', 'loan', 'contact', 'month', 'day_of_week','poutcome','y']\nfeature_scale=[feature for feature in bank_scale.columns if feature not in categorical_cols]\n\nscaler=StandardScaler()\nscaler.fit(bank_scale[feature_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scaled_data = pd.concat([bank_scale[categorical_cols].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(bank_scale[feature_scale]), columns=feature_scale)],\n                    axis=1)\n\ncategorical_cols1= ['job','marital', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week','poutcome']\nscaled_data= pd.get_dummies(scaled_data, columns = categorical_cols1, drop_first=True)\nscaled_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test Split\n<a id=\"split\"></a>"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = scaled_data.iloc[:,1:]\nY = scaled_data.iloc[:,-0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach 1: Baseline Model\n<a id=\"approach1\"></a>\n*In this approach, we do not make the data balance to understand whether the models improved after balancing data.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Tuning parameter for RF ( tuning parameters are choosen based on best parameters of RandomizedSearchCV)\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\nmin_samples_split = [5, 10]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\ntuning_rf = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\ntuning_rf.fit(X_train,y_train)\nprint('Best Parameter for Random Forest', tuning_rf.best_params_, tuning_rf.best_score_)\n\n# Tuning parameter for Tree\nparam_dict= {\"criterion\": ['gini', 'entropy'],\n            \"max_depth\": range(1,10),\n            \"min_samples_split\": range(1,10),\n            \"min_samples_leaf\": range(1,5)}\ntuning_tree = GridSearchCV(DecisionTreeClassifier(random_state=12),  param_grid=param_dict, cv=10, verbose=1, n_jobs=-1)\ntuning_tree.fit(X_train,y_train)\nprint('Best Parameter for Tree', tuning_tree.best_params_, tuning_tree.best_score_)\n\n# Xgboost Parameters\nparam_xgb = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6],\n 'gamma':[i/10.0 for i in range(0,5)]\n}\ntuning_xgb = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_xgb, scoring='roc_auc',n_jobs=4, cv=5)\ntuning_xgb.fit(X_train,y_train)\nprint('Best Parameter for XGBoost', tuning_xgb.best_params_, tuning_xgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Voting Classifier\nclf1 = DecisionTreeClassifier()\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4= KNeighborsClassifier()\nclf5= LinearDiscriminantAnalysis()\nclf6= XGBClassifier()\n\n# Instantiate the classfiers and make a list\nclassifiers = [LinearDiscriminantAnalysis(),\n               KNeighborsClassifier(),\n               GaussianNB(), \n               SVC(kernel='linear'),\n               DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_split=9,min_samples_leaf=2, random_state=12),\n               RandomForestClassifier(n_estimators=155, max_features='auto', max_depth=45, min_samples_split=10, random_state=27),\n               XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=4, gamma=0.3, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n               VotingClassifier(estimators = [('DTree', clf1), ('rf', clf2), ('gnb', clf3),  ('knn', clf4),('lda', clf5), ('xgb', clf6)], voting ='soft')]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr1','tpr1','fpr','tpr','train_accuracy','test_accuracy', 'train_auc', 'test_auc', 'f1_score', 'precision','recall','confusion matrix','Report'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    train_accuracy= accuracy_score(y_train, y_train_pred)\n    test_accuracy= accuracy_score(y_test, y_test_pred)\n     \n    fpr, tpr, _ = roc_curve(y_test,  y_test_pred)\n    fpr1, tpr1, _ = roc_curve(y_train,  y_train_pred)\n    \n    train_auc = roc_auc_score(y_train, y_train_pred)\n    test_auc = roc_auc_score(y_test, y_test_pred)\n    \n    f1_score= metrics.f1_score(y_test, y_test_pred)\n    precision = metrics.precision_score(y_test, y_test_pred)\n    recall = metrics.recall_score(y_test, y_test_pred)\n    \n    conf_mat= confusion_matrix(y_test,y_test_pred)\n    report=classification_report(y_test,y_test_pred, digits=3, output_dict=True)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr1':fpr1,\n                                        'tpr1':tpr1,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'train_accuracy': train_accuracy,\n                                        'test_accuracy': test_accuracy,\n                                        'train_auc':train_auc,\n                                        'test_auc':test_auc,\n                                        'f1_score': f1_score,\n                                        'precision': precision,\n                                        'recall': recall,\n                                        'confusion matrix':conf_mat,\n                                        'Report':report}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_table.rename(index={'VotingClassifier':'Model Ensemble'},inplace=True)\nresult_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(result_table.iloc[0,12]).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\n\nfor i in range(result_table.shape[0]):\n    plt.plot(result_table.iloc[i,]['fpr'], \n             result_table.iloc[i,]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(result_table.index[i], result_table.iloc[i,]['test_auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table.iloc[:,[5,7,8,9,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['Accuracy', 'ROC_AUC','F1 score','Precision','Recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table.iloc[:,[7,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['ROC_AUC','Recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach 2: Oversampling - RandomOverSampler\n<a id=\"approach2\"></a>\n*In this approach, to alleviate the effects of imbalance during model training, we use oversampling technique which imputes additional data points to improve balance across classes using RandomOverSampler.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n\nfrom imblearn.over_sampling import RandomOverSampler\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority') \nX_train_over, y_train_over = oversample.fit_resample(X_train, y_train)\n  \nprint('After OverSampling, the shape of X_train: {}'.format(X_train_over.shape)) \nprint('After OverSampling, the shape of y_train: {} \\n'.format(y_train_over.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_over == 0))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*All tuning parameters are choosen based on best parameters of RandomizedSearchCV and GridSearchCV.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Tuning parameter for RF \nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\nmin_samples_split = [5, 10]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\ntuning_rf = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\ntuning_rf.fit(X_train_over,y_train_over)\nprint('Best Parameter for Random Forest', tuning_rf.best_params_, tuning_rf.best_score_)\n\n# Tuning parameter for Tree\nparam_dict= {\"criterion\": ['gini', 'entropy'],\n            \"max_depth\": range(1,10),\n            \"min_samples_split\": range(1,10),\n            \"min_samples_leaf\": range(1,5)}\ntuning_tree = GridSearchCV(DecisionTreeClassifier(random_state=12),  param_grid=param_dict, cv=10, verbose=1, n_jobs=-1)\ntuning_tree.fit(X_train_over,y_train_over)\nprint('Best Parameter for Tree', tuning_tree.best_params_, tuning_tree.best_score_)\n\n# Xgboost Parameters\nparam_xgb = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6],\n 'gamma':[i/10.0 for i in range(0,5)]}\ntuning_xgb = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_xgb, scoring='roc_auc',n_jobs=4, cv=5)\ntuning_xgb.fit(X_train_over,y_train_over)\nprint('Best Parameter for XGBoost', tuning_xgb.best_params_, tuning_xgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Voting Classifier\nclf1 = DecisionTreeClassifier()\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = KNeighborsClassifier()\nclf5= LinearDiscriminantAnalysis()\nclf6= XGBClassifier()\n\n# Instantiate the classfiers and make a list\nclassifiers = [LinearDiscriminantAnalysis(),\n               KNeighborsClassifier(),\n               GaussianNB(), \n               SVC(kernel='linear'),\n               DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_split=5,min_samples_leaf=1, random_state=12),\n               RandomForestClassifier(n_estimators=200, max_features='sqrt', max_depth=45, min_samples_split=5, random_state=27),\n               XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=6, gamma=0.4, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27),\n               VotingClassifier(estimators = [('DTree', clf1), ('rf', clf2), ('gnb', clf3), ('knn', clf4), ('lda', clf5), ('xgb', clf6)], voting ='soft')]\n\n# Define a result table as a DataFrame\nresult_table1 = pd.DataFrame(columns=['classifiers', 'fpr1','tpr1','fpr','tpr','train_accuracy','test_accuracy', 'train_auc', 'test_auc', 'f1_score', 'precision','recall','confusion matrix','Report'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train_over, y_train_over)\n    y_train_pred = model.predict(X_train_over)\n    y_test_pred = model.predict(X_test)\n    \n    train_accuracy= accuracy_score(y_train_over, y_train_pred)\n    test_accuracy= accuracy_score(y_test, y_test_pred)\n     \n    fpr, tpr, _ = roc_curve(y_test,  y_test_pred)\n    fpr1, tpr1, _ = roc_curve(y_train_over,  y_train_pred)\n    \n    train_auc = roc_auc_score(y_train_over, y_train_pred)\n    test_auc = roc_auc_score(y_test, y_test_pred)\n    \n    f1_score= metrics.f1_score(y_test, y_test_pred)\n    precision = metrics.precision_score(y_test, y_test_pred)\n    recall = metrics.recall_score(y_test, y_test_pred)\n    \n    conf_mat= confusion_matrix(y_test,y_test_pred)\n    report=classification_report(y_test,y_test_pred, digits=3, output_dict=True)\n    \n    result_table1 = result_table1.append({'classifiers':cls.__class__.__name__,\n                                        'fpr1':fpr1,\n                                        'tpr1':tpr1,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'train_accuracy': train_accuracy,\n                                        'test_accuracy': test_accuracy,\n                                        'train_auc':train_auc,\n                                        'test_auc':test_auc,\n                                        'f1_score': f1_score,\n                                        'precision': precision,\n                                        'recall': recall,\n                                        'confusion matrix':conf_mat,\n                                        'Report':report}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table1.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_table1.rename(index={'VotingClassifier':'Model Ensemble'},inplace=True)\nresult_table1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\n\nfor i in range(result_table1.shape[0]):\n    plt.plot(result_table1.iloc[i,]['fpr'], \n             result_table1.iloc[i,]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(result_table1.index[i], result_table1.iloc[i,]['test_auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table1.iloc[:,[5,7,8,9,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['Accuracy', 'ROC_AUC','f1_score','precision','recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table1.iloc[:,[7,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['ROC_AUC','Recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## APPROACH 3: UNDERSAMPLING - RandomUnderSampler\n<a id=\"approach3\"></a>\n*In this approach, we use undersampling technique which randomly removes observations of the majority class to improve the balance accross classes using RandomOverSampler function in imblearn.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n  \n# import the Random Under Sampler object.\nfrom imblearn.under_sampling import RandomUnderSampler\n# create the object.\nunder_sampler = RandomUnderSampler(random_state=2)\n# fit the object to the training data.\nX_train_under, y_train_under = under_sampler.fit_sample(X_train, y_train.ravel())\n  \nprint('After Undersampling, the shape of X_train: {}'.format(X_train_under.shape)) \nprint('After Undersampling, the shape of y_train: {} \\n'.format(y_train_under.shape)) \n\nprint(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_under == 1))) \nprint(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_under == 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Tuning parameter for RF ( tuning parameters are choosen based on best parameters of RandomizedSearchCV)\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\nmin_samples_split = [5, 10]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\ntuning_rf = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\ntuning_rf.fit(X_train_under,y_train_under)\nprint('Best Parameter for Random Forest', tuning_rf.best_params_, tuning_rf.best_score_)\n\n# Tuning parameter for Tree\nparam_dict= {\"criterion\": ['gini', 'entropy'],\n            \"max_depth\": range(1,10),\n            \"min_samples_split\": range(1,10),\n            \"min_samples_leaf\": range(1,5)}\ntuning_tree = GridSearchCV(DecisionTreeClassifier(random_state=12),  param_grid=param_dict, cv=10, verbose=1, n_jobs=-1)\ntuning_tree.fit(X_train_under,y_train_under)\nprint('Best Parameter for Tree', tuning_tree.best_params_, tuning_tree.best_score_)\n\n# Xgboost Parameters\nparam_xgb = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6],\n 'gamma':[i/10.0 for i in range(0,5)]}\ntuning_xgb = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_xgb, scoring='roc_auc',n_jobs=4, cv=5)\ntuning_xgb.fit(X_train_under,y_train_under)\nprint('Best Parameter for XGBoost', tuning_xgb.best_params_, tuning_xgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Voting Classifier\nclf1 = DecisionTreeClassifier()\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4 = KNeighborsClassifier()\nclf5= LinearDiscriminantAnalysis()\nclf6= XGBClassifier()\n\n# Instantiate the classfiers and make a list\nclassifiers = [LinearDiscriminantAnalysis(),\n               KNeighborsClassifier(),\n               GaussianNB(), \n               SVC(kernel='linear'),\n               DecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_split=2,min_samples_leaf=4, random_state=12),\n               RandomForestClassifier(n_estimators=155, max_features='auto', max_depth=45, min_samples_split=10, random_state=27),\n               XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=4,min_child_weight=5,gamma=0.4,subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=27),\n               VotingClassifier(estimators = [('DTree', clf1), ('rf', clf2), ('gnb', clf3), ('knn', clf4), ('lda', clf5), ('xgb', clf6)], voting ='soft')]\n\n# Define a result table as a DataFrame\nresult_table2 = pd.DataFrame(columns=['classifiers', 'fpr1','tpr1','fpr','tpr','train_accuracy','test_accuracy', 'train_auc', 'test_auc', 'f1_score', 'precision','recall','confusion matrix','Report'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model2 = cls.fit(X_train_under, y_train_under)\n    y_train_pred2 = model2.predict(X_train_under)\n    y_test_pred2 = model2.predict(X_test)\n    \n    train_accuracy= accuracy_score(y_train_under, y_train_pred2)\n    test_accuracy= accuracy_score(y_test, y_test_pred2)\n     \n    fpr, tpr, _ = roc_curve(y_test,  y_test_pred2)\n    fpr1, tpr1, _ = roc_curve(y_train_under,  y_train_pred2)\n    \n    train_auc = roc_auc_score(y_train_under, y_train_pred2)\n    test_auc = roc_auc_score(y_test, y_test_pred2)\n    \n    f1_score= metrics.f1_score(y_test, y_test_pred2)\n    precision = metrics.precision_score(y_test, y_test_pred2)\n    recall = metrics.recall_score(y_test, y_test_pred2)\n    \n    conf_mat= confusion_matrix(y_test,y_test_pred2)\n    report=classification_report(y_test,y_test_pred2, digits=3, output_dict=True)\n    \n    result_table2 = result_table2.append({'classifiers':cls.__class__.__name__,\n                                        'fpr1':fpr1,\n                                        'tpr1':tpr1,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'train_accuracy': train_accuracy,\n                                        'test_accuracy': test_accuracy,\n                                        'train_auc':train_auc,\n                                        'test_auc':test_auc,\n                                        'f1_score': f1_score,\n                                        'precision': precision,\n                                        'recall': recall,\n                                        'confusion matrix':conf_mat,\n                                        'Report':report}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table2.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_table2.rename(index={'VotingClassifier':'Model Ensemble'},inplace=True)\nresult_table2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\n\nfor i in range(result_table2.shape[0]):\n    plt.plot(result_table2.iloc[i,]['fpr'], \n             result_table2.iloc[i,]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(result_table2.index[i], result_table2.iloc[i,]['test_auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table2.iloc[:,[5,7,8,9,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['Accuracy', 'ROC_AUC','f1_score','precision','recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table2.iloc[:,[7,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend(['ROC_AUC','Recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## APPROACH 4 : OVERSAMPLING- SMOTE\n<a id=\"approach4\"></a>\nIn this approach, we use oversampling technique using Synthetic Minority Oversampling Technique, or SMOTE for short."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n\nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state = 2 )#, sampling_strategy=0.25) \nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n  \nprint('After OverSampling, the shape of X_train: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of y_train: {} \\n'.format(y_train_res.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Tuning parameter for RF ( tuning parameters are choosen based on best parameters of RandomizedSearchCV)\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\nmin_samples_split = [5, 10]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\ntuning_rf = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n#tuning_rf.fit(X_train_res,y_train_res)\n#tuning_rf.best_params_, tuning_rf.best_score_\n\n# Tuning parameter for Tree\nparam_dict= {\"criterion\": ['gini', 'entropy'],\n            \"max_depth\": range(1,10),\n            \"min_samples_split\": range(1,10),\n            \"min_samples_leaf\": range(1,5)}\ntuning_tree = GridSearchCV(DecisionTreeClassifier(random_state=12),  param_grid=param_dict, cv=10, verbose=1, n_jobs=-1)\n#tuning_tree.fit(X_train_res,y_train_res)\n#tuning_tree.best_params_, tuning_tree.best_score_\n\n# Xgboost Parameters\nparam_xgb = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6],\n 'gamma':[i/10.0 for i in range(0,5)]\n}\ntuning_xgb = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_xgb, scoring='roc_auc',n_jobs=4, cv=5)\n#tuning_xgb.fit(X_train_res,y_train_res)\n#tuning_xgb.best_params_, tuning_xgb.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Voting Classifier\nclf1 = DecisionTreeClassifier()\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nclf4= KNeighborsClassifier()\nclf5= LinearDiscriminantAnalysis()\nclf6= XGBClassifier()\n\n# Instantiate the classfiers and make a list\nclassifiers = [LinearDiscriminantAnalysis(),\n               KNeighborsClassifier(),\n               GaussianNB(), \n               SVC(kernel='linear'),\n               DecisionTreeClassifier(criterion='gini', max_depth=9,min_samples_split=2,min_samples_leaf=3, random_state=12),\n               RandomForestClassifier(n_estimators=200, max_features='sqrt', max_depth=45, min_samples_split=5, random_state=27),\n               XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=5, gamma=0.4, subsample=0.8, colsample_bytree=0.8, nthread=4,scale_pos_weight=1,seed=27),\n               VotingClassifier(estimators = [('DTree', clf1), ('rf', clf2), ('gnb', clf3), ('knn', clf4), ('lda', clf5), ('xgb', clf6)], voting ='soft')]\n\n# Define a result table as a DataFrame\nresult_table3 = pd.DataFrame(columns=['classifiers', 'fpr1','tpr1','fpr','tpr','train_accuracy','test_accuracy', 'train_auc', 'test_auc', 'f1_score', 'precision','recall','confusion matrix','Report'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model3 = cls.fit(X_train_res, y_train_res)\n    y_train_pred3 = model3.predict(X_train_res)\n    y_test_pred3 = model3.predict(X_test)\n    \n    train_accuracy= accuracy_score(y_train_res, y_train_pred3)\n    test_accuracy= accuracy_score(y_test, y_test_pred3)\n     \n    fpr, tpr, _ = roc_curve(y_test,  y_test_pred3)\n    fpr1, tpr1, _ = roc_curve(y_train_res,  y_train_pred3)\n    \n    train_auc = roc_auc_score(y_train_res, y_train_pred3)\n    test_auc = roc_auc_score(y_test, y_test_pred3)\n    \n    f1_score= metrics.f1_score(y_test, y_test_pred3)\n    precision = metrics.precision_score(y_test, y_test_pred3)\n    recall = metrics.recall_score(y_test, y_test_pred3)\n    \n    conf_mat= confusion_matrix(y_test,y_test_pred3)\n    report=classification_report(y_test,y_test_pred3, digits=3, output_dict=True)\n    \n    result_table3 = result_table3.append({'classifiers':cls.__class__.__name__,\n                                        'fpr1':fpr1,\n                                        'tpr1':tpr1,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'train_accuracy': train_accuracy,\n                                        'test_accuracy': test_accuracy,\n                                        'train_auc':train_auc,\n                                        'test_auc':test_auc,\n                                        'f1_score': f1_score,\n                                        'precision': precision,\n                                        'recall': recall,\n                                        'confusion matrix':conf_mat,\n                                        'Report':report}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table3.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_table3.rename(index={'VotingClassifier':'Model Ensemble'},inplace=True)\n\nresult_table3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\n\nfor i in range(result_table3.shape[0]):\n    plt.plot(result_table3.iloc[i,]['fpr'], \n             result_table3.iloc[i,]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(result_table3.index[i], result_table3.iloc[i,]['test_auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table3.iloc[:,[5,7,8,9,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Scores')\nplt.title('Result of Models')\nplt.legend(['Accuracy', 'ROC_AUC','f1_score','precision','recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(result_table3.iloc[:,[7,10]])\nplt.xlabel('Models')\nplt.xticks(rotation=90)\nplt.ylabel('Score')\nplt.title('Result of Models')\nplt.legend([ 'ROC_AUC','Recall'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Baseline Model\nresult_table.iloc[:,[4,5,6,7,8,9,10]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Oversampling with RandomOverSampler\nresult_table1.iloc[:,[4,5,6,7,8,9,10]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Undersampling\nresult_table2.iloc[:,[4,5,6,7,8,9,10]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Oversampling with SMOTE\nresult_table3.iloc[:,[4,5,6,7,8,9,10]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance and Final Model Selection\n<a id=\"keyfeatures\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance in RandomOverSampler\n<a id=\"keyfeatures1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*After 4 different approaches, we decide to choose best model according to AUC, f1, precision and recall score. The best model is Xgboost in RandomOverSampler.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=6, gamma=0.4, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27)\nmodel_xgb = xgb.fit(X_train_over, y_train_over)\ny_train_xgb = model_xgb.predict(X_train_over)\ny_test_xgb = model_xgb.predict(X_test)\n\nprint(confusion_matrix(y_test,y_test_xgb))\nprint(classification_report(y_test,y_test_xgb, digits=3))\n\nprint('Train accuracy: %0.3f' % accuracy_score(y_train_over, y_train_xgb))\nprint('Test accuracy: %0.3f' % accuracy_score(y_test, y_test_xgb))\n\nprint('Train AUC: %0.3f' % roc_auc_score(y_train_over, y_train_xgb))\nprint('Test AUC: %0.3f' % roc_auc_score(y_test, y_test_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shap Value"},{"metadata":{"trusted":false},"cell_type":"code","source":"import shap\nexpl_xgb = shap.TreeExplainer(model_xgb)\nshap_xgb = expl_xgb.shap_values(X_train_over)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_xgb, X_train_over, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_xgb, X_train_over)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBoost Tree SHAP algorithm computes the SHAP values with respect to the margin, not the transformed probability. So we are seeing log odds values. In order to have probability values link='logit' is added.\nshap.initjs()\nshap.force_plot(expl_xgb.expected_value, shap_xgb[1050,:], X_train_over.iloc[1050,:], link='logit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*For example: Above forceplot shows that the output value is the prediction for 1050th observation which is lower than base value. The base value is “the value that would be predicted if we did not know any features for the current output.” In other words, it is the mean prediction, or mean(yhat). Duration variable tends to push prediction lower the most.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBoost Tree SHAP algorithm computes the SHAP values with respect to the margin, not the transformed probability. So we are seeing log odds values. In order to have probability values link='logit' is added.\nshap.initjs()\nshap.force_plot(expl_xgb.expected_value, shap_xgb[4000,:], X_train_over.iloc[4000,:], link='logit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Above forceplot shows that the output value is the prediction for 4000th observation which is higher than base value. Duration variable tends to push prediction higher the most.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"# base value\ny_train_over.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_over.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_over.iloc[4000,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\n# Fit model using each importance as a threshold\nthresholds = sort(model_xgb.feature_importances_)\nfor thresh in thresholds:\n\t# select features using threshold\n\tselection = SelectFromModel(model_xgb, threshold=thresh, prefit=True)\n\tselect_X_train = selection.transform(X_train_over)\n\t# train model\n\tselection_model =  XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=6, gamma=0.4, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27)\n\tselection_model.fit(select_X_train, y_train_over)\n\t# eval model\n\tselect_X_test = selection.transform(X_test)\n\ty_pred = selection_model.predict(select_X_test)\n\tpredictions = [round(value) for value in y_pred]\n\tauc= roc_auc_score(y_test, predictions)\n\tprint(\"Thresh=%.3f, n=%d, AUC: %.2f%%\" % (thresh, select_X_train.shape[1], auc*100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n# configure to select a subset of features\n    fs = SelectFromModel( XGBClassifier(learning_rate =0.1, \n                                        n_estimators=140, \n                                        max_depth=4, \n                                        min_child_weight=6, \n                                        gamma=0.4, \n                                        subsample=0.8, \n                                        colsample_bytree=0.8, \n                                        nthread=4, \n                                        scale_pos_weight=1,\n                                        seed=27), max_features=22).fit(X_train_over, y_train_over)\n    # transform train input data\n    X_train_fs = fs.transform(X_train_over)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_fs, X_test_fs, fs = select_features(X_train_over, y_train_over, X_test)\n# fit the model\nmodel = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=6, gamma=0.4, subsample=0.8, colsample_bytree=0.8, nthread=4, scale_pos_weight=1,seed=27)\nmodel.fit(X_train_fs, y_train_over)\ny_train_pred = model.predict(X_train_fs)\n# evaluate the model\nyhat = model.predict(X_test_fs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_idx = fs.get_support()\nfeature_name = X_train_over.columns[feature_idx]\nfeature_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(confusion_matrix(y_test,yhat))\nprint(classification_report(y_test,yhat, digits=3))\n\nprint('Train accuracy: %0.3f' % accuracy_score(y_train_over, y_train_pred))\nprint('Test accuracy: %0.3f' % accuracy_score(y_test, yhat))\n\nprint('Train AUC: %0.3f' % roc_auc_score(y_train_over, y_train_pred))\nprint('Test AUC: %0.3f' % roc_auc_score(y_test, yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap1 = shap.TreeExplainer(model)\nshap_xgb1 = shap1.shap_values(X_train_fs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_xgb1, X_train_fs, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_xgb1, X_train_fs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(shap1.expected_value, shap_xgb1[4000,:], X_train_fs[4000,:], link='logit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance in RandomUnderSampler\n<a id=\"keyfeatures2\"></a>\n*Second best model was Xgboost in RandomUnderSampler, let's have a look at it.*"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_under = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=4,min_child_weight=5,gamma=0.4,subsample=0.8,colsample_bytree=0.8,nthread=4,scale_pos_weight=1,seed=27)\nxgb_under.fit(X_train_under, y_train_under)\ny_train_pred= xgb_under.predict(X_train_under)\n\n# making predictions on the testing set \ny_pred = xgb_under.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint('Train accuracy:%0.3f' %(accuracy_score(y_train_under, y_train_pred)))\nprint('Test accuracy:%0.3f' %(accuracy_score(y_test, y_pred)))\n\nprint('AUC score for train: %0.3f ' % (roc_auc_score(y_train_under,y_train_pred)))\nprint('AUC score for test: %0.3f' % (roc_auc_score(y_test, y_pred)))\n\n#Draw ROC Curve\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nroc_auc = metrics.roc_auc_score(y_test, y_pred)\nplt.figure(figsize=(12,6))\nplt.plot(fpr,tpr,label=\"ROC Curve (area = %0.2f)\" % roc_auc , color='darkorange')\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import shap\nexplainer = shap.TreeExplainer(xgb_under)\nshap_under = explainer.shap_values(X_train_under)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_under, X_train_under, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shap.summary_plot(shap_under, X_train_under)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBoost Tree SHAP algorithm computes the SHAP values with respect to the margin, not the transformed probability. So we are seeing log odds values. In order to have probability values link='logit' is added.\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_under[1050,:], X_train_under.iloc[1050,:], link='logit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBoost Tree SHAP algorithm computes the SHAP values with respect to the margin, not the transformed probability. So we are seeing log odds values. In order to have probability values link='logit' is added.\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_under[4000,:], X_train_under.iloc[4000,:], link='logit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\n# Fit model using each importance as a threshold\nthresholds = sort(xgb_under.feature_importances_)\nfor thresh in thresholds:\n\t# select features using threshold\n\tselection = SelectFromModel(xgb_under, threshold=thresh, prefit=True)\n\tselect_X_train = selection.transform(X_train_under)\n\t# train model\n\tselection_model = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=4,min_child_weight=5,gamma=0.4,subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=27)\n\tselection_model.fit(select_X_train, y_train_under)\n\t# eval model\n\tselect_X_test = selection.transform(X_test)\n\ty_pred = selection_model.predict(select_X_test)\n\tpredictions = [round(value) for value in y_pred]\n\tauc= roc_auc_score(y_test, predictions)\n\tprint(\"Thresh=%.3f, n=%d, AUC: %.2f%%\" % (thresh, select_X_train.shape[1], auc*100.0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}