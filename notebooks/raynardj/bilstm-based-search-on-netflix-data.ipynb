{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Netflix Dataset\n>and all those embedding funs"},{"metadata":{},"cell_type":"markdown","source":"* Create a pytorch dataset for text\n* A BiLSTM model to predict multiple genre\n* Encode the text to vectors using the model we trained\n* Search the closest description"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install forgebox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA = Path(\"/kaggle/input/netflix-shows/netflix_titles.csv\")\n\ndf = pd.read_csv(DATA)\n\ndf.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So... what Y?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.listed_in.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rating.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"listed_in\"] = df.listed_in.str\\\n.replace(\"&\",\",\")\\\n.replace(\" , \",\",\")\\\n.replace(\" ,\",\",\")\\\n.replace(\", \",\",\")\\\n.replace(\" , \",\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre = list(set(i.strip() for i in (\",\".join(list(df.listed_in))).split(\",\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total genre: {len(genre)}\\n\")\nfor g in genre:\n    print(g,end=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eye = np.eye(len(genre))\ngenre_dict = dict((v,eye[k]) for k,v in enumerate(genre))\n\ndef to_nhot(text):\n    return np.sum(list(genre_dict[g.strip()] for g in text.split(\",\")),axis=0).astype(np.int)\n\ndf[\"genre\"] = df.listed_in.apply(to_nhot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PROCESSED = \"processed.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(PROCESSED,index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Process the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from forgebox.ftorch.prepro import split_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df,val_df = split_df(df,valid=0.1)\nprint(f\"train:{len(train_df)}\\tvalid:{len(val_df)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\ntkz = TweetTokenizer()\ndef tokenize(txt):\n    return tkz.tokenize(txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize(\"A man returns home after being released from \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate vocabulary map from material"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfrom multiprocessing import Pool\nfrom collections import Counter\nfrom torch.utils.data.dataset import Dataset\n\nclass Vocab(object):\n    def __init__(self, iterative, max_vocab = 12000, tokenize = tkz.tokenize):\n        \"\"\"\n        Count the most frequent words\n        Make the word<=>index mapping\n        \"\"\"\n        self.l = list(iterative)\n        self.max_vocab = max_vocab\n        self.tokenize = tokenize\n        self.word_beads = self.word_beads_()\n        self.counter()\n        \n    def __len__(self):\n        return len(self.words)\n        \n    def __repr__(self):\n        return f\"vocab {self.max_vocab}\"\n        \n    def word_beads_(self, nproc=10):\n        self.p = Pool(nproc)\n        return list(chain(*list(self.p.map(self.tokenize,self.l))))\n    \n    def counter(self):\n        vals = np.array(list((k,v) for k,v in dict(Counter(self.word_beads)).items()))\n        self.words = pd.DataFrame({\"tok\":vals[:,0],\"ct\":vals[:,1]}).sort_values(by= \"ct\",ascending=False)\\\n        .reset_index().drop(\"index\",axis=1).head(self.max_vocab-2)\n        self.words[\"idx\"] = (np.arange(len(self.words))+2)\n        self.words=pd.concat([self.words,pd.DataFrame({\"tok\":[\"<eos>\",\"<mtk>\"],\"ct\":[-1,-1],\"idx\":[0,1]})])\n        return self.words\n    \n    def to_i(self):\n        self.t2i = dict(zip(self.words[\"tok\"],self.words[\"idx\"]))\n        def to_index(t):\n            i = self.t2i.get(t)\n            if i==None:\n                return 1\n            else:\n                return i\n        return to_index\n    \n    def to_t(self):\n        return np.roll(self.words[\"tok\"],2)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = Vocab(df.description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vocabulary build from training"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab.words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class seqData(Dataset):\n    def __init__(self,lines,vocab):\n        self.lines = list(lines)\n        self.vocab = vocab\n        self.to_i = np.vectorize(vocab.to_i())\n        self.to_t = vocab.to_t()\n        self.bs=1\n        \n    def __len__(self):\n        return len(self.lines)\n    \n    def __getitem__(self,idx):\n        \"\"\"\n        Translate words to indices\n        \"\"\"\n        line = self.lines[idx]\n        words = self.vocab.tokenize(line)\n        words = [\"<eos>\",]+words+[\"<eos>\"]\n        return self.to_i(np.array(words))\n    \n    def backward(self,seq):\n        \"\"\"\n        This backward has nothing to do with gradrient\n        Just to error proof the tokenized line\n        \"\"\"\n        return \" \".join(self.to_t[seq])\n    \nclass arrData(Dataset):\n    def __init__(self, *arrs):\n        self.arr = np.concatenate(arrs,axis=1)\n    \n    def __len__(self):\n        return self.arr.shape[0]\n    \n    def __getitem__(self,idx):\n        return self.arr[idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build vocabulary and train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = Vocab(df.description)\n\ntrain_seq = seqData(train_df.description,vocab)\ntrain_y = arrData(np.stack(train_df.genre.values))\n\nval_seq = seqData(val_df.description,vocab)\nval_y = arrData(np.stack(val_df.genre.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Size of train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_seq),len(train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_line = train_seq[10]\ntokenized_line","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reconstruct the sentence from indices\n\n>**<mtk\\>** means the missing tokens, for they are less frequent than we should hav cared"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq.backward(tokenized_line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data.dataloader import DataLoader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A custom made collate function\n\n* Collate function will do the following:\n>Make rows of dataset output into a batch of tensor"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_collate(rows):\n    \"\"\"\n    this collate will pad any sentence that is less then the max length\n    \"\"\"\n    line_len = torch.LongTensor(list(len(row) for row in rows));\n    max_len = line_len.max()\n    ones = torch.ones(max_len.item()).long()\n    line_pad = max_len-line_len\n    return torch.stack(list(torch.cat([torch.LongTensor(row),ones[:pad.item()]]) for row,pad in zip(rows,line_pad[:,None])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen = iter(DataLoader(train_seq,batch_size=16, collate_fn=pad_collate))\nnext(gen).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class fuse(Dataset):\n    def __init__(self, *datasets):\n        \"\"\"\n        A pytorch dataset combining the dataset\n        :param datasets:\n        \"\"\"\n        self.datasets = datasets\n        length_s = set(list(len(d) for d in self.datasets))\n        assert len(length_s) == 1, \"dataset lenth not matched\"\n        self.length = list(length_s)[0]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        return tuple(d.__getitem__(idx) for d in self.datasets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_collate(*funcs):\n    def combined(rows):\n        xs = list(zip(*rows))\n        return tuple(func(x) for func, x in zip(funcs,xs))\n    return combined\n\nfrom torch.utils.data._utils.collate import default_collate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DataLoader(train_y).collate_fn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fusing data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = fuse(train_seq,train_y)\nval_ds = fuse(val_seq,val_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen = iter(DataLoader(train_ds,batch_size=16, collate_fn=combine_collate(pad_collate,default_collate)))\nx,y = next(gen)\nprint(x.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class basicNLP(nn.Module):\n    def __init__(self, hs):\n        super().__init__()\n        self.hs = hs\n        self.emb = nn.Embedding(len(vocab),hs)\n        self.rnn = nn.LSTM(input_size = hs,hidden_size = hs,batch_first = True)\n        self.fc = nn.Sequential(*[\n            nn.BatchNorm1d(hs*2),\n            nn.ReLU(),\n            nn.Linear(hs*2,hs*2),\n            nn.BatchNorm1d(hs*2),\n            nn.ReLU(),\n            nn.Linear(hs*2,49),\n        ])\n        \n    def encoder(self,x):\n        x = self.emb(x)\n        o1,(h1,c1) = self.rnn(x)\n        # run sentence backward\n        o2,(h2,c2) = self.rnn(x.flip(dims=[1]))\n        return torch.cat([h1[0],h2[0]],dim=1)\n        \n    def forward(self,x):\n        vec = self.encoder(x)\n        return self.fc(vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = basicNLP(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What does embedding do?"},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape,model.emb(x).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What does LSTM return?"},{"metadata":{},"cell_type":"markdown","source":"For what is LSTM, read this [awesome blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), from which I stole the following visualization from\n\n#### In short version\nRNN, it's about sharing model weights throughout temporal sequence, as convolusion share weights in spatial point of view\n![image.png](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n\n* The above green \"A\" area re shared linear layer\n* GRU & LSTM are advanced version of RNN, with gate control\n* The black arrows above in GRU & LSTM are controlled by gates\n* Gates, are just linear layer with sigmoid activation $\\sigma(x)$, its outputs are between (0,1), hence the name gate, the following illustration is one of the gates in a lstm cell, called input gate\n![gate controls](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n* Other gates control other things like should we forget the early part of then sentence, should we output this .etc\n\n"},{"metadata":{},"cell_type":"markdown","source":"### In terms of code"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time \noutput,(hidden_state, cell_state) = model.rnn(model.emb(x))\nfor t in (output,hidden_state, cell_state):\n    print(t.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Disect the iteration through the sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ninit_hidden = torch.zeros((1,16,100))\ninit_cell = torch.zeros((1,16,100))\nlast_h,last_c = init_hidden,init_cell\noutputs = []\nx_vec = model.emb(x)\nfor row in range(x.shape[1]):\n    last_o, (last_h,last_c) = model.rnn(x_vec[:,row:row+1,:],(last_h,last_c))\n    outputs.append(last_o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_iteration_result = torch.cat(outputs,dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_iteration_result.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 2 results are the same, of course, I thought manual python iteration is slower,but they are really close by the above test"},{"metadata":{"trusted":true},"cell_type":"code","source":"(manual_iteration_result==output).float().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"lossf = nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from forgebox.ftorch.train import Trainer\nfrom forgebox.ftorch.callbacks import stat\nfrom forgebox.ftorch.metrics import metric4_bi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Trainer(train_ds, val_dataset=val_ds,batch_size=16,callbacks=[stat], val_callbacks=[stat] ,shuffle=True,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.opt[\"adm1\"] = torch.optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combined collate function"},{"metadata":{"trusted":true},"cell_type":"code","source":"t.train_data.collate_fn = combine_collate(pad_collate,default_collate)\nt.val_data.collate_fn = combine_collate(pad_collate,default_collate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@t.step_train\ndef train_step(self):\n    self.opt.zero_all()\n    x,y = self.data\n    y_= model(x)\n    loss = lossf(y_,y.float())\n    loss.backward()\n    self.opt.step_all()\n    acc,rec,prec,f1 = metric4_bi(torch.sigmoid(y_),y)\n    return dict((k,v.item()) for k,v in zip([\"loss\",\"acc\",\"rec\",\"prec\",\"f1\"],(loss,acc,rec,prec,f1)))\n                \n@t.step_val\ndef val_step(self):\n    x,y = self.data\n    y_= model(x)\n    loss = lossf(y_,y.float())\n    acc,rec,prec,f1 = metric4_bi(torch.sigmoid(y_),y)\n    return dict((k,v.item()) for k,v in zip([\"loss\",\"acc\",\"rec\",\"prec\",\"f1\"],(loss,acc,rec,prec,f1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.train(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Search similar"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.eval()\ndl = DataLoader(train_seq, batch_size=32, collate_fn=pad_collate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_gen = iter(dl)\nresult = []\nfor i in range(len(dl)):\n    x=next(text_gen)\n    x = x.cuda()\n    x_vec = model.encoder(x)\n    result.append(x_vec.cpu())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A vector representing each of the sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_vec = torch.cat(result,dim=0).detach().numpy()\nresult_vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_idx(line):\n    words = train_seq.vocab.tokenize(line)\n    words = [\"<eos>\",]+words+[\"<eos>\"]\n    return train_seq.to_i(np.array(words))[None,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_idx(\"this\"), to_idx(\"to be or not to be\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_vec(line):\n    vec = torch.LongTensor(to_idx(line)).cuda()\n    return model.encoder(vec).cpu().detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_vec(\"this\"), to_vec(\"to be or not to be\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def l2norm(x):\n    \"\"\"\n    L2 Norm\n    \"\"\"\n    return np.linalg.norm(x,2,1).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"max_colwidth\",150)\n\ndef search(line):\n    vec = to_vec(line)\n    sim = ((vec* result_vec)/l2norm(result_vec)).sum(-1)\n    return pd.DataFrame({\"text\":train_seq.lines,\"sim\":sim})\\\n        .sort_values(by=\"sim\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search(\"Experience our planet's natural beauty\").head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search(\"love story,marriage, girl\").head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, usually it should be more accurate if we have more data"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}