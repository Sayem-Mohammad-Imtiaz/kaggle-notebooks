{"cells":[{"metadata":{},"cell_type":"markdown","source":"**DATASET COLUMNS EXPLANATION** :\n\n* Age: (age in years)\n* Sex: (1 = male; 0 = female)\n* CP: (chest pain type)\n* TRESTBPS: (resting blood pressure (in mm Hg on admission to the hospital))\n* CHOL: (serum cholestoral in mg/dl)\n* FPS: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* RESTECH: (resting electrocardiographic results)\n* THALACH:  (maximum heart rate achieved)\n* EXANG : (exercise induced angina (1 = yes; 0 = no))\n* OLDPEAK : (ST depression induced by exercise relative to rest)\n* SLOPE : (the slope of the peak exercise ST segment)\n* CA : (number of major vessels (0-3) colored by flourosopy)\n* THAL : (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* TARGET :(1 or 0)"},{"metadata":{},"cell_type":"markdown","source":"**Load Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier \nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, accuracy_score, f1_score, roc_curve, roc_auc_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df.loc[:, df.columns!='target']\ny=df['target']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will use heatmap to see if there is some correlations with features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In this task will try different features selection approaches such as SelectKBest, feature selections provided by models and RFECV**"},{"metadata":{},"cell_type":"markdown","source":"1. SelectKBest"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train , y_test= train_test_split(x, y, test_size=0.2)\nselect_feature= SelectKBest(chi2, k=5).fit(x_train, y_train)\nfeature_importances_kbest= pd.DataFrame(columns=['scores'], index= x.columns)\nfeature_importances_kbest['scores']= select_feature.scores_\nfeature_importances_kbest=feature_importances_kbest.sort_values('scores', ascending= False)\nprint(feature_importances_kbest)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I'm going to fit RandomForest model on different combination of features starting with only 3 top features recommended by Scikitlearn to all 13 features and repeat the same steps with the 2 other features selection methods, after that I will put the results in a DataFrame so we can visualize it and compare between different combinations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=['3 features', '4 features', '5 features', '6 features', '7 features', '8 features', '9 features', '10 features', '11 features', '12 features', '13 features' ]\nresults_rf_features_kbest=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc score'], index=num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model= RandomForestClassifier()\nfor ind in range(3, len(feature_importances_kbest.index)+1):\n    \n    x_rf_kbest_train= x_train.loc[:, feature_importances_kbest.index[0:ind]]\n    x_rf_kbest_test= x_test.loc[:, feature_importances_kbest.index[0:ind]]\n    rf_model.fit(x_rf_kbest_train, y_train)\n    y_pred_kbest= rf_model.predict(x_rf_kbest_test)\n    \n    results_rf_features_kbest.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 1]=recall_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 2]=precision_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 3]=f1_score(y_test, y_pred_kbest)\n    results_rf_features_kbest.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred_kbest)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_kbest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Now I'm going to sort only the two best combinations of features in terms of accuracy, recall and auc score**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_kbest.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_kbest.sort_values(['recall score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_kbest.sort_values(['roc score'], ascending= False).iloc[0:2,:] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we got the best results with 8 top features\n"},{"metadata":{},"cell_type":"markdown","source":"**Now let's try the second method which is provided by the model and repeat the same steps as we did before** "},{"metadata":{},"cell_type":"markdown","source":"2. Feature_importances "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeature_importances_rf= pd.DataFrame(columns=['scores'], index= x.columns)\nfeature_importances_rf['scores']= rf_model.feature_importances_\nfeature_importances_rf=feature_importances_rf.sort_values('scores', ascending= False)\nfeature_importances_rf\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_rf=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc auc score'], index=num_features)\n\nfor ind in range(1, len(feature_importances_rf.index)+1):\n        \n    x_rf_rf_train=x_train.loc[:, feature_importances_rf.index[0:ind]]\n    x_rf_rf_test= x_test.loc[:, feature_importances_rf.index[0:ind]]\n    \n    rf_model.fit(x_rf_rf_train, y_train)\n    y_pred3= rf_model.predict(x_rf_rf_test)\n\n\n    results_rf_features_rf.iloc[ind-3, 0]=accuracy_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 1]=recall_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 2]=precision_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 3]=f1_score(y_test, y_pred3)\n    results_rf_features_rf.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred3)\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_rf.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_rf.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rf_features_rf.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in this approach  we got the best results with all features."},{"metadata":{},"cell_type":"markdown","source":"But what about selecting features with recursive feature elimination with cross-validation ? let's see "},{"metadata":{},"cell_type":"markdown","source":"3. RFCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model=RandomForestClassifier()\nrfe= RFECV(estimator=rf_model, cv=5, step=1, scoring='accuracy')\nrfe=rfe.fit(x_train, y_train)\nprint('choosen best  features ', x_train.columns[rfe.support_])\nfeature_importances_rfe= pd.DataFrame(columns=['important features'] )\nfeature_importances_rfe['important features'] = x_train.columns[rfe.support_]\nfeature_importances_rfe\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'f1 score', 'roc auc score'], index=num_features)\nfor ind in range(3, len(feature_importances_rfe.index)+1):\n    \n    \n    x_f4_train=x_train.loc[:, feature_importances_rfe['important features'] [0:ind]]\n    x_f4_test= x_test.loc[:, feature_importances_rfe['important features'] [0:ind]]\n    \n    rf_model.fit(x_f4_train, y_train)\n    y_pred4= rf_model.predict(x_f4_test)\n    \n    \n    results_rfecv_features.iloc[ind-3, 0]=accuracy_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 1]=recall_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 2]=precision_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 3]=f1_score(y_test, y_pred4)\n    results_rfecv_features.iloc[ind-3, 4]=roc_auc_score(y_test, y_pred4)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**as we can see above, eliminating the least good feature 'thalassemia' shows the greatest scores**"},{"metadata":{},"cell_type":"markdown","source":"Now I'm going to compare between the 3 methods of features selection and take the best combination for each method and compare these 3 selected ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_random_forest=pd.DataFrame(columns=[ 'RF_features_importances','RF_KBEST' ,'RF_rfecv'], index=['accuracy_score', 'recall_score', 'precision_score', 'f1score','roc_score'])\nresults_rf_features_sorted= results_rf_features_rf.sort_values('accuracy score', ascending=False)\nresults_rf_features_kbest_sorted= results_rf_features_kbest.sort_values('accuracy score', ascending= False)\nresults_rfecv_features_sorted= results_rfecv_features.sort_values('accuracy score', ascending= False)\n\n\nresults_random_forest.iloc[0,0]=results_rf_features_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,0]=results_rf_features_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,0]=results_rf_features_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,0]=results_rf_features_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,0]=results_rf_features_sorted.iloc[0,:][4]\n\nresults_random_forest.iloc[0,1]=results_rf_features_kbest_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,1]=results_rf_features_kbest_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,1]=results_rf_features_kbest_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,1]=results_rf_features_kbest_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,1]=results_rf_features_kbest_sorted.iloc[0,:][4]\n\nresults_random_forest.iloc[0,2]=results_rfecv_features_sorted.iloc[0,:][0]\nresults_random_forest.iloc[1,2]=results_rfecv_features_sorted.iloc[0,:][1]\nresults_random_forest.iloc[2,2]=results_rfecv_features_sorted.iloc[0,:][2]\nresults_random_forest.iloc[3,2]=results_rfecv_features_sorted.iloc[0,:][3]\nresults_random_forest.iloc[4,2]=results_rfecv_features_sorted.iloc[0,:][4]\n\nresults_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_random_forest[results_random_forest.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'f1score', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can observe, RandomForest is giving the best results using KBest feature selections which include 8 features \n"},{"metadata":{},"cell_type":"markdown","source":"For the seek of better results I'm going to repeat these steps with XGboost"},{"metadata":{},"cell_type":"markdown","source":"1. XGBOOST Feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model= XGBClassifier(random_state= 22, use_label_encoder=False)\nxgb_model.fit(x_train, y_train)\nf_imp_xgboost= pd.DataFrame(columns= ['feature importances'], index= x_train.columns)\nresults_xgboost_scores= pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'roc auc score'], index= num_features)\n\n\nf_imp_xgboost['feature importances']=abs( xgb_model.feature_importances_)    \nf_imp_xgboost=f_imp_xgboost.sort_values('feature importances', ascending= False)    \nf_imp_xgboost\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor ind in range(3, len(f_imp_xgboost.index)+1):\n        \n    x_xgb_train2=x_train.loc[:, f_imp_xgboost.index[0:ind]]\n    x_xgb_test2= x_test.loc[:, f_imp_xgboost.index[0:ind]]\n    \n    xgb_model.fit(x_xgb_train2, y_train)\n    y_pred_xgb2= xgb_model.predict(x_xgb_test2)\n\n\n    results_xgboost_scores.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgb2)\n    results_xgboost_scores.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgb2)\n\n\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_scores.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_scores.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_scores.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, including all features shows greatest results."},{"metadata":{},"cell_type":"markdown","source":"2. XGBoost with SelectKBest"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgb_kbest= pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score', 'roc auc score'], index= num_features)\nfor ind in range(3,  len(feature_importances_kbest.index)+1):\n        \n    x_xgb_train1=x_train.loc[:,feature_importances_kbest.index[0:ind]]\n    x_xgb_test1= x_test.loc[:, feature_importances_kbest.index[0:ind]]\n    \n    xgb_model.fit(x_xgb_train1, y_train)\n    y_pred_xgb1= xgb_model.predict(x_xgb_test1)\n\n\n    results_xgb_kbest.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgb1)\n    results_xgb_kbest.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgb1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgb_kbest.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgb_kbest.sort_values(['recall score'], ascending= False).iloc[0:2,:]\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgb_kbest.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for this type of features selection, 8 features give the highest results"},{"metadata":{},"cell_type":"markdown","source":"3. XGBOOST with RFECV"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost_model=XGBClassifier(random_state= 22, use_label_encoder=False)     \n\nrfe2=RFE(estimator=xgboost_model, n_features_to_select=13, step=1)\n\nrfe2=rfe2.fit(x_train, y_train)\n\nfeature_importances_rfe2= pd.DataFrame(columns=['important features'] )\nfeature_importances_rfe2['important features'] = x_train.columns[rfe2.support_]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances_rfe2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features2=pd.DataFrame(columns=['accuracy score', 'recall score', 'precision score',  'roc auc score'], index=num_features)\nfor ind in range(3, len(feature_importances_rfe2.index)+1):\n    \n    \n    x_gboost_train=x_train.loc[:, feature_importances_rfe2['important features'] [0:ind]]\n    x_gboost_test= x_test.loc[:, feature_importances_rfe2['important features'] [0:ind]]\n    \n    xgboost_model.fit(x_gboost_train, y_train)\n    y_pred_xgboost2= xgboost_model.predict(x_gboost_test)\n    \n    \n    results_rfecv_features2.iloc[ind-3, 0]=accuracy_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 1]=recall_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 2]=precision_score(y_test, y_pred_xgboost2)\n    results_rfecv_features2.iloc[ind-3, 3]=roc_auc_score(y_test, y_pred_xgboost2)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features2.sort_values(['roc auc score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features2.sort_values(['recall score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_rfecv_features2.sort_values(['accuracy score'], ascending= False).iloc[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"12 features provide the best scores\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost=pd.DataFrame(columns=[ 'XGBOOST_features_importances','XGBOOST_KBEST' ,'XGBOOST_rfecv'], index=['accuracy_score', 'recall_score', 'precision_score','roc_score'])\nresults_xgboost_features_sorted= results_xgboost_scores.sort_values('accuracy score', ascending=False)\nresults_xgboost_features_kbest_sorted= results_xgb_kbest.sort_values('accuracy score', ascending= False)\nresults_xgboost_rfecv_features_sorted= results_rfecv_features2.sort_values('accuracy score', ascending= False)\n\n\nresults_xgboost.iloc[0,0]=results_xgboost_features_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,0]=results_xgboost_features_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,0]=results_xgboost_features_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,0]=results_xgboost_features_sorted.iloc[0,:][3]\n\n\nresults_xgboost.iloc[0,1]=results_xgboost_features_kbest_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,1]=results_xgboost_features_kbest_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,1]=results_xgboost_features_kbest_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,1]=results_xgboost_features_kbest_sorted.iloc[0,:][3]\n\n\nresults_xgboost.iloc[0,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][0]\nresults_xgboost.iloc[1,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][1]\nresults_xgboost.iloc[2,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][2]\nresults_xgboost.iloc[3,2]=results_xgboost_rfecv_features_sorted.iloc[0,:][3]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_xgboost[results_xgboost.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the best reuslts with XGBOOST are  with 12 features.\n"},{"metadata":{},"cell_type":"markdown","source":"**now lets compare the highest results achieved in both xgboost and RandomForest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_random_forest=pd.DataFrame(columns=['xgboost', 'RandomForest'], index= ['accuracy_score', 'recall_score', 'precision_score', 'roc auc score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_random_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will take XGboost with rfe features selection and RandomFOrest with SelectKBest features selections and compare them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_xgboost_random_forest.iloc[0,0]= results_xgboost.iloc[0, 2]\nresults_xgboost_random_forest.iloc[1,0]=results_xgboost.iloc[1, 2]\nresults_xgboost_random_forest.iloc[2,0]=results_xgboost.iloc[2, 2]\nresults_xgboost_random_forest.iloc[3,0]=results_xgboost.iloc[3, 2]\n\nresults_xgboost_random_forest.iloc[0,1]=results_random_forest.iloc[0, 1]\nresults_xgboost_random_forest.iloc[1,1]=results_random_forest.iloc[1, 1]\nresults_xgboost_random_forest.iloc[2,1]=results_random_forest.iloc[2, 1]\nresults_xgboost_random_forest.iloc[3,1]=results_random_forest.iloc[4, 1]\nresults_xgboost_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 7))\nsns.heatmap(results_xgboost_random_forest[results_xgboost_random_forest.columns.to_list()].astype(float), cmap = 'Blues', annot = True, linewidths = 1, cbar = False, annot_kws = {'fontsize': 12},\n           yticklabels = ['accuracy score', 'Recall', 'precison', 'ROC AUC'])\nsns.set(font_scale = 1.5)\nplt.yticks(rotation = 0)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, RF did slightly better with accuracy and Auc score."},{"metadata":{},"cell_type":"markdown","source":"**finally just to get better insights I will use heatmap to see confusion matrix for both models**"},{"metadata":{},"cell_type":"markdown","source":"1. Confusion Matrix with XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_xgb_3=x_test.loc[: ,feature_importances_rfe2['important features'][0:12].to_list()]\nxgb_model2= XGBClassifier(random_state= 22, use_label_encoder=False)\nxgb_model2.fit(x_train.loc[: ,feature_importances_rfe2['important features'][0:12].to_list()], y_train)\n\ny_pred_xgb_3=xgb_model2.predict(x_test_xgb_3)\nxgb_cm= confusion_matrix(y_test, y_pred_xgb_3)\n\nsns.heatmap(xgb_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No heart disease', 'heart disease'], xticklabels = ['Predicted no heat disease', 'Predicted heart disease'])\nplt.yticks(rotation = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Confusion Matrix with RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_rf_11=x_test.loc[: ,feature_importances_kbest.index[0:9]]\nrf_model2= RandomForestClassifier()\nrf_model2.fit(x_train.loc[: ,feature_importances_kbest.index[0:9]], y_train)\n\ny_pred_rf_11=rf_model2.predict(x_test_rf_11)\nrf_cm= confusion_matrix(y_test, y_pred_rf_11)\n\nsns.heatmap(rf_cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No heart disease', 'heart disease'], xticklabels = ['Predicted no heat disease', 'Predicted heart disease'])\nplt.yticks(rotation = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well, fitting for the second time both models with the selected features gives relatively same results "},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nI think both models did well to be used for diagnosing Heart Disease based on this analysis maybe RandomForest slightly better.. \n\nthe RandomForest with the top 9 features ['thalach', 'ca', 'cp', 'oldpeak', 'exang', 'age', 'chol', 'trestbps','slope'].\n\nXGBOOST with 12 top features['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca']\n\nThis is my first notebook, please give me your feedback or some advice into how to improve the quality of my work and Thank you for your time. \n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}